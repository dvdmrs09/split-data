{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_and_write(self, reader, writer, chunk_size):\n        data = reader.recv(chunk_size)\n        if len(data) == 0:\n            return True\n        writer.sendall(data)", "response": "Read chunk_size from reader and write result to writer."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_local_user():\n    # TODO: I don't understand why these lines were added outside the\n    # try/except, since presumably it means the attempt at catching ImportError\n    # wouldn't work. However, that's how the contributing user committed it.\n    # Need an older Windows box to test it out, most likely.\n    import getpass\n\n    username = None\n    # All Unix and most Windows systems support the getpass module.\n    try:\n        username = getpass.getuser()\n    # Some SaaS platforms raise KeyError, implying there is no real user\n    # involved. They get the default value of None.\n    except KeyError:\n        pass\n    # Older (?) Windows systems don't support getpass well; they should\n    # have the `win32` module instead.\n    except ImportError:  # pragma: nocover\n        if win32:\n            import win32api\n            import win32security  # noqa\n            import win32profile  # noqa\n\n            username = win32api.GetUserName()\n    return username", "response": "Return the local executing username or None if one can t be found."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parameterize(self, call, host):\n        debug(\"Parameterizing {!r} for host {!r}\".format(call, host))\n        # Generate a custom ConnectionCall that knows how to yield a Connection\n        # in its make_context(), specifically one to the host requested here.\n        clone = call.clone(into=ConnectionCall)\n        # TODO: using bag-of-attrs is mildly gross but whatever, I'll take it.\n        clone.host = host\n        return clone", "response": "Parameterize a Call with its Context set to a per - host Config."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mixed_use_of_local_and_run(self):\n        cxn = Connection(\"localhost\")\n        result = cxn.local(\"echo foo\", hide=True)\n        assert result.stdout == \"foo\\n\"\n        assert not cxn.is_connected  # meh way of proving it didn't use SSH yet\n        result = cxn.run(\"echo foo\", hide=True)\n        assert cxn.is_connected  # NOW it's using SSH\n        assert result.stdout == \"foo\\n\"", "response": "mixed use of local and run command truly over SSH via localhost"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nopen an SSH connection to the host and port.", "response": "def open(self):\n        \"\"\"\n        Initiate an SSH connection to the host/port this object is bound to.\n\n        This may include activating the configured gateway connection, if one\n        is set.\n\n        Also saves a handle to the now-set Transport object for easier access.\n\n        Various connect-time settings (and/or their corresponding :ref:`SSH\n        config options <ssh-config>`) are utilized here in the call to\n        `SSHClient.connect <paramiko.client.SSHClient.connect>`. (For details,\n        see :doc:`the configuration docs </concepts/configuration>`.)\n\n        .. versionadded:: 2.0\n        \"\"\"\n        # Short-circuit\n        if self.is_connected:\n            return\n        err = \"Refusing to be ambiguous: connect() kwarg '{}' was given both via regular arg and via connect_kwargs!\"  # noqa\n        # These may not be given, period\n        for key in \"\"\"\n            hostname\n            port\n            username\n        \"\"\".split():\n            if key in self.connect_kwargs:\n                raise ValueError(err.format(key))\n        # These may be given one way or the other, but not both\n        if (\n            \"timeout\" in self.connect_kwargs\n            and self.connect_timeout is not None\n        ):\n            raise ValueError(err.format(\"timeout\"))\n        # No conflicts -> merge 'em together\n        kwargs = dict(\n            self.connect_kwargs,\n            username=self.user,\n            hostname=self.host,\n            port=self.port,\n        )\n        if self.gateway:\n            kwargs[\"sock\"] = self.open_gateway()\n        if self.connect_timeout:\n            kwargs[\"timeout\"] = self.connect_timeout\n        # Strip out empty defaults for less noisy debugging\n        if \"key_filename\" in kwargs and not kwargs[\"key_filename\"]:\n            del kwargs[\"key_filename\"]\n        # Actually connect!\n        self.client.connect(**kwargs)\n        self.transport = self.client.get_transport()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef open_gateway(self):\n        # ProxyCommand is faster to set up, so do it first.\n        if isinstance(self.gateway, string_types):\n            # Leverage a dummy SSHConfig to ensure %h/%p/etc are parsed.\n            # TODO: use real SSH config once loading one properly is\n            # implemented.\n            ssh_conf = SSHConfig()\n            dummy = \"Host {}\\n    ProxyCommand {}\"\n            ssh_conf.parse(StringIO(dummy.format(self.host, self.gateway)))\n            return ProxyCommand(ssh_conf.lookup(self.host)[\"proxycommand\"])\n        # Handle inner-Connection gateway type here.\n        # TODO: logging\n        self.gateway.open()\n        # TODO: expose the opened channel itself as an attribute? (another\n        # possible argument for separating the two gateway types...) e.g. if\n        # someone wanted to piggyback on it for other same-interpreter socket\n        # needs...\n        # TODO: and the inverse? allow users to supply their own socket/like\n        # object they got via $WHEREEVER?\n        # TODO: how best to expose timeout param? reuse general connection\n        # timeout from config?\n        return self.gateway.transport.open_channel(\n            kind=\"direct-tcpip\",\n            dest_addr=(self.host, int(self.port)),\n            # NOTE: src_addr needs to be 'empty but not None' values to\n            # correctly encode into a network message. Theoretically Paramiko\n            # could auto-interpret None sometime & save us the trouble.\n            src_addr=(\"\", 0),\n        )", "response": "Open a new connection from the gateway."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nterminate the network connection to the remote end if open.", "response": "def close(self):\n        \"\"\"\n        Terminate the network connection to the remote end, if open.\n\n        If no connection is open, this method does nothing.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        if self.is_connected:\n            self.client.close()\n            if self.forward_agent and self._agent_handler is not None:\n                self._agent_handler.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting a shell command on the remote end of this connection.", "response": "def run(self, command, **kwargs):\n        \"\"\"\n        Execute a shell command on the remote end of this connection.\n\n        This method wraps an SSH-capable implementation of\n        `invoke.runners.Runner.run`; see its documentation for details.\n\n        .. warning::\n            There are a few spots where Fabric departs from Invoke's default\n            settings/behaviors; they are documented under\n            `.Config.global_defaults`.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        runner = self.config.runners.remote(self)\n        return self._run(runner, command, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sudo(self, command, **kwargs):\n        runner = self.config.runners.remote(self)\n        return self._sudo(runner, command, **kwargs)", "response": "Execute a shell command on the remote end."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes a shell command on the local system.", "response": "def local(self, *args, **kwargs):\n        \"\"\"\n        Execute a shell command on the local system.\n\n        This method is effectively a wrapper of `invoke.run`; see its docs for\n        details and call signature.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        # Superclass run() uses runners.local, so we can literally just call it\n        # straight.\n        return super(Connection, self).run(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a new instance of the SFTP client.", "response": "def sftp(self):\n        \"\"\"\n        Return a `~paramiko.sftp_client.SFTPClient` object.\n\n        If called more than one time, memoizes the first result; thus, any\n        given `.Connection` instance will only ever have a single SFTP client,\n        and state (such as that managed by\n        `~paramiko.sftp_client.SFTPClient.chdir`) will be preserved.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        if self._sftp is None:\n            self._sftp = self.client.open_sftp()\n        return self._sftp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef forward_local(\n        self,\n        local_port,\n        remote_port=None,\n        remote_host=\"localhost\",\n        local_host=\"localhost\",\n    ):\n        \"\"\"\n        Open a tunnel connecting ``local_port`` to the server's environment.\n\n        For example, say you want to connect to a remote PostgreSQL database\n        which is locked down and only accessible via the system it's running\n        on. You have SSH access to this server, so you can temporarily make\n        port 5432 on your local system act like port 5432 on the server::\n\n            import psycopg2\n            from fabric import Connection\n\n            with Connection('my-db-server').forward_local(5432):\n                db = psycopg2.connect(\n                    host='localhost', port=5432, database='mydb'\n                )\n                # Do things with 'db' here\n\n        This method is analogous to using the ``-L`` option of OpenSSH's\n        ``ssh`` program.\n\n        :param int local_port: The local port number on which to listen.\n\n        :param int remote_port:\n            The remote port number. Defaults to the same value as\n            ``local_port``.\n\n        :param str local_host:\n            The local hostname/interface on which to listen. Default:\n            ``localhost``.\n\n        :param str remote_host:\n            The remote hostname serving the forwarded remote port. Default:\n            ``localhost`` (i.e., the host this `.Connection` is connected to.)\n\n        :returns:\n            Nothing; this method is only useful as a context manager affecting\n            local operating system state.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        if not remote_port:\n            remote_port = local_port\n\n        # TunnelManager does all of the work, sitting in the background (so we\n        # can yield) and spawning threads every time somebody connects to our\n        # local port.\n        finished = Event()\n        manager = TunnelManager(\n            local_port=local_port,\n            local_host=local_host,\n            remote_port=remote_port,\n            remote_host=remote_host,\n            # TODO: not a huge fan of handing in our transport, but...?\n            transport=self.transport,\n            finished=finished,\n        )\n        manager.start()\n\n        # Return control to caller now that things ought to be operational\n        try:\n            yield\n        # Teardown once user exits block\n        finally:\n            # Signal to manager that it should close all open tunnels\n            finished.set()\n            # Then wait for it to do so\n            manager.join()\n            # Raise threading errors from within the manager, which would be\n            # one of:\n            # - an inner ThreadException, which was created by the manager on\n            # behalf of its Tunnels; this gets directly raised.\n            # - some other exception, which would thus have occurred in the\n            # manager itself; we wrap this in a new ThreadException.\n            # NOTE: in these cases, some of the metadata tracking in\n            # ExceptionHandlingThread/ExceptionWrapper/ThreadException (which\n            # is useful when dealing with multiple nearly-identical sibling IO\n            # threads) is superfluous, but it doesn't feel worth breaking\n            # things up further; we just ignore it for now.\n            wrapper = manager.exception()\n            if wrapper is not None:\n                if wrapper.type is ThreadException:\n                    raise wrapper.value\n                else:\n                    raise ThreadException([wrapper])", "response": "Forward a local entry to the local operating system."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef forward_remote(\n        self,\n        remote_port,\n        local_port=None,\n        remote_host=\"127.0.0.1\",\n        local_host=\"localhost\",\n    ):\n        \"\"\"\n        Open a tunnel connecting ``remote_port`` to the local environment.\n\n        For example, say you're running a daemon in development mode on your\n        workstation at port 8080, and want to funnel traffic to it from a\n        production or staging environment.\n\n        In most situations this isn't possible as your office/home network\n        probably blocks inbound traffic. But you have SSH access to this\n        server, so you can temporarily make port 8080 on that server act like\n        port 8080 on your workstation::\n\n            from fabric import Connection\n\n            c = Connection('my-remote-server')\n            with c.forward_remote(8080):\n                c.run(\"remote-data-writer --port 8080\")\n                # Assuming remote-data-writer runs until interrupted, this will\n                # stay open until you Ctrl-C...\n\n        This method is analogous to using the ``-R`` option of OpenSSH's\n        ``ssh`` program.\n\n        :param int remote_port: The remote port number on which to listen.\n\n        :param int local_port:\n            The local port number. Defaults to the same value as\n            ``remote_port``.\n\n        :param str local_host:\n            The local hostname/interface the forwarded connection talks to.\n            Default: ``localhost``.\n\n        :param str remote_host:\n            The remote interface address to listen on when forwarding\n            connections. Default: ``127.0.0.1`` (i.e. only listen on the remote\n            localhost).\n\n        :returns:\n            Nothing; this method is only useful as a context manager affecting\n            local operating system state.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        if not local_port:\n            local_port = remote_port\n        # Callback executes on each connection to the remote port and is given\n        # a Channel hooked up to said port. (We don't actually care about the\n        # source/dest host/port pairs at all; only whether the channel has data\n        # to read and suchlike.)\n        # We then pair that channel with a new 'outbound' socket connection to\n        # the local host/port being forwarded, in a new Tunnel.\n        # That Tunnel is then added to a shared data structure so we can track\n        # & close them during shutdown.\n        #\n        # TODO: this approach is less than ideal because we have to share state\n        # between ourselves & the callback handed into the transport's own\n        # thread handling (which is roughly analogous to our self-controlled\n        # TunnelManager for local forwarding). See if we can use more of\n        # Paramiko's API (or improve it and then do so) so that isn't\n        # necessary.\n        tunnels = []\n\n        def callback(channel, src_addr_tup, dst_addr_tup):\n            sock = socket.socket()\n            # TODO: handle connection failure such that channel, etc get closed\n            sock.connect((local_host, local_port))\n            # TODO: we don't actually need to generate the Events at our level,\n            # do we? Just let Tunnel.__init__ do it; all we do is \"press its\n            # button\" on shutdown...\n            tunnel = Tunnel(channel=channel, sock=sock, finished=Event())\n            tunnel.start()\n            # Communication between ourselves & the Paramiko handling subthread\n            tunnels.append(tunnel)\n\n        # Ask Paramiko (really, the remote sshd) to call our callback whenever\n        # connections are established on the remote iface/port.\n        # transport.request_port_forward(remote_host, remote_port, callback)\n        try:\n            self.transport.request_port_forward(\n                address=remote_host, port=remote_port, handler=callback\n            )\n            yield\n        finally:\n            # TODO: see above re: lack of a TunnelManager\n            # TODO: and/or also refactor with TunnelManager re: shutdown logic.\n            # E.g. maybe have a non-thread TunnelManager-alike with a method\n            # that acts as the callback? At least then there's a tiny bit more\n            # encapsulation...meh.\n            for tunnel in tunnels:\n                tunnel.finished.set()\n                tunnel.join()\n            self.transport.cancel_port_forward(\n                address=remote_host, port=remote_port\n            )", "response": "Forward a new connection to the remote server."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dedup_list(l):\n    dedup = set()\n    return [ x for x in l if not (x in dedup or dedup.add(x))]", "response": "Given a list l will remove duplicates from the list and return a list with the original order of the list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse(s, g):\n    # The CYK table. Indexed with a 2-tuple: (start pos, end pos)\n    table = defaultdict(set)\n    # Top-level structure is similar to the CYK table. Each cell is a dict from\n    # rule name to the best (lightest) tree for that rule.\n    trees = defaultdict(dict)\n    # Populate base case with existing terminal production rules\n    for i, w in enumerate(s):\n        for terminal, rules in g.terminal_rules.items():\n            if match(terminal, w):\n                for rule in rules:\n                    table[(i, i)].add(rule)\n                    if (rule.lhs not in trees[(i, i)] or\n                        rule.weight < trees[(i, i)][rule.lhs].weight):\n                        trees[(i, i)][rule.lhs] = RuleNode(rule, [T(w)], weight=rule.weight)\n\n    # Iterate over lengths of sub-sentences\n    for l in xrange(2, len(s) + 1):\n        # Iterate over sub-sentences with the given length\n        for i in xrange(len(s) - l + 1):\n            # Choose partition of the sub-sentence in [1, l)\n            for p in xrange(i + 1, i + l):\n                span1 = (i, p - 1)\n                span2 = (p, i + l - 1)\n                for r1, r2 in itertools.product(table[span1], table[span2]):\n                    for rule in g.nonterminal_rules.get((r1.lhs, r2.lhs), []):\n                        table[(i, i + l - 1)].add(rule)\n                        r1_tree = trees[span1][r1.lhs]\n                        r2_tree = trees[span2][r2.lhs]\n                        rule_total_weight = rule.weight + r1_tree.weight + r2_tree.weight\n                        if (rule.lhs not in trees[(i, i + l - 1)]\n                            or rule_total_weight < trees[(i, i + l - 1)][rule.lhs].weight):\n                            trees[(i, i + l - 1)][rule.lhs] = RuleNode(rule, [r1_tree, r2_tree], weight=rule_total_weight)\n    return table, trees", "response": "Parses a sentence s using CNF grammar g."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_any_nt_unit_rule(g):\n    for rule in g.rules:\n        if len(rule.rhs) == 1 and isinstance(rule.rhs[0], NT):\n            return rule\n    return None", "response": "Returns a non - terminal unit rule from g or None if there is none."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving a rule from g without changing the langugage produced by g.", "response": "def _remove_unit_rule(g, rule):\n    \"\"\"Removes 'rule' from 'g' without changing the langugage produced by 'g'.\"\"\"\n    new_rules = [x for x in g.rules if x != rule]\n    refs = [x for x in g.rules if x.lhs == rule.rhs[0]]\n    new_rules += [build_unit_skiprule(rule, ref) for ref in refs]\n    return Grammar(new_rules)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsplit a rule into shorter rules.", "response": "def _split(rule):\n    \"\"\"Splits a rule whose len(rhs) > 2 into shorter rules.\"\"\"\n    rule_str = str(rule.lhs) + '__' + '_'.join(str(x) for x in rule.rhs)\n    rule_name = '__SP_%s' % (rule_str) + '_%d'\n    yield Rule(rule.lhs, [rule.rhs[0], NT(rule_name % 1)], weight=rule.weight, alias=rule.alias)\n    for i in xrange(1, len(rule.rhs) - 2):\n        yield Rule(NT(rule_name % i), [rule.rhs[i], NT(rule_name % (i + 1))], weight=0, alias='Split')\n    yield Rule(NT(rule_name % (len(rule.rhs) - 2)), rule.rhs[-2:], weight=0, alias='Split')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\napplying the TERM rule on g.", "response": "def _term(g):\n    \"\"\"Applies the TERM rule on 'g' (see top comment).\"\"\"\n    all_t = {x for rule in g.rules for x in rule.rhs if isinstance(x, T)}\n    t_rules = {t: Rule(NT('__T_%s' % str(t)), [t], weight=0, alias='Term') for t in all_t}\n    new_rules = []\n    for rule in g.rules:\n        if len(rule.rhs) > 1 and any(isinstance(x, T) for x in rule.rhs):\n            new_rhs = [t_rules[x].lhs if isinstance(x, T) else x for x in rule.rhs]\n            new_rules.append(Rule(rule.lhs, new_rhs, weight=rule.weight, alias=rule.alias))\n            new_rules.extend(v for k, v in t_rules.items() if k in rule.rhs)\n        else:\n            new_rules.append(rule)\n    return Grammar(new_rules)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\napplying the BIN rule to g.", "response": "def _bin(g):\n    \"\"\"Applies the BIN rule to 'g' (see top comment).\"\"\"\n    new_rules = []\n    for rule in g.rules:\n        if len(rule.rhs) > 2:\n            new_rules += _split(rule)\n        else:\n            new_rules.append(rule)\n    return Grammar(new_rules)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\napply the UNIT rule to g.", "response": "def _unit(g):\n    \"\"\"Applies the UNIT rule to 'g' (see top comment).\"\"\"\n    nt_unit_rule = get_any_nt_unit_rule(g)\n    while nt_unit_rule:\n        g = _remove_unit_rule(g, nt_unit_rule)\n        nt_unit_rule = get_any_nt_unit_rule(g)\n    return g"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef revert_cnf(node):\n    if isinstance(node, T):\n        return node\n    # Reverts TERM rule.\n    if node.rule.lhs.name.startswith('__T_'):\n        return node.children[0]\n    else:\n        children = []\n        for child in map(revert_cnf, node.children):\n            # Reverts BIN rule.\n            if isinstance(child, RuleNode) and child.rule.lhs.name.startswith('__SP_'):\n                children += child.children\n            else:\n                children.append(child)\n        # Reverts UNIT rule.\n        if isinstance(node.rule, UnitSkipRule):\n            return unroll_unit_skiprule(node.rule.lhs, node.rule.rhs,\n                                    node.rule.skipped_rules, children,\n                                    node.rule.weight, node.rule.alias)\n        else:\n            return RuleNode(node.rule, children)", "response": "Reverts a parse tree ( RuleNode to its original non - CNF form ( Node )."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a lark rule from a Lark VM to a Rule.", "response": "def _to_rule(self, lark_rule):\n        \"\"\"Converts a lark rule, (lhs, rhs, callback, options), to a Rule.\"\"\"\n        assert isinstance(lark_rule.origin, NT)\n        assert all(isinstance(x, Symbol) for x in lark_rule.expansion)\n        return Rule(\n            lark_rule.origin, lark_rule.expansion,\n            weight=lark_rule.options.priority if lark_rule.options and lark_rule.options.priority else 0,\n            alias=lark_rule)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse(self, tokenized):  # pylint: disable=invalid-name\n        table, trees = _parse(tokenized, self.grammar)\n        # Check if the parse succeeded.\n        if all(r.lhs != self.start for r in table[(0, len(tokenized) - 1)]):\n            raise ParseError('Parsing failed.')\n        parse = trees[(0, len(tokenized) - 1)][self.start]\n        return self._to_tree(revert_cnf(parse))", "response": "Parses input which is a list of tokens."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _to_tree(self, rule_node):\n        orig_rule = self.orig_rules[rule_node.rule.alias]\n        children = []\n        for child in rule_node.children:\n            if isinstance(child, RuleNode):\n                children.append(self._to_tree(child))\n            else:\n                assert isinstance(child.name, Token)\n                children.append(child.name)\n        t = Tree(orig_rule.origin, children)\n        t.rule=orig_rule\n        return t", "response": "Converts a RuleNode parse tree to a lark Tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pydot__tree_to_png(tree, filename, rankdir=\"LR\"):\n\n    import pydot\n    graph = pydot.Dot(graph_type='digraph', rankdir=rankdir)\n\n    i = [0]\n\n    def new_leaf(leaf):\n        node = pydot.Node(i[0], label=repr(leaf))\n        i[0] += 1\n        graph.add_node(node)\n        return node\n\n    def _to_pydot(subtree):\n        color = hash(subtree.data) & 0xffffff\n        color |= 0x808080\n\n        subnodes = [_to_pydot(child) if isinstance(child, Tree) else new_leaf(child)\n                    for child in subtree.children]\n        node = pydot.Node(i[0], style=\"filled\", fillcolor=\"#%x\"%color, label=subtree.data)\n        i[0] += 1\n        graph.add_node(node)\n\n        for subnode in subnodes:\n            graph.add_edge(pydot.Edge(node, subnode))\n\n        return node\n\n    _to_pydot(tree)\n    graph.write_png(filename)", "response": "Writes a colorful image that represents the tree."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexpanding ( inline ) children at the given indices", "response": "def expand_kids_by_index(self, *indices):\n        \"Expand (inline) children at the given indices\"\n        for i in sorted(indices, reverse=True): # reverse so that changing tail won't affect indices\n            kid = self.children[i]\n            self.children[i:i+1] = kid.children"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a parser instance and a dictionary mapping some label with the same name and a list of malformed syntax examples it ll return the label for the other examples that bests matches the current error.", "response": "def match_examples(self, parse_fn, examples):\n        \"\"\" Given a parser instance and a dictionary mapping some label with\n            some malformed syntax examples, it'll return the label for the\n            example that bests matches the current error.\n        \"\"\"\n        assert self.state is not None, \"Not supported for this exception\"\n\n        candidate = None\n        for label, example in examples.items():\n            assert not isinstance(example, STRING_TYPE)\n\n            for malformed in example:\n                try:\n                    parse_fn(malformed)\n                except UnexpectedInput as ut:\n                    if ut.state == self.state:\n                        try:\n                            if ut.token == self.token:  # Try exact match first\n                                return label\n                        except AttributeError:\n                            pass\n                        if not candidate:\n                            candidate = label\n\n        return candidate"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef feed(self, token, test_newline=True):\n        if test_newline:\n            newlines = token.count(self.newline_char)\n            if newlines:\n                self.line += newlines\n                self.line_start_pos = self.char_pos + token.rindex(self.newline_char) + 1\n\n        self.char_pos += len(token)\n        self.column = self.char_pos - self.line_start_pos + 1", "response": "Consume a token and calculate the new line & column."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an instance of Lark with the grammar given by its filename.", "response": "def open(cls, grammar_filename, rel_to=None, **options):\n        \"\"\"Create an instance of Lark with the grammar given by its filename\n\n        If rel_to is provided, the function will find the grammar filename in relation to it.\n\n        Example:\n\n            >>> Lark.open(\"grammar_file.lark\", rel_to=__file__, parser=\"lalr\")\n            Lark(...)\n\n        \"\"\"\n        if rel_to:\n            basepath = os.path.dirname(rel_to)\n            grammar_filename = os.path.join(basepath, grammar_filename)\n        with open(grammar_filename, encoding='utf8') as f:\n            return cls(f, **options)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate FOLLOW sets. Adapted from: http://lara.epfl.ch/w/cc09:algorithm_for_first_and_follow_sets", "response": "def calculate_sets(rules):\n    \"\"\"Calculate FOLLOW sets.\n\n    Adapted from: http://lara.epfl.ch/w/cc09:algorithm_for_first_and_follow_sets\"\"\"\n    symbols = {sym for rule in rules for sym in rule.expansion} | {rule.origin for rule in rules}\n\n    # foreach grammar rule X ::= Y(1) ... Y(k)\n    # if k=0 or {Y(1),...,Y(k)} subset of NULLABLE then\n    #   NULLABLE = NULLABLE union {X}\n    # for i = 1 to k\n    #   if i=1 or {Y(1),...,Y(i-1)} subset of NULLABLE then\n    #     FIRST(X) = FIRST(X) union FIRST(Y(i))\n    #   for j = i+1 to k\n    #     if i=k or {Y(i+1),...Y(k)} subset of NULLABLE then\n    #       FOLLOW(Y(i)) = FOLLOW(Y(i)) union FOLLOW(X)\n    #     if i+1=j or {Y(i+1),...,Y(j-1)} subset of NULLABLE then\n    #       FOLLOW(Y(i)) = FOLLOW(Y(i)) union FIRST(Y(j))\n    # until none of NULLABLE,FIRST,FOLLOW changed in last iteration\n\n    NULLABLE = set()\n    FIRST = {}\n    FOLLOW = {}\n    for sym in symbols:\n        FIRST[sym]={sym} if sym.is_term else set()\n        FOLLOW[sym]=set()\n\n    # Calculate NULLABLE and FIRST\n    changed = True\n    while changed:\n        changed = False\n\n        for rule in rules:\n            if set(rule.expansion) <= NULLABLE:\n                if update_set(NULLABLE, {rule.origin}):\n                    changed = True\n\n            for i, sym in enumerate(rule.expansion):\n                if set(rule.expansion[:i]) <= NULLABLE:\n                    if update_set(FIRST[rule.origin], FIRST[sym]):\n                        changed = True\n\n    # Calculate FOLLOW\n    changed = True\n    while changed:\n        changed = False\n\n        for rule in rules:\n            for i, sym in enumerate(rule.expansion):\n                if i==len(rule.expansion)-1 or set(rule.expansion[i+1:]) <= NULLABLE:\n                    if update_set(FOLLOW[sym], FOLLOW[rule.origin]):\n                        changed = True\n\n                for j in range(i+1, len(rule.expansion)):\n                    if set(rule.expansion[i+1:j]) <= NULLABLE:\n                        if update_set(FOLLOW[sym], FIRST[rule.expansion[j]]):\n                            changed = True\n\n    return FIRST, FOLLOW, NULLABLE"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns all init_ptrs accessible by rule ( recursive )", "response": "def expand_rule(self, rule):\n        \"Returns all init_ptrs accessible by rule (recursive)\"\n        init_ptrs = set()\n        def _expand_rule(rule):\n            assert not rule.is_term, rule\n\n            for r in self.rules_by_origin[rule]:\n                init_ptr = RulePtr(r, 0)\n                init_ptrs.add(init_ptr)\n\n                if r.expansion: # if not empty rule\n                    new_r = init_ptr.next\n                    if not new_r.is_term:\n                        yield new_r\n\n        for _ in bfs([rule], _expand_rule):\n            pass\n\n        return fzset(init_ptrs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn all rules and terminals of grammar prepended by a namespace prefix except for those which are aliased.", "response": "def import_from_grammar_into_namespace(grammar, namespace, aliases):\n    \"\"\"Returns all rules and terminals of grammar, prepended\n    with a 'namespace' prefix, except for those which are aliased.\n    \"\"\"\n\n    imported_terms = dict(grammar.term_defs)\n    imported_rules = {n:(n,deepcopy(t),o) for n,t,o in grammar.rule_defs}\n\n    term_defs = []\n    rule_defs = []\n\n    def rule_dependencies(symbol):\n        if symbol.type != 'RULE':\n            return []\n        try:\n            _, tree, _ = imported_rules[symbol]\n        except KeyError:\n            raise GrammarError(\"Missing symbol '%s' in grammar %s\" % (symbol, namespace))\n        return tree.scan_values(lambda x: x.type in ('RULE', 'TERMINAL'))\n\n    def get_namespace_name(name):\n        try:\n            return aliases[name].value\n        except KeyError:\n            if name[0] == '_':\n                return '_%s__%s' % (namespace, name[1:])\n            return '%s__%s' % (namespace, name)\n\n    to_import = list(bfs(aliases, rule_dependencies))\n    for symbol in to_import:\n        if symbol.type == 'TERMINAL':\n            term_defs.append([get_namespace_name(symbol), imported_terms[symbol]])\n        else:\n            assert symbol.type == 'RULE'\n            rule = imported_rules[symbol]\n            for t in rule[1].iter_subtrees():\n                for i, c in enumerate(t.children):\n                    if isinstance(c, Token) and c.type in ('RULE', 'TERMINAL'):\n                        t.children[i] = Token(c.type, get_namespace_name(c))\n            rule_defs.append((get_namespace_name(symbol), rule[1], rule[2]))\n\n    return term_defs, rule_defs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing grammar_text verify and create Grammar object. Display nice messages on error.", "response": "def load_grammar(self, grammar_text, grammar_name='<?>'):\n        \"Parse grammar_text, verify, and create Grammar object. Display nice messages on error.\"\n\n        try:\n            tree = self.canonize_tree.transform( self.parser.parse(grammar_text+'\\n') )\n        except UnexpectedCharacters as e:\n            context = e.get_context(grammar_text)\n            raise GrammarError(\"Unexpected input at line %d column %d in %s: \\n\\n%s\" %\n                               (e.line, e.column, grammar_name, context))\n        except UnexpectedToken as e:\n            context = e.get_context(grammar_text)\n            error = e.match_examples(self.parser.parse, {\n                'Unclosed parenthesis': ['a: (\\n'],\n                'Umatched closing parenthesis': ['a: )\\n', 'a: [)\\n', 'a: (]\\n'],\n                'Expecting rule or terminal definition (missing colon)': ['a\\n', 'a->\\n', 'A->\\n', 'a A\\n'],\n                'Alias expects lowercase name': ['a: -> \"a\"\\n'],\n                'Unexpected colon': ['a::\\n', 'a: b:\\n', 'a: B:\\n', 'a: \"a\":\\n'],\n                'Misplaced operator': ['a: b??', 'a: b(?)', 'a:+\\n', 'a:?\\n', 'a:*\\n', 'a:|*\\n'],\n                'Expecting option (\"|\") or a new rule or terminal definition': ['a:a\\n()\\n'],\n                '%import expects a name': ['%import \"a\"\\n'],\n                '%ignore expects a value': ['%ignore %import\\n'],\n            })\n            if error:\n                raise GrammarError(\"%s at line %s column %s\\n\\n%s\" % (error, e.line, e.column, context))\n            elif 'STRING' in e.expected:\n                raise GrammarError(\"Expecting a value at line %s column %s\\n\\n%s\" % (e.line, e.column, context))\n            raise\n\n        tree = PrepareGrammar().transform(tree)\n\n        # Extract grammar items\n        defs = classify(tree.children, lambda c: c.data, lambda c: c.children)\n        term_defs = defs.pop('term', [])\n        rule_defs = defs.pop('rule', [])\n        statements = defs.pop('statement', [])\n        assert not defs\n\n        term_defs = [td if len(td)==3 else (td[0], 1, td[1]) for td in term_defs]\n        term_defs = [(name.value, (t, int(p))) for name, p, t in term_defs]\n        rule_defs = [options_from_rule(*x) for x in rule_defs]\n\n        # Execute statements\n        ignore, imports = [], {}\n        for (stmt,) in statements:\n            if stmt.data == 'ignore':\n                t ,= stmt.children\n                ignore.append(t)\n            elif stmt.data == 'import':\n                if len(stmt.children) > 1:\n                    path_node, arg1 = stmt.children\n                else:\n                    path_node, = stmt.children\n                    arg1 = None\n\n                if isinstance(arg1, Tree):  # Multi import\n                    dotted_path = tuple(path_node.children)\n                    names = arg1.children\n                    aliases = dict(zip(names, names))  # Can't have aliased multi import, so all aliases will be the same as names\n                else:  # Single import\n                    dotted_path = tuple(path_node.children[:-1])\n                    name = path_node.children[-1]  # Get name from dotted path\n                    aliases = {name: arg1 or name}  # Aliases if exist\n\n                if path_node.data == 'import_lib':  # Import from library\n                    base_paths = []\n                else:  # Relative import\n                    if grammar_name == '<string>':  # Import relative to script file path if grammar is coded in script\n                        try:\n                            base_file = os.path.abspath(sys.modules['__main__'].__file__)\n                        except AttributeError:\n                            base_file = None\n                    else:\n                        base_file = grammar_name  # Import relative to grammar file path if external grammar file\n                    if base_file:\n                        base_paths = [os.path.split(base_file)[0]]\n                    else:\n                        base_paths = [os.path.abspath(os.path.curdir)]\n\n                try:\n                    import_base_paths, import_aliases = imports[dotted_path]\n                    assert base_paths == import_base_paths, 'Inconsistent base_paths for %s.' % '.'.join(dotted_path)\n                    import_aliases.update(aliases)\n                except KeyError:\n                    imports[dotted_path] = base_paths, aliases\n\n            elif stmt.data == 'declare':\n                for t in stmt.children:\n                    term_defs.append([t.value, (None, None)])\n            else:\n                assert False, stmt\n\n        # import grammars\n        for dotted_path, (base_paths, aliases) in imports.items():\n            grammar_path = os.path.join(*dotted_path) + EXT\n            g = import_grammar(grammar_path, base_paths=base_paths)\n            new_td, new_rd = import_from_grammar_into_namespace(g, '__'.join(dotted_path), aliases)\n\n            term_defs += new_td\n            rule_defs += new_rd\n\n        # Verify correctness 1\n        for name, _ in term_defs:\n            if name.startswith('__'):\n                raise GrammarError('Names starting with double-underscore are reserved (Error at %s)' % name)\n\n        # Handle ignore tokens\n        # XXX A slightly hacky solution. Recognition of %ignore TERMINAL as separate comes from the lexer's\n        #     inability to handle duplicate terminals (two names, one value)\n        ignore_names = []\n        for t in ignore:\n            if t.data=='expansions' and len(t.children) == 1:\n                t2 ,= t.children\n                if t2.data=='expansion' and len(t2.children) == 1:\n                    item ,= t2.children\n                    if item.data == 'value':\n                        item ,= item.children\n                        if isinstance(item, Token) and item.type == 'TERMINAL':\n                            ignore_names.append(item.value)\n                            continue\n\n            name = '__IGNORE_%d'% len(ignore_names)\n            ignore_names.append(name)\n            term_defs.append((name, (t, 1)))\n\n        # Verify correctness 2\n        terminal_names = set()\n        for name, _ in term_defs:\n            if name in terminal_names:\n                raise GrammarError(\"Terminal '%s' defined more than once\" % name)\n            terminal_names.add(name)\n\n        if set(ignore_names) > terminal_names:\n            raise GrammarError(\"Terminals %s were marked to ignore but were not defined!\" % (set(ignore_names) - terminal_names))\n\n        resolve_term_references(term_defs)\n\n        rules = rule_defs\n\n        rule_names = set()\n        for name, _x, _o in rules:\n            if name.startswith('__'):\n                raise GrammarError('Names starting with double-underscore are reserved (Error at %s)' % name)\n            if name in rule_names:\n                raise GrammarError(\"Rule '%s' defined more than once\" % name)\n            rule_names.add(name)\n\n        for name, expansions, _o in rules:\n            used_symbols = {t for x in expansions.find_data('expansion')\n                              for t in x.scan_values(lambda t: t.type in ('RULE', 'TERMINAL'))}\n            for sym in used_symbols:\n                if sym.type == 'TERMINAL':\n                    if sym not in terminal_names:\n                        raise GrammarError(\"Token '%s' used but not defined (in rule %s)\" % (sym, name))\n                else:\n                    if sym not in rule_names:\n                        raise GrammarError(\"Rule '%s' used but not defined (in rule %s)\" % (sym, name))\n\n        return Grammar(rules, term_defs, ignore_names)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\napplying the style rules to the radio group.", "response": "def applyStyleRules(self):\r\n        \"\"\"\r\n        Conditionally disabled/enables form fields based on the current\r\n        section in the radio group\r\n        \"\"\"\r\n        for button, widget in zip(self.radioButtons, self.widgets):\r\n            if isinstance(widget, CheckBox):\r\n                widget.hideInput()\r\n            if not button.GetValue(): # not checked\r\n                widget.widget.Disable()\r\n            else:\r\n                widget.widget.Enable()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handleImplicitCheck(self):\r\n        for button, widget in zip(self.radioButtons, self.widgets):\r\n            if isinstance(widget, CheckBox):\r\n                if button.GetValue(): # checked\r\n                    widget.setValue(True)\r\n                else:\r\n                    widget.setValue(False)", "response": "Handles implicit check of the Radio button."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef createWidgets(self):\r\n        from gooey.gui.components import widgets\r\n        return [getattr(widgets, item['type'])(self, item)\r\n                for item in getin(self.widgetInfo, ['data', 'widgets'], [])]", "response": "Instantiate the Gooey Widgets that are used within the RadioGroup"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef onStart(self, *args, **kwarg):\r\n        with transactUI(self):\r\n            config = self.navbar.getActiveConfig()\r\n            config.resetErrors()\r\n            if config.isValid():\r\n                self.clientRunner.run(self.buildCliString())\r\n                self.showConsole()\r\n            else:\r\n                config.displayErrors()\r\n                self.Layout()", "response": "Called when the user is ready to start the client."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef onEdit(self):\r\n        with transactUI(self):\r\n            if self.buildSpec['poll_external_updates']:\r\n                self.fetchExternalUpdates()\r\n            self.showSettings()", "response": "Return the user to the settings screen for further editing"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef buildCliString(self):\r\n        config = self.navbar.getActiveConfig()\r\n        group = self.buildSpec['widgets'][self.navbar.getSelectedGroup()]\r\n        positional = config.getPositionalArgs()\r\n        optional = config.getOptionalArgs()\r\n        print(cli.buildCliString(\r\n            self.buildSpec['target'],\r\n            group['command'],\r\n            positional,\r\n            optional\r\n        ))\r\n        return cli.buildCliString(\r\n            self.buildSpec['target'],\r\n            group['command'],\r\n            positional,\r\n            optional\r\n        )", "response": "Collect all of the required information from the config screen and build a CLI string which can be used to invoke the client program"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall when the client is finished.", "response": "def onComplete(self, *args, **kwargs):\r\n        \"\"\"\r\n        Display the appropriate screen based on the success/fail of the\r\n        host program\r\n        \"\"\"\r\n        with transactUI(self):\r\n            if self.clientRunner.was_success():\r\n                if self.buildSpec.get('return_to_config', False):\r\n                    self.showSettings()\r\n                else:\r\n                    self.showSuccess()\r\n                    if self.buildSpec.get('show_success_modal', True):\r\n                        wx.CallAfter(modals.showSuccess)\r\n            else:\r\n                if self.clientRunner.wasForcefullyStopped:\r\n                    self.showForceStopped()\r\n                else:\r\n                    self.showError()\r\n                    wx.CallAfter(modals.showFailure)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fetchExternalUpdates(self):\r\n        seeds = seeder.fetchDynamicProperties(\r\n            self.buildSpec['target'],\r\n            self.buildSpec['encoding']\r\n        )\r\n        for config in self.configs:\r\n            config.seedUI(seeds)", "response": "Fetch the seed values from the seeder and update the UI"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the appropriate layout navigation component based on user preferences", "response": "def buildNavigation(self):\r\n        \"\"\"\r\n        Chooses the appropriate layout navigation component based on user prefs\r\n        \"\"\"\r\n        if self.buildSpec['navigation'] == constants.TABBED:\r\n            navigation = Tabbar(self, self.buildSpec, self.configs)\r\n        else:\r\n            navigation = Sidebar(self, self.buildSpec, self.configs)\r\n            if self.buildSpec['navigation'] == constants.HIDDEN:\r\n                navigation.Hide()\r\n        return navigation"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getFontFace(self):\r\n        userFace = self.buildSpec['terminal_font_family'] or self.defaultFont.GetFaceName()\r\n        return (''\r\n                if self.buildSpec['monospace_display']\r\n                else userFace)", "response": "Choose the best font face available given the user options"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _forward_stdout(self, process):\r\n        '''\r\n        Reads the stdout of `process` and forwards lines and progress\r\n        to any interested subscribers\r\n        '''\r\n        while True:\r\n            line = process.stdout.readline()\r\n            if not line:\r\n                break\r\n            pub.send_message(events.CONSOLE_UPDATE, msg=line.decode(self.encoding))\r\n            pub.send_message(events.PROGRESS_UPDATE,\r\n                             progress=self._extract_progress(line))\r\n        pub.send_message(events.EXECUTION_COMPLETE)", "response": "Reads the stdout of process and forwards lines and progress to any interested subscribers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract the progress information from the text using the progress_regex and calculate instructions", "response": "def _extract_progress(self, text):\r\n        '''\r\n        Finds progress information in the text using the\r\n        user-supplied regex and calculation instructions\r\n        '''\r\n        # monad-ish dispatch to avoid the if/else soup\r\n        find = partial(re.search, string=text.strip().decode(self.encoding))\r\n        regex = unit(self.progress_regex)\r\n        match = bind(regex, find)\r\n        result = bind(match, self._calculate_progress)\r\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the final progress value found by the regex match", "response": "def _calculate_progress(self, match):\r\n        '''\r\n        Calculates the final progress value found by the regex\r\n        '''\r\n        if not self.progress_expr:\r\n            return safe_float(match.group(1))\r\n        else:\r\n            return self._eval_progress(match)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhides and show configuration panels based on the currently selected option in the sidebar", "response": "def swapConfigPanels(self, event):\r\n        \"\"\"Hide/show configuration panels based on the currently selected\r\n         option in the sidebar \"\"\"\r\n        for id, panel in enumerate(self.configPanels):\r\n            panel.Hide()\r\n        self.activeSelection = event.Selection\r\n        self.configPanels[event.Selection].Show()\r\n        self._parent.Layout()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef updateProgressBar(self, *args, **kwargs):\r\n        '''\r\n         value, disable_animation=False\r\n        :param args:\r\n        :param kwargs:\r\n        :return:\r\n        '''\r\n        value = kwargs.get('progress')\r\n        pb = self.progress_bar\r\n        if value is None:\r\n            return\r\n        if value < 0:\r\n            pb.Pulse()\r\n        else:\r\n            value = min(int(value), pb.GetRange())\r\n            if pb.GetValue() != value:\r\n                # Windows 7 progress bar animation hack\r\n                # http://stackoverflow.com/questions/5332616/disabling-net-progressbar-animation-when-changing-value\r\n                if self.buildSpec['disable_progress_bar_animation'] \\\r\n                        and sys.platform.startswith(\"win\"):\r\n                    if pb.GetRange() == value:\r\n                        pb.SetValue(value)\r\n                        pb.SetValue(value - 1)\r\n                    else:\r\n                        pb.SetValue(value + 1)\r\n                pb.SetValue(value)", "response": "Updates the progress bar with the new value."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fetchDynamicProperties(target, encoding):\r\n    cmd = '{} {}'.format(target, 'gooey-seed-ui --ignore-gooey')\r\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\r\n    if proc.returncode != 0:\r\n        out, _ = proc.communicate()\r\n        return json.loads(out.decode(encoding))\r\n    else:\r\n        # TODO: useful feedback\r\n        return {}", "response": "Sends a gooey - seed - ui request to the client program it retrieve dynamically generated defaults with which to seed the UI"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a group of items and returns a wx. BoxSizer instance that can be used to create the group of items.", "response": "def makeGroup(self, parent, thissizer, group, *args):\r\n        '''\r\n        Messily builds the (potentially) nested and grouped layout\r\n\r\n        Note! Mutates `self.reifiedWidgets` in place with the widgets as they're\r\n        instantiated! I cannot figure out how to split out the creation of the\r\n        widgets from their styling without WxPython violently exploding\r\n\r\n        TODO: sort out the WX quirks and clean this up.\r\n        '''\r\n\r\n        # determine the type of border , if any, the main sizer will use\r\n        if getin(group, ['options', 'show_border'], False):\r\n            boxDetails = wx.StaticBox(parent, -1, group['name'] or '')\r\n            boxSizer = wx.StaticBoxSizer(boxDetails, wx.VERTICAL)\r\n        else:\r\n            boxSizer = wx.BoxSizer(wx.VERTICAL)\r\n            boxSizer.AddSpacer(10)\r\n            if group['name']:\r\n                boxSizer.Add(wx_util.h1(parent, group['name'] or ''), 0, wx.TOP | wx.BOTTOM | wx.LEFT, 8)\r\n\r\n        group_description = getin(group, ['description'])\r\n        if group_description:\r\n            description = wx.StaticText(parent, label=group_description)\r\n            boxSizer.Add(description, 0,  wx.EXPAND | wx.LEFT, 10)\r\n\r\n        # apply an underline when a grouping border is not specified\r\n        if not getin(group, ['options', 'show_border'], False) and group['name']:\r\n            boxSizer.Add(wx_util.horizontal_rule(parent), 0, wx.EXPAND | wx.LEFT, 10)\r\n\r\n        ui_groups = self.chunkWidgets(group)\r\n\r\n        for uigroup in ui_groups:\r\n            sizer = wx.BoxSizer(wx.HORIZONTAL)\r\n            for item in uigroup:\r\n                widget = self.reifyWidget(parent, item)\r\n                # !Mutate the reifiedWidgets instance variable in place\r\n                self.reifiedWidgets.append(widget)\r\n                sizer.Add(widget, 1, wx.ALL, 5)\r\n            boxSizer.Add(sizer, 0, wx.ALL | wx.EXPAND, 5)\r\n\r\n        # apply the same layout rules recursively for subgroups\r\n        hs = wx.BoxSizer(wx.HORIZONTAL)\r\n        for e, subgroup in enumerate(group['groups']):\r\n            self.makeGroup(parent, hs, subgroup, 1, wx.ALL | wx.EXPAND, 5)\r\n            if e % getin(group, ['options', 'columns'], 2) \\\r\n                    or e == len(group['groups']):\r\n                boxSizer.Add(hs, *args)\r\n                hs = wx.BoxSizer(wx.HORIZONTAL)\r\n\r\n        thissizer.Add(boxSizer, *args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nchunk the widgets up into groups based on their sizing hints", "response": "def chunkWidgets(self, group):\r\n        ''' chunk the widgets up into groups based on their sizing hints '''\r\n        ui_groups = []\r\n        subgroup = []\r\n        for index, item in enumerate(group['items']):\r\n            if getin(item, ['options', 'full_width'], False):\r\n                ui_groups.append(subgroup)\r\n                ui_groups.append([item])\r\n                subgroup = []\r\n            else:\r\n                subgroup.append(item)\r\n            if len(subgroup) == getin(group, ['options', 'columns'], 2) \\\r\n                    or item == group['items'][-1]:\r\n                ui_groups.append(subgroup)\r\n                subgroup = []\r\n        return ui_groups"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a JSON description of a widget into a WxObject", "response": "def reifyWidget(self, parent, item):\r\n        ''' Convert a JSON description of a widget into a WxObject '''\r\n        from gooey.gui.components import widgets\r\n        widgetClass = getattr(widgets, item['type'])\r\n        return widgetClass(parent, item)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns true if the given action is required.", "response": "def is_required(action):\r\n    '''\r\n    _actions possessing the `required` flag and not implicitly optional\r\n    through `nargs` being '*' or '?'\r\n    '''\r\n    return not isinstance(action, _SubParsersAction) and (\r\n    action.required == True and action.nargs not in ['*', '?'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if the given action is standard.", "response": "def is_standard(action):\r\n    \"\"\" actions which are general \"store\" instructions.\r\n    e.g. anything which has an argument style like:\r\n       $ script.py -f myfilename.txt\r\n    \"\"\"\r\n    boolean_actions = (\r\n        _StoreConstAction, _StoreFalseAction,\r\n        _StoreTrueAction\r\n    )\r\n    return (not action.choices\r\n            and not isinstance(action, _CountAction)\r\n            and not isinstance(action, _HelpAction)\r\n            and type(action) not in boolean_actions)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if the given action is a flag.", "response": "def is_flag(action):\r\n    \"\"\" _actions which are either storeconst, store_bool, etc.. \"\"\"\r\n    action_types = [_StoreTrueAction, _StoreFalseAction, _StoreConstAction]\r\n    return any(list(map(lambda Action: isinstance(action, Action), action_types)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the string that will be used to drop down the next value", "response": "def counter(metatdata, value):\r\n    '''\r\n    Returns\r\n      str(option_string * DropDown Value)\r\n      e.g.\r\n      -vvvvv\r\n    '''\r\n    if not str(value).isdigit():\r\n        return None\r\n    arg = str(metatdata['commands'][0]).replace('-', '')\r\n    repeated_args = arg * int(value)\r\n    return '-' + repeated_args"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load(language_dir, filename, encoding):\r\n  ''' Open and return the supplied json file '''\r\n  global _DICTIONARY\r\n  try:\r\n    json_file = filename + '.json'\r\n    with io.open(os.path.join(language_dir, json_file), 'r', encoding=encoding) as f:\r\n      _DICTIONARY = json.load(f)\r\n  except IOError:\r\n    raise IOError('{0} Language file not found at location {1}. '\r\n                  'Make sure that your translation file is in the '\r\n                  'listed language directory'.format(filename.title(), language_dir))", "response": "Open and return the supplied json file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the value in a nested dict", "response": "def getin(m, path, default=None):\r\n    \"\"\"returns the value in a nested dict\"\"\"\r\n    keynotfound = ':com.gooey-project/not-found'\r\n    result = reduce(lambda acc, val: acc.get(val, {keynotfound: None}), path, m)\r\n    # falsey values like 0 would incorrectly trigger the default to be returned\r\n    # so the keynotfound val is used to signify a miss vs just a falesy val\r\n    if isinstance(result, dict) and keynotfound in result:\r\n        return default\r\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncopies - on - write associates a value in a dict", "response": "def assoc(m, key, val):\r\n    \"\"\"Copy-on-write associates a value in a dict\"\"\"\r\n    cpy = deepcopy(m)\r\n    cpy[key] = val\r\n    return cpy"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncopy - on - write associates a value in a nested dict", "response": "def associn(m, path, value):\r\n    \"\"\" Copy-on-write associates a value in a nested dict \"\"\"\r\n    def assoc_recursively(m, path, value):\r\n        if not path:\r\n            return value\r\n        p = path[0]\r\n        return assoc(m, p, assoc_recursively(m.get(p,{}), path[1:], value))\r\n    return assoc_recursively(m, path, value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmerging all maps left to right", "response": "def merge(*maps):\r\n    \"\"\"Merge all maps left to right\"\"\"\r\n    copies = map(deepcopy, maps)\r\n    return reduce(lambda acc, val: acc.update(val) or acc, copies)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef findfirst(f, coll):\r\n    result = list(dropwhile(f, coll))\r\n    return result[0] if result else None", "response": "Return first occurrence matching f otherwise None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate the input_tensor that minimizes the weighted losses.", "response": "def visualize_activation_with_losses(input_tensor, losses, wrt_tensor=None,\n                                     seed_input=None, input_range=(0, 255),\n                                     **optimizer_params):\n    \"\"\"Generates the `input_tensor` that minimizes the weighted `losses`. This function is intended for advanced\n    use cases where a custom loss is desired.\n\n    Args:\n        input_tensor: An input tensor of shape: `(samples, channels, image_dims...)` if `image_data_format=\n            channels_first` or `(samples, image_dims..., channels)` if `image_data_format=channels_last`.\n        wrt_tensor: Short for, with respect to. The gradients of losses are computed with respect to this tensor.\n            When None, this is assumed to be the same as `input_tensor` (Default value: None)\n        losses: List of ([Loss](vis.losses#Loss), weight) tuples.\n        seed_input: Seeds the optimization with a starting image. Initialized with a random value when set to None.\n            (Default value = None)\n        input_range: Specifies the input range as a `(min, max)` tuple. This is used to rescale the\n            final optimized input to the given range. (Default value=(0, 255))\n        optimizer_params: The **kwargs for optimizer [params](vis.optimizer#optimizerminimize). Will default to\n            reasonable values when required keys are not found.\n\n    Returns:\n        The model input that minimizes the weighted `losses`.\n    \"\"\"\n    # Default optimizer kwargs.\n    optimizer_params = utils.add_defaults_to_kwargs({\n        'seed_input': seed_input,\n        'max_iter': 200,\n        'verbose': False\n    }, **optimizer_params)\n\n    opt = Optimizer(input_tensor, losses, input_range, wrt_tensor=wrt_tensor)\n    img = opt.minimize(**optimizer_params)[0]\n\n    # If range has integer numbers, cast to 'uint8'\n    if isinstance(input_range[0], int) and isinstance(input_range[1], int):\n        img = np.clip(img, input_range[0], input_range[1]).astype('uint8')\n\n    if K.image_data_format() == 'channels_first':\n        img = np.moveaxis(img, 0, -1)\n    return img"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef visualize_activation(model, layer_idx, filter_indices=None, wrt_tensor=None,\n                         seed_input=None, input_range=(0, 255),\n                         backprop_modifier=None, grad_modifier=None,\n                         act_max_weight=1, lp_norm_weight=10, tv_weight=10,\n                         **optimizer_params):\n    \"\"\"Generates the model input that maximizes the output of all `filter_indices` in the given `layer_idx`.\n\n    Args:\n        model: The `keras.models.Model` instance. The model input shape must be: `(samples, channels, image_dims...)`\n            if `image_data_format=channels_first` or `(samples, image_dims..., channels)` if\n            `image_data_format=channels_last`.\n        layer_idx: The layer index within `model.layers` whose filters needs to be visualized.\n        filter_indices: filter indices within the layer to be maximized.\n            If None, all filters are visualized. (Default value = None)\n            For `keras.layers.Dense` layer, `filter_idx` is interpreted as the output index.\n            If you are visualizing final `keras.layers.Dense` layer, consider switching 'softmax' activation for\n            'linear' using [utils.apply_modifications](vis.utils.utils#apply_modifications) for better results.\n        wrt_tensor: Short for, with respect to. The gradients of losses are computed with respect to this tensor.\n            When None, this is assumed to be the same as `input_tensor` (Default value: None)\n        seed_input: Seeds the optimization with a starting input. Initialized with a random value when set to None.\n            (Default value = None)\n        input_range: Specifies the input range as a `(min, max)` tuple. This is used to rescale the\n            final optimized input to the given range. (Default value=(0, 255))\n        backprop_modifier: backprop modifier to use. See [backprop_modifiers](vis.backprop_modifiers.md). If you don't\n            specify anything, no backprop modification is applied. (Default value = None)\n        grad_modifier: gradient modifier to use. See [grad_modifiers](vis.grad_modifiers.md). If you don't\n            specify anything, gradients are unchanged (Default value = None)\n        act_max_weight: The weight param for `ActivationMaximization` loss. Not used if 0 or None. (Default value = 1)\n        lp_norm_weight: The weight param for `LPNorm` regularization loss. Not used if 0 or None. (Default value = 10)\n        tv_weight: The weight param for `TotalVariation` regularization loss. Not used if 0 or None. (Default value = 10)\n        optimizer_params: The **kwargs for optimizer [params](vis.optimizer#optimizerminimize). Will default to\n            reasonable values when required keys are not found.\n\n    Example:\n        If you wanted to visualize the input image that would maximize the output index 22, say on\n        final `keras.layers.Dense` layer, then, `filter_indices = [22]`, `layer_idx = dense_layer_idx`.\n\n        If `filter_indices = [22, 23]`, then it should generate an input image that shows features of both classes.\n\n    Returns:\n        The model input that maximizes the output of `filter_indices` in the given `layer_idx`.\n    \"\"\"\n    if backprop_modifier is not None:\n        modifier_fn = get(backprop_modifier)\n        model = modifier_fn(model)\n\n    losses = [\n        (ActivationMaximization(model.layers[layer_idx], filter_indices), act_max_weight),\n        (LPNorm(model.input), lp_norm_weight),\n        (TotalVariation(model.input), tv_weight)\n    ]\n\n    # Add grad_filter to optimizer_params.\n    optimizer_params = utils.add_defaults_to_kwargs({\n        'grad_modifier': grad_modifier\n    }, **optimizer_params)\n\n    return visualize_activation_with_losses(model.input, losses, wrt_tensor,\n                                            seed_input, input_range, **optimizer_params)", "response": "Generates the model input that maximizes the output of all filter_indices in the given layer_idx."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nuse RMSProp to compute step from gradients.", "response": "def _rmsprop(self, grads, cache=None, decay_rate=0.95):\n        \"\"\"Uses RMSProp to compute step from gradients.\n\n        Args:\n            grads: numpy array of gradients.\n            cache: numpy array of same shape as `grads` as RMSProp cache\n            decay_rate: How fast to decay cache\n\n        Returns:\n            A tuple of\n                step: numpy array of the same shape as `grads` giving the step.\n                    Note that this does not yet take the learning rate into account.\n                cache: Updated RMSProp cache.\n        \"\"\"\n        if cache is None:\n            cache = np.zeros_like(grads)\n        cache = decay_rate * cache + (1 - decay_rate) * grads ** 2\n        step = -grads / np.sqrt(cache + K.epsilon())\n        return step, cache"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a random seed_input if None. Otherwise returns a random seed_input.", "response": "def _get_seed_input(self, seed_input):\n        \"\"\"Creates a random `seed_input` if None. Otherwise:\n            - Ensures batch_size dim on provided `seed_input`.\n            - Shuffle axis according to expected `image_data_format`.\n        \"\"\"\n        desired_shape = (1, ) + K.int_shape(self.input_tensor)[1:]\n        if seed_input is None:\n            return utils.random_array(desired_shape, mean=np.mean(self.input_range),\n                                      std=0.05 * (self.input_range[1] - self.input_range[0]))\n\n        # Add batch dim if needed.\n        if len(seed_input.shape) != len(desired_shape):\n            seed_input = np.expand_dims(seed_input, 0)\n\n        # Only possible if channel idx is out of place.\n        if seed_input.shape[-1] != desired_shape[-1] and \\\n           seed_input.shape[1] != desired_shape[1]:\n            seed_input = np.moveaxis(seed_input, -1, 1)\n        return seed_input.astype(K.floatx())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform gradient descent on the input image with respect to defined losses.", "response": "def minimize(self, seed_input=None, max_iter=200,\n                 input_modifiers=None, grad_modifier=None,\n                 callbacks=None, verbose=True):\n        \"\"\"Performs gradient descent on the input image with respect to defined losses.\n\n        Args:\n            seed_input: An N-dim numpy array of shape: `(samples, channels, image_dims...)` if `image_data_format=\n                channels_first` or `(samples, image_dims..., channels)` if `image_data_format=channels_last`.\n                Seeded with random noise if set to None. (Default value = None)\n            max_iter: The maximum number of gradient descent iterations. (Default value = 200)\n            input_modifiers: A list of [InputModifier](vis.input_modifiers#inputmodifier) instances specifying\n                how to make `pre` and `post` changes to the optimized input during the optimization process.\n                `pre` is applied in list order while `post` is applied in reverse order. For example,\n                `input_modifiers = [f, g]` means that `pre_input = g(f(inp))` and `post_input = f(g(inp))`\n            grad_modifier: gradient modifier to use. See [grad_modifiers](vis.grad_modifiers.md). If you don't\n                specify anything, gradients are unchanged. (Default value = None)\n            callbacks: A list of [OptimizerCallback](vis.callbacks#optimizercallback) instances to trigger.\n            verbose: Logs individual losses at the end of every gradient descent iteration.\n                Very useful to estimate loss weight factor(s). (Default value = True)\n\n        Returns:\n            The tuple of `(optimized input, grads with respect to wrt, wrt_value)` after gradient descent iterations.\n        \"\"\"\n        seed_input = self._get_seed_input(seed_input)\n        input_modifiers = input_modifiers or []\n        grad_modifier = _identity if grad_modifier is None else get(grad_modifier)\n\n        callbacks = callbacks or []\n        if verbose:\n            callbacks.append(_PRINT_CALLBACK)\n\n        cache = None\n        best_loss = float('inf')\n        best_input = None\n\n        grads = None\n        wrt_value = None\n\n        for i in range(max_iter):\n            # Apply modifiers `pre` step\n            for modifier in input_modifiers:\n                seed_input = modifier.pre(seed_input)\n\n            # 0 learning phase for 'test'\n            computed_values = self.compute_fn([seed_input, 0])\n            losses = computed_values[:len(self.loss_names)]\n            named_losses = list(zip(self.loss_names, losses))\n            overall_loss, grads, wrt_value = computed_values[len(self.loss_names):]\n\n            # TODO: theano grads shape is inconsistent for some reason. Patch for now and investigate later.\n            if grads.shape != wrt_value.shape:\n                grads = np.reshape(grads, wrt_value.shape)\n\n            # Apply grad modifier.\n            grads = grad_modifier(grads)\n\n            # Trigger callbacks\n            for c in callbacks:\n                c.callback(i, named_losses, overall_loss, grads, wrt_value)\n\n            # Gradient descent update.\n            # It only makes sense to do this if wrt_tensor is input_tensor. Otherwise shapes wont match for the update.\n            if self.wrt_tensor_is_input_tensor:\n                step, cache = self._rmsprop(grads, cache)\n                seed_input += step\n\n            # Apply modifiers `post` step\n            for modifier in reversed(input_modifiers):\n                seed_input = modifier.post(seed_input)\n\n            if overall_loss < best_loss:\n                best_loss = overall_loss.copy()\n                best_input = seed_input.copy()\n\n        # Trigger on_end\n        for c in callbacks:\n            c.on_end()\n\n        return utils.deprocess_input(best_input[0], self.input_range), grads, wrt_value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef order_by_line_nos(objs, line_nos):\n    ordering = sorted(range(len(line_nos)), key=line_nos.__getitem__)\n    return [objs[i] for i in ordering]", "response": "Orders the set of objects by line_nos"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_md_file(string, filename, out_path=\".\"):\n    md_file = \"%s.md\" % filename\n    with open(os.path.join(out_path, md_file), \"w\") as f:\n        f.write(string)\n    print(\"wrote {}.\".format(md_file))", "response": "Write a string with line breaks to a. md file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a src path string with line info for use as markdown link.", "response": "def get_src_path(self, obj, append_base=True):\n        \"\"\"Creates a src path string with line info for use as markdown link.\n        \"\"\"\n        path = getsourcefile(obj)\n        if self.src_root not in path:\n            # this can happen with e.g.\n            # inlinefunc-wrapped functions\n            if hasattr(obj, \"__module__\"):\n                path = \"%s.%s\" % (obj.__module__, obj.__name__)\n            else:\n                path = obj.__name__\n            path = path.replace(\".\", \"/\")\n        pre, post = path.rsplit(self.src_root + \"/\", 1)\n\n        lineno = self.get_line_no(obj)\n        lineno = \"\" if lineno is None else \"#L{}\".format(lineno)\n\n        path = self.src_root + \"/\" + post + lineno\n        if append_base:\n            path = os.path.join(self.github_link, path)\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef doc2md(self, func):\n        doc = getdoc(func) or \"\"\n        blockindent = 0\n        argindent = 1\n        out = []\n\n        for line in doc.split(\"\\n\"):\n            indent = len(line) - len(line.lstrip())\n            line = line.lstrip()\n            if _RE_BLOCKSTART.match(line):\n                # start of a new block\n                blockindent = indent\n                out.append(\"\\n*{}*\\n\".format(line))\n            elif indent > blockindent:\n                if _RE_ARGSTART.match(line):\n                    # start of new argument\n                    out.append(\"\\n\" + \" \" * blockindent + \" - \" + _RE_ARGSTART.sub(r\"**\\1** (\\2): \\3\", line))\n                    argindent = indent\n                elif _RE_EXCSTART.match(line):\n                    # start of an exception-type block\n                    out.append(\"\\n\" + \" \" * blockindent + \" - \" + _RE_EXCSTART.sub(r\"**\\1**: \\2\", line))\n                    argindent = indent\n                elif indent > argindent:\n                    out.append(\"\\n\" + \" \" * (blockindent + 2) + line)\n                else:\n                    out.append(\"\\n\" + line)\n            else:\n                out.append(\"\\n\" + line)\n\n        return \"\".join(out)", "response": "Parse the docstring of a function and return a markdown string."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntakes a function or method and returns a string that contains the contents of the function and its docstring.", "response": "def func2md(self, func, clsname=None, names=None, depth=3):\n        \"\"\"Takes a function (or method) and documents it.\n\n        Args:\n            clsname (str, optional): class name to prepend to funcname.\n            depth (int, optional): number of ### to append to function name\n\n        \"\"\"\n        section = \"#\" * depth\n        if names is None:\n            names = [func.__name__]\n\n        funcname = \", \".join(names)\n        escfuncname = \", \".join([\"`%s`\" % funcname if funcname.startswith(\"_\") else funcname for funcname in names])\n        header = \"%s%s\" % (\"%s.\" % clsname if clsname else \"\", escfuncname)\n\n        path = self.get_src_path(func)\n        doc = self.doc2md(func)\n\n        args, kwargs = [], []\n        spec = getargspec(func)\n        vargsname, kwargsname = spec.varargs, spec.keywords\n        vargs = list(make_iter(spec.args)) if spec.args else []\n        defaults = list(make_iter(spec.defaults)) if spec.defaults else []\n\n        while vargs:\n            if vargs and vargs[0] == \"self\":\n                args.append(vargs.pop(0))\n            elif len(vargs) > len(defaults):\n                args.append(vargs.pop(0))\n            else:\n                default = defaults.pop(0)\n                if isinstance(default, str):\n                    default = \"\\\"%s\\\"\" % default\n                else:\n                    default = \"%s\" % str(default)\n\n                kwargs.append((vargs.pop(0), default))\n\n        if args:\n            args = \", \".join(\"%s\" % arg for arg in args)\n        if kwargs:\n            kwargs = \", \".join(\"%s=%s\" % kwarg for kwarg in kwargs)\n            if args:\n                kwargs = \", \" + kwargs\n        if vargsname:\n            vargsname = \"*%s\" % vargsname\n            if args or kwargs:\n                vargsname = \", \" + vargsname\n        if kwargsname:\n            kwargsname = \"**%s\" % kwargsname\n            if args or kwargs or vargsname:\n                kwargsname = \", \" + kwargsname\n\n        _FUNCDEF = \"{funcname}({args}{kwargs}{vargs}{vkwargs})\"\n        funcdef = _FUNCDEF.format(funcname=funcname,\n                                  args=args or \"\",\n                                  kwargs=kwargs or \"\",\n                                  vargs=vargsname or \"\",\n                                  vkwargs=kwargsname or \"\")\n\n        # split the function definition if it is too long\n        lmax = 90\n        if len(funcdef) > lmax:\n            # wrap in the args list\n            split = funcdef.split(\"(\", 1)\n            # we gradually build the string again\n            rest = split[1]\n            args = rest.split(\", \")\n\n            funcname = \"(\".join(split[:1]) + \"(\"\n            lline = len(funcname)\n            parts = []\n            for arg in args:\n                larg = len(arg)\n                if larg > lmax - 5:\n                    # not much to do if arg is so long\n                    parts.append(arg)\n                elif lline + larg > lmax:\n                    # the next arg is too long, break the line\n                    parts.append(\"\\\\\\n    \" + arg)\n                    lline = 0\n                else:\n                    parts.append(arg)\n                lline += len(parts[-1])\n            funcdef = funcname + \", \".join(parts)\n\n        # build the signature\n        string = FUNC_TEMPLATE.format(section=section,\n                                      header=header,\n                                      funcdef=funcdef,\n                                      path=path,\n                                      doc=doc if doc else \"*No documentation found.*\")\n        return string"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntakes a class and creates markdown text to document its methods and variables.", "response": "def class2md(self, cls, depth=2):\n        \"\"\"Takes a class and creates markdown text to document its methods and variables.\n        \"\"\"\n\n        section = \"#\" * depth\n        subsection = \"#\" * (depth + 2)\n        clsname = cls.__name__\n        modname = cls.__module__\n        header = clsname\n        path = self.get_src_path(cls)\n        doc = self.doc2md(cls)\n\n        try:\n            init = self.func2md(cls.__init__, clsname=clsname)\n        except (ValueError, TypeError):\n            # this happens if __init__ is outside the repo\n            init = \"\"\n\n        variables = []\n        for name, obj in getmembers(cls, lambda a: not (inspect.isroutine(a) or inspect.ismethod(a))):\n            if not name.startswith(\"_\") and type(obj) == property:\n                comments = self.doc2md(obj) or inspect.getcomments(obj)\n                comments = \"\\n %s\" % comments if comments else \"\"\n                variables.append(\"\\n%s %s.%s%s\\n\" % (subsection, clsname, name, comments))\n\n        handlers = []\n        for name, obj in getmembers(cls, inspect.ismethoddescriptor):\n            if not name.startswith(\"_\") and hasattr(obj, \"__module__\") and obj.__module__ == modname:\n                handlers.append(\"\\n%s %s.%s\\n *Handler*\" % (subsection, clsname, name))\n\n        methods = []\n        for name, obj in getmembers(cls, inspect.ismethod):\n            if not name.startswith(\"_\") and hasattr(obj,\n                                                    \"__module__\") and obj.__module__ == modname and name not in handlers:\n                methods.append(self.func2md(obj, clsname=clsname, depth=depth + 1))\n\n        string = CLASS_TEMPLATE.format(section=section,\n                                       header=header,\n                                       path=path,\n                                       doc=doc if doc else \"\",\n                                       init=init,\n                                       variables=\"\".join(variables),\n                                       handlers=\"\".join(handlers),\n                                       methods=\"\".join(methods))\n        return string"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntake an imported module object and creates a Markdown string containing functions and classes and classes.", "response": "def module2md(self, module):\n        \"\"\"Takes an imported module object and create a Markdown string containing functions and classes.\n        \"\"\"\n        modname = module.__name__\n        path = self.get_src_path(module, append_base=False)\n        path = \"[{}]({})\".format(path, os.path.join(self.github_link, path))\n        found = set()\n\n        classes = []\n        line_nos = []\n        for name, obj in getmembers(module, inspect.isclass):\n            # handle classes\n            found.add(name)\n            if not name.startswith(\"_\") and hasattr(obj, \"__module__\") and obj.__module__ == modname:\n                classes.append(self.class2md(obj))\n                line_nos.append(self.get_line_no(obj) or 0)\n        classes = order_by_line_nos(classes, line_nos)\n\n        # Since functions can have multiple aliases.\n        func2names = defaultdict(list)\n        for name, obj in getmembers(module, inspect.isfunction):\n            func2names[obj].append(name)\n\n        functions = []\n        line_nos = []\n        for obj in func2names:\n            names = func2names[obj]\n            found.update(names)\n\n            # Include if within module or included modules within __init__.py and exclude from global variables\n            is_module_within_init = '__init__.py' in path and obj.__module__.startswith(modname)\n            if is_module_within_init:\n                found.add(obj.__module__.replace(modname + '.', ''))\n\n            if hasattr(obj, \"__module__\") and (obj.__module__ == modname or is_module_within_init):\n                names = list(filter(lambda name: not name.startswith(\"_\"), names))\n                if len(names) > 0:\n                    functions.append(self.func2md(obj, names=names))\n                    line_nos.append(self.get_line_no(obj) or 0)\n        functions = order_by_line_nos(functions, line_nos)\n\n        variables = []\n        line_nos = []\n        for name, obj in module.__dict__.items():\n            if not name.startswith(\"_\") and name not in found:\n                if hasattr(obj, \"__module__\") and obj.__module__ != modname:\n                    continue\n                if hasattr(obj, \"__name__\") and not obj.__name__.startswith(modname):\n                    continue\n\n                comments = inspect.getcomments(obj)\n                comments = \": %s\" % comments if comments else \"\"\n                variables.append(\"- **%s**%s\" % (name, comments))\n                line_nos.append(self.get_line_no(obj) or 0)\n\n        variables = order_by_line_nos(variables, line_nos)\n        if variables:\n            new_list = [\"**Global Variables**\", \"---------------\"]\n            new_list.extend(variables)\n            variables = new_list\n\n        string = MODULE_TEMPLATE.format(path=path,\n                                        global_vars=\"\\n\".join(variables) if variables else \"\",\n                                        functions=\"\\n\".join(functions) if functions else \"\",\n                                        classes=\"\".join(classes) if classes else \"\")\n        return string"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _find_penultimate_layer(model, layer_idx, penultimate_layer_idx):\n    if penultimate_layer_idx is None:\n        for idx, layer in utils.reverse_enumerate(model.layers[:layer_idx - 1]):\n            if isinstance(layer, Wrapper):\n                layer = layer.layer\n            if isinstance(layer, (_Conv, _Pooling1D, _Pooling2D, _Pooling3D)):\n                penultimate_layer_idx = idx\n                break\n\n    if penultimate_layer_idx is None:\n        raise ValueError('Unable to determine penultimate `Conv` or `Pooling` '\n                         'layer for layer_idx: {}'.format(layer_idx))\n\n    # Handle negative indexing otherwise the next check can fail.\n    if layer_idx < 0:\n        layer_idx = len(model.layers) + layer_idx\n    if penultimate_layer_idx > layer_idx:\n        raise ValueError('`penultimate_layer_idx` needs to be before `layer_idx`')\n\n    return model.layers[penultimate_layer_idx]", "response": "Searches for the nearest penultimate CNN or Pooling layer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating an attention heatmap over the seed_input with respect to weighted losses.", "response": "def visualize_saliency_with_losses(input_tensor, losses, seed_input, wrt_tensor=None, grad_modifier='absolute', keepdims=False):\n    \"\"\"Generates an attention heatmap over the `seed_input` by using positive gradients of `input_tensor`\n    with respect to weighted `losses`.\n\n    This function is intended for advanced use cases where a custom loss is desired. For common use cases,\n    refer to `visualize_class_saliency` or `visualize_regression_saliency`.\n\n    For a full description of saliency, see the paper:\n    [Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps]\n    (https://arxiv.org/pdf/1312.6034v2.pdf)\n\n    Args:\n        input_tensor: An input tensor of shape: `(samples, channels, image_dims...)` if `image_data_format=\n            channels_first` or `(samples, image_dims..., channels)` if `image_data_format=channels_last`.\n        losses: List of ([Loss](vis.losses#Loss), weight) tuples.\n        seed_input: The model input for which activation map needs to be visualized.\n        wrt_tensor: Short for, with respect to. The gradients of losses are computed with respect to this tensor.\n            When None, this is assumed to be the same as `input_tensor` (Default value: None)\n        grad_modifier: gradient modifier to use. See [grad_modifiers](vis.grad_modifiers.md). By default `absolute`\n            value of gradients are used. To visualize positive or negative gradients, use `relu` and `negate`\n            respectively. (Default value = 'absolute')\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If keepdims is False, the channels axis is deleted.\n            If keepdims is True, the grad with same shape as input_tensor is returned. (Default value: False)\n\n    Returns:\n        The normalized gradients of `seed_input` with respect to weighted `losses`.\n    \"\"\"\n    opt = Optimizer(input_tensor, losses, wrt_tensor=wrt_tensor, norm_grads=False)\n    grads = opt.minimize(seed_input=seed_input, max_iter=1, grad_modifier=grad_modifier, verbose=False)[1]\n\n    if not keepdims:\n        channel_idx = 1 if K.image_data_format() == 'channels_first' else -1\n        grads = np.max(grads, axis=channel_idx)\n    return utils.normalize(grads)[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a visualised Saliency layer over the seed_input for maximizing filter_indices.", "response": "def visualize_saliency(model, layer_idx, filter_indices, seed_input, wrt_tensor=None,\n                       backprop_modifier=None, grad_modifier='absolute', keepdims=False):\n    \"\"\"Generates an attention heatmap over the `seed_input` for maximizing `filter_indices`\n    output in the given `layer_idx`.\n\n    Args:\n        model: The `keras.models.Model` instance. The model input shape must be: `(samples, channels, image_dims...)`\n            if `image_data_format=channels_first` or `(samples, image_dims..., channels)` if\n            `image_data_format=channels_last`.\n        layer_idx: The layer index within `model.layers` whose filters needs to be visualized.\n        filter_indices: filter indices within the layer to be maximized.\n            If None, all filters are visualized. (Default value = None)\n            For `keras.layers.Dense` layer, `filter_idx` is interpreted as the output index.\n            If you are visualizing final `keras.layers.Dense` layer, consider switching 'softmax' activation for\n            'linear' using [utils.apply_modifications](vis.utils.utils#apply_modifications) for better results.\n        seed_input: The model input for which activation map needs to be visualized.\n        wrt_tensor: Short for, with respect to. The gradients of losses are computed with respect to this tensor.\n            When None, this is assumed to be the same as `input_tensor` (Default value: None)\n        backprop_modifier: backprop modifier to use. See [backprop_modifiers](vis.backprop_modifiers.md). If you don't\n            specify anything, no backprop modification is applied. (Default value = None)\n        grad_modifier: gradient modifier to use. See [grad_modifiers](vis.grad_modifiers.md). By default `absolute`\n            value of gradients are used. To visualize positive or negative gradients, use `relu` and `negate`\n            respectively. (Default value = 'absolute')\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If keepdims is False, the channels axis is deleted.\n            If keepdims is True, the grad with same shape as input_tensor is returned. (Default value: False)\n\n    Example:\n        If you wanted to visualize attention over 'bird' category, say output index 22 on the\n        final `keras.layers.Dense` layer, then, `filter_indices = [22]`, `layer = dense_layer`.\n\n        One could also set filter indices to more than one value. For example, `filter_indices = [22, 23]` should\n        (hopefully) show attention map that corresponds to both 22, 23 output categories.\n\n    Returns:\n        The heatmap image indicating the `seed_input` regions whose change would most contribute towards\n        maximizing the output of `filter_indices`.\n    \"\"\"\n    if backprop_modifier is not None:\n        modifier_fn = get(backprop_modifier)\n        model = modifier_fn(model)\n\n    # `ActivationMaximization` loss reduces as outputs get large, hence negative gradients indicate the direction\n    # for increasing activations. Multiply with -1 so that positive gradients indicate increase instead.\n    losses = [\n        (ActivationMaximization(model.layers[layer_idx], filter_indices), -1)\n    ]\n    return visualize_saliency_with_losses(model.input, losses, seed_input, wrt_tensor, grad_modifier, keepdims)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef visualize_cam_with_losses(input_tensor, losses, seed_input, penultimate_layer, grad_modifier=None):\n    penultimate_output = penultimate_layer.output\n    opt = Optimizer(input_tensor, losses, wrt_tensor=penultimate_output, norm_grads=False)\n    _, grads, penultimate_output_value = opt.minimize(seed_input, max_iter=1, grad_modifier=grad_modifier, verbose=False)\n\n    # For numerical stability. Very small grad values along with small penultimate_output_value can cause\n    # w * penultimate_output_value to zero out, even for reasonable fp precision of float32.\n    grads = grads / (np.max(grads) + K.epsilon())\n\n    # Average pooling across all feature maps.\n    # This captures the importance of feature map (channel) idx to the output.\n    channel_idx = 1 if K.image_data_format() == 'channels_first' else -1\n    other_axis = np.delete(np.arange(len(grads.shape)), channel_idx)\n    weights = np.mean(grads, axis=tuple(other_axis))\n\n    # Generate heatmap by computing weight * output over feature maps\n    output_dims = utils.get_img_shape(penultimate_output)[2:]\n    heatmap = np.zeros(shape=output_dims, dtype=K.floatx())\n    for i, w in enumerate(weights):\n        if channel_idx == -1:\n            heatmap += w * penultimate_output_value[0, ..., i]\n        else:\n            heatmap += w * penultimate_output_value[0, i, ...]\n\n    # ReLU thresholding to exclude pattern mismatch information (negative gradients).\n    heatmap = np.maximum(heatmap, 0)\n\n    # The penultimate feature map size is definitely smaller than input image.\n    input_dims = utils.get_img_shape(input_tensor)[2:]\n\n    # Figure out the zoom factor.\n    zoom_factor = [i / (j * 1.0) for i, j in iter(zip(input_dims, output_dims))]\n    heatmap = zoom(heatmap, zoom_factor)\n    return utils.normalize(heatmap)", "response": "Generates a gradient based class activation map by using positive gradients of input_tensor with respect to weighted losses."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a gradient based class activation map that maximizes the outputs of filter_indices in layer_idx.", "response": "def visualize_cam(model, layer_idx, filter_indices,\n                  seed_input, penultimate_layer_idx=None,\n                  backprop_modifier=None, grad_modifier=None):\n    \"\"\"Generates a gradient based class activation map (grad-CAM) that maximizes the outputs of\n    `filter_indices` in `layer_idx`.\n\n    Args:\n        model: The `keras.models.Model` instance. The model input shape must be: `(samples, channels, image_dims...)`\n            if `image_data_format=channels_first` or `(samples, image_dims..., channels)` if\n            `image_data_format=channels_last`.\n        layer_idx: The layer index within `model.layers` whose filters needs to be visualized.\n        filter_indices: filter indices within the layer to be maximized.\n            If None, all filters are visualized. (Default value = None)\n            For `keras.layers.Dense` layer, `filter_idx` is interpreted as the output index.\n            If you are visualizing final `keras.layers.Dense` layer, consider switching 'softmax' activation for\n            'linear' using [utils.apply_modifications](vis.utils.utils#apply_modifications) for better results.\n        seed_input: The input image for which activation map needs to be visualized.\n        penultimate_layer_idx: The pre-layer to `layer_idx` whose feature maps should be used to compute gradients\n            wrt filter output. If not provided, it is set to the nearest penultimate `Conv` or `Pooling` layer.\n        backprop_modifier: backprop modifier to use. See [backprop_modifiers](vis.backprop_modifiers.md). If you don't\n            specify anything, no backprop modification is applied. (Default value = None)\n        grad_modifier: gradient modifier to use. See [grad_modifiers](vis.grad_modifiers.md). If you don't\n            specify anything, gradients are unchanged (Default value = None)\n\n     Example:\n        If you wanted to visualize attention over 'bird' category, say output index 22 on the\n        final `keras.layers.Dense` layer, then, `filter_indices = [22]`, `layer = dense_layer`.\n\n        One could also set filter indices to more than one value. For example, `filter_indices = [22, 23]` should\n        (hopefully) show attention map that corresponds to both 22, 23 output categories.\n\n    Returns:\n        The heatmap image indicating the input regions whose change would most contribute towards\n        maximizing the output of `filter_indices`.\n    \"\"\"\n    if backprop_modifier is not None:\n        modifier_fn = get(backprop_modifier)\n        model = modifier_fn(model)\n\n    penultimate_layer = _find_penultimate_layer(model, layer_idx, penultimate_layer_idx)\n\n    # `ActivationMaximization` outputs negative gradient values for increase in activations. Multiply with -1\n    # so that positive gradients indicate increase instead.\n    losses = [\n        (ActivationMaximization(model.layers[layer_idx], filter_indices), -1)\n    ]\n    return visualize_cam_with_losses(model.input, losses, seed_input, penultimate_layer, grad_modifier)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef modify_model_backprop(model, backprop_modifier):\n    # The general strategy is as follows:\n    # - Save original model so that upstream callers don't see unexpected results with their models.\n    # - Call backend specific function that registers the custom op and loads the model under modified context manager.\n    # - Maintain cache to save this expensive process on subsequent calls.\n    # - Load model with custom context modifying backprop behavior.\n    #\n    # The reason for this round about way is because the graph needs to be rebuild when any of its layer builder\n    # functions are changed. This is very complicated to do in Keras and makes the implementation very tightly bound\n    # with keras internals. By saving and loading models, we dont have to worry about future compatibility.\n    #\n    # The only exception to this is the way advanced activations are handled which makes use of some keras internal\n    # knowledge and might break in the future.\n    # ADD on 22 Jul 2018:\n    #     In fact, it has broken. Currently, advanced activations are not supported.\n\n    # 0. Retrieve from cache if previously computed.\n    modified_model = _MODIFIED_MODEL_CACHE.get((model, backprop_modifier))\n    if modified_model is not None:\n        return modified_model\n\n    model_path = os.path.join(tempfile.gettempdir(), next(tempfile._get_candidate_names()) + '.h5')\n    try:\n        # 1. Save original model\n        model.save(model_path)\n\n        # 2. Register modifier and load modified model under custom context.\n        modifier_fn = _BACKPROP_MODIFIERS.get(backprop_modifier)\n        if modifier_fn is None:\n            raise ValueError(\"'{}' modifier is not supported\".format(backprop_modifier))\n        modifier_fn(backprop_modifier)\n\n        # 3. Create graph under custom context manager.\n        with tf.get_default_graph().gradient_override_map({'Relu': backprop_modifier}):\n            #  This should rebuild graph with modifications.\n            modified_model = load_model(model_path)\n\n            # Cache to improve subsequent call performance.\n            _MODIFIED_MODEL_CACHE[(model, backprop_modifier)] = modified_model\n            return modified_model\n    finally:\n        os.remove(model_path)", "response": "Creates a copy of model with modified activations."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef normalize(input_tensor, output_tensor):\n    image_dims = utils.get_img_shape(input_tensor)[1:]\n    return output_tensor / np.prod(image_dims)", "response": "Normalizes the output_tensor with respect to input_tensor dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_loss(self):\n        image_dims = K.ndim(self.img) - 2\n\n        # Constructing slice [1:] + [:-1] * (image_dims - 1) and [:-1] * (image_dims)\n        start_slice = [slice(1, None, None)] + [slice(None, -1, None) for _ in range(image_dims - 1)]\n        end_slice = [slice(None, -1, None) for _ in range(image_dims)]\n        samples_channels_slice = [slice(None, None, None), slice(None, None, None)]\n\n        # Compute pixel diffs by rolling slices to the right per image dim.\n        tv = None\n        for i in range(image_dims):\n            ss = tuple(samples_channels_slice + start_slice)\n            es = tuple(samples_channels_slice + end_slice)\n            diff_square = K.square(self.img[utils.slicer[ss]] - self.img[utils.slicer[es]])\n            tv = diff_square if tv is None else tv + diff_square\n\n            # Roll over to next image dim\n            start_slice = np.roll(start_slice, 1).tolist()\n            end_slice = np.roll(end_slice, 1).tolist()\n\n        tv = K.sum(K.pow(tv, self.beta / 2.))\n        return normalize(self.img, tv)", "response": "Implements the N - dim version of function\n       $$TV^{ beta } ( x ) = \\ sum_{whc } \\ right"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _find_font_file(query):\n    return list(filter(lambda path: query.lower() in os.path.basename(path).lower(), fontman.findSystemFonts()))", "response": "Utility to find font file.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_defaults_to_kwargs(defaults, **kwargs):\n    defaults = dict(defaults)\n    defaults.update(kwargs)\n    return defaults", "response": "Updates kwargs with dict of defaults"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\napplying modifications to the model layers to create a new Graph.", "response": "def apply_modifications(model, custom_objects=None):\n    \"\"\"Applies modifications to the model layers to create a new Graph. For example, simply changing\n    `model.layers[idx].activation = new activation` does not change the graph. The entire graph needs to be updated\n    with modified inbound and outbound tensors because of change in layer building function.\n\n    Args:\n        model: The `keras.models.Model` instance.\n\n    Returns:\n        The modified model with changes applied. Does not mutate the original `model`.\n    \"\"\"\n    # The strategy is to save the modified model and load it back. This is done because setting the activation\n    # in a Keras layer doesnt actually change the graph. We have to iterate the entire graph and change the\n    # layer inbound and outbound nodes with modified tensors. This is doubly complicated in Keras 2.x since\n    # multiple inbound and outbound nodes are allowed with the Graph API.\n    model_path = os.path.join(tempfile.gettempdir(), next(tempfile._get_candidate_names()) + '.h5')\n    try:\n        model.save(model_path)\n        return load_model(model_path, custom_objects=custom_objects)\n    finally:\n        os.remove(model_path)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef random_array(shape, mean=128., std=20.):\n    x = np.random.random(shape)\n    # normalize around mean=0, std=1\n    x = (x - np.mean(x)) / (np.std(x) + K.epsilon())\n    # and then around the desired mean/std\n    x = (x * std) + mean\n    return x", "response": "Creates a uniformly distributed random array of the given shape with the given mean and std."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlook up the layer index corresponding to layer_name from model.", "response": "def find_layer_idx(model, layer_name):\n    \"\"\"Looks up the layer index corresponding to `layer_name` from `model`.\n\n    Args:\n        model: The `keras.models.Model` instance.\n        layer_name: The name of the layer to lookup.\n\n    Returns:\n        The layer index if found. Raises an exception otherwise.\n    \"\"\"\n    layer_idx = None\n    for idx, layer in enumerate(model.layers):\n        if layer.name == layer_name:\n            layer_idx = idx\n            break\n\n    if layer_idx is None:\n        raise ValueError(\"No layer with name '{}' within the model\".format(layer_name))\n    return layer_idx"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_img_shape(img):\n    if isinstance(img, np.ndarray):\n        shape = img.shape\n    else:\n        shape = K.int_shape(img)\n\n    if K.image_data_format() == 'channels_last':\n        shape = list(shape)\n        shape.insert(1, shape[-1])\n        shape = tuple(shape[:-1])\n    return shape", "response": "Returns image shape in a backend agnostic manner."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_img(path, grayscale=False, target_size=None):\n    img = io.imread(path, grayscale)\n    if target_size:\n        img = transform.resize(img, target_size, preserve_range=True).astype('uint8')\n    return img", "response": "Utility function to load an image from disk."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lookup_imagenet_labels(indices):\n    global _CLASS_INDEX\n    if _CLASS_INDEX is None:\n        with open(os.path.join(os.path.dirname(__file__), '../../resources/imagenet_class_index.json')) as f:\n            _CLASS_INDEX = json.load(f)\n\n    indices = listify(indices)\n    return [_CLASS_INDEX[str(idx)][1] for idx in indices]", "response": "Utility function to return the image net label corresponding to the image category."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndraws text over the image. Requires PIL.", "response": "def draw_text(img, text, position=(10, 10), font='FreeSans.ttf', font_size=14, color=(0, 0, 0)):\n    \"\"\"Draws text over the image. Requires PIL.\n\n    Args:\n        img: The image to use.\n        text: The text string to overlay.\n        position: The text (x, y) position. (Default value = (10, 10))\n        font: The ttf or open type font to use. (Default value = 'FreeSans.ttf')\n        font_size: The text font size. (Default value = 12)\n        color: The (r, g, b) values for text color. (Default value = (0, 0, 0))\n\n    Returns: Image overlayed with text.\n    \"\"\"\n    _check_pil()\n\n    font_files = _find_font_file(font)\n    if len(font_files) == 0:\n        logger.warn(\"Failed to lookup font '{}', falling back to default\".format(font))\n        font = ImageFont.load_default()\n    else:\n        font = ImageFont.truetype(font_files[0], font_size)\n\n    # Don't mutate original image\n    img = Image.fromarray(img)\n    draw = ImageDraw.Draw(img)\n    draw.text(position, text, fill=color, font=font)\n    return np.asarray(img)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef normalize(array, min_value=0., max_value=1.):\n    arr_min = np.min(array)\n    arr_max = np.max(array)\n    normalized = (array - arr_min) / (arr_max - arr_min + K.epsilon())\n    return (max_value - min_value) * normalized + min_value", "response": "Normalizes the numpy array to range between min_value and max_value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_num_filters(layer):\n    # Handle layers with no channels.\n    if K.ndim(layer.output) == 2:\n        return K.int_shape(layer.output)[-1]\n\n    channel_idx = 1 if K.image_data_format() == 'channels_first' else -1\n    return K.int_shape(layer.output)[channel_idx]", "response": "Determines the number of filters within the given keras layer."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef overlay(array1, array2, alpha=0.5):\n    if alpha < 0. or alpha > 1.:\n        raise ValueError(\"`alpha` needs to be between [0, 1]\")\n    if array1.shape != array2.shape:\n        raise ValueError('`array1` and `array2` must have the same shapes')\n\n    return (array1 * alpha + array2 * (1. - alpha)).astype(array1.dtype)", "response": "Overlays array1 onto array2 using alpha blending."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninfers the coordinates and dimensions of a new DataArray.", "response": "def _infer_coords_and_dims(shape, coords, dims):\n    \"\"\"All the logic for creating a new DataArray\"\"\"\n\n    if (coords is not None and not utils.is_dict_like(coords) and\n            len(coords) != len(shape)):\n        raise ValueError('coords is not dict-like, but it has %s items, '\n                         'which does not match the %s dimensions of the '\n                         'data' % (len(coords), len(shape)))\n\n    if isinstance(dims, str):\n        dims = (dims,)\n\n    if dims is None:\n        dims = ['dim_%s' % n for n in range(len(shape))]\n        if coords is not None and len(coords) == len(shape):\n            # try to infer dimensions from coords\n            if utils.is_dict_like(coords):\n                # deprecated in GH993, removed in GH1539\n                raise ValueError('inferring DataArray dimensions from '\n                                 'dictionary like ``coords`` is no longer '\n                                 'supported. Use an explicit list of '\n                                 '``dims`` instead.')\n            for n, (dim, coord) in enumerate(zip(dims, coords)):\n                coord = as_variable(coord,\n                                    name=dims[n]).to_index_variable()\n                dims[n] = coord.name\n        dims = tuple(dims)\n    else:\n        for d in dims:\n            if not isinstance(d, str):\n                raise TypeError('dimension %s is not a string' % d)\n\n    new_coords = OrderedDict()\n\n    if utils.is_dict_like(coords):\n        for k, v in coords.items():\n            new_coords[k] = as_variable(v, name=k)\n    elif coords is not None:\n        for dim, coord in zip(dims, coords):\n            var = as_variable(coord, name=dim)\n            var.dims = (dim,)\n            new_coords[dim] = var\n\n    sizes = dict(zip(dims, shape))\n    for k, v in new_coords.items():\n        if any(d not in dims for d in v.dims):\n            raise ValueError('coordinate %s has dimensions %s, but these '\n                             'are not a subset of the DataArray '\n                             'dimensions %s' % (k, v.dims, dims))\n\n        for d, s in zip(v.dims, v.shape):\n            if s != sizes[d]:\n                raise ValueError('conflicting sizes for dimension %r: '\n                                 'length %s on the data but length %s on '\n                                 'coordinate %r' % (d, sizes[d], s, k))\n\n        if k in sizes and v.shape != (sizes[k],):\n            raise ValueError('coordinate %r is a DataArray dimension, but '\n                             'it has shape %r rather than expected shape %r '\n                             'matching the dimension size'\n                             % (k, v.shape, (sizes[k],)))\n\n    assert_unique_multiindex_level_names(new_coords)\n\n    return new_coords, dims"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_dataset(self, dim=None, name=None):\n        if dim is not None and dim not in self.dims:\n            warnings.warn('the order of the arguments on DataArray.to_dataset '\n                          'has changed; you now need to supply ``name`` as '\n                          'a keyword argument',\n                          FutureWarning, stacklevel=2)\n            name = dim\n            dim = None\n\n        if dim is not None:\n            if name is not None:\n                raise TypeError('cannot supply both dim and name arguments')\n            return self._to_dataset_split(dim)\n        else:\n            return self._to_dataset_whole(name)", "response": "Convert a DataArray into a Dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a mapping of all MultiIndex levels and their corresponding coordinate name.", "response": "def _level_coords(self):\n        \"\"\"Return a mapping of all MultiIndex levels and their corresponding\n        coordinate name.\n        \"\"\"\n        level_coords = OrderedDict()\n        for cname, var in self._coords.items():\n            if var.ndim == 1 and isinstance(var, IndexVariable):\n                level_names = var.level_names\n                if level_names is not None:\n                    dim, = var.dims\n                    level_coords.update({lname: dim for lname in level_names})\n        return level_coords"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef indexes(self):\n        if self._indexes is None:\n            self._indexes = default_indexes(self._coords, self.dims)\n        return Indexes(self._indexes)", "response": "Mapping of pandas. Index objects used for label based indexing."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nresetting the coordinates of the object to become variables.", "response": "def reset_coords(self, names=None, drop=False, inplace=None):\n        \"\"\"Given names of coordinates, reset them to become variables.\n\n        Parameters\n        ----------\n        names : str or list of str, optional\n            Name(s) of non-index coordinates in this dataset to reset into\n            variables. By default, all non-index coordinates are reset.\n        drop : bool, optional\n            If True, remove coordinates instead of converting them into\n            variables.\n        inplace : bool, optional\n            If True, modify this dataset inplace. Otherwise, create a new\n            object.\n\n        Returns\n        -------\n        Dataset, or DataArray if ``drop == True``\n        \"\"\"\n        inplace = _check_inplace(inplace)\n        if inplace and not drop:\n            raise ValueError('cannot reset coordinates in-place on a '\n                             'DataArray without ``drop == True``')\n        if names is None:\n            names = set(self.coords) - set(self.dims)\n        dataset = self.coords.to_dataset().reset_coords(names, drop)\n        if drop:\n            if inplace:\n                self._coords = dataset._variables\n            else:\n                return self._replace(coords=dataset._variables)\n        else:\n            if self.name is None:\n                raise ValueError('cannot reset_coords with drop=False '\n                                 'on an unnamed DataArrray')\n            dataset[self.name] = self.variable\n            return dataset"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load(self, **kwargs):\n        ds = self._to_temp_dataset().load(**kwargs)\n        new = self._from_temp_dataset(ds)\n        self._variable = new._variable\n        self._coords = new._coords\n        return self", "response": "Manually trigger loading of this array s data from disk or a\n        remote source into memory and return this array."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntriggering computation in constituent dask arrays This keeps them as dask arrays but encourages them as dask arrays but encourages them as dask arrays.", "response": "def persist(self, **kwargs):\n        \"\"\" Trigger computation in constituent dask arrays\n\n        This keeps them as dask arrays but encourages them to keep data in\n        memory.  This is particularly useful when on a distributed machine.\n        When on a single machine consider using ``.compute()`` instead.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Additional keyword arguments passed on to ``dask.persist``.\n\n        See Also\n        --------\n        dask.persist\n        \"\"\"\n        ds = self._to_temp_dataset().persist(**kwargs)\n        return self._from_temp_dataset(ds)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef copy(self, deep=True, data=None):\n        variable = self.variable.copy(deep=deep, data=data)\n        coords = OrderedDict((k, v.copy(deep=deep))\n                             for k, v in self._coords.items())\n        return self._replace(variable, coords)", "response": "Returns a shallow copy of the current array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncoerce this array s data into a dask array with the given chunks.", "response": "def chunk(self, chunks=None, name_prefix='xarray-', token=None,\n              lock=False):\n        \"\"\"Coerce this array's data into a dask arrays with the given chunks.\n\n        If this variable is a non-dask array, it will be converted to dask\n        array. If it's a dask array, it will be rechunked to the given chunk\n        sizes.\n\n        If neither chunks is not provided for one or more dimensions, chunk\n        sizes along that dimension will not be updated; non-dask arrays will be\n        converted into dask arrays with a single block.\n\n        Parameters\n        ----------\n        chunks : int, tuple or dict, optional\n            Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or\n            ``{'x': 5, 'y': 5}``.\n        name_prefix : str, optional\n            Prefix for the name of the new dask array.\n        token : str, optional\n            Token uniquely identifying this array.\n        lock : optional\n            Passed on to :py:func:`dask.array.from_array`, if the array is not\n            already as dask array.\n\n        Returns\n        -------\n        chunked : xarray.DataArray\n        \"\"\"\n        if isinstance(chunks, (list, tuple)):\n            chunks = dict(zip(self.dims, chunks))\n\n        ds = self._to_temp_dataset().chunk(chunks, name_prefix=name_prefix,\n                                           token=token, lock=lock)\n        return self._from_temp_dataset(ds)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef isel(self, indexers=None, drop=False, **indexers_kwargs):\n        indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'isel')\n        ds = self._to_temp_dataset().isel(drop=drop, indexers=indexers)\n        return self._from_temp_dataset(ds)", "response": "Return a new DataArray whose dataset is given by integer indexing\n        along the specified dimension."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sel(self, indexers=None, method=None, tolerance=None, drop=False,\n            **indexers_kwargs):\n        \"\"\"Return a new DataArray whose dataset is given by selecting\n        index labels along the specified dimension(s).\n\n        .. warning::\n\n          Do not try to assign values when using any of the indexing methods\n          ``isel`` or ``sel``::\n\n            da = xr.DataArray([0, 1, 2, 3], dims=['x'])\n            # DO NOT do this\n            da.isel(x=[0, 1, 2])[1] = -1\n\n          Assigning values with the chained indexing using ``.sel`` or\n          ``.isel`` fails silently.\n\n        See Also\n        --------\n        Dataset.sel\n        DataArray.isel\n\n        \"\"\"\n        ds = self._to_temp_dataset().sel(\n            indexers=indexers, drop=drop, method=method, tolerance=tolerance,\n            **indexers_kwargs)\n        return self._from_temp_dataset(ds)", "response": "Return a new DataArray whose dataset is given by selecting\n            index labels along the specified dimension."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new DataArray whose dataset is given by pointwise integer indexing along the specified dimension.", "response": "def isel_points(self, dim='points', **indexers):\n        \"\"\"Return a new DataArray whose dataset is given by pointwise integer\n        indexing along the specified dimension(s).\n\n        See Also\n        --------\n        Dataset.isel_points\n        \"\"\"\n        ds = self._to_temp_dataset().isel_points(dim=dim, **indexers)\n        return self._from_temp_dataset(ds)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sel_points(self, dim='points', method=None, tolerance=None,\n                   **indexers):\n        \"\"\"Return a new DataArray whose dataset is given by pointwise selection\n        of index labels along the specified dimension(s).\n\n        See Also\n        --------\n        Dataset.sel_points\n        \"\"\"\n        ds = self._to_temp_dataset().sel_points(\n            dim=dim, method=method, tolerance=tolerance, **indexers)\n        return self._from_temp_dataset(ds)", "response": "Return a new DataArray whose dataset is given by pointwise selection\n            of index labels along the specified dimension."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconforming this object onto a new set of indexes.", "response": "def reindex(self, indexers=None, method=None, tolerance=None, copy=True,\n                **indexers_kwargs):\n        \"\"\"Conform this object onto a new set of indexes, filling in\n        missing values with NaN.\n\n        Parameters\n        ----------\n        indexers : dict, optional\n            Dictionary with keys given by dimension names and values given by\n            arrays of coordinates tick labels. Any mis-matched coordinate\n            values will be filled in with NaN, and any mis-matched dimension\n            names will simply be ignored.\n            One of indexers or indexers_kwargs must be provided.\n        copy : bool, optional\n            If ``copy=True``, data in the return value is always copied. If\n            ``copy=False`` and reindexing is unnecessary, or can be performed\n            with only slice operations, then the output may share memory with\n            the input. In either case, a new xarray object is always returned.\n        method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n            Method to use for filling index values in ``indexers`` not found on\n            this data array:\n\n            * None (default): don't fill gaps\n            * pad / ffill: propagate last valid index value forward\n            * backfill / bfill: propagate next valid index value backward\n            * nearest: use nearest valid index value (requires pandas>=0.16)\n        tolerance : optional\n            Maximum distance between original and new labels for inexact\n            matches. The values of the index at the matching locations must\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n        **indexers_kwarg : {dim: indexer, ...}, optional\n            The keyword arguments form of ``indexers``.\n            One of indexers or indexers_kwargs must be provided.\n\n        Returns\n        -------\n        reindexed : DataArray\n            Another dataset array, with this array's data but replaced\n            coordinates.\n\n        See Also\n        --------\n        DataArray.reindex_like\n        align\n        \"\"\"\n        indexers = either_dict_or_kwargs(\n            indexers, indexers_kwargs, 'reindex')\n        ds = self._to_temp_dataset().reindex(\n            indexers=indexers, method=method, tolerance=tolerance, copy=copy)\n        return self._from_temp_dataset(ds)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninterpolate the multidimensional array with the given coordinates.", "response": "def interp(self, coords=None, method='linear', assume_sorted=False,\n               kwargs={}, **coords_kwargs):\n        \"\"\" Multidimensional interpolation of variables.\n\n        coords : dict, optional\n            Mapping from dimension names to the new coordinates.\n            new coordinate can be an scalar, array-like or DataArray.\n            If DataArrays are passed as new coordates, their dimensions are\n            used for the broadcasting.\n        method: {'linear', 'nearest'} for multidimensional array,\n            {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}\n            for 1-dimensional array.\n        assume_sorted: boolean, optional\n            If False, values of x can be in any order and they are sorted\n            first. If True, x has to be an array of monotonically increasing\n            values.\n        kwargs: dictionary\n            Additional keyword passed to scipy's interpolator.\n        **coords_kwarg : {dim: coordinate, ...}, optional\n            The keyword arguments form of ``coords``.\n            One of coords or coords_kwargs must be provided.\n\n        Returns\n        -------\n        interpolated: xr.DataArray\n            New dataarray on the new coordinates.\n\n        Notes\n        -----\n        scipy is required.\n\n        See Also\n        --------\n        scipy.interpolate.interp1d\n        scipy.interpolate.interpn\n\n        Examples\n        --------\n        >>> da = xr.DataArray([1, 3], [('x', np.arange(2))])\n        >>> da.interp(x=0.5)\n        <xarray.DataArray ()>\n        array(2.0)\n        Coordinates:\n            x        float64 0.5\n        \"\"\"\n        if self.dtype.kind not in 'uifc':\n            raise TypeError('interp only works for a numeric type array. '\n                            'Given {}.'.format(self.dtype))\n\n        ds = self._to_temp_dataset().interp(\n            coords, method=method, kwargs=kwargs, assume_sorted=assume_sorted,\n            **coords_kwargs)\n        return self._from_temp_dataset(ds)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef interp_like(self, other, method='linear', assume_sorted=False,\n                    kwargs={}):\n        \"\"\"Interpolate this object onto the coordinates of another object,\n        filling out of range values with NaN.\n\n        Parameters\n        ----------\n        other : Dataset or DataArray\n            Object with an 'indexes' attribute giving a mapping from dimension\n            names to an 1d array-like, which provides coordinates upon\n            which to index the variables in this dataset.\n        method: string, optional.\n            {'linear', 'nearest'} for multidimensional array,\n            {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}\n            for 1-dimensional array. 'linear' is used by default.\n        assume_sorted: boolean, optional\n            If False, values of coordinates that are interpolated over can be\n            in any order and they are sorted first. If True, interpolated\n            coordinates are assumed to be an array of monotonically increasing\n            values.\n        kwargs: dictionary, optional\n            Additional keyword passed to scipy's interpolator.\n\n        Returns\n        -------\n        interpolated: xr.DataArray\n            Another dataarray by interpolating this dataarray's data along the\n            coordinates of the other object.\n\n        Notes\n        -----\n        scipy is required.\n        If the dataarray has object-type coordinates, reindex is used for these\n        coordinates instead of the interpolation.\n\n        See Also\n        --------\n        DataArray.interp\n        DataArray.reindex_like\n        \"\"\"\n        if self.dtype.kind not in 'uifc':\n            raise TypeError('interp only works for a numeric type array. '\n                            'Given {}.'.format(self.dtype))\n\n        ds = self._to_temp_dataset().interp_like(\n            other, method=method, kwargs=kwargs, assume_sorted=assume_sorted)\n        return self._from_temp_dataset(ds)", "response": "Interpolate this object onto the coordinates of another object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new DataArray with renamed coordinates or a new name.", "response": "def rename(self, new_name_or_name_dict=None, **names):\n        \"\"\"Returns a new DataArray with renamed coordinates or a new name.\n\n        Parameters\n        ----------\n        new_name_or_name_dict : str or dict-like, optional\n            If the argument is dict-like, it it used as a mapping from old\n            names to new names for coordinates. Otherwise, use the argument\n            as the new name for this array.\n        **names, optional\n            The keyword arguments form of a mapping from old names to\n            new names for coordinates.\n            One of new_name_or_name_dict or names must be provided.\n\n\n        Returns\n        -------\n        renamed : DataArray\n            Renamed array or array with renamed coordinates.\n\n        See Also\n        --------\n        Dataset.rename\n        DataArray.swap_dims\n        \"\"\"\n        if names or utils.is_dict_like(new_name_or_name_dict):\n            name_dict = either_dict_or_kwargs(\n                new_name_or_name_dict, names, 'rename')\n            dataset = self._to_temp_dataset().rename(name_dict)\n            return self._from_temp_dataset(dataset)\n        else:\n            return self._replace(name=new_name_or_name_dict)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a new Dataset containing swapped dimensions.", "response": "def swap_dims(self, dims_dict):\n        \"\"\"Returns a new DataArray with swapped dimensions.\n\n        Parameters\n        ----------\n        dims_dict : dict-like\n            Dictionary whose keys are current dimension names and whose values\n            are new names. Each value must already be a coordinate on this\n            array.\n\n        Returns\n        -------\n        renamed : Dataset\n            DataArray with swapped dimensions.\n\n        See Also\n        --------\n\n        DataArray.rename\n        Dataset.swap_dims\n        \"\"\"\n        ds = self._to_temp_dataset().swap_dims(dims_dict)\n        return self._from_temp_dataset(ds)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new object with an additional dimension inserted at the corresponding position in the array.", "response": "def expand_dims(self, dim=None, axis=None, **dim_kwargs):\n        \"\"\"Return a new object with an additional axis (or axes) inserted at\n        the corresponding position in the array shape.\n\n        If dim is already a scalar coordinate, it will be promoted to a 1D\n        coordinate consisting of a single value.\n\n        Parameters\n        ----------\n        dim : str, sequence of str, dict, or None\n            Dimensions to include on the new variable.\n            If provided as str or sequence of str, then dimensions are inserted\n            with length 1. If provided as a dict, then the keys are the new\n            dimensions and the values are either integers (giving the length of\n            the new dimensions) or sequence/ndarray (giving the coordinates of\n            the new dimensions). **WARNING** for python 3.5, if ``dim`` is\n            dict-like, then it must be an ``OrderedDict``. This is to ensure\n            that the order in which the dims are given is maintained.\n        axis : integer, list (or tuple) of integers, or None\n            Axis position(s) where new axis is to be inserted (position(s) on\n            the result array). If a list (or tuple) of integers is passed,\n            multiple axes are inserted. In this case, dim arguments should be\n            same length list. If axis=None is passed, all the axes will be\n            inserted to the start of the result array.\n        **dim_kwargs : int or sequence/ndarray\n            The keywords are arbitrary dimensions being inserted and the values\n            are either the lengths of the new dims (if int is given), or their\n            coordinates. Note, this is an alternative to passing a dict to the\n            dim kwarg and will only be used if dim is None. **WARNING** for\n            python 3.5 ``dim_kwargs`` is not available.\n\n        Returns\n        -------\n        expanded : same type as caller\n            This object, but with an additional dimension(s).\n        \"\"\"\n        if isinstance(dim, int):\n            raise TypeError('dim should be str or sequence of strs or dict')\n        elif isinstance(dim, str):\n            dim = OrderedDict(((dim, 1),))\n        elif isinstance(dim, (list, tuple)):\n            if len(dim) != len(set(dim)):\n                raise ValueError('dims should not contain duplicate values.')\n            dim = OrderedDict(((d, 1) for d in dim))\n\n        # TODO: get rid of the below code block when python 3.5 is no longer\n        #   supported.\n        python36_plus = sys.version_info[0] == 3 and sys.version_info[1] > 5\n        not_ordereddict = dim is not None and not isinstance(dim, OrderedDict)\n        if not python36_plus and not_ordereddict:\n            raise TypeError(\"dim must be an OrderedDict for python <3.6\")\n        elif not python36_plus and dim_kwargs:\n            raise ValueError(\"dim_kwargs isn't available for python <3.6\")\n        dim_kwargs = OrderedDict(dim_kwargs)\n\n        dim = either_dict_or_kwargs(dim, dim_kwargs, 'expand_dims')\n        ds = self._to_temp_dataset().expand_dims(dim, axis)\n        return self._from_temp_dataset(ds)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the index of the data array using one or more existing coordinates.", "response": "def set_index(self, indexes=None, append=False, inplace=None,\n                  **indexes_kwargs):\n        \"\"\"Set DataArray (multi-)indexes using one or more existing\n        coordinates.\n\n        Parameters\n        ----------\n        indexes : {dim: index, ...}\n            Mapping from names matching dimensions and values given\n            by (lists of) the names of existing coordinates or variables to set\n            as new (multi-)index.\n        append : bool, optional\n            If True, append the supplied index(es) to the existing index(es).\n            Otherwise replace the existing index(es) (default).\n        inplace : bool, optional\n            If True, set new index(es) in-place. Otherwise, return a new\n            DataArray object.\n        **indexes_kwargs: optional\n            The keyword arguments form of ``indexes``.\n            One of indexes or indexes_kwargs must be provided.\n\n        Returns\n        -------\n        obj : DataArray\n            Another dataarray, with this data but replaced coordinates.\n\n        See Also\n        --------\n        DataArray.reset_index\n        \"\"\"\n        inplace = _check_inplace(inplace)\n        indexes = either_dict_or_kwargs(indexes, indexes_kwargs, 'set_index')\n        coords, _ = merge_indexes(indexes, self._coords, set(), append=append)\n        if inplace:\n            self._coords = coords\n        else:\n            return self._replace(coords=coords)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nresets the index of the dataarray to the specified index or multi - index level.", "response": "def reset_index(self, dims_or_levels, drop=False, inplace=None):\n        \"\"\"Reset the specified index(es) or multi-index level(s).\n\n        Parameters\n        ----------\n        dims_or_levels : str or list\n            Name(s) of the dimension(s) and/or multi-index level(s) that will\n            be reset.\n        drop : bool, optional\n            If True, remove the specified indexes and/or multi-index levels\n            instead of extracting them as new coordinates (default: False).\n        inplace : bool, optional\n            If True, modify the dataarray in-place. Otherwise, return a new\n            DataArray object.\n\n        Returns\n        -------\n        obj : DataArray\n            Another dataarray, with this dataarray's data but replaced\n            coordinates.\n\n        See Also\n        --------\n        DataArray.set_index\n        \"\"\"\n        inplace = _check_inplace(inplace)\n        coords, _ = split_indexes(dims_or_levels, self._coords, set(),\n                                  self._level_coords, drop=drop)\n        if inplace:\n            self._coords = coords\n        else:\n            return self._replace(coords=coords)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrearranging index levels using input order.", "response": "def reorder_levels(self, dim_order=None, inplace=None,\n                       **dim_order_kwargs):\n        \"\"\"Rearrange index levels using input order.\n\n        Parameters\n        ----------\n        dim_order : optional\n            Mapping from names matching dimensions and values given\n            by lists representing new level orders. Every given dimension\n            must have a multi-index.\n        inplace : bool, optional\n            If True, modify the dataarray in-place. Otherwise, return a new\n            DataArray object.\n        **dim_order_kwargs: optional\n            The keyword arguments form of ``dim_order``.\n            One of dim_order or dim_order_kwargs must be provided.\n\n        Returns\n        -------\n        obj : DataArray\n            Another dataarray, with this dataarray's data but replaced\n            coordinates.\n        \"\"\"\n        inplace = _check_inplace(inplace)\n        dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs,\n                                          'reorder_levels')\n        replace_coords = {}\n        for dim, order in dim_order.items():\n            coord = self._coords[dim]\n            index = coord.to_index()\n            if not isinstance(index, pd.MultiIndex):\n                raise ValueError(\"coordinate %r has no MultiIndex\" % dim)\n            replace_coords[dim] = IndexVariable(coord.dims,\n                                                index.reorder_levels(order))\n        coords = self._coords.copy()\n        coords.update(replace_coords)\n        if inplace:\n            self._coords = coords\n        else:\n            return self._replace(coords=coords)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstacking any number of existing dimensions into a single new dimension.", "response": "def stack(self, dimensions=None, **dimensions_kwargs):\n        \"\"\"\n        Stack any number of existing dimensions into a single new dimension.\n\n        New dimensions will be added at the end, and the corresponding\n        coordinate variables will be combined into a MultiIndex.\n\n        Parameters\n        ----------\n        dimensions : Mapping of the form new_name=(dim1, dim2, ...)\n            Names of new dimensions, and the existing dimensions that they\n            replace.\n        **dimensions_kwargs:\n            The keyword arguments form of ``dimensions``.\n            One of dimensions or dimensions_kwargs must be provided.\n\n        Returns\n        -------\n        stacked : DataArray\n            DataArray with stacked data.\n\n        Examples\n        --------\n\n        >>> arr = DataArray(np.arange(6).reshape(2, 3),\n        ...                 coords=[('x', ['a', 'b']), ('y', [0, 1, 2])])\n        >>> arr\n        <xarray.DataArray (x: 2, y: 3)>\n        array([[0, 1, 2],\n               [3, 4, 5]])\n        Coordinates:\n          * x        (x) |S1 'a' 'b'\n          * y        (y) int64 0 1 2\n        >>> stacked = arr.stack(z=('x', 'y'))\n        >>> stacked.indexes['z']\n        MultiIndex(levels=[['a', 'b'], [0, 1, 2]],\n                   labels=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],\n                   names=['x', 'y'])\n\n        See also\n        --------\n        DataArray.unstack\n        \"\"\"\n        ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)\n        return self._from_temp_dataset(ds)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unstack(self, dim=None):\n        ds = self._to_temp_dataset().unstack(dim)\n        return self._from_temp_dataset(ds)", "response": "Unstack existing dimensions into multiple new dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new DataArray with transposed dimensions.", "response": "def transpose(self, *dims) -> 'DataArray':\n        \"\"\"Return a new DataArray object with transposed dimensions.\n\n        Parameters\n        ----------\n        *dims : str, optional\n            By default, reverse the dimensions. Otherwise, reorder the\n            dimensions to this order.\n\n        Returns\n        -------\n        transposed : DataArray\n            The returned DataArray's array is transposed.\n\n        Notes\n        -----\n        This operation returns a view of this array's data. It is\n        lazy for dask-backed DataArrays but not for numpy-backed DataArrays\n        -- the data will be fully loaded.\n\n        See Also\n        --------\n        numpy.transpose\n        Dataset.transpose\n        \"\"\"\n        variable = self.variable.transpose(*dims)\n        return self._replace(variable)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndrop coordinates or index labels from this DataArray.", "response": "def drop(self, labels, dim=None):\n        \"\"\"Drop coordinates or index labels from this DataArray.\n\n        Parameters\n        ----------\n        labels : scalar or list of scalars\n            Name(s) of coordinate variables or index labels to drop.\n        dim : str, optional\n            Dimension along which to drop index labels. By default (if\n            ``dim is None``), drops coordinates rather than index labels.\n\n        Returns\n        -------\n        dropped : DataArray\n        \"\"\"\n        if utils.is_scalar(labels):\n            labels = [labels]\n        ds = self._to_temp_dataset().drop(labels, dim)\n        return self._from_temp_dataset(ds)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dropna(self, dim, how='any', thresh=None):\n        ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)\n        return self._from_temp_dataset(ds)", "response": "Returns a new array with dropped labels for missing values along the provided dimension."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fillna(self, value):\n        if utils.is_dict_like(value):\n            raise TypeError('cannot provide fill value as a dictionary with '\n                            'fillna on a DataArray')\n        out = ops.fillna(self, value)\n        return out", "response": "Fill missing values in this array with value."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfill NaN values by propogating values forward", "response": "def ffill(self, dim, limit=None):\n        '''Fill NaN values by propogating values forward\n\n        *Requires bottleneck.*\n\n        Parameters\n        ----------\n        dim : str\n            Specifies the dimension along which to propagate values when\n            filling.\n        limit : int, default None\n            The maximum number of consecutive NaN values to forward fill. In\n            other words, if there is a gap with more than this number of\n            consecutive NaNs, it will only be partially filled. Must be greater\n            than 0 or None for no limit.\n\n        Returns\n        -------\n        DataArray\n        '''\n        from .missing import ffill\n        return ffill(self, dim, limit=limit)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bfill(self, dim, limit=None):\n        '''Fill NaN values by propogating values backward\n\n        *Requires bottleneck.*\n\n        Parameters\n        ----------\n        dim : str\n            Specifies the dimension along which to propagate values when\n            filling.\n        limit : int, default None\n            The maximum number of consecutive NaN values to backward fill. In\n            other words, if there is a gap with more than this number of\n            consecutive NaNs, it will only be partially filled. Must be greater\n            than 0 or None for no limit.\n\n        Returns\n        -------\n        DataArray\n        '''\n        from .missing import bfill\n        return bfill(self, dim, limit=limit)", "response": "Fill NaN values by propogating values backward\n            filling."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reduce(self, func, dim=None, axis=None, keep_attrs=None, **kwargs):\n\n        var = self.variable.reduce(func, dim, axis, keep_attrs, **kwargs)\n        return self._replace_maybe_drop_dims(var)", "response": "Reduce this array by applying func along some dimension."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_pandas(self):\n        # TODO: consolidate the info about pandas constructors and the\n        # attributes that correspond to their indexes into a separate module?\n        constructors = {0: lambda x: x,\n                        1: pd.Series,\n                        2: pd.DataFrame,\n                        3: pd.Panel}\n        try:\n            constructor = constructors[self.ndim]\n        except KeyError:\n            raise ValueError('cannot convert arrays with %s dimensions into '\n                             'pandas objects' % self.ndim)\n        indexes = [self.get_index(dim) for dim in self.dims]\n        return constructor(self.values, *indexes)", "response": "Convert this array into a pandas object with the same shape."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert this array and its coordinates into a tidy pandas. DataFrame.", "response": "def to_dataframe(self, name=None):\n        \"\"\"Convert this array and its coordinates into a tidy pandas.DataFrame.\n\n        The DataFrame is indexed by the Cartesian product of index coordinates\n        (in the form of a :py:class:`pandas.MultiIndex`).\n\n        Other coordinates are included as columns in the DataFrame.\n        \"\"\"\n        if name is None:\n            name = self.name\n        if name is None:\n            raise ValueError('cannot convert an unnamed DataArray to a '\n                             'DataFrame: use the ``name`` parameter')\n\n        dims = OrderedDict(zip(self.dims, self.shape))\n        # By using a unique name, we can convert a DataArray into a DataFrame\n        # even if it shares a name with one of its coordinates.\n        # I would normally use unique_name = object() but that results in a\n        # dataframe with columns in the wrong order, for reasons I have not\n        # been able to debug (possibly a pandas bug?).\n        unique_name = '__unique_name_identifier_z98xfz98xugfg73ho__'\n        ds = self._to_dataset_whole(name=unique_name)\n        df = ds._to_dataframe(dims)\n        df.columns = [name if c == unique_name else c\n                      for c in df.columns]\n        return df"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_series(self):\n        index = self.coords.to_index()\n        return pd.Series(self.values.reshape(-1), index=index, name=self.name)", "response": "Convert this array into a pandas. Series."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_masked_array(self, copy=True):\n        isnull = pd.isnull(self.values)\n        return np.ma.MaskedArray(data=self.values, mask=isnull, copy=copy)", "response": "Convert this array into a numpy. ma. MaskedArray."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite the contents of the array to a netCDF file.", "response": "def to_netcdf(self, *args, **kwargs):\n        \"\"\"Write DataArray contents to a netCDF file.\n\n        Parameters\n        ----------\n        path : str or Path, optional\n            Path to which to save this dataset. If no path is provided, this\n            function returns the resulting netCDF file as a bytes object; in\n            this case, we need to use scipy.io.netcdf, which does not support\n            netCDF version 4 (the default format becomes NETCDF3_64BIT).\n        mode : {'w', 'a'}, optional\n            Write ('w') or append ('a') mode. If mode='w', any existing file at\n            this location will be overwritten.\n        format : {'NETCDF4', 'NETCDF4_CLASSIC', 'NETCDF3_64BIT',\n                  'NETCDF3_CLASSIC'}, optional\n            File format for the resulting netCDF file:\n\n            * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API\n              features.\n            * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only\n              netCDF 3 compatible API features.\n            * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,\n              which fully supports 2+ GB files, but is only compatible with\n              clients linked against netCDF version 3.6.0 or later.\n            * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not\n              handle 2+ GB files very well.\n\n            All formats are supported by the netCDF4-python library.\n            scipy.io.netcdf only supports the last two formats.\n\n            The default format is NETCDF4 if you are saving a file to disk and\n            have the netCDF4-python library available. Otherwise, xarray falls\n            back to using scipy to write netCDF files and defaults to the\n            NETCDF3_64BIT format (scipy does not support netCDF4).\n        group : str, optional\n            Path to the netCDF4 group in the given file to open (only works for\n            format='NETCDF4'). The group(s) will be created if necessary.\n        engine : {'netcdf4', 'scipy', 'h5netcdf'}, optional\n            Engine to use when writing netCDF files. If not provided, the\n            default engine is chosen based on available dependencies, with a\n            preference for 'netcdf4' if writing to a file on disk.\n        encoding : dict, optional\n            Nested dictionary with variable names as keys and dictionaries of\n            variable specific encodings as values, e.g.,\n            ``{'my_variable': {'dtype': 'int16', 'scale_factor': 0.1,\n               'zlib': True}, ...}``\n\n        Notes\n        -----\n        Only xarray.Dataset objects can be written to netCDF files, so\n        the xarray.DataArray is converted to a xarray.Dataset object\n        containing a single variable. If the DataArray has no name, or if the\n        name is the same as a co-ordinate name, then it is given the name\n        '__xarray_dataarray_variable__'.\n\n        All parameters are passed directly to `xarray.Dataset.to_netcdf`.\n        \"\"\"\n        from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE\n\n        if self.name is None:\n            # If no name is set then use a generic xarray name\n            dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n        elif self.name in self.coords or self.name in self.dims:\n            # The name is the same as one of the coords names, which netCDF\n            # doesn't support, so rename it but keep track of the old name\n            dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n            dataset.attrs[DATAARRAY_NAME] = self.name\n        else:\n            # No problems with the name - so we're fine!\n            dataset = self.to_dataset()\n\n        return dataset.to_netcdf(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts this xarray. DataArray into a dictionary following xarray. NameConventions.", "response": "def to_dict(self, data=True):\n        \"\"\"\n        Convert this xarray.DataArray into a dictionary following xarray\n        naming conventions.\n\n        Converts all variables and attributes to native Python objects.\n        Useful for coverting to json. To avoid datetime incompatibility\n        use decode_times=False kwarg in xarrray.open_dataset.\n\n        Parameters\n        ----------\n        data : bool, optional\n            Whether to include the actual data in the dictionary. When set to\n            False, returns just the schema.\n\n        See also\n        --------\n        DataArray.from_dict\n        \"\"\"\n        d = self.variable.to_dict(data=data)\n        d.update({'coords': {}, 'name': self.name})\n        for k in self.coords:\n            d['coords'][k] = self.coords[k].variable.to_dict(data=data)\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_dict(cls, d):\n        coords = None\n        if 'coords' in d:\n            try:\n                coords = OrderedDict([(k, (v['dims'],\n                                           v['data'],\n                                           v.get('attrs')))\n                                      for k, v in d['coords'].items()])\n            except KeyError as e:\n                raise ValueError(\n                    \"cannot convert dict when coords are missing the key \"\n                    \"'{dims_data}'\".format(dims_data=str(e.args[0])))\n        try:\n            data = d['data']\n        except KeyError:\n            raise ValueError(\"cannot convert dict without the key 'data''\")\n        else:\n            obj = cls(data, coords, d.get('dims'), d.get('name'),\n                      d.get('attrs'))\n        return obj", "response": "Convert a dictionary into an xarray. DataArray\n            object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a pandas. Series into an xarray. DataArray.", "response": "def from_series(cls, series):\n        \"\"\"Convert a pandas.Series into an xarray.DataArray.\n\n        If the series's index is a MultiIndex, it will be expanded into a\n        tensor product of one-dimensional coordinates (filling in missing\n        values with NaN). Thus this operation should be the inverse of the\n        `to_series` method.\n        \"\"\"\n        # TODO: add a 'name' parameter\n        name = series.name\n        df = pd.DataFrame({name: series})\n        ds = Dataset.from_dataframe(df)\n        return ds[name]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlike equals but also checks the array name attributes and attributes on all coordinates.", "response": "def identical(self, other):\n        \"\"\"Like equals, but also checks the array name and attributes, and\n        attributes on all coordinates.\n\n        See Also\n        --------\n        DataArray.broadcast_equals\n        DataArray.equal\n        \"\"\"\n        try:\n            return (self.name == other.name and\n                    self._all_compat(other, 'identical'))\n        except (TypeError, AttributeError):\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _title_for_slice(self, truncate=50):\n        one_dims = []\n        for dim, coord in self.coords.items():\n            if coord.size == 1:\n                one_dims.append('{dim} = {v}'.format(\n                    dim=dim, v=format_item(coord.values)))\n\n        title = ', '.join(one_dims)\n        if len(title) > truncate:\n            title = title[:(truncate - 3)] + '...'\n\n        return title", "response": "Returns a string that can be used for plot titles for a slice of the dataarray."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the n - th order discrete difference along given axis.", "response": "def diff(self, dim, n=1, label='upper'):\n        \"\"\"Calculate the n-th order discrete difference along given axis.\n\n        Parameters\n        ----------\n        dim : str, optional\n            Dimension over which to calculate the finite difference.\n        n : int, optional\n            The number of times values are differenced.\n        label : str, optional\n            The new coordinate in dimension ``dim`` will have the\n            values of either the minuend's or subtrahend's coordinate\n            for values 'upper' and 'lower', respectively.  Other\n            values are not supported.\n\n        Returns\n        -------\n        difference : same type as caller\n            The n-th order finite difference of this object.\n\n        Examples\n        --------\n        >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ['x'])\n        >>> arr.diff('x')\n        <xarray.DataArray (x: 3)>\n        array([0, 1, 0])\n        Coordinates:\n        * x        (x) int64 2 3 4\n        >>> arr.diff('x', 2)\n        <xarray.DataArray (x: 2)>\n        array([ 1, -1])\n        Coordinates:\n        * x        (x) int64 3 4\n\n        See Also\n        --------\n        DataArray.differentiate\n        \"\"\"\n        ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)\n        return self._from_temp_dataset(ds)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):\n        variable = self.variable.shift(\n            shifts=shifts, fill_value=fill_value, **shifts_kwargs)\n        return self._replace(variable=variable)", "response": "Shifts this array by an offset along one or more dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrolls this array by an offset along one or more dimensions.", "response": "def roll(self, shifts=None, roll_coords=None, **shifts_kwargs):\n        \"\"\"Roll this array by an offset along one or more dimensions.\n\n        Unlike shift, roll may rotate all variables, including coordinates\n        if specified. The direction of rotation is consistent with\n        :py:func:`numpy.roll`.\n\n        Parameters\n        ----------\n        roll_coords : bool\n            Indicates whether to  roll the coordinates by the offset\n            The current default of roll_coords (None, equivalent to True) is\n            deprecated and will change to False in a future version.\n            Explicitly pass roll_coords to silence the warning.\n        **shifts : keyword arguments of the form {dim: offset}\n            Integer offset to rotate each of the given dimensions. Positive\n            offsets roll to the right; negative offsets roll to the left.\n\n        Returns\n        -------\n        rolled : DataArray\n            DataArray with the same attributes but rolled data and coordinates.\n\n        See also\n        --------\n        shift\n\n        Examples\n        --------\n\n        >>> arr = xr.DataArray([5, 6, 7], dims='x')\n        >>> arr.roll(x=1)\n        <xarray.DataArray (x: 3)>\n        array([7, 5, 6])\n        Coordinates:\n          * x        (x) int64 2 0 1\n        \"\"\"\n        ds = self._to_temp_dataset().roll(\n            shifts=shifts, roll_coords=roll_coords, **shifts_kwargs)\n        return self._from_temp_dataset(ds)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dot(self, other, dims=None):\n        if isinstance(other, Dataset):\n            raise NotImplementedError('dot products are not yet supported '\n                                      'with Dataset objects.')\n        if not isinstance(other, DataArray):\n            raise TypeError('dot only operates on DataArrays.')\n\n        return computation.dot(self, other, dims=dims)", "response": "Perform the dot product of two DataArrays along their shared dims."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sortby(self, variables, ascending=True):\n        ds = self._to_temp_dataset().sortby(variables, ascending=ascending)\n        return self._from_temp_dataset(ds)", "response": "Sort the data array by the specified variables."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef quantile(self, q, dim=None, interpolation='linear', keep_attrs=None):\n\n        ds = self._to_temp_dataset().quantile(\n            q, dim=dim, keep_attrs=keep_attrs, interpolation=interpolation)\n        return self._from_temp_dataset(ds)", "response": "Compute the qth quantile of the data along the specified dimension."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rank(self, dim, pct=False, keep_attrs=None):\n\n        ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)\n        return self._from_temp_dataset(ds)", "response": "Ranks the data array."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndifferentiating the array with the second order accurate central differences. .. note:: This feature is limited to simple cartesian geometry, i.e. coord must be one dimensional. Parameters ---------- coord: str The coordinate to be used to compute the gradient. edge_order: 1 or 2. Default 1 N-th order accurate differences at the boundaries. datetime_unit: None or any of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', 'ps', 'fs', 'as'} Unit to compute gradient. Only valid for datetime coordinate. Returns ------- differentiated: DataArray See also -------- numpy.gradient: corresponding numpy function Examples -------- >>> da = xr.DataArray(np.arange(12).reshape(4, 3), dims=['x', 'y'], ... coords={'x': [0, 0.1, 1.1, 1.2]}) >>> da <xarray.DataArray (x: 4, y: 3)> array([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11]]) Coordinates: * x (x) float64 0.0 0.1 1.1 1.2 Dimensions without coordinates: y >>> >>> da.differentiate('x') <xarray.DataArray (x: 4, y: 3)> array([[30. , 30. , 30. ], [27.545455, 27.545455, 27.545455], [27.545455, 27.545455, 27.545455], [30. , 30. , 30. ]]) Coordinates: * x (x) float64 0.0 0.1 1.1 1.2 Dimensions without coordinates: y", "response": "def differentiate(self, coord, edge_order=1, datetime_unit=None):\n        \"\"\" Differentiate the array with the second order accurate central\n        differences.\n\n        .. note::\n            This feature is limited to simple cartesian geometry, i.e. coord\n            must be one dimensional.\n\n        Parameters\n        ----------\n        coord: str\n            The coordinate to be used to compute the gradient.\n        edge_order: 1 or 2. Default 1\n            N-th order accurate differences at the boundaries.\n        datetime_unit: None or any of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms',\n            'us', 'ns', 'ps', 'fs', 'as'}\n            Unit to compute gradient. Only valid for datetime coordinate.\n\n        Returns\n        -------\n        differentiated: DataArray\n\n        See also\n        --------\n        numpy.gradient: corresponding numpy function\n\n        Examples\n        --------\n\n        >>> da = xr.DataArray(np.arange(12).reshape(4, 3), dims=['x', 'y'],\n        ...                   coords={'x': [0, 0.1, 1.1, 1.2]})\n        >>> da\n        <xarray.DataArray (x: 4, y: 3)>\n        array([[ 0,  1,  2],\n               [ 3,  4,  5],\n               [ 6,  7,  8],\n               [ 9, 10, 11]])\n        Coordinates:\n          * x        (x) float64 0.0 0.1 1.1 1.2\n        Dimensions without coordinates: y\n        >>>\n        >>> da.differentiate('x')\n        <xarray.DataArray (x: 4, y: 3)>\n        array([[30.      , 30.      , 30.      ],\n               [27.545455, 27.545455, 27.545455],\n               [27.545455, 27.545455, 27.545455],\n               [30.      , 30.      , 30.      ]])\n        Coordinates:\n          * x        (x) float64 0.0 0.1 1.1 1.2\n        Dimensions without coordinates: y\n        \"\"\"\n        ds = self._to_temp_dataset().differentiate(\n            coord, edge_order, datetime_unit)\n        return self._from_temp_dataset(ds)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef integrate(self, dim, datetime_unit=None):\n        ds = self._to_temp_dataset().integrate(dim, datetime_unit)\n        return self._from_temp_dataset(ds)", "response": "Integrate the array with the trapezoidal rule."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef construct(self, window_dim, stride=1, fill_value=dtypes.NA):\n\n        from .dataarray import DataArray\n\n        window = self.obj.variable.rolling_window(self.dim, self.window,\n                                                  window_dim, self.center,\n                                                  fill_value=fill_value)\n        result = DataArray(window, dims=self.obj.dims + (window_dim,),\n                           coords=self.obj.coords)\n        return result.isel(**{self.dim: slice(None, None, stride)})", "response": "Convert this rolling object to xr. DataArray where the window dimension is stacked as a new dimension."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reduce(self, func, **kwargs):\n        rolling_dim = utils.get_temp_dimname(self.obj.dims, '_rolling_dim')\n        windows = self.construct(rolling_dim)\n        result = windows.reduce(func, dim=rolling_dim, **kwargs)\n\n        # Find valid windows based on count.\n        counts = self._counts()\n        return result.where(counts >= self._min_periods)", "response": "Reduce the items in this group by applying func along some tier dimension."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nnumbers of non - nan entries in each rolling window.", "response": "def _counts(self):\n        \"\"\" Number of non-nan entries in each rolling window. \"\"\"\n\n        rolling_dim = utils.get_temp_dimname(self.obj.dims, '_rolling_dim')\n        # We use False as the fill_value instead of np.nan, since boolean\n        # array is faster to be reduced than object array.\n        # The use of skipna==False is also faster since it does not need to\n        # copy the strided array.\n        counts = (self.obj.notnull()\n                  .rolling(center=self.center, **{self.dim: self.window})\n                  .construct(rolling_dim, fill_value=False)\n                  .sum(dim=rolling_dim, skipna=False))\n        return counts"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a wrapped function for any function func for the n - tuple class.", "response": "def _reduce_method(cls, func):\n        \"\"\"\n        Methods to return a wrapped function for any function `func` for\n        numpy methods.\n        \"\"\"\n\n        def wrapped_func(self, **kwargs):\n            return self.reduce(func, **kwargs)\n        return wrapped_func"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reduce(self, func, **kwargs):\n        from .dataset import Dataset\n        reduced = OrderedDict()\n        for key, da in self.obj.data_vars.items():\n            if self.dim in da.dims:\n                reduced[key] = self.rollings[key].reduce(func, **kwargs)\n            else:\n                reduced[key] = self.obj[key]\n        return Dataset(reduced, coords=self.obj.coords)", "response": "Reduce the items in this group by applying func along some tier dimension."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a wrapped function for injecting numpy and bottoleneck methods. is the name of the method that is used to reduce the object.", "response": "def _reduce_method(cls, func):\n        \"\"\"\n        Return a wrapped function for injecting numpy and bottoleneck methods.\n        see ops.inject_datasetrolling_methods\n        \"\"\"\n\n        def wrapped_func(self, **kwargs):\n            from .dataset import Dataset\n            reduced = OrderedDict()\n            for key, da in self.obj.data_vars.items():\n                if self.dim in da.dims:\n                    reduced[key] = getattr(self.rollings[key],\n                                           func.__name__)(**kwargs)\n                else:\n                    reduced[key] = self.obj[key]\n            return Dataset(reduced, coords=self.obj.coords)\n        return wrapped_func"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef construct(self, window_dim, stride=1, fill_value=dtypes.NA):\n\n        from .dataset import Dataset\n\n        dataset = OrderedDict()\n        for key, da in self.obj.data_vars.items():\n            if self.dim in da.dims:\n                dataset[key] = self.rollings[key].construct(\n                    window_dim, fill_value=fill_value)\n            else:\n                dataset[key] = da\n        return Dataset(dataset, coords=self.obj.coords).isel(\n            **{self.dim: slice(None, None, stride)})", "response": "Convert this rolling object to xr. Dataset where the dimension is stacked as a new dimension."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _reduce_method(cls, func):\n        def wrapped_func(self, **kwargs):\n            from .dataarray import DataArray\n\n            reduced = self.obj.variable.coarsen(\n                self.windows, func, self.boundary, self.side)\n            coords = {}\n            for c, v in self.obj.coords.items():\n                if c == self.obj.name:\n                    coords[c] = reduced\n                else:\n                    if any(d in self.windows for d in v.dims):\n                        coords[c] = v.variable.coarsen(\n                            self.windows, self.coord_func[c],\n                            self.boundary, self.side)\n                    else:\n                        coords[c] = v\n            return DataArray(reduced, dims=self.obj.dims, coords=coords)\n\n        return wrapped_func", "response": "Returns a wrapped function for injecting numpy methods."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a wrapped function for injecting numpy methods.", "response": "def _reduce_method(cls, func):\n        \"\"\"\n        Return a wrapped function for injecting numpy methods.\n        see ops.inject_coarsen_methods\n        \"\"\"\n        def wrapped_func(self, **kwargs):\n            from .dataset import Dataset\n\n            reduced = OrderedDict()\n            for key, da in self.obj.data_vars.items():\n                reduced[key] = da.variable.coarsen(\n                    self.windows, func, self.boundary, self.side)\n\n            coords = {}\n            for c, v in self.obj.coords.items():\n                if any(d in self.windows for d in v.dims):\n                    coords[c] = v.variable.coarsen(\n                        self.windows, self.coord_func[c],\n                        self.boundary, self.side)\n                else:\n                    coords[c] = v.variable\n            return Dataset(reduced, coords=coords)\n\n        return wrapped_func"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ensure_fixed_length_bytes(var):\n    dims, data, attrs, encoding = unpack_for_encoding(var)\n    if check_vlen_dtype(data.dtype) == bytes:\n        # TODO: figure out how to handle this with dask\n        data = np.asarray(data, dtype=np.string_)\n    return Variable(dims, data, attrs, encoding)", "response": "Ensure that a variable with vlen bytes is converted to fixed width."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting numpy arrays from fixed width bytes to characters.", "response": "def bytes_to_char(arr):\n    \"\"\"Convert numpy/dask arrays from fixed width bytes to characters.\"\"\"\n    if arr.dtype.kind != 'S':\n        raise ValueError('argument must have a fixed-width bytes dtype')\n\n    if isinstance(arr, dask_array_type):\n        import dask.array as da\n        return da.map_blocks(_numpy_bytes_to_char, arr,\n                             dtype='S1',\n                             chunks=arr.chunks + ((arr.dtype.itemsize,)),\n                             new_axis=[arr.ndim])\n    else:\n        return _numpy_bytes_to_char(arr)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _numpy_bytes_to_char(arr):\n    # ensure the array is contiguous\n    arr = np.array(arr, copy=False, order='C', dtype=np.string_)\n    return arr.reshape(arr.shape + (1,)).view('S1')", "response": "Like netCDF4. stringtochar but faster and more flexible.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef char_to_bytes(arr):\n    if arr.dtype != 'S1':\n        raise ValueError(\"argument must have dtype='S1'\")\n\n    if not arr.ndim:\n        # no dimension to concatenate along\n        return arr\n\n    size = arr.shape[-1]\n\n    if not size:\n        # can't make an S0 dtype\n        return np.zeros(arr.shape[:-1], dtype=np.string_)\n\n    if isinstance(arr, dask_array_type):\n        import dask.array as da\n\n        if len(arr.chunks[-1]) > 1:\n            raise ValueError('cannot stacked dask character array with '\n                             'multiple chunks in the last dimension: {}'\n                             .format(arr))\n\n        dtype = np.dtype('S' + str(arr.shape[-1]))\n        return da.map_blocks(_numpy_char_to_bytes, arr,\n                             dtype=dtype,\n                             chunks=arr.chunks[:-1],\n                             drop_axis=[arr.ndim - 1])\n    else:\n        return StackedBytesArray(arr)", "response": "Convert numpy arrays from characters to fixed width bytes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nliking netCDF4. chartostring but faster and more flexible.", "response": "def _numpy_char_to_bytes(arr):\n    \"\"\"Like netCDF4.chartostring, but faster and more flexible.\n    \"\"\"\n    # based on: http://stackoverflow.com/a/10984878/809705\n    arr = np.array(arr, copy=False, order='C')\n    dtype = 'S' + str(arr.shape[-1])\n    return arr.view(dtype).reshape(arr.shape[:-1])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef safe_cast_to_index(array: Any) -> pd.Index:\n    if isinstance(array, pd.Index):\n        index = array\n    elif hasattr(array, 'to_index'):\n        index = array.to_index()\n    else:\n        kwargs = {}\n        if hasattr(array, 'dtype') and array.dtype.kind == 'O':\n            kwargs['dtype'] = object\n        index = pd.Index(np.asarray(array), **kwargs)\n    return _maybe_cast_to_cftimeindex(index)", "response": "Given an array safely cast it to a pandas. Index."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a MultiIndex from a product with refactorizing levels.", "response": "def multiindex_from_product_levels(levels: Sequence[pd.Index],\n                                   names: Optional[Sequence[str]] = None\n                                   ) -> pd.MultiIndex:\n    \"\"\"Creating a MultiIndex from a product without refactorizing levels.\n\n    Keeping levels the same gives back the original labels when we unstack.\n\n    Parameters\n    ----------\n    levels : sequence of pd.Index\n        Values for each MultiIndex level.\n    names : optional sequence of objects\n        Names for each level.\n\n    Returns\n    -------\n    pandas.MultiIndex\n    \"\"\"\n    if any(not isinstance(lev, pd.Index) for lev in levels):\n        raise TypeError('levels must be a list of pd.Index objects')\n\n    split_labels, levels = zip(*[lev.factorize() for lev in levels])\n    labels_mesh = np.meshgrid(*split_labels, indexing='ij')\n    labels = [x.ravel() for x in labels_mesh]\n    return pd.MultiIndex(levels, labels, sortorder=0, names=names)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef maybe_wrap_array(original, new_array):\n    # in case func lost array's metadata\n    if isinstance(new_array, np.ndarray) and new_array.shape == original.shape:\n        return original.__array_wrap__(new_array)\n    else:\n        return new_array", "response": "Wrap a transformed array with __array_wrap__ is it can be done safely."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncompares two objects for equivalence ( identity or equality", "response": "def equivalent(first: T, second: T) -> bool:\n    \"\"\"Compare two objects for equivalence (identity or equality), using\n    array_equiv if either object is an ndarray\n    \"\"\"\n    # TODO: refactor to avoid circular import\n    from . import duck_array_ops\n    if isinstance(first, np.ndarray) or isinstance(second, np.ndarray):\n        return duck_array_ops.array_equiv(first, second)\n    else:\n        return ((first is second) or\n                (first == second) or\n                (pd.isnull(first) and pd.isnull(second)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef peek_at(iterable: Iterable[T]) -> Tuple[T, Iterator[T]]:\n    gen = iter(iterable)\n    peek = next(gen)\n    return peek, itertools.chain([peek], gen)", "response": "Returns the first value from iterable as well as a new iterator with\n    the same content as the original iterable"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_safety_check(first_dict: MutableMapping[K, V],\n                        second_dict: Mapping[K, V],\n                        compat: Callable[[V, V], bool] = equivalent) -> None:\n    \"\"\"Check the safety of updating one dictionary with another.\n\n    Raises ValueError if dictionaries have non-compatible values for any key,\n    where compatibility is determined by identity (they are the same item) or\n    the `compat` function.\n\n    Parameters\n    ----------\n    first_dict, second_dict : dict-like\n        All items in the second dictionary are checked against for conflicts\n        against items in the first dictionary.\n    compat : function, optional\n        Binary operator to determine if two values are compatible. By default,\n        checks for equivalence.\n    \"\"\"\n    for k, v in second_dict.items():\n        if k in first_dict and not compat(v, first_dict[k]):\n            raise ValueError('unsafe to merge dictionaries without '\n                             'overriding values; conflicting key %r' % k)", "response": "Check the safety of updating one dictionary with another."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_incompatible_items(first_dict: MutableMapping[K, V],\n                              second_dict: Mapping[K, V],\n                              compat: Callable[[V, V], bool] = equivalent\n                              ) -> None:\n    \"\"\"Remove incompatible items from the first dictionary in-place.\n\n    Items are retained if their keys are found in both dictionaries and the\n    values are compatible.\n\n    Parameters\n    ----------\n    first_dict, second_dict : dict-like\n        Mappings to merge.\n    compat : function, optional\n        Binary operator to determine if two values are compatible. By default,\n        checks for equivalence.\n    \"\"\"\n    for k in list(first_dict):\n        if k not in second_dict or not compat(first_dict[k], second_dict[k]):\n            del first_dict[k]", "response": "Removes incompatible items from the first dictionary in - place."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_scalar(value: Any) -> bool:\n    return (\n        getattr(value, 'ndim', None) == 0 or\n        isinstance(value, (str, bytes)) or not\n        isinstance(value, (Iterable, ) + dask_array_type))", "response": "Whether a value is a scalar."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_0d_object_array(value: Any) -> np.ndarray:\n    result = np.empty((), dtype=object)\n    result[()] = value\n    return result", "response": "Given a value wrap it in a 0 - D numpy. ndarray with dtype = object.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_0d_array(value: Any) -> np.ndarray:\n    if np.isscalar(value) or (isinstance(value, np.ndarray) and\n                              value.ndim == 0):\n        return np.array(value)\n    else:\n        return to_0d_object_array(value)", "response": "Given a value wrap it in a 0 - D numpy. ndarray.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntest equivalence of two dict - like objects.", "response": "def dict_equiv(first: Mapping[K, V], second: Mapping[K, V],\n               compat: Callable[[V, V], bool] = equivalent) -> bool:\n    \"\"\"Test equivalence of two dict-like objects. If any of the values are\n    numpy arrays, compare them correctly.\n\n    Parameters\n    ----------\n    first, second : dict-like\n        Dictionaries to compare for equality\n    compat : function, optional\n        Binary operator to determine if two values are compatible. By default,\n        checks for equivalence.\n\n    Returns\n    -------\n    equals : bool\n        True if the dictionaries are equal\n    \"\"\"\n    for k in first:\n        if k not in second or not compat(first[k], second[k]):\n            return False\n    for k in second:\n        if k not in first:\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the intersection of two dictionaries.", "response": "def ordered_dict_intersection(first_dict: Mapping[K, V],\n                              second_dict: Mapping[K, V],\n                              compat: Callable[[V, V], bool] = equivalent\n                              ) -> MutableMapping[K, V]:\n    \"\"\"Return the intersection of two dictionaries as a new OrderedDict.\n\n    Items are retained if their keys are found in both dictionaries and the\n    values are compatible.\n\n    Parameters\n    ----------\n    first_dict, second_dict : dict-like\n        Mappings to merge.\n    compat : function, optional\n        Binary operator to determine if two values are compatible. By default,\n        checks for equivalence.\n\n    Returns\n    -------\n    intersection : OrderedDict\n        Intersection of the contents.\n    \"\"\"\n    new_dict = OrderedDict(first_dict)\n    remove_incompatible_items(new_dict, second_dict, compat)\n    return new_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_uniform_spaced(arr, **kwargs) -> bool:\n    arr = np.array(arr, dtype=float)\n    diffs = np.diff(arr)\n    return bool(np.isclose(diffs.min(), diffs.max(), **kwargs))", "response": "Return True if values of an array are uniformly spaced and sorted."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert numpy objects to native Python objects", "response": "def decode_numpy_dict_values(attrs: Mapping[K, V]) -> Dict[K, V]:\n    \"\"\"Convert attribute values from numpy objects to native Python objects,\n    for use in to_dict\n    \"\"\"\n    attrs = dict(attrs)\n    for k, v in attrs.items():\n        if isinstance(v, np.ndarray):\n            attrs[k] = v.tolist()\n        elif isinstance(v, np.generic):\n            attrs[k] = v.item()\n    return attrs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ensure_us_time_resolution(val):\n    if np.issubdtype(val.dtype, np.datetime64):\n        val = val.astype('datetime64[us]')\n    elif np.issubdtype(val.dtype, np.timedelta64):\n        val = val.astype('timedelta64[us]')\n    return val", "response": "Convert val out of numpy time for use in to_dict.\n    Needed because of numpy bug GH#7619"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a new dimension name based on new_dim.", "response": "def get_temp_dimname(dims: Container[Hashable], new_dim: Hashable) -> Hashable:\n    \"\"\" Get an new dimension name based on new_dim, that is not used in dims.\n    If the same name exists, we add an underscore(s) in the head.\n\n    Example1:\n        dims: ['a', 'b', 'c']\n        new_dim: ['_rolling']\n        -> ['_rolling']\n    Example2:\n        dims: ['a', 'b', 'c', '_rolling']\n        new_dim: ['_rolling']\n        -> ['__rolling']\n    \"\"\"\n    while new_dim in dims:\n        new_dim = '_' + str(new_dim)\n    return new_dim"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive an object array with no missing values infer its dtype from its first element", "response": "def _infer_dtype(array, name=None):\n    \"\"\"Given an object array with no missing values, infer its dtype from its\n    first element\n    \"\"\"\n    if array.dtype.kind != 'O':\n        raise TypeError('infer_type must be called on a dtype=object array')\n\n    if array.size == 0:\n        return np.dtype(float)\n\n    element = array[(0,) * array.ndim]\n    if isinstance(element, (bytes, str)):\n        return strings.create_vlen_dtype(type(element))\n\n    dtype = np.array(element).dtype\n    if dtype.kind != 'O':\n        return dtype\n\n    raise ValueError('unable to infer dtype on variable {!r}; xarray '\n                     'cannot serialize arbitrary Python objects'\n                     .format(name))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a copy of an array with the given dtype.", "response": "def _copy_with_dtype(data, dtype):\n    \"\"\"Create a copy of an array with the given dtype.\n\n    We use this instead of np.array() to ensure that custom object dtypes end\n    up on the resulting array.\n    \"\"\"\n    result = np.empty(data.shape, dtype)\n    result[...] = data\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nencode a variable into a CF - encoded version of the variable.", "response": "def encode_cf_variable(var, needs_copy=True, name=None):\n    \"\"\"\n    Converts an Variable into an Variable which follows some\n    of the CF conventions:\n\n        - Nans are masked using _FillValue (or the deprecated missing_value)\n        - Rescaling via: scale_factor and add_offset\n        - datetimes are converted to the CF 'units since time' format\n        - dtype encodings are enforced.\n\n    Parameters\n    ----------\n    var : xarray.Variable\n        A variable holding un-encoded data.\n\n    Returns\n    -------\n    out : xarray.Variable\n        A variable which has been encoded as described above.\n    \"\"\"\n    ensure_not_multiindex(var, name=name)\n\n    for coder in [times.CFDatetimeCoder(),\n                  times.CFTimedeltaCoder(),\n                  variables.CFScaleOffsetCoder(),\n                  variables.CFMaskCoder(),\n                  variables.UnsignedIntegerCoder()]:\n        var = coder.encode(var, name=name)\n\n    # TODO(shoyer): convert all of these to use coders, too:\n    var = maybe_encode_nonstring_dtype(var, name=name)\n    var = maybe_default_fill_value(var)\n    var = maybe_encode_bools(var)\n    var = ensure_dtype_not_object(var, name=name)\n    return var"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decode_cf_variable(name, var, concat_characters=True, mask_and_scale=True,\n                       decode_times=True, decode_endianness=True,\n                       stack_char_dim=True, use_cftime=None):\n    \"\"\"\n    Decodes a variable which may hold CF encoded information.\n\n    This includes variables that have been masked and scaled, which\n    hold CF style time variables (this is almost always the case if\n    the dataset has been serialized) and which have strings encoded\n    as character arrays.\n\n    Parameters\n    ----------\n    name: str\n        Name of the variable. Used for better error messages.\n    var : Variable\n        A variable holding potentially CF encoded information.\n    concat_characters : bool\n        Should character arrays be concatenated to strings, for\n        example: ['h', 'e', 'l', 'l', 'o'] -> 'hello'\n    mask_and_scale: bool\n        Lazily scale (using scale_factor and add_offset) and mask\n        (using _FillValue). If the _Unsigned attribute is present\n        treat integer arrays as unsigned.\n    decode_times : bool\n        Decode cf times ('hours since 2000-01-01') to np.datetime64.\n    decode_endianness : bool\n        Decode arrays from non-native to native endianness.\n    stack_char_dim : bool\n        Whether to stack characters into bytes along the last dimension of this\n        array. Passed as an argument because we need to look at the full\n        dataset to figure out if this is appropriate.\n    use_cftime: bool, optional\n        Only relevant if encoded dates come from a standard calendar\n        (e.g. 'gregorian', 'proleptic_gregorian', 'standard', or not\n        specified).  If None (default), attempt to decode times to\n        ``np.datetime64[ns]`` objects; if this is not possible, decode times to\n        ``cftime.datetime`` objects. If True, always decode times to\n        ``cftime.datetime`` objects, regardless of whether or not they can be\n        represented using ``np.datetime64[ns]`` objects.  If False, always\n        decode times to ``np.datetime64[ns]`` objects; if this is not possible\n        raise an error.\n\n    Returns\n    -------\n    out : Variable\n        A variable holding the decoded equivalent of var.\n    \"\"\"\n    var = as_variable(var)\n    original_dtype = var.dtype\n\n    if concat_characters:\n        if stack_char_dim:\n            var = strings.CharacterArrayCoder().decode(var, name=name)\n        var = strings.EncodedStringCoder().decode(var)\n\n    if mask_and_scale:\n        for coder in [variables.UnsignedIntegerCoder(),\n                      variables.CFMaskCoder(),\n                      variables.CFScaleOffsetCoder()]:\n            var = coder.decode(var, name=name)\n\n    if decode_times:\n        for coder in [times.CFTimedeltaCoder(),\n                      times.CFDatetimeCoder(use_cftime=use_cftime)]:\n            var = coder.decode(var, name=name)\n\n    dimensions, data, attributes, encoding = (\n        variables.unpack_for_decoding(var))\n    # TODO(shoyer): convert everything below to use coders\n\n    if decode_endianness and not data.dtype.isnative:\n        # do this last, so it's only done if we didn't already unmask/scale\n        data = NativeEndiannessArray(data)\n        original_dtype = data.dtype\n\n    encoding.setdefault('dtype', original_dtype)\n\n    if 'dtype' in attributes and attributes['dtype'] == 'bool':\n        del attributes['dtype']\n        data = BoolTypeArray(data)\n\n    if not isinstance(data, dask_array_type):\n        data = indexing.LazilyOuterIndexedArray(data)\n\n    return Variable(dimensions, data, attributes, encoding=encoding)", "response": "Decodes a variable in CF - encoded form into a new variable in the CF - encoded form."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the time attributes of the time variables that are in the time bounds variable.", "response": "def _update_bounds_attributes(variables):\n    \"\"\"Adds time attributes to time bounds variables.\n\n    Variables handling time bounds (\"Cell boundaries\" in the CF\n    conventions) do not necessarily carry the necessary attributes to be\n    decoded. This copies the attributes from the time variable to the\n    associated boundaries.\n\n    See Also:\n\n    http://cfconventions.org/Data/cf-conventions/cf-conventions-1.7/\n         cf-conventions.html#cell-boundaries\n\n    https://github.com/pydata/xarray/issues/2565\n    \"\"\"\n\n    # For all time variables with bounds\n    for v in variables.values():\n        attrs = v.attrs\n        has_date_units = 'units' in attrs and 'since' in attrs['units']\n        if has_date_units and 'bounds' in attrs:\n            if attrs['bounds'] in variables:\n                bounds_attrs = variables[attrs['bounds']].attrs\n                bounds_attrs.setdefault('units', attrs['units'])\n                if 'calendar' in attrs:\n                    bounds_attrs.setdefault('calendar', attrs['calendar'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndecodes several CF encoded variables.", "response": "def decode_cf_variables(variables, attributes, concat_characters=True,\n                        mask_and_scale=True, decode_times=True,\n                        decode_coords=True, drop_variables=None,\n                        use_cftime=None):\n    \"\"\"\n    Decode several CF encoded variables.\n\n    See: decode_cf_variable\n    \"\"\"\n    dimensions_used_by = defaultdict(list)\n    for v in variables.values():\n        for d in v.dims:\n            dimensions_used_by[d].append(v)\n\n    def stackable(dim):\n        # figure out if a dimension can be concatenated over\n        if dim in variables:\n            return False\n        for v in dimensions_used_by[dim]:\n            if v.dtype.kind != 'S' or dim != v.dims[-1]:\n                return False\n        return True\n\n    coord_names = set()\n\n    if isinstance(drop_variables, str):\n        drop_variables = [drop_variables]\n    elif drop_variables is None:\n        drop_variables = []\n    drop_variables = set(drop_variables)\n\n    # Time bounds coordinates might miss the decoding attributes\n    if decode_times:\n        _update_bounds_attributes(variables)\n\n    new_vars = OrderedDict()\n    for k, v in variables.items():\n        if k in drop_variables:\n            continue\n        stack_char_dim = (concat_characters and v.dtype == 'S1' and\n                          v.ndim > 0 and stackable(v.dims[-1]))\n        new_vars[k] = decode_cf_variable(\n            k, v, concat_characters=concat_characters,\n            mask_and_scale=mask_and_scale, decode_times=decode_times,\n            stack_char_dim=stack_char_dim, use_cftime=use_cftime)\n        if decode_coords:\n            var_attrs = new_vars[k].attrs\n            if 'coordinates' in var_attrs:\n                coord_str = var_attrs['coordinates']\n                var_coord_names = coord_str.split()\n                if all(k in variables for k in var_coord_names):\n                    new_vars[k].encoding['coordinates'] = coord_str\n                    del var_attrs['coordinates']\n                    coord_names.update(var_coord_names)\n\n    if decode_coords and 'coordinates' in attributes:\n        attributes = OrderedDict(attributes)\n        coord_names.update(attributes.pop('coordinates').split())\n\n    return new_vars, attributes, coord_names"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndecode the given Dataset or DataStore into a new Dataset.", "response": "def decode_cf(obj, concat_characters=True, mask_and_scale=True,\n              decode_times=True, decode_coords=True, drop_variables=None,\n              use_cftime=None):\n    \"\"\"Decode the given Dataset or Datastore according to CF conventions into\n    a new Dataset.\n\n    Parameters\n    ----------\n    obj : Dataset or DataStore\n        Object to decode.\n    concat_characters : bool, optional\n        Should character arrays be concatenated to strings, for\n        example: ['h', 'e', 'l', 'l', 'o'] -> 'hello'\n    mask_and_scale: bool, optional\n        Lazily scale (using scale_factor and add_offset) and mask\n        (using _FillValue).\n    decode_times : bool, optional\n        Decode cf times (e.g., integers since 'hours since 2000-01-01') to\n        np.datetime64.\n    decode_coords : bool, optional\n        Use the 'coordinates' attribute on variable (or the dataset itself) to\n        identify coordinates.\n    drop_variables: string or iterable, optional\n        A variable or list of variables to exclude from being parsed from the\n        dataset. This may be useful to drop variables with problems or\n        inconsistent values.\n    use_cftime: bool, optional\n        Only relevant if encoded dates come from a standard calendar\n        (e.g. 'gregorian', 'proleptic_gregorian', 'standard', or not\n        specified).  If None (default), attempt to decode times to\n        ``np.datetime64[ns]`` objects; if this is not possible, decode times to\n        ``cftime.datetime`` objects. If True, always decode times to\n        ``cftime.datetime`` objects, regardless of whether or not they can be\n        represented using ``np.datetime64[ns]`` objects.  If False, always\n        decode times to ``np.datetime64[ns]`` objects; if this is not possible\n        raise an error.\n\n    Returns\n    -------\n    decoded : Dataset\n    \"\"\"\n    from .core.dataset import Dataset\n    from .backends.common import AbstractDataStore\n\n    if isinstance(obj, Dataset):\n        vars = obj._variables\n        attrs = obj.attrs\n        extra_coords = set(obj.coords)\n        file_obj = obj._file_obj\n        encoding = obj.encoding\n    elif isinstance(obj, AbstractDataStore):\n        vars, attrs = obj.load()\n        extra_coords = set()\n        file_obj = obj\n        encoding = obj.get_encoding()\n    else:\n        raise TypeError('can only decode Dataset or DataStore objects')\n\n    vars, attrs, coord_names = decode_cf_variables(\n        vars, attrs, concat_characters, mask_and_scale, decode_times,\n        decode_coords, drop_variables=drop_variables, use_cftime=use_cftime)\n    ds = Dataset(vars, attrs=attrs)\n    ds = ds.set_coords(coord_names.union(extra_coords).intersection(vars))\n    ds._file_obj = file_obj\n    ds.encoding = encoding\n\n    return ds"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cf_decoder(variables, attributes,\n               concat_characters=True, mask_and_scale=True,\n               decode_times=True):\n    \"\"\"\n    Decode a set of CF encoded variables and attributes.\n\n    See Also, decode_cf_variable\n\n    Parameters\n    ----------\n    variables : dict\n        A dictionary mapping from variable name to xarray.Variable\n    attributes : dict\n        A dictionary mapping from attribute name to value\n    concat_characters : bool\n        Should character arrays be concatenated to strings, for\n        example: ['h', 'e', 'l', 'l', 'o'] -> 'hello'\n    mask_and_scale: bool\n        Lazily scale (using scale_factor and add_offset) and mask\n        (using _FillValue).\n    decode_times : bool\n        Decode cf times ('hours since 2000-01-01') to np.datetime64.\n\n    Returns\n    -------\n    decoded_variables : dict\n        A dictionary mapping from variable name to xarray.Variable objects.\n    decoded_attributes : dict\n        A dictionary mapping from attribute name to values.\n    \"\"\"\n    variables, attributes, _ = decode_cf_variables(\n        variables, attributes, concat_characters, mask_and_scale, decode_times)\n    return variables, attributes", "response": "Decode a set of CF encoded variables and attributes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nencoding coordinates on the given dataset object into variable specific and global attributes.", "response": "def encode_dataset_coordinates(dataset):\n    \"\"\"Encode coordinates on the given dataset object into variable specific\n    and global attributes.\n\n    When possible, this is done according to CF conventions.\n\n    Parameters\n    ----------\n    dataset : Dataset\n        Object to encode.\n\n    Returns\n    -------\n    variables : dict\n    attrs : dict\n    \"\"\"\n    non_dim_coord_names = set(dataset.coords) - set(dataset.dims)\n    return _encode_coordinates(dataset._variables, dataset.attrs,\n                               non_dim_coord_names=non_dim_coord_names)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cf_encoder(variables, attributes):\n    new_vars = OrderedDict((k, encode_cf_variable(v, name=k))\n                           for k, v in variables.items())\n    return new_vars, attributes", "response": "A function which takes a dict of variables and attributes and encodes them to conform to CF conventions as much\n    as possible."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef coerce_nc3_dtype(arr):\n    dtype = str(arr.dtype)\n    if dtype in _nc3_dtype_coercions:\n        new_dtype = _nc3_dtype_coercions[dtype]\n        # TODO: raise a warning whenever casting the data-type instead?\n        cast_arr = arr.astype(new_dtype)\n        if not (cast_arr == arr).all():\n            raise ValueError('could not safely cast array from dtype %s to %s'\n                             % (dtype, new_dtype))\n        arr = cast_arr\n    return arr", "response": "Coerce an array to a data type that can be stored in a netCDF - 3 file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_valid_nc3_name(s):\n    if not isinstance(s, str):\n        return False\n    if not isinstance(s, str):\n        s = s.decode('utf-8')\n    num_bytes = len(s.encode('utf-8'))\n    return ((unicodedata.normalize('NFC', s) == s) and\n            (s not in _reserved_names) and\n            (num_bytes >= 0) and\n            ('/' not in s) and\n            (s[-1] != ' ') and\n            (_isalnumMUTF8(s[0]) or (s[0] == '_')) and\n            all((_isalnumMUTF8(c) or c in _specialchars for c in s)))", "response": "Test whether an object can be converted to a netCDF - 3 name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef as_variable(obj, name=None) -> 'Union[Variable, IndexVariable]':\n    from .dataarray import DataArray\n\n    # TODO: consider extending this method to automatically handle Iris and\n    if isinstance(obj, DataArray):\n        # extract the primary Variable from DataArrays\n        obj = obj.variable\n\n    if isinstance(obj, Variable):\n        obj = obj.copy(deep=False)\n    elif isinstance(obj, tuple):\n        try:\n            obj = Variable(*obj)\n        except (TypeError, ValueError) as error:\n            # use .format() instead of % because it handles tuples consistently\n            raise error.__class__('Could not convert tuple of form '\n                                  '(dims, data[, attrs, encoding]): '\n                                  '{} to Variable.'.format(obj))\n    elif utils.is_scalar(obj):\n        obj = Variable([], obj)\n    elif isinstance(obj, (pd.Index, IndexVariable)) and obj.name is not None:\n        obj = Variable(obj.name, obj)\n    elif isinstance(obj, (set, dict)):\n        raise TypeError(\n            \"variable %r has invalid type %r\" % (name, type(obj)))\n    elif name is not None:\n        data = as_compatible_data(obj)\n        if data.ndim != 1:\n            raise MissingDimensionsError(\n                'cannot set variable %r with %r-dimensional data '\n                'without explicit dimension names. Pass a tuple of '\n                '(dims, data) instead.' % (name, data.ndim))\n        obj = Variable(name, data, fastpath=True)\n    else:\n        raise TypeError('unable to convert object into a variable without an '\n                        'explicit list of dimensions: %r' % obj)\n\n    if name is not None and name in obj.dims:\n        # convert the Variable into an Index\n        if obj.ndim != 1:\n            raise MissingDimensionsError(\n                '%r has more than 1-dimension and the same name as one of its '\n                'dimensions %r. xarray disallows such variables because they '\n                'conflict with the coordinates used to label '\n                'dimensions.' % (name, obj.dims))\n        obj = obj.to_index_variable()\n\n    return obj", "response": "Convert an object into a Variable."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _possibly_convert_objects(values):\n    return np.asarray(pd.Series(values.ravel())).reshape(values.shape)", "response": "Convert arrays of datetime. datetime and datetime. timedelta objects into\n    datetime64 and timedelta64 according to the pandas convention."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef as_compatible_data(data, fastpath=False):\n    if fastpath and getattr(data, 'ndim', 0) > 0:\n        # can't use fastpath (yet) for scalars\n        return _maybe_wrap_data(data)\n\n    if isinstance(data, Variable):\n        return data.data\n\n    if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):\n        return _maybe_wrap_data(data)\n\n    if isinstance(data, tuple):\n        data = utils.to_0d_object_array(data)\n\n    if isinstance(data, pd.Timestamp):\n        # TODO: convert, handle datetime objects, too\n        data = np.datetime64(data.value, 'ns')\n\n    if isinstance(data, timedelta):\n        data = np.timedelta64(getattr(data, 'value', data), 'ns')\n\n    # we don't want nested self-described arrays\n    data = getattr(data, 'values', data)\n\n    if isinstance(data, np.ma.MaskedArray):\n        mask = np.ma.getmaskarray(data)\n        if mask.any():\n            dtype, fill_value = dtypes.maybe_promote(data.dtype)\n            data = np.asarray(data, dtype=dtype)\n            data[mask] = fill_value\n        else:\n            data = np.asarray(data)\n\n    # validate whether the data is valid data types\n    data = np.asarray(data)\n\n    if isinstance(data, np.ndarray):\n        if data.dtype.kind == 'O':\n            data = _possibly_convert_objects(data)\n        elif data.dtype.kind == 'M':\n            data = np.asarray(data, 'datetime64[ns]')\n        elif data.dtype.kind == 'm':\n            data = np.asarray(data, 'timedelta64[ns]')\n\n    return _maybe_wrap_data(data)", "response": "Prepare and wrap data to put in a Variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _as_array_or_item(data):\n    data = np.asarray(data)\n    if data.ndim == 0:\n        if data.dtype.kind == 'M':\n            data = np.datetime64(data, 'ns')\n        elif data.dtype.kind == 'm':\n            data = np.timedelta64(data, 'ns')\n    return data", "response": "Return the given values as a numpy array or as an individual item if the given values are 0d datetime64 or timedelta64 array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate broadcast compatible variables with the same dimensions.", "response": "def _broadcast_compat_variables(*variables):\n    \"\"\"Create broadcast compatible variables, with the same dimensions.\n\n    Unlike the result of broadcast_variables(), some variables may have\n    dimensions of size 1 instead of the the size of the broadcast dimension.\n    \"\"\"\n    dims = tuple(_unified_dims(variables))\n    return tuple(var.set_dims(dims) if var.dims != dims else var\n                 for var in variables)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives any number of variables return variables with matching dimensions and broadcast data.", "response": "def broadcast_variables(*variables):\n    \"\"\"Given any number of variables, return variables with matching dimensions\n    and broadcast data.\n\n    The data on the returned variables will be a view of the data on the\n    corresponding original arrays, but dimensions will be reordered and\n    inserted so that both broadcast arrays have the same dimensions. The new\n    dimensions are sorted in order of appearance in the first variable's\n    dimensions followed by the second variable's dimensions.\n    \"\"\"\n    dims_map = _unified_dims(variables)\n    dims_tuple = tuple(dims_map)\n    return tuple(var.set_dims(dims_map) if var.dims != dims_tuple else var\n                 for var in variables)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef concat(variables, dim='concat_dim', positions=None, shortcut=False):\n    variables = list(variables)\n    if all(isinstance(v, IndexVariable) for v in variables):\n        return IndexVariable.concat(variables, dim, positions, shortcut)\n    else:\n        return Variable.concat(variables, dim, positions, shortcut)", "response": "Concatenate a list of Variable objects along a new or existing dimension."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef assert_unique_multiindex_level_names(variables):\n    level_names = defaultdict(list)\n    all_level_names = set()\n    for var_name, var in variables.items():\n        if isinstance(var._data, PandasIndexAdapter):\n            idx_level_names = var.to_index_variable().level_names\n            if idx_level_names is not None:\n                for n in idx_level_names:\n                    level_names[n].append('%r (%s)' % (n, var_name))\n            if idx_level_names:\n                all_level_names.update(idx_level_names)\n\n    for k, v in level_names.items():\n        if k in variables:\n            v.append('(%s)' % k)\n\n    duplicate_names = [v for v in level_names.values() if len(v) > 1]\n    if duplicate_names:\n        conflict_str = '\\n'.join([', '.join(v) for v in duplicate_names])\n        raise ValueError('conflicting MultiIndex level name(s):\\n%s'\n                         % conflict_str)\n    # Check confliction between level names and dimensions GH:2299\n    for k, v in variables.items():\n        for d in v.dims:\n            if d in all_level_names:\n                raise ValueError('conflicting level / dimension names. {} '\n                                 'already exists as a level name.'.format(d))", "response": "Check for uniqueness of MultiIndex level names in all given variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_base_variable(self):\n        return Variable(self.dims, self._data, self._attrs,\n                        encoding=self._encoding, fastpath=True)", "response": "Return this variable as a base xarray. Variable"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_index_variable(self):\n        return IndexVariable(self.dims, self._data, self._attrs,\n                             encoding=self._encoding, fastpath=True)", "response": "Return this variable as an xarray. IndexVariable"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_dict(self, data=True):\n        item = {'dims': self.dims,\n                'attrs': decode_numpy_dict_values(self.attrs)}\n        if data:\n            item['data'] = ensure_us_time_resolution(self.values).tolist()\n        else:\n            item.update({'dtype': str(self.dtype), 'shape': self.shape})\n        return item", "response": "Returns a dictionary representation of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _broadcast_indexes(self, key):\n        key = self._item_key_to_tuple(key)  # key is a tuple\n        # key is a tuple of full size\n        key = indexing.expanded_indexer(key, self.ndim)\n        # Convert a scalar Variable to an integer\n        key = tuple(\n            k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k\n            for k in key)\n        # Convert a 0d-array to an integer\n        key = tuple(\n            k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k\n            for k in key)\n\n        if all(isinstance(k, BASIC_INDEXING_TYPES) for k in key):\n            return self._broadcast_indexes_basic(key)\n\n        self._validate_indexers(key)\n        # Detect it can be mapped as an outer indexer\n        # If all key is unlabeled, or\n        # key can be mapped as an OuterIndexer.\n        if all(not isinstance(k, Variable) for k in key):\n            return self._broadcast_indexes_outer(key)\n\n        # If all key is 1-dimensional and there are no duplicate labels,\n        # key can be mapped as an OuterIndexer.\n        dims = []\n        for k, d in zip(key, self.dims):\n            if isinstance(k, Variable):\n                if len(k.dims) > 1:\n                    return self._broadcast_indexes_vectorized(key)\n                dims.append(k.dims[0])\n            elif not isinstance(k, integer_types):\n                dims.append(d)\n        if len(set(dims)) == len(dims):\n            return self._broadcast_indexes_outer(key)\n\n        return self._broadcast_indexes_vectorized(key)", "response": "This method is used to broadcast the indexes of the items in the item."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake sanity checks for the indexes.", "response": "def _validate_indexers(self, key):\n        \"\"\" Make sanity checks \"\"\"\n        for dim, k in zip(self.dims, key):\n            if isinstance(k, BASIC_INDEXING_TYPES):\n                pass\n            else:\n                if not isinstance(k, Variable):\n                    k = np.asarray(k)\n                    if k.ndim > 1:\n                        raise IndexError(\n                            \"Unlabeled multi-dimensional array cannot be \"\n                            \"used for indexing: {}\".format(k))\n                if k.dtype.kind == 'b':\n                    if self.shape[self.get_axis_num(dim)] != len(k):\n                        raise IndexError(\n                            \"Boolean array size {0:d} is used to index array \"\n                            \"with shape {1:s}.\".format(len(k),\n                                                       str(self.shape)))\n                    if k.ndim > 1:\n                        raise IndexError(\"{}-dimensional boolean indexing is \"\n                                         \"not supported. \".format(k.ndim))\n                    if getattr(k, 'dims', (dim, )) != (dim, ):\n                        raise IndexError(\n                            \"Boolean indexer should be unlabeled or on the \"\n                            \"same dimension to the indexed array. Indexer is \"\n                            \"on {0:s} but the target dimension is \"\n                            \"{1:s}.\".format(str(k.dims), dim))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _finalize_indexing_result(self, dims, data):\n        return type(self)(dims, data, self._attrs, self._encoding,\n                          fastpath=True)", "response": "Used by IndexVariable to return IndexVariable objects when possible."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nindexing this Variable with - 1 remapped to fill_value.", "response": "def _getitem_with_mask(self, key, fill_value=dtypes.NA):\n        \"\"\"Index this Variable with -1 remapped to fill_value.\"\"\"\n        # TODO(shoyer): expose this method in public API somewhere (isel?) and\n        # use it for reindex.\n        # TODO(shoyer): add a sanity check that all other integers are\n        # non-negative\n        # TODO(shoyer): add an optimization, remapping -1 to an adjacent value\n        # that is actually indexed rather than mapping it to the last value\n        # along each axis.\n\n        if fill_value is dtypes.NA:\n            fill_value = dtypes.get_fill_value(self.dtype)\n\n        dims, indexer, new_order = self._broadcast_indexes(key)\n\n        if self.size:\n            if isinstance(self._data, dask_array_type):\n                # dask's indexing is faster this way; also vindex does not\n                # support negative indices yet:\n                # https://github.com/dask/dask/pull/2967\n                actual_indexer = indexing.posify_mask_indexer(indexer)\n            else:\n                actual_indexer = indexer\n\n            data = as_indexable(self._data)[actual_indexer]\n            chunks_hint = getattr(data, 'chunks', None)\n            mask = indexing.create_mask(indexer, self.shape, chunks_hint)\n            data = duck_array_ops.where(mask, fill_value, data)\n        else:\n            # array cannot be indexed along dimensions of size 0, so just\n            # build the mask directly instead.\n            mask = indexing.create_mask(indexer, self.shape)\n            data = np.broadcast_to(fill_value, getattr(mask, 'shape', ()))\n\n        if new_order:\n            data = np.moveaxis(data, range(len(new_order)), new_order)\n        return self._finalize_indexing_result(dims, data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef copy(self, deep=True, data=None):\n        if data is None:\n            data = self._data\n\n            if isinstance(data, indexing.MemoryCachedArray):\n                # don't share caching between copies\n                data = indexing.MemoryCachedArray(data.array)\n\n            if deep:\n                if isinstance(data, dask_array_type):\n                    data = data.copy()\n                elif not isinstance(data, PandasIndexAdapter):\n                    # pandas.Index is immutable\n                    data = np.array(data)\n        else:\n            data = as_compatible_data(data)\n            if self.shape != data.shape:\n                raise ValueError(\"Data shape {} must match shape of object {}\"\n                                 .format(data.shape, self.shape))\n\n        # note:\n        # dims is already an immutable tuple\n        # attributes and encoding will be copied when the new Array is created\n        return type(self)(self.dims, data, self._attrs, self._encoding,\n                          fastpath=True)", "response": "Returns a copy of this object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef chunk(self, chunks=None, name=None, lock=False):\n        import dask.array as da\n\n        if utils.is_dict_like(chunks):\n            chunks = dict((self.get_axis_num(dim), chunk)\n                          for dim, chunk in chunks.items())\n\n        if chunks is None:\n            chunks = self.chunks or self.shape\n\n        data = self._data\n        if isinstance(data, da.Array):\n            data = data.rechunk(chunks)\n        else:\n            if utils.is_dict_like(chunks):\n                chunks = tuple(chunks.get(n, s)\n                               for n, s in enumerate(self.shape))\n            # da.from_array works by using lazily indexing with a tuple of\n            # slices. Using OuterIndexer is a pragmatic choice: dask does not\n            # yet handle different indexing types in an explicit way:\n            # https://github.com/dask/dask/issues/2883\n            data = indexing.ImplicitToExplicitIndexingAdapter(\n                data, indexing.OuterIndexer)\n            data = da.from_array(data, chunks, name=name, lock=lock)\n\n        return type(self)(self.dims, data, self._attrs, self._encoding,\n                          fastpath=True)", "response": "Coerce this array s data into a dask array with the given chunks."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new array indexed along the specified dimension.", "response": "def isel(self, indexers=None, drop=False, **indexers_kwargs):\n        \"\"\"Return a new array indexed along the specified dimension(s).\n\n        Parameters\n        ----------\n        **indexers : {dim: indexer, ...}\n            Keyword arguments with names matching dimensions and values given\n            by integers, slice objects or arrays.\n\n        Returns\n        -------\n        obj : Array object\n            A new Array with the selected data and dimensions. In general,\n            the new variable's data will be a view of this variable's data,\n            unless numpy fancy indexing was triggered by using an array\n            indexer, in which case the data will be a copy.\n        \"\"\"\n        indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'isel')\n\n        invalid = [k for k in indexers if k not in self.dims]\n        if invalid:\n            raise ValueError(\"dimensions %r do not exist\" % invalid)\n\n        key = [slice(None)] * self.ndim\n        for i, dim in enumerate(self.dims):\n            if dim in indexers:\n                key[i] = indexers[dim]\n        return self[tuple(key)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new object with squeezed data.", "response": "def squeeze(self, dim=None):\n        \"\"\"Return a new object with squeezed data.\n\n        Parameters\n        ----------\n        dim : None or str or tuple of str, optional\n            Selects a subset of the length one dimensions. If a dimension is\n            selected with length greater than one, an error is raised. If\n            None, all length one dimensions are squeezed.\n\n        Returns\n        -------\n        squeezed : same type as caller\n            This object, but with with all or a subset of the dimensions of\n            length 1 removed.\n\n        See Also\n        --------\n        numpy.squeeze\n        \"\"\"\n        dims = common.get_squeeze_dims(self, dim)\n        return self.isel({d: 0 for d in dims})"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new Variable with shifted data.", "response": "def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):\n        \"\"\"\n        Return a new Variable with shifted data.\n\n        Parameters\n        ----------\n        shifts : mapping of the form {dim: offset}\n            Integer offset to shift along each of the given dimensions.\n            Positive offsets shift to the right; negative offsets shift to the\n            left.\n        fill_value: scalar, optional\n            Value to use for newly missing values\n        **shifts_kwargs:\n            The keyword arguments form of ``shifts``.\n            One of shifts or shifts_kwarg must be provided.\n\n        Returns\n        -------\n        shifted : Variable\n            Variable with the same dimensions and attributes but shifted data.\n        \"\"\"\n        shifts = either_dict_or_kwargs(shifts, shifts_kwargs, 'shift')\n        result = self\n        for dim, count in shifts.items():\n            result = result._shift_one_dim(dim, count, fill_value=fill_value)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a new Variable with paddings padded to the edges of each dimension.", "response": "def pad_with_fill_value(self, pad_widths=None, fill_value=dtypes.NA,\n                            **pad_widths_kwargs):\n        \"\"\"\n        Return a new Variable with paddings.\n\n        Parameters\n        ----------\n        pad_width: Mapping of the form {dim: (before, after)}\n            Number of values padded to the edges of each dimension.\n        **pad_widths_kwargs:\n            Keyword argument for pad_widths\n        \"\"\"\n        pad_widths = either_dict_or_kwargs(pad_widths, pad_widths_kwargs,\n                                           'pad')\n\n        if fill_value is dtypes.NA:\n            dtype, fill_value = dtypes.maybe_promote(self.dtype)\n        else:\n            dtype = self.dtype\n\n        if isinstance(self.data, dask_array_type):\n            array = self.data\n\n            # Dask does not yet support pad. We manually implement it.\n            # https://github.com/dask/dask/issues/1926\n            for d, pad in pad_widths.items():\n                axis = self.get_axis_num(d)\n                before_shape = list(array.shape)\n                before_shape[axis] = pad[0]\n                before_chunks = list(array.chunks)\n                before_chunks[axis] = (pad[0], )\n                after_shape = list(array.shape)\n                after_shape[axis] = pad[1]\n                after_chunks = list(array.chunks)\n                after_chunks[axis] = (pad[1], )\n\n                arrays = []\n                if pad[0] > 0:\n                    arrays.append(da.full(before_shape, fill_value,\n                                          dtype=dtype, chunks=before_chunks))\n                arrays.append(array)\n                if pad[1] > 0:\n                    arrays.append(da.full(after_shape, fill_value,\n                                          dtype=dtype, chunks=after_chunks))\n                if len(arrays) > 1:\n                    array = da.concatenate(arrays, axis=axis)\n        else:\n            pads = [(0, 0) if d not in pad_widths else pad_widths[d]\n                    for d in self.dims]\n            array = np.pad(self.data.astype(dtype, copy=False), pads,\n                           mode='constant', constant_values=fill_value)\n        return type(self)(self.dims, array)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef roll(self, shifts=None, **shifts_kwargs):\n        shifts = either_dict_or_kwargs(shifts, shifts_kwargs, 'roll')\n\n        result = self\n        for dim, count in shifts.items():\n            result = result._roll_one_dim(dim, count)\n        return result", "response": "Return a new Variable with rolld data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef transpose(self, *dims) -> 'Variable':\n        if len(dims) == 0:\n            dims = self.dims[::-1]\n        axes = self.get_axis_num(dims)\n        if len(dims) < 2:  # no need to transpose if only one dimension\n            return self.copy(deep=False)\n\n        data = as_indexable(self._data).transpose(axes)\n        return type(self)(dims, data, self._attrs, self._encoding,\n                          fastpath=True)", "response": "Return a new Variable object with transposed dimensions."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a new Variable with given set of dimensions.", "response": "def set_dims(self, dims, shape=None):\n        \"\"\"Return a new variable with given set of dimensions.\n        This method might be used to attach new dimension(s) to variable.\n\n        When possible, this operation does not copy this variable's data.\n\n        Parameters\n        ----------\n        dims : str or sequence of str or dict\n            Dimensions to include on the new variable. If a dict, values are\n            used to provide the sizes of new dimensions; otherwise, new\n            dimensions are inserted with length 1.\n\n        Returns\n        -------\n        Variable\n        \"\"\"\n        if isinstance(dims, str):\n            dims = [dims]\n\n        if shape is None and utils.is_dict_like(dims):\n            shape = dims.values()\n\n        missing_dims = set(self.dims) - set(dims)\n        if missing_dims:\n            raise ValueError('new dimensions %r must be a superset of '\n                             'existing dimensions %r' % (dims, self.dims))\n\n        self_dims = set(self.dims)\n        expanded_dims = tuple(\n            d for d in dims if d not in self_dims) + self.dims\n\n        if self.dims == expanded_dims:\n            # don't use broadcast_to unless necessary so the result remains\n            # writeable if possible\n            expanded_data = self.data\n        elif shape is not None:\n            dims_map = dict(zip(dims, shape))\n            tmp_shape = tuple(dims_map[d] for d in expanded_dims)\n            expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)\n        else:\n            expanded_data = self.data[\n                (None,) * (len(expanded_dims) - self.ndim)]\n\n        expanded_var = Variable(expanded_dims, expanded_data, self._attrs,\n                                self._encoding, fastpath=True)\n        return expanded_var.transpose(*dims)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstacks any number of existing dimensions into a single new dimension.", "response": "def stack(self, dimensions=None, **dimensions_kwargs):\n        \"\"\"\n        Stack any number of existing dimensions into a single new dimension.\n\n        New dimensions will be added at the end, and the order of the data\n        along each new dimension will be in contiguous (C) order.\n\n        Parameters\n        ----------\n        dimensions : Mapping of form new_name=(dim1, dim2, ...)\n            Names of new dimensions, and the existing dimensions that they\n            replace.\n        **dimensions_kwargs:\n            The keyword arguments form of ``dimensions``.\n            One of dimensions or dimensions_kwargs must be provided.\n\n        Returns\n        -------\n        stacked : Variable\n            Variable with the same attributes but stacked data.\n\n        See also\n        --------\n        Variable.unstack\n        \"\"\"\n        dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs,\n                                           'stack')\n        result = self\n        for new_dim, dims in dimensions.items():\n            result = result._stack_once(dims, new_dim)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreducing this array by applying func along some dimension.", "response": "def reduce(self, func, dim=None, axis=None,\n               keep_attrs=None, allow_lazy=False, **kwargs):\n        \"\"\"Reduce this array by applying `func` along some dimension(s).\n\n        Parameters\n        ----------\n        func : function\n            Function which can be called in the form\n            `func(x, axis=axis, **kwargs)` to return the result of reducing an\n            np.ndarray over an integer valued axis.\n        dim : str or sequence of str, optional\n            Dimension(s) over which to apply `func`.\n        axis : int or sequence of int, optional\n            Axis(es) over which to apply `func`. Only one of the 'dim'\n            and 'axis' arguments can be supplied. If neither are supplied, then\n            the reduction is calculated over the flattened array (by calling\n            `func(x)` without an axis argument).\n        keep_attrs : bool, optional\n            If True, the variable's attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n        **kwargs : dict\n            Additional keyword arguments passed on to `func`.\n\n        Returns\n        -------\n        reduced : Array\n            Array with summarized data and the indicated dimension(s)\n            removed.\n        \"\"\"\n        if dim is common.ALL_DIMS:\n            dim = None\n        if dim is not None and axis is not None:\n            raise ValueError(\"cannot supply both 'axis' and 'dim' arguments\")\n\n        if dim is not None:\n            axis = self.get_axis_num(dim)\n        input_data = self.data if allow_lazy else self.values\n        if axis is not None:\n            data = func(input_data, axis=axis, **kwargs)\n        else:\n            data = func(input_data, **kwargs)\n\n        if getattr(data, 'shape', ()) == self.shape:\n            dims = self.dims\n        else:\n            removed_axes = (range(self.ndim) if axis is None\n                            else np.atleast_1d(axis) % self.ndim)\n            dims = [adim for n, adim in enumerate(self.dims)\n                    if n not in removed_axes]\n\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n        attrs = self._attrs if keep_attrs else None\n\n        return Variable(dims, data, attrs=attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef concat(cls, variables, dim='concat_dim', positions=None,\n               shortcut=False):\n        \"\"\"Concatenate variables along a new or existing dimension.\n\n        Parameters\n        ----------\n        variables : iterable of Array\n            Arrays to stack together. Each variable is expected to have\n            matching dimensions and shape except for along the stacked\n            dimension.\n        dim : str or DataArray, optional\n            Name of the dimension to stack along. This can either be a new\n            dimension name, in which case it is added along axis=0, or an\n            existing dimension name, in which case the location of the\n            dimension is unchanged. Where to insert the new dimension is\n            determined by the first variable.\n        positions : None or list of integer arrays, optional\n            List of integer arrays which specifies the integer positions to\n            which to assign each dataset along the concatenated dimension.\n            If not supplied, objects are concatenated in the provided order.\n        shortcut : bool, optional\n            This option is used internally to speed-up groupby operations.\n            If `shortcut` is True, some checks of internal consistency between\n            arrays to concatenate are skipped.\n\n        Returns\n        -------\n        stacked : Variable\n            Concatenated Variable formed by stacking all the supplied variables\n            along the given dimension.\n        \"\"\"\n        if not isinstance(dim, str):\n            dim, = dim.dims\n\n        # can't do this lazily: we need to loop through variables at least\n        # twice\n        variables = list(variables)\n        first_var = variables[0]\n\n        arrays = [v.data for v in variables]\n\n        if dim in first_var.dims:\n            axis = first_var.get_axis_num(dim)\n            dims = first_var.dims\n            data = duck_array_ops.concatenate(arrays, axis=axis)\n            if positions is not None:\n                # TODO: deprecate this option -- we don't need it for groupby\n                # any more.\n                indices = nputils.inverse_permutation(\n                    np.concatenate(positions))\n                data = duck_array_ops.take(data, indices, axis=axis)\n        else:\n            axis = 0\n            dims = (dim,) + first_var.dims\n            data = duck_array_ops.stack(arrays, axis=axis)\n\n        attrs = OrderedDict(first_var.attrs)\n        encoding = OrderedDict(first_var.encoding)\n        if not shortcut:\n            for var in variables:\n                if var.dims != first_var.dims:\n                    raise ValueError('inconsistent dimensions')\n                utils.remove_incompatible_items(attrs, var.attrs)\n\n        return cls(dims, data, attrs, encoding)", "response": "Concatenate a list of ArrayLogEntry objects along a new or existing dimension."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntrue if two Variables have the same dimensions and values ; otherwise False.", "response": "def equals(self, other, equiv=duck_array_ops.array_equiv):\n        \"\"\"True if two Variables have the same dimensions and values;\n        otherwise False.\n\n        Variables can still be equal (like pandas objects) if they have NaN\n        values in the same locations.\n\n        This method is necessary because `v1 == v2` for Variables\n        does element-wise comparisons (like numpy.ndarrays).\n        \"\"\"\n        other = getattr(other, 'variable', other)\n        try:\n            return (self.dims == other.dims and\n                    (self._data is other._data or\n                     equiv(self.data, other.data)))\n        except (TypeError, AttributeError):\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef broadcast_equals(self, other, equiv=duck_array_ops.array_equiv):\n        try:\n            self, other = broadcast_variables(self, other)\n        except (ValueError, AttributeError):\n            return False\n        return self.equals(other, equiv=equiv)", "response": "True if two Variables have the values after being broadcast against each other."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlikes equals but also checks attributes.", "response": "def identical(self, other):\n        \"\"\"Like equals, but also checks attributes.\n        \"\"\"\n        try:\n            return (utils.dict_equiv(self.attrs, other.attrs) and\n                    self.equals(other))\n        except (TypeError, AttributeError):\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the qth quantile of the data along the specified dimension.", "response": "def quantile(self, q, dim=None, interpolation='linear'):\n        \"\"\"Compute the qth quantile of the data along the specified dimension.\n\n        Returns the qth quantiles(s) of the array elements.\n\n        Parameters\n        ----------\n        q : float in range of [0,1] (or sequence of floats)\n            Quantile to compute, which must be between 0 and 1\n            inclusive.\n        dim : str or sequence of str, optional\n            Dimension(s) over which to apply quantile.\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n            This optional parameter specifies the interpolation method to\n            use when the desired quantile lies between two data points\n            ``i < j``:\n                * linear: ``i + (j - i) * fraction``, where ``fraction`` is\n                  the fractional part of the index surrounded by ``i`` and\n                  ``j``.\n                * lower: ``i``.\n                * higher: ``j``.\n                * nearest: ``i`` or ``j``, whichever is nearest.\n                * midpoint: ``(i + j) / 2``.\n\n        Returns\n        -------\n        quantiles : Variable\n            If `q` is a single quantile, then the result\n            is a scalar. If multiple percentiles are given, first axis of\n            the result corresponds to the quantile and a quantile dimension\n            is added to the return array. The other dimensions are the\n             dimensions that remain after the reduction of the array.\n\n        See Also\n        --------\n        numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile,\n        DataArray.quantile\n        \"\"\"\n        if isinstance(self.data, dask_array_type):\n            raise TypeError(\"quantile does not work for arrays stored as dask \"\n                            \"arrays. Load the data via .compute() or .load() \"\n                            \"prior to calling this method.\")\n\n        q = np.asarray(q, dtype=np.float64)\n\n        new_dims = list(self.dims)\n        if dim is not None:\n            axis = self.get_axis_num(dim)\n            if utils.is_scalar(dim):\n                new_dims.remove(dim)\n            else:\n                for d in dim:\n                    new_dims.remove(d)\n        else:\n            axis = None\n            new_dims = []\n\n        # only add the quantile dimension if q is array like\n        if q.ndim != 0:\n            new_dims = ['quantile'] + new_dims\n\n        qs = np.nanpercentile(self.data, q * 100., axis=axis,\n                              interpolation=interpolation)\n        return Variable(new_dims, qs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rank(self, dim, pct=False):\n        import bottleneck as bn\n\n        if isinstance(self.data, dask_array_type):\n            raise TypeError(\"rank does not work for arrays stored as dask \"\n                            \"arrays. Load the data via .compute() or .load() \"\n                            \"prior to calling this method.\")\n\n        axis = self.get_axis_num(dim)\n        func = bn.nanrankdata if self.dtype.kind == 'f' else bn.rankdata\n        ranked = func(self.data, axis=axis)\n        if pct:\n            count = np.sum(~np.isnan(self.data), axis=axis, keepdims=True)\n            ranked /= count\n        return Variable(self.dims, ranked)", "response": "Ranks the data.\n\n        Equal values are assigned a rank that is the average of the ranks that\n        would have been otherwise assigned to all of the values within that\n        set.  Ranks begin at 1, not 0. If `pct`, computes percentage ranks.\n\n        NaNs in the input array are returned as NaNs.\n\n        The `bottleneck` library is required.\n\n        Parameters\n        ----------\n        dim : str\n            Dimension over which to compute rank.\n        pct : bool, optional\n            If True, compute percentage ranks, otherwise compute integer ranks.\n\n        Returns\n        -------\n        ranked : Variable\n\n        See Also\n        --------\n        Dataset.rank, DataArray.rank"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rolling_window(self, dim, window, window_dim, center=False,\n                       fill_value=dtypes.NA):\n        \"\"\"\n        Make a rolling_window along dim and add a new_dim to the last place.\n\n        Parameters\n        ----------\n        dim: str\n            Dimension over which to compute rolling_window\n        window: int\n            Window size of the rolling\n        window_dim: str\n            New name of the window dimension.\n        center: boolean. default False.\n            If True, pad fill_value for both ends. Otherwise, pad in the head\n            of the axis.\n        fill_value:\n            value to be filled.\n\n        Returns\n        -------\n        Variable that is a view of the original array with a added dimension of\n        size w.\n        The return dim: self.dims + (window_dim, )\n        The return shape: self.shape + (window, )\n\n        Examples\n        --------\n        >>> v=Variable(('a', 'b'), np.arange(8).reshape((2,4)))\n        >>> v.rolling_window(x, 'b', 3, 'window_dim')\n        <xarray.Variable (a: 2, b: 4, window_dim: 3)>\n        array([[[nan, nan, 0], [nan, 0, 1], [0, 1, 2], [1, 2, 3]],\n               [[nan, nan, 4], [nan, 4, 5], [4, 5, 6], [5, 6, 7]]])\n\n        >>> v.rolling_window(x, 'b', 3, 'window_dim', center=True)\n        <xarray.Variable (a: 2, b: 4, window_dim: 3)>\n        array([[[nan, 0, 1], [0, 1, 2], [1, 2, 3], [2, 3, nan]],\n               [[nan, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, nan]]])\n        \"\"\"\n        if fill_value is dtypes.NA:  # np.nan is passed\n            dtype, fill_value = dtypes.maybe_promote(self.dtype)\n            array = self.astype(dtype, copy=False).data\n        else:\n            dtype = self.dtype\n            array = self.data\n\n        new_dims = self.dims + (window_dim, )\n        return Variable(new_dims, duck_array_ops.rolling_window(\n            array, axis=self.get_axis_num(dim), window=window,\n            center=center, fill_value=fill_value))", "response": "This function creates a rolling_window of the array along dim and adds a new_dim to the last place."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef coarsen(self, windows, func, boundary='exact', side='left'):\n        windows = {k: v for k, v in windows.items() if k in self.dims}\n        if not windows:\n            return self.copy()\n\n        reshaped, axes = self._coarsen_reshape(windows, boundary, side)\n        if isinstance(func, str):\n            name = func\n            func = getattr(duck_array_ops, name, None)\n            if func is None:\n                raise NameError('{} is not a valid method.'.format(name))\n        return type(self)(self.dims, func(reshaped, axis=axes), self._attrs)", "response": "Return a new array with coarsened data."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconstructs a reshaped array for corsen .", "response": "def _coarsen_reshape(self, windows, boundary, side):\n        \"\"\"\n        Construct a reshaped-array for corsen\n        \"\"\"\n        if not utils.is_dict_like(boundary):\n            boundary = {d: boundary for d in windows.keys()}\n\n        if not utils.is_dict_like(side):\n            side = {d: side for d in windows.keys()}\n\n        # remove unrelated dimensions\n        boundary = {k: v for k, v in boundary.items() if k in windows}\n        side = {k: v for k, v in side.items() if k in windows}\n\n        for d, window in windows.items():\n            if window <= 0:\n                raise ValueError('window must be > 0. Given {}'.format(window))\n\n        variable = self\n        for d, window in windows.items():\n            # trim or pad the object\n            size = variable.shape[self._get_axis_num(d)]\n            n = int(size / window)\n            if boundary[d] == 'exact':\n                if n * window != size:\n                    raise ValueError(\n                        'Could not coarsen a dimension of size {} with '\n                        'window {}'.format(size, window))\n            elif boundary[d] == 'trim':\n                if side[d] == 'left':\n                    variable = variable.isel({d: slice(0, window * n)})\n                else:\n                    excess = size - window * n\n                    variable = variable.isel({d: slice(excess, None)})\n            elif boundary[d] == 'pad':  # pad\n                pad = window * n - size\n                if pad < 0:\n                    pad += window\n                if side[d] == 'left':\n                    pad_widths = {d: (0, pad)}\n                else:\n                    pad_widths = {d: (pad, 0)}\n                variable = variable.pad_with_fill_value(pad_widths)\n            else:\n                raise TypeError(\n                    \"{} is invalid for boundary. Valid option is 'exact', \"\n                    \"'trim' and 'pad'\".format(boundary[d]))\n\n        shape = []\n        axes = []\n        axis_count = 0\n        for i, d in enumerate(variable.dims):\n            if d in windows:\n                size = variable.shape[i]\n                shape.append(int(size / windows[d]))\n                shape.append(windows[d])\n                axis_count += 1\n                axes.append(i + axis_count)\n            else:\n                shape.append(variable.shape[i])\n\n        return variable.data.reshape(shape), tuple(axes)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _to_numeric(self, offset=None, datetime_unit=None, dtype=float):\n        numeric_array = duck_array_ops.datetime_to_numeric(\n            self.data, offset, datetime_unit, dtype)\n        return type(self)(self.dims, numeric_array, self._attrs)", "response": "Convert the object to numeric."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nspecialize version of Variable. concat for IndexVariable objects.", "response": "def concat(cls, variables, dim='concat_dim', positions=None,\n               shortcut=False):\n        \"\"\"Specialized version of Variable.concat for IndexVariable objects.\n\n        This exists because we want to avoid converting Index objects to NumPy\n        arrays, if possible.\n        \"\"\"\n        if not isinstance(dim, str):\n            dim, = dim.dims\n\n        variables = list(variables)\n        first_var = variables[0]\n\n        if any(not isinstance(v, cls) for v in variables):\n            raise TypeError('IndexVariable.concat requires that all input '\n                            'variables be IndexVariable objects')\n\n        indexes = [v._data.array for v in variables]\n\n        if not indexes:\n            data = []\n        else:\n            data = indexes[0].append(indexes[1:])\n\n            if positions is not None:\n                indices = nputils.inverse_permutation(\n                    np.concatenate(positions))\n                data = data.take(indices)\n\n        attrs = OrderedDict(first_var.attrs)\n        if not shortcut:\n            for var in variables:\n                if var.dims != first_var.dims:\n                    raise ValueError('inconsistent dimensions')\n                utils.remove_incompatible_items(attrs, var.attrs)\n\n        return cls(first_var.dims, data, attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a copy of this VariableReservedEntry object.", "response": "def copy(self, deep=True, data=None):\n        \"\"\"Returns a copy of this object.\n\n        `deep` is ignored since data is stored in the form of\n        pandas.Index, which is already immutable. Dimensions, attributes\n        and encodings are always copied.\n\n        Use `data` to create a new object with the same structure as\n        original but entirely new data.\n\n        Parameters\n        ----------\n        deep : bool, optional\n            Deep is always ignored.\n        data : array_like, optional\n            Data to use in the new object. Must have same shape as original.\n\n        Returns\n        -------\n        object : Variable\n            New object with dimensions, attributes, encodings, and optionally\n            data copied from original.\n        \"\"\"\n        if data is None:\n            data = self._data\n        else:\n            data = as_compatible_data(data)\n            if self.shape != data.shape:\n                raise ValueError(\"Data shape {} must match shape of object {}\"\n                                 .format(data.shape, self.shape))\n        return type(self)(self.dims, data, self._attrs,\n                          self._encoding, fastpath=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts this variable to a pandas. Index.", "response": "def to_index(self):\n        \"\"\"Convert this variable to a pandas.Index\"\"\"\n        # n.b. creating a new pandas.Index from an old pandas.Index is\n        # basically free as pandas.Index objects are immutable\n        assert self.ndim == 1\n        index = self._data.array\n        if isinstance(index, pd.MultiIndex):\n            # set default names for multi-index unnamed levels so that\n            # we can safely rename dimension / coordinate later\n            valid_level_names = [name or '{}_level_{}'.format(self.dims[0], i)\n                                 for i, name in enumerate(index.names)]\n            index = index.set_names(valid_level_names)\n        else:\n            index = index.set_names(self.name)\n        return index"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef level_names(self):\n        index = self.to_index()\n        if isinstance(index, pd.MultiIndex):\n            return index.names\n        else:\n            return None", "response": "Return the level names of this IndexVariable or None if this IndexVariable has no\n        MultiIndex."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_level_variable(self, level):\n        if self.level_names is None:\n            raise ValueError(\"IndexVariable %r has no MultiIndex\" % self.name)\n        index = self.to_index()\n        return type(self)(self.dims, index.get_level_values(level))", "response": "Return a new IndexVariable from a given MultiIndex level."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parsed_string_to_bounds(date_type, resolution, parsed):\n    if resolution == 'year':\n        return (date_type(parsed.year, 1, 1),\n                date_type(parsed.year + 1, 1, 1) - timedelta(microseconds=1))\n    elif resolution == 'month':\n        if parsed.month == 12:\n            end = date_type(parsed.year + 1, 1, 1) - timedelta(microseconds=1)\n        else:\n            end = (date_type(parsed.year, parsed.month + 1, 1) -\n                   timedelta(microseconds=1))\n        return date_type(parsed.year, parsed.month, 1), end\n    elif resolution == 'day':\n        start = date_type(parsed.year, parsed.month, parsed.day)\n        return start, start + timedelta(days=1, microseconds=-1)\n    elif resolution == 'hour':\n        start = date_type(parsed.year, parsed.month, parsed.day, parsed.hour)\n        return start, start + timedelta(hours=1, microseconds=-1)\n    elif resolution == 'minute':\n        start = date_type(parsed.year, parsed.month, parsed.day, parsed.hour,\n                          parsed.minute)\n        return start, start + timedelta(minutes=1, microseconds=-1)\n    elif resolution == 'second':\n        start = date_type(parsed.year, parsed.month, parsed.day, parsed.hour,\n                          parsed.minute, parsed.second)\n        return start, start + timedelta(seconds=1, microseconds=-1)\n    else:\n        raise KeyError", "response": "Generalization of pandas. tseries. index. DatetimeIndex. _parsed_string_to_bounds\n    for use with non - standard calendars and cftime. datetime\n    objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_date_field(datetimes, field):\n    return np.array([getattr(date, field) for date in datetimes])", "response": "Adapted from pandas. tslib. get_date_field"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a property that can be used to access the given field.", "response": "def _field_accessor(name, docstring=None, min_cftime_version='0.0'):\n    \"\"\"Adapted from pandas.tseries.index._field_accessor\"\"\"\n\n    def f(self, min_cftime_version=min_cftime_version):\n        import cftime\n\n        version = cftime.__version__\n\n        if LooseVersion(version) >= LooseVersion(min_cftime_version):\n            return get_date_field(self._data, name)\n        else:\n            raise ImportError('The {!r} accessor requires a minimum '\n                              'version of cftime of {}. Found an '\n                              'installed version of {}.'.format(\n                                  name, min_cftime_version, version))\n\n    f.__name__ = name\n    f.__doc__ = docstring\n    return property(f)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a numpy array from an array of strings.", "response": "def _parse_array_of_cftime_strings(strings, date_type):\n    \"\"\"Create a numpy array from an array of strings.\n\n    For use in generating dates from strings for use with interp.  Assumes the\n    array is either 0-dimensional or 1-dimensional.\n\n    Parameters\n    ----------\n    strings : array of strings\n        Strings to convert to dates\n    date_type : cftime.datetime type\n        Calendar type to use for dates\n\n    Returns\n    -------\n    np.array\n    \"\"\"\n    return np.array([_parse_iso8601_without_reso(date_type, s)\n                     for s in strings.ravel()]).reshape(strings.shape)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _partial_date_slice(self, resolution, parsed):\n        start, end = _parsed_string_to_bounds(self.date_type, resolution,\n                                              parsed)\n\n        times = self._data\n\n        if self.is_monotonic:\n            if (len(times) and ((start < times[0] and end < times[0]) or\n                                (start > times[-1] and end > times[-1]))):\n                # we are out of range\n                raise KeyError\n\n            # a monotonic (sorted) series can be sliced\n            left = times.searchsorted(start, side='left')\n            right = times.searchsorted(end, side='right')\n            return slice(left, right)\n\n        lhs_mask = times >= start\n        rhs_mask = times <= end\n        return np.flatnonzero(lhs_mask & rhs_mask)", "response": "Adapted from pandas. tseries. index. DatetimeIndex. _partial_date_slice\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadapts from pandas. tseries. index. DatetimeIndex. _get_string_slice", "response": "def _get_string_slice(self, key):\n        \"\"\"Adapted from pandas.tseries.index.DatetimeIndex._get_string_slice\"\"\"\n        parsed, resolution = _parse_iso8601_with_reso(self.date_type, key)\n        try:\n            loc = self._partial_date_slice(resolution, parsed)\n        except KeyError:\n            raise KeyError(key)\n        return loc"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_loc(self, key, method=None, tolerance=None):\n        if isinstance(key, str):\n            return self._get_string_slice(key)\n        else:\n            return pd.Index.get_loc(self, key, method=method,\n                                    tolerance=tolerance)", "response": "Adapted from pandas. tseries. index. DatetimeIndex. get_loc"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadapt from pandas. tseries. index. DatetimeIndex. _maybe_cast_slice_bound", "response": "def _maybe_cast_slice_bound(self, label, side, kind):\n        \"\"\"Adapted from\n        pandas.tseries.index.DatetimeIndex._maybe_cast_slice_bound\"\"\"\n        if isinstance(label, str):\n            parsed, resolution = _parse_iso8601_with_reso(self.date_type,\n                                                          label)\n            start, end = _parsed_string_to_bounds(self.date_type, resolution,\n                                                  parsed)\n            if self.is_monotonic_decreasing and len(self) > 1:\n                return end if side == 'left' else start\n            return start if side == 'left' else end\n        else:\n            return label"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_value(self, series, key):\n        if np.asarray(key).dtype == np.dtype(bool):\n            return series.iloc[key]\n        elif isinstance(key, slice):\n            return series.iloc[self.slice_indexer(\n                key.start, key.stop, key.step)]\n        else:\n            return series.iloc[self.get_loc(key)]", "response": "Adapted from pandas. tseries. index. DatetimeIndex. get_value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nshift the CFTimeIndex a multiple of the given frequency.", "response": "def shift(self, n, freq):\n        \"\"\"Shift the CFTimeIndex a multiple of the given frequency.\n\n        See the documentation for :py:func:`~xarray.cftime_range` for a\n        complete listing of valid frequency strings.\n\n        Parameters\n        ----------\n        n : int\n            Periods to shift by\n        freq : str or datetime.timedelta\n            A frequency string or datetime.timedelta object to shift by\n\n        Returns\n        -------\n        CFTimeIndex\n\n        See also\n        --------\n        pandas.DatetimeIndex.shift\n\n        Examples\n        --------\n        >>> index = xr.cftime_range('2000', periods=1, freq='M')\n        >>> index\n        CFTimeIndex([2000-01-31 00:00:00], dtype='object')\n        >>> index.shift(1, 'M')\n        CFTimeIndex([2000-02-29 00:00:00], dtype='object')\n        \"\"\"\n        from .cftime_offsets import to_offset\n\n        if not isinstance(n, int):\n            raise TypeError(\"'n' must be an int, got {}.\".format(n))\n        if isinstance(freq, timedelta):\n            return self + n * freq\n        elif isinstance(freq, str):\n            return self + n * to_offset(freq)\n        else:\n            raise TypeError(\n                \"'freq' must be of type \"\n                \"str or datetime.timedelta, got {}.\".format(freq))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_datetimeindex(self, unsafe=False):\n        nptimes = cftime_to_nptime(self)\n        calendar = infer_calendar_name(self)\n        if calendar not in _STANDARD_CALENDARS and not unsafe:\n            warnings.warn(\n                'Converting a CFTimeIndex with dates from a non-standard '\n                'calendar, {!r}, to a pandas.DatetimeIndex, which uses dates '\n                'from the standard calendar.  This may lead to subtle errors '\n                'in operations that depend on the length of time between '\n                'dates.'.format(calendar), RuntimeWarning, stacklevel=2)\n        return pd.DatetimeIndex(nptimes)", "response": "Convert this index to a pandas. DatetimeIndex."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a function that dispatches to dask for dask array inputs.", "response": "def _dask_or_eager_func(name, eager_module=np, dask_module=dask_array,\n                        list_of_args=False, array_args=slice(1),\n                        requires_dask=None):\n    \"\"\"Create a function that dispatches to dask for dask array inputs.\"\"\"\n    if dask_module is not None:\n        def f(*args, **kwargs):\n            if list_of_args:\n                dispatch_args = args[0]\n            else:\n                dispatch_args = args[array_args]\n            if any(isinstance(a, dask_array.Array) for a in dispatch_args):\n                try:\n                    wrapped = getattr(dask_module, name)\n                except AttributeError as e:\n                    raise AttributeError(\"%s: requires dask >=%s\" %\n                                         (e, requires_dask))\n            else:\n                wrapped = getattr(eager_module, name)\n            return wrapped(*args, **kwargs)\n    else:\n        def f(*args, **kwargs):\n            return getattr(eager_module, name)(*args, **kwargs)\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef as_shared_dtype(scalars_or_arrays):\n    arrays = [asarray(x) for x in scalars_or_arrays]\n    # Pass arrays directly instead of dtypes to result_type so scalars\n    # get handled properly.\n    # Note that result_type() safely gets the dtype from dask arrays without\n    # evaluating them.\n    out_type = dtypes.result_type(*arrays)\n    return [x.astype(out_type, copy=False) for x in arrays]", "response": "Cast a list of scalars to a shared dtype using xarray s type promotion rules."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef allclose_or_equiv(arr1, arr2, rtol=1e-5, atol=1e-8):\n    arr1, arr2 = as_like_arrays(arr1, arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    return bool(\n        isclose(arr1, arr2, rtol=rtol, atol=atol, equal_nan=True).all())", "response": "Like np. allclose but also allows values to be NaN in both arrays\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef array_equiv(arr1, arr2):\n    arr1, arr2 = as_like_arrays(arr1, arr2)\n    if arr1.shape != arr2.shape:\n        return False\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', \"In the future, 'NAT == x'\")\n\n        flag_array = (arr1 == arr2)\n        flag_array |= (isnull(arr1) & isnull(arr2))\n\n        return bool(flag_array.all())", "response": "Like np. array_equal but also allows values to be NaN in both arrays\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncounting the number of non - NA in this array along the given axis or axes.", "response": "def count(data, axis=None):\n    \"\"\"Count the number of non-NA in this array along the given axis or axes\n    \"\"\"\n    return np.sum(np.logical_not(isnull(data)), axis=axis)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef datetime_to_numeric(array, offset=None, datetime_unit=None, dtype=float):\n    # TODO: make this function dask-compatible?\n    if offset is None:\n        offset = array.min()\n    array = array - offset\n\n    if not hasattr(array, 'dtype'):  # scalar is converted to 0d-array\n        array = np.array(array)\n\n    if array.dtype.kind in 'O':\n        # possibly convert object array containing datetime.timedelta\n        array = np.asarray(pd.Series(array.ravel())).reshape(array.shape)\n\n    if datetime_unit:\n        array = array / np.timedelta64(1, datetime_unit)\n\n    # convert np.NaT to np.nan\n    if array.dtype.kind in 'mM':\n        return np.where(isnull(array), np.nan, array.astype(dtype))\n    return array.astype(dtype)", "response": "Convert an array containing datetime - like data to an array of floats."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the first non - NA elements in this array along the given axis", "response": "def first(values, axis, skipna=None):\n    \"\"\"Return the first non-NA elements in this array along the given axis\n    \"\"\"\n    if (skipna or skipna is None) and values.dtype.kind not in 'iSU':\n        # only bother for dtypes that can hold NaN\n        _fail_on_dask_array_input_skipna(values)\n        return nanfirst(values, axis)\n    return take(values, 0, axis=axis)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef last(values, axis, skipna=None):\n    if (skipna or skipna is None) and values.dtype.kind not in 'iSU':\n        # only bother for dtypes that can hold NaN\n        _fail_on_dask_array_input_skipna(values)\n        return nanlast(values, axis)\n    return take(values, -1, axis=axis)", "response": "Return the last non - NA elements in this array along the given axis"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rolling_window(array, axis, window, center, fill_value):\n    if isinstance(array, dask_array_type):\n        return dask_array_ops.rolling_window(\n            array, axis, window, center, fill_value)\n    else:  # np.ndarray\n        return nputils.rolling_window(\n            array, axis, window, center, fill_value)", "response": "Make an ndarray with a rolling window of axis - th dimension."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef concat(objs, dim=None, data_vars='all', coords='different',\n           compat='equals', positions=None, indexers=None, mode=None,\n           concat_over=None):\n    \"\"\"Concatenate xarray objects along a new or existing dimension.\n\n    Parameters\n    ----------\n    objs : sequence of Dataset and DataArray objects\n        xarray objects to concatenate together. Each object is expected to\n        consist of variables and coordinates with matching shapes except for\n        along the concatenated dimension.\n    dim : str or DataArray or pandas.Index\n        Name of the dimension to concatenate along. This can either be a new\n        dimension name, in which case it is added along axis=0, or an existing\n        dimension name, in which case the location of the dimension is\n        unchanged. If dimension is provided as a DataArray or Index, its name\n        is used as the dimension to concatenate along and the values are added\n        as a coordinate.\n    data_vars : {'minimal', 'different', 'all' or list of str}, optional\n        These data variables will be concatenated together:\n          * 'minimal': Only data variables in which the dimension already\n            appears are included.\n          * 'different': Data variables which are not equal (ignoring\n            attributes) across all datasets are also concatenated (as well as\n            all for which dimension already appears). Beware: this option may\n            load the data payload of data variables into memory if they are not\n            already loaded.\n          * 'all': All data variables will be concatenated.\n          * list of str: The listed data variables will be concatenated, in\n            addition to the 'minimal' data variables.\n        If objects are DataArrays, data_vars must be 'all'.\n    coords : {'minimal', 'different', 'all' or list of str}, optional\n        These coordinate variables will be concatenated together:\n          * 'minimal': Only coordinates in which the dimension already appears\n            are included.\n          * 'different': Coordinates which are not equal (ignoring attributes)\n            across all datasets are also concatenated (as well as all for which\n            dimension already appears). Beware: this option may load the data\n            payload of coordinate variables into memory if they are not already\n            loaded.\n          * 'all': All coordinate variables will be concatenated, except\n            those corresponding to other dimensions.\n          * list of str: The listed coordinate variables will be concatenated,\n            in addition the 'minimal' coordinates.\n    compat : {'equals', 'identical'}, optional\n        String indicating how to compare non-concatenated variables and\n        dataset global attributes for potential conflicts. 'equals' means\n        that all variable values and dimensions must be the same;\n        'identical' means that variable attributes and global attributes\n        must also be equal.\n    positions : None or list of integer arrays, optional\n        List of integer arrays which specifies the integer positions to which\n        to assign each dataset along the concatenated dimension. If not\n        supplied, objects are concatenated in the provided order.\n    indexers, mode, concat_over : deprecated\n\n    Returns\n    -------\n    concatenated : type of objs\n\n    See also\n    --------\n    merge\n    auto_combine\n    \"\"\"\n    # TODO: add join and ignore_index arguments copied from pandas.concat\n    # TODO: support concatenating scalar coordinates even if the concatenated\n    # dimension already exists\n    from .dataset import Dataset\n    from .dataarray import DataArray\n\n    try:\n        first_obj, objs = utils.peek_at(objs)\n    except StopIteration:\n        raise ValueError('must supply at least one object to concatenate')\n\n    if dim is None:\n        warnings.warn('the `dim` argument to `concat` will be required '\n                      'in a future version of xarray; for now, setting it to '\n                      \"the old default of 'concat_dim'\",\n                      FutureWarning, stacklevel=2)\n        dim = 'concat_dims'\n\n    if indexers is not None:  # pragma: nocover\n        warnings.warn('indexers has been renamed to positions; the alias '\n                      'will be removed in a future version of xarray',\n                      FutureWarning, stacklevel=2)\n        positions = indexers\n\n    if mode is not None:\n        raise ValueError('`mode` is no longer a valid argument to '\n                         'xarray.concat; it has been split into the '\n                         '`data_vars` and `coords` arguments')\n    if concat_over is not None:\n        raise ValueError('`concat_over` is no longer a valid argument to '\n                         'xarray.concat; it has been split into the '\n                         '`data_vars` and `coords` arguments')\n\n    if isinstance(first_obj, DataArray):\n        f = _dataarray_concat\n    elif isinstance(first_obj, Dataset):\n        f = _dataset_concat\n    else:\n        raise TypeError('can only concatenate xarray Dataset and DataArray '\n                        'objects, got %s' % type(first_obj))\n    return f(objs, dim, data_vars, coords, compat, positions)", "response": "Concatenate two or more xarray objects along a new or existing dimension."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the name and 1d coordinate variable for concatenating along the new dimension.", "response": "def _calc_concat_dim_coord(dim):\n    \"\"\"\n    Infer the dimension name and 1d coordinate variable (if appropriate)\n    for concatenating along the new dimension.\n    \"\"\"\n    from .dataarray import DataArray\n\n    if isinstance(dim, str):\n        coord = None\n    elif not isinstance(dim, (DataArray, Variable)):\n        dim_name = getattr(dim, 'name', None)\n        if dim_name is None:\n            dim_name = 'concat_dim'\n        coord = IndexVariable(dim_name, dim)\n        dim = dim_name\n    elif not isinstance(dim, DataArray):\n        coord = as_variable(dim).to_index_variable()\n        dim, = coord.dims\n    else:\n        coord = dim\n        dim, = coord.dims\n    return dim, coord"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _calc_concat_over(datasets, dim, data_vars, coords):\n    # Return values\n    concat_over = set()\n    equals = {}\n\n    if dim in datasets[0]:\n        concat_over.add(dim)\n    for ds in datasets:\n        concat_over.update(k for k, v in ds.variables.items()\n                           if dim in v.dims)\n\n    def process_subset_opt(opt, subset):\n        if isinstance(opt, str):\n            if opt == 'different':\n                # all nonindexes that are not the same in each dataset\n                for k in getattr(datasets[0], subset):\n                    if k not in concat_over:\n                        # Compare the variable of all datasets vs. the one\n                        # of the first dataset. Perform the minimum amount of\n                        # loads in order to avoid multiple loads from disk\n                        # while keeping the RAM footprint low.\n                        v_lhs = datasets[0].variables[k].load()\n                        # We'll need to know later on if variables are equal.\n                        computed = []\n                        for ds_rhs in datasets[1:]:\n                            v_rhs = ds_rhs.variables[k].compute()\n                            computed.append(v_rhs)\n                            if not v_lhs.equals(v_rhs):\n                                concat_over.add(k)\n                                equals[k] = False\n                                # computed variables are not to be re-computed\n                                # again in the future\n                                for ds, v in zip(datasets[1:], computed):\n                                    ds.variables[k].data = v.data\n                                break\n                        else:\n                            equals[k] = True\n\n            elif opt == 'all':\n                concat_over.update(set(getattr(datasets[0], subset)) -\n                                   set(datasets[0].dims))\n            elif opt == 'minimal':\n                pass\n            else:\n                raise ValueError(\"unexpected value for %s: %s\" % (subset, opt))\n        else:\n            invalid_vars = [k for k in opt\n                            if k not in getattr(datasets[0], subset)]\n            if invalid_vars:\n                if subset == 'coords':\n                    raise ValueError(\n                        'some variables in coords are not coordinates on '\n                        'the first dataset: %s' % (invalid_vars,))\n                else:\n                    raise ValueError(\n                        'some variables in data_vars are not data variables '\n                        'on the first dataset: %s' % (invalid_vars,))\n            concat_over.update(opt)\n\n    process_subset_opt(data_vars, 'data_vars')\n    process_subset_opt(coords, 'coords')\n    return concat_over, equals", "response": "Calculates which dataset variables need to be concatenated in the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _dataset_concat(datasets, dim, data_vars, coords, compat, positions):\n    from .dataset import Dataset\n\n    if compat not in ['equals', 'identical']:\n        raise ValueError(\"compat=%r invalid: must be 'equals' \"\n                         \"or 'identical'\" % compat)\n\n    dim, coord = _calc_concat_dim_coord(dim)\n    # Make sure we're working on a copy (we'll be loading variables)\n    datasets = [ds.copy() for ds in datasets]\n    datasets = align(*datasets, join='outer', copy=False, exclude=[dim])\n\n    concat_over, equals = _calc_concat_over(datasets, dim, data_vars, coords)\n\n    def insert_result_variable(k, v):\n        assert isinstance(v, Variable)\n        if k in datasets[0].coords:\n            result_coord_names.add(k)\n        result_vars[k] = v\n\n    # create the new dataset and add constant variables\n    result_vars = OrderedDict()\n    result_coord_names = set(datasets[0].coords)\n    result_attrs = datasets[0].attrs\n    result_encoding = datasets[0].encoding\n\n    for k, v in datasets[0].variables.items():\n        if k not in concat_over:\n            insert_result_variable(k, v)\n\n    # check that global attributes and non-concatenated variables are fixed\n    # across all datasets\n    for ds in datasets[1:]:\n        if (compat == 'identical' and\n                not utils.dict_equiv(ds.attrs, result_attrs)):\n            raise ValueError('dataset global attributes not equal')\n        for k, v in ds.variables.items():\n            if k not in result_vars and k not in concat_over:\n                raise ValueError('encountered unexpected variable %r' % k)\n            elif (k in result_coord_names) != (k in ds.coords):\n                raise ValueError('%r is a coordinate in some datasets but not '\n                                 'others' % k)\n            elif k in result_vars and k != dim:\n                # Don't use Variable.identical as it internally invokes\n                # Variable.equals, and we may already know the answer\n                if compat == 'identical' and not utils.dict_equiv(\n                        v.attrs, result_vars[k].attrs):\n                    raise ValueError(\n                        'variable %s not identical across datasets' % k)\n\n                # Proceed with equals()\n                try:\n                    # May be populated when using the \"different\" method\n                    is_equal = equals[k]\n                except KeyError:\n                    result_vars[k].load()\n                    is_equal = v.equals(result_vars[k])\n                if not is_equal:\n                    raise ValueError(\n                        'variable %s not equal across datasets' % k)\n\n    # we've already verified everything is consistent; now, calculate\n    # shared dimension sizes so we can expand the necessary variables\n    dim_lengths = [ds.dims.get(dim, 1) for ds in datasets]\n    non_concat_dims = {}\n    for ds in datasets:\n        non_concat_dims.update(ds.dims)\n    non_concat_dims.pop(dim, None)\n\n    def ensure_common_dims(vars):\n        # ensure each variable with the given name shares the same\n        # dimensions and the same shape for all of them except along the\n        # concat dimension\n        common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n        if dim not in common_dims:\n            common_dims = (dim,) + common_dims\n        for var, dim_len in zip(vars, dim_lengths):\n            if var.dims != common_dims:\n                common_shape = tuple(non_concat_dims.get(d, dim_len)\n                                     for d in common_dims)\n                var = var.set_dims(common_dims, common_shape)\n            yield var\n\n    # stack up each variable to fill-out the dataset (in order)\n    for k in datasets[0].variables:\n        if k in concat_over:\n            vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n            combined = concat_vars(vars, dim, positions)\n            insert_result_variable(k, combined)\n\n    result = Dataset(result_vars, attrs=result_attrs)\n    result = result.set_coords(result_coord_names)\n    result.encoding = result_encoding\n\n    if coord is not None:\n        # add concat dimension last to ensure that its in the final Dataset\n        result[coord.name] = coord\n\n    return result", "response": "Concatenate a sequence of datasets along a new or existing dimension."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _infer_tile_ids_from_nested_list(entry, current_pos):\n\n    if isinstance(entry, list):\n        for i, item in enumerate(entry):\n            for result in _infer_tile_ids_from_nested_list(item,\n                                                           current_pos + (i,)):\n                yield result\n    else:\n        yield current_pos, entry", "response": "A generator function that yields tuples containing the tile IDs of each object in the nested list."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _combine_nd(combined_ids, concat_dims, data_vars='all',\n                coords='different', compat='no_conflicts'):\n    \"\"\"\n    Concatenates and merges an N-dimensional structure of datasets.\n\n    No checks are performed on the consistency of the datasets, concat_dims or\n    tile_IDs, because it is assumed that this has already been done.\n\n    Parameters\n    ----------\n    combined_ids : Dict[Tuple[int, ...]], xarray.Dataset]\n        Structure containing all datasets to be concatenated with \"tile_IDs\" as\n        keys, which specify position within the desired final combined result.\n    concat_dims : sequence of str\n        The dimensions along which the datasets should be concatenated. Must be\n        in order, and the length must match\n\n    Returns\n    -------\n    combined_ds : xarray.Dataset\n    \"\"\"\n\n    # Perform N-D dimensional concatenation\n    # Each iteration of this loop reduces the length of the tile_ids tuples\n    # by one. It always combines along the first dimension, removing the first\n    # element of the tuple\n    for concat_dim in concat_dims:\n        combined_ids = _auto_combine_all_along_first_dim(combined_ids,\n                                                         dim=concat_dim,\n                                                         data_vars=data_vars,\n                                                         coords=coords,\n                                                         compat=compat)\n    combined_ds = list(combined_ids.values())[0]\n    return combined_ds", "response": "Concatenates a set of datasets into a single N - dimensional structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _auto_combine(datasets, concat_dims, compat, data_vars, coords,\n                  infer_order_from_coords, ids):\n    \"\"\"\n    Calls logic to decide concatenation order before concatenating.\n    \"\"\"\n\n    # Arrange datasets for concatenation\n    if infer_order_from_coords:\n        raise NotImplementedError\n        # TODO Use coordinates to determine tile_ID for each dataset in N-D\n        # Ignore how they were ordered previously\n        # Should look like:\n        # combined_ids, concat_dims = _infer_tile_ids_from_coords(datasets,\n        # concat_dims)\n    else:\n        # Use information from the shape of the user input\n        if not ids:\n            # Determine tile_IDs by structure of input in N-D\n            # (i.e. ordering in list-of-lists)\n            combined_ids, concat_dims = _infer_concat_order_from_positions(\n                datasets, concat_dims)\n        else:\n            # Already sorted so just use the ids already passed\n            combined_ids = OrderedDict(zip(ids, datasets))\n\n    # Check that the inferred shape is combinable\n    _check_shape_tile_ids(combined_ids)\n\n    # Repeatedly concatenate then merge along each dimension\n    combined = _combine_nd(combined_ids, concat_dims, compat=compat,\n                           data_vars=data_vars, coords=coords)\n    return combined", "response": "Combines the datasets into a single N - D tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef auto_combine(datasets, concat_dim=_CONCAT_DIM_DEFAULT,\n                 compat='no_conflicts', data_vars='all', coords='different'):\n    \"\"\"Attempt to auto-magically combine the given datasets into one.\n    This method attempts to combine a list of datasets into a single entity by\n    inspecting metadata and using a combination of concat and merge.\n    It does not concatenate along more than one dimension or sort data under\n    any circumstances. It does align coordinates, but different variables on\n    datasets can cause it to fail under some scenarios. In complex cases, you\n    may need to clean up your data and use ``concat``/``merge`` explicitly.\n    ``auto_combine`` works well if you have N years of data and M data\n    variables, and each combination of a distinct time period and set of data\n    variables is saved its own dataset.\n\n    Parameters\n    ----------\n    datasets : sequence of xarray.Dataset\n        Dataset objects to merge.\n    concat_dim : str or DataArray or Index, optional\n        Dimension along which to concatenate variables, as used by\n        :py:func:`xarray.concat`. You only need to provide this argument if\n        the dimension along which you want to concatenate is not a dimension\n        in the original datasets, e.g., if you want to stack a collection of\n        2D arrays along a third dimension.\n        By default, xarray attempts to infer this argument by examining\n        component files. Set ``concat_dim=None`` explicitly to disable\n        concatenation.\n    compat : {'identical', 'equals', 'broadcast_equals', 'no_conflicts'}, optional\n        String indicating how to compare variables of the same name for\n        potential conflicts:\n\n        - 'broadcast_equals': all values must be equal when variables are\n          broadcast against each other to ensure common dimensions.\n        - 'equals': all values and dimensions must be the same.\n        - 'identical': all values, dimensions and attributes must be the\n          same.\n        - 'no_conflicts': only values which are not null in both datasets\n          must be equal. The returned dataset then contains the combination\n          of all non-null values.\n    data_vars : {'minimal', 'different', 'all' or list of str}, optional\n        Details are in the documentation of concat\n    coords : {'minimal', 'different', 'all' or list of str}, optional\n        Details are in the documentation of conca\n\n    Returns\n    -------\n    combined : xarray.Dataset\n\n    See also\n    --------\n    concat\n    Dataset.merge\n    \"\"\"  # noqa\n\n    # Coerce 1D input into ND to maintain backwards-compatible API until API\n    # for N-D combine decided\n    # (see https://github.com/pydata/xarray/pull/2553/#issuecomment-445892746)\n    if concat_dim is None or concat_dim == _CONCAT_DIM_DEFAULT:\n        concat_dims = concat_dim\n    elif not isinstance(concat_dim, list):\n        concat_dims = [concat_dim]\n    else:\n        concat_dims = concat_dim\n    infer_order_from_coords = False\n\n    # The IDs argument tells _auto_combine that the datasets are not yet sorted\n    return _auto_combine(datasets, concat_dims=concat_dims, compat=compat,\n                         data_vars=data_vars, coords=coords,\n                         infer_order_from_coords=infer_order_from_coords,\n                         ids=False)", "response": "This method attempts to auto - magically combine the given datasets into one."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_date_type(calendar):\n    try:\n        import cftime\n    except ImportError:\n        raise ImportError(\n            'cftime is required for dates with non-standard calendars')\n    else:\n        calendars = {\n            'noleap': cftime.DatetimeNoLeap,\n            '360_day': cftime.Datetime360Day,\n            '365_day': cftime.DatetimeNoLeap,\n            '366_day': cftime.DatetimeAllLeap,\n            'gregorian': cftime.DatetimeGregorian,\n            'proleptic_gregorian': cftime.DatetimeProlepticGregorian,\n            'julian': cftime.DatetimeJulian,\n            'all_leap': cftime.DatetimeAllLeap,\n            'standard': cftime.DatetimeGregorian\n        }\n        return calendars[calendar]", "response": "Return the cftime date type for a given calendar name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the day in the month that satisfies a BaseCFTimeOffset s onOffset policy.", "response": "def _get_day_of_month(other, day_option):\n    \"\"\"Find the day in `other`'s month that satisfies a BaseCFTimeOffset's\n    onOffset policy, as described by the `day_option` argument.\n\n    Parameters\n    ----------\n    other : cftime.datetime\n    day_option : 'start', 'end'\n        'start': returns 1\n        'end': returns last day of the month\n\n    Returns\n    -------\n    day_of_month : int\n\n    \"\"\"\n\n    if day_option == 'start':\n        return 1\n    elif day_option == 'end':\n        days_in_month = _days_in_month(other)\n        return days_in_month\n    elif day_option is None:\n        # Note: unlike `_shift_month`, _get_day_of_month does not\n        # allow day_option = None\n        raise NotImplementedError\n    else:\n        raise ValueError(day_option)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _days_in_month(date):\n    if date.month == 12:\n        reference = type(date)(date.year + 1, 1, 1)\n    else:\n        reference = type(date)(date.year, date.month + 1, 1)\n    return (reference - timedelta(days=1)).day", "response": "The number of days in the month of the given date"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _adjust_n_months(other_day, n, reference_day):\n    if n > 0 and other_day < reference_day:\n        n = n - 1\n    elif n <= 0 and other_day > reference_day:\n        n = n + 1\n    return n", "response": "Adjust the number of times a monthly offset is applied based\n    on the day of a given date."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadjusts the number of times an annual offset is applied based on another date and the reference day provided", "response": "def _adjust_n_years(other, n, month, reference_day):\n    \"\"\"Adjust the number of times an annual offset is applied based on\n    another date, and the reference day provided\"\"\"\n    if n > 0:\n        if other.month < month or (other.month == month and\n                                   other.day < reference_day):\n            n -= 1\n    else:\n        if other.month > month or (other.month == month and\n                                   other.day > reference_day):\n            n += 1\n    return n"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _shift_month(date, months, day_option='start'):\n    delta_year = (date.month + months) // 12\n    month = (date.month + months) % 12\n\n    if month == 0:\n        month = 12\n        delta_year = delta_year - 1\n    year = date.year + delta_year\n\n    if day_option == 'start':\n        day = 1\n    elif day_option == 'end':\n        reference = type(date)(year, month, 1)\n        day = _days_in_month(reference)\n    else:\n        raise ValueError(day_option)\n    # dayofwk=-1 is required to update the dayofwk and dayofyr attributes of\n    # the returned date object in versions of cftime between 1.0.2 and\n    # 1.0.3.4.  It can be removed for versions of cftime greater than\n    # 1.0.3.4.\n    return date.replace(year=year, month=month, day=day, dayofwk=-1)", "response": "Shift the date to a given number of months away."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef roll_qtrday(other, n, month, day_option, modby=3):\n\n    months_since = other.month % modby - month % modby\n\n    if n > 0:\n        if months_since < 0 or (\n                months_since == 0 and\n                other.day < _get_day_of_month(other, day_option)):\n            # pretend to roll back if on same month but\n            # before compare_day\n            n -= 1\n    else:\n        if months_since > 0 or (\n                months_since == 0 and\n                other.day > _get_day_of_month(other, day_option)):\n            # make sure to roll forward, so negate\n            n += 1\n    return n", "response": "Roll the number of periods to a given day in a given month."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_offset(freq):\n    if isinstance(freq, BaseCFTimeOffset):\n        return freq\n    else:\n        try:\n            freq_data = re.match(_PATTERN, freq).groupdict()\n        except AttributeError:\n            raise ValueError('Invalid frequency string provided')\n\n    freq = freq_data['freq']\n    multiples = freq_data['multiple']\n    if multiples is None:\n        multiples = 1\n    else:\n        multiples = int(multiples)\n\n    return _FREQUENCIES[freq](n=multiples)", "response": "Convert a frequency string to the appropriate subclass of BaseCFTimeOffset."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating an equally - spaced sequence of cftime. datetime objects between start and end.", "response": "def _generate_linear_range(start, end, periods):\n    \"\"\"Generate an equally-spaced sequence of cftime.datetime objects between\n    and including two dates (whose length equals the number of periods).\"\"\"\n    import cftime\n\n    total_seconds = (end - start).total_seconds()\n    values = np.linspace(0., total_seconds, periods, endpoint=True)\n    units = 'seconds since {}'.format(format_cftime_datetime(start))\n    calendar = start.calendar\n    return cftime.num2date(values, units=units, calendar=calendar,\n                           only_use_cftime_datetimes=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _generate_range(start, end, periods, offset):\n    if start:\n        start = offset.rollforward(start)\n\n    if end:\n        end = offset.rollback(end)\n\n    if periods is None and end < start:\n        end = None\n        periods = 0\n\n    if end is None:\n        end = start + (periods - 1) * offset\n\n    if start is None:\n        start = end - (periods - 1) * offset\n\n    current = start\n    if offset.n >= 0:\n        while current <= end:\n            yield current\n\n            next_date = current + offset\n            if next_date <= current:\n                raise ValueError('Offset {offset} did not increment date'\n                                 .format(offset=offset))\n            current = next_date\n    else:\n        while current >= end:\n            yield current\n\n            next_date = current + offset\n            if next_date >= current:\n                raise ValueError('Offset {offset} did not decrement date'\n                                 .format(offset=offset))\n            current = next_date", "response": "Generate a regular range of cftime. datetime objects with a specific time offset."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a fixed frequency CFTimeIndex for the given start and end dates.", "response": "def cftime_range(start=None, end=None, periods=None, freq='D',\n                 normalize=False, name=None, closed=None,\n                 calendar='standard'):\n    \"\"\"Return a fixed frequency CFTimeIndex.\n\n    Parameters\n    ----------\n    start : str or cftime.datetime, optional\n        Left bound for generating dates.\n    end : str or cftime.datetime, optional\n        Right bound for generating dates.\n    periods : integer, optional\n        Number of periods to generate.\n    freq : str, default 'D', BaseCFTimeOffset, or None\n       Frequency strings can have multiples, e.g. '5H'.\n    normalize : bool, default False\n        Normalize start/end dates to midnight before generating date range.\n    name : str, default None\n        Name of the resulting index\n    closed : {None, 'left', 'right'}, optional\n        Make the interval closed with respect to the given frequency to the\n        'left', 'right', or both sides (None, the default).\n    calendar : str\n        Calendar type for the datetimes (default 'standard').\n\n    Returns\n    -------\n    CFTimeIndex\n\n    Notes\n    -----\n\n    This function is an analog of ``pandas.date_range`` for use in generating\n    sequences of ``cftime.datetime`` objects.  It supports most of the\n    features of ``pandas.date_range`` (e.g. specifying how the index is\n    ``closed`` on either side, or whether or not to ``normalize`` the start and\n    end bounds); however, there are some notable exceptions:\n\n    - You cannot specify a ``tz`` (time zone) argument.\n    - Start or end dates specified as partial-datetime strings must use the\n      `ISO-8601 format <https://en.wikipedia.org/wiki/ISO_8601>`_.\n    - It supports many, but not all, frequencies supported by\n      ``pandas.date_range``.  For example it does not currently support any of\n      the business-related, semi-monthly, or sub-second frequencies.\n    - Compound sub-monthly frequencies are not supported, e.g. '1H1min', as\n      these can easily be written in terms of the finest common resolution,\n      e.g. '61min'.\n\n    Valid simple frequency strings for use with ``cftime``-calendars include\n    any multiples of the following.\n\n    +--------+--------------------------+\n    | Alias  | Description              |\n    +========+==========================+\n    | A, Y   | Year-end frequency       |\n    +--------+--------------------------+\n    | AS, YS | Year-start frequency     |\n    +--------+--------------------------+\n    | Q      | Quarter-end frequency    |\n    +--------+--------------------------+\n    | QS     | Quarter-start frequency  |\n    +--------+--------------------------+\n    | M      | Month-end frequency      |\n    +--------+--------------------------+\n    | MS     | Month-start frequency    |\n    +--------+--------------------------+\n    | D      | Day frequency            |\n    +--------+--------------------------+\n    | H      | Hour frequency           |\n    +--------+--------------------------+\n    | T, min | Minute frequency         |\n    +--------+--------------------------+\n    | S      | Second frequency         |\n    +--------+--------------------------+\n\n    Any multiples of the following anchored offsets are also supported.\n\n    +----------+--------------------------------------------------------------------+\n    | Alias    | Description                                                        |\n    +==========+====================================================================+\n    | A(S)-JAN | Annual frequency, anchored at the end (or beginning) of January    |\n    +----------+--------------------------------------------------------------------+\n    | A(S)-FEB | Annual frequency, anchored at the end (or beginning) of February   |\n    +----------+--------------------------------------------------------------------+\n    | A(S)-MAR | Annual frequency, anchored at the end (or beginning) of March      |\n    +----------+--------------------------------------------------------------------+\n    | A(S)-APR | Annual frequency, anchored at the end (or beginning) of April      |\n    +----------+--------------------------------------------------------------------+\n    | A(S)-MAY | Annual frequency, anchored at the end (or beginning) of May        |\n    +----------+--------------------------------------------------------------------+\n    | A(S)-JUN | Annual frequency, anchored at the end (or beginning) of June       |\n    +----------+--------------------------------------------------------------------+\n    | A(S)-JUL | Annual frequency, anchored at the end (or beginning) of July       |\n    +----------+--------------------------------------------------------------------+\n    | A(S)-AUG | Annual frequency, anchored at the end (or beginning) of August     |\n    +----------+--------------------------------------------------------------------+\n    | A(S)-SEP | Annual frequency, anchored at the end (or beginning) of September  |\n    +----------+--------------------------------------------------------------------+\n    | A(S)-OCT | Annual frequency, anchored at the end (or beginning) of October    |\n    +----------+--------------------------------------------------------------------+\n    | A(S)-NOV | Annual frequency, anchored at the end (or beginning) of November   |\n    +----------+--------------------------------------------------------------------+\n    | A(S)-DEC | Annual frequency, anchored at the end (or beginning) of December   |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-JAN | Quarter frequency, anchored at the end (or beginning) of January   |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-FEB | Quarter frequency, anchored at the end (or beginning) of February  |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-MAR | Quarter frequency, anchored at the end (or beginning) of March     |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-APR | Quarter frequency, anchored at the end (or beginning) of April     |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-MAY | Quarter frequency, anchored at the end (or beginning) of May       |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-JUN | Quarter frequency, anchored at the end (or beginning) of June      |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-JUL | Quarter frequency, anchored at the end (or beginning) of July      |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-AUG | Quarter frequency, anchored at the end (or beginning) of August    |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-SEP | Quarter frequency, anchored at the end (or beginning) of September |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-OCT | Quarter frequency, anchored at the end (or beginning) of October   |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-NOV | Quarter frequency, anchored at the end (or beginning) of November  |\n    +----------+--------------------------------------------------------------------+\n    | Q(S)-DEC | Quarter frequency, anchored at the end (or beginning) of December  |\n    +----------+--------------------------------------------------------------------+\n\n\n    Finally, the following calendar aliases are supported.\n\n    +--------------------------------+---------------------------------------+\n    | Alias                          | Date type                             |\n    +================================+=======================================+\n    | standard, gregorian            | ``cftime.DatetimeGregorian``          |\n    +--------------------------------+---------------------------------------+\n    | proleptic_gregorian            | ``cftime.DatetimeProlepticGregorian`` |\n    +--------------------------------+---------------------------------------+\n    | noleap, 365_day                | ``cftime.DatetimeNoLeap``             |\n    +--------------------------------+---------------------------------------+\n    | all_leap, 366_day              | ``cftime.DatetimeAllLeap``            |\n    +--------------------------------+---------------------------------------+\n    | 360_day                        | ``cftime.Datetime360Day``             |\n    +--------------------------------+---------------------------------------+\n    | julian                         | ``cftime.DatetimeJulian``             |\n    +--------------------------------+---------------------------------------+\n\n    Examples\n    --------\n\n    This function returns a ``CFTimeIndex``, populated with ``cftime.datetime``\n    objects associated with the specified calendar type, e.g.\n\n    >>> xr.cftime_range(start='2000', periods=6, freq='2MS', calendar='noleap')\n    CFTimeIndex([2000-01-01 00:00:00, 2000-03-01 00:00:00, 2000-05-01 00:00:00,\n                 2000-07-01 00:00:00, 2000-09-01 00:00:00, 2000-11-01 00:00:00],\n                dtype='object')\n\n    As in the standard pandas function, three of the ``start``, ``end``,\n    ``periods``, or ``freq`` arguments must be specified at a given time, with\n    the other set to ``None``.  See the `pandas documentation\n    <https://pandas.pydata.org/pandas-docs/stable/generated/pandas.date_range.html#pandas.date_range>`_\n    for more examples of the behavior of ``date_range`` with each of the\n    parameters.\n\n    See Also\n    --------\n    pandas.date_range\n    \"\"\"  # noqa: E501\n    # Adapted from pandas.core.indexes.datetimes._generate_range.\n    if _count_not_none(start, end, periods, freq) != 3:\n        raise ValueError(\n            \"Of the arguments 'start', 'end', 'periods', and 'freq', three \"\n            \"must be specified at a time.\")\n\n    if start is not None:\n        start = to_cftime_datetime(start, calendar)\n        start = _maybe_normalize_date(start, normalize)\n    if end is not None:\n        end = to_cftime_datetime(end, calendar)\n        end = _maybe_normalize_date(end, normalize)\n\n    if freq is None:\n        dates = _generate_linear_range(start, end, periods)\n    else:\n        offset = to_offset(freq)\n        dates = np.array(list(_generate_range(start, end, periods, offset)))\n\n    left_closed = False\n    right_closed = False\n\n    if closed is None:\n        left_closed = True\n        right_closed = True\n    elif closed == 'left':\n        left_closed = True\n    elif closed == 'right':\n        right_closed = True\n    else:\n        raise ValueError(\"Closed must be either 'left', 'right' or None\")\n\n    if (not left_closed and len(dates) and\n            start is not None and dates[0] == start):\n        dates = dates[1:]\n    if (not right_closed and len(dates) and\n            end is not None and dates[-1] == end):\n        dates = dates[:-1]\n\n    return CFTimeIndex(dates, name=name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the given date is in the set of possible dates created using a length - one version of this offset class.", "response": "def onOffset(self, date):\n        \"\"\"Check if the given date is in the set of possible dates created\n        using a length-one version of this offset class.\"\"\"\n        mod_month = (date.month - self.month) % 3\n        return mod_month == 0 and date.day == self._get_offset_day(date)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrolls date forward to nearest start of quarter", "response": "def rollforward(self, date):\n        \"\"\"Roll date forward to nearest start of quarter\"\"\"\n        if self.onOffset(date):\n            return date\n        else:\n            return date + QuarterBegin(month=self.month)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rollback(self, date):\n        if self.onOffset(date):\n            return date\n        else:\n            return date - QuarterBegin(month=self.month)", "response": "Roll date backward to nearest start of quarter"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rollforward(self, date):\n        if self.onOffset(date):\n            return date\n        else:\n            return date + QuarterEnd(month=self.month)", "response": "Roll date forward to nearest end of quarter"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nroll date backward to nearest end of quarter", "response": "def rollback(self, date):\n        \"\"\"Roll date backward to nearest end of quarter\"\"\"\n        if self.onOffset(date):\n            return date\n        else:\n            return date - QuarterEnd(month=self.month)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rollforward(self, date):\n        if self.onOffset(date):\n            return date\n        else:\n            return date + YearBegin(month=self.month)", "response": "Roll date forward to nearest start of year"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rollback(self, date):\n        if self.onOffset(date):\n            return date\n        else:\n            return date - YearBegin(month=self.month)", "response": "Roll date backward to nearest start of year"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the given date is in the set of possible dates created using a length - one version of this offset class.", "response": "def onOffset(self, date):\n        \"\"\"Check if the given date is in the set of possible dates created\n        using a length-one version of this offset class.\"\"\"\n        return date.day == _days_in_month(date) and date.month == self.month"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrolling date forward to nearest end of year", "response": "def rollforward(self, date):\n        \"\"\"Roll date forward to nearest end of year\"\"\"\n        if self.onOffset(date):\n            return date\n        else:\n            return date + YearEnd(month=self.month)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrolls date backward to nearest end of year", "response": "def rollback(self, date):\n        \"\"\"Roll date backward to nearest end of year\"\"\"\n        if self.onOffset(date):\n            return date\n        else:\n            return date - YearEnd(month=self.month)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving an object x and numchars long format it so that it is numchars long and truncate it with trailing spaces as necessary.", "response": "def pretty_print(x, numchars):\n    \"\"\"Given an object `x`, call `str(x)` and format the returned string so\n    that it is numchars long, padding with trailing spaces or truncating with\n    ellipses as necessary\n    \"\"\"\n    s = maybe_truncate(x, numchars)\n    return s + ' ' * max(numchars - len(s), 0)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef first_n_items(array, n_desired):\n    # Unfortunately, we can't just do array.flat[:n_desired] here because it\n    # might not be a numpy.ndarray. Moreover, access to elements of the array\n    # could be very expensive (e.g. if it's only available over DAP), so go out\n    # of our way to get them in a single call to __getitem__ using only slices.\n    if n_desired < 1:\n        raise ValueError('must request at least one item')\n\n    if array.size == 0:\n        # work around for https://github.com/numpy/numpy/issues/5195\n        return []\n\n    if n_desired < array.size:\n        indexer = _get_indexer_at_least_n_items(array.shape, n_desired,\n                                                from_end=False)\n        array = array[indexer]\n    return np.asarray(array).flat[:n_desired]", "response": "Returns the first n_desired items of an array"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the last n_desired items of an array", "response": "def last_n_items(array, n_desired):\n    \"\"\"Returns the last n_desired items of an array\"\"\"\n    # Unfortunately, we can't just do array.flat[-n_desired:] here because it\n    # might not be a numpy.ndarray. Moreover, access to elements of the array\n    # could be very expensive (e.g. if it's only available over DAP), so go out\n    # of our way to get them in a single call to __getitem__ using only slices.\n    if (n_desired == 0) or (array.size == 0):\n        return []\n\n    if n_desired < array.size:\n        indexer = _get_indexer_at_least_n_items(array.shape, n_desired,\n                                                from_end=True)\n        array = array[indexer]\n    return np.asarray(array).flat[-n_desired:]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef last_item(array):\n    if array.size == 0:\n        # work around for https://github.com/numpy/numpy/issues/5195\n        return []\n\n    indexer = (slice(-1, None),) * array.ndim\n    return np.ravel(array[indexer]).tolist()", "response": "Returns the last item of an array in a list or an empty list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_timestamp(t):\n    # Timestamp is only valid for 1678 to 2262\n    try:\n        datetime_str = str(pd.Timestamp(t))\n    except OutOfBoundsDatetime:\n        datetime_str = str(t)\n\n    try:\n        date_str, time_str = datetime_str.split()\n    except ValueError:\n        # catch NaT and others that don't split nicely\n        return datetime_str\n    else:\n        if time_str == '00:00:00':\n            return date_str\n        else:\n            return '{}T{}'.format(date_str, time_str)", "response": "Cast given object to a Timestamp and return a nicely formatted string"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncasting given object to a Timestamp and return a nicely formatted string", "response": "def format_timedelta(t, timedelta_format=None):\n    \"\"\"Cast given object to a Timestamp and return a nicely formatted string\"\"\"\n    timedelta_str = str(pd.Timedelta(t))\n    try:\n        days_str, time_str = timedelta_str.split(' days ')\n    except ValueError:\n        # catch NaT and others that don't split nicely\n        return timedelta_str\n    else:\n        if timedelta_format == 'date':\n            return days_str + ' days'\n        elif timedelta_format == 'time':\n            return time_str\n        else:\n            return timedelta_str"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a succinct summary of an object as a string", "response": "def format_item(x, timedelta_format=None, quote_strings=True):\n    \"\"\"Returns a succinct summary of an object as a string\"\"\"\n    if isinstance(x, (np.datetime64, datetime)):\n        return format_timestamp(x)\n    if isinstance(x, (np.timedelta64, timedelta)):\n        return format_timedelta(x, timedelta_format=timedelta_format)\n    elif isinstance(x, (str, bytes)):\n        return repr(x) if quote_strings else x\n    elif isinstance(x, (float, np.float)):\n        return '{0:.4}'.format(x)\n    else:\n        return str(x)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a succinct summaries of all items in a sequence as strings", "response": "def format_items(x):\n    \"\"\"Returns a succinct summaries of all items in a sequence as strings\"\"\"\n    x = np.asarray(x)\n    timedelta_format = 'datetime'\n    if np.issubdtype(x.dtype, np.timedelta64):\n        x = np.asarray(x, dtype='timedelta64[ns]')\n        day_part = (x[~pd.isnull(x)]\n                    .astype('timedelta64[D]')\n                    .astype('timedelta64[ns]'))\n        time_needed = x[~pd.isnull(x)] != day_part\n        day_needed = day_part != np.timedelta64(0, 'ns')\n        if np.logical_not(day_needed).all():\n            timedelta_format = 'time'\n        elif np.logical_not(time_needed).all():\n            timedelta_format = 'date'\n\n    formatted = [format_item(xi, timedelta_format) for xi in x]\n    return formatted"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a string for as many items in the flattened version of the array that will fit within max_width characters.", "response": "def format_array_flat(array, max_width):\n    \"\"\"Return a formatted string for as many items in the flattened version of\n    array that will fit within max_width characters.\n    \"\"\"\n    # every item will take up at least two characters, but we always want to\n    # print at least first and last items\n    max_possibly_relevant = min(max(array.size, 1),\n                                max(int(np.ceil(max_width / 2.)), 2))\n    relevant_front_items = format_items(\n        first_n_items(array, (max_possibly_relevant + 1) // 2))\n    relevant_back_items = format_items(\n        last_n_items(array, max_possibly_relevant // 2))\n    # interleave relevant front and back items:\n    #     [a, b, c] and [y, z] -> [a, z, b, y, c]\n    relevant_items = sum(zip_longest(relevant_front_items,\n                                     reversed(relevant_back_items)),\n                         ())[:max_possibly_relevant]\n\n    cum_len = np.cumsum([len(s) + 1 for s in relevant_items]) - 1\n    if (array.size > 2) and ((max_possibly_relevant < array.size) or\n                             (cum_len > max_width).any()):\n        padding = ' ... '\n        count = min(array.size,\n                    max(np.argmax(cum_len + len(padding) - 1 > max_width), 2))\n    else:\n        count = array.size\n        padding = '' if (count <= 1) else ' '\n\n    num_front = (count + 1) // 2\n    num_back = count - num_front\n    # note that num_back is 0 <--> array.size is 0 or 1\n    #                         <--> relevant_back_items is []\n    pprint_str = (' '.join(relevant_front_items[:num_front]) +\n                  padding +\n                  ' '.join(relevant_back_items[-num_back:]))\n    return pprint_str"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets all column items to format including both keys of mapping and MultiIndex levels if any.", "response": "def _get_col_items(mapping):\n    \"\"\"Get all column items to format, including both keys of `mapping`\n    and MultiIndex levels if any.\n    \"\"\"\n    from .variable import IndexVariable\n\n    col_items = []\n    for k, v in mapping.items():\n        col_items.append(k)\n        var = getattr(v, 'variable', v)\n        if isinstance(var, IndexVariable):\n            level_names = var.to_index_variable().level_names\n            if level_names is not None:\n                col_items += list(level_names)\n    return col_items"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef short_dask_repr(array, show_dtype=True):\n    chunksize = tuple(c[0] for c in array.chunks)\n    if show_dtype:\n        return 'dask.array<shape={}, dtype={}, chunksize={}>'.format(\n            array.shape, array.dtype, chunksize)\n    else:\n        return 'dask.array<shape={}, chunksize={}>'.format(\n            array.shape, chunksize)", "response": "Similar to dask. array. DataArray. __repr__ but without\n    redundant information that s already printed by the repr\n    function of the xarray wrapper."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _upsample(self, method, *args, **kwargs):\n\n        upsampled_index = self._full_index\n\n        # Drop non-dimension coordinates along the resampled dimension\n        for k, v in self._obj.coords.items():\n            if k == self._dim:\n                continue\n            if self._dim in v.dims:\n                self._obj = self._obj.drop(k)\n\n        if method == 'asfreq':\n            return self.mean(self._dim)\n\n        elif method in ['pad', 'ffill', 'backfill', 'bfill', 'nearest']:\n            kwargs = kwargs.copy()\n            kwargs.update(**{self._dim: upsampled_index})\n            return self._obj.reindex(method=method, *args, **kwargs)\n\n        elif method == 'interpolate':\n            return self._interpolate(*args, **kwargs)\n\n        else:\n            raise ValueError('Specified method was \"{}\" but must be one of'\n                             '\"asfreq\", \"ffill\", \"bfill\", or \"interpolate\"'\n                             .format(method))", "response": "Dispatch function to call appropriate up - sampling methods on the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _interpolate(self, kind='linear'):\n        # drop any existing non-dimension coordinates along the resampling\n        # dimension\n        dummy = self._obj.copy()\n        for k, v in self._obj.coords.items():\n            if k != self._dim and self._dim in v.dims:\n                dummy = dummy.drop(k)\n        return dummy.interp(assume_sorted=True, method=kind,\n                            kwargs={'bounds_error': False},\n                            **{self._dim: self._full_index})", "response": "Apply scipy. interpolate. interp1d along resampling dimension."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef apply(self, func, shortcut=False, args=(), **kwargs):\n        combined = super(DataArrayResample, self).apply(\n            func, shortcut=shortcut, args=args, **kwargs)\n\n        # If the aggregation function didn't drop the original resampling\n        # dimension, then we need to do so before we can rename the proxy\n        # dimension we used.\n        if self._dim in combined.coords:\n            combined = combined.drop(self._dim)\n\n        if self._resample_dim in combined.dims:\n            combined = combined.rename({self._resample_dim: self._dim})\n\n        return combined", "response": "Apply a function over each array in the group and concatenate them together into a new array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\napplying a function over each Dataset in the groups generated for resampling and concatenate them together into a new Dataset.", "response": "def apply(self, func, args=(), **kwargs):\n        \"\"\"Apply a function over each Dataset in the groups generated for\n        resampling  and concatenate them together into a new Dataset.\n\n        `func` is called like `func(ds, *args, **kwargs)` for each dataset `ds`\n        in this group.\n\n        Apply uses heuristics (like `pandas.GroupBy.apply`) to figure out how\n        to stack together the datasets. The rule is:\n        1. If the dimension along which the group coordinate is defined is\n           still in the first grouped item after applying `func`, then stack\n           over this dimension.\n        2. Otherwise, stack over the new dimension given by name of this\n           grouping (the argument to the `groupby` function).\n\n        Parameters\n        ----------\n        func : function\n            Callable to apply to each sub-dataset.\n        args : tuple, optional\n            Positional arguments passed on to `func`.\n        **kwargs\n            Used to call `func(ds, **kwargs)` for each sub-dataset `ar`.\n\n        Returns\n        -------\n        applied : Dataset or DataArray\n            The result of splitting, applying and combining this dataset.\n        \"\"\"\n        kwargs.pop('shortcut', None)  # ignore shortcut if set (for now)\n        applied = (func(ds, *args, **kwargs) for ds in self._iter_grouped())\n        combined = self._combine(applied)\n\n        return combined.rename({self._resample_dim: self._dim})"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreduces the items in this group by applying func along the specified resampling dimension.", "response": "def reduce(self, func, dim=None, keep_attrs=None, **kwargs):\n        \"\"\"Reduce the items in this group by applying `func` along the\n        pre-defined resampling dimension.\n\n        Parameters\n        ----------\n        func : function\n            Function which can be called in the form\n            `func(x, axis=axis, **kwargs)` to return the result of collapsing\n            an np.ndarray over an integer valued axis.\n        dim : str or sequence of str, optional\n            Dimension(s) over which to apply `func`.\n        keep_attrs : bool, optional\n            If True, the datasets's attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n        **kwargs : dict\n            Additional keyword arguments passed on to `func`.\n\n        Returns\n        -------\n        reduced : Array\n            Array with summarized data and the indicated dimension(s)\n            removed.\n        \"\"\"\n        if dim == DEFAULT_DIMS:\n            dim = None\n\n        return super(DatasetResample, self).reduce(\n            func, dim, keep_attrs, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef default_indexes(\n    coords: Mapping[Any, Variable],\n    dims: Iterable,\n) -> 'OrderedDict[Any, pd.Index]':\n    \"\"\"Default indexes for a Dataset/DataArray.\n\n    Parameters\n    ----------\n    coords : Mapping[Any, xarray.Variable]\n       Coordinate variables from which to draw default indexes.\n    dims : iterable\n        Iterable of dimension names.\n\n    Returns\n    -------\n    Mapping from indexing keys (levels/dimension names) to indexes used for\n    indexing along that dimension.\n    \"\"\"\n    return OrderedDict((key, coords[key].to_index())\n                       for key in dims if key in coords)", "response": "Returns a default indexes for a Dataset or DataArray."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef isel_variable_and_index(\n    name: Hashable,\n    variable: Variable,\n    index: pd.Index,\n    indexers: Mapping[Any, Union[slice, Variable]],\n) -> Tuple[Variable, Optional[pd.Index]]:\n    \"\"\"Index a Variable and pandas.Index together.\"\"\"\n    if not indexers:\n        # nothing to index\n        return variable.copy(deep=False), index\n\n    if len(variable.dims) > 1:\n        raise NotImplementedError(\n            'indexing multi-dimensional variable with indexes is not '\n            'supported yet')\n\n    new_variable = variable.isel(indexers)\n\n    if new_variable.dims != (name,):\n        # can't preserve a index if result has new dimensions\n        return new_variable, None\n\n    # we need to compute the new index\n    (dim,) = variable.dims\n    indexer = indexers[dim]\n    if isinstance(indexer, Variable):\n        indexer = indexer.data\n    new_index = index[indexer]\n    return new_variable, new_index", "response": "Index a Variable and pandas. Index together."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_output_coords(\n    args: list,\n    signature: _UFuncSignature,\n    exclude_dims: AbstractSet = frozenset(),\n) -> 'List[OrderedDict[Any, Variable]]':\n    \"\"\"Build output coordinates for an operation.\n\n    Parameters\n    ----------\n    args : list\n        List of raw operation arguments. Any valid types for xarray operations\n        are OK, e.g., scalars, Variable, DataArray, Dataset.\n    signature : _UfuncSignature\n        Core dimensions signature for the operation.\n    exclude_dims : optional set\n        Dimensions excluded from the operation. Coordinates along these\n        dimensions are dropped.\n\n    Returns\n    -------\n    OrderedDict of Variable objects with merged coordinates.\n    \"\"\"\n    input_coords = _get_coord_variables(args)\n\n    if exclude_dims:\n        input_coords = [OrderedDict((k, v) for k, v in coord_vars.items()\n                                    if exclude_dims.isdisjoint(v.dims))\n                        for coord_vars in input_coords]\n\n    if len(input_coords) == 1:\n        # we can skip the expensive merge\n        unpacked_input_coords, = input_coords\n        merged = OrderedDict(unpacked_input_coords)\n    else:\n        merged = expand_and_merge_variables(input_coords)\n\n    output_coords = []\n    for output_dims in signature.output_core_dims:\n        dropped_dims = signature.all_input_core_dims - set(output_dims)\n        if dropped_dims:\n            filtered = OrderedDict((k, v) for k, v in merged.items()\n                                   if dropped_dims.isdisjoint(v.dims))\n        else:\n            filtered = merged\n        output_coords.append(filtered)\n\n    return output_coords", "response": "Builds the output coordinates for an operation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply a variable level function over DataArray Variable and ndarray objects.", "response": "def apply_dataarray_vfunc(\n    func,\n    *args,\n    signature,\n    join='inner',\n    exclude_dims=frozenset(),\n    keep_attrs=False\n):\n    \"\"\"Apply a variable level function over DataArray, Variable and/or ndarray\n    objects.\n    \"\"\"\n    from .dataarray import DataArray\n\n    if len(args) > 1:\n        args = deep_align(args, join=join, copy=False, exclude=exclude_dims,\n                          raise_on_invalid=False)\n\n    if keep_attrs and hasattr(args[0], 'name'):\n        name = args[0].name\n    else:\n        name = result_name(args)\n    result_coords = build_output_coords(args, signature, exclude_dims)\n\n    data_vars = [getattr(a, 'variable', a) for a in args]\n    result_var = func(*data_vars)\n\n    if signature.num_outputs > 1:\n        out = tuple(DataArray(variable, coords, name=name, fastpath=True)\n                    for variable, coords in zip(result_var, result_coords))\n    else:\n        coords, = result_coords\n        out = DataArray(result_var, coords, name=name, fastpath=True)\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef apply_dict_of_variables_vfunc(\n    func, *args, signature, join='inner', fill_value=None\n):\n    \"\"\"Apply a variable level function over dicts of DataArray, DataArray,\n    Variable and ndarray objects.\n    \"\"\"\n    args = [_as_variables_or_variable(arg) for arg in args]\n    names = join_dict_keys(args, how=join)\n    grouped_by_name = collect_dict_values(args, names, fill_value)\n\n    result_vars = OrderedDict()\n    for name, variable_args in zip(names, grouped_by_name):\n        result_vars[name] = func(*variable_args)\n\n    if signature.num_outputs > 1:\n        return _unpack_dict_tuples(result_vars, signature.num_outputs)\n    else:\n        return result_vars", "response": "Apply a variable level function over dicts of DataArray Variable and ndarray objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _fast_dataset(\n    variables: 'OrderedDict[Any, Variable]',\n    coord_variables: Mapping[Any, Variable],\n) -> 'Dataset':\n    \"\"\"Create a dataset as quickly as possible.\n\n    Beware: the `variables` OrderedDict is modified INPLACE.\n    \"\"\"\n    from .dataset import Dataset\n    variables.update(coord_variables)\n    coord_names = set(coord_variables)\n    return Dataset._from_vars_and_coord_names(variables, coord_names)", "response": "Create a Dataset as quickly as possible."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply a variable level function over Dataset dict of DataArray Variable and ndarray objects.", "response": "def apply_dataset_vfunc(\n    func,\n    *args,\n    signature,\n    join='inner',\n    dataset_join='exact',\n    fill_value=_NO_FILL_VALUE,\n    exclude_dims=frozenset(),\n    keep_attrs=False\n):\n    \"\"\"Apply a variable level function over Dataset, dict of DataArray,\n    DataArray, Variable and/or ndarray objects.\n    \"\"\"\n    from .dataset import Dataset\n    first_obj = args[0]  # we'll copy attrs from this in case keep_attrs=True\n\n    if (dataset_join not in _JOINS_WITHOUT_FILL_VALUES and\n            fill_value is _NO_FILL_VALUE):\n        raise TypeError('to apply an operation to datasets with different '\n                        'data variables with apply_ufunc, you must supply the '\n                        'dataset_fill_value argument.')\n\n    if len(args) > 1:\n        args = deep_align(args, join=join, copy=False, exclude=exclude_dims,\n                          raise_on_invalid=False)\n\n    list_of_coords = build_output_coords(args, signature, exclude_dims)\n    args = [getattr(arg, 'data_vars', arg) for arg in args]\n\n    result_vars = apply_dict_of_variables_vfunc(\n        func, *args, signature=signature, join=dataset_join,\n        fill_value=fill_value)\n\n    if signature.num_outputs > 1:\n        out = tuple(_fast_dataset(*args)\n                    for args in zip(result_vars, list_of_coords))\n    else:\n        coord_vars, = list_of_coords\n        out = _fast_dataset(result_vars, coord_vars)\n\n    if keep_attrs and isinstance(first_obj, Dataset):\n        if isinstance(out, tuple):\n            out = tuple(ds._copy_attrs_from(first_obj) for ds in out)\n        else:\n            out._copy_attrs_from(first_obj)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\niterate over selections of an xarray object in the provided order.", "response": "def _iter_over_selections(obj, dim, values):\n    \"\"\"Iterate over selections of an xarray object in the provided order.\"\"\"\n    from .groupby import _dummy_copy\n\n    dummy = None\n    for value in values:\n        try:\n            obj_sel = obj.sel(**{dim: value})\n        except (KeyError, IndexError):\n            if dummy is None:\n                dummy = _dummy_copy(obj)\n            obj_sel = dummy\n        yield obj_sel"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply a function over a GroupBy Dataset DataArray or ndarray objects.", "response": "def apply_groupby_func(func, *args):\n    \"\"\"Apply a dataset or datarray level function over GroupBy, Dataset,\n    DataArray, Variable and/or ndarray objects.\n    \"\"\"\n    from .groupby import GroupBy, peek_at\n    from .variable import Variable\n\n    groupbys = [arg for arg in args if isinstance(arg, GroupBy)]\n    assert groupbys, 'must have at least one groupby to iterate over'\n    first_groupby = groupbys[0]\n    if any(not first_groupby._group.equals(gb._group) for gb in groupbys[1:]):\n        raise ValueError('apply_ufunc can only perform operations over '\n                         'multiple GroupBy objets at once if they are all '\n                         'grouped the same way')\n\n    grouped_dim = first_groupby._group.name\n    unique_values = first_groupby._unique_coord.values\n\n    iterators = []\n    for arg in args:\n        if isinstance(arg, GroupBy):\n            iterator = (value for _, value in arg)\n        elif hasattr(arg, 'dims') and grouped_dim in arg.dims:\n            if isinstance(arg, Variable):\n                raise ValueError(\n                    'groupby operations cannot be performed with '\n                    'xarray.Variable objects that share a dimension with '\n                    'the grouped dimension')\n            iterator = _iter_over_selections(arg, grouped_dim, unique_values)\n        else:\n            iterator = itertools.repeat(arg)\n        iterators.append(iterator)\n\n    applied = (func(*zipped_args) for zipped_args in zip(*iterators))\n    applied_example, applied = peek_at(applied)\n    combine = first_groupby._combine\n    if isinstance(applied_example, tuple):\n        combined = tuple(combine(output) for output in zip(*applied))\n    else:\n        combined = combine(applied)\n    return combined"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply a function over a Variable and ndarray objects.", "response": "def apply_variable_ufunc(\n    func,\n    *args,\n    signature,\n    exclude_dims=frozenset(),\n    dask='forbidden',\n    output_dtypes=None,\n    output_sizes=None,\n    keep_attrs=False\n):\n    \"\"\"Apply a ndarray level function over Variable and/or ndarray objects.\n    \"\"\"\n    from .variable import Variable, as_compatible_data\n\n    dim_sizes = unified_dim_sizes((a for a in args if hasattr(a, 'dims')),\n                                  exclude_dims=exclude_dims)\n    broadcast_dims = tuple(dim for dim in dim_sizes\n                           if dim not in signature.all_core_dims)\n    output_dims = [broadcast_dims + out for out in signature.output_core_dims]\n\n    input_data = [broadcast_compat_data(arg, broadcast_dims, core_dims)\n                  if isinstance(arg, Variable)\n                  else arg\n                  for arg, core_dims in zip(args, signature.input_core_dims)]\n\n    if any(isinstance(array, dask_array_type) for array in input_data):\n        if dask == 'forbidden':\n            raise ValueError('apply_ufunc encountered a dask array on an '\n                             'argument, but handling for dask arrays has not '\n                             'been enabled. Either set the ``dask`` argument '\n                             'or load your data into memory first with '\n                             '``.load()`` or ``.compute()``')\n        elif dask == 'parallelized':\n            input_dims = [broadcast_dims + dims\n                          for dims in signature.input_core_dims]\n            numpy_func = func\n\n            def func(*arrays):\n                return _apply_with_dask_atop(\n                    numpy_func, arrays, input_dims, output_dims,\n                    signature, output_dtypes, output_sizes)\n        elif dask == 'allowed':\n            pass\n        else:\n            raise ValueError('unknown setting for dask array handling in '\n                             'apply_ufunc: {}'.format(dask))\n    result_data = func(*input_data)\n\n    if signature.num_outputs == 1:\n        result_data = (result_data,)\n    elif (not isinstance(result_data, tuple) or\n            len(result_data) != signature.num_outputs):\n        raise ValueError('applied function does not have the number of '\n                         'outputs specified in the ufunc signature. '\n                         'Result is not a tuple of {} elements: {!r}'\n                         .format(signature.num_outputs, result_data))\n\n    output = []\n    for dims, data in zip(output_dims, result_data):\n        data = as_compatible_data(data)\n        if data.ndim != len(dims):\n            raise ValueError(\n                'applied function returned data with unexpected '\n                'number of dimensions: {} vs {}, for dimensions {}'\n                .format(data.ndim, len(dims), dims))\n\n        var = Variable(dims, data, fastpath=True)\n        for dim, new_size in var.sizes.items():\n            if dim in dim_sizes and new_size != dim_sizes[dim]:\n                raise ValueError(\n                    'size of dimension {!r} on inputs was unexpectedly '\n                    'changed by applied function from {} to {}. Only '\n                    'dimensions specified in ``exclude_dims`` with '\n                    'xarray.apply_ufunc are allowed to change size.'\n                    .format(dim, dim_sizes[dim], new_size))\n\n        if keep_attrs and isinstance(args[0], Variable):\n            var.attrs.update(args[0].attrs)\n        output.append(var)\n\n    if signature.num_outputs == 1:\n        return output[0]\n    else:\n        return tuple(output)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apply_array_ufunc(func, *args, dask='forbidden'):\n    if any(isinstance(arg, dask_array_type) for arg in args):\n        if dask == 'forbidden':\n            raise ValueError('apply_ufunc encountered a dask array on an '\n                             'argument, but handling for dask arrays has not '\n                             'been enabled. Either set the ``dask`` argument '\n                             'or load your data into memory first with '\n                             '``.load()`` or ``.compute()``')\n        elif dask == 'parallelized':\n            raise ValueError(\"cannot use dask='parallelized' for apply_ufunc \"\n                             'unless at least one input is an xarray object')\n        elif dask == 'allowed':\n            pass\n        else:\n            raise ValueError('unknown setting for dask array handling: {}'\n                             .format(dask))\n    return func(*args)", "response": "Apply a ndarray level function over ndarray objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\napply a function to unlabeled arrays on xarray objects.", "response": "def apply_ufunc(\n    func: Callable,\n    *args: Any,\n    input_core_dims: Optional[Sequence[Sequence]] = None,\n    output_core_dims: Optional[Sequence[Sequence]] = ((),),\n    exclude_dims: AbstractSet = frozenset(),\n    vectorize: bool = False,\n    join: str = 'exact',\n    dataset_join: str = 'exact',\n    dataset_fill_value: object = _NO_FILL_VALUE,\n    keep_attrs: bool = False,\n    kwargs: Mapping = None,\n    dask: str = 'forbidden',\n    output_dtypes: Optional[Sequence] = None,\n    output_sizes: Optional[Mapping[Any, int]] = None\n) -> Any:\n    \"\"\"Apply a vectorized function for unlabeled arrays on xarray objects.\n\n    The function will be mapped over the data variable(s) of the input\n    arguments using xarray's standard rules for labeled computation, including\n    alignment, broadcasting, looping over GroupBy/Dataset variables, and\n    merging of coordinates.\n\n    Parameters\n    ----------\n    func : callable\n        Function to call like ``func(*args, **kwargs)`` on unlabeled arrays\n        (``.data``) that returns an array or tuple of arrays. If multiple\n        arguments with non-matching dimensions are supplied, this function is\n        expected to vectorize (broadcast) over axes of positional arguments in\n        the style of NumPy universal functions [1]_ (if this is not the case,\n        set ``vectorize=True``). If this function returns multiple outputs, you\n        must set ``output_core_dims`` as well.\n    *args : Dataset, DataArray, GroupBy, Variable, numpy/dask arrays or scalars\n        Mix of labeled and/or unlabeled arrays to which to apply the function.\n    input_core_dims : Sequence[Sequence], optional\n        List of the same length as ``args`` giving the list of core dimensions\n        on each input argument that should not be broadcast. By default, we\n        assume there are no core dimensions on any input arguments.\n\n        For example, ``input_core_dims=[[], ['time']]`` indicates that all\n        dimensions on the first argument and all dimensions other than 'time'\n        on the second argument should be broadcast.\n\n        Core dimensions are automatically moved to the last axes of input\n        variables before applying ``func``, which facilitates using NumPy style\n        generalized ufuncs [2]_.\n    output_core_dims : List[tuple], optional\n        List of the same length as the number of output arguments from\n        ``func``, giving the list of core dimensions on each output that were\n        not broadcast on the inputs. By default, we assume that ``func``\n        outputs exactly one array, with axes corresponding to each broadcast\n        dimension.\n\n        Core dimensions are assumed to appear as the last dimensions of each\n        output in the provided order.\n    exclude_dims : set, optional\n        Core dimensions on the inputs to exclude from alignment and\n        broadcasting entirely. Any input coordinates along these dimensions\n        will be dropped. Each excluded dimension must also appear in\n        ``input_core_dims`` for at least one argument. Only dimensions listed\n        here are allowed to change size between input and output objects.\n    vectorize : bool, optional\n        If True, then assume ``func`` only takes arrays defined over core\n        dimensions as input and vectorize it automatically with\n        :py:func:`numpy.vectorize`. This option exists for convenience, but is\n        almost always slower than supplying a pre-vectorized function.\n        Using this option requires NumPy version 1.12 or newer.\n    join : {'outer', 'inner', 'left', 'right', 'exact'}, optional\n        Method for joining the indexes of the passed objects along each\n        dimension, and the variables of Dataset objects with mismatched\n        data variables:\n\n        - 'outer': use the union of object indexes\n        - 'inner': use the intersection of object indexes\n        - 'left': use indexes from the first object with each dimension\n        - 'right': use indexes from the last object with each dimension\n        - 'exact': raise `ValueError` instead of aligning when indexes to be\n          aligned are not equal\n    dataset_join : {'outer', 'inner', 'left', 'right', 'exact'}, optional\n        Method for joining variables of Dataset objects with mismatched\n        data variables.\n\n        - 'outer': take variables from both Dataset objects\n        - 'inner': take only overlapped variables\n        - 'left': take only variables from the first object\n        - 'right': take only variables from the last object\n        - 'exact': data variables on all Dataset objects must match exactly\n    dataset_fill_value : optional\n        Value used in place of missing variables on Dataset inputs when the\n        datasets do not share the exact same ``data_vars``. Required if\n        ``dataset_join not in {'inner', 'exact'}``, otherwise ignored.\n    keep_attrs: boolean, Optional\n        Whether to copy attributes from the first argument to the output.\n    kwargs: dict, optional\n        Optional keyword arguments passed directly on to call ``func``.\n    dask: 'forbidden', 'allowed' or 'parallelized', optional\n        How to handle applying to objects containing lazy data in the form of\n        dask arrays:\n\n        - 'forbidden' (default): raise an error if a dask array is encountered.\n        - 'allowed': pass dask arrays directly on to ``func``.\n        - 'parallelized': automatically parallelize ``func`` if any of the\n          inputs are a dask array. If used, the ``output_dtypes`` argument must\n          also be provided. Multiple output arguments are not yet supported.\n    output_dtypes : list of dtypes, optional\n        Optional list of output dtypes. Only used if dask='parallelized'.\n    output_sizes : dict, optional\n        Optional mapping from dimension names to sizes for outputs. Only used\n        if dask='parallelized' and new dimensions (not found on inputs) appear\n        on outputs.\n\n    Returns\n    -------\n    Single value or tuple of Dataset, DataArray, Variable, dask.array.Array or\n    numpy.ndarray, the first type on that list to appear on an input.\n\n    Examples\n    --------\n\n    Calculate the vector magnitude of two arguments:\n\n    >>> def magnitude(a, b):\n    ...     func = lambda x, y: np.sqrt(x ** 2 + y ** 2)\n    ...     return xr.apply_ufunc(func, a, b)\n\n    You can now apply ``magnitude()`` to ``xr.DataArray`` and ``xr.Dataset``\n    objects, with automatically preserved dimensions and coordinates, e.g.,\n\n    >>> array = xr.DataArray([1, 2, 3], coords=[('x', [0.1, 0.2, 0.3])])\n    >>> magnitude(array, -array)\n    <xarray.DataArray (x: 3)>\n    array([1.414214, 2.828427, 4.242641])\n    Coordinates:\n      * x        (x) float64 0.1 0.2 0.3\n\n    Plain scalars, numpy arrays and a mix of these with xarray objects is also\n    supported:\n\n    >>> magnitude(4, 5)\n    5.0\n    >>> magnitude(3, np.array([0, 4]))\n    array([3., 5.])\n    >>> magnitude(array, 0)\n    <xarray.DataArray (x: 3)>\n    array([1., 2., 3.])\n    Coordinates:\n      * x        (x) float64 0.1 0.2 0.3\n\n    Other examples of how you could use ``apply_ufunc`` to write functions to\n    (very nearly) replicate existing xarray functionality:\n\n    Compute the mean (``.mean``) over one dimension::\n\n        def mean(obj, dim):\n            # note: apply always moves core dimensions to the end\n            return apply_ufunc(np.mean, obj,\n                               input_core_dims=[[dim]],\n                               kwargs={'axis': -1})\n\n    Inner product over a specific dimension (like ``xr.dot``)::\n\n        def _inner(x, y):\n            result = np.matmul(x[..., np.newaxis, :], y[..., :, np.newaxis])\n            return result[..., 0, 0]\n\n        def inner_product(a, b, dim):\n            return apply_ufunc(_inner, a, b, input_core_dims=[[dim], [dim]])\n\n    Stack objects along a new dimension (like ``xr.concat``)::\n\n        def stack(objects, dim, new_coord):\n            # note: this version does not stack coordinates\n            func = lambda *x: np.stack(x, axis=-1)\n            result = apply_ufunc(func, *objects,\n                                 output_core_dims=[[dim]],\n                                 join='outer',\n                                 dataset_fill_value=np.nan)\n            result[dim] = new_coord\n            return result\n\n    If your function is not vectorized but can be applied only to core\n    dimensions, you can use ``vectorize=True`` to turn into a vectorized\n    function. This wraps :py:func:`numpy.vectorize`, so the operation isn't\n    terribly fast. Here we'll use it to calculate the distance between\n    empirical samples from two probability distributions, using a scipy\n    function that needs to be applied to vectors::\n\n        import scipy.stats\n\n        def earth_mover_distance(first_samples,\n                                 second_samples,\n                                 dim='ensemble'):\n            return apply_ufunc(scipy.stats.wasserstein_distance,\n                               first_samples, second_samples,\n                               input_core_dims=[[dim], [dim]],\n                               vectorize=True)\n\n    Most of NumPy's builtin functions already broadcast their inputs\n    appropriately for use in `apply`. You may find helper functions such as\n    numpy.broadcast_arrays helpful in writing your function. `apply_ufunc` also\n    works well with numba's vectorize and guvectorize. Further explanation with\n    examples are provided in the xarray documentation [3].\n\n    See also\n    --------\n    numpy.broadcast_arrays\n    numba.vectorize\n    numba.guvectorize\n\n    References\n    ----------\n    .. [1] http://docs.scipy.org/doc/numpy/reference/ufuncs.html\n    .. [2] http://docs.scipy.org/doc/numpy/reference/c-api.generalized-ufuncs.html\n    .. [3] http://xarray.pydata.org/en/stable/computation.html#wrapping-custom-computation\n    \"\"\"  # noqa: E501  # don't error on that URL one line up\n    from .groupby import GroupBy\n    from .dataarray import DataArray\n    from .variable import Variable\n\n    if input_core_dims is None:\n        input_core_dims = ((),) * (len(args))\n    elif len(input_core_dims) != len(args):\n        raise ValueError(\n            'input_core_dims must be None or a tuple with the length same to '\n            'the number of arguments. Given input_core_dims: {}, '\n            'number of args: {}.'.format(input_core_dims, len(args)))\n\n    if kwargs is None:\n        kwargs = {}\n\n    signature = _UFuncSignature(input_core_dims, output_core_dims)\n\n    if exclude_dims and not exclude_dims <= signature.all_core_dims:\n        raise ValueError('each dimension in `exclude_dims` must also be a '\n                         'core dimension in the function signature')\n\n    if kwargs:\n        func = functools.partial(func, **kwargs)\n\n    if vectorize:\n        if signature.all_core_dims:\n            # we need the signature argument\n            if LooseVersion(np.__version__) < '1.12':  # pragma: no cover\n                raise NotImplementedError(\n                    'numpy 1.12 or newer required when using vectorize=True '\n                    'in xarray.apply_ufunc with non-scalar output core '\n                    'dimensions.')\n            func = np.vectorize(func,\n                                otypes=output_dtypes,\n                                signature=signature.to_gufunc_string())\n        else:\n            func = np.vectorize(func, otypes=output_dtypes)\n\n    variables_vfunc = functools.partial(apply_variable_ufunc, func,\n                                        signature=signature,\n                                        exclude_dims=exclude_dims,\n                                        keep_attrs=keep_attrs,\n                                        dask=dask,\n                                        output_dtypes=output_dtypes,\n                                        output_sizes=output_sizes)\n\n    if any(isinstance(a, GroupBy) for a in args):\n        this_apply = functools.partial(apply_ufunc, func,\n                                       input_core_dims=input_core_dims,\n                                       output_core_dims=output_core_dims,\n                                       exclude_dims=exclude_dims,\n                                       join=join,\n                                       dataset_join=dataset_join,\n                                       dataset_fill_value=dataset_fill_value,\n                                       keep_attrs=keep_attrs,\n                                       dask=dask)\n        return apply_groupby_func(this_apply, *args)\n    elif any(is_dict_like(a) for a in args):\n        return apply_dataset_vfunc(variables_vfunc, *args,\n                                   signature=signature,\n                                   join=join,\n                                   exclude_dims=exclude_dims,\n                                   dataset_join=dataset_join,\n                                   fill_value=dataset_fill_value,\n                                   keep_attrs=keep_attrs)\n    elif any(isinstance(a, DataArray) for a in args):\n        return apply_dataarray_vfunc(variables_vfunc, *args,\n                                     signature=signature,\n                                     join=join,\n                                     exclude_dims=exclude_dims,\n                                     keep_attrs=keep_attrs)\n    elif any(isinstance(a, Variable) for a in args):\n        return variables_vfunc(*args)\n    else:\n        return apply_array_ufunc(func, *args, dask=dask)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngeneralize dot product for xarray objects.", "response": "def dot(*arrays, dims=None, **kwargs):\n    \"\"\"Generalized dot product for xarray objects. Like np.einsum, but\n    provides a simpler interface based on array dimensions.\n\n    Parameters\n    ----------\n    arrays: DataArray (or Variable) objects\n        Arrays to compute.\n    dims: str or tuple of strings, optional\n        Which dimensions to sum over.\n        If not speciified, then all the common dimensions are summed over.\n    **kwargs: dict\n        Additional keyword arguments passed to numpy.einsum or\n        dask.array.einsum\n\n    Returns\n    -------\n    dot: DataArray\n\n    Examples\n    --------\n\n    >>> da_a = xr.DataArray(np.arange(3 * 4).reshape(3, 4), dims=['a', 'b'])\n    >>> da_b = xr.DataArray(np.arange(3 * 4 * 5).reshape(3, 4, 5),\n    >>>                     dims=['a', 'b', 'c'])\n    >>> da_c = xr.DataArray(np.arange(5 * 6).reshape(5, 6), dims=['c', 'd'])\n    >>>\n    >>> xr.dot(da_a, da_b, dims=['a', 'b']).dims\n    ('c', )\n    >>> xr.dot(da_a, da_b, dims=['a']).dims\n    ('b', 'c')\n    >>> xr.dot(da_a, da_b, da_c, dims=['b', 'c']).dims\n    ('a', 'd')\n    \"\"\"\n    from .dataarray import DataArray\n    from .variable import Variable\n\n    if any(not isinstance(arr, (Variable, DataArray)) for arr in arrays):\n        raise TypeError('Only xr.DataArray and xr.Variable are supported.'\n                        'Given {}.'.format([type(arr) for arr in arrays]))\n\n    if len(arrays) == 0:\n        raise TypeError('At least one array should be given.')\n\n    if isinstance(dims, str):\n        dims = (dims, )\n\n    common_dims = set.intersection(*[set(arr.dims) for arr in arrays])\n    all_dims = []\n    for arr in arrays:\n        all_dims += [d for d in arr.dims if d not in all_dims]\n\n    einsum_axes = 'abcdefghijklmnopqrstuvwxyz'\n    dim_map = {d: einsum_axes[i] for i, d in enumerate(all_dims)}\n\n    if dims is None:\n        # find dimensions that occur more than one times\n        dim_counts = Counter()\n        for arr in arrays:\n            dim_counts.update(arr.dims)\n        dims = tuple(d for d, c in dim_counts.items() if c > 1)\n\n    dims = tuple(dims)  # make dims a tuple\n\n    # dimensions to be parallelized\n    broadcast_dims = tuple(d for d in all_dims\n                           if d in common_dims and d not in dims)\n    input_core_dims = [[d for d in arr.dims if d not in broadcast_dims]\n                       for arr in arrays]\n    output_core_dims = [tuple(d for d in all_dims if d not in\n                              dims + broadcast_dims)]\n\n    # older dask than 0.17.4, we use tensordot if possible.\n    if isinstance(arr.data, dask_array_type):\n        import dask\n        if LooseVersion(dask.__version__) < LooseVersion('0.17.4'):\n            if len(broadcast_dims) == 0 and len(arrays) == 2:\n                axes = [[arr.get_axis_num(d) for d in arr.dims if d in dims]\n                        for arr in arrays]\n                return apply_ufunc(duck_array_ops.tensordot, *arrays,\n                                   dask='allowed',\n                                   input_core_dims=input_core_dims,\n                                   output_core_dims=output_core_dims,\n                                   kwargs={'axes': axes})\n\n    # construct einsum subscripts, such as '...abc,...ab->...c'\n    # Note: input_core_dims are always moved to the last position\n    subscripts_list = ['...' + ''.join([dim_map[d] for d in ds]) for ds\n                       in input_core_dims]\n    subscripts = ','.join(subscripts_list)\n    subscripts += '->...' + ''.join([dim_map[d] for d in output_core_dims[0]])\n\n    # subscripts should be passed to np.einsum as arg, not as kwargs. We need\n    # to construct a partial function for apply_ufunc to work.\n    func = functools.partial(duck_array_ops.einsum, subscripts, **kwargs)\n    result = apply_ufunc(func, *arrays,\n                         input_core_dims=input_core_dims,\n                         output_core_dims=output_core_dims,\n                         dask='allowed')\n    return result.transpose(*[d for d in all_dims if d in result.dims])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns elements from x or y depending on cond.", "response": "def where(cond, x, y):\n    \"\"\"Return elements from `x` or `y` depending on `cond`.\n\n    Performs xarray-like broadcasting across input arguments.\n\n    Parameters\n    ----------\n    cond : scalar, array, Variable, DataArray or Dataset with boolean dtype\n        When True, return values from `x`, otherwise returns values from `y`.\n    x, y : scalar, array, Variable, DataArray or Dataset\n        Values from which to choose. All dimension coordinates on these objects\n        must be aligned with each other and with `cond`.\n\n    Returns\n    -------\n    In priority order: Dataset, DataArray, Variable or array, whichever\n    type appears as an input argument.\n\n    Examples\n    --------\n\n    >>> cond = xr.DataArray([True, False], dims=['x'])\n    >>> x = xr.DataArray([1, 2], dims=['y'])\n    >>> xr.where(cond, x, 0)\n    <xarray.DataArray (x: 2, y: 2)>\n    array([[1, 2],\n           [0, 0]])\n    Dimensions without coordinates: x, y\n\n    See also\n    --------\n    numpy.where : corresponding numpy function\n    Dataset.where, DataArray.where : equivalent methods\n    \"\"\"\n    # alignment for three arguments is complicated, so don't support it yet\n    return apply_ufunc(duck_array_ops.where,\n                       cond, x, y,\n                       join='exact',\n                       dataset_join='exact',\n                       dask='allowed')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_gufunc_string(self):\n        all_dims = self.all_core_dims\n        dims_map = dict(zip(sorted(all_dims), range(len(all_dims))))\n        input_core_dims = [['dim%d' % dims_map[dim] for dim in core_dims]\n                           for core_dims in self.input_core_dims]\n        output_core_dims = [['dim%d' % dims_map[dim] for dim in core_dims]\n                            for core_dims in self.output_core_dims]\n        alt_signature = type(self)(input_core_dims, output_core_dims)\n        return str(alt_signature)", "response": "Create an equivalent signature string for a NumPy gufunc.\n        object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns indices for an inverse permutation.", "response": "def inverse_permutation(indices):\n    \"\"\"Return indices for an inverse permutation.\n\n    Parameters\n    ----------\n    indices : 1D np.ndarray with dtype=int\n        Integer positions to assign elements to.\n\n    Returns\n    -------\n    inverse_permutation : 1D np.ndarray with dtype=int\n        Integer indices to take from the original array to create the\n        permutation.\n    \"\"\"\n    # use intp instead of int64 because of windows :(\n    inverse_permutation = np.empty(len(indices), dtype=np.intp)\n    inverse_permutation[indices] = np.arange(len(indices), dtype=np.intp)\n    return inverse_permutation"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _is_contiguous(positions):\n    previous = positions[0]\n    for current in positions[1:]:\n        if current != previous + 1:\n            return False\n        previous = current\n    return True", "response": "Given a non - empty list returns True if all of the elements in the list are contiguous integers."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _advanced_indexer_subspaces(key):\n    if not isinstance(key, tuple):\n        key = (key,)\n    advanced_index_positions = [i for i, k in enumerate(key)\n                                if not isinstance(k, slice)]\n\n    if (not advanced_index_positions or\n            not _is_contiguous(advanced_index_positions)):\n        # Nothing to reorder: dimensions on the indexing result are already\n        # ordered like vindex. See NumPy's rule for \"Combining advanced and\n        # basic indexing\":\n        # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#combining-advanced-and-basic-indexing\n        return (), ()\n\n    non_slices = [k for k in key if not isinstance(k, slice)]\n    ndim = len(np.broadcast(*non_slices).shape)\n    mixed_positions = advanced_index_positions[0] + np.arange(ndim)\n    vindex_positions = np.arange(ndim)\n    return mixed_positions, vindex_positions", "response": "Indices of the advanced indexes subspaces for mixed indexing and vindex."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nroll window with padding.", "response": "def rolling_window(a, axis, window, center, fill_value):\n    \"\"\" rolling window with padding. \"\"\"\n    pads = [(0, 0) for s in a.shape]\n    if center:\n        start = int(window / 2)  # 10 -> 5,  9 -> 4\n        end = window - 1 - start\n        pads[axis] = (start, end)\n    else:\n        pads[axis] = (window - 1, 0)\n    a = np.pad(a, pads, mode='constant', constant_values=fill_value)\n    return _rolling_window(a, window, axis)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an array_like object with a rolling window along an axis.", "response": "def _rolling_window(a, window, axis=-1):\n    \"\"\"\n    Make an ndarray with a rolling window along axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Array to add rolling window to\n    axis: int\n        axis position along which rolling window will be applied.\n    window : int\n        Size of rolling window\n\n    Returns\n    -------\n    Array that is a view of the original array with a added dimension\n    of size w.\n\n    Examples\n    --------\n    >>> x=np.arange(10).reshape((2,5))\n    >>> np.rolling_window(x, 3, axis=-1)\n    array([[[0, 1, 2], [1, 2, 3], [2, 3, 4]],\n           [[5, 6, 7], [6, 7, 8], [7, 8, 9]]])\n\n    Calculate rolling mean of last dimension:\n    >>> np.mean(np.rolling_window(x, 3, axis=-1), -1)\n    array([[ 1.,  2.,  3.],\n           [ 6.,  7.,  8.]])\n\n    This function is taken from https://github.com/numpy/numpy/pull/31\n    but slightly modified to accept axis option.\n    \"\"\"\n    axis = _validate_axis(a, axis)\n    a = np.swapaxes(a, axis, -1)\n\n    if window < 1:\n        raise ValueError(\n            \"`window` must be at least 1. Given : {}\".format(window))\n    if window > a.shape[-1]:\n        raise ValueError(\"`window` is too long. Given : {}\".format(window))\n\n    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n    strides = a.strides + (a.strides[-1],)\n    rolling = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides,\n                                              writeable=False)\n    return np.swapaxes(rolling, -2, axis)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_virtual_variable(variables, key, level_vars=None, dim_sizes=None):\n    if level_vars is None:\n        level_vars = {}\n    if dim_sizes is None:\n        dim_sizes = {}\n\n    if key in dim_sizes:\n        data = pd.Index(range(dim_sizes[key]), name=key)\n        variable = IndexVariable((key,), data)\n        return key, key, variable\n\n    if not isinstance(key, str):\n        raise KeyError(key)\n\n    split_key = key.split('.', 1)\n    if len(split_key) == 2:\n        ref_name, var_name = split_key\n    elif len(split_key) == 1:\n        ref_name, var_name = key, None\n    else:\n        raise KeyError(key)\n\n    if ref_name in level_vars:\n        dim_var = variables[level_vars[ref_name]]\n        ref_var = dim_var.to_index_variable().get_level_variable(ref_name)\n    else:\n        ref_var = variables[ref_name]\n\n    if var_name is None:\n        virtual_var = ref_var\n        var_name = key\n    else:\n        if _contains_datetime_like_objects(ref_var):\n            ref_var = xr.DataArray(ref_var)\n            data = getattr(ref_var.dt, var_name).data\n        else:\n            data = getattr(ref_var, var_name).data\n        virtual_var = Variable(ref_var.dims, data)\n\n    return ref_name, var_name, virtual_var", "response": "Get a virtual variable from a dict of xarray. Variable objects or MultiIndex variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the dimensions corresponding to a set of variables.", "response": "def calculate_dimensions(variables):\n    \"\"\"Calculate the dimensions corresponding to a set of variables.\n\n    Returns dictionary mapping from dimension names to sizes. Raises ValueError\n    if any of the dimension sizes conflict.\n    \"\"\"\n    dims = OrderedDict()\n    last_used = {}\n    scalar_vars = set(k for k, v in variables.items() if not v.dims)\n    for k, var in variables.items():\n        for dim, size in zip(var.dims, var.shape):\n            if dim in scalar_vars:\n                raise ValueError('dimension %r already exists as a scalar '\n                                 'variable' % dim)\n            if dim not in dims:\n                dims[dim] = size\n                last_used[dim] = k\n            elif dims[dim] != size:\n                raise ValueError('conflicting sizes for dimension %r: '\n                                 'length %s on %r and length %s on %r' %\n                                 (dim, size, k, dims[dim], last_used[dim]))\n    return dims"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmerge the given indexes into a single multi - index.", "response": "def merge_indexes(\n        indexes,  # type: Dict[Any, Union[Any, List[Any]]]\n        variables,  # type: Dict[Any, Variable]\n        coord_names,  # type: Set\n        append=False,  # type: bool\n):\n    # type: (...) -> Tuple[OrderedDict[Any, Variable], Set]\n    \"\"\"Merge variables into multi-indexes.\n\n    Not public API. Used in Dataset and DataArray set_index\n    methods.\n    \"\"\"\n    vars_to_replace = {}  # Dict[Any, Variable]\n    vars_to_remove = []  # type: list\n\n    for dim, var_names in indexes.items():\n        if isinstance(var_names, str):\n            var_names = [var_names]\n\n        names, codes, levels = [], [], []  # type: (list, list, list)\n        current_index_variable = variables.get(dim)\n\n        for n in var_names:\n            var = variables[n]\n            if (current_index_variable is not None and\n                    var.dims != current_index_variable.dims):\n                raise ValueError(\n                    \"dimension mismatch between %r %s and %r %s\"\n                    % (dim, current_index_variable.dims, n, var.dims))\n\n        if current_index_variable is not None and append:\n            current_index = current_index_variable.to_index()\n            if isinstance(current_index, pd.MultiIndex):\n                try:\n                    current_codes = current_index.codes\n                except AttributeError:\n                    # fpr pandas<0.24\n                    current_codes = current_index.labels\n                names.extend(current_index.names)\n                codes.extend(current_codes)\n                levels.extend(current_index.levels)\n            else:\n                names.append('%s_level_0' % dim)\n                cat = pd.Categorical(current_index.values, ordered=True)\n                codes.append(cat.codes)\n                levels.append(cat.categories)\n\n        if not len(names) and len(var_names) == 1:\n            idx = pd.Index(variables[var_names[0]].values)\n\n        else:\n            for n in var_names:\n                names.append(n)\n                var = variables[n]\n                cat = pd.Categorical(var.values, ordered=True)\n                codes.append(cat.codes)\n                levels.append(cat.categories)\n\n            idx = pd.MultiIndex(levels, codes, names=names)\n\n        vars_to_replace[dim] = IndexVariable(dim, idx)\n        vars_to_remove.extend(var_names)\n\n    new_variables = OrderedDict([(k, v) for k, v in variables.items()\n                                 if k not in vars_to_remove])\n    new_variables.update(vars_to_replace)\n    new_coord_names = coord_names | set(vars_to_replace)\n    new_coord_names -= set(vars_to_remove)\n\n    return new_variables, new_coord_names"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef split_indexes(\n    dims_or_levels,  # type: Union[Any, List[Any]]\n    variables,  # type: OrderedDict[Any, Variable]\n    coord_names,  # type: Set\n    level_coords,  # type: Dict[Any, Any]\n    drop=False,  # type: bool\n):\n    # type: (...) -> Tuple[OrderedDict[Any, Variable], Set]\n    \"\"\"Extract (multi-)indexes (levels) as variables.\n\n    Not public API. Used in Dataset and DataArray reset_index\n    methods.\n    \"\"\"\n    if isinstance(dims_or_levels, str):\n        dims_or_levels = [dims_or_levels]\n\n    dim_levels = defaultdict(list)  # type: Dict[Any, list]\n    dims = []\n    for k in dims_or_levels:\n        if k in level_coords:\n            dim_levels[level_coords[k]].append(k)\n        else:\n            dims.append(k)\n\n    vars_to_replace = {}\n    vars_to_create = OrderedDict()  # type: OrderedDict[Any, Variable]\n    vars_to_remove = []\n\n    for d in dims:\n        index = variables[d].to_index()\n        if isinstance(index, pd.MultiIndex):\n            dim_levels[d] = index.names\n        else:\n            vars_to_remove.append(d)\n            if not drop:\n                vars_to_create[d + '_'] = Variable(d, index)\n\n    for d, levs in dim_levels.items():\n        index = variables[d].to_index()\n        if len(levs) == index.nlevels:\n            vars_to_remove.append(d)\n        else:\n            vars_to_replace[d] = IndexVariable(d, index.droplevel(levs))\n\n        if not drop:\n            for lev in levs:\n                idx = index.get_level_values(lev)\n                vars_to_create[idx.name] = Variable(d, idx)\n\n    new_variables = variables.copy()\n    for v in set(vars_to_remove):\n        del new_variables[v]\n    new_variables.update(vars_to_replace)\n    new_variables.update(vars_to_create)\n    new_coord_names = (coord_names | set(vars_to_create)) - set(vars_to_remove)\n\n    return new_variables, new_coord_names", "response": "Splits the indexes of a single object into two sets of variables."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef as_dataset(obj):\n    if hasattr(obj, 'to_dataset'):\n        obj = obj.to_dataset()\n    if not isinstance(obj, Dataset):\n        obj = Dataset(obj)\n    return obj", "response": "Cast the given object to a Dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ipython_key_completions_(self):\n        return [key for key in self._dataset._ipython_key_completions_()\n                if key not in self._dataset._coord_names]", "response": "Provide method for the key - completer in IPython."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _set_init_vars_and_dims(self, data_vars, coords, compat):\n        both_data_and_coords = [k for k in data_vars if k in coords]\n        if both_data_and_coords:\n            raise ValueError('variables %r are found in both data_vars and '\n                             'coords' % both_data_and_coords)\n\n        if isinstance(coords, Dataset):\n            coords = coords.variables\n\n        variables, coord_names, dims = merge_data_and_coords(\n            data_vars, coords, compat=compat)\n\n        self._variables = variables\n        self._coord_names = coord_names\n        self._dims = dims", "response": "Set the initial value of Dataset variables and dimensions\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new dataset from the contents of a backends. DataStore object.", "response": "def load_store(cls, store, decoder=None):\n        \"\"\"Create a new dataset from the contents of a backends.*DataStore\n        object\n        \"\"\"\n        variables, attributes = store.load()\n        if decoder:\n            variables, attributes = decoder(variables, attributes)\n        obj = cls(variables, attrs=attributes)\n        obj._file_obj = store\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _persist_inplace(self, **kwargs):\n        # access .data to coerce everything to numpy or dask arrays\n        lazy_data = {k: v._data for k, v in self.variables.items()\n                     if isinstance(v._data, dask_array_type)}\n        if lazy_data:\n            import dask\n\n            # evaluate all the dask arrays simultaneously\n            evaluated_data = dask.persist(*lazy_data.values(), **kwargs)\n\n            for k, data in zip(lazy_data, evaluated_data):\n                self.variables[k].data = data\n\n        return self", "response": "Persist all the Dask arrays in memory"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef persist(self: T, **kwargs) -> T:\n        new = self.copy(deep=False)\n        return new._persist_inplace(**kwargs)", "response": "Persist the current state of the object to the distributed memory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconstruct a new object of the same class with the given variables coord_names dims attrs indexes and file_obj.", "response": "def _construct_direct(cls, variables, coord_names, dims, attrs=None,\n                          indexes=None, encoding=None, file_obj=None):\n        \"\"\"Shortcut around __init__ for internal use when we want to skip\n        costly validation\n        \"\"\"\n        obj = object.__new__(cls)\n        obj._variables = variables\n        obj._coord_names = coord_names\n        obj._dims = dims\n        obj._indexes = indexes\n        obj._attrs = attrs\n        obj._file_obj = file_obj\n        obj._encoding = encoding\n        obj._initialized = True\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreplace variables with recalculated dimensions.", "response": "def _replace_with_new_dims(  # type: ignore\n        self: T,\n        variables: 'OrderedDict[Any, Variable]' = None,\n        coord_names: set = None,\n        attrs: 'Optional[OrderedDict]' = __default,\n        indexes: 'Optional[OrderedDict[Any, pd.Index]]' = __default,\n        inplace: bool = False,\n    ) -> T:\n        \"\"\"Replace variables with recalculated dimensions.\"\"\"\n        dims = dict(calculate_dimensions(variables))\n        return self._replace(\n            variables, coord_names, dims, attrs, indexes, inplace=inplace)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a shallow copy of the current dataset.", "response": "def copy(self: T, deep: bool = False, data: Mapping = None) -> T:\n        \"\"\"Returns a copy of this dataset.\n\n        If `deep=True`, a deep copy is made of each of the component variables.\n        Otherwise, a shallow copy of each of the component variable is made, so\n        that the underlying memory region of the new dataset is the same as in\n        the original dataset.\n\n        Use `data` to create a new object with the same structure as\n        original but entirely new data.\n\n        Parameters\n        ----------\n        deep : bool, optional\n            Whether each component variable is loaded into memory and copied onto\n            the new object. Default is False.\n        data : dict-like, optional\n            Data to use in the new object. Each item in `data` must have same\n            shape as corresponding data variable in original. When `data` is\n            used, `deep` is ignored for the data variables and only used for\n            coords.\n\n        Returns\n        -------\n        object : Dataset\n            New object with dimensions, attributes, coordinates, name, encoding,\n            and optionally data copied from original.\n\n        Examples\n        --------\n\n        Shallow copy versus deep copy\n\n        >>> da = xr.DataArray(np.random.randn(2, 3))\n        >>> ds = xr.Dataset({'foo': da, 'bar': ('x', [-1, 2])},\n                            coords={'x': ['one', 'two']})\n        >>> ds.copy()\n        <xarray.Dataset>\n        Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n        Coordinates:\n        * x        (x) <U3 'one' 'two'\n        Dimensions without coordinates: dim_0, dim_1\n        Data variables:\n            foo      (dim_0, dim_1) float64 -0.8079 0.3897 -1.862 -0.6091 -1.051 -0.3003\n            bar      (x) int64 -1 2\n        >>> ds_0 = ds.copy(deep=False)\n        >>> ds_0['foo'][0, 0] = 7\n        >>> ds_0\n        <xarray.Dataset>\n        Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n        Coordinates:\n        * x        (x) <U3 'one' 'two'\n        Dimensions without coordinates: dim_0, dim_1\n        Data variables:\n            foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003\n            bar      (x) int64 -1 2\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n        Coordinates:\n        * x        (x) <U3 'one' 'two'\n        Dimensions without coordinates: dim_0, dim_1\n        Data variables:\n            foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003\n            bar      (x) int64 -1 2\n\n        Changing the data using the ``data`` argument maintains the\n        structure of the original object, but with the new data. Original\n        object is unaffected.\n\n        >>> ds.copy(data={'foo': np.arange(6).reshape(2, 3), 'bar': ['a', 'b']})\n        <xarray.Dataset>\n        Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n        Coordinates:\n        * x        (x) <U3 'one' 'two'\n        Dimensions without coordinates: dim_0, dim_1\n        Data variables:\n            foo      (dim_0, dim_1) int64 0 1 2 3 4 5\n            bar      (x) <U1 'a' 'b'\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n        Coordinates:\n        * x        (x) <U3 'one' 'two'\n        Dimensions without coordinates: dim_0, dim_1\n        Data variables:\n            foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003\n            bar      (x) int64 -1 2\n\n        See Also\n        --------\n        pandas.DataFrame.copy\n        \"\"\"  # noqa\n        if data is None:\n            variables = OrderedDict((k, v.copy(deep=deep))\n                                    for k, v in self._variables.items())\n        elif not utils.is_dict_like(data):\n            raise ValueError('Data must be dict-like')\n        else:\n            var_keys = set(self.data_vars.keys())\n            data_keys = set(data.keys())\n            keys_not_in_vars = data_keys - var_keys\n            if keys_not_in_vars:\n                raise ValueError(\n                    'Data must only contain variables in original '\n                    'dataset. Extra variables: {}'\n                    .format(keys_not_in_vars))\n            keys_missing_from_data = var_keys - data_keys\n            if keys_missing_from_data:\n                raise ValueError(\n                    'Data must contain all variables in original '\n                    'dataset. Data is missing {}'\n                    .format(keys_missing_from_data))\n            variables = OrderedDict((k, v.copy(deep=deep, data=data.get(k)))\n                                    for k, v in self._variables.items())\n\n        attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)\n\n        return self._replace(variables, attrs=attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a mapping of all MultiIndex levels and their corresponding coordinate name.", "response": "def _level_coords(self):\n        \"\"\"Return a mapping of all MultiIndex levels and their corresponding\n        coordinate name.\n        \"\"\"\n        level_coords = OrderedDict()\n        for name, index in self.indexes.items():\n            if isinstance(index, pd.MultiIndex):\n                level_names = index.names\n                (dim,) = self.variables[name].dims\n                level_coords.update({lname: dim for lname in level_names})\n        return level_coords"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new Dataset with the listed variables from this dataset and the all relevant coordinates. Skips all validation.", "response": "def _copy_listed(self: T, names) -> T:\n        \"\"\"Create a new Dataset with the listed variables from this dataset and\n        the all relevant coordinates. Skips all validation.\n        \"\"\"\n        variables = OrderedDict()  # type: OrderedDict[Any, Variable]\n        coord_names = set()\n        indexes = OrderedDict()  # type: OrderedDict[Any, pd.Index]\n\n        for name in names:\n            try:\n                variables[name] = self._variables[name]\n            except KeyError:\n                ref_name, var_name, var = _get_virtual_variable(\n                    self._variables, name, self._level_coords, self.dims)\n                variables[var_name] = var\n                if ref_name in self._coord_names or ref_name in self.dims:\n                    coord_names.add(var_name)\n                if (var_name,) == var.dims:\n                    indexes[var_name] = var.to_index()\n\n        needed_dims = set()  # type: set\n        for v in variables.values():\n            needed_dims.update(v.dims)\n\n        dims = dict((k, self.dims[k]) for k in needed_dims)\n\n        for k in self._coord_names:\n            if set(self.variables[k].dims) <= needed_dims:\n                variables[k] = self._variables[k]\n                coord_names.add(k)\n                if k in self.indexes:\n                    indexes[k] = self.indexes[k]\n\n        return self._replace(variables, coord_names, dims, indexes=indexes)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconstructs a DataArray by indexing this dataset", "response": "def _construct_dataarray(self, name) -> 'DataArray':\n        \"\"\"Construct a DataArray by indexing this dataset\n        \"\"\"\n        from .dataarray import DataArray\n\n        try:\n            variable = self._variables[name]\n        except KeyError:\n            _, name, variable = _get_virtual_variable(\n                self._variables, name, self._level_coords, self.dims)\n\n        needed_dims = set(variable.dims)\n\n        coords = OrderedDict()  # type: OrderedDict[Any, Variable]\n        for k in self.coords:\n            if set(self.variables[k].dims) <= needed_dims:\n                coords[k] = self.variables[k]\n\n        if self._indexes is None:\n            indexes = None\n        else:\n            indexes = OrderedDict((k, v) for k, v in self._indexes.items()\n                                  if k in coords)\n\n        return DataArray(variable, coords, name=name, indexes=indexes,\n                         fastpath=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting of items to look - up for key - completion", "response": "def _item_sources(self):\n        \"\"\"List of places to look-up items for key-completion\"\"\"\n        return [self.data_vars, self.coords, {d: self[d] for d in self.dims},\n                LevelCoordinatesSource(self)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmapping of pandas. Index objects used for label based indexing.", "response": "def indexes(self) -> 'Mapping[Any, pd.Index]':\n        \"\"\"Mapping of pandas.Index objects used for label based indexing\n        \"\"\"\n        if self._indexes is None:\n            self._indexes = default_indexes(self._variables, self._dims)\n        return Indexes(self._indexes)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_coords(self, names, inplace=None):\n        # TODO: allow inserting new coordinates with this method, like\n        # DataFrame.set_index?\n        # nb. check in self._variables, not self.data_vars to insure that the\n        # operation is idempotent\n        inplace = _check_inplace(inplace)\n        if isinstance(names, str):\n            names = [names]\n        self._assert_all_in_dataset(names)\n        obj = self if inplace else self.copy()\n        obj._coord_names.update(names)\n        return obj", "response": "Set the names of one or more variables as coordinates in this dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reset_coords(self, names=None, drop=False, inplace=None):\n        inplace = _check_inplace(inplace)\n        if names is None:\n            names = self._coord_names - set(self.dims)\n        else:\n            if isinstance(names, str):\n                names = [names]\n            self._assert_all_in_dataset(names)\n            bad_coords = set(names) & set(self.dims)\n            if bad_coords:\n                raise ValueError(\n                    'cannot remove index coordinates with reset_coords: %s'\n                    % bad_coords)\n        obj = self if inplace else self.copy()\n        obj._coord_names.difference_update(names)\n        if drop:\n            for name in names:\n                del obj._variables[name]\n        return obj", "response": "Reset the coordinates of the specified names of variables in this dataset to become variables."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dump_to_store(self, store, **kwargs):\n        from ..backends.api import dump_to_store\n        # TODO: rename and/or cleanup this method to make it more consistent\n        # with to_netcdf()\n        return dump_to_store(self, store, **kwargs)", "response": "Store dataset contents to a backends. DataStore object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_netcdf(self, path=None, mode='w', format=None, group=None,\n                  engine=None, encoding=None, unlimited_dims=None,\n                  compute=True):\n        \"\"\"Write dataset contents to a netCDF file.\n\n        Parameters\n        ----------\n        path : str, Path or file-like object, optional\n            Path to which to save this dataset. File-like objects are only\n            supported by the scipy engine. If no path is provided, this\n            function returns the resulting netCDF file as bytes; in this case,\n            we need to use scipy, which does not support netCDF version 4 (the\n            default format becomes NETCDF3_64BIT).\n        mode : {'w', 'a'}, optional\n            Write ('w') or append ('a') mode. If mode='w', any existing file at\n            this location will be overwritten. If mode='a', existing variables\n            will be overwritten.\n        format : {'NETCDF4', 'NETCDF4_CLASSIC', 'NETCDF3_64BIT',\n                  'NETCDF3_CLASSIC'}, optional\n            File format for the resulting netCDF file:\n\n            * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API\n              features.\n            * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only\n              netCDF 3 compatible API features.\n            * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,\n              which fully supports 2+ GB files, but is only compatible with\n              clients linked against netCDF version 3.6.0 or later.\n            * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not\n              handle 2+ GB files very well.\n\n            All formats are supported by the netCDF4-python library.\n            scipy.io.netcdf only supports the last two formats.\n\n            The default format is NETCDF4 if you are saving a file to disk and\n            have the netCDF4-python library available. Otherwise, xarray falls\n            back to using scipy to write netCDF files and defaults to the\n            NETCDF3_64BIT format (scipy does not support netCDF4).\n        group : str, optional\n            Path to the netCDF4 group in the given file to open (only works for\n            format='NETCDF4'). The group(s) will be created if necessary.\n        engine : {'netcdf4', 'scipy', 'h5netcdf'}, optional\n            Engine to use when writing netCDF files. If not provided, the\n            default engine is chosen based on available dependencies, with a\n            preference for 'netcdf4' if writing to a file on disk.\n        encoding : dict, optional\n            Nested dictionary with variable names as keys and dictionaries of\n            variable specific encodings as values, e.g.,\n            ``{'my_variable': {'dtype': 'int16', 'scale_factor': 0.1,\n                               'zlib': True}, ...}``\n\n            The `h5netcdf` engine supports both the NetCDF4-style compression\n            encoding parameters ``{'zlib': True, 'complevel': 9}`` and the h5py\n            ones ``{'compression': 'gzip', 'compression_opts': 9}``.\n            This allows using any compression plugin installed in the HDF5\n            library, e.g. LZF.\n\n        unlimited_dims : sequence of str, optional\n            Dimension(s) that should be serialized as unlimited dimensions.\n            By default, no dimensions are treated as unlimited dimensions.\n            Note that unlimited_dims may also be set via\n            ``dataset.encoding['unlimited_dims']``.\n        compute: boolean\n            If true compute immediately, otherwise return a\n            ``dask.delayed.Delayed`` object that can be computed later.\n        \"\"\"\n        if encoding is None:\n            encoding = {}\n        from ..backends.api import to_netcdf\n        return to_netcdf(self, path, mode, format=format, group=group,\n                         engine=engine, encoding=encoding,\n                         unlimited_dims=unlimited_dims,\n                         compute=compute)", "response": "Write the contents of this object to a netCDF file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_zarr(self, store=None, mode='w-', synchronizer=None, group=None,\n                encoding=None, compute=True, consolidated=False):\n        \"\"\"Write dataset contents to a zarr group.\n\n        .. note:: Experimental\n                  The Zarr backend is new and experimental. Please report any\n                  unexpected behavior via github issues.\n\n        Parameters\n        ----------\n        store : MutableMapping or str, optional\n            Store or path to directory in file system.\n        mode : {'w', 'w-'}\n            Persistence mode: 'w' means create (overwrite if exists);\n            'w-' means create (fail if exists).\n        synchronizer : object, optional\n            Array synchronizer\n        group : str, obtional\n            Group path. (a.k.a. `path` in zarr terminology.)\n        encoding : dict, optional\n            Nested dictionary with variable names as keys and dictionaries of\n            variable specific encodings as values, e.g.,\n            ``{'my_variable': {'dtype': 'int16', 'scale_factor': 0.1,}, ...}``\n        compute: bool, optional\n            If True compute immediately, otherwise return a\n            ``dask.delayed.Delayed`` object that can be computed later.\n        consolidated: bool, optional\n            If True, apply zarr's `consolidate_metadata` function to the store\n            after writing.\n\n        References\n        ----------\n        https://zarr.readthedocs.io/\n        \"\"\"\n        if encoding is None:\n            encoding = {}\n        if mode not in ['w', 'w-']:\n            # TODO: figure out how to handle 'r+' and 'a'\n            raise ValueError(\"The only supported options for mode are 'w' \"\n                             \"and 'w-'.\")\n        from ..backends.api import to_zarr\n        return to_zarr(self, store=store, mode=mode, synchronizer=synchronizer,\n                       group=group, encoding=encoding, compute=compute,\n                       consolidated=consolidated)", "response": "Write the contents of this object to a zarr group."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef info(self, buf=None):\n\n        if buf is None:  # pragma: no cover\n            buf = sys.stdout\n\n        lines = []\n        lines.append('xarray.Dataset {')\n        lines.append('dimensions:')\n        for name, size in self.dims.items():\n            lines.append('\\t{name} = {size} ;'.format(name=name, size=size))\n        lines.append('\\nvariables:')\n        for name, da in self.variables.items():\n            dims = ', '.join(da.dims)\n            lines.append('\\t{type} {name}({dims}) ;'.format(\n                type=da.dtype, name=name, dims=dims))\n            for k, v in da.attrs.items():\n                lines.append('\\t\\t{name}:{k} = {v} ;'.format(name=name, k=k,\n                                                             v=v))\n        lines.append('\\n// global attributes:')\n        for k, v in self.attrs.items():\n            lines.append('\\t:{k} = {v} ;'.format(k=k, v=v))\n        lines.append('}')\n\n        buf.write('\\n'.join(lines))", "response": "Prints out a summary of the current state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef chunks(self):\n        chunks = {}\n        for v in self.variables.values():\n            if v.chunks is not None:\n                for dim, c in zip(v.dims, v.chunks):\n                    if dim in chunks and c != chunks[dim]:\n                        raise ValueError('inconsistent chunks')\n                    chunks[dim] = c\n        return Frozen(SortedKeysDict(chunks))", "response": "Returns a Frozen dictionary of the block dimensions for this dataset s data or None if it s not a dask\n        array."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncoerces all arrays in this dataset into dask arrays with the given chunk sizes along each dimension.", "response": "def chunk(self, chunks=None, name_prefix='xarray-', token=None,\n              lock=False):\n        \"\"\"Coerce all arrays in this dataset into dask arrays with the given\n        chunks.\n\n        Non-dask arrays in this dataset will be converted to dask arrays. Dask\n        arrays will be rechunked to the given chunk sizes.\n\n        If neither chunks is not provided for one or more dimensions, chunk\n        sizes along that dimension will not be updated; non-dask arrays will be\n        converted into dask arrays with a single block.\n\n        Parameters\n        ----------\n        chunks : int or dict, optional\n            Chunk sizes along each dimension, e.g., ``5`` or\n            ``{'x': 5, 'y': 5}``.\n        name_prefix : str, optional\n            Prefix for the name of any new dask arrays.\n        token : str, optional\n            Token uniquely identifying this dataset.\n        lock : optional\n            Passed on to :py:func:`dask.array.from_array`, if the array is not\n            already as dask array.\n\n        Returns\n        -------\n        chunked : xarray.Dataset\n        \"\"\"\n        try:\n            from dask.base import tokenize\n        except ImportError:\n            # raise the usual error if dask is entirely missing\n            import dask  # noqa\n            raise ImportError('xarray requires dask version 0.9 or newer')\n\n        if isinstance(chunks, Number):\n            chunks = dict.fromkeys(self.dims, chunks)\n\n        if chunks is not None:\n            bad_dims = [d for d in chunks if d not in self.dims]\n            if bad_dims:\n                raise ValueError('some chunks keys are not dimensions on this '\n                                 'object: %s' % bad_dims)\n\n        def selkeys(dict_, keys):\n            if dict_ is None:\n                return None\n            return dict((d, dict_[d]) for d in keys if d in dict_)\n\n        def maybe_chunk(name, var, chunks):\n            chunks = selkeys(chunks, var.dims)\n            if not chunks:\n                chunks = None\n            if var.ndim > 0:\n                token2 = tokenize(name, token if token else var._data)\n                name2 = '%s%s-%s' % (name_prefix, name, token2)\n                return var.chunk(chunks, name=name2, lock=lock)\n            else:\n                return var\n\n        variables = OrderedDict([(k, maybe_chunk(k, v, chunks))\n                                 for k, v in self.variables.items()])\n        return self._replace(variables)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_indexers_coords_and_indexes(self, indexers):\n        from .dataarray import DataArray\n\n        coord_list = []\n        indexes = OrderedDict()\n        for k, v in indexers.items():\n            if isinstance(v, DataArray):\n                v_coords = v.coords\n                if v.dtype.kind == 'b':\n                    if v.ndim != 1:  # we only support 1-d boolean array\n                        raise ValueError(\n                            '{:d}d-boolean array is used for indexing along '\n                            'dimension {!r}, but only 1d boolean arrays are '\n                            'supported.'.format(v.ndim, k))\n                    # Make sure in case of boolean DataArray, its\n                    # coordinate also should be indexed.\n                    v_coords = v[v.values.nonzero()[0]].coords\n\n                coord_list.append({d: v_coords[d].variable for d in v.coords})\n                indexes.update(v.indexes)\n\n        # we don't need to call align() explicitly or check indexes for\n        # alignment, because merge_variables already checks for exact alignment\n        # between dimension coordinates\n        coords = merge_variables(coord_list)\n        assert_coordinate_consistent(self, coords)\n\n        # silently drop the conflicted variables.\n        attached_coords = OrderedDict(\n            (k, v) for k, v in coords.items() if k not in self._variables\n        )\n        attached_indexes = OrderedDict(\n            (k, v) for k, v in indexes.items() if k not in self._variables\n        )\n        return attached_coords, attached_indexes", "response": "Extract coordinates from indexers. indexers returns an OrderedDict mapping from coordinate name to the corresponding coordinate variable."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new Dataset with each array indexed along the specified dimension.", "response": "def isel(self, indexers=None, drop=False, **indexers_kwargs):\n        \"\"\"Returns a new dataset with each array indexed along the specified\n        dimension(s).\n\n        This method selects values from each array using its `__getitem__`\n        method, except this method does not require knowing the order of\n        each array's dimensions.\n\n        Parameters\n        ----------\n        indexers : dict, optional\n            A dict with keys matching dimensions and values given\n            by integers, slice objects or arrays.\n            indexer can be a integer, slice, array-like or DataArray.\n            If DataArrays are passed as indexers, xarray-style indexing will be\n            carried out. See :ref:`indexing` for the details.\n            One of indexers or indexers_kwargs must be provided.\n        drop : bool, optional\n            If ``drop=True``, drop coordinates variables indexed by integers\n            instead of making them scalar.\n        **indexers_kwarg : {dim: indexer, ...}, optional\n            The keyword arguments form of ``indexers``.\n            One of indexers or indexers_kwargs must be provided.\n\n        Returns\n        -------\n        obj : Dataset\n            A new Dataset with the same contents as this dataset, except each\n            array and dimension is indexed by the appropriate indexers.\n            If indexer DataArrays have coordinates that do not conflict with\n            this object, then these coordinates will be attached.\n            In general, each array's data will be a view of the array's data\n            in this dataset, unless vectorized indexing was triggered by using\n            an array indexer, in which case the data will be a copy.\n\n        See Also\n        --------\n        Dataset.sel\n        DataArray.isel\n        \"\"\"\n\n        indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'isel')\n\n        indexers_list = self._validate_indexers(indexers)\n\n        variables = OrderedDict()\n        indexes = OrderedDict()\n        for name, var in self.variables.items():\n            var_indexers = {k: v for k, v in indexers_list if k in var.dims}\n            if drop and name in var_indexers:\n                continue  # drop this variable\n\n            if name in self.indexes:\n                new_var, new_index = isel_variable_and_index(\n                    name, var, self.indexes[name], var_indexers)\n                if new_index is not None:\n                    indexes[name] = new_index\n            else:\n                new_var = var.isel(indexers=var_indexers)\n\n            variables[name] = new_var\n\n        coord_names = set(variables).intersection(self._coord_names)\n        selected = self._replace_with_new_dims(\n            variables, coord_names, indexes)\n\n        # Extract coordinates from indexers\n        coord_vars, new_indexes = (\n            selected._get_indexers_coords_and_indexes(indexers))\n        variables.update(coord_vars)\n        indexes.update(new_indexes)\n        coord_names = (set(variables)\n                       .intersection(self._coord_names)\n                       .union(coord_vars))\n        return self._replace_with_new_dims(\n            variables, coord_names, indexes=indexes)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sel(self, indexers=None, method=None, tolerance=None, drop=False,\n            **indexers_kwargs):\n        \"\"\"Returns a new dataset with each array indexed by tick labels\n        along the specified dimension(s).\n\n        In contrast to `Dataset.isel`, indexers for this method should use\n        labels instead of integers.\n\n        Under the hood, this method is powered by using pandas's powerful Index\n        objects. This makes label based indexing essentially just as fast as\n        using integer indexing.\n\n        It also means this method uses pandas's (well documented) logic for\n        indexing. This means you can use string shortcuts for datetime indexes\n        (e.g., '2000-01' to select all values in January 2000). It also means\n        that slices are treated as inclusive of both the start and stop values,\n        unlike normal Python indexing.\n\n        Parameters\n        ----------\n        indexers : dict, optional\n            A dict with keys matching dimensions and values given\n            by scalars, slices or arrays of tick labels. For dimensions with\n            multi-index, the indexer may also be a dict-like object with keys\n            matching index level names.\n            If DataArrays are passed as indexers, xarray-style indexing will be\n            carried out. See :ref:`indexing` for the details.\n            One of indexers or indexers_kwargs must be provided.\n        method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n            Method to use for inexact matches (requires pandas>=0.16):\n\n            * None (default): only exact matches\n            * pad / ffill: propagate last valid index value forward\n            * backfill / bfill: propagate next valid index value backward\n            * nearest: use nearest valid index value\n        tolerance : optional\n            Maximum distance between original and new labels for inexact\n            matches. The values of the index at the matching locations must\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n            Requires pandas>=0.17.\n        drop : bool, optional\n            If ``drop=True``, drop coordinates variables in `indexers` instead\n            of making them scalar.\n        **indexers_kwarg : {dim: indexer, ...}, optional\n            The keyword arguments form of ``indexers``.\n            One of indexers or indexers_kwargs must be provided.\n\n        Returns\n        -------\n        obj : Dataset\n            A new Dataset with the same contents as this dataset, except each\n            variable and dimension is indexed by the appropriate indexers.\n            If indexer DataArrays have coordinates that do not conflict with\n            this object, then these coordinates will be attached.\n            In general, each array's data will be a view of the array's data\n            in this dataset, unless vectorized indexing was triggered by using\n            an array indexer, in which case the data will be a copy.\n\n\n        See Also\n        --------\n        Dataset.isel\n        DataArray.sel\n        \"\"\"\n        indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'sel')\n        pos_indexers, new_indexes = remap_label_indexers(\n            self, indexers=indexers, method=method, tolerance=tolerance)\n        result = self.isel(indexers=pos_indexers, drop=drop)\n        return result._overwrite_indexes(new_indexes)", "response": "Returns a new dataset with each array indexed by tick labels and optionally by tick labels."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef isel_points(self, dim='points', **indexers):\n        # type: (...) -> Dataset\n        \"\"\"Returns a new dataset with each array indexed pointwise along the\n        specified dimension(s).\n\n        This method selects pointwise values from each array and is akin to\n        the NumPy indexing behavior of `arr[[0, 1], [0, 1]]`, except this\n        method does not require knowing the order of each array's dimensions.\n\n        Parameters\n        ----------\n        dim : str or DataArray or pandas.Index or other list-like object, optional\n            Name of the dimension to concatenate along. If dim is provided as a\n            string, it must be a new dimension name, in which case it is added\n            along axis=0. If dim is provided as a DataArray or Index or\n            list-like object, its name, which must not be present in the\n            dataset, is used as the dimension to concatenate along and the\n            values are added as a coordinate.\n        **indexers : {dim: indexer, ...}\n            Keyword arguments with names matching dimensions and values given\n            by array-like objects. All indexers must be the same length and\n            1 dimensional.\n\n        Returns\n        -------\n        obj : Dataset\n            A new Dataset with the same contents as this dataset, except each\n            array and dimension is indexed by the appropriate indexers. With\n            pointwise indexing, the new Dataset will always be a copy of the\n            original.\n\n        See Also\n        --------\n        Dataset.sel\n        Dataset.isel\n        Dataset.sel_points\n        DataArray.isel_points\n        \"\"\"  # noqa\n        warnings.warn('Dataset.isel_points is deprecated: use Dataset.isel()'\n                      'instead.', DeprecationWarning, stacklevel=2)\n\n        indexer_dims = set(indexers)\n\n        def take(variable, slices):\n            # Note: remove helper function when once when numpy\n            # supports vindex https://github.com/numpy/numpy/pull/6075\n            if hasattr(variable.data, 'vindex'):\n                # Special case for dask backed arrays to use vectorised list\n                # indexing\n                sel = variable.data.vindex[slices]\n            else:\n                # Otherwise assume backend is numpy array with 'fancy' indexing\n                sel = variable.data[slices]\n            return sel\n\n        def relevant_keys(mapping):\n            return [k for k, v in mapping.items()\n                    if any(d in indexer_dims for d in v.dims)]\n\n        coords = relevant_keys(self.coords)\n        indexers = [(k, np.asarray(v))  # type: ignore\n                    for k, v in indexers.items()]\n        indexers_dict = dict(indexers)\n        non_indexed_dims = set(self.dims) - indexer_dims\n        non_indexed_coords = set(self.coords) - set(coords)\n\n        # All the indexers should be iterables\n        # Check that indexers are valid dims, integers, and 1D\n        for k, v in indexers:\n            if k not in self.dims:\n                raise ValueError(\"dimension %s does not exist\" % k)\n            if v.dtype.kind != 'i':  # type: ignore\n                raise TypeError('Indexers must be integers')\n            if v.ndim != 1:  # type: ignore\n                raise ValueError('Indexers must be 1 dimensional')\n\n        # all the indexers should have the same length\n        lengths = set(len(v) for k, v in indexers)\n        if len(lengths) > 1:\n            raise ValueError('All indexers must be the same length')\n\n        # Existing dimensions are not valid choices for the dim argument\n        if isinstance(dim, str):\n            if dim in self.dims:\n                # dim is an invalid string\n                raise ValueError('Existing dimension names are not valid '\n                                 'choices for the dim argument in sel_points')\n\n        elif hasattr(dim, 'dims'):\n            # dim is a DataArray or Coordinate\n            if dim.name in self.dims:\n                # dim already exists\n                raise ValueError('Existing dimensions are not valid choices '\n                                 'for the dim argument in sel_points')\n\n        # Set the new dim_name, and optionally the new dim coordinate\n        # dim is either an array-like or a string\n        if not utils.is_scalar(dim):\n            # dim is array like get name or assign 'points', get as variable\n            dim_name = 'points' if not hasattr(dim, 'name') else dim.name\n            dim_coord = as_variable(dim, name=dim_name)\n        else:\n            # dim is a string\n            dim_name = dim\n            dim_coord = None  # type: ignore\n\n        reordered = self.transpose(\n            *(list(indexer_dims) + list(non_indexed_dims)))\n\n        variables = OrderedDict()  # type: ignore\n\n        for name, var in reordered.variables.items():\n            if name in indexers_dict or any(\n                    d in indexer_dims for d in var.dims):\n                # slice if var is an indexer or depends on an indexed dim\n                slc = [indexers_dict[k]\n                       if k in indexers_dict\n                       else slice(None) for k in var.dims]\n\n                var_dims = [dim_name] + [d for d in var.dims\n                                         if d in non_indexed_dims]\n                selection = take(var, tuple(slc))\n                var_subset = type(var)(var_dims, selection, var.attrs)\n                variables[name] = var_subset\n            else:\n                # If not indexed just add it back to variables or coordinates\n                variables[name] = var\n\n        coord_names = (set(coords) & set(variables)) | non_indexed_coords\n\n        dset = self._replace_vars_and_dims(variables, coord_names=coord_names)\n        # Add the dim coord to the new dset. Must be done after creation\n        # because_replace_vars_and_dims can only access existing coords,\n        # not add new ones\n        if dim_coord is not None:\n            dset.coords[dim_name] = dim_coord\n        return dset", "response": "Returns a new Dataset with each array indexed pointwise along the\n            specified dimension."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new Dataset with each array indexed pointwise by tick labels along the specified dimension.", "response": "def sel_points(self, dim='points', method=None, tolerance=None,\n                   **indexers):\n        \"\"\"Returns a new dataset with each array indexed pointwise by tick\n        labels along the specified dimension(s).\n\n        In contrast to `Dataset.isel_points`, indexers for this method should\n        use labels instead of integers.\n\n        In contrast to `Dataset.sel`, this method selects points along the\n        diagonal of multi-dimensional arrays, not the intersection.\n\n        Parameters\n        ----------\n        dim : str or DataArray or pandas.Index or other list-like object, optional\n            Name of the dimension to concatenate along. If dim is provided as a\n            string, it must be a new dimension name, in which case it is added\n            along axis=0. If dim is provided as a DataArray or Index or\n            list-like object, its name, which must not be present in the\n            dataset, is used as the dimension to concatenate along and the\n            values are added as a coordinate.\n        method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n            Method to use for inexact matches (requires pandas>=0.16):\n\n            * None (default): only exact matches\n            * pad / ffill: propagate last valid index value forward\n            * backfill / bfill: propagate next valid index value backward\n            * nearest: use nearest valid index value\n        tolerance : optional\n            Maximum distance between original and new labels for inexact\n            matches. The values of the index at the matching locations must\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n            Requires pandas>=0.17.\n        **indexers : {dim: indexer, ...}\n            Keyword arguments with names matching dimensions and values given\n            by array-like objects. All indexers must be the same length and\n            1 dimensional.\n\n        Returns\n        -------\n        obj : Dataset\n            A new Dataset with the same contents as this dataset, except each\n            array and dimension is indexed by the appropriate indexers. With\n            pointwise indexing, the new Dataset will always be a copy of the\n            original.\n\n        See Also\n        --------\n        Dataset.sel\n        Dataset.isel\n        Dataset.isel_points\n        DataArray.sel_points\n        \"\"\"  # noqa\n        warnings.warn('Dataset.sel_points is deprecated: use Dataset.sel()'\n                      'instead.', DeprecationWarning, stacklevel=2)\n\n        pos_indexers, _ = indexing.remap_label_indexers(\n            self, indexers, method=method, tolerance=tolerance\n        )\n        return self.isel_points(dim=dim, **pos_indexers)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reindex_like(self, other, method=None, tolerance=None, copy=True):\n        indexers = alignment.reindex_like_indexers(self, other)\n        return self.reindex(indexers=indexers, method=method, copy=copy,\n                            tolerance=tolerance)", "response": "Conform this object onto the indexes of another object filling missing values with NaN."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reindex(self, indexers=None, method=None, tolerance=None, copy=True,\n                **indexers_kwargs):\n        \"\"\"Conform this object onto a new set of indexes, filling in\n        missing values with NaN.\n\n        Parameters\n        ----------\n        indexers : dict. optional\n            Dictionary with keys given by dimension names and values given by\n            arrays of coordinates tick labels. Any mis-matched coordinate\n            values will be filled in with NaN, and any mis-matched dimension\n            names will simply be ignored.\n            One of indexers or indexers_kwargs must be provided.\n        method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n            Method to use for filling index values in ``indexers`` not found in\n            this dataset:\n\n            * None (default): don't fill gaps\n            * pad / ffill: propagate last valid index value forward\n            * backfill / bfill: propagate next valid index value backward\n            * nearest: use nearest valid index value (requires pandas>=0.16)\n        tolerance : optional\n            Maximum distance between original and new labels for inexact\n            matches. The values of the index at the matching locations must\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n            Requires pandas>=0.17.\n        copy : bool, optional\n            If ``copy=True``, data in the return value is always copied. If\n            ``copy=False`` and reindexing is unnecessary, or can be performed\n            with only slice operations, then the output may share memory with\n            the input. In either case, a new xarray object is always returned.\n        **indexers_kwarg : {dim: indexer, ...}, optional\n            Keyword arguments in the same form as ``indexers``.\n            One of indexers or indexers_kwargs must be provided.\n\n        Returns\n        -------\n        reindexed : Dataset\n            Another dataset, with this dataset's data but replaced coordinates.\n\n        See Also\n        --------\n        Dataset.reindex_like\n        align\n        pandas.Index.get_indexer\n        \"\"\"\n        indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs,\n                                               'reindex')\n\n        bad_dims = [d for d in indexers if d not in self.dims]\n        if bad_dims:\n            raise ValueError('invalid reindex dimensions: %s' % bad_dims)\n\n        variables, indexes = alignment.reindex_variables(\n            self.variables, self.sizes, self.indexes, indexers, method,\n            tolerance, copy=copy)\n        coord_names = set(self._coord_names)\n        coord_names.update(indexers)\n        return self._replace_with_new_dims(\n            variables, coord_names, indexes=indexes)", "response": "Conform this object onto a new set of indexes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef interp(self, coords=None, method='linear', assume_sorted=False,\n               kwargs={}, **coords_kwargs):\n        \"\"\" Multidimensional interpolation of Dataset.\n\n        Parameters\n        ----------\n        coords : dict, optional\n            Mapping from dimension names to the new coordinates.\n            New coordinate can be a scalar, array-like or DataArray.\n            If DataArrays are passed as new coordates, their dimensions are\n            used for the broadcasting.\n        method: string, optional.\n            {'linear', 'nearest'} for multidimensional array,\n            {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}\n            for 1-dimensional array. 'linear' is used by default.\n        assume_sorted: boolean, optional\n            If False, values of coordinates that are interpolated over can be\n            in any order and they are sorted first. If True, interpolated\n            coordinates are assumed to be an array of monotonically increasing\n            values.\n        kwargs: dictionary, optional\n            Additional keyword passed to scipy's interpolator.\n        **coords_kwarg : {dim: coordinate, ...}, optional\n            The keyword arguments form of ``coords``.\n            One of coords or coords_kwargs must be provided.\n\n        Returns\n        -------\n        interpolated: xr.Dataset\n            New dataset on the new coordinates.\n\n        Notes\n        -----\n        scipy is required.\n\n        See Also\n        --------\n        scipy.interpolate.interp1d\n        scipy.interpolate.interpn\n        \"\"\"\n        from . import missing\n\n        coords = either_dict_or_kwargs(coords, coords_kwargs, 'interp')\n        indexers = OrderedDict(self._validate_indexers(coords))\n\n        obj = self if assume_sorted else self.sortby([k for k in coords])\n\n        def maybe_variable(obj, k):\n            # workaround to get variable for dimension without coordinate.\n            try:\n                return obj._variables[k]\n            except KeyError:\n                return as_variable((k, range(obj.dims[k])))\n\n        def _validate_interp_indexer(x, new_x):\n            # In the case of datetimes, the restrictions placed on indexers\n            # used with interp are stronger than those which are placed on\n            # isel, so we need an additional check after _validate_indexers.\n            if (_contains_datetime_like_objects(x) and\n                    not _contains_datetime_like_objects(new_x)):\n                raise TypeError('When interpolating over a datetime-like '\n                                'coordinate, the coordinates to '\n                                'interpolate to must be either datetime '\n                                'strings or datetimes. '\n                                'Instead got\\n{}'.format(new_x))\n            else:\n                return (x, new_x)\n\n        variables = OrderedDict()\n        for name, var in obj._variables.items():\n            if name not in indexers:\n                if var.dtype.kind in 'uifc':\n                    var_indexers = {\n                        k: _validate_interp_indexer(maybe_variable(obj, k), v)\n                        for k, v in indexers.items()\n                        if k in var.dims\n                    }\n                    variables[name] = missing.interp(\n                        var, var_indexers, method, **kwargs)\n                elif all(d not in indexers for d in var.dims):\n                    # keep unrelated object array\n                    variables[name] = var\n\n        coord_names = set(variables).intersection(obj._coord_names)\n        indexes = OrderedDict(\n            (k, v) for k, v in obj.indexes.items() if k not in indexers)\n        selected = self._replace_with_new_dims(\n            variables.copy(), coord_names, indexes=indexes)\n\n        # attach indexer as coordinate\n        variables.update(indexers)\n        indexes.update(\n            (k, v.to_index()) for k, v in indexers.items() if v.dims == (k,)\n        )\n\n        # Extract coordinates from indexers\n        coord_vars, new_indexes = (\n            selected._get_indexers_coords_and_indexes(coords))\n        variables.update(coord_vars)\n        indexes.update(new_indexes)\n\n        coord_names = (set(variables)\n                       .intersection(obj._coord_names)\n                       .union(coord_vars))\n        return self._replace_with_new_dims(\n            variables, coord_names, indexes=indexes)", "response": "Interpolate a multidimensional array over a set of coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninterpolate this object onto the coordinates of another object.", "response": "def interp_like(self, other, method='linear', assume_sorted=False,\n                    kwargs={}):\n        \"\"\"Interpolate this object onto the coordinates of another object,\n        filling the out of range values with NaN.\n\n        Parameters\n        ----------\n        other : Dataset or DataArray\n            Object with an 'indexes' attribute giving a mapping from dimension\n            names to an 1d array-like, which provides coordinates upon\n            which to index the variables in this dataset.\n        method: string, optional.\n            {'linear', 'nearest'} for multidimensional array,\n            {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}\n            for 1-dimensional array. 'linear' is used by default.\n        assume_sorted: boolean, optional\n            If False, values of coordinates that are interpolated over can be\n            in any order and they are sorted first. If True, interpolated\n            coordinates are assumed to be an array of monotonically increasing\n            values.\n        kwargs: dictionary, optional\n            Additional keyword passed to scipy's interpolator.\n\n        Returns\n        -------\n        interpolated: xr.Dataset\n            Another dataset by interpolating this dataset's data along the\n            coordinates of the other object.\n\n        Notes\n        -----\n        scipy is required.\n        If the dataset has object-type coordinates, reindex is used for these\n        coordinates instead of the interpolation.\n\n        See Also\n        --------\n        Dataset.interp\n        Dataset.reindex_like\n        \"\"\"\n        coords = alignment.reindex_like_indexers(self, other)\n\n        numeric_coords = OrderedDict()\n        object_coords = OrderedDict()\n        for k, v in coords.items():\n            if v.dtype.kind in 'uifcMm':\n                numeric_coords[k] = v\n            else:\n                object_coords[k] = v\n\n        ds = self\n        if object_coords:\n            # We do not support interpolation along object coordinate.\n            # reindex instead.\n            ds = self.reindex(object_coords)\n        return ds.interp(numeric_coords, method, assume_sorted, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rename(self, name_dict=None, inplace=None, **names):\n        # TODO: add separate rename_vars and rename_dims methods.\n        inplace = _check_inplace(inplace)\n        name_dict = either_dict_or_kwargs(name_dict, names, 'rename')\n        for k, v in name_dict.items():\n            if k not in self and k not in self.dims:\n                raise ValueError(\"cannot rename %r because it is not a \"\n                                 \"variable or dimension in this dataset\" % k)\n\n        variables, coord_names, dims, indexes = self._rename_all(\n            name_dict=name_dict, dim_dict=name_dict)\n        return self._replace(variables, coord_names, dims=dims,\n                             indexes=indexes, inplace=inplace)", "response": "Returns a new Dataset object with renamed variables and dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef swap_dims(self, dims_dict, inplace=None):\n        # TODO: deprecate this method in favor of a (less confusing)\n        # rename_dims() method that only renames dimensions.\n        inplace = _check_inplace(inplace)\n        for k, v in dims_dict.items():\n            if k not in self.dims:\n                raise ValueError('cannot swap from dimension %r because it is '\n                                 'not an existing dimension' % k)\n            if self.variables[v].dims != (k,):\n                raise ValueError('replacement dimension %r is not a 1D '\n                                 'variable along the old dimension %r'\n                                 % (v, k))\n\n        result_dims = set(dims_dict.get(dim, dim) for dim in self.dims)\n\n        coord_names = self._coord_names.copy()\n        coord_names.update(dims_dict.values())\n\n        variables = OrderedDict()\n        indexes = OrderedDict()\n        for k, v in self.variables.items():\n            dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)\n            if k in result_dims:\n                var = v.to_index_variable()\n                if k in self.indexes:\n                    indexes[k] = self.indexes[k]\n                else:\n                    indexes[k] = var.to_index()\n            else:\n                var = v.to_base_variable()\n            var.dims = dims\n            variables[k] = var\n\n        return self._replace_with_new_dims(variables, coord_names,\n                                           indexes=indexes, inplace=inplace)", "response": "Returns a new Dataset containing swapped dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new object with an additional dimension inserted at the corresponding position in the array.", "response": "def expand_dims(self, dim=None, axis=None, **dim_kwargs):\n        \"\"\"Return a new object with an additional axis (or axes) inserted at\n        the corresponding position in the array shape.\n\n        If dim is already a scalar coordinate, it will be promoted to a 1D\n        coordinate consisting of a single value.\n\n        Parameters\n        ----------\n        dim : str, sequence of str, dict, or None\n            Dimensions to include on the new variable.\n            If provided as str or sequence of str, then dimensions are inserted\n            with length 1. If provided as a dict, then the keys are the new\n            dimensions and the values are either integers (giving the length of\n            the new dimensions) or sequence/ndarray (giving the coordinates of\n            the new dimensions). **WARNING** for python 3.5, if ``dim`` is\n            dict-like, then it must be an ``OrderedDict``. This is to ensure\n            that the order in which the dims are given is maintained.\n        axis : integer, list (or tuple) of integers, or None\n            Axis position(s) where new axis is to be inserted (position(s) on\n            the result array). If a list (or tuple) of integers is passed,\n            multiple axes are inserted. In this case, dim arguments should be\n            same length list. If axis=None is passed, all the axes will be\n            inserted to the start of the result array.\n        **dim_kwargs : int or sequence/ndarray\n            The keywords are arbitrary dimensions being inserted and the values\n            are either the lengths of the new dims (if int is given), or their\n            coordinates. Note, this is an alternative to passing a dict to the\n            dim kwarg and will only be used if dim is None. **WARNING** for\n            python 3.5 ``dim_kwargs`` is not available.\n\n        Returns\n        -------\n        expanded : same type as caller\n            This object, but with an additional dimension(s).\n        \"\"\"\n        if isinstance(dim, int):\n            raise TypeError('dim should be str or sequence of strs or dict')\n        elif isinstance(dim, str):\n            dim = OrderedDict(((dim, 1),))\n        elif isinstance(dim, (list, tuple)):\n            if len(dim) != len(set(dim)):\n                raise ValueError('dims should not contain duplicate values.')\n            dim = OrderedDict(((d, 1) for d in dim))\n\n        # TODO: get rid of the below code block when python 3.5 is no longer\n        #   supported.\n        python36_plus = sys.version_info[0] == 3 and sys.version_info[1] > 5\n        not_ordereddict = dim is not None and not isinstance(dim, OrderedDict)\n        if not python36_plus and not_ordereddict:\n            raise TypeError(\"dim must be an OrderedDict for python <3.6\")\n        elif not python36_plus and dim_kwargs:\n            raise ValueError(\"dim_kwargs isn't available for python <3.6\")\n\n        dim = either_dict_or_kwargs(dim, dim_kwargs, 'expand_dims')\n\n        if axis is not None and not isinstance(axis, (list, tuple)):\n            axis = [axis]\n\n        if axis is None:\n            axis = list(range(len(dim)))\n\n        if len(dim) != len(axis):\n            raise ValueError('lengths of dim and axis should be identical.')\n        for d in dim:\n            if d in self.dims:\n                raise ValueError(\n                    'Dimension {dim} already exists.'.format(dim=d))\n            if (d in self._variables and\n                    not utils.is_scalar(self._variables[d])):\n                raise ValueError(\n                    '{dim} already exists as coordinate or'\n                    ' variable name.'.format(dim=d))\n\n        variables = OrderedDict()\n        coord_names = self._coord_names.copy()\n        # If dim is a dict, then ensure that the values are either integers\n        # or iterables.\n        for k, v in dim.items():\n            if hasattr(v, \"__iter__\"):\n                # If the value for the new dimension is an iterable, then\n                # save the coordinates to the variables dict, and set the\n                # value within the dim dict to the length of the iterable\n                # for later use.\n                variables[k] = xr.IndexVariable((k,), v)\n                coord_names.add(k)\n                dim[k] = variables[k].size\n            elif isinstance(v, int):\n                pass  # Do nothing if the dimensions value is just an int\n            else:\n                raise TypeError('The value of new dimension {k} must be '\n                                'an iterable or an int'.format(k=k))\n\n        for k, v in self._variables.items():\n            if k not in dim:\n                if k in coord_names:  # Do not change coordinates\n                    variables[k] = v\n                else:\n                    result_ndim = len(v.dims) + len(axis)\n                    for a in axis:\n                        if a < -result_ndim or result_ndim - 1 < a:\n                            raise IndexError(\n                                'Axis {a} is out of bounds of the expanded'\n                                ' dimension size {dim}.'.format(\n                                    a=a, v=k, dim=result_ndim))\n\n                    axis_pos = [a if a >= 0 else result_ndim + a\n                                for a in axis]\n                    if len(axis_pos) != len(set(axis_pos)):\n                        raise ValueError('axis should not contain duplicate'\n                                         ' values.')\n                    # We need to sort them to make sure `axis` equals to the\n                    # axis positions of the result array.\n                    zip_axis_dim = sorted(zip(axis_pos, dim.items()))\n\n                    all_dims = list(zip(v.dims, v.shape))\n                    for d, c in zip_axis_dim:\n                        all_dims.insert(d, c)\n                    all_dims = OrderedDict(all_dims)\n\n                    variables[k] = v.set_dims(all_dims)\n            else:\n                # If dims includes a label of a non-dimension coordinate,\n                # it will be promoted to a 1D coordinate with a single value.\n                variables[k] = v.set_dims(k)\n\n        new_dims = self._dims.copy()\n        new_dims.update(dim)\n\n        return self._replace_vars_and_dims(\n            variables, dims=new_dims, coord_names=coord_names)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_index(self, indexes=None, append=False, inplace=None,\n                  **indexes_kwargs):\n        \"\"\"Set Dataset (multi-)indexes using one or more existing coordinates\n        or variables.\n\n        Parameters\n        ----------\n        indexes : {dim: index, ...}\n            Mapping from names matching dimensions and values given\n            by (lists of) the names of existing coordinates or variables to set\n            as new (multi-)index.\n        append : bool, optional\n            If True, append the supplied index(es) to the existing index(es).\n            Otherwise replace the existing index(es) (default).\n        inplace : bool, optional\n            If True, set new index(es) in-place. Otherwise, return a new\n            Dataset object.\n        **indexes_kwargs: optional\n            The keyword arguments form of ``indexes``.\n            One of indexes or indexes_kwargs must be provided.\n\n        Returns\n        -------\n        obj : Dataset\n            Another dataset, with this dataset's data but replaced coordinates.\n\n        See Also\n        --------\n        Dataset.reset_index\n        Dataset.swap_dims\n        \"\"\"\n        inplace = _check_inplace(inplace)\n        indexes = either_dict_or_kwargs(indexes, indexes_kwargs, 'set_index')\n        variables, coord_names = merge_indexes(indexes, self._variables,\n                                               self._coord_names,\n                                               append=append)\n        return self._replace_vars_and_dims(variables, coord_names=coord_names,\n                                           inplace=inplace)", "response": "Set the index of the data for the given locations."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reset_index(self, dims_or_levels, drop=False, inplace=None):\n        inplace = _check_inplace(inplace)\n        variables, coord_names = split_indexes(dims_or_levels, self._variables,\n                                               self._coord_names,\n                                               self._level_coords, drop=drop)\n        return self._replace_vars_and_dims(variables, coord_names=coord_names,\n                                           inplace=inplace)", "response": "Reset the index of the specified dimension or multi - index levels."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reorder_levels(self, dim_order=None, inplace=None,\n                       **dim_order_kwargs):\n        \"\"\"Rearrange index levels using input order.\n\n        Parameters\n        ----------\n        dim_order : optional\n            Mapping from names matching dimensions and values given\n            by lists representing new level orders. Every given dimension\n            must have a multi-index.\n        inplace : bool, optional\n            If True, modify the dataset in-place. Otherwise, return a new\n            DataArray object.\n        **dim_order_kwargs: optional\n            The keyword arguments form of ``dim_order``.\n            One of dim_order or dim_order_kwargs must be provided.\n\n        Returns\n        -------\n        obj : Dataset\n            Another dataset, with this dataset's data but replaced\n            coordinates.\n        \"\"\"\n        inplace = _check_inplace(inplace)\n        dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs,\n                                          'reorder_levels')\n        replace_variables = {}\n        for dim, order in dim_order.items():\n            coord = self._variables[dim]\n            index = coord.to_index()\n            if not isinstance(index, pd.MultiIndex):\n                raise ValueError(\"coordinate %r has no MultiIndex\" % dim)\n            replace_variables[dim] = IndexVariable(coord.dims,\n                                                   index.reorder_levels(order))\n        variables = self._variables.copy()\n        variables.update(replace_variables)\n        return self._replace_vars_and_dims(variables, inplace=inplace)", "response": "Rearrange index levels using input order."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self, other, inplace=None):\n        inplace = _check_inplace(inplace, default=True)\n        variables, coord_names, dims = dataset_update_method(self, other)\n\n        return self._replace_vars_and_dims(variables, coord_names, dims,\n                                           inplace=inplace)", "response": "Update this dataset s variables with those from another dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmerge two set of exception - related datasets into a single dataset.", "response": "def merge(self, other, inplace=None, overwrite_vars=frozenset(),\n              compat='no_conflicts', join='outer'):\n        \"\"\"Merge the arrays of two datasets into a single dataset.\n\n        This method generally not allow for overriding data, with the exception\n        of attributes, which are ignored on the second dataset. Variables with\n        the same name are checked for conflicts via the equals or identical\n        methods.\n\n        Parameters\n        ----------\n        other : Dataset or castable to Dataset\n            Dataset or variables to merge with this dataset.\n        inplace : bool, optional\n            If True, merge the other dataset into this dataset in-place.\n            Otherwise, return a new dataset object.\n        overwrite_vars : str or sequence, optional\n            If provided, update variables of these name(s) without checking for\n            conflicts in this dataset.\n        compat : {'broadcast_equals', 'equals', 'identical',\n                  'no_conflicts'}, optional\n            String indicating how to compare variables of the same name for\n            potential conflicts:\n            - 'broadcast_equals': all values must be equal when variables are\n              broadcast against each other to ensure common dimensions.\n            - 'equals': all values and dimensions must be the same.\n            - 'identical': all values, dimensions and attributes must be the\n              same.\n            - 'no_conflicts': only values which are not null in both datasets\n              must be equal. The returned dataset then contains the combination\n              of all non-null values.\n        join : {'outer', 'inner', 'left', 'right', 'exact'}, optional\n            Method for joining ``self`` and ``other`` along shared dimensions:\n\n            - 'outer': use the union of the indexes\n            - 'inner': use the intersection of the indexes\n            - 'left': use indexes from ``self``\n            - 'right': use indexes from ``other``\n            - 'exact': error instead of aligning non-equal indexes\n\n        Returns\n        -------\n        merged : Dataset\n            Merged dataset.\n\n        Raises\n        ------\n        MergeError\n            If any variables conflict (see ``compat``).\n        \"\"\"\n        inplace = _check_inplace(inplace)\n        variables, coord_names, dims = dataset_merge_method(\n            self, other, overwrite_vars=overwrite_vars, compat=compat,\n            join=join)\n\n        return self._replace_vars_and_dims(variables, coord_names, dims,\n                                           inplace=inplace)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndrops variables or index labels from this dataset.", "response": "def drop(self, labels, dim=None):\n        \"\"\"Drop variables or index labels from this dataset.\n\n        Parameters\n        ----------\n        labels : scalar or list of scalars\n            Name(s) of variables or index labels to drop.\n        dim : None or str, optional\n            Dimension along which to drop index labels. By default (if\n            ``dim is None``), drops variables rather than index labels.\n\n        Returns\n        -------\n        dropped : Dataset\n        \"\"\"\n        if utils.is_scalar(labels):\n            labels = [labels]\n        if dim is None:\n            return self._drop_vars(labels)\n        else:\n            try:\n                index = self.indexes[dim]\n            except KeyError:\n                raise ValueError(\n                    'dimension %r does not have coordinate labels' % dim)\n            new_index = index.drop(labels)\n            return self.loc[{dim: new_index}]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef drop_dims(self, drop_dims):\n        if utils.is_scalar(drop_dims):\n            drop_dims = [drop_dims]\n\n        missing_dimensions = [d for d in drop_dims if d not in self.dims]\n        if missing_dimensions:\n            raise ValueError('Dataset does not contain the dimensions: %s'\n                             % missing_dimensions)\n\n        drop_vars = set(k for k, v in self._variables.items()\n                        for d in v.dims if d in drop_dims)\n\n        variables = OrderedDict((k, v) for k, v in self._variables.items()\n                                if k not in drop_vars)\n        coord_names = set(k for k in self._coord_names if k in variables)\n\n        return self._replace_with_new_dims(variables, coord_names)", "response": "Drop dimensions and associated variables from this dataset."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a new Dataset object with all array dimensions transposed.", "response": "def transpose(self, *dims):\n        \"\"\"Return a new Dataset object with all array dimensions transposed.\n\n        Although the order of dimensions on each array will change, the dataset\n        dimensions themselves will remain in fixed (sorted) order.\n\n        Parameters\n        ----------\n        *dims : str, optional\n            By default, reverse the dimensions on each array. Otherwise,\n            reorder the dimensions to this order.\n\n        Returns\n        -------\n        transposed : Dataset\n            Each array in the dataset (including) coordinates will be\n            transposed to the given order.\n\n        Notes\n        -----\n        This operation returns a view of each array's data. It is\n        lazy for dask-backed DataArrays but not for numpy-backed DataArrays\n        -- the data will be fully loaded into memory.\n\n        See Also\n        --------\n        numpy.transpose\n        DataArray.transpose\n        \"\"\"\n        if dims:\n            if set(dims) ^ set(self.dims):\n                raise ValueError('arguments to transpose (%s) must be '\n                                 'permuted dataset dimensions (%s)'\n                                 % (dims, tuple(self.dims)))\n        ds = self.copy()\n        for name, var in self._variables.items():\n            var_dims = tuple(dim for dim in dims if dim in var.dims)\n            ds._variables[name] = var.transpose(*var_dims)\n        return ds"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dropna(self, dim, how='any', thresh=None, subset=None):\n        # TODO: consider supporting multiple dimensions? Or not, given that\n        # there are some ugly edge cases, e.g., pandas's dropna differs\n        # depending on the order of the supplied axes.\n\n        if dim not in self.dims:\n            raise ValueError('%s must be a single dataset dimension' % dim)\n\n        if subset is None:\n            subset = list(self.data_vars)\n\n        count = np.zeros(self.dims[dim], dtype=np.int64)\n        size = 0\n\n        for k in subset:\n            array = self._variables[k]\n            if dim in array.dims:\n                dims = [d for d in array.dims if d != dim]\n                count += np.asarray(array.count(dims))\n                size += np.prod([self.dims[d] for d in dims])\n\n        if thresh is not None:\n            mask = count >= thresh\n        elif how == 'any':\n            mask = count == size\n        elif how == 'all':\n            mask = count > 0\n        elif how is not None:\n            raise ValueError('invalid how option: %s' % how)\n        else:\n            raise TypeError('must specify how or thresh')\n\n        return self.isel({dim: mask})", "response": "Returns a new dataset with dropped labels for missing values along the provided dimension."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfilling missing values in this object with the given value.", "response": "def fillna(self, value):\n        \"\"\"Fill missing values in this object.\n\n        This operation follows the normal broadcasting and alignment rules that\n        xarray uses for binary arithmetic, except the result is aligned to this\n        object (``join='left'``) instead of aligned to the intersection of\n        index coordinates (``join='inner'``).\n\n        Parameters\n        ----------\n        value : scalar, ndarray, DataArray, dict or Dataset\n            Used to fill all matching missing values in this dataset's data\n            variables. Scalars, ndarrays or DataArrays arguments are used to\n            fill all data with aligned coordinates (for DataArrays).\n            Dictionaries or datasets match data variables and then align\n            coordinates if necessary.\n\n        Returns\n        -------\n        Dataset\n        \"\"\"\n        if utils.is_dict_like(value):\n            value_keys = getattr(value, 'data_vars', value).keys()\n            if not set(value_keys) <= set(self.data_vars.keys()):\n                raise ValueError('all variables in the argument to `fillna` '\n                                 'must be contained in the original dataset')\n        out = ops.fillna(self, value)\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninterpolate values along a specified dimension.", "response": "def interpolate_na(self, dim=None, method='linear', limit=None,\n                       use_coordinate=True,\n                       **kwargs):\n        \"\"\"Interpolate values according to different methods.\n\n        Parameters\n        ----------\n        dim : str\n            Specifies the dimension along which to interpolate.\n        method : {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic',\n                  'polynomial', 'barycentric', 'krog', 'pchip',\n                  'spline'}, optional\n            String indicating which method to use for interpolation:\n\n            - 'linear': linear interpolation (Default). Additional keyword\n              arguments are passed to ``numpy.interp``\n            - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic',\n              'polynomial': are passed to ``scipy.interpolate.interp1d``. If\n              method=='polynomial', the ``order`` keyword argument must also be\n              provided.\n            - 'barycentric', 'krog', 'pchip', 'spline': use their respective\n              ``scipy.interpolate`` classes.\n        use_coordinate : boolean or str, default True\n            Specifies which index to use as the x values in the interpolation\n            formulated as `y = f(x)`. If False, values are treated as if\n            eqaully-spaced along `dim`. If True, the IndexVariable `dim` is\n            used. If use_coordinate is a string, it specifies the name of a\n            coordinate variariable to use as the index.\n        limit : int, default None\n            Maximum number of consecutive NaNs to fill. Must be greater than 0\n            or None for no limit.\n\n        Returns\n        -------\n        Dataset\n\n        See also\n        --------\n        numpy.interp\n        scipy.interpolate\n        \"\"\"\n        from .missing import interp_na, _apply_over_vars_with_dim\n\n        new = _apply_over_vars_with_dim(interp_na, self, dim=dim,\n                                        method=method, limit=limit,\n                                        use_coordinate=use_coordinate,\n                                        **kwargs)\n        return new"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfilling NaN values by propogating values forward", "response": "def ffill(self, dim, limit=None):\n        '''Fill NaN values by propogating values forward\n\n        *Requires bottleneck.*\n\n        Parameters\n        ----------\n        dim : str\n            Specifies the dimension along which to propagate values when\n            filling.\n        limit : int, default None\n            The maximum number of consecutive NaN values to forward fill. In\n            other words, if there is a gap with more than this number of\n            consecutive NaNs, it will only be partially filled. Must be greater\n            than 0 or None for no limit.\n\n        Returns\n        -------\n        Dataset\n        '''\n        from .missing import ffill, _apply_over_vars_with_dim\n\n        new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)\n        return new"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bfill(self, dim, limit=None):\n        '''Fill NaN values by propogating values backward\n\n        *Requires bottleneck.*\n\n        Parameters\n        ----------\n        dim : str\n            Specifies the dimension along which to propagate values when\n            filling.\n        limit : int, default None\n            The maximum number of consecutive NaN values to backward fill. In\n            other words, if there is a gap with more than this number of\n            consecutive NaNs, it will only be partially filled. Must be greater\n            than 0 or None for no limit.\n\n        Returns\n        -------\n        Dataset\n        '''\n        from .missing import bfill, _apply_over_vars_with_dim\n\n        new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)\n        return new", "response": "Fill NaN values by propogating values backward\n            filling."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef combine_first(self, other):\n        out = ops.fillna(self, other, join=\"outer\", dataset_join=\"outer\")\n        return out", "response": "Combine two Datasets default to data_vars of self."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reduce(self, func, dim=None, keep_attrs=None, numeric_only=False,\n               allow_lazy=False, **kwargs):\n        \"\"\"Reduce this dataset by applying `func` along some dimension(s).\n\n        Parameters\n        ----------\n        func : function\n            Function which can be called in the form\n            `f(x, axis=axis, **kwargs)` to return the result of reducing an\n            np.ndarray over an integer valued axis.\n        dim : str or sequence of str, optional\n            Dimension(s) over which to apply `func`.  By default `func` is\n            applied over all dimensions.\n        keep_attrs : bool, optional\n            If True, the dataset's attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n        numeric_only : bool, optional\n            If True, only apply ``func`` to variables with a numeric dtype.\n        **kwargs : dict\n            Additional keyword arguments passed on to ``func``.\n\n        Returns\n        -------\n        reduced : Dataset\n            Dataset with this object's DataArrays replaced with new DataArrays\n            of summarized data and the indicated dimension(s) removed.\n        \"\"\"\n        if dim is ALL_DIMS:\n            dim = None\n        if isinstance(dim, str):\n            dims = set([dim])\n        elif dim is None:\n            dims = set(self.dims)\n        else:\n            dims = set(dim)\n\n        missing_dimensions = [d for d in dims if d not in self.dims]\n        if missing_dimensions:\n            raise ValueError('Dataset does not contain the dimensions: %s'\n                             % missing_dimensions)\n\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n\n        variables = OrderedDict()\n        for name, var in self._variables.items():\n            reduce_dims = [d for d in var.dims if d in dims]\n            if name in self.coords:\n                if not reduce_dims:\n                    variables[name] = var\n            else:\n                if (not numeric_only or\n                        np.issubdtype(var.dtype, np.number) or\n                        (var.dtype == np.bool_)):\n                    if len(reduce_dims) == 1:\n                        # unpack dimensions for the benefit of functions\n                        # like np.argmin which can't handle tuple arguments\n                        reduce_dims, = reduce_dims\n                    elif len(reduce_dims) == var.ndim:\n                        # prefer to aggregate over axis=None rather than\n                        # axis=(0, 1) if they will be equivalent, because\n                        # the former is often more efficient\n                        reduce_dims = None\n                    variables[name] = var.reduce(func, dim=reduce_dims,\n                                                 keep_attrs=keep_attrs,\n                                                 allow_lazy=allow_lazy,\n                                                 **kwargs)\n\n        coord_names = set(k for k in self.coords if k in variables)\n        attrs = self.attrs if keep_attrs else None\n        return self._replace_vars_and_dims(variables, coord_names, attrs=attrs)", "response": "Reduces this Dataset by applying func along some dimension."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\napply a function over the data variables in this dataset and return a new dataset with the result.", "response": "def apply(self, func, keep_attrs=None, args=(), **kwargs):\n        \"\"\"Apply a function over the data variables in this dataset.\n\n        Parameters\n        ----------\n        func : function\n            Function which can be called in the form `func(x, *args, **kwargs)`\n            to transform each DataArray `x` in this dataset into another\n            DataArray.\n        keep_attrs : bool, optional\n            If True, the dataset's attributes (`attrs`) will be copied from\n            the original object to the new one. If False, the new object will\n            be returned without attributes.\n        args : tuple, optional\n            Positional arguments passed on to `func`.\n        **kwargs : dict\n            Keyword arguments passed on to `func`.\n\n        Returns\n        -------\n        applied : Dataset\n            Resulting dataset from applying ``func`` over each data variable.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(np.random.randn(2, 3))\n        >>> ds = xr.Dataset({'foo': da, 'bar': ('x', [-1, 2])})\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n        Dimensions without coordinates: dim_0, dim_1, x\n        Data variables:\n            foo      (dim_0, dim_1) float64 -0.3751 -1.951 -1.945 0.2948 0.711 -0.3948\n            bar      (x) int64 -1 2\n        >>> ds.apply(np.fabs)\n        <xarray.Dataset>\n        Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n        Dimensions without coordinates: dim_0, dim_1, x\n        Data variables:\n            foo      (dim_0, dim_1) float64 0.3751 1.951 1.945 0.2948 0.711 0.3948\n            bar      (x) float64 1.0 2.0\n        \"\"\"  # noqa\n        variables = OrderedDict(\n            (k, maybe_wrap_array(v, func(v, *args, **kwargs)))\n            for k, v in self.data_vars.items())\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n        attrs = self.attrs if keep_attrs else None\n        return type(self)(variables, attrs=attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef assign(self, variables=None, **variables_kwargs):\n        variables = either_dict_or_kwargs(\n            variables, variables_kwargs, 'assign')\n        data = self.copy()\n        # do all calculations first...\n        results = data._calc_assign_results(variables)\n        # ... and then assign\n        data.update(results)\n        return data", "response": "Assign new data variables to a Dataset returning a new Dataset with all the original variables added to the new ones."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_array(self, dim='variable', name=None):\n        from .dataarray import DataArray\n\n        data_vars = [self.variables[k] for k in self.data_vars]\n        broadcast_vars = broadcast_variables(*data_vars)\n        data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)\n\n        coords = dict(self.coords)\n        coords[dim] = list(self.data_vars)\n\n        dims = (dim,) + broadcast_vars[0].dims\n\n        return DataArray(data, coords, dims, attrs=self.attrs, name=name)", "response": "Convert this dataset into an xarray. DataArray."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting this dataset into a dask. dataframe. DataFrame.", "response": "def to_dask_dataframe(self, dim_order=None, set_index=False):\n        \"\"\"\n        Convert this dataset into a dask.dataframe.DataFrame.\n\n        The dimensions, coordinates and data variables in this dataset form\n        the columns of the DataFrame.\n\n        Parameters\n        ----------\n        dim_order : list, optional\n            Hierarchical dimension order for the resulting dataframe. All\n            arrays are transposed to this order and then written out as flat\n            vectors in contiguous order, so the last dimension in this list\n            will be contiguous in the resulting DataFrame. This has a major\n            influence on which operations are efficient on the resulting dask\n            dataframe.\n\n            If provided, must include all dimensions on this dataset. By\n            default, dimensions are sorted alphabetically.\n        set_index : bool, optional\n            If set_index=True, the dask DataFrame is indexed by this dataset's\n            coordinate. Since dask DataFrames to not support multi-indexes,\n            set_index only works if the dataset only contains one dimension.\n\n        Returns\n        -------\n        dask.dataframe.DataFrame\n        \"\"\"\n\n        import dask.array as da\n        import dask.dataframe as dd\n\n        if dim_order is None:\n            dim_order = list(self.dims)\n        elif set(dim_order) != set(self.dims):\n            raise ValueError(\n                'dim_order {} does not match the set of dimensions on this '\n                'Dataset: {}'.format(dim_order, list(self.dims)))\n\n        ordered_dims = OrderedDict((k, self.dims[k]) for k in dim_order)\n\n        columns = list(ordered_dims)\n        columns.extend(k for k in self.coords if k not in self.dims)\n        columns.extend(self.data_vars)\n\n        series_list = []\n        for name in columns:\n            try:\n                var = self.variables[name]\n            except KeyError:\n                # dimension without a matching coordinate\n                size = self.dims[name]\n                data = da.arange(size, chunks=size, dtype=np.int64)\n                var = Variable((name,), data)\n\n            # IndexVariable objects have a dummy .chunk() method\n            if isinstance(var, IndexVariable):\n                var = var.to_base_variable()\n\n            dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data\n            series = dd.from_array(dask_array.reshape(-1), columns=[name])\n            series_list.append(series)\n\n        df = dd.concat(series_list, axis=1)\n\n        if set_index:\n            if len(dim_order) == 1:\n                (dim,) = dim_order\n                df = df.set_index(dim)\n            else:\n                # triggers an error about multi-indexes, even if only one\n                # dimension is passed\n                df = df.set_index(dim_order)\n\n        return df"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts this dataset to a dictionary following xarray naming conventions.", "response": "def to_dict(self, data=True):\n        \"\"\"\n        Convert this dataset to a dictionary following xarray naming\n        conventions.\n\n        Converts all variables and attributes to native Python objects\n        Useful for coverting to json. To avoid datetime incompatibility\n        use decode_times=False kwarg in xarrray.open_dataset.\n\n        Parameters\n        ----------\n        data : bool, optional\n            Whether to include the actual data in the dictionary. When set to\n            False, returns just the schema.\n\n        See also\n        --------\n        Dataset.from_dict\n        \"\"\"\n        d = {'coords': {}, 'attrs': decode_numpy_dict_values(self.attrs),\n             'dims': dict(self.dims), 'data_vars': {}}\n        for k in self.coords:\n            d['coords'].update({k: self[k].variable.to_dict(data=data)})\n        for k in self.data_vars:\n            d['data_vars'].update({k: self[k].variable.to_dict(data=data)})\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a dictionary into an xarray. Dataset.", "response": "def from_dict(cls, d):\n        \"\"\"\n        Convert a dictionary into an xarray.Dataset.\n\n        Input dict can take several forms::\n\n            d = {'t': {'dims': ('t'), 'data': t},\n                 'a': {'dims': ('t'), 'data': x},\n                 'b': {'dims': ('t'), 'data': y}}\n\n            d = {'coords': {'t': {'dims': 't', 'data': t,\n                                  'attrs': {'units':'s'}}},\n                 'attrs': {'title': 'air temperature'},\n                 'dims': 't',\n                 'data_vars': {'a': {'dims': 't', 'data': x, },\n                               'b': {'dims': 't', 'data': y}}}\n\n        where 't' is the name of the dimesion, 'a' and 'b' are names of data\n        variables and t, x, and y are lists, numpy.arrays or pandas objects.\n\n        Parameters\n        ----------\n        d : dict, with a minimum structure of {'var_0': {'dims': [..], \\\n                                                         'data': [..]}, \\\n                                               ...}\n\n        Returns\n        -------\n        obj : xarray.Dataset\n\n        See also\n        --------\n        Dataset.to_dict\n        DataArray.from_dict\n        \"\"\"\n\n        if not set(['coords', 'data_vars']).issubset(set(d)):\n            variables = d.items()\n        else:\n            import itertools\n            variables = itertools.chain(d.get('coords', {}).items(),\n                                        d.get('data_vars', {}).items())\n        try:\n            variable_dict = OrderedDict([(k, (v['dims'],\n                                              v['data'],\n                                              v.get('attrs'))) for\n                                         k, v in variables])\n        except KeyError as e:\n            raise ValueError(\n                \"cannot convert dict without the key \"\n                \"'{dims_data}'\".format(dims_data=str(e.args[0])))\n        obj = cls(variable_dict)\n\n        # what if coords aren't dims?\n        coords = set(d.get('coords', {})) - set(d.get('dims', {}))\n        obj = obj.set_coords(coords)\n\n        obj.attrs.update(d.get('attrs', {}))\n\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the n - th order discrete difference along a given dimension.", "response": "def diff(self, dim, n=1, label='upper'):\n        \"\"\"Calculate the n-th order discrete difference along given axis.\n\n        Parameters\n        ----------\n        dim : str, optional\n            Dimension over which to calculate the finite difference.\n        n : int, optional\n            The number of times values are differenced.\n        label : str, optional\n            The new coordinate in dimension ``dim`` will have the\n            values of either the minuend's or subtrahend's coordinate\n            for values 'upper' and 'lower', respectively.  Other\n            values are not supported.\n\n        Returns\n        -------\n        difference : same type as caller\n            The n-th order finite difference of this object.\n\n        Examples\n        --------\n        >>> ds = xr.Dataset({'foo': ('x', [5, 5, 6, 6])})\n        >>> ds.diff('x')\n        <xarray.Dataset>\n        Dimensions:  (x: 3)\n        Coordinates:\n          * x        (x) int64 1 2 3\n        Data variables:\n            foo      (x) int64 0 1 0\n        >>> ds.diff('x', 2)\n        <xarray.Dataset>\n        Dimensions:  (x: 2)\n        Coordinates:\n        * x        (x) int64 2 3\n        Data variables:\n        foo      (x) int64 1 -1\n\n        See Also\n        --------\n        Dataset.differentiate\n        \"\"\"\n        if n == 0:\n            return self\n        if n < 0:\n            raise ValueError('order `n` must be non-negative but got {0}'\n                             ''.format(n))\n\n        # prepare slices\n        kwargs_start = {dim: slice(None, -1)}\n        kwargs_end = {dim: slice(1, None)}\n\n        # prepare new coordinate\n        if label == 'upper':\n            kwargs_new = kwargs_end\n        elif label == 'lower':\n            kwargs_new = kwargs_start\n        else:\n            raise ValueError('The \\'label\\' argument has to be either '\n                             '\\'upper\\' or \\'lower\\'')\n\n        variables = OrderedDict()\n\n        for name, var in self.variables.items():\n            if dim in var.dims:\n                if name in self.data_vars:\n                    variables[name] = (var.isel(**kwargs_end) -\n                                       var.isel(**kwargs_start))\n                else:\n                    variables[name] = var.isel(**kwargs_new)\n            else:\n                variables[name] = var\n\n        difference = self._replace_vars_and_dims(variables)\n\n        if n > 1:\n            return difference.diff(dim, n - 1)\n        else:\n            return difference"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nshift this Dataset by an offset along one or more dimensions.", "response": "def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):\n        \"\"\"Shift this dataset by an offset along one or more dimensions.\n\n        Only data variables are moved; coordinates stay in place. This is\n        consistent with the behavior of ``shift`` in pandas.\n\n        Parameters\n        ----------\n        shifts : Mapping with the form of {dim: offset}\n            Integer offset to shift along each of the given dimensions.\n            Positive offsets shift to the right; negative offsets shift to the\n            left.\n        fill_value: scalar, optional\n            Value to use for newly missing values\n        **shifts_kwargs:\n            The keyword arguments form of ``shifts``.\n            One of shifts or shifts_kwarg must be provided.\n\n        Returns\n        -------\n        shifted : Dataset\n            Dataset with the same coordinates and attributes but shifted data\n            variables.\n\n        See also\n        --------\n        roll\n\n        Examples\n        --------\n\n        >>> ds = xr.Dataset({'foo': ('x', list('abcde'))})\n        >>> ds.shift(x=2)\n        <xarray.Dataset>\n        Dimensions:  (x: 5)\n        Coordinates:\n          * x        (x) int64 0 1 2 3 4\n        Data variables:\n            foo      (x) object nan nan 'a' 'b' 'c'\n        \"\"\"\n        shifts = either_dict_or_kwargs(shifts, shifts_kwargs, 'shift')\n        invalid = [k for k in shifts if k not in self.dims]\n        if invalid:\n            raise ValueError(\"dimensions %r do not exist\" % invalid)\n\n        variables = OrderedDict()\n        for name, var in self.variables.items():\n            if name in self.data_vars:\n                var_shifts = {k: v for k, v in shifts.items()\n                              if k in var.dims}\n                variables[name] = var.shift(\n                    fill_value=fill_value, shifts=var_shifts)\n            else:\n                variables[name] = var\n\n        return self._replace_vars_and_dims(variables)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef roll(self, shifts=None, roll_coords=None, **shifts_kwargs):\n        shifts = either_dict_or_kwargs(shifts, shifts_kwargs, 'roll')\n        invalid = [k for k in shifts if k not in self.dims]\n        if invalid:\n            raise ValueError(\"dimensions %r do not exist\" % invalid)\n\n        if roll_coords is None:\n            warnings.warn(\"roll_coords will be set to False in the future.\"\n                          \" Explicitly set roll_coords to silence warning.\",\n                          FutureWarning, stacklevel=2)\n            roll_coords = True\n\n        unrolled_vars = () if roll_coords else self.coords\n\n        variables = OrderedDict()\n        for k, v in self.variables.items():\n            if k not in unrolled_vars:\n                variables[k] = v.roll(**{k: s for k, s in shifts.items()\n                                         if k in v.dims})\n            else:\n                variables[k] = v\n\n        return self._replace_vars_and_dims(variables)", "response": "Roll this Dataset by an offset along one or more dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsorting the dataset by the specified variables.", "response": "def sortby(self, variables, ascending=True):\n        \"\"\"\n        Sort object by labels or values (along an axis).\n\n        Sorts the dataset, either along specified dimensions,\n        or according to values of 1-D dataarrays that share dimension\n        with calling object.\n\n        If the input variables are dataarrays, then the dataarrays are aligned\n        (via left-join) to the calling object prior to sorting by cell values.\n        NaNs are sorted to the end, following Numpy convention.\n\n        If multiple sorts along the same dimension is\n        given, numpy's lexsort is performed along that dimension:\n        https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html\n        and the FIRST key in the sequence is used as the primary sort key,\n        followed by the 2nd key, etc.\n\n        Parameters\n        ----------\n        variables: str, DataArray, or list of either\n            1D DataArray objects or name(s) of 1D variable(s) in\n            coords/data_vars whose values are used to sort the dataset.\n        ascending: boolean, optional\n            Whether to sort by ascending or descending order.\n\n        Returns\n        -------\n        sorted: Dataset\n            A new dataset where all the specified dims are sorted by dim\n            labels.\n        \"\"\"\n        from .dataarray import DataArray\n\n        if not isinstance(variables, list):\n            variables = [variables]\n        else:\n            variables = variables\n        variables = [v if isinstance(v, DataArray) else self[v]\n                     for v in variables]\n        aligned_vars = align(self, *variables, join='left')\n        aligned_self = aligned_vars[0]\n        aligned_other_vars = aligned_vars[1:]\n        vars_by_dim = defaultdict(list)\n        for data_array in aligned_other_vars:\n            if data_array.ndim != 1:\n                raise ValueError(\"Input DataArray is not 1-D.\")\n            if (data_array.dtype == object and\n                    LooseVersion(np.__version__) < LooseVersion('1.11.0')):\n                raise NotImplementedError(\n                    'sortby uses np.lexsort under the hood, which requires '\n                    'numpy 1.11.0 or later to support object data-type.')\n            (key,) = data_array.dims\n            vars_by_dim[key].append(data_array)\n\n        indices = {}\n        for key, arrays in vars_by_dim.items():\n            order = np.lexsort(tuple(reversed(arrays)))\n            indices[key] = order if ascending else order[::-1]\n        return aligned_self.isel(**indices)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef quantile(self, q, dim=None, interpolation='linear',\n                 numeric_only=False, keep_attrs=None):\n        \"\"\"Compute the qth quantile of the data along the specified dimension.\n\n        Returns the qth quantiles(s) of the array elements for each variable\n        in the Dataset.\n\n        Parameters\n        ----------\n        q : float in range of [0,1] (or sequence of floats)\n            Quantile to compute, which must be between 0 and 1 inclusive.\n        dim : str or sequence of str, optional\n            Dimension(s) over which to apply quantile.\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n            This optional parameter specifies the interpolation method to\n            use when the desired quantile lies between two data points\n            ``i < j``:\n\n                * linear: ``i + (j - i) * fraction``, where ``fraction`` is\n                  the fractional part of the index surrounded by ``i`` and\n                  ``j``.\n                * lower: ``i``.\n                * higher: ``j``.\n                * nearest: ``i`` or ``j``, whichever is nearest.\n                * midpoint: ``(i + j) / 2``.\n        keep_attrs : bool, optional\n            If True, the dataset's attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n        numeric_only : bool, optional\n            If True, only apply ``func`` to variables with a numeric dtype.\n\n        Returns\n        -------\n        quantiles : Dataset\n            If `q` is a single quantile, then the result is a scalar for each\n            variable in data_vars. If multiple percentiles are given, first\n            axis of the result corresponds to the quantile and a quantile\n            dimension is added to the return Dataset. The other dimensions are\n            the dimensions that remain after the reduction of the array.\n\n        See Also\n        --------\n        numpy.nanpercentile, pandas.Series.quantile, DataArray.quantile\n        \"\"\"\n\n        if isinstance(dim, str):\n            dims = set([dim])\n        elif dim is None:\n            dims = set(self.dims)\n        else:\n            dims = set(dim)\n\n        _assert_empty([d for d in dims if d not in self.dims],\n                      'Dataset does not contain the dimensions: %s')\n\n        q = np.asarray(q, dtype=np.float64)\n\n        variables = OrderedDict()\n        for name, var in self.variables.items():\n            reduce_dims = [d for d in var.dims if d in dims]\n            if reduce_dims or not var.dims:\n                if name not in self.coords:\n                    if (not numeric_only or\n                        np.issubdtype(var.dtype, np.number) or\n                            var.dtype == np.bool_):\n                        if len(reduce_dims) == var.ndim:\n                            # prefer to aggregate over axis=None rather than\n                            # axis=(0, 1) if they will be equivalent, because\n                            # the former is often more efficient\n                            reduce_dims = None\n                        variables[name] = var.quantile(\n                            q, dim=reduce_dims, interpolation=interpolation)\n\n            else:\n                variables[name] = var\n\n        # construct the new dataset\n        coord_names = set(k for k in self.coords if k in variables)\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n        attrs = self.attrs if keep_attrs else None\n        new = self._replace_vars_and_dims(variables, coord_names, attrs=attrs)\n        if 'quantile' in new.dims:\n            new.coords['quantile'] = Variable('quantile', q)\n        else:\n            new.coords['quantile'] = q\n        return new", "response": "Compute the qth quantile of the data along the specified dimension."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rank(self, dim, pct=False, keep_attrs=None):\n        if dim not in self.dims:\n            raise ValueError(\n                'Dataset does not contain the dimension: %s' % dim)\n\n        variables = OrderedDict()\n        for name, var in self.variables.items():\n            if name in self.data_vars:\n                if dim in var.dims:\n                    variables[name] = var.rank(dim, pct=pct)\n            else:\n                variables[name] = var\n\n        coord_names = set(self.coords)\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n        attrs = self.attrs if keep_attrs else None\n        return self._replace_vars_and_dims(variables, coord_names, attrs=attrs)", "response": "Ranks the data.\n\n        Equal values are assigned a rank that is the average of the ranks that\n        would have been otherwise assigned to all of the values within\n        that set.\n        Ranks begin at 1, not 0. If pct is True, computes percentage ranks.\n\n        NaNs in the input array are returned as NaNs.\n\n        The `bottleneck` library is required.\n\n        Parameters\n        ----------\n        dim : str\n            Dimension over which to compute rank.\n        pct : bool, optional\n            If True, compute percentage ranks, otherwise compute integer ranks.\n        keep_attrs : bool, optional\n            If True, the dataset's attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n\n        Returns\n        -------\n        ranked : Dataset\n            Variables that do not depend on `dim` are dropped."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef differentiate(self, coord, edge_order=1, datetime_unit=None):\n        from .variable import Variable\n\n        if coord not in self.variables and coord not in self.dims:\n            raise ValueError('Coordinate {} does not exist.'.format(coord))\n\n        coord_var = self[coord].variable\n        if coord_var.ndim != 1:\n            raise ValueError('Coordinate {} must be 1 dimensional but is {}'\n                             ' dimensional'.format(coord, coord_var.ndim))\n\n        dim = coord_var.dims[0]\n        if _contains_datetime_like_objects(coord_var):\n            if coord_var.dtype.kind in 'mM' and datetime_unit is None:\n                datetime_unit, _ = np.datetime_data(coord_var.dtype)\n            elif datetime_unit is None:\n                datetime_unit = 's'  # Default to seconds for cftime objects\n            coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)\n\n        variables = OrderedDict()\n        for k, v in self.variables.items():\n            if (k in self.data_vars and dim in v.dims and\n                    k not in self.coords):\n                if _contains_datetime_like_objects(v):\n                    v = v._to_numeric(datetime_unit=datetime_unit)\n                grad = duck_array_ops.gradient(\n                    v.data, coord_var, edge_order=edge_order,\n                    axis=v.get_axis_num(dim))\n                variables[k] = Variable(v.dims, grad)\n            else:\n                variables[k] = v\n        return self._replace_vars_and_dims(variables)", "response": "Differentiate with the second order accurate central centralCOOKIE entries."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nintegrating the array with the trapezoidal rule.", "response": "def integrate(self, coord, datetime_unit=None):\n        \"\"\" integrate the array with the trapezoidal rule.\n\n        .. note::\n            This feature is limited to simple cartesian geometry, i.e. coord\n            must be one dimensional.\n\n        Parameters\n        ----------\n        dim: str, or a sequence of str\n            Coordinate(s) used for the integration.\n        datetime_unit\n            Can be specify the unit if datetime coordinate is used. One of\n            {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', 'ps', 'fs',\n             'as'}\n\n        Returns\n        -------\n        integrated: Dataset\n\n        See also\n        --------\n        DataArray.integrate\n        numpy.trapz: corresponding numpy function\n        \"\"\"\n        if not isinstance(coord, (list, tuple)):\n            coord = (coord, )\n        result = self\n        for c in coord:\n            result = result._integrate_one(c, datetime_unit=datetime_unit)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filter_by_attrs(self, **kwargs):\n        selection = []\n        for var_name, variable in self.data_vars.items():\n            has_value_flag = False\n            for attr_name, pattern in kwargs.items():\n                attr_value = variable.attrs.get(attr_name)\n                if ((callable(pattern) and pattern(attr_value)) or\n                        attr_value == pattern):\n                    has_value_flag = True\n                else:\n                    has_value_flag = False\n                    break\n            if has_value_flag is True:\n                selection.append(var_name)\n        return self[selection]", "response": "Returns a new Dataset with variables that match specific conditions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _apply_mask(\n    data: np.ndarray,\n    encoded_fill_values: list,\n    decoded_fill_value: Any,\n    dtype: Any,\n) -> np.ndarray:\n    \"\"\"Mask all matching values in a NumPy arrays.\"\"\"\n    data = np.asarray(data, dtype=dtype)\n    condition = False\n    for fv in encoded_fill_values:\n        condition |= data == fv\n    return np.where(condition, decoded_fill_value, data)", "response": "Mask all matching values in a NumPy arrays."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a float dtype that can losslessly represent dtype values.", "response": "def _choose_float_dtype(dtype, has_offset):\n    \"\"\"Return a float dtype that can losslessly represent `dtype` values.\"\"\"\n    # Keep float32 as-is.  Upcast half-precision to single-precision,\n    # because float16 is \"intended for storage but not computation\"\n    if dtype.itemsize <= 4 and np.issubdtype(dtype, np.floating):\n        return np.float32\n    # float32 can exactly represent all integers up to 24 bits\n    if dtype.itemsize <= 2 and np.issubdtype(dtype, np.integer):\n        # A scale factor is entirely safe (vanishing into the mantissa),\n        # but a large integer offset could lead to loss of precision.\n        # Sensitivity analysis can be tricky, so we just use a float64\n        # if there's any offset at all - better unoptimised than wrong!\n        if not has_offset:\n            return np.float32\n    # For all other types and circumstances, we just use float64.\n    # (safe because eg. complex numbers are not supported in NetCDF)\n    return np.float64"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _apply_over_vars_with_dim(func, self, dim=None, **kwargs):\n    '''wrapper for datasets'''\n\n    ds = type(self)(coords=self.coords, attrs=self.attrs)\n\n    for name, var in self.data_vars.items():\n        if dim in var.dims:\n            ds[name] = func(var, dim=dim, **kwargs)\n        else:\n            ds[name] = var\n\n    return ds", "response": "wrapper for datasets with dim = None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_clean_interp_index(arr, dim, use_coordinate=True, **kwargs):\n    '''get index to use for x values in interpolation.\n\n    If use_coordinate is True, the coordinate that shares the name of the\n    dimension along which interpolation is being performed will be used as the\n    x values.\n\n    If use_coordinate is False, the x values are set as an equally spaced\n    sequence.\n    '''\n    if use_coordinate:\n        if use_coordinate is True:\n            index = arr.get_index(dim)\n        else:\n            index = arr.coords[use_coordinate]\n            if index.ndim != 1:\n                raise ValueError(\n                    'Coordinates used for interpolation must be 1D, '\n                    '%s is %dD.' % (use_coordinate, index.ndim))\n\n        # raise if index cannot be cast to a float (e.g. MultiIndex)\n        try:\n            index = index.values.astype(np.float64)\n        except (TypeError, ValueError):\n            # pandas raises a TypeError\n            # xarray/nuppy raise a ValueError\n            raise TypeError('Index must be castable to float64 to support'\n                            'interpolation, got: %s' % type(index))\n        # check index sorting now so we can skip it later\n        if not (np.diff(index) > 0).all():\n            raise ValueError(\"Index must be monotonicly increasing\")\n    else:\n        axis = arr.get_axis_num(dim)\n        index = np.arange(arr.shape[axis], dtype=np.float64)\n\n    return index", "response": "get index to use for x values in interpolation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef interp_na(self, dim=None, use_coordinate=True, method='linear', limit=None,\n              **kwargs):\n    '''Interpolate values according to different methods.'''\n\n    if dim is None:\n        raise NotImplementedError('dim is a required argument')\n\n    if limit is not None:\n        valids = _get_valid_fill_mask(self, dim, limit)\n\n    # method\n    index = get_clean_interp_index(self, dim, use_coordinate=use_coordinate,\n                                   **kwargs)\n    interp_class, kwargs = _get_interpolator(method, **kwargs)\n    interpolator = partial(func_interpolate_na, interp_class, **kwargs)\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', 'overflow', RuntimeWarning)\n        warnings.filterwarnings('ignore', 'invalid value', RuntimeWarning)\n        arr = apply_ufunc(interpolator, index, self,\n                          input_core_dims=[[dim], [dim]],\n                          output_core_dims=[[dim]],\n                          output_dtypes=[self.dtype],\n                          dask='parallelized',\n                          vectorize=True,\n                          keep_attrs=True).transpose(*self.dims)\n\n    if limit is not None:\n        arr = arr.where(valids)\n\n    return arr", "response": "Interpolate values according to different methods."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef func_interpolate_na(interpolator, x, y, **kwargs):\n    '''helper function to apply interpolation along 1 dimension'''\n    # it would be nice if this wasn't necessary, works around:\n    # \"ValueError: assignment destination is read-only\" in assignment below\n    out = y.copy()\n\n    nans = pd.isnull(y)\n    nonans = ~nans\n\n    # fast track for no-nans and all-nans cases\n    n_nans = nans.sum()\n    if n_nans == 0 or n_nans == len(y):\n        return y\n\n    f = interpolator(x[nonans], y[nonans], **kwargs)\n    out[nans] = f(x[nans])\n    return out", "response": "helper function to apply interpolation along 1 dimension"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ffill(arr, dim=None, limit=None):\n    '''forward fill missing values'''\n    import bottleneck as bn\n\n    axis = arr.get_axis_num(dim)\n\n    # work around for bottleneck 178\n    _limit = limit if limit is not None else arr.shape[axis]\n\n    return apply_ufunc(bn.push, arr,\n                       dask='parallelized',\n                       keep_attrs=True,\n                       output_dtypes=[arr.dtype],\n                       kwargs=dict(n=_limit, axis=axis)).transpose(*arr.dims)", "response": "forward fill missing values"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_interpolator(method, vectorizeable_only=False, **kwargs):\n    '''helper function to select the appropriate interpolator class\n\n    returns interpolator class and keyword arguments for the class\n    '''\n    interp1d_methods = ['linear', 'nearest', 'zero', 'slinear', 'quadratic',\n                        'cubic', 'polynomial']\n    valid_methods = interp1d_methods + ['barycentric', 'krog', 'pchip',\n                                        'spline', 'akima']\n\n    has_scipy = True\n    try:\n        from scipy import interpolate\n    except ImportError:\n        has_scipy = False\n\n    # prioritize scipy.interpolate\n    if (method == 'linear' and not\n            kwargs.get('fill_value', None) == 'extrapolate' and\n            not vectorizeable_only):\n        kwargs.update(method=method)\n        interp_class = NumpyInterpolator\n\n    elif method in valid_methods:\n        if not has_scipy:\n            raise ImportError(\n                'Interpolation with method `%s` requires scipy' % method)\n\n        if method in interp1d_methods:\n            kwargs.update(method=method)\n            interp_class = ScipyInterpolator\n        elif vectorizeable_only:\n            raise ValueError('{} is not a vectorizeable interpolator. '\n                             'Available methods are {}'.format(\n                                 method, interp1d_methods))\n        elif method == 'barycentric':\n            interp_class = interpolate.BarycentricInterpolator\n        elif method == 'krog':\n            interp_class = interpolate.KroghInterpolator\n        elif method == 'pchip':\n            interp_class = interpolate.PchipInterpolator\n        elif method == 'spline':\n            kwargs.update(method=method)\n            interp_class = SplineInterpolator\n        elif method == 'akima':\n            interp_class = interpolate.Akima1DInterpolator\n        else:\n            raise ValueError('%s is not a valid scipy interpolator' % method)\n    else:\n        raise ValueError('%s is not a valid interpolator' % method)\n\n    return interp_class, kwargs", "response": "helper function to select the appropriate interpolator class for the appropriate class\n    returns interpolator class and keyword arguments for the class\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_interpolator_nd(method, **kwargs):\n    '''helper function to select the appropriate interpolator class\n\n    returns interpolator class and keyword arguments for the class\n    '''\n    valid_methods = ['linear', 'nearest']\n\n    try:\n        from scipy import interpolate\n    except ImportError:\n        raise ImportError(\n            'Interpolation with method `%s` requires scipy' % method)\n\n    if method in valid_methods:\n        kwargs.update(method=method)\n        interp_class = interpolate.interpn\n    else:\n        raise ValueError('%s is not a valid interpolator for interpolating '\n                         'over multiple dimensions.' % method)\n\n    return interp_class, kwargs", "response": "helper function to select the appropriate interpolator class and keyword arguments for the class\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_valid_fill_mask(arr, dim, limit):\n    '''helper function to determine values that can be filled when limit is not\n    None'''\n    kw = {dim: limit + 1}\n    # we explicitly use construct method to avoid copy.\n    new_dim = utils.get_temp_dimname(arr.dims, '_window')\n    return (arr.isnull().rolling(min_periods=1, **kw)\n            .construct(new_dim, fill_value=False)\n            .sum(new_dim, skipna=False)) <= limit", "response": "helper function to determine values that can be filled when limit is not\n    None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlocalize a variable by linear and nearest neighbor method.", "response": "def _localize(var, indexes_coords):\n    \"\"\" Speed up for linear and nearest neighbor method.\n    Only consider a subspace that is needed for the interpolation\n    \"\"\"\n    indexes = {}\n    for dim, [x, new_x] in indexes_coords.items():\n        index = x.to_index()\n        imin = index.get_loc(np.min(new_x.values), method='nearest')\n        imax = index.get_loc(np.max(new_x.values), method='nearest')\n\n        indexes[dim] = slice(max(imin - 2, 0), imax + 2)\n        indexes_coords[dim] = (x[indexes[dim]], new_x)\n    return var.isel(**indexes), indexes_coords"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking x and new_x float.", "response": "def _floatize_x(x, new_x):\n    \"\"\" Make x and new_x float.\n    This is particulary useful for datetime dtype.\n    x, new_x: tuple of np.ndarray\n    \"\"\"\n    x = list(x)\n    new_x = list(new_x)\n    for i in range(len(x)):\n        if _contains_datetime_like_objects(x[i]):\n            # Scipy casts coordinates to np.float64, which is not accurate\n            # enough for datetime64 (uses 64bit integer).\n            # We assume that the most of the bits are used to represent the\n            # offset (min(x)) and the variation (x - min(x)) can be\n            # represented by float.\n            xmin = x[i].values.min()\n            x[i] = x[i]._to_numeric(offset=xmin, dtype=np.float64)\n            new_x[i] = new_x[i]._to_numeric(offset=xmin, dtype=np.float64)\n    return x, new_x"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef interp(var, indexes_coords, method, **kwargs):\n    if not indexes_coords:\n        return var.copy()\n\n    # simple speed up for the local interpolation\n    if method in ['linear', 'nearest']:\n        var, indexes_coords = _localize(var, indexes_coords)\n\n    # default behavior\n    kwargs['bounds_error'] = kwargs.get('bounds_error', False)\n\n    # target dimensions\n    dims = list(indexes_coords)\n    x, new_x = zip(*[indexes_coords[d] for d in dims])\n    destination = broadcast_variables(*new_x)\n\n    # transpose to make the interpolated axis to the last position\n    broadcast_dims = [d for d in var.dims if d not in dims]\n    original_dims = broadcast_dims + dims\n    new_dims = broadcast_dims + list(destination[0].dims)\n    interped = interp_func(var.transpose(*original_dims).data,\n                           x, destination, method, kwargs)\n\n    result = Variable(new_dims, interped, attrs=var.attrs)\n\n    # dimension of the output array\n    out_dims = OrderedSet()\n    for d in var.dims:\n        if d in dims:\n            out_dims.update(indexes_coords[d][1].dims)\n        else:\n            out_dims.add(d)\n    return result.transpose(*tuple(out_dims))", "response": "Interpolate a Variable with a set of indexes coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninterpolating a 1 - dimensional array - like or array - like into a new array - like.", "response": "def interp_func(var, x, new_x, method, kwargs):\n    \"\"\"\n    multi-dimensional interpolation for array-like. Interpolated axes should be\n    located in the last position.\n\n    Parameters\n    ----------\n    var: np.ndarray or dask.array.Array\n        Array to be interpolated. The final dimension is interpolated.\n    x: a list of 1d array.\n        Original coordinates. Should not contain NaN.\n    new_x: a list of 1d array\n        New coordinates. Should not contain NaN.\n    method: string\n        {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'} for\n        1-dimensional itnterpolation.\n        {'linear', 'nearest'} for multidimensional interpolation\n    **kwargs:\n        Optional keyword arguments to be passed to scipy.interpolator\n\n    Returns\n    -------\n    interpolated: array\n        Interpolated array\n\n    Note\n    ----\n    This requiers scipy installed.\n\n    See Also\n    --------\n    scipy.interpolate.interp1d\n    \"\"\"\n    if not x:\n        return var.copy()\n\n    if len(x) == 1:\n        func, kwargs = _get_interpolator(method, vectorizeable_only=True,\n                                         **kwargs)\n    else:\n        func, kwargs = _get_interpolator_nd(method, **kwargs)\n\n    if isinstance(var, dask_array_type):\n        import dask.array as da\n\n        _assert_single_chunk(var, range(var.ndim - len(x), var.ndim))\n        chunks = var.chunks[:-len(x)] + new_x[0].shape\n        drop_axis = range(var.ndim - len(x), var.ndim)\n        new_axis = range(var.ndim - len(x), var.ndim - len(x) + new_x[0].ndim)\n        return da.map_blocks(_interpnd, var, x, new_x, func, kwargs,\n                             dtype=var.dtype, chunks=chunks,\n                             new_axis=new_axis, drop_axis=drop_axis)\n\n    return _interpnd(var, x, new_x, func, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _nicetitle(coord, value, maxchar, template):\n    prettyvalue = format_item(value, quote_strings=False)\n    title = template.format(coord=coord, value=prettyvalue)\n\n    if len(title) > maxchar:\n        title = title[:(maxchar - 3)] + '...'\n\n    return title", "response": "Return a nicetitle for a given coordinate and value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _easy_facetgrid(data, plotfunc, kind, x=None, y=None, row=None,\n                    col=None, col_wrap=None, sharex=True, sharey=True,\n                    aspect=None, size=None, subplot_kws=None, **kwargs):\n    \"\"\"\n    Convenience method to call xarray.plot.FacetGrid from 2d plotting methods\n\n    kwargs are the arguments to 2d plotting method\n    \"\"\"\n    ax = kwargs.pop('ax', None)\n    figsize = kwargs.pop('figsize', None)\n    if ax is not None:\n        raise ValueError(\"Can't use axes when making faceted plots.\")\n    if aspect is None:\n        aspect = 1\n    if size is None:\n        size = 3\n    elif figsize is not None:\n        raise ValueError('cannot provide both `figsize` and `size` arguments')\n\n    g = FacetGrid(data=data, col=col, row=row, col_wrap=col_wrap,\n                  sharex=sharex, sharey=sharey, figsize=figsize,\n                  aspect=aspect, size=size, subplot_kws=subplot_kws)\n\n    if kind == 'line':\n        return g.map_dataarray_line(plotfunc, x, y, **kwargs)\n\n    if kind == 'dataarray':\n        return g.map_dataarray(plotfunc, x, y, **kwargs)", "response": "This method is used to make faceted plots from 2d plotting."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef map_dataarray(self, func, x, y, **kwargs):\n\n        if kwargs.get('cbar_ax', None) is not None:\n            raise ValueError('cbar_ax not supported by FacetGrid.')\n\n        cmap_params, cbar_kwargs = _process_cmap_cbar_kwargs(\n            func, kwargs, self.data.values)\n\n        self._cmap_extend = cmap_params.get('extend')\n\n        # Order is important\n        func_kwargs = kwargs.copy()\n        func_kwargs.update(cmap_params)\n        func_kwargs.update({'add_colorbar': False, 'add_labels': False})\n\n        # Get x, y labels for the first subplot\n        x, y = _infer_xy_labels(\n            darray=self.data.loc[self.name_dicts.flat[0]], x=x, y=y,\n            imshow=func.__name__ == 'imshow', rgb=kwargs.get('rgb', None))\n\n        for d, ax in zip(self.name_dicts.flat, self.axes.flat):\n            # None is the sentinel value\n            if d is not None:\n                subset = self.data.loc[d]\n                mappable = func(subset, x=x, y=y, ax=ax, **func_kwargs)\n                self._mappables.append(mappable)\n\n        self._cmap_extend = cmap_params.get('extend')\n        self._finalize_grid(x, y)\n\n        if kwargs.get('add_colorbar', True):\n            self.add_colorbar(**cbar_kwargs)\n\n        return self", "response": "Applies a function to a 2d xarray containing a subset of the data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfinalize the annotations and layout.", "response": "def _finalize_grid(self, *axlabels):\n        \"\"\"Finalize the annotations and layout.\"\"\"\n        if not self._finalized:\n            self.set_axis_labels(*axlabels)\n            self.set_titles()\n            self.fig.tight_layout()\n\n            for ax, namedict in zip(self.axes.flat, self.name_dicts.flat):\n                if namedict is None:\n                    ax.set_visible(False)\n\n            self._finalized = True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndrawing a colorbar for the current object", "response": "def add_colorbar(self, **kwargs):\n        \"\"\"Draw a colorbar\n        \"\"\"\n        kwargs = kwargs.copy()\n        if self._cmap_extend is not None:\n            kwargs.setdefault('extend', self._cmap_extend)\n        if 'label' not in kwargs:\n            kwargs.setdefault('label', label_from_attrs(self.data))\n        self.cbar = self.fig.colorbar(self._mappables[-1],\n                                      ax=list(self.axes.flat),\n                                      **kwargs)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_axis_labels(self, x_var=None, y_var=None):\n        if x_var is not None:\n            if x_var in self.data.coords:\n                self._x_var = x_var\n                self.set_xlabels(label_from_attrs(self.data[x_var]))\n            else:\n                # x_var is a string\n                self.set_xlabels(x_var)\n\n        if y_var is not None:\n            if y_var in self.data.coords:\n                self._y_var = y_var\n                self.set_ylabels(label_from_attrs(self.data[y_var]))\n            else:\n                self.set_ylabels(y_var)\n        return self", "response": "Set axis labels on the left column and bottom row of the grid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlabeling the x axis on the bottom row of the grid.", "response": "def set_xlabels(self, label=None, **kwargs):\n        \"\"\"Label the x axis on the bottom row of the grid.\"\"\"\n        if label is None:\n            label = label_from_attrs(self.data[self._x_var])\n        for ax in self._bottom_axes:\n            ax.set_xlabel(label, **kwargs)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_ylabels(self, label=None, **kwargs):\n        if label is None:\n            label = label_from_attrs(self.data[self._y_var])\n        for ax in self._left_axes:\n            ax.set_ylabel(label, **kwargs)\n        return self", "response": "Label the y axis on the left column of the grid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_titles(self, template=\"{coord} = {value}\", maxchar=30,\n                   **kwargs):\n        \"\"\"\n        Draw titles either above each facet or on the grid margins.\n\n        Parameters\n        ----------\n        template : string\n            Template for plot titles containing {coord} and {value}\n        maxchar : int\n            Truncate titles at maxchar\n        kwargs : keyword args\n            additional arguments to matplotlib.text\n\n        Returns\n        -------\n        self: FacetGrid object\n\n        \"\"\"\n        import matplotlib as mpl\n\n        kwargs[\"size\"] = kwargs.pop(\"size\", mpl.rcParams[\"axes.labelsize\"])\n\n        nicetitle = functools.partial(_nicetitle, maxchar=maxchar,\n                                      template=template)\n\n        if self._single_group:\n            for d, ax in zip(self.name_dicts.flat, self.axes.flat):\n                # Only label the ones with data\n                if d is not None:\n                    coord, value = list(d.items()).pop()\n                    title = nicetitle(coord, value, maxchar=maxchar)\n                    ax.set_title(title, **kwargs)\n        else:\n            # The row titles on the right edge of the grid\n            for ax, row_name in zip(self.axes[:, -1], self.row_names):\n                title = nicetitle(coord=self._row_var, value=row_name,\n                                  maxchar=maxchar)\n                ax.annotate(title, xy=(1.02, .5), xycoords=\"axes fraction\",\n                            rotation=270, ha=\"left\", va=\"center\", **kwargs)\n\n            # The column titles on the top row\n            for ax, col_name in zip(self.axes[0, :], self.col_names):\n                title = nicetitle(coord=self._col_var, value=col_name,\n                                  maxchar=maxchar)\n                ax.set_title(title, **kwargs)\n\n        return self", "response": "Draw titles on the grid and the grid margins."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset and control tick behavior on x and y axes.", "response": "def set_ticks(self, max_xticks=_NTICKS, max_yticks=_NTICKS,\n                  fontsize=_FONTSIZE):\n        \"\"\"\n        Set and control tick behavior\n\n        Parameters\n        ----------\n        max_xticks, max_yticks : int, optional\n            Maximum number of labeled ticks to plot on x, y axes\n        fontsize : string or int\n            Font size as used by matplotlib text\n\n        Returns\n        -------\n        self : FacetGrid object\n\n        \"\"\"\n        from matplotlib.ticker import MaxNLocator\n\n        # Both are necessary\n        x_major_locator = MaxNLocator(nbins=max_xticks)\n        y_major_locator = MaxNLocator(nbins=max_yticks)\n\n        for ax in self.axes.flat:\n            ax.xaxis.set_major_locator(x_major_locator)\n            ax.yaxis.set_major_locator(y_major_locator)\n            for tick in itertools.chain(ax.xaxis.get_major_ticks(),\n                                        ax.yaxis.get_major_ticks()):\n                tick.label.set_fontsize(fontsize)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef map(self, func, *args, **kwargs):\n        plt = import_matplotlib_pyplot()\n\n        for ax, namedict in zip(self.axes.flat, self.name_dicts.flat):\n            if namedict is not None:\n                data = self.data.loc[namedict]\n                plt.sca(ax)\n                innerargs = [data[a].values for a in args]\n                maybe_mappable = func(*innerargs, **kwargs)\n                # TODO: better way to verify that an artist is mappable?\n                # https://stackoverflow.com/questions/33023036/is-it-possible-to-detect-if-a-matplotlib-artist-is-a-mappable-suitable-for-use-w#33023522\n                if (maybe_mappable and\n                   hasattr(maybe_mappable, 'autoscale_None')):\n                    self._mappables.append(maybe_mappable)\n\n        self._finalize_grid(*args[:2])\n\n        return self", "response": "Applies a function to each facet s subset of the data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_time_bins(index, freq, closed, label, base):\n\n    if not isinstance(index, CFTimeIndex):\n        raise TypeError('index must be a CFTimeIndex, but got '\n                        'an instance of %r' % type(index).__name__)\n    if len(index) == 0:\n        datetime_bins = labels = CFTimeIndex(data=[], name=index.name)\n        return datetime_bins, labels\n\n    first, last = _get_range_edges(index.min(), index.max(), freq,\n                                   closed=closed,\n                                   base=base)\n    datetime_bins = labels = cftime_range(freq=freq,\n                                          start=first,\n                                          end=last,\n                                          name=index.name)\n\n    datetime_bins, labels = _adjust_bin_edges(datetime_bins, freq, closed,\n                                              index, labels)\n\n    if label == 'right':\n        labels = labels[1:]\n    else:\n        labels = labels[:-1]\n\n    # TODO: when CFTimeIndex supports missing values, if the reference index\n    # contains missing values, insert the appropriate NaN value at the\n    # beginning of the datetime_bins and labels indexes.\n\n    return datetime_bins, labels", "response": "Returns the bin labels and the corresponding datetime_bins for resampling operations."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadjusts the bin edges of a single time series to match the given bin frequencies.", "response": "def _adjust_bin_edges(datetime_bins, offset, closed, index, labels):\n    \"\"\"This is required for determining the bin edges resampling with\n    daily frequencies greater than one day, month end, and year end\n    frequencies.\n\n    Consider the following example.  Let's say you want to downsample the\n    time series with the following coordinates to month end frequency:\n\n    CFTimeIndex([2000-01-01 12:00:00, 2000-01-31 12:00:00,\n                 2000-02-01 12:00:00], dtype='object')\n\n    Without this adjustment, _get_time_bins with month-end frequency will\n    return the following index for the bin edges (default closed='right' and\n    label='right' in this case):\n\n    CFTimeIndex([1999-12-31 00:00:00, 2000-01-31 00:00:00,\n                 2000-02-29 00:00:00], dtype='object')\n\n    If 2000-01-31 is used as a bound for a bin, the value on\n    2000-01-31T12:00:00 (at noon on January 31st), will not be included in the\n    month of January.  To account for this, pandas adds a day minus one worth\n    of microseconds to the bin edges generated by cftime range, so that we do\n    bin the value at noon on January 31st in the January bin.  This results in\n    an index with bin edges like the following:\n\n    CFTimeIndex([1999-12-31 23:59:59, 2000-01-31 23:59:59,\n                 2000-02-29 23:59:59], dtype='object')\n\n    The labels are still:\n\n    CFTimeIndex([2000-01-31 00:00:00, 2000-02-29 00:00:00], dtype='object')\n\n    This is also required for daily frequencies longer than one day and\n    year-end frequencies.\n    \"\"\"\n    is_super_daily = (isinstance(offset, (MonthEnd, QuarterEnd, YearEnd)) or\n                      (isinstance(offset, Day) and offset.n > 1))\n    if is_super_daily:\n        if closed == 'right':\n            datetime_bins = datetime_bins + datetime.timedelta(days=1,\n                                                               microseconds=-1)\n        if datetime_bins[-2] > index.max():\n            datetime_bins = datetime_bins[:-1]\n            labels = labels[:-1]\n\n    return datetime_bins, labels"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the correct starting and ending datetimes for the resampled CFTimeIndex range.", "response": "def _get_range_edges(first, last, offset, closed='left', base=0):\n    \"\"\" Get the correct starting and ending datetimes for the resampled\n    CFTimeIndex range.\n\n    Parameters\n    ----------\n    first : cftime.datetime\n        Uncorrected starting datetime object for resampled CFTimeIndex range.\n        Usually the min of the original CFTimeIndex.\n    last : cftime.datetime\n        Uncorrected ending datetime object for resampled CFTimeIndex range.\n        Usually the max of the original CFTimeIndex.\n    offset : xarray.coding.cftime_offsets.BaseCFTimeOffset\n        The offset object representing target conversion a.k.a. resampling\n        frequency. Contains information on offset type (e.g. Day or 'D') and\n        offset magnitude (e.g., n = 3).\n    closed : 'left' or 'right', optional\n        Which side of bin interval is closed. Defaults to 'left'.\n    base : int, optional\n        For frequencies that evenly subdivide 1 day, the \"origin\" of the\n        aggregated intervals. For example, for '5min' frequency, base could\n        range from 0 through 4. Defaults to 0.\n\n    Returns\n    -------\n    first : cftime.datetime\n        Corrected starting datetime object for resampled CFTimeIndex range.\n    last : cftime.datetime\n        Corrected ending datetime object for resampled CFTimeIndex range.\n    \"\"\"\n    if isinstance(offset, CFTIME_TICKS):\n        first, last = _adjust_dates_anchored(first, last, offset,\n                                             closed=closed, base=base)\n        return first, last\n    else:\n        first = normalize_date(first)\n        last = normalize_date(last)\n\n    if closed == 'left':\n        first = offset.rollback(first)\n    else:\n        first = first - offset\n\n    last = last + offset\n    return first, last"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _adjust_dates_anchored(first, last, offset, closed='right', base=0):\n\n    base = base % offset.n\n    start_day = normalize_date(first)\n    base_td = type(offset)(n=base).as_timedelta()\n    start_day += base_td\n    foffset = exact_cftime_datetime_difference(\n        start_day, first) % offset.as_timedelta()\n    loffset = exact_cftime_datetime_difference(\n        start_day, last) % offset.as_timedelta()\n    if closed == 'right':\n        if foffset.total_seconds() > 0:\n            fresult = first - foffset\n        else:\n            fresult = first - offset.as_timedelta()\n\n        if loffset.total_seconds() > 0:\n            lresult = last + (offset.as_timedelta() - loffset)\n        else:\n            lresult = last\n    else:\n        if foffset.total_seconds() > 0:\n            fresult = first - foffset\n        else:\n            fresult = first\n\n        if loffset.total_seconds() > 0:\n            lresult = last + (offset.as_timedelta() - loffset)\n        else:\n            lresult = last + offset.as_timedelta()\n    return fresult, lresult", "response": "Adjusts the dates of the first and last time series to fix resampling errors."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexact computation of a and b.", "response": "def exact_cftime_datetime_difference(a, b):\n    \"\"\"Exact computation of b - a\n\n    Assumes:\n\n        a = a_0 + a_m\n        b = b_0 + b_m\n\n    Here a_0, and b_0 represent the input dates rounded\n    down to the nearest second, and a_m, and b_m represent\n    the remaining microseconds associated with date a and\n    date b.\n\n    We can then express the value of b - a as:\n\n        b - a = (b_0 + b_m) - (a_0 + a_m) = b_0 - a_0 + b_m - a_m\n\n    By construction, we know that b_0 - a_0 must be a round number\n    of seconds.  Therefore we can take the result of b_0 - a_0 using\n    ordinary cftime.datetime arithmetic and round to the nearest\n    second.  b_m - a_m is the remainder, in microseconds, and we\n    can simply add this to the rounded timedelta.\n\n    Parameters\n    ----------\n    a : cftime.datetime\n        Input datetime\n    b : cftime.datetime\n        Input datetime\n\n    Returns\n    -------\n    datetime.timedelta\n    \"\"\"\n    seconds = b.replace(microsecond=0) - a.replace(microsecond=0)\n    seconds = int(round(seconds.total_seconds()))\n    microseconds = b.microsecond - a.microsecond\n    return datetime.timedelta(seconds=seconds, microseconds=microseconds)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef first_items(self, index):\n\n        datetime_bins, labels = _get_time_bins(index, self.freq, self.closed,\n                                               self.label, self.base)\n        if self.loffset is not None:\n            if isinstance(self.loffset, datetime.timedelta):\n                labels = labels + self.loffset\n            else:\n                labels = labels + to_offset(self.loffset)\n\n        # check binner fits data\n        if index[0] < datetime_bins[0]:\n            raise ValueError(\"Value falls before first bin\")\n        if index[-1] > datetime_bins[-1]:\n            raise ValueError(\"Value falls after last bin\")\n\n        integer_bins = np.searchsorted(\n            index, datetime_bins, side=self.closed)[:-1]\n        first_items = pd.Series(integer_bins, labels)\n\n        # Mask duplicate values with NaNs, preserving the last values\n        non_duplicate = ~first_items.duplicated('last')\n        return first_items.where(non_duplicate)", "response": "Meant to reproduce the results of the following\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef align(*objects, **kwargs):\n    join = kwargs.pop('join', 'inner')\n    copy = kwargs.pop('copy', True)\n    indexes = kwargs.pop('indexes', None)\n    exclude = kwargs.pop('exclude', _DEFAULT_EXCLUDE)\n    if indexes is None:\n        indexes = {}\n    if kwargs:\n        raise TypeError('align() got unexpected keyword arguments: %s'\n                        % list(kwargs))\n\n    if not indexes and len(objects) == 1:\n        # fast path for the trivial case\n        obj, = objects\n        return (obj.copy(deep=copy),)\n\n    all_indexes = defaultdict(list)\n    unlabeled_dim_sizes = defaultdict(set)\n    for obj in objects:\n        for dim in obj.dims:\n            if dim not in exclude:\n                try:\n                    index = obj.indexes[dim]\n                except KeyError:\n                    unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n                else:\n                    all_indexes[dim].append(index)\n\n    # We don't reindex over dimensions with all equal indexes for two reasons:\n    # - It's faster for the usual case (already aligned objects).\n    # - It ensures it's possible to do operations that don't require alignment\n    #   on indexes with duplicate values (which cannot be reindexed with\n    #   pandas). This is useful, e.g., for overwriting such duplicate indexes.\n    joiner = _get_joiner(join)\n    joined_indexes = {}\n    for dim, matching_indexes in all_indexes.items():\n        if dim in indexes:\n            index = utils.safe_cast_to_index(indexes[dim])\n            if (any(not index.equals(other) for other in matching_indexes) or\n                    dim in unlabeled_dim_sizes):\n                joined_indexes[dim] = index\n        else:\n            if (any(not matching_indexes[0].equals(other)\n                    for other in matching_indexes[1:]) or\n                    dim in unlabeled_dim_sizes):\n                if join == 'exact':\n                    raise ValueError(\n                        'indexes along dimension {!r} are not equal'\n                        .format(dim))\n                index = joiner(matching_indexes)\n                joined_indexes[dim] = index\n            else:\n                index = matching_indexes[0]\n\n        if dim in unlabeled_dim_sizes:\n            unlabeled_sizes = unlabeled_dim_sizes[dim]\n            labeled_size = index.size\n            if len(unlabeled_sizes | {labeled_size}) > 1:\n                raise ValueError(\n                    'arguments without labels along dimension %r cannot be '\n                    'aligned because they have different dimension size(s) %r '\n                    'than the size of the aligned dimension labels: %r'\n                    % (dim, unlabeled_sizes, labeled_size))\n\n    for dim in unlabeled_dim_sizes:\n        if dim not in all_indexes:\n            sizes = unlabeled_dim_sizes[dim]\n            if len(sizes) > 1:\n                raise ValueError(\n                    'arguments without labels along dimension %r cannot be '\n                    'aligned because they have different dimension sizes: %r'\n                    % (dim, sizes))\n\n    result = []\n    for obj in objects:\n        valid_indexers = {k: v for k, v in joined_indexes.items()\n                          if k in obj.dims}\n        if not valid_indexers:\n            # fast path for no reindexing necessary\n            new_obj = obj.copy(deep=copy)\n        else:\n            new_obj = obj.reindex(copy=copy, **valid_indexers)\n        new_obj.encoding = obj.encoding\n        result.append(new_obj)\n\n    return tuple(result)", "response": "Aligns the objects in the input list with the same size as the input list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef deep_align(objects, join='inner', copy=True, indexes=None,\n               exclude=frozenset(), raise_on_invalid=True):\n    \"\"\"Align objects for merging, recursing into dictionary values.\n\n    This function is not public API.\n    \"\"\"\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n    if indexes is None:\n        indexes = {}\n\n    def is_alignable(obj):\n        return isinstance(obj, (DataArray, Dataset))\n\n    positions = []\n    keys = []\n    out = []\n    targets = []\n    no_key = object()\n    not_replaced = object()\n    for n, variables in enumerate(objects):\n        if is_alignable(variables):\n            positions.append(n)\n            keys.append(no_key)\n            targets.append(variables)\n            out.append(not_replaced)\n        elif is_dict_like(variables):\n            for k, v in variables.items():\n                if is_alignable(v) and k not in indexes:\n                    # Skip variables in indexes for alignment, because these\n                    # should to be overwritten instead:\n                    # https://github.com/pydata/xarray/issues/725\n                    positions.append(n)\n                    keys.append(k)\n                    targets.append(v)\n            out.append(OrderedDict(variables))\n        elif raise_on_invalid:\n            raise ValueError('object to align is neither an xarray.Dataset, '\n                             'an xarray.DataArray nor a dictionary: %r'\n                             % variables)\n        else:\n            out.append(variables)\n\n    aligned = align(*targets, join=join, copy=copy, indexes=indexes,\n                    exclude=exclude)\n\n    for position, key, aligned_obj in zip(positions, keys, aligned):\n        if key is no_key:\n            out[position] = aligned_obj\n        else:\n            out[position][key] = aligned_obj\n\n    # something went wrong: we should have replaced all sentinel values\n    assert all(arg is not not_replaced for arg in out)\n\n    return out", "response": "Align objects for merging into dictionary values."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reindex_like_indexers(target, other):\n    indexers = {k: v for k, v in other.indexes.items() if k in target.dims}\n\n    for dim in other.dims:\n        if dim not in indexers and dim in target.dims:\n            other_size = other.sizes[dim]\n            target_size = target.sizes[dim]\n            if other_size != target_size:\n                raise ValueError('different size for unlabeled '\n                                 'dimension on argument %r: %r vs %r'\n                                 % (dim, other_size, target_size))\n    return indexers", "response": "Extract indexers to align target with other."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reindex_variables(\n    variables: Mapping[Any, Variable],\n    sizes: Mapping[Any, int],\n    indexes: Mapping[Any, pd.Index],\n    indexers: Mapping,\n    method: Optional[str] = None,\n    tolerance: Any = None,\n    copy: bool = True,\n) -> 'Tuple[OrderedDict[Any, Variable], OrderedDict[Any, pd.Index]]':\n    \"\"\"Conform a dictionary of aligned variables onto a new set of variables,\n    filling in missing values with NaN.\n\n    Not public API.\n\n    Parameters\n    ----------\n    variables : dict-like\n        Dictionary of xarray.Variable objects.\n    sizes : dict-like\n        Dictionary from dimension names to integer sizes.\n    indexes : dict-like\n        Dictionary of indexes associated with variables.\n    indexers : dict\n        Dictionary with keys given by dimension names and values given by\n        arrays of coordinates tick labels. Any mis-matched coordinate values\n        will be filled in with NaN, and any mis-matched dimension names will\n        simply be ignored.\n    method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n        Method to use for filling index values in ``indexers`` not found in\n        this dataset:\n          * None (default): don't fill gaps\n          * pad / ffill: propagate last valid index value forward\n          * backfill / bfill: propagate next valid index value backward\n          * nearest: use nearest valid index value\n    tolerance : optional\n        Maximum distance between original and new labels for inexact matches.\n        The values of the index at the matching locations must satisfy the\n        equation ``abs(index[indexer] - target) <= tolerance``.\n    copy : bool, optional\n        If ``copy=True``, data in the return values is always copied. If\n        ``copy=False`` and reindexing is unnecessary, or can be performed\n        with only slice operations, then the output may share memory with\n        the input. In either case, new xarray objects are always returned.\n\n    Returns\n    -------\n    reindexed : OrderedDict\n        Dict of reindexed variables.\n    new_indexes : OrderedDict\n        Dict of indexes associated with the reindexed variables.\n    \"\"\"\n    from .dataarray import DataArray\n\n    # create variables for the new dataset\n    reindexed = OrderedDict()  # type: OrderedDict[Any, Variable]\n\n    # build up indexers for assignment along each dimension\n    int_indexers = {}\n    new_indexes = OrderedDict(indexes)\n    masked_dims = set()\n    unchanged_dims = set()\n\n    for dim, indexer in indexers.items():\n        if isinstance(indexer, DataArray) and indexer.dims != (dim,):\n            warnings.warn(\n                \"Indexer has dimensions {0:s} that are different \"\n                \"from that to be indexed along {1:s}. \"\n                \"This will behave differently in the future.\".format(\n                    str(indexer.dims), dim),\n                FutureWarning, stacklevel=3)\n\n        target = new_indexes[dim] = utils.safe_cast_to_index(indexers[dim])\n\n        if dim in indexes:\n            index = indexes[dim]\n\n            if not index.is_unique:\n                raise ValueError(\n                    'cannot reindex or align along dimension %r because the '\n                    'index has duplicate values' % dim)\n\n            int_indexer = get_indexer_nd(index, target, method, tolerance)\n\n            # We uses negative values from get_indexer_nd to signify\n            # values that are missing in the index.\n            if (int_indexer < 0).any():\n                masked_dims.add(dim)\n            elif np.array_equal(int_indexer, np.arange(len(index))):\n                unchanged_dims.add(dim)\n\n            int_indexers[dim] = int_indexer\n\n        if dim in variables:\n            var = variables[dim]\n            args = (var.attrs, var.encoding)  # type: tuple\n        else:\n            args = ()\n        reindexed[dim] = IndexVariable((dim,), target, *args)\n\n    for dim in sizes:\n        if dim not in indexes and dim in indexers:\n            existing_size = sizes[dim]\n            new_size = indexers[dim].size\n            if existing_size != new_size:\n                raise ValueError(\n                    'cannot reindex or align along dimension %r without an '\n                    'index because its size %r is different from the size of '\n                    'the new index %r' % (dim, existing_size, new_size))\n\n    for name, var in variables.items():\n        if name not in indexers:\n            key = tuple(slice(None)\n                        if d in unchanged_dims\n                        else int_indexers.get(d, slice(None))\n                        for d in var.dims)\n            needs_masking = any(d in masked_dims for d in var.dims)\n\n            if needs_masking:\n                new_var = var._getitem_with_mask(key)\n            elif all(is_full_slice(k) for k in key):\n                # no reindexing necessary\n                # here we need to manually deal with copying data, since\n                # we neither created a new ndarray nor used fancy indexing\n                new_var = var.copy(deep=copy)\n            else:\n                new_var = var[key]\n\n            reindexed[name] = new_var\n\n    return reindexed, new_indexes", "response": "Conform a dictionary of aligned variables onto a new set of variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef assert_coordinate_consistent(obj, coords):\n    for k in obj.dims:\n        # make sure there are no conflict in dimension coordinates\n        if k in coords and k in obj.coords:\n            if not coords[k].equals(obj[k].variable):\n                raise IndexError(\n                    'dimension coordinate {!r} conflicts between '\n                    'indexed and indexing objects:\\n{}\\nvs.\\n{}'\n                    .format(k, obj[k], coords[k]))", "response": "Maeke sure the dimension coordinate of obj is\n    consistent with coords."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remap_label_indexers(obj, indexers=None, method=None, tolerance=None,\n                         **indexers_kwargs):\n    \"\"\"\n    Remap **indexers from obj.coords.\n    If indexer is an instance of DataArray and it has coordinate, then this\n    coordinate will be attached to pos_indexers.\n\n    Returns\n    -------\n    pos_indexers: Same type of indexers.\n        np.ndarray or Variable or DataArra\n    new_indexes: mapping of new dimensional-coordinate.\n    \"\"\"\n    from .dataarray import DataArray\n    indexers = either_dict_or_kwargs(\n        indexers, indexers_kwargs, 'remap_label_indexers')\n\n    v_indexers = {k: v.variable.data if isinstance(v, DataArray) else v\n                  for k, v in indexers.items()}\n\n    pos_indexers, new_indexes = indexing.remap_label_indexers(\n        obj, v_indexers, method=method, tolerance=tolerance\n    )\n    # attach indexer's coordinate to pos_indexers\n    for k, v in indexers.items():\n        if isinstance(v, Variable):\n            pos_indexers[k] = Variable(v.dims, pos_indexers[k])\n        elif isinstance(v, DataArray):\n            # drop coordinates found in indexers since .sel() already\n            # ensures alignments\n            coords = OrderedDict((k, v) for k, v in v._coords.items()\n                                 if k not in indexers)\n            pos_indexers[k] = DataArray(pos_indexers[k],\n                                        coords=coords, dims=v.dims)\n    return pos_indexers, new_indexes", "response": "Remap indexers from obj. coords to new dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert all index coordinates into a pandas. Index.", "response": "def to_index(self, ordered_dims=None):\n        \"\"\"Convert all index coordinates into a :py:class:`pandas.Index`.\n\n        Parameters\n        ----------\n        ordered_dims : sequence, optional\n            Possibly reordered version of this object's dimensions indicating\n            the order in which dimensions should appear on the result.\n\n        Returns\n        -------\n        pandas.Index\n            Index subclass corresponding to the outer-product of all dimension\n            coordinates. This will be a MultiIndex if this object is has more\n            than more dimension.\n        \"\"\"\n        if ordered_dims is None:\n            ordered_dims = self.dims\n        elif set(ordered_dims) != set(self.dims):\n            raise ValueError('ordered_dims must match dims, but does not: '\n                             '{} vs {}'.format(ordered_dims, self.dims))\n\n        if len(ordered_dims) == 0:\n            raise ValueError('no valid index for a 0-dimensional object')\n        elif len(ordered_dims) == 1:\n            (dim,) = ordered_dims\n            return self._data.get_index(dim)\n        else:\n            indexes = [self._data.get_index(k) for k in ordered_dims]\n            names = list(ordered_dims)\n            return pd.MultiIndex.from_product(indexes, names=names)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _merge_inplace(self, other):\n        if other is None:\n            yield\n        else:\n            # don't include indexes in priority_vars, because we didn't align\n            # first\n            priority_vars = OrderedDict(\n                kv for kv in self.variables.items() if kv[0] not in self.dims)\n            variables = merge_coords_for_inplace_math(\n                [self.variables, other.variables], priority_vars=priority_vars)\n            yield\n            self._update_coords(variables)", "response": "For use with in - place binary arithmetic."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmerge two sets of coordinates to create a new Dataset The method implements the logic used for joining coordinates in the result of a binary operation performed on xarray objects: - If two index coordinates conflict (are not equal), an exception is raised. You must align your data before passing it to this method. - If an index coordinate and a non-index coordinate conflict, the non- index coordinate is dropped. - If two non-index coordinates conflict, both are dropped. Parameters ---------- other : DatasetCoordinates or DataArrayCoordinates The coordinates from another dataset or data array. Returns ------- merged : Dataset A new Dataset with merged coordinates.", "response": "def merge(self, other):\n        \"\"\"Merge two sets of coordinates to create a new Dataset\n\n        The method implements the logic used for joining coordinates in the\n        result of a binary operation performed on xarray objects:\n\n        - If two index coordinates conflict (are not equal), an exception is\n          raised. You must align your data before passing it to this method.\n        - If an index coordinate and a non-index coordinate conflict, the non-\n          index coordinate is dropped.\n        - If two non-index coordinates conflict, both are dropped.\n\n        Parameters\n        ----------\n        other : DatasetCoordinates or DataArrayCoordinates\n            The coordinates from another dataset or data array.\n\n        Returns\n        -------\n        merged : Dataset\n            A new Dataset with merged coordinates.\n        \"\"\"\n        from .dataset import Dataset\n\n        if other is None:\n            return self.to_dataset()\n        else:\n            other_vars = getattr(other, 'variables', other)\n            coords = expand_and_merge_variables([self.variables, other_vars])\n            return Dataset._from_vars_and_coord_names(coords, set(coords))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ipython_key_completions_(self):\n        return [key for key in self._data._ipython_key_completions_()\n                if key not in self._data.data_vars]", "response": "Provide method for the key - completerions in IPython."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwraps to apply bottleneck moving window funcs on dask arrays", "response": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1):\n    '''wrapper to apply bottleneck moving window funcs on dask arrays'''\n    dtype, fill_value = dtypes.maybe_promote(a.dtype)\n    a = a.astype(dtype)\n    # inputs for overlap\n    if axis < 0:\n        axis = a.ndim + axis\n    depth = {d: 0 for d in range(a.ndim)}\n    depth[axis] = (window + 1) // 2\n    boundary = {d: fill_value for d in range(a.ndim)}\n    # Create overlap array.\n    ag = overlap(a, depth=depth, boundary=boundary)\n    # apply rolling func\n    out = ag.map_blocks(moving_func, window, min_count=min_count,\n                        axis=axis, dtype=a.dtype)\n    # trim array\n    result = trim_internal(out, depth)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rolling_window(a, axis, window, center, fill_value):\n    orig_shape = a.shape\n    if axis < 0:\n        axis = a.ndim + axis\n    depth = {d: 0 for d in range(a.ndim)}\n    depth[axis] = int(window / 2)\n    # For evenly sized window, we need to crop the first point of each block.\n    offset = 1 if window % 2 == 0 else 0\n\n    if depth[axis] > min(a.chunks[axis]):\n        raise ValueError(\n            \"For window size %d, every chunk should be larger than %d, \"\n            \"but the smallest chunk size is %d. Rechunk your array\\n\"\n            \"with a larger chunk size or a chunk size that\\n\"\n            \"more evenly divides the shape of your array.\" %\n            (window, depth[axis], min(a.chunks[axis])))\n\n    # Although dask.overlap pads values to boundaries of the array,\n    # the size of the generated array is smaller than what we want\n    # if center == False.\n    if center:\n        start = int(window / 2)  # 10 -> 5,  9 -> 4\n        end = window - 1 - start\n    else:\n        start, end = window - 1, 0\n    pad_size = max(start, end) + offset - depth[axis]\n    drop_size = 0\n    # pad_size becomes more than 0 when the overlapped array is smaller than\n    # needed. In this case, we need to enlarge the original array by padding\n    # before overlapping.\n    if pad_size > 0:\n        if pad_size < depth[axis]:\n            # overlapping requires each chunk larger than depth. If pad_size is\n            # smaller than the depth, we enlarge this and truncate it later.\n            drop_size = depth[axis] - pad_size\n            pad_size = depth[axis]\n        shape = list(a.shape)\n        shape[axis] = pad_size\n        chunks = list(a.chunks)\n        chunks[axis] = (pad_size, )\n        fill_array = da.full(shape, fill_value, dtype=a.dtype, chunks=chunks)\n        a = da.concatenate([fill_array, a], axis=axis)\n\n    boundary = {d: fill_value for d in range(a.ndim)}\n\n    # create overlap arrays\n    ag = overlap(a, depth=depth, boundary=boundary)\n\n    # apply rolling func\n    def func(x, window, axis=-1):\n        x = np.asarray(x)\n        rolling = nputils._rolling_window(x, window, axis)\n        return rolling[(slice(None), ) * axis + (slice(offset, None), )]\n\n    chunks = list(a.chunks)\n    chunks.append(window)\n    out = ag.map_blocks(func, dtype=a.dtype, new_axis=a.ndim, chunks=chunks,\n                        window=window, axis=axis)\n\n    # crop boundary.\n    index = (slice(None),) * axis + (slice(drop_size,\n                                           drop_size + orig_shape[axis]), )\n    return out[index]", "response": "Dask s equivalence to np. utils. rolling_window"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining the size of the zarr array based on the encoding and variable data.", "response": "def _determine_zarr_chunks(enc_chunks, var_chunks, ndim):\n    \"\"\"\n    Given encoding chunks (possibly None) and variable chunks (possibly None)\n    \"\"\"\n\n    # zarr chunk spec:\n    # chunks : int or tuple of ints, optional\n    #   Chunk shape. If not provided, will be guessed from shape and dtype.\n\n    # if there are no chunks in encoding and the variable data is a numpy\n    # array, then we let zarr use its own heuristics to pick the chunks\n    if var_chunks is None and enc_chunks is None:\n        return None\n\n    # if there are no chunks in encoding but there are dask chunks, we try to\n    # use the same chunks in zarr\n    # However, zarr chunks needs to be uniform for each array\n    # http://zarr.readthedocs.io/en/latest/spec/v1.html#chunks\n    # while dask chunks can be variable sized\n    # http://dask.pydata.org/en/latest/array-design.html#chunks\n    if var_chunks and enc_chunks is None:\n        if any(len(set(chunks[:-1])) > 1 for chunks in var_chunks):\n            raise ValueError(\n                \"Zarr requires uniform chunk sizes except for final chunk.\"\n                \" Variable dask chunks %r are incompatible. Consider \"\n                \"rechunking using `chunk()`.\" % (var_chunks,))\n        if any((chunks[0] < chunks[-1]) for chunks in var_chunks):\n            raise ValueError(\n                \"Final chunk of Zarr array must be the same size or smaller \"\n                \"than the first. Variable Dask chunks %r are incompatible. \"\n                \"Consider rechunking using `chunk()`.\" % var_chunks)\n        # return the first chunk for each dimension\n        return tuple(chunk[0] for chunk in var_chunks)\n\n    # from here on, we are dealing with user-specified chunks in encoding\n    # zarr allows chunks to be an integer, in which case it uses the same chunk\n    # size on each dimension.\n    # Here we re-implement this expansion ourselves. That makes the logic of\n    # checking chunk compatibility easier\n\n    if isinstance(enc_chunks, integer_types):\n        enc_chunks_tuple = ndim * (enc_chunks,)\n    else:\n        enc_chunks_tuple = tuple(enc_chunks)\n\n    if len(enc_chunks_tuple) != ndim:\n        # throw away encoding chunks, start over\n        return _determine_zarr_chunks(None, var_chunks, ndim)\n\n    for x in enc_chunks_tuple:\n        if not isinstance(x, int):\n            raise TypeError(\"zarr chunks must be an int or a tuple of ints. \"\n                            \"Instead found %r\" % (enc_chunks_tuple,))\n\n    # if there are chunks in encoding and the variable data is a numpy array,\n    # we use the specified chunks\n    if var_chunks is None:\n        return enc_chunks_tuple\n\n    # the hard case\n    # DESIGN CHOICE: do not allow multiple dask chunks on a single zarr chunk\n    # this avoids the need to get involved in zarr synchronization / locking\n    # From zarr docs:\n    #  \"If each worker in a parallel computation is writing to a separate\n    #   region of the array, and if region boundaries are perfectly aligned\n    #   with chunk boundaries, then no synchronization is required.\"\n    # TODO: incorporate synchronizer to allow writes from multiple dask\n    # threads\n    if var_chunks and enc_chunks_tuple:\n        for zchunk, dchunks in zip(enc_chunks_tuple, var_chunks):\n            for dchunk in dchunks[:-1]:\n                if dchunk % zchunk:\n                    raise NotImplementedError(\n                        \"Specified zarr chunks %r would overlap multiple dask \"\n                        \"chunks %r. This is not implemented in xarray yet. \"\n                        \" Consider rechunking the data using \"\n                        \"`chunk()` or specifying different chunks in encoding.\"\n                        % (enc_chunks_tuple, var_chunks))\n            if dchunks[-1] > zchunk:\n                raise ValueError(\n                    \"Final chunk of Zarr array must be the same size or \"\n                    \"smaller than the first. The specified Zarr chunk \"\n                    \"encoding is %r, but %r in variable Dask chunks %r is \"\n                    \"incompatible. Consider rechunking using `chunk()`.\"\n                    % (enc_chunks_tuple, dchunks, var_chunks))\n        return enc_chunks_tuple\n\n    raise AssertionError(\n        \"We should never get here. Function logic must be wrong.\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef encode_zarr_variable(var, needs_copy=True, name=None):\n\n    var = conventions.encode_cf_variable(var, name=name)\n\n    # zarr allows unicode, but not variable-length strings, so it's both\n    # simpler and more compact to always encode as UTF-8 explicitly.\n    # TODO: allow toggling this explicitly via dtype in encoding.\n    coder = coding.strings.EncodedStringCoder(allows_unicode=False)\n    var = coder.encode(var, name=name)\n    var = coding.strings.ensure_fixed_length_bytes(var)\n\n    return var", "response": "Encodes a variable into a zarr - compatible variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads and decode a dataset from a Zarr store.", "response": "def open_zarr(store, group=None, synchronizer=None, chunks='auto',\n              decode_cf=True, mask_and_scale=True, decode_times=True,\n              concat_characters=True, decode_coords=True,\n              drop_variables=None, consolidated=False,\n              overwrite_encoded_chunks=False, **kwargs):\n    \"\"\"Load and decode a dataset from a Zarr store.\n\n    .. note:: Experimental\n              The Zarr backend is new and experimental. Please report any\n              unexpected behavior via github issues.\n\n    The `store` object should be a valid store for a Zarr group. `store`\n    variables must contain dimension metadata encoded in the\n    `_ARRAY_DIMENSIONS` attribute.\n\n    Parameters\n    ----------\n    store : MutableMapping or str\n        A MutableMapping where a Zarr Group has been stored or a path to a\n        directory in file system where a Zarr DirectoryStore has been stored.\n    synchronizer : object, optional\n        Array synchronizer provided to zarr\n    group : str, obtional\n        Group path. (a.k.a. `path` in zarr terminology.)\n    chunks : int or dict or tuple or {None, 'auto'}, optional\n        Chunk sizes along each dimension, e.g., ``5`` or\n        ``{'x': 5, 'y': 5}``. If `chunks='auto'`, dask chunks are created\n        based on the variable's zarr chunks. If `chunks=None`, zarr array\n        data will lazily convert to numpy arrays upon access. This accepts\n        all the chunk specifications as Dask does.\n    overwrite_encoded_chunks: bool, optional\n        Whether to drop the zarr chunks encoded for each variable when a\n        dataset is loaded with specified chunk sizes (default: False)\n    decode_cf : bool, optional\n        Whether to decode these variables, assuming they were saved according\n        to CF conventions.\n    mask_and_scale : bool, optional\n        If True, replace array values equal to `_FillValue` with NA and scale\n        values according to the formula `original_values * scale_factor +\n        add_offset`, where `_FillValue`, `scale_factor` and `add_offset` are\n        taken from variable attributes (if they exist).  If the `_FillValue` or\n        `missing_value` attribute contains multiple values a warning will be\n        issued and all array values matching one of the multiple values will\n        be replaced by NA.\n    decode_times : bool, optional\n        If True, decode times encoded in the standard NetCDF datetime format\n        into datetime objects. Otherwise, leave them encoded as numbers.\n    concat_characters : bool, optional\n        If True, concatenate along the last dimension of character arrays to\n        form string arrays. Dimensions will only be concatenated over (and\n        removed) if they have no corresponding variable and if they are only\n        used as the last dimension of character arrays.\n    decode_coords : bool, optional\n        If True, decode the 'coordinates' attribute to identify coordinates in\n        the resulting dataset.\n    drop_variables : string or iterable, optional\n        A variable or list of variables to exclude from being parsed from the\n        dataset. This may be useful to drop variables with problems or\n        inconsistent values.\n    consolidated : bool, optional\n        Whether to open the store using zarr's consolidated metadata\n        capability. Only works for stores that have already been consolidated.\n\n    Returns\n    -------\n    dataset : Dataset\n        The newly created dataset.\n\n    See Also\n    --------\n    open_dataset\n\n    References\n    ----------\n    http://zarr.readthedocs.io/\n    \"\"\"\n    if 'auto_chunk' in kwargs:\n        auto_chunk = kwargs.pop('auto_chunk')\n        if auto_chunk:\n            chunks = 'auto'  # maintain backwards compatibility\n        else:\n            chunks = None\n\n        warnings.warn(\"auto_chunk is deprecated. Use chunks='auto' instead.\",\n                      FutureWarning, stacklevel=2)\n\n    if kwargs:\n        raise TypeError(\"open_zarr() got unexpected keyword arguments \" +\n                        \",\".join(kwargs.keys()))\n\n    if not isinstance(chunks, (int, dict)):\n        if chunks != 'auto' and chunks is not None:\n            raise ValueError(\"chunks must be an int, dict, 'auto', or None. \"\n                             \"Instead found %s. \" % chunks)\n\n    if not decode_cf:\n        mask_and_scale = False\n        decode_times = False\n        concat_characters = False\n        decode_coords = False\n\n    def maybe_decode_store(store, lock=False):\n        ds = conventions.decode_cf(\n            store, mask_and_scale=mask_and_scale, decode_times=decode_times,\n            concat_characters=concat_characters, decode_coords=decode_coords,\n            drop_variables=drop_variables)\n\n        # TODO: this is where we would apply caching\n\n        return ds\n\n    # Zarr supports a wide range of access modes, but for now xarray either\n    # reads or writes from a store, never both. For open_zarr, we only read\n    mode = 'r'\n    zarr_store = ZarrStore.open_group(store, mode=mode,\n                                      synchronizer=synchronizer,\n                                      group=group, consolidated=consolidated)\n    ds = maybe_decode_store(zarr_store)\n\n    # auto chunking needs to be here and not in ZarrStore because variable\n    # chunks do not survive decode_cf\n    # return trivial case\n    if not chunks:\n        return ds\n\n    # adapted from Dataset.Chunk()\n    if isinstance(chunks, int):\n        chunks = dict.fromkeys(ds.dims, chunks)\n\n    if isinstance(chunks, tuple) and len(chunks) == len(ds.dims):\n        chunks = dict(zip(ds.dims, chunks))\n\n    def get_chunk(name, var, chunks):\n        chunk_spec = dict(zip(var.dims, var.encoding.get('chunks')))\n\n        # Coordinate labels aren't chunked\n        if var.ndim == 1 and var.dims[0] == name:\n            return chunk_spec\n\n        if chunks == 'auto':\n            return chunk_spec\n\n        for dim in var.dims:\n            if dim in chunks:\n                spec = chunks[dim]\n                if isinstance(spec, int):\n                    spec = (spec,)\n                if isinstance(spec, (tuple, list)) and chunk_spec[dim]:\n                    if any(s % chunk_spec[dim] for s in spec):\n                        warnings.warn(\"Specified Dask chunks %r would \"\n                                      \"separate Zarr chunk shape %r for \"\n                                      \"dimension %r. This significantly \"\n                                      \"degrades performance. Consider \"\n                                      \"rechunking after loading instead.\"\n                                      % (chunks[dim], chunk_spec[dim], dim),\n                                      stacklevel=2)\n                chunk_spec[dim] = chunks[dim]\n        return chunk_spec\n\n    def maybe_chunk(name, var, chunks):\n        from dask.base import tokenize\n\n        chunk_spec = get_chunk(name, var, chunks)\n\n        if (var.ndim > 0) and (chunk_spec is not None):\n            # does this cause any data to be read?\n            token2 = tokenize(name, var._data)\n            name2 = 'zarr-%s' % token2\n            var = var.chunk(chunk_spec, name=name2, lock=None)\n            if overwrite_encoded_chunks and var.chunks is not None:\n                var.encoding['chunks'] = tuple(x[0] for x in var.chunks)\n            return var\n        else:\n            return var\n\n    variables = OrderedDict([(k, maybe_chunk(k, v, chunks))\n                            for k, v in ds.variables.items()])\n    return ds._replace_vars_and_dims(variables)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting dimension sizes from a dictionary of variables.", "response": "def broadcast_dimension_size(\n    variables: List[Variable],\n) -> 'OrderedDict[Any, int]':\n    \"\"\"Extract dimension sizes from a dictionary of variables.\n\n    Raises ValueError if any dimensions have different sizes.\n    \"\"\"\n    dims = OrderedDict()  # type: OrderedDict[Any, int]\n    for var in variables:\n        for dim, size in zip(var.dims, var.shape):\n            if dim in dims and size != dims[dim]:\n                raise ValueError('index %r not aligned' % dim)\n            dims[dim] = size\n    return dims"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unique_variable(name, variables, compat='broadcast_equals'):\n    # type: (Any, List[Variable], str) -> Variable\n    \"\"\"Return the unique variable from a list of variables or raise MergeError.\n\n    Parameters\n    ----------\n    name : hashable\n        Name for this variable.\n    variables : list of xarray.Variable\n        List of Variable objects, all of which go by the same name in different\n        inputs.\n    compat : {'identical', 'equals', 'broadcast_equals', 'no_conflicts'}, optional\n        Type of equality check to use.\n\n    Returns\n    -------\n    Variable to use in the result.\n\n    Raises\n    ------\n    MergeError: if any of the variables are not equal.\n    \"\"\"  # noqa\n    out = variables[0]\n    if len(variables) > 1:\n        combine_method = None\n\n        if compat == 'minimal':\n            compat = 'broadcast_equals'\n\n        if compat == 'broadcast_equals':\n            dim_lengths = broadcast_dimension_size(variables)\n            out = out.set_dims(dim_lengths)\n\n        if compat == 'no_conflicts':\n            combine_method = 'fillna'\n\n        for var in variables[1:]:\n            if not getattr(out, compat)(var):\n                raise MergeError('conflicting values for variable %r on '\n                                 'objects to be combined:\\n'\n                                 'first value: %r\\nsecond value: %r'\n                                 % (name, out, var))\n            if combine_method:\n                # TODO: add preservation of attrs into fillna\n                out = getattr(out, combine_method)(var)\n                out.attrs = var.attrs\n\n    return out", "response": "Returns the unique variable from a list of variables."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef merge_variables(\n        list_of_variables_dicts,  # type: List[Mapping[Any, Variable]]\n        priority_vars=None,       # type: Optional[Mapping[Any, Variable]]\n        compat='minimal',         # type: str\n):\n    # type: (...) -> OrderedDict[Any, Variable]\n    \"\"\"Merge dicts of variables, while resolving conflicts appropriately.\n\n    Parameters\n    ----------\n    lists_of_variables_dicts : list of mappings with Variable values\n        List of mappings for which each value is a xarray.Variable object.\n    priority_vars : mapping with Variable or None values, optional\n        If provided, variables are always taken from this dict in preference to\n        the input variable dictionaries, without checking for conflicts.\n    compat : {'identical', 'equals', 'broadcast_equals', 'minimal', 'no_conflicts'}, optional\n        Type of equality check to use when checking for conflicts.\n\n    Returns\n    -------\n    OrderedDict with keys taken by the union of keys on list_of_variable_dicts,\n    and Variable values corresponding to those that should be found on the\n    merged result.\n    \"\"\"  # noqa\n    if priority_vars is None:\n        priority_vars = {}\n\n    _assert_compat_valid(compat)\n    dim_compat = min(compat, 'equals', key=_VALID_COMPAT.get)\n\n    lookup = OrderedDefaultDict(list)\n    for variables in list_of_variables_dicts:\n        for name, var in variables.items():\n            lookup[name].append(var)\n\n    # n.b. it's important to fill up merged in the original order in which\n    # variables appear\n    merged = OrderedDict()  # type: OrderedDict[Any, Variable]\n\n    for name, var_list in lookup.items():\n        if name in priority_vars:\n            # one of these arguments (e.g., the first for in-place arithmetic\n            # or the second for Dataset.update) takes priority\n            merged[name] = priority_vars[name]\n        else:\n            dim_variables = [var for var in var_list if (name,) == var.dims]\n            if dim_variables:\n                # if there are dimension coordinates, these must be equal (or\n                # identical), and they take priority over non-dimension\n                # coordinates\n                merged[name] = unique_variable(name, dim_variables, dim_compat)\n            else:\n                try:\n                    merged[name] = unique_variable(name, var_list, compat)\n                except MergeError:\n                    if compat != 'minimal':\n                        # we need more than \"minimal\" compatibility (for which\n                        # we drop conflicting coordinates)\n                        raise\n\n    return merged", "response": "Merges the variables in the list of variables_dicts into a single dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a list of dicts with xarray object values expand the values.", "response": "def expand_variable_dicts(\n    list_of_variable_dicts: 'List[Union[Dataset, OrderedDict]]',\n) -> 'List[Mapping[Any, Variable]]':\n    \"\"\"Given a list of dicts with xarray object values, expand the values.\n\n    Parameters\n    ----------\n    list_of_variable_dicts : list of dict or Dataset objects\n        Each value for the mappings must be of the following types:\n        - an xarray.Variable\n        - a tuple `(dims, data[, attrs[, encoding]])` that can be converted in\n          an xarray.Variable\n        - or an xarray.DataArray\n\n    Returns\n    -------\n    A list of ordered dictionaries corresponding to inputs, or coordinates from\n    an input's values. The values of each ordered dictionary are all\n    xarray.Variable objects.\n    \"\"\"\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n    var_dicts = []\n\n    for variables in list_of_variable_dicts:\n        if isinstance(variables, Dataset):\n            var_dicts.append(variables.variables)\n            continue\n\n        # append coords to var_dicts before appending sanitized_vars,\n        # because we want coords to appear first\n        sanitized_vars = OrderedDict()  # type: OrderedDict[Any, Variable]\n\n        for name, var in variables.items():\n            if isinstance(var, DataArray):\n                # use private API for speed\n                coords = var._coords.copy()\n                # explicitly overwritten variables should take precedence\n                coords.pop(name, None)\n                var_dicts.append(coords)\n\n            var = as_variable(var, name=name)\n            sanitized_vars[name] = var\n\n        var_dicts.append(sanitized_vars)\n\n    return var_dicts"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a list of dicts with xarray object values identify coordinates.", "response": "def determine_coords(list_of_variable_dicts):\n    # type: (List[Dict]) -> Tuple[Set, Set]\n    \"\"\"Given a list of dicts with xarray object values, identify coordinates.\n\n    Parameters\n    ----------\n    list_of_variable_dicts : list of dict or Dataset objects\n        Of the same form as the arguments to expand_variable_dicts.\n\n    Returns\n    -------\n    coord_names : set of variable names\n    noncoord_names : set of variable names\n        All variable found in the input should appear in either the set of\n        coordinate or non-coordinate names.\n    \"\"\"\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n    coord_names = set()  # type: set\n    noncoord_names = set()  # type: set\n\n    for variables in list_of_variable_dicts:\n        if isinstance(variables, Dataset):\n            coord_names.update(variables.coords)\n            noncoord_names.update(variables.data_vars)\n        else:\n            for name, var in variables.items():\n                if isinstance(var, DataArray):\n                    coords = set(var._coords)  # use private API for speed\n                    # explicitly overwritten variables should take precedence\n                    coords.discard(name)\n                    coord_names.update(coords)\n\n    return coord_names, noncoord_names"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef coerce_pandas_values(objects):\n    from .dataset import Dataset\n    from .dataarray import DataArray\n\n    out = []\n    for obj in objects:\n        if isinstance(obj, Dataset):\n            variables = obj\n        else:\n            variables = OrderedDict()\n            if isinstance(obj, PANDAS_TYPES):\n                obj = OrderedDict(obj.iteritems())\n            for k, v in obj.items():\n                if isinstance(v, PANDAS_TYPES):\n                    v = DataArray(v)\n                variables[k] = v\n        out.append(variables)\n    return out", "response": "Convert pandas values found in a list of labeled objects into native xarray objects."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmerge coordinate variables without worrying about alignment. This function is used for merging variables in coordinates.py.", "response": "def merge_coords_for_inplace_math(objs, priority_vars=None):\n    \"\"\"Merge coordinate variables without worrying about alignment.\n\n    This function is used for merging variables in coordinates.py.\n    \"\"\"\n    expanded = expand_variable_dicts(objs)\n    variables = merge_variables(expanded, priority_vars)\n    assert_unique_multiindex_level_names(variables)\n    return variables"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting the priority variable from a list of mappings. We need this method because in some cases the priority argument itself might have conflicting values (e.g., if it is a dict with two DataArray values with conflicting coordinate values). Parameters ---------- objects : list of dictionaries of variables Dictionaries in which to find the priority variables. priority_arg : int or None Integer object whose variable should take priority. compat : {'identical', 'equals', 'broadcast_equals', 'no_conflicts'}, optional Compatibility checks to use when merging variables. Returns ------- None, if priority_arg is None, or an OrderedDict with Variable objects as values indicating priority variables.", "response": "def _get_priority_vars(objects, priority_arg, compat='equals'):\n    \"\"\"Extract the priority variable from a list of mappings.\n\n    We need this method because in some cases the priority argument itself\n    might have conflicting values (e.g., if it is a dict with two DataArray\n    values with conflicting coordinate values).\n\n    Parameters\n    ----------\n    objects : list of dictionaries of variables\n        Dictionaries in which to find the priority variables.\n    priority_arg : int or None\n        Integer object whose variable should take priority.\n    compat : {'identical', 'equals', 'broadcast_equals', 'no_conflicts'}, optional\n        Compatibility checks to use when merging variables.\n\n    Returns\n    -------\n    None, if priority_arg is None, or an OrderedDict with Variable objects as\n    values indicating priority variables.\n    \"\"\"  # noqa\n    if priority_arg is None:\n        priority_vars = {}\n    else:\n        expanded = expand_variable_dicts([objects[priority_arg]])\n        priority_vars = merge_variables(expanded, compat=compat)\n    return priority_vars"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexpanding variables with worrying about alignment.", "response": "def expand_and_merge_variables(objs, priority_arg=None):\n    \"\"\"Merge coordinate variables without worrying about alignment.\n\n    This function is used for merging variables in computation.py.\n    \"\"\"\n    expanded = expand_variable_dicts(objs)\n    priority_vars = _get_priority_vars(objs, priority_arg)\n    variables = merge_variables(expanded, priority_vars)\n    return variables"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmerging coordinate variables. See merge_core below for argument descriptions. This works similarly to merge_core, except everything we don't worry about whether variables are coordinates or not.", "response": "def merge_coords(objs, compat='minimal', join='outer', priority_arg=None,\n                 indexes=None):\n    \"\"\"Merge coordinate variables.\n\n    See merge_core below for argument descriptions. This works similarly to\n    merge_core, except everything we don't worry about whether variables are\n    coordinates or not.\n    \"\"\"\n    _assert_compat_valid(compat)\n    coerced = coerce_pandas_values(objs)\n    aligned = deep_align(coerced, join=join, copy=False, indexes=indexes)\n    expanded = expand_variable_dicts(aligned)\n    priority_vars = _get_priority_vars(aligned, priority_arg, compat=compat)\n    variables = merge_variables(expanded, priority_vars, compat=compat)\n    assert_unique_multiindex_level_names(variables)\n\n    return variables"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nuses in Dataset. __init__.", "response": "def merge_data_and_coords(data, coords, compat='broadcast_equals',\n                          join='outer'):\n    \"\"\"Used in Dataset.__init__.\"\"\"\n    objs = [data, coords]\n    explicit_coords = coords.keys()\n    indexes = dict(extract_indexes(coords))\n    return merge_core(objs, compat, join, explicit_coords=explicit_coords,\n                      indexes=indexes)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nyields the name & index of valid indexes from a mapping of coords", "response": "def extract_indexes(coords):\n    \"\"\"Yields the name & index of valid indexes from a mapping of coords\"\"\"\n    for name, variable in coords.items():\n        variable = as_variable(variable, name=name)\n        if variable.dims == (name,):\n            yield name, variable.to_index()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvalidating explicit coordinate names/dims. Raise a MergeError if an explicit coord shares a name with a dimension but is comprised of arbitrary dimensions.", "response": "def assert_valid_explicit_coords(variables, dims, explicit_coords):\n    \"\"\"Validate explicit coordinate names/dims.\n\n    Raise a MergeError if an explicit coord shares a name with a dimension\n    but is comprised of arbitrary dimensions.\n    \"\"\"\n    for coord_name in explicit_coords:\n        if coord_name in dims and variables[coord_name].dims != (coord_name,):\n            raise MergeError(\n                'coordinate %s shares a name with a dataset dimension, but is '\n                'not a 1D variable along that dimension. This is disallowed '\n                'by the xarray data model.' % coord_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef merge_core(objs,\n               compat='broadcast_equals',\n               join='outer',\n               priority_arg=None,\n               explicit_coords=None,\n               indexes=None):\n    \"\"\"Core logic for merging labeled objects.\n\n    This is not public API.\n\n    Parameters\n    ----------\n    objs : list of mappings\n        All values must be convertable to labeled arrays.\n    compat : {'identical', 'equals', 'broadcast_equals', 'no_conflicts'}, optional\n        Compatibility checks to use when merging variables.\n    join : {'outer', 'inner', 'left', 'right'}, optional\n        How to combine objects with different indexes.\n    priority_arg : integer, optional\n        Optional argument in `objs` that takes precedence over the others.\n    explicit_coords : set, optional\n        An explicit list of variables from `objs` that are coordinates.\n    indexes : dict, optional\n        Dictionary with values given by pandas.Index objects.\n\n    Returns\n    -------\n    variables : OrderedDict\n        Ordered dictionary of Variable objects.\n    coord_names : set\n        Set of coordinate names.\n    dims : dict\n        Dictionary mapping from dimension names to sizes.\n\n    Raises\n    ------\n    MergeError if the merge cannot be done successfully.\n    \"\"\"  # noqa\n    from .dataset import calculate_dimensions\n\n    _assert_compat_valid(compat)\n\n    coerced = coerce_pandas_values(objs)\n    aligned = deep_align(coerced, join=join, copy=False, indexes=indexes)\n    expanded = expand_variable_dicts(aligned)\n\n    coord_names, noncoord_names = determine_coords(coerced)\n\n    priority_vars = _get_priority_vars(aligned, priority_arg, compat=compat)\n    variables = merge_variables(expanded, priority_vars, compat=compat)\n    assert_unique_multiindex_level_names(variables)\n\n    dims = calculate_dimensions(variables)\n\n    if explicit_coords is not None:\n        assert_valid_explicit_coords(variables, dims, explicit_coords)\n        coord_names.update(explicit_coords)\n\n    for dim, size in dims.items():\n        if dim in variables:\n            coord_names.add(dim)\n\n    ambiguous_coords = coord_names.intersection(noncoord_names)\n    if ambiguous_coords:\n        raise MergeError('unable to determine if these variables should be '\n                         'coordinates or not in the merged result: %s'\n                         % ambiguous_coords)\n\n    return variables, coord_names, dict(dims)", "response": "This function is not public API. It will merge labeled arrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmerges any number of xarray objects into a single Dataset.", "response": "def merge(objects, compat='no_conflicts', join='outer'):\n    \"\"\"Merge any number of xarray objects into a single Dataset as variables.\n\n    Parameters\n    ----------\n    objects : Iterable[Union[xarray.Dataset, xarray.DataArray, dict]]\n        Merge together all variables from these objects. If any of them are\n        DataArray objects, they must have a name.\n    compat : {'identical', 'equals', 'broadcast_equals', 'no_conflicts'}, optional\n        String indicating how to compare variables of the same name for\n        potential conflicts:\n\n        - 'broadcast_equals': all values must be equal when variables are\n          broadcast against each other to ensure common dimensions.\n        - 'equals': all values and dimensions must be the same.\n        - 'identical': all values, dimensions and attributes must be the\n          same.\n        - 'no_conflicts': only values which are not null in both datasets\n          must be equal. The returned dataset then contains the combination\n          of all non-null values.\n    join : {'outer', 'inner', 'left', 'right', 'exact'}, optional\n        How to combine objects with different indexes.\n\n    Returns\n    -------\n    Dataset\n        Dataset with combined variables from each object.\n\n    Examples\n    --------\n    >>> arrays = [xr.DataArray(n, name='var%d' % n) for n in range(5)]\n    >>> xr.merge(arrays)\n    <xarray.Dataset>\n    Dimensions:  ()\n    Coordinates:\n        *empty*\n    Data variables:\n        var0     int64 0\n        var1     int64 1\n        var2     int64 2\n        var3     int64 3\n        var4     int64 4\n\n    Raises\n    ------\n    xarray.MergeError\n        If any variables with the same name have conflicting values.\n\n    See also\n    --------\n    concat\n    \"\"\"  # noqa\n    from .dataarray import DataArray\n    from .dataset import Dataset\n\n    dict_like_objects = [\n        obj.to_dataset() if isinstance(obj, DataArray) else obj\n        for obj in objects]\n\n    variables, coord_names, dims = merge_core(dict_like_objects, compat, join)\n    # TODO: don't always recompute indexes\n    merged = Dataset._construct_direct(\n        variables, coord_names, dims, indexes=None)\n\n    return merged"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dataset_merge_method(dataset, other, overwrite_vars, compat, join):\n\n    # we are locked into supporting overwrite_vars for the Dataset.merge\n    # method due for backwards compatibility\n    # TODO: consider deprecating it?\n\n    if isinstance(overwrite_vars, str):\n        overwrite_vars = set([overwrite_vars])\n    overwrite_vars = set(overwrite_vars)\n\n    if not overwrite_vars:\n        objs = [dataset, other]\n        priority_arg = None\n    elif overwrite_vars == set(other):\n        objs = [dataset, other]\n        priority_arg = 1\n    else:\n        other_overwrite = OrderedDict()\n        other_no_overwrite = OrderedDict()\n        for k, v in other.items():\n            if k in overwrite_vars:\n                other_overwrite[k] = v\n            else:\n                other_no_overwrite[k] = v\n        objs = [dataset, other_no_overwrite, other_overwrite]\n        priority_arg = 2\n\n    return merge_core(objs, compat, join, priority_arg=priority_arg)", "response": "Guts of the Dataset. merge method."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dataset_update_method(dataset, other):\n    from .dataset import Dataset\n    from .dataarray import DataArray\n\n    if not isinstance(other, Dataset):\n        other = OrderedDict(other)\n        for key, value in other.items():\n            if isinstance(value, DataArray):\n                # drop conflicting coordinates\n                coord_names = [c for c in value.coords\n                               if c not in value.dims and c in dataset.coords]\n                if coord_names:\n                    other[key] = value.drop(coord_names)\n\n    return merge_core([dataset, other], priority_arg=1,\n                      indexes=dataset.indexes)", "response": "Guts of the Dataset. update method."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreplaces nan in a by val and returns the replaced array and the nan position", "response": "def _replace_nan(a, val):\n    \"\"\"\n    replace nan in a by val, and returns the replaced array and the nan\n    position\n    \"\"\"\n    mask = isnull(a)\n    return where_method(val, mask, a), mask"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _maybe_null_out(result, axis, mask, min_count=1):\n    if hasattr(axis, '__len__'):  # if tuple or list\n        raise ValueError('min_count is not available for reduction '\n                         'with more than one dimensions.')\n\n    if axis is not None and getattr(result, 'ndim', False):\n        null_mask = (mask.shape[axis] - mask.sum(axis) - min_count) < 0\n        if null_mask.any():\n            dtype, fill_value = dtypes.maybe_promote(result.dtype)\n            result = result.astype(dtype)\n            result[null_mask] = fill_value\n\n    elif getattr(result, 'dtype', None) not in dtypes.NAT_TYPES:\n        null_mask = mask.size - mask.sum()\n        if null_mask < min_count:\n            result = np.nan\n\n    return result", "response": "Internal function to handle null out of an array."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _disable_auto_decode_variable(var):\n    var.set_auto_maskandscale(False)\n\n    # only added in netCDF4-python v1.2.8\n    with suppress(AttributeError):\n        var.set_auto_chartostring(False)", "response": "Disable automatic decoding on a netCDF4. Variable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an appropriate positive infinity value for this dtype.", "response": "def get_pos_infinity(dtype):\n    \"\"\"Return an appropriate positive infinity for this dtype.\n\n    Parameters\n    ----------\n    dtype : np.dtype\n\n    Returns\n    -------\n    fill_value : positive infinity value corresponding to this dtype.\n    \"\"\"\n    if issubclass(dtype.type, (np.floating, np.integer)):\n        return np.inf\n\n    if issubclass(dtype.type, np.complexfloating):\n        return np.inf + 1j * np.inf\n\n    return INF"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_neg_infinity(dtype):\n    if issubclass(dtype.type, (np.floating, np.integer)):\n        return -np.inf\n\n    if issubclass(dtype.type, np.complexfloating):\n        return -np.inf - 1j * np.inf\n\n    return NINF", "response": "Return an appropriate positive infinity value for this dtype."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_datetime_like(dtype):\n    return (np.issubdtype(dtype, np.datetime64) or\n            np.issubdtype(dtype, np.timedelta64))", "response": "Check if a dtype is a subclass of the numpy datetime types\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef result_type(*arrays_and_dtypes):\n    types = {np.result_type(t).type for t in arrays_and_dtypes}\n\n    for left, right in PROMOTE_TO_OBJECT:\n        if (any(issubclass(t, left) for t in types) and\n                any(issubclass(t, right) for t in types)):\n            return np.dtype(object)\n\n    return np.result_type(*arrays_and_dtypes)", "response": "Like np. result_type but with type promotion rules matching pandas."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_envi(meta):\n\n    def parsevec(s):\n        return np.fromstring(s.strip('{}'), dtype='float', sep=',')\n\n    def default(s):\n        return s.strip('{}')\n\n    parse = {'wavelength': parsevec,\n             'fwhm': parsevec}\n    parsed_meta = {k: parse.get(k, default)(v) for k, v in meta.items()}\n    return parsed_meta", "response": "Parse the ENVI metadata into a dictionary of dicts."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef open_rasterio(filename, parse_coordinates=None, chunks=None, cache=None,\n                  lock=None):\n    \"\"\"Open a file with rasterio (experimental).\n\n    This should work with any file that rasterio can open (most often:\n    geoTIFF). The x and y coordinates are generated automatically from the\n    file's geoinformation, shifted to the center of each pixel (see\n    `\"PixelIsArea\" Raster Space\n    <http://web.archive.org/web/20160326194152/http://remotesensing.org/geotiff/spec/geotiff2.5.html#2.5.2>`_\n    for more information).\n\n    You can generate 2D coordinates from the file's attributes with::\n\n        from affine import Affine\n        da = xr.open_rasterio('path_to_file.tif')\n        transform = Affine.from_gdal(*da.attrs['transform'])\n        nx, ny = da.sizes['x'], da.sizes['y']\n        x, y = np.meshgrid(np.arange(nx)+0.5, np.arange(ny)+0.5) * transform\n\n\n    Parameters\n    ----------\n    filename : str, rasterio.DatasetReader, or rasterio.WarpedVRT\n        Path to the file to open. Or already open rasterio dataset.\n    parse_coordinates : bool, optional\n        Whether to parse the x and y coordinates out of the file's\n        ``transform`` attribute or not. The default is to automatically\n        parse the coordinates only if they are rectilinear (1D).\n        It can be useful to set ``parse_coordinates=False``\n        if your files are very large or if you don't need the coordinates.\n    chunks : int, tuple or dict, optional\n        Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or\n        ``{'x': 5, 'y': 5}``. If chunks is provided, it used to load the new\n        DataArray into a dask array.\n    cache : bool, optional\n        If True, cache data loaded from the underlying datastore in memory as\n        NumPy arrays when accessed to avoid reading from the underlying data-\n        store multiple times. Defaults to True unless you specify the `chunks`\n        argument to use dask, in which case it defaults to False.\n    lock : False, True or threading.Lock, optional\n        If chunks is provided, this argument is passed on to\n        :py:func:`dask.array.from_array`. By default, a global lock is\n        used to avoid issues with concurrent access to the same file when using\n        dask's multithreaded backend.\n\n    Returns\n    -------\n    data : DataArray\n        The newly created DataArray.\n    \"\"\"\n    import rasterio\n    from rasterio.vrt import WarpedVRT\n    vrt_params = None\n    if isinstance(filename, rasterio.io.DatasetReader):\n        filename = filename.name\n    elif isinstance(filename, rasterio.vrt.WarpedVRT):\n        vrt = filename\n        filename = vrt.src_dataset.name\n        vrt_params = dict(crs=vrt.crs.to_string(),\n                          resampling=vrt.resampling,\n                          src_nodata=vrt.src_nodata,\n                          dst_nodata=vrt.dst_nodata,\n                          tolerance=vrt.tolerance,\n                          transform=vrt.transform,\n                          width=vrt.width,\n                          height=vrt.height,\n                          warp_extras=vrt.warp_extras)\n\n    if lock is None:\n        lock = RASTERIO_LOCK\n\n    manager = CachingFileManager(rasterio.open, filename, lock=lock, mode='r')\n    riods = manager.acquire()\n    if vrt_params is not None:\n        riods = WarpedVRT(riods, **vrt_params)\n\n    if cache is None:\n        cache = chunks is None\n\n    coords = OrderedDict()\n\n    # Get bands\n    if riods.count < 1:\n        raise ValueError('Unknown dims')\n    coords['band'] = np.asarray(riods.indexes)\n\n    # Get coordinates\n    if LooseVersion(rasterio.__version__) < '1.0':\n        transform = riods.affine\n    else:\n        transform = riods.transform\n    if transform.is_rectilinear:\n        # 1d coordinates\n        parse = True if parse_coordinates is None else parse_coordinates\n        if parse:\n            nx, ny = riods.width, riods.height\n            # xarray coordinates are pixel centered\n            x, _ = (np.arange(nx) + 0.5, np.zeros(nx) + 0.5) * transform\n            _, y = (np.zeros(ny) + 0.5, np.arange(ny) + 0.5) * transform\n            coords['y'] = y\n            coords['x'] = x\n    else:\n        # 2d coordinates\n        parse = False if (parse_coordinates is None) else parse_coordinates\n        if parse:\n            warnings.warn(\n                \"The file coordinates' transformation isn't \"\n                \"rectilinear: xarray won't parse the coordinates \"\n                \"in this case. Set `parse_coordinates=False` to \"\n                \"suppress this warning.\",\n                RuntimeWarning, stacklevel=3)\n\n    # Attributes\n    attrs = dict()\n    # Affine transformation matrix (always available)\n    # This describes coefficients mapping pixel coordinates to CRS\n    # For serialization store as tuple of 6 floats, the last row being\n    # always (0, 0, 1) per definition (see\n    # https://github.com/sgillies/affine)\n    attrs['transform'] = tuple(transform)[:6]\n    if hasattr(riods, 'crs') and riods.crs:\n        # CRS is a dict-like object specific to rasterio\n        # If CRS is not None, we convert it back to a PROJ4 string using\n        # rasterio itself\n        try:\n            attrs['crs'] = riods.crs.to_proj4()\n        except AttributeError:\n            attrs['crs'] = riods.crs.to_string()\n    if hasattr(riods, 'res'):\n        # (width, height) tuple of pixels in units of CRS\n        attrs['res'] = riods.res\n    if hasattr(riods, 'is_tiled'):\n        # Is the TIF tiled? (bool)\n        # We cast it to an int for netCDF compatibility\n        attrs['is_tiled'] = np.uint8(riods.is_tiled)\n    if hasattr(riods, 'nodatavals'):\n        # The nodata values for the raster bands\n        attrs['nodatavals'] = tuple(\n            np.nan if nodataval is None else nodataval\n            for nodataval in riods.nodatavals)\n\n    # Parse extra metadata from tags, if supported\n    parsers = {'ENVI': _parse_envi}\n\n    driver = riods.driver\n    if driver in parsers:\n        meta = parsers[driver](riods.tags(ns=driver))\n\n        for k, v in meta.items():\n            # Add values as coordinates if they match the band count,\n            # as attributes otherwise\n            if (isinstance(v, (list, np.ndarray))\n                    and len(v) == riods.count):\n                coords[k] = ('band', np.asarray(v))\n            else:\n                attrs[k] = v\n\n    data = indexing.LazilyOuterIndexedArray(\n        RasterioArrayWrapper(manager, lock, vrt_params))\n\n    # this lets you write arrays loaded with rasterio\n    data = indexing.CopyOnWriteArray(data)\n    if cache and chunks is None:\n        data = indexing.MemoryCachedArray(data)\n\n    result = DataArray(data=data, dims=('band', 'y', 'x'),\n                       coords=coords, attrs=attrs)\n\n    if chunks is not None:\n        from dask.base import tokenize\n        # augment the token with the file modification time\n        try:\n            mtime = os.path.getmtime(filename)\n        except OSError:\n            # the filename is probably an s3 bucket rather than a regular file\n            mtime = None\n        token = tokenize(filename, mtime, chunks)\n        name_prefix = 'open_rasterio-%s' % token\n        result = result.chunk(chunks, name_prefix=name_prefix, token=token)\n\n    # Make the file closeable\n    result._file_obj = manager\n\n    return result", "response": "Open a file with rasterio."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_indexer(self, key):\n        assert len(key) == 3, 'rasterio datasets should always be 3D'\n\n        # bands cannot be windowed but they can be listed\n        band_key = key[0]\n        np_inds = []\n        # bands (axis=0) cannot be windowed but they can be listed\n        if isinstance(band_key, slice):\n            start, stop, step = band_key.indices(self.shape[0])\n            band_key = np.arange(start, stop, step)\n        # be sure we give out a list\n        band_key = (np.asarray(band_key) + 1).tolist()\n        if isinstance(band_key, list):  # if band_key is not a scalar\n            np_inds.append(slice(None))\n\n        # but other dims can only be windowed\n        window = []\n        squeeze_axis = []\n        for i, (k, n) in enumerate(zip(key[1:], self.shape[1:])):\n            if isinstance(k, slice):\n                # step is always positive. see indexing.decompose_indexer\n                start, stop, step = k.indices(n)\n                np_inds.append(slice(None, None, step))\n            elif is_scalar(k):\n                # windowed operations will always return an array\n                # we will have to squeeze it later\n                squeeze_axis.append(- (2 - i))\n                start = k\n                stop = k + 1\n            else:\n                start, stop = np.min(k), np.max(k) + 1\n                np_inds.append(k - start)\n            window.append((start, stop))\n\n        if isinstance(key[1], np.ndarray) and isinstance(key[2], np.ndarray):\n            # do outer-style indexing\n            np_inds[-2:] = np.ix_(*np_inds[-2:])\n\n        return band_key, tuple(window), tuple(squeeze_axis), tuple(np_inds)", "response": "Get indexer for rasterio array."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngroup an array by its unique values.", "response": "def unique_value_groups(ar, sort=True):\n    \"\"\"Group an array by its unique values.\n\n    Parameters\n    ----------\n    ar : array-like\n        Input array. This will be flattened if it is not already 1-D.\n    sort : boolean, optional\n        Whether or not to sort unique values.\n\n    Returns\n    -------\n    values : np.ndarray\n        Sorted, unique values as returned by `np.unique`.\n    indices : list of lists of int\n        Each element provides the integer indices in `ar` with values given by\n        the corresponding value in `unique_values`.\n    \"\"\"\n    inverse, values = pd.factorize(ar, sort=sort)\n    groups = [[] for _ in range(len(values))]\n    for n, g in enumerate(inverse):\n        if g >= 0:\n            # pandas uses -1 to mark NaN, but doesn't include them in values\n            groups[g].append(n)\n    return values, groups"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _consolidate_slices(slices):\n    result = []\n    last_slice = slice(None)\n    for slice_ in slices:\n        if not isinstance(slice_, slice):\n            raise ValueError('list element is not a slice: %r' % slice_)\n        if (result and last_slice.stop == slice_.start and\n                _is_one_or_none(last_slice.step) and\n                _is_one_or_none(slice_.step)):\n            last_slice = slice(last_slice.start, slice_.stop, slice_.step)\n            result[-1] = last_slice\n        else:\n            result.append(slice_)\n            last_slice = slice_\n    return result", "response": "Consolidate adjacent slices in a list of slices."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlike inverse_permutation but also handles slices.", "response": "def _inverse_permutation_indices(positions):\n    \"\"\"Like inverse_permutation, but also handles slices.\n\n    Parameters\n    ----------\n    positions : list of np.ndarray or slice objects.\n        If slice objects, all are assumed to be slices.\n\n    Returns\n    -------\n    np.ndarray of indices or None, if no permutation is necessary.\n    \"\"\"\n    if not positions:\n        return None\n\n    if isinstance(positions[0], slice):\n        positions = _consolidate_slices(positions)\n        if positions == slice(None):\n            return None\n        positions = [np.arange(sl.start, sl.stop, sl.step) for sl in positions]\n\n    indices = nputils.inverse_permutation(np.concatenate(positions))\n    return indices"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _apply_loffset(grouper, result):\n\n    needs_offset = (\n        isinstance(grouper.loffset, (pd.DateOffset, datetime.timedelta))\n        and isinstance(result.index, pd.DatetimeIndex)\n        and len(result.index) > 0\n    )\n\n    if needs_offset:\n        result.index = result.index + grouper.loffset\n\n    grouper.loffset = None", "response": "Applies the loffset to the result DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\niterates over each element in this group", "response": "def _iter_grouped(self):\n        \"\"\"Iterate over each element in this group\"\"\"\n        for indices in self._group_indices:\n            yield self._obj.isel(**{self._group_dim: indices})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrestore empty groups if needed.", "response": "def _maybe_restore_empty_groups(self, combined):\n        \"\"\"Our index contained empty groups (e.g., from a resampling). If we\n        reduced on that dimension, we want to restore the full index.\n        \"\"\"\n        if (self._full_index is not None and\n                self._group.name in combined.dims):\n            indexers = {self._group.name: self._full_index}\n            combined = combined.reindex(**indexers)\n        return combined"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _maybe_unstack(self, obj):\n        if self._stacked_dim is not None and self._stacked_dim in obj.dims:\n            obj = obj.unstack(self._stacked_dim)\n            for dim in self._inserted_dims:\n                if dim in obj.coords:\n                    del obj.coords[dim]\n        return obj", "response": "Unstacks the array with the specified dimension if there is one."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the first element of each group along the group dimension", "response": "def first(self, skipna=None, keep_attrs=None):\n        \"\"\"Return the first element of each group along the group dimension\n        \"\"\"\n        return self._first_or_last(duck_array_ops.first, skipna, keep_attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the last element of each group along the group dimension", "response": "def last(self, skipna=None, keep_attrs=None):\n        \"\"\"Return the last element of each group along the group dimension\n        \"\"\"\n        return self._first_or_last(duck_array_ops.last, skipna, keep_attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfasts version of _iter_grouped that yields Variables without metadata", "response": "def _iter_grouped_shortcut(self):\n        \"\"\"Fast version of `_iter_grouped` that yields Variables without\n        metadata\n        \"\"\"\n        var = self._obj.variable\n        for indices in self._group_indices:\n            yield var[{self._group_dim: indices}]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apply(self, func, shortcut=False, args=(), **kwargs):\n        if shortcut:\n            grouped = self._iter_grouped_shortcut()\n        else:\n            grouped = self._iter_grouped()\n        applied = (maybe_wrap_array(arr, func(arr, *args, **kwargs))\n                   for arr in grouped)\n        return self._combine(applied, shortcut=shortcut)", "response": "Apply a function over each array in the group and concatenate them together into a new array."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _combine(self, applied, shortcut=False):\n        applied_example, applied = peek_at(applied)\n        coord, dim, positions = self._infer_concat_args(applied_example)\n        if shortcut:\n            combined = self._concat_shortcut(applied, dim, positions)\n        else:\n            combined = concat(applied, dim)\n            combined = _maybe_reorder(combined, dim, positions)\n\n        if isinstance(combined, type(self._obj)):\n            # only restore dimension order for arrays\n            combined = self._restore_dim_order(combined)\n        if coord is not None:\n            if shortcut:\n                combined._coords[coord.name] = as_variable(coord)\n            else:\n                combined.coords[coord.name] = coord\n        combined = self._maybe_restore_empty_groups(combined)\n        combined = self._maybe_unstack(combined)\n        return combined", "response": "Recombine the applied objects like the original."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reduce(self, func, dim=None, axis=None,\n               keep_attrs=None, shortcut=True, **kwargs):\n        \"\"\"Reduce the items in this group by applying `func` along some\n        dimension(s).\n\n        Parameters\n        ----------\n        func : function\n            Function which can be called in the form\n            `func(x, axis=axis, **kwargs)` to return the result of collapsing\n            an np.ndarray over an integer valued axis.\n        dim : str or sequence of str, optional\n            Dimension(s) over which to apply `func`.\n        axis : int or sequence of int, optional\n            Axis(es) over which to apply `func`. Only one of the 'dimension'\n            and 'axis' arguments can be supplied. If neither are supplied, then\n            `func` is calculated over all dimension for each group item.\n        keep_attrs : bool, optional\n            If True, the datasets's attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n        **kwargs : dict\n            Additional keyword arguments passed on to `func`.\n\n        Returns\n        -------\n        reduced : Array\n            Array with summarized data and the indicated dimension(s)\n            removed.\n        \"\"\"\n        if dim == DEFAULT_DIMS:\n            dim = ALL_DIMS\n            # TODO change this to dim = self._group_dim after\n            # the deprecation process\n            if self._obj.ndim > 1:\n                warnings.warn(\n                    \"Default reduction dimension will be changed to the \"\n                    \"grouped dimension in a future version of xarray. To \"\n                    \"silence this warning, pass dim=xarray.ALL_DIMS \"\n                    \"explicitly.\",\n                    FutureWarning, stacklevel=2)\n\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n\n        def reduce_array(ar):\n            return ar.reduce(func, dim, axis, keep_attrs=keep_attrs, **kwargs)\n        return self.apply(reduce_array, shortcut=shortcut)", "response": "Reduce the items in this group by applying func along some dimension and axis."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef apply(self, func, args=(), **kwargs):\n        kwargs.pop('shortcut', None)  # ignore shortcut if set (for now)\n        applied = (func(ds, *args, **kwargs) for ds in self._iter_grouped())\n        return self._combine(applied)", "response": "Apply a function over each Dataset in the group and concatenate them\n        together into a new Dataset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrecombine the applied objects like the original.", "response": "def _combine(self, applied):\n        \"\"\"Recombine the applied objects like the original.\"\"\"\n        applied_example, applied = peek_at(applied)\n        coord, dim, positions = self._infer_concat_args(applied_example)\n        combined = concat(applied, dim)\n        combined = _maybe_reorder(combined, dim, positions)\n        if coord is not None:\n            combined[coord.name] = coord\n        combined = self._maybe_restore_empty_groups(combined)\n        combined = self._maybe_unstack(combined)\n        return combined"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreduces the items in this group by applying func along some dimension.", "response": "def reduce(self, func, dim=None, keep_attrs=None, **kwargs):\n        \"\"\"Reduce the items in this group by applying `func` along some\n        dimension(s).\n\n        Parameters\n        ----------\n        func : function\n            Function which can be called in the form\n            `func(x, axis=axis, **kwargs)` to return the result of collapsing\n            an np.ndarray over an integer valued axis.\n        dim : str or sequence of str, optional\n            Dimension(s) over which to apply `func`.\n        axis : int or sequence of int, optional\n            Axis(es) over which to apply `func`. Only one of the 'dimension'\n            and 'axis' arguments can be supplied. If neither are supplied, then\n            `func` is calculated over all dimension for each group item.\n        keep_attrs : bool, optional\n            If True, the datasets's attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n        **kwargs : dict\n            Additional keyword arguments passed on to `func`.\n\n        Returns\n        -------\n        reduced : Array\n            Array with summarized data and the indicated dimension(s)\n            removed.\n        \"\"\"\n        if dim == DEFAULT_DIMS:\n            dim = ALL_DIMS\n            # TODO change this to dim = self._group_dim after\n            # the deprecation process. Do not forget to remove _reduce_method\n            warnings.warn(\n                \"Default reduction dimension will be changed to the \"\n                \"grouped dimension in a future version of xarray. To \"\n                \"silence this warning, pass dim=xarray.ALL_DIMS \"\n                \"explicitly.\",\n                FutureWarning, stacklevel=2)\n        elif dim is None:\n            dim = self._group_dim\n\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n\n        def reduce_dataset(ds):\n            return ds.reduce(func, dim, keep_attrs, **kwargs)\n        return self.apply(reduce_dataset)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns attrs that are not in ignored_attrs", "response": "def _filter_attrs(attrs, ignored_attrs):\n    \"\"\" Return attrs that are not in ignored_attrs\n    \"\"\"\n    return dict((k, v) for k, v in attrs.items() if k not in ignored_attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_cdms2(variable):\n    values = np.asarray(variable)\n    name = variable.id\n    dims = variable.getAxisIds()\n    coords = {}\n    for axis in variable.getAxisList():\n        coords[axis.id] = DataArray(\n            np.asarray(axis), dims=[axis.id],\n            attrs=_filter_attrs(axis.attributes, cdms2_ignored_attrs))\n    grid = variable.getGrid()\n    if grid is not None:\n        ids = [a.id for a in grid.getAxisList()]\n        for axis in grid.getLongitude(), grid.getLatitude():\n            if axis.id not in variable.getAxisIds():\n                coords[axis.id] = DataArray(\n                    np.asarray(axis[:]), dims=ids,\n                    attrs=_filter_attrs(axis.attributes,\n                                        cdms2_ignored_attrs))\n    attrs = _filter_attrs(variable.attributes, cdms2_ignored_attrs)\n    dataarray = DataArray(values, dims=dims, coords=coords, name=name,\n                          attrs=attrs)\n    return decode_cf(dataarray.to_dataset())[dataarray.name]", "response": "Convert a cdms2 variable into a DataArray\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a DataArray into a cdms2 variable and its axes and grids.", "response": "def to_cdms2(dataarray, copy=True):\n    \"\"\"Convert a DataArray into a cdms2 variable\n    \"\"\"\n    # we don't want cdms2 to be a hard dependency\n    import cdms2\n\n    def set_cdms2_attrs(var, attrs):\n        for k, v in attrs.items():\n            setattr(var, k, v)\n\n    # 1D axes\n    axes = []\n    for dim in dataarray.dims:\n        coord = encode(dataarray.coords[dim])\n        axis = cdms2.createAxis(coord.values, id=dim)\n        set_cdms2_attrs(axis, coord.attrs)\n        axes.append(axis)\n\n    # Data\n    var = encode(dataarray)\n    cdms2_var = cdms2.createVariable(var.values, axes=axes, id=dataarray.name,\n                                     mask=pd.isnull(var.values), copy=copy)\n\n    # Attributes\n    set_cdms2_attrs(cdms2_var, var.attrs)\n\n    # Curvilinear and unstructured grids\n    if dataarray.name not in dataarray.coords:\n\n        cdms2_axes = OrderedDict()\n        for coord_name in set(dataarray.coords.keys()) - set(dataarray.dims):\n\n            coord_array = dataarray.coords[coord_name].to_cdms2()\n\n            cdms2_axis_cls = (cdms2.coord.TransientAxis2D\n                              if coord_array.ndim else\n                              cdms2.auxcoord.TransientAuxAxis1D)\n            cdms2_axis = cdms2_axis_cls(coord_array)\n            if cdms2_axis.isLongitude():\n                cdms2_axes['lon'] = cdms2_axis\n            elif cdms2_axis.isLatitude():\n                cdms2_axes['lat'] = cdms2_axis\n\n        if 'lon' in cdms2_axes and 'lat' in cdms2_axes:\n            if len(cdms2_axes['lon'].shape) == 2:\n                cdms2_grid = cdms2.hgrid.TransientCurveGrid(\n                    cdms2_axes['lat'], cdms2_axes['lon'])\n            else:\n                cdms2_grid = cdms2.gengrid.AbstractGenericGrid(\n                    cdms2_axes['lat'], cdms2_axes['lon'])\n            for axis in cdms2_grid.getAxisList():\n                cdms2_var.setAxis(cdms2_var.getAxisIds().index(axis.id), axis)\n            cdms2_var.setGrid(cdms2_grid)\n\n    return cdms2_var"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn attrs with keys in keys list", "response": "def _pick_attrs(attrs, keys):\n    \"\"\" Return attrs with keys in keys list\n    \"\"\"\n    return dict((k, v) for k, v in attrs.items() if k in keys)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_iris_args(attrs):\n    # iris.unit is deprecated in Iris v1.9\n    import cf_units\n    args = {'attributes': _filter_attrs(attrs, iris_forbidden_keys)}\n    args.update(_pick_attrs(attrs, ('standard_name', 'long_name',)))\n    unit_args = _pick_attrs(attrs, ('calendar',))\n    if 'units' in attrs:\n        args['units'] = cf_units.Unit(attrs['units'], **unit_args)\n    return args", "response": "Converts the xarray attrs into args that can be passed into Iris\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_iris(dataarray):\n    # Iris not a hard dependency\n    import iris\n    from iris.fileformats.netcdf import parse_cell_methods\n\n    dim_coords = []\n    aux_coords = []\n\n    for coord_name in dataarray.coords:\n        coord = encode(dataarray.coords[coord_name])\n        coord_args = _get_iris_args(coord.attrs)\n        coord_args['var_name'] = coord_name\n        axis = None\n        if coord.dims:\n            axis = dataarray.get_axis_num(coord.dims)\n        if coord_name in dataarray.dims:\n            try:\n                iris_coord = iris.coords.DimCoord(coord.values, **coord_args)\n                dim_coords.append((iris_coord, axis))\n            except ValueError:\n                iris_coord = iris.coords.AuxCoord(coord.values, **coord_args)\n                aux_coords.append((iris_coord, axis))\n        else:\n            iris_coord = iris.coords.AuxCoord(coord.values, **coord_args)\n            aux_coords.append((iris_coord, axis))\n\n    args = _get_iris_args(dataarray.attrs)\n    args['var_name'] = dataarray.name\n    args['dim_coords_and_dims'] = dim_coords\n    args['aux_coords_and_dims'] = aux_coords\n    if 'cell_methods' in dataarray.attrs:\n        args['cell_methods'] = \\\n            parse_cell_methods(dataarray.attrs['cell_methods'])\n\n    masked_data = duck_array_ops.masked_invalid(dataarray.data)\n    cube = iris.cube.Cube(masked_data, **args)\n\n    return cube", "response": "Convert a DataArray into a Iris Cube\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _iris_obj_to_attrs(obj):\n    attrs = {'standard_name': obj.standard_name,\n             'long_name': obj.long_name}\n    if obj.units.calendar:\n        attrs['calendar'] = obj.units.calendar\n    if obj.units.origin != '1' and not obj.units.is_unknown():\n        attrs['units'] = obj.units.origin\n    attrs.update(obj.attributes)\n    return dict((k, v) for k, v in attrs.items() if v is not None)", "response": "Return a dictionary of attrs when given a Iris object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _iris_cell_methods_to_str(cell_methods_obj):\n    cell_methods = []\n    for cell_method in cell_methods_obj:\n        names = ''.join(['{}: '.format(n) for n in cell_method.coord_names])\n        intervals = ' '.join(['interval: {}'.format(interval)\n                              for interval in cell_method.intervals])\n        comments = ' '.join(['comment: {}'.format(comment)\n                             for comment in cell_method.comments])\n        extra = ' '.join([intervals, comments]).strip()\n        if extra:\n            extra = ' ({})'.format(extra)\n        cell_methods.append(names + cell_method.method + extra)\n    return ' '.join(cell_methods)", "response": "Converts a Iris cell methods into a string"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _name(iris_obj, default='unknown'):\n    return (iris_obj.var_name or iris_obj.standard_name or\n            iris_obj.long_name or default)", "response": "Mimicks name method but with different name resolution order."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a Iris cube into a DataArray .", "response": "def from_iris(cube):\n    \"\"\" Convert a Iris cube into an DataArray\n    \"\"\"\n    import iris.exceptions\n    from xarray.core.pycompat import dask_array_type\n\n    name = _name(cube)\n    if name == 'unknown':\n        name = None\n    dims = []\n    for i in range(cube.ndim):\n        try:\n            dim_coord = cube.coord(dim_coords=True, dimensions=(i,))\n            dims.append(_name(dim_coord))\n        except iris.exceptions.CoordinateNotFoundError:\n            dims.append(\"dim_{}\".format(i))\n\n    if len(set(dims)) != len(dims):\n        duplicates = [k for k, v in Counter(dims).items() if v > 1]\n        raise ValueError('Duplicate coordinate name {}.'.format(duplicates))\n\n    coords = OrderedDict()\n\n    for coord in cube.coords():\n        coord_attrs = _iris_obj_to_attrs(coord)\n        coord_dims = [dims[i] for i in cube.coord_dims(coord)]\n        if coord_dims:\n            coords[_name(coord)] = (coord_dims, coord.points, coord_attrs)\n        else:\n            coords[_name(coord)] = ((), coord.points.item(), coord_attrs)\n\n    array_attrs = _iris_obj_to_attrs(cube)\n    cell_methods = _iris_cell_methods_to_str(cube.cell_methods)\n    if cell_methods:\n        array_attrs['cell_methods'] = cell_methods\n\n    # Deal with iris 1.* and 2.*\n    cube_data = cube.core_data() if hasattr(cube, 'core_data') else cube.data\n\n    # Deal with dask and numpy masked arrays\n    if isinstance(cube_data, dask_array_type):\n        from dask.array import ma as dask_ma\n        filled_data = dask_ma.filled(cube_data, get_fill_value(cube.dtype))\n    elif isinstance(cube_data, np.ma.MaskedArray):\n        filled_data = np.ma.filled(cube_data, get_fill_value(cube.dtype))\n    else:\n        filled_data = cube_data\n\n    dataarray = DataArray(filled_data, coords=coords, name=name,\n                          attrs=array_attrs, dims=dims)\n    decoded_ds = decode_cf(dataarray._to_temp_dataset())\n    return dataarray._from_temp_dataset(decoded_ds)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive an array of numeric dates in netCDF format convert it into numpy array of date time objects.", "response": "def decode_cf_datetime(num_dates, units, calendar=None, use_cftime=None):\n    \"\"\"Given an array of numeric dates in netCDF format, convert it into a\n    numpy array of date time objects.\n\n    For standard (Gregorian) calendars, this function uses vectorized\n    operations, which makes it much faster than cftime.num2date. In such a\n    case, the returned array will be of type np.datetime64.\n\n    Note that time unit in `units` must not be smaller than microseconds and\n    not larger than days.\n\n    See also\n    --------\n    cftime.num2date\n    \"\"\"\n    num_dates = np.asarray(num_dates)\n    flat_num_dates = num_dates.ravel()\n    if calendar is None:\n        calendar = 'standard'\n\n    if use_cftime is None:\n        try:\n            dates = _decode_datetime_with_pandas(flat_num_dates, units,\n                                                 calendar)\n        except (OutOfBoundsDatetime, OverflowError):\n            dates = _decode_datetime_with_cftime(\n                flat_num_dates.astype(np.float), units, calendar)\n\n            if (dates[np.nanargmin(num_dates)].year < 1678 or\n               dates[np.nanargmax(num_dates)].year >= 2262):\n                if calendar in _STANDARD_CALENDARS:\n                    warnings.warn(\n                        'Unable to decode time axis into full '\n                        'numpy.datetime64 objects, continuing using '\n                        'cftime.datetime objects instead, reason: dates out '\n                        'of range', SerializationWarning, stacklevel=3)\n            else:\n                if calendar in _STANDARD_CALENDARS:\n                    dates = cftime_to_nptime(dates)\n    elif use_cftime:\n        dates = _decode_datetime_with_cftime(\n            flat_num_dates.astype(np.float), units, calendar)\n    else:\n        dates = _decode_datetime_with_pandas(flat_num_dates, units, calendar)\n\n    return dates.reshape(num_dates.shape)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives an array of numeric timedeltas in netCDF format convert it into a numpy timedelta64 [ ns ] array.", "response": "def decode_cf_timedelta(num_timedeltas, units):\n    \"\"\"Given an array of numeric timedeltas in netCDF format, convert it into a\n    numpy timedelta64[ns] array.\n    \"\"\"\n    num_timedeltas = np.asarray(num_timedeltas)\n    units = _netcdf_to_numpy_timeunit(units)\n\n    shape = num_timedeltas.shape\n    num_timedeltas = num_timedeltas.ravel()\n\n    result = pd.to_timedelta(num_timedeltas, unit=units, box=False)\n    # NaT is returned unboxed with wrong units; this should be fixed in pandas\n    if result.dtype != 'timedelta64[ns]':\n        result = result.astype('timedelta64[ns]')\n    return result.reshape(shape)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef infer_calendar_name(dates):\n    if np.asarray(dates).dtype == 'datetime64[ns]':\n        return 'proleptic_gregorian'\n    else:\n        return np.asarray(dates).ravel()[0].calendar", "response": "Given an array of datetimes infer the CF calendar name"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef infer_datetime_units(dates):\n    dates = np.asarray(dates).ravel()\n    if np.asarray(dates).dtype == 'datetime64[ns]':\n        dates = pd.to_datetime(dates, box=False)\n        dates = dates[pd.notnull(dates)]\n        reference_date = dates[0] if len(dates) > 0 else '1970-01-01'\n        reference_date = pd.Timestamp(reference_date)\n    else:\n        reference_date = dates[0] if len(dates) > 0 else '1970-01-01'\n        reference_date = format_cftime_datetime(reference_date)\n    unique_timedeltas = np.unique(np.diff(dates))\n    if unique_timedeltas.dtype == np.dtype('O'):\n        # Convert to np.timedelta64 objects using pandas to work around a\n        # NumPy casting bug: https://github.com/numpy/numpy/issues/11096\n        unique_timedeltas = pd.to_timedelta(unique_timedeltas, box=False)\n    units = _infer_time_units_from_diff(unique_timedeltas)\n    return '%s since %s' % (units, reference_date)", "response": "Given an array of datetimes returns a CF compatible time - unit string of\n    the form days hours minutes or seconds"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_cftime_datetime(date):\n    return '{:04d}-{:02d}-{:02d} {:02d}:{:02d}:{:02d}.{:06d}'.format(\n        date.year, date.month, date.day, date.hour, date.minute, date.second,\n        date.microsecond)", "response": "Converts a cftime. datetime object to a string with the format YYYY - MM - DD. UUUUUU\n    YYYY - MM - DD. UUUUUUU\n    YYYY - MM - DD. UUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUU"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef infer_timedelta_units(deltas):\n    deltas = pd.to_timedelta(np.asarray(deltas).ravel(), box=False)\n    unique_timedeltas = np.unique(deltas[pd.notnull(deltas)])\n    units = _infer_time_units_from_diff(unique_timedeltas)\n    return units", "response": "Given an array of timedeltas returns a CF compatible time - unit from that array."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cftime_to_nptime(times):\n    times = np.asarray(times)\n    new = np.empty(times.shape, dtype='M8[ns]')\n    for i, t in np.ndenumerate(times):\n        try:\n            # Use pandas.Timestamp in place of datetime.datetime, because\n            # NumPy casts it safely it np.datetime64[ns] for dates outside\n            # 1678 to 2262 (this is not currently the case for\n            # datetime.datetime).\n            dt = pd.Timestamp(t.year, t.month, t.day, t.hour, t.minute,\n                              t.second, t.microsecond)\n        except ValueError as e:\n            raise ValueError('Cannot convert date {} to a date in the '\n                             'standard calendar.  Reason: {}.'.format(t, e))\n        new[i] = np.datetime64(dt)\n    return new", "response": "Given an array of cftime. datetime objects return an array of numpy. datetime64 objects of the same size"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _encode_datetime_with_cftime(dates, units, calendar):\n    cftime = _import_cftime()\n\n    if np.issubdtype(dates.dtype, np.datetime64):\n        # numpy's broken datetime conversion only works for us precision\n        dates = dates.astype('M8[us]').astype(datetime)\n\n    def encode_datetime(d):\n        return np.nan if d is None else cftime.date2num(d, units, calendar)\n\n    return np.vectorize(encode_datetime)(dates)", "response": "Fallback method for encoding dates using cftime.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef encode_cf_datetime(dates, units=None, calendar=None):\n    dates = np.asarray(dates)\n\n    if units is None:\n        units = infer_datetime_units(dates)\n    else:\n        units = _cleanup_netcdf_time_units(units)\n\n    if calendar is None:\n        calendar = infer_calendar_name(dates)\n\n    delta, ref_date = _unpack_netcdf_time_units(units)\n    try:\n        if calendar not in _STANDARD_CALENDARS or dates.dtype.kind == 'O':\n            # parse with cftime instead\n            raise OutOfBoundsDatetime\n        assert dates.dtype == 'datetime64[ns]'\n\n        delta_units = _netcdf_to_numpy_timeunit(delta)\n        time_delta = np.timedelta64(1, delta_units).astype('timedelta64[ns]')\n        ref_date = pd.Timestamp(ref_date)\n\n        # If the ref_date Timestamp is timezone-aware, convert to UTC and\n        # make it timezone-naive (GH 2649).\n        if ref_date.tz is not None:\n            ref_date = ref_date.tz_convert(None)\n\n        # Wrap the dates in a DatetimeIndex to do the subtraction to ensure\n        # an OverflowError is raised if the ref_date is too far away from\n        # dates to be encoded (GH 2272).\n        num = (pd.DatetimeIndex(dates.ravel()) - ref_date) / time_delta\n        num = num.values.reshape(dates.shape)\n\n    except (OutOfBoundsDatetime, OverflowError):\n        num = _encode_datetime_with_cftime(dates, units, calendar)\n\n    num = cast_to_int_if_safe(num)\n    return (num, units, calendar)", "response": "Given an array of datetime objects returns the tuple num units and calendar suitable for a CF compliant time variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _validate_dataset_names(dataset):\n    def check_name(name):\n        if isinstance(name, str):\n            if not name:\n                raise ValueError('Invalid name for DataArray or Dataset key: '\n                                 'string must be length 1 or greater for '\n                                 'serialization to netCDF files')\n        elif name is not None:\n            raise TypeError('DataArray.name or Dataset key must be either a '\n                            'string or None for serialization to netCDF files')\n\n    for k in dataset.variables:\n        check_name(k)", "response": "Validate the names of the DataArray and Dataset keys."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _validate_attrs(dataset):\n    def check_attr(name, value):\n        if isinstance(name, str):\n            if not name:\n                raise ValueError('Invalid name for attr: string must be '\n                                 'length 1 or greater for serialization to '\n                                 'netCDF files')\n        else:\n            raise TypeError(\"Invalid name for attr: {} must be a string for \"\n                            \"serialization to netCDF files\".format(name))\n\n        if not isinstance(value, (str, Number, np.ndarray, np.number,\n                                  list, tuple)):\n            raise TypeError('Invalid value for attr: {} must be a number, '\n                            'a string, an ndarray or a list/tuple of '\n                            'numbers/strings for serialization to netCDF '\n                            'files'.format(value))\n\n    # Check attrs on the dataset itself\n    for k, v in dataset.attrs.items():\n        check_attr(k, v)\n\n    # Check attrs on each variable within the dataset\n    for variable in dataset.variables.values():\n        for k, v in variable.attrs.items():\n            check_attr(k, v)", "response": "Validate the attrs of the dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads and decode a dataset from a file or file - like object.", "response": "def open_dataset(filename_or_obj, group=None, decode_cf=True,\n                 mask_and_scale=None, decode_times=True, autoclose=None,\n                 concat_characters=True, decode_coords=True, engine=None,\n                 chunks=None, lock=None, cache=None, drop_variables=None,\n                 backend_kwargs=None, use_cftime=None):\n    \"\"\"Load and decode a dataset from a file or file-like object.\n\n    Parameters\n    ----------\n    filename_or_obj : str, Path, file or xarray.backends.*DataStore\n        Strings and Path objects are interpreted as a path to a netCDF file\n        or an OpenDAP URL and opened with python-netCDF4, unless the filename\n        ends with .gz, in which case the file is gunzipped and opened with\n        scipy.io.netcdf (only netCDF3 supported). Byte-strings or file-like\n        objects are opened by scipy.io.netcdf (netCDF3) or h5py (netCDF4/HDF).\n    group : str, optional\n        Path to the netCDF4 group in the given file to open (only works for\n        netCDF4 files).\n    decode_cf : bool, optional\n        Whether to decode these variables, assuming they were saved according\n        to CF conventions.\n    mask_and_scale : bool, optional\n        If True, replace array values equal to `_FillValue` with NA and scale\n        values according to the formula `original_values * scale_factor +\n        add_offset`, where `_FillValue`, `scale_factor` and `add_offset` are\n        taken from variable attributes (if they exist).  If the `_FillValue` or\n        `missing_value` attribute contains multiple values a warning will be\n        issued and all array values matching one of the multiple values will\n        be replaced by NA. mask_and_scale defaults to True except for the\n        pseudonetcdf backend.\n    decode_times : bool, optional\n        If True, decode times encoded in the standard NetCDF datetime format\n        into datetime objects. Otherwise, leave them encoded as numbers.\n    autoclose : bool, optional\n        If True, automatically close files to avoid OS Error of too many files\n        being open.  However, this option doesn't work with streams, e.g.,\n        BytesIO.\n    concat_characters : bool, optional\n        If True, concatenate along the last dimension of character arrays to\n        form string arrays. Dimensions will only be concatenated over (and\n        removed) if they have no corresponding variable and if they are only\n        used as the last dimension of character arrays.\n    decode_coords : bool, optional\n        If True, decode the 'coordinates' attribute to identify coordinates in\n        the resulting dataset.\n    engine : {'netcdf4', 'scipy', 'pydap', 'h5netcdf', 'pynio', 'cfgrib', \\\n        'pseudonetcdf'}, optional\n        Engine to use when reading files. If not provided, the default engine\n        is chosen based on available dependencies, with a preference for\n        'netcdf4'.\n    chunks : int or dict, optional\n        If chunks is provided, it used to load the new dataset into dask\n        arrays. ``chunks={}`` loads the dataset with dask using a single\n        chunk for all arrays.\n    lock : False or duck threading.Lock, optional\n        Resource lock to use when reading data from disk. Only relevant when\n        using dask or another form of parallelism. By default, appropriate\n        locks are chosen to safely read and write files with the currently\n        active dask scheduler.\n    cache : bool, optional\n        If True, cache data loaded from the underlying datastore in memory as\n        NumPy arrays when accessed to avoid reading from the underlying data-\n        store multiple times. Defaults to True unless you specify the `chunks`\n        argument to use dask, in which case it defaults to False. Does not\n        change the behavior of coordinates corresponding to dimensions, which\n        always load their data from disk into a ``pandas.Index``.\n    drop_variables: string or iterable, optional\n        A variable or list of variables to exclude from being parsed from the\n        dataset. This may be useful to drop variables with problems or\n        inconsistent values.\n    backend_kwargs: dictionary, optional\n        A dictionary of keyword arguments to pass on to the backend. This\n        may be useful when backend options would improve performance or\n        allow user control of dataset processing.\n    use_cftime: bool, optional\n        Only relevant if encoded dates come from a standard calendar\n        (e.g. 'gregorian', 'proleptic_gregorian', 'standard', or not\n        specified).  If None (default), attempt to decode times to\n        ``np.datetime64[ns]`` objects; if this is not possible, decode times to\n        ``cftime.datetime`` objects. If True, always decode times to\n        ``cftime.datetime`` objects, regardless of whether or not they can be\n        represented using ``np.datetime64[ns]`` objects.  If False, always\n        decode times to ``np.datetime64[ns]`` objects; if this is not possible\n        raise an error.\n\n    Returns\n    -------\n    dataset : Dataset\n        The newly created dataset.\n\n    Notes\n    -----\n    ``open_dataset`` opens the file with read-only access. When you modify\n    values of a Dataset, even one linked to files on disk, only the in-memory\n    copy you are manipulating in xarray is modified: the original file on disk\n    is never touched.\n\n    See Also\n    --------\n    open_mfdataset\n    \"\"\"\n    engines = [None, 'netcdf4', 'scipy', 'pydap', 'h5netcdf', 'pynio',\n               'cfgrib', 'pseudonetcdf']\n    if engine not in engines:\n        raise ValueError('unrecognized engine for open_dataset: {}\\n'\n                         'must be one of: {}'\n                         .format(engine, engines))\n\n    if autoclose is not None:\n        warnings.warn(\n            'The autoclose argument is no longer used by '\n            'xarray.open_dataset() and is now ignored; it will be removed in '\n            'a future version of xarray. If necessary, you can control the '\n            'maximum number of simultaneous open files with '\n            'xarray.set_options(file_cache_maxsize=...).',\n            FutureWarning, stacklevel=2)\n\n    if mask_and_scale is None:\n        mask_and_scale = not engine == 'pseudonetcdf'\n\n    if not decode_cf:\n        mask_and_scale = False\n        decode_times = False\n        concat_characters = False\n        decode_coords = False\n\n    if cache is None:\n        cache = chunks is None\n\n    if backend_kwargs is None:\n        backend_kwargs = {}\n\n    def maybe_decode_store(store, lock=False):\n        ds = conventions.decode_cf(\n            store, mask_and_scale=mask_and_scale, decode_times=decode_times,\n            concat_characters=concat_characters, decode_coords=decode_coords,\n            drop_variables=drop_variables, use_cftime=use_cftime)\n\n        _protect_dataset_variables_inplace(ds, cache)\n\n        if chunks is not None:\n            from dask.base import tokenize\n            # if passed an actual file path, augment the token with\n            # the file modification time\n            if (isinstance(filename_or_obj, str) and\n                    not is_remote_uri(filename_or_obj)):\n                mtime = os.path.getmtime(filename_or_obj)\n            else:\n                mtime = None\n            token = tokenize(filename_or_obj, mtime, group, decode_cf,\n                             mask_and_scale, decode_times, concat_characters,\n                             decode_coords, engine, chunks, drop_variables,\n                             use_cftime)\n            name_prefix = 'open_dataset-%s' % token\n            ds2 = ds.chunk(chunks, name_prefix=name_prefix, token=token)\n            ds2._file_obj = ds._file_obj\n        else:\n            ds2 = ds\n\n        return ds2\n\n    if isinstance(filename_or_obj, Path):\n        filename_or_obj = str(filename_or_obj)\n\n    if isinstance(filename_or_obj, backends.AbstractDataStore):\n        store = filename_or_obj\n\n    elif isinstance(filename_or_obj, str):\n        filename_or_obj = _normalize_path(filename_or_obj)\n\n        if engine is None:\n            engine = _get_default_engine(filename_or_obj,\n                                         allow_remote=True)\n        if engine == 'netcdf4':\n            store = backends.NetCDF4DataStore.open(\n                filename_or_obj, group=group, lock=lock, **backend_kwargs)\n        elif engine == 'scipy':\n            store = backends.ScipyDataStore(filename_or_obj, **backend_kwargs)\n        elif engine == 'pydap':\n            store = backends.PydapDataStore.open(\n                filename_or_obj, **backend_kwargs)\n        elif engine == 'h5netcdf':\n            store = backends.H5NetCDFStore(\n                filename_or_obj, group=group, lock=lock, **backend_kwargs)\n        elif engine == 'pynio':\n            store = backends.NioDataStore(\n                filename_or_obj, lock=lock, **backend_kwargs)\n        elif engine == 'pseudonetcdf':\n            store = backends.PseudoNetCDFDataStore.open(\n                filename_or_obj, lock=lock, **backend_kwargs)\n        elif engine == 'cfgrib':\n            store = backends.CfGribDataStore(\n                filename_or_obj, lock=lock, **backend_kwargs)\n\n    else:\n        if engine not in [None, 'scipy', 'h5netcdf']:\n            raise ValueError(\"can only read bytes or file-like objects \"\n                             \"with engine='scipy' or 'h5netcdf'\")\n        engine = _get_engine_from_magic_number(filename_or_obj)\n        if engine == 'scipy':\n            store = backends.ScipyDataStore(filename_or_obj, **backend_kwargs)\n        elif engine == 'h5netcdf':\n            store = backends.H5NetCDFStore(filename_or_obj, group=group,\n                                           lock=lock, **backend_kwargs)\n\n    with close_on_error(store):\n        ds = maybe_decode_store(store)\n\n    # Ensure source filename always stored in dataset object (GH issue #2550)\n    if 'source' not in ds.encoding:\n        if isinstance(filename_or_obj, str):\n            ds.encoding['source'] = filename_or_obj\n\n    return ds"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef open_dataarray(filename_or_obj, group=None, decode_cf=True,\n                   mask_and_scale=None, decode_times=True, autoclose=None,\n                   concat_characters=True, decode_coords=True, engine=None,\n                   chunks=None, lock=None, cache=None, drop_variables=None,\n                   backend_kwargs=None, use_cftime=None):\n    \"\"\"Open an DataArray from a netCDF file containing a single data variable.\n\n    This is designed to read netCDF files with only one data variable. If\n    multiple variables are present then a ValueError is raised.\n\n    Parameters\n    ----------\n    filename_or_obj : str, Path, file or xarray.backends.*DataStore\n        Strings and Paths are interpreted as a path to a netCDF file or an\n        OpenDAP URL and opened with python-netCDF4, unless the filename ends\n        with .gz, in which case the file is gunzipped and opened with\n        scipy.io.netcdf (only netCDF3 supported). Byte-strings or file-like\n        objects are opened by scipy.io.netcdf (netCDF3) or h5py (netCDF4/HDF).\n    group : str, optional\n        Path to the netCDF4 group in the given file to open (only works for\n        netCDF4 files).\n    decode_cf : bool, optional\n        Whether to decode these variables, assuming they were saved according\n        to CF conventions.\n    mask_and_scale : bool, optional\n        If True, replace array values equal to `_FillValue` with NA and scale\n        values according to the formula `original_values * scale_factor +\n        add_offset`, where `_FillValue`, `scale_factor` and `add_offset` are\n        taken from variable attributes (if they exist).  If the `_FillValue` or\n        `missing_value` attribute contains multiple values a warning will be\n        issued and all array values matching one of the multiple values will\n        be replaced by NA. mask_and_scale defaults to True except for the\n        pseudonetcdf backend.\n    decode_times : bool, optional\n        If True, decode times encoded in the standard NetCDF datetime format\n        into datetime objects. Otherwise, leave them encoded as numbers.\n    concat_characters : bool, optional\n        If True, concatenate along the last dimension of character arrays to\n        form string arrays. Dimensions will only be concatenated over (and\n        removed) if they have no corresponding variable and if they are only\n        used as the last dimension of character arrays.\n    decode_coords : bool, optional\n        If True, decode the 'coordinates' attribute to identify coordinates in\n        the resulting dataset.\n    engine : {'netcdf4', 'scipy', 'pydap', 'h5netcdf', 'pynio', 'cfgrib'}, \\\n        optional\n        Engine to use when reading files. If not provided, the default engine\n        is chosen based on available dependencies, with a preference for\n        'netcdf4'.\n    chunks : int or dict, optional\n        If chunks is provided, it used to load the new dataset into dask\n        arrays.\n    lock : False or duck threading.Lock, optional\n        Resource lock to use when reading data from disk. Only relevant when\n        using dask or another form of parallelism. By default, appropriate\n        locks are chosen to safely read and write files with the currently\n        active dask scheduler.\n    cache : bool, optional\n        If True, cache data loaded from the underlying datastore in memory as\n        NumPy arrays when accessed to avoid reading from the underlying data-\n        store multiple times. Defaults to True unless you specify the `chunks`\n        argument to use dask, in which case it defaults to False. Does not\n        change the behavior of coordinates corresponding to dimensions, which\n        always load their data from disk into a ``pandas.Index``.\n    drop_variables: string or iterable, optional\n        A variable or list of variables to exclude from being parsed from the\n        dataset. This may be useful to drop variables with problems or\n        inconsistent values.\n    backend_kwargs: dictionary, optional\n        A dictionary of keyword arguments to pass on to the backend. This\n        may be useful when backend options would improve performance or\n        allow user control of dataset processing.\n    use_cftime: bool, optional\n        Only relevant if encoded dates come from a standard calendar\n        (e.g. 'gregorian', 'proleptic_gregorian', 'standard', or not\n        specified).  If None (default), attempt to decode times to\n        ``np.datetime64[ns]`` objects; if this is not possible, decode times to\n        ``cftime.datetime`` objects. If True, always decode times to\n        ``cftime.datetime`` objects, regardless of whether or not they can be\n        represented using ``np.datetime64[ns]`` objects.  If False, always\n        decode times to ``np.datetime64[ns]`` objects; if this is not possible\n        raise an error.\n\n    Notes\n    -----\n    This is designed to be fully compatible with `DataArray.to_netcdf`. Saving\n    using `DataArray.to_netcdf` and then loading with this function will\n    produce an identical result.\n\n    All parameters are passed directly to `xarray.open_dataset`. See that\n    documentation for further details.\n\n    See also\n    --------\n    open_dataset\n    \"\"\"\n\n    dataset = open_dataset(filename_or_obj, group=group, decode_cf=decode_cf,\n                           mask_and_scale=mask_and_scale,\n                           decode_times=decode_times, autoclose=autoclose,\n                           concat_characters=concat_characters,\n                           decode_coords=decode_coords, engine=engine,\n                           chunks=chunks, lock=lock, cache=cache,\n                           drop_variables=drop_variables,\n                           backend_kwargs=backend_kwargs,\n                           use_cftime=use_cftime)\n\n    if len(dataset.data_vars) != 1:\n        raise ValueError('Given file dataset contains more than one data '\n                         'variable. Please read with xarray.open_dataset and '\n                         'then select the variable you want.')\n    else:\n        data_array, = dataset.data_vars.values()\n\n    data_array._file_obj = dataset._file_obj\n\n    # Reset names if they were changed during saving\n    # to ensure that we can 'roundtrip' perfectly\n    if DATAARRAY_NAME in dataset.attrs:\n        data_array.name = dataset.attrs[DATAARRAY_NAME]\n        del dataset.attrs[DATAARRAY_NAME]\n\n    if data_array.name == DATAARRAY_VARIABLE:\n        data_array.name = None\n\n    return data_array", "response": "Open an array from a netCDF file containing a single data variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef open_mfdataset(paths, chunks=None, concat_dim=_CONCAT_DIM_DEFAULT,\n                   compat='no_conflicts', preprocess=None, engine=None,\n                   lock=None, data_vars='all', coords='different',\n                   autoclose=None, parallel=False, **kwargs):\n    \"\"\"Open multiple files as a single dataset.\n\n    Requires dask to be installed. See documentation for details on dask [1].\n    Attributes from the first dataset file are used for the combined dataset.\n\n    Parameters\n    ----------\n    paths : str or sequence\n        Either a string glob in the form \"path/to/my/files/*.nc\" or an explicit\n        list of files to open.  Paths can be given as strings or as pathlib\n        Paths.\n    chunks : int or dict, optional\n        Dictionary with keys given by dimension names and values given by chunk\n        sizes. In general, these should divide the dimensions of each dataset.\n        If int, chunk each dimension by ``chunks``.\n        By default, chunks will be chosen to load entire input files into\n        memory at once. This has a major impact on performance: please see the\n        full documentation for more details [2].\n    concat_dim : None, str, DataArray or Index, optional\n        Dimension to concatenate files along. This argument is passed on to\n        :py:func:`xarray.auto_combine` along with the dataset objects. You only\n        need to provide this argument if the dimension along which you want to\n        concatenate is not a dimension in the original datasets, e.g., if you\n        want to stack a collection of 2D arrays along a third dimension.\n        By default, xarray attempts to infer this argument by examining\n        component files. Set ``concat_dim=None`` explicitly to disable\n        concatenation.\n    compat : {'identical', 'equals', 'broadcast_equals', 'no_conflicts'}, optional\n        String indicating how to compare variables of the same name for\n        potential conflicts when merging:\n         * 'broadcast_equals': all values must be equal when variables are\n           broadcast against each other to ensure common dimensions.\n         * 'equals': all values and dimensions must be the same.\n         * 'identical': all values, dimensions and attributes must be the\n           same.\n         * 'no_conflicts': only values which are not null in both datasets\n           must be equal. The returned dataset then contains the combination\n           of all non-null values.\n    preprocess : callable, optional\n        If provided, call this function on each dataset prior to concatenation.\n        You can find the file-name from which each dataset was loaded in\n        ``ds.encoding['source']``.\n    engine : {'netcdf4', 'scipy', 'pydap', 'h5netcdf', 'pynio', 'cfgrib'}, \\\n        optional\n        Engine to use when reading files. If not provided, the default engine\n        is chosen based on available dependencies, with a preference for\n        'netcdf4'.\n    lock : False or duck threading.Lock, optional\n        Resource lock to use when reading data from disk. Only relevant when\n        using dask or another form of parallelism. By default, appropriate\n        locks are chosen to safely read and write files with the currently\n        active dask scheduler.\n    data_vars : {'minimal', 'different', 'all' or list of str}, optional\n        These data variables will be concatenated together:\n\n         * 'minimal': Only data variables in which the dimension already\n           appears are included.\n         * 'different': Data variables which are not equal (ignoring\n           attributes) across all datasets are also concatenated (as well as\n           all for which dimension already appears). Beware: this option may\n           load the data payload of data variables into memory if they are not\n           already loaded.\n         * 'all': All data variables will be concatenated.\n         * list of str: The listed data variables will be concatenated, in\n           addition to the 'minimal' data variables.\n    coords : {'minimal', 'different', 'all' o list of str}, optional\n        These coordinate variables will be concatenated together:\n\n         * 'minimal': Only coordinates in which the dimension already appears\n           are included.\n         * 'different': Coordinates which are not equal (ignoring attributes)\n           across all datasets are also concatenated (as well as all for which\n           dimension already appears). Beware: this option may load the data\n           payload of coordinate variables into memory if they are not already\n           loaded.\n         * 'all': All coordinate variables will be concatenated, except\n           those corresponding to other dimensions.\n         * list of str: The listed coordinate variables will be concatenated,\n           in addition the 'minimal' coordinates.\n    parallel : bool, optional\n        If True, the open and preprocess steps of this function will be\n        performed in parallel using ``dask.delayed``. Default is False.\n    **kwargs : optional\n        Additional arguments passed on to :py:func:`xarray.open_dataset`.\n\n    Returns\n    -------\n    xarray.Dataset\n\n    Notes\n    -----\n    ``open_mfdataset`` opens files with read-only access. When you modify values\n    of a Dataset, even one linked to files on disk, only the in-memory copy you\n    are manipulating in xarray is modified: the original file on disk is never\n    touched.\n\n    See Also\n    --------\n    auto_combine\n    open_dataset\n\n    References\n    ----------\n\n    .. [1] http://xarray.pydata.org/en/stable/dask.html\n    .. [2] http://xarray.pydata.org/en/stable/dask.html#chunking-and-performance\n    \"\"\"  # noqa\n    if isinstance(paths, str):\n        if is_remote_uri(paths):\n            raise ValueError(\n                'cannot do wild-card matching for paths that are remote URLs: '\n                '{!r}. Instead, supply paths as an explicit list of strings.'\n                .format(paths))\n        paths = sorted(glob(paths))\n    else:\n        paths = [str(p) if isinstance(p, Path) else p for p in paths]\n\n    if not paths:\n        raise IOError('no files to open')\n\n    # Coerce 1D input into ND to maintain backwards-compatible API until API\n    # for N-D combine decided\n    # (see https://github.com/pydata/xarray/pull/2553/#issuecomment-445892746)\n    if concat_dim is None or concat_dim is _CONCAT_DIM_DEFAULT:\n        concat_dims = concat_dim\n    elif not isinstance(concat_dim, list):\n        concat_dims = [concat_dim]\n    else:\n        concat_dims = concat_dim\n    infer_order_from_coords = False\n\n    # If infer_order_from_coords=True then this is unnecessary, but quick.\n    # If infer_order_from_coords=False then this creates a flat list which is\n    # easier to iterate over, while saving the originally-supplied structure\n    combined_ids_paths, concat_dims = _infer_concat_order_from_positions(\n        paths, concat_dims)\n    ids, paths = (\n        list(combined_ids_paths.keys()), list(combined_ids_paths.values()))\n\n    open_kwargs = dict(engine=engine, chunks=chunks or {}, lock=lock,\n                       autoclose=autoclose, **kwargs)\n\n    if parallel:\n        import dask\n        # wrap the open_dataset, getattr, and preprocess with delayed\n        open_ = dask.delayed(open_dataset)\n        getattr_ = dask.delayed(getattr)\n        if preprocess is not None:\n            preprocess = dask.delayed(preprocess)\n    else:\n        open_ = open_dataset\n        getattr_ = getattr\n\n    datasets = [open_(p, **open_kwargs) for p in paths]\n    file_objs = [getattr_(ds, '_file_obj') for ds in datasets]\n    if preprocess is not None:\n        datasets = [preprocess(ds) for ds in datasets]\n\n    if parallel:\n        # calling compute here will return the datasets/file_objs lists,\n        # the underlying datasets will still be stored as dask arrays\n        datasets, file_objs = dask.compute(datasets, file_objs)\n\n    # Close datasets in case of a ValueError\n    try:\n        if infer_order_from_coords:\n            # Discard ordering because it should be redone from coordinates\n            ids = False\n\n        combined = _auto_combine(\n            datasets, concat_dims=concat_dims,\n            compat=compat,\n            data_vars=data_vars, coords=coords,\n            infer_order_from_coords=infer_order_from_coords,\n            ids=ids)\n    except ValueError:\n        for ds in datasets:\n            ds.close()\n        raise\n\n    combined._file_obj = _MultiFileCloser(file_objs)\n    combined.attrs = datasets[0].attrs\n    return combined", "response": "Open multiple files as a single dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_netcdf(dataset, path_or_file=None, mode='w', format=None, group=None,\n              engine=None, encoding=None, unlimited_dims=None, compute=True,\n              multifile=False):\n    \"\"\"This function creates an appropriate datastore for writing a dataset to\n    disk as a netCDF file\n\n    See `Dataset.to_netcdf` for full API docs.\n\n    The ``multifile`` argument is only for the private use of save_mfdataset.\n    \"\"\"\n    if isinstance(path_or_file, Path):\n        path_or_file = str(path_or_file)\n\n    if encoding is None:\n        encoding = {}\n\n    if path_or_file is None:\n        if engine is None:\n            engine = 'scipy'\n        elif engine != 'scipy':\n            raise ValueError('invalid engine for creating bytes with '\n                             'to_netcdf: %r. Only the default engine '\n                             \"or engine='scipy' is supported\" % engine)\n        if not compute:\n            raise NotImplementedError(\n                'to_netcdf() with compute=False is not yet implemented when '\n                'returning bytes')\n    elif isinstance(path_or_file, str):\n        if engine is None:\n            engine = _get_default_engine(path_or_file)\n        path_or_file = _normalize_path(path_or_file)\n    else:  # file-like object\n        engine = 'scipy'\n\n    # validate Dataset keys, DataArray names, and attr keys/values\n    _validate_dataset_names(dataset)\n    _validate_attrs(dataset)\n\n    try:\n        store_open = WRITEABLE_STORES[engine]\n    except KeyError:\n        raise ValueError('unrecognized engine for to_netcdf: %r' % engine)\n\n    if format is not None:\n        format = format.upper()\n\n    # handle scheduler specific logic\n    scheduler = _get_scheduler()\n    have_chunks = any(v.chunks for v in dataset.variables.values())\n\n    autoclose = have_chunks and scheduler in ['distributed', 'multiprocessing']\n    if autoclose and engine == 'scipy':\n        raise NotImplementedError(\"Writing netCDF files with the %s backend \"\n                                  \"is not currently supported with dask's %s \"\n                                  \"scheduler\" % (engine, scheduler))\n\n    target = path_or_file if path_or_file is not None else BytesIO()\n    kwargs = dict(autoclose=True) if autoclose else {}\n    store = store_open(target, mode, format, group, **kwargs)\n\n    if unlimited_dims is None:\n        unlimited_dims = dataset.encoding.get('unlimited_dims', None)\n    if isinstance(unlimited_dims, str):\n        unlimited_dims = [unlimited_dims]\n\n    writer = ArrayWriter()\n\n    # TODO: figure out how to refactor this logic (here and in save_mfdataset)\n    # to avoid this mess of conditionals\n    try:\n        # TODO: allow this work (setting up the file for writing array data)\n        # to be parallelized with dask\n        dump_to_store(dataset, store, writer, encoding=encoding,\n                      unlimited_dims=unlimited_dims)\n        if autoclose:\n            store.close()\n\n        if multifile:\n            return writer, store\n\n        writes = writer.sync(compute=compute)\n\n        if path_or_file is None:\n            store.sync()\n            return target.getvalue()\n    finally:\n        if not multifile and compute:\n            store.close()\n\n    if not compute:\n        import dask\n        return dask.delayed(_finalize_store)(writes, store)", "response": "This function creates an appropriate datastore for writing a dataset to a netCDF file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstore dataset contents to a backends. DataStore object.", "response": "def dump_to_store(dataset, store, writer=None, encoder=None,\n                  encoding=None, unlimited_dims=None):\n    \"\"\"Store dataset contents to a backends.*DataStore object.\"\"\"\n    if writer is None:\n        writer = ArrayWriter()\n\n    if encoding is None:\n        encoding = {}\n\n    variables, attrs = conventions.encode_dataset_coordinates(dataset)\n\n    check_encoding = set()\n    for k, enc in encoding.items():\n        # no need to shallow copy the variable again; that already happened\n        # in encode_dataset_coordinates\n        variables[k].encoding = enc\n        check_encoding.add(k)\n\n    if encoder:\n        variables, attrs = encoder(variables, attrs)\n\n    store.store(variables, attrs, check_encoding, writer,\n                unlimited_dims=unlimited_dims)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save_mfdataset(datasets, paths, mode='w', format=None, groups=None,\n                   engine=None, compute=True):\n    \"\"\"Write multiple datasets to disk as netCDF files simultaneously.\n\n    This function is intended for use with datasets consisting of dask.array\n    objects, in which case it can write the multiple datasets to disk\n    simultaneously using a shared thread pool.\n\n    When not using dask, it is no different than calling ``to_netcdf``\n    repeatedly.\n\n    Parameters\n    ----------\n    datasets : list of xarray.Dataset\n        List of datasets to save.\n    paths : list of str or list of Paths\n        List of paths to which to save each corresponding dataset.\n    mode : {'w', 'a'}, optional\n        Write ('w') or append ('a') mode. If mode='w', any existing file at\n        these locations will be overwritten.\n    format : {'NETCDF4', 'NETCDF4_CLASSIC', 'NETCDF3_64BIT',\n              'NETCDF3_CLASSIC'}, optional\n\n        File format for the resulting netCDF file:\n\n        * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API\n          features.\n        * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only\n          netCDF 3 compatible API features.\n        * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,\n          which fully supports 2+ GB files, but is only compatible with\n          clients linked against netCDF version 3.6.0 or later.\n        * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not\n          handle 2+ GB files very well.\n\n        All formats are supported by the netCDF4-python library.\n        scipy.io.netcdf only supports the last two formats.\n\n        The default format is NETCDF4 if you are saving a file to disk and\n        have the netCDF4-python library available. Otherwise, xarray falls\n        back to using scipy to write netCDF files and defaults to the\n        NETCDF3_64BIT format (scipy does not support netCDF4).\n    groups : list of str, optional\n        Paths to the netCDF4 group in each corresponding file to which to save\n        datasets (only works for format='NETCDF4'). The groups will be created\n        if necessary.\n    engine : {'netcdf4', 'scipy', 'h5netcdf'}, optional\n        Engine to use when writing netCDF files. If not provided, the\n        default engine is chosen based on available dependencies, with a\n        preference for 'netcdf4' if writing to a file on disk.\n        See `Dataset.to_netcdf` for additional information.\n    compute: boolean\n        If true compute immediately, otherwise return a\n        ``dask.delayed.Delayed`` object that can be computed later.\n\n    Examples\n    --------\n\n    Save a dataset into one netCDF per year of data:\n\n    >>> years, datasets = zip(*ds.groupby('time.year'))\n    >>> paths = ['%s.nc' % y for y in years]\n    >>> xr.save_mfdataset(datasets, paths)\n    \"\"\"\n    if mode == 'w' and len(set(paths)) < len(paths):\n        raise ValueError(\"cannot use mode='w' when writing multiple \"\n                         'datasets to the same path')\n\n    for obj in datasets:\n        if not isinstance(obj, Dataset):\n            raise TypeError('save_mfdataset only supports writing Dataset '\n                            'objects, received type %s' % type(obj))\n\n    if groups is None:\n        groups = [None] * len(datasets)\n\n    if len(set([len(datasets), len(paths), len(groups)])) > 1:\n        raise ValueError('must supply lists of the same length for the '\n                         'datasets, paths and groups arguments to '\n                         'save_mfdataset')\n\n    writers, stores = zip(*[\n        to_netcdf(ds, path, mode, format, group, engine, compute=compute,\n                  multifile=True)\n        for ds, path, group in zip(datasets, paths, groups)])\n\n    try:\n        writes = [w.sync(compute=compute) for w in writers]\n    finally:\n        if compute:\n            for store in stores:\n                store.close()\n\n    if not compute:\n        import dask\n        return dask.delayed([dask.delayed(_finalize_store)(w, s)\n                             for w, s in zip(writes, stores)])", "response": "Save multiple datasets to disk as netCDF files simultaneously."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_zarr(dataset, store=None, mode='w-', synchronizer=None, group=None,\n            encoding=None, compute=True, consolidated=False):\n    \"\"\"This function creates an appropriate datastore for writing a dataset to\n    a zarr ztore\n\n    See `Dataset.to_zarr` for full API docs.\n    \"\"\"\n    if isinstance(store, Path):\n        store = str(store)\n    if encoding is None:\n        encoding = {}\n\n    # validate Dataset keys, DataArray names, and attr keys/values\n    _validate_dataset_names(dataset)\n    _validate_attrs(dataset)\n\n    zstore = backends.ZarrStore.open_group(store=store, mode=mode,\n                                           synchronizer=synchronizer,\n                                           group=group,\n                                           consolidate_on_close=consolidated)\n\n    writer = ArrayWriter()\n    # TODO: figure out how to properly handle unlimited_dims\n    dump_to_store(dataset, zstore, writer, encoding=encoding)\n    writes = writer.sync(compute=compute)\n\n    if compute:\n        _finalize_store(writes, zstore)\n    else:\n        import dask\n        return dask.delayed(_finalize_store)(writes, zstore)\n\n    return zstore", "response": "This function creates an appropriate datastore for writing a dataset to a zarr ztore\n \\'' + store + '\\'."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a new MultiIndex that contains all the unused levels in the current MultiIndex.", "response": "def remove_unused_levels(self):\n    \"\"\"\n    create a new MultiIndex from the current that removing\n    unused levels, meaning that they are not expressed in the labels\n    The resulting MultiIndex will have the same outward\n    appearance, meaning the same .values and ordering. It will also\n    be .equals() to the original.\n    .. versionadded:: 0.20.0\n    Returns\n    -------\n    MultiIndex\n    Examples\n    --------\n    >>> i = pd.MultiIndex.from_product([range(2), list('ab')])\n    MultiIndex(levels=[[0, 1], ['a', 'b']],\n               labels=[[0, 0, 1, 1], [0, 1, 0, 1]])\n    >>> i[2:]\n    MultiIndex(levels=[[0, 1], ['a', 'b']],\n               labels=[[1, 1], [0, 1]])\n    The 0 from the first level is not represented\n    and can be removed\n    >>> i[2:].remove_unused_levels()\n    MultiIndex(levels=[[1], ['a', 'b']],\n               labels=[[0, 0], [0, 1]])\n    \"\"\"\n    import pandas.core.algorithms as algos\n\n    new_levels = []\n    new_labels = []\n\n    changed = False\n    for lev, lab in zip(self.levels, self.labels):\n\n        # Since few levels are typically unused, bincount() is more\n        # efficient than unique() - however it only accepts positive values\n        # (and drops order):\n        uniques = np.where(np.bincount(lab + 1) > 0)[0] - 1\n        has_na = int(len(uniques) and (uniques[0] == -1))\n\n        if len(uniques) != len(lev) + has_na:\n            # We have unused levels\n            changed = True\n\n            # Recalculate uniques, now preserving order.\n            # Can easily be cythonized by exploiting the already existing\n            # \"uniques\" and stop parsing \"lab\" when all items are found:\n            uniques = algos.unique(lab)\n            if has_na:\n                na_idx = np.where(uniques == -1)[0]\n                # Just ensure that -1 is in first position:\n                uniques[[0, na_idx[0]]] = uniques[[na_idx[0], 0]]\n\n            # labels get mapped from uniques to 0:len(uniques)\n            # -1 (if present) is mapped to last position\n            label_mapping = np.zeros(len(lev) + has_na)\n            # ... and reassigned value -1:\n            label_mapping[uniques] = np.arange(len(uniques)) - has_na\n\n            lab = label_mapping[lab]\n\n            # new levels are simple\n            lev = lev.take(uniques[has_na:])\n\n        new_levels.append(lev)\n        new_labels.append(lab)\n\n    result = self._shallow_copy()\n\n    if changed:\n        result._reset_identity()\n        result._set_levels(new_levels, validate=False)\n        result._set_labels(new_labels, validate=False)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef robust_getitem(array, key, catch=Exception, max_retries=6,\n                   initial_delay=500):\n    \"\"\"\n    Robustly index an array, using retry logic with exponential backoff if any\n    of the errors ``catch`` are raised. The initial_delay is measured in ms.\n\n    With the default settings, the maximum delay will be in the range of 32-64\n    seconds.\n    \"\"\"\n    assert max_retries >= 0\n    for n in range(max_retries + 1):\n        try:\n            return array[key]\n        except catch:\n            if n == max_retries:\n                raise\n            base_delay = initial_delay * 2 ** n\n            next_delay = base_delay + np.random.randint(base_delay)\n            msg = ('getitem failed, waiting %s ms before trying again '\n                   '(%s tries remaining). Full traceback: %s' %\n                   (next_delay, max_retries - n, traceback.format_exc()))\n            logger.debug(msg)\n            time.sleep(1e-3 * next_delay)", "response": "Robustly index an array using exponential backoff."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nencode the variables and attributes in this store", "response": "def encode(self, variables, attributes):\n        \"\"\"\n        Encode the variables and attributes in this store\n\n        Parameters\n        ----------\n        variables : dict-like\n            Dictionary of key/value (variable name / xr.Variable) pairs\n        attributes : dict-like\n            Dictionary of key/value (attribute name / attribute) pairs\n\n        Returns\n        -------\n        variables : dict-like\n        attributes : dict-like\n\n        \"\"\"\n        variables = OrderedDict([(k, self.encode_variable(v))\n                                 for k, v in variables.items()])\n        attributes = OrderedDict([(k, self.encode_attribute(v))\n                                  for k, v in attributes.items()])\n        return variables, attributes"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntops level method for putting data on this store, this method: - encodes variables/attributes - sets dimensions - sets variables Parameters ---------- variables : dict-like Dictionary of key/value (variable name / xr.Variable) pairs attributes : dict-like Dictionary of key/value (attribute name / attribute) pairs check_encoding_set : list-like List of variables that should be checked for invalid encoding values writer : ArrayWriter unlimited_dims : list-like List of dimension names that should be treated as unlimited dimensions.", "response": "def store(self, variables, attributes, check_encoding_set=frozenset(),\n              writer=None, unlimited_dims=None):\n        \"\"\"\n        Top level method for putting data on this store, this method:\n          - encodes variables/attributes\n          - sets dimensions\n          - sets variables\n\n        Parameters\n        ----------\n        variables : dict-like\n            Dictionary of key/value (variable name / xr.Variable) pairs\n        attributes : dict-like\n            Dictionary of key/value (attribute name / attribute) pairs\n        check_encoding_set : list-like\n            List of variables that should be checked for invalid encoding\n            values\n        writer : ArrayWriter\n        unlimited_dims : list-like\n            List of dimension names that should be treated as unlimited\n            dimensions.\n        \"\"\"\n        if writer is None:\n            writer = ArrayWriter()\n\n        variables, attributes = self.encode(variables, attributes)\n\n        self.set_attributes(attributes)\n        self.set_dimensions(variables, unlimited_dims=unlimited_dims)\n        self.set_variables(variables, check_encoding_set, writer,\n                           unlimited_dims=unlimited_dims)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_attributes(self, attributes):\n        for k, v in attributes.items():\n            self.set_attribute(k, v)", "response": "This method sets the attributes on the object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_dimensions(self, variables, unlimited_dims=None):\n        if unlimited_dims is None:\n            unlimited_dims = set()\n\n        existing_dims = self.get_dimensions()\n\n        dims = OrderedDict()\n        for v in unlimited_dims:  # put unlimited_dims first\n            dims[v] = None\n        for v in variables.values():\n            dims.update(dict(zip(v.dims, v.shape)))\n\n        for dim, length in dims.items():\n            if dim in existing_dims and length != existing_dims[dim]:\n                raise ValueError(\n                    \"Unable to update size for existing dimension\"\n                    \"%r (%d != %d)\" % (dim, length, existing_dims[dim]))\n            elif dim not in existing_dims:\n                is_unlimited = dim in unlimited_dims\n                self.set_dimension(dim, length, is_unlimited)", "response": "This method sets the dimensions of the data store."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _season_from_months(months):\n    # TODO: Move \"season\" accessor upstream into pandas\n    seasons = np.array(['DJF', 'MAM', 'JJA', 'SON'])\n    months = np.asarray(months)\n    return seasons[(months // 3) % 4]", "response": "Compute season from month ordinal\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _access_through_cftimeindex(values, name):\n    from ..coding.cftimeindex import CFTimeIndex\n    values_as_cftimeindex = CFTimeIndex(values.ravel())\n    if name == 'season':\n        months = values_as_cftimeindex.month\n        field_values = _season_from_months(months)\n    else:\n        field_values = getattr(values_as_cftimeindex, name)\n    return field_values.reshape(values.shape)", "response": "Coerce an array of datetime - like values to a CFTimeIndex\n    and access requested datetime component\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _access_through_series(values, name):\n    values_as_series = pd.Series(values.ravel())\n    if name == \"season\":\n        months = values_as_series.dt.month.values\n        field_values = _season_from_months(months)\n    else:\n        field_values = getattr(values_as_series.dt, name).values\n    return field_values.reshape(values.shape)", "response": "Coerce an array of datetime - like values to a pandas Series and\n    access requested datetime component\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_date_field(values, name, dtype):\n    if is_np_datetime_like(values.dtype):\n        access_method = _access_through_series\n    else:\n        access_method = _access_through_cftimeindex\n\n    if isinstance(values, dask_array_type):\n        from dask.array import map_blocks\n        return map_blocks(access_method,\n                          values, name, dtype=dtype)\n    else:\n        return access_method(values, name)", "response": "Indirectly access pandas libts. get_date_field by wrapping data\n    as a Series and calling through. dt attribute."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncoerce an array of datetime - like values to a pandas Series and apply requested rounding", "response": "def _round_series(values, name, freq):\n    \"\"\"Coerce an array of datetime-like values to a pandas Series and\n    apply requested rounding\n    \"\"\"\n    values_as_series = pd.Series(values.ravel())\n    method = getattr(values_as_series.dt, name)\n    field_values = method(freq=freq).values\n\n    return field_values.reshape(values.shape)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fillna(data, other, join=\"left\", dataset_join=\"left\"):\n    from .computation import apply_ufunc\n\n    return apply_ufunc(duck_array_ops.fillna, data, other,\n                       join=join,\n                       dask=\"allowed\",\n                       dataset_join=dataset_join,\n                       dataset_fill_value=np.nan,\n                       keep_attrs=True)", "response": "Fill missing values in this object with data from other object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef where_method(self, cond, other=dtypes.NA):\n    from .computation import apply_ufunc\n    # alignment for three arguments is complicated, so don't support it yet\n    join = 'inner' if other is dtypes.NA else 'exact'\n    return apply_ufunc(duck_array_ops.where_method,\n                       self, cond, other,\n                       join=join,\n                       dataset_join=join,\n                       dask='allowed',\n                       keep_attrs=True)", "response": "Return elements from self or other depending on cond."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nopen a netcdf dataset from the online repository.", "response": "def open_dataset(name, cache=True, cache_dir=_default_cache_dir,\n                 github_url='https://github.com/pydata/xarray-data',\n                 branch='master', **kws):\n    \"\"\"\n    Load a dataset from the online repository (requires internet).\n\n    If a local copy is found then always use that to avoid network traffic.\n\n    Parameters\n    ----------\n    name : str\n        Name of the netcdf file containing the dataset\n        ie. 'air_temperature'\n    cache_dir : string, optional\n        The directory in which to search for and write cached data.\n    cache : boolean, optional\n        If True, then cache data locally for use on subsequent calls\n    github_url : string\n        Github repository where the data is stored\n    branch : string\n        The git branch to download from\n    kws : dict, optional\n        Passed to xarray.open_dataset\n\n    See Also\n    --------\n    xarray.open_dataset\n\n    \"\"\"\n    longdir = _os.path.expanduser(cache_dir)\n    fullname = name + '.nc'\n    localfile = _os.sep.join((longdir, fullname))\n    md5name = name + '.md5'\n    md5file = _os.sep.join((longdir, md5name))\n\n    if not _os.path.exists(localfile):\n\n        # This will always leave this directory on disk.\n        # May want to add an option to remove it.\n        if not _os.path.isdir(longdir):\n            _os.mkdir(longdir)\n\n        url = '/'.join((github_url, 'raw', branch, fullname))\n        urlretrieve(url, localfile)\n        url = '/'.join((github_url, 'raw', branch, md5name))\n        urlretrieve(url, md5file)\n\n        localmd5 = file_md5_checksum(localfile)\n        with open(md5file, 'r') as f:\n            remotemd5 = f.read()\n        if localmd5 != remotemd5:\n            _os.remove(localfile)\n            msg = \"\"\"\n            MD5 checksum does not match, try downloading dataset again.\n            \"\"\"\n            raise IOError(msg)\n\n    ds = _open_dataset(localfile, **kws)\n\n    if not cache:\n        ds = ds.load()\n        _os.remove(localfile)\n\n    return ds"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking a key for caching files in the LRU cache.", "response": "def _make_key(self):\n        \"\"\"Make a key for caching files in the LRU cache.\"\"\"\n        value = (self._opener,\n                 self._args,\n                 'a' if self._mode == 'w' else self._mode,\n                 tuple(sorted(self._kwargs.items())))\n        return _HashedSequence(value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nacquire a file object from the manager.", "response": "def acquire(self, needs_lock=True):\n        \"\"\"Acquiring a file object from the manager.\n\n        A new file is only opened if it has expired from the\n        least-recently-used cache.\n\n        This method uses a lock, which ensures that it is thread-safe. You can\n        safely acquire a file in multiple threads at the same time, as long as\n        the underlying file object is thread-safe.\n\n        Returns\n        -------\n        An open file object, as returned by ``opener(*args, **kwargs)``.\n        \"\"\"\n        with self._optional_lock(needs_lock):\n            try:\n                file = self._cache[self._key]\n            except KeyError:\n                kwargs = self._kwargs\n                if self._mode is not _DEFAULT_MODE:\n                    kwargs = kwargs.copy()\n                    kwargs['mode'] = self._mode\n                file = self._opener(*self._args, **kwargs)\n                if self._mode == 'w':\n                    # ensure file doesn't get overriden when opened again\n                    self._mode = 'a'\n                self._cache[self._key] = file\n        return file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _enforce_size_limit(self, capacity):\n        while len(self._cache) > capacity:\n            key, value = self._cache.popitem(last=False)\n            if self._on_evict is not None:\n                self._on_evict(key, value)", "response": "Shrink the cache if necessary evicting the oldest items."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef maxsize(self, size):\n        if size < 0:\n            raise ValueError('maxsize must be non-negative')\n        with self._lock:\n            self._enforce_size_limit(size)\n            self._maxsize = size", "response": "Resize the cache evicting the oldest items if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning system information as a dict", "response": "def get_sys_info():\n    \"Returns system information as a dict\"\n\n    blob = []\n\n    # get full commit hash\n    commit = None\n    if os.path.isdir(\".git\") and os.path.isdir(\"xarray\"):\n        try:\n            pipe = subprocess.Popen('git log --format=\"%H\" -n 1'.split(\" \"),\n                                    stdout=subprocess.PIPE,\n                                    stderr=subprocess.PIPE)\n            so, serr = pipe.communicate()\n        except Exception:\n            pass\n        else:\n            if pipe.returncode == 0:\n                commit = so\n                try:\n                    commit = so.decode('utf-8')\n                except ValueError:\n                    pass\n                commit = commit.strip().strip('\"')\n\n    blob.append(('commit', commit))\n\n    try:\n        (sysname, nodename, release,\n         version, machine, processor) = platform.uname()\n        blob.extend([\n            (\"python\", sys.version),\n            (\"python-bits\", struct.calcsize(\"P\") * 8),\n            (\"OS\", \"%s\" % (sysname)),\n            (\"OS-release\", \"%s\" % (release)),\n            # (\"Version\", \"%s\" % (version)),\n            (\"machine\", \"%s\" % (machine)),\n            (\"processor\", \"%s\" % (processor)),\n            (\"byteorder\", \"%s\" % sys.byteorder),\n            (\"LC_ALL\", \"%s\" % os.environ.get('LC_ALL', \"None\")),\n            (\"LANG\", \"%s\" % os.environ.get('LANG', \"None\")),\n            (\"LOCALE\", \"%s.%s\" % locale.getlocale()),\n\n        ])\n    except Exception:\n        pass\n\n    return blob"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives a key for indexing an ndarray return an equivalent key which is a tuple with length equal to the number of dimensions.", "response": "def expanded_indexer(key, ndim):\n    \"\"\"Given a key for indexing an ndarray, return an equivalent key which is a\n    tuple with length equal to the number of dimensions.\n\n    The expansion is done by replacing all `Ellipsis` items with the right\n    number of full slices and then padding the key with full slices so that it\n    reaches the appropriate dimensionality.\n    \"\"\"\n    if not isinstance(key, tuple):\n        # numpy treats non-tuple keys equivalent to tuples of length 1\n        key = (key,)\n    new_key = []\n    # handling Ellipsis right is a little tricky, see:\n    # http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing\n    found_ellipsis = False\n    for k in key:\n        if k is Ellipsis:\n            if not found_ellipsis:\n                new_key.extend((ndim + 1 - len(key)) * [slice(None)])\n                found_ellipsis = True\n            else:\n                new_key.append(slice(None))\n        else:\n            new_key.append(k)\n    if len(new_key) > ndim:\n        raise IndexError('too many indices')\n    new_key.extend((ndim - len(new_key)) * [slice(None)])\n    return tuple(new_key)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting values into a numpy array of at most 1 - dimension while preserving tuples.", "response": "def _asarray_tuplesafe(values):\n    \"\"\"\n    Convert values into a numpy array of at most 1-dimension, while preserving\n    tuples.\n\n    Adapted from pandas.core.common._asarray_tuplesafe\n    \"\"\"\n    if isinstance(values, tuple):\n        result = utils.to_0d_object_array(values)\n    else:\n        result = np.asarray(values)\n        if result.ndim == 2:\n            result = np.empty(len(values), dtype=object)\n            result[:] = values\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the indexer for the N - dimensional index.", "response": "def get_indexer_nd(index, labels, method=None, tolerance=None):\n    \"\"\" Call pd.Index.get_indexer(labels). \"\"\"\n    kwargs = _index_method_kwargs(method, tolerance)\n\n    flat_labels = np.ravel(labels)\n    flat_indexer = index.get_indexer(flat_labels, **kwargs)\n    indexer = flat_indexer.reshape(labels.shape)\n    return indexer"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert_label_indexer(index, label, index_name='', method=None,\n                          tolerance=None):\n    \"\"\"Given a pandas.Index and labels (e.g., from __getitem__) for one\n    dimension, return an indexer suitable for indexing an ndarray along that\n    dimension. If `index` is a pandas.MultiIndex and depending on `label`,\n    return a new pandas.Index or pandas.MultiIndex (otherwise return None).\n    \"\"\"\n    new_index = None\n\n    if isinstance(label, slice):\n        if method is not None or tolerance is not None:\n            raise NotImplementedError(\n                'cannot use ``method`` argument if any indexers are '\n                'slice objects')\n        indexer = index.slice_indexer(_sanitize_slice_element(label.start),\n                                      _sanitize_slice_element(label.stop),\n                                      _sanitize_slice_element(label.step))\n        if not isinstance(indexer, slice):\n            # unlike pandas, in xarray we never want to silently convert a\n            # slice indexer into an array indexer\n            raise KeyError('cannot represent labeled-based slice indexer for '\n                           'dimension %r with a slice over integer positions; '\n                           'the index is unsorted or non-unique' % index_name)\n\n    elif is_dict_like(label):\n        is_nested_vals = _is_nested_tuple(tuple(label.values()))\n        if not isinstance(index, pd.MultiIndex):\n            raise ValueError('cannot use a dict-like object for selection on '\n                             'a dimension that does not have a MultiIndex')\n        elif len(label) == index.nlevels and not is_nested_vals:\n            indexer = index.get_loc(tuple((label[k] for k in index.names)))\n        else:\n            for k, v in label.items():\n                # index should be an item (i.e. Hashable) not an array-like\n                if not isinstance(v, Hashable):\n                    raise ValueError('Vectorized selection is not '\n                                     'available along level variable: ' + k)\n            indexer, new_index = index.get_loc_level(\n                tuple(label.values()), level=tuple(label.keys()))\n\n            # GH2619. Raise a KeyError if nothing is chosen\n            if indexer.dtype.kind == 'b' and indexer.sum() == 0:\n                raise KeyError('{} not found'.format(label))\n\n    elif isinstance(label, tuple) and isinstance(index, pd.MultiIndex):\n        if _is_nested_tuple(label):\n            indexer = index.get_locs(label)\n        elif len(label) == index.nlevels:\n            indexer = index.get_loc(label)\n        else:\n            indexer, new_index = index.get_loc_level(\n                label, level=list(range(len(label)))\n            )\n    else:\n        label = (label if getattr(label, 'ndim', 1) > 1  # vectorized-indexing\n                 else _asarray_tuplesafe(label))\n        if label.ndim == 0:\n            if isinstance(index, pd.MultiIndex):\n                indexer, new_index = index.get_loc_level(label.item(), level=0)\n            else:\n                indexer = get_loc(index, label.item(), method, tolerance)\n        elif label.dtype.kind == 'b':\n            indexer = label\n        else:\n            if isinstance(index, pd.MultiIndex) and label.ndim > 1:\n                raise ValueError('Vectorized selection is not available along '\n                                 'MultiIndex variable: ' + index_name)\n            indexer = get_indexer_nd(index, label, method, tolerance)\n            if np.any(indexer < 0):\n                raise KeyError('not all values found in index %r'\n                               % index_name)\n    return indexer, new_index", "response": "Converts a pandas. Index and labels to a pandas. MultiIndex."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a xarray data object and label based indexers return a mapping of label indexers with only dimension names as keys.", "response": "def get_dim_indexers(data_obj, indexers):\n    \"\"\"Given a xarray data object and label based indexers, return a mapping\n    of label indexers with only dimension names as keys.\n\n    It groups multiple level indexers given on a multi-index dimension\n    into a single, dictionary indexer for that dimension (Raise a ValueError\n    if it is not possible).\n    \"\"\"\n    invalid = [k for k in indexers\n               if k not in data_obj.dims and k not in data_obj._level_coords]\n    if invalid:\n        raise ValueError(\"dimensions or multi-index levels %r do not exist\"\n                         % invalid)\n\n    level_indexers = defaultdict(dict)\n    dim_indexers = {}\n    for key, label in indexers.items():\n        dim, = data_obj[key].dims\n        if key != dim:\n            # assume here multi-index level indexer\n            level_indexers[dim][key] = label\n        else:\n            dim_indexers[key] = label\n\n    for dim, level_labels in level_indexers.items():\n        if dim_indexers.get(dim, False):\n            raise ValueError(\"cannot combine multi-index level indexers \"\n                             \"with an indexer for dimension %s\" % dim)\n        dim_indexers[dim] = level_labels\n\n    return dim_indexers"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive an xarray data object and label based indexers return a mapping of updated pandas index objects.", "response": "def remap_label_indexers(data_obj, indexers, method=None, tolerance=None):\n    \"\"\"Given an xarray data object and label based indexers, return a mapping\n    of equivalent location based indexers. Also return a mapping of updated\n    pandas index objects (in case of multi-index level drop).\n    \"\"\"\n    if method is not None and not isinstance(method, str):\n        raise TypeError('``method`` must be a string')\n\n    pos_indexers = {}\n    new_indexes = {}\n\n    dim_indexers = get_dim_indexers(data_obj, indexers)\n    for dim, label in dim_indexers.items():\n        try:\n            index = data_obj.indexes[dim]\n        except KeyError:\n            # no index for this dimension: reuse the provided labels\n            if method is not None or tolerance is not None:\n                raise ValueError('cannot supply ``method`` or ``tolerance`` '\n                                 'when the indexed dimension does not have '\n                                 'an associated coordinate.')\n            pos_indexers[dim] = label\n        else:\n            idxr, new_idx = convert_label_indexer(index, label,\n                                                  dim, method, tolerance)\n            pos_indexers[dim] = idxr\n            if new_idx is not None:\n                new_indexes[dim] = new_idx\n\n    return pos_indexers, new_indexes"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a slice and the size of the dimension to which it will be applied return a new slice equivalent to applying the slices sequentially", "response": "def slice_slice(old_slice, applied_slice, size):\n    \"\"\"Given a slice and the size of the dimension to which it will be applied,\n    index it with another slice to return a new slice equivalent to applying\n    the slices sequentially\n    \"\"\"\n    step = (old_slice.step or 1) * (applied_slice.step or 1)\n\n    # For now, use the hack of turning old_slice into an ndarray to reconstruct\n    # the slice start and stop. This is not entirely ideal, but it is still\n    # definitely better than leaving the indexer as an array.\n    items = _expand_slice(old_slice, size)[applied_slice]\n    if len(items) > 0:\n        start = items[0]\n        stop = items[-1] + int(np.sign(step))\n        if stop < 0:\n            stop = None\n    else:\n        start = 0\n        stop = 0\n    return slice(start, stop, step)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new object that can be indexed by the given array.", "response": "def as_indexable(array):\n    \"\"\"\n    This function always returns a ExplicitlyIndexed subclass,\n    so that the vectorized indexing is always possible with the returned\n    object.\n    \"\"\"\n    if isinstance(array, ExplicitlyIndexed):\n        return array\n    if isinstance(array, np.ndarray):\n        return NumpyIndexingAdapter(array)\n    if isinstance(array, pd.Index):\n        return PandasIndexAdapter(array)\n    if isinstance(array, dask_array_type):\n        return DaskIndexingAdapter(array)\n    raise TypeError('Invalid array type: {}'.format(type(array)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts an OuterIndexer into a VectorizedIndexer.", "response": "def _outer_to_vectorized_indexer(key, shape):\n    \"\"\"Convert an OuterIndexer into an vectorized indexer.\n\n    Parameters\n    ----------\n    key : Outer/Basic Indexer\n        An indexer to convert.\n    shape : tuple\n        Shape of the array subject to the indexing.\n\n    Returns\n    -------\n    VectorizedIndexer\n        Tuple suitable for use to index a NumPy array with vectorized indexing.\n        Each element is an array: broadcasting them together gives the shape\n        of the result.\n    \"\"\"\n    key = key.tuple\n\n    n_dim = len([k for k in key if not isinstance(k, integer_types)])\n    i_dim = 0\n    new_key = []\n    for k, size in zip(key, shape):\n        if isinstance(k, integer_types):\n            new_key.append(np.array(k).reshape((1,) * n_dim))\n        else:  # np.ndarray or slice\n            if isinstance(k, slice):\n                k = np.arange(*k.indices(size))\n            assert k.dtype.kind in {'i', 'u'}\n            shape = [(1,) * i_dim + (k.size, ) +\n                     (1,) * (n_dim - i_dim - 1)]\n            new_key.append(k.reshape(*shape))\n            i_dim += 1\n    return VectorizedIndexer(tuple(new_key))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _outer_to_numpy_indexer(key, shape):\n    if len([k for k in key.tuple if not isinstance(k, slice)]) <= 1:\n        # If there is only one vector and all others are slice,\n        # it can be safely used in mixed basic/advanced indexing.\n        # Boolean index should already be converted to integer array.\n        return key.tuple\n    else:\n        return _outer_to_vectorized_indexer(key, shape).tuple", "response": "Convert an OuterIndexer into a NumPy indexer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncombine two indexers. Parameters ---------- old_key: ExplicitIndexer The first indexer for the original array shape: tuple of ints Shape of the original array to be indexed by old_key new_key: The second indexer for indexing original[old_key]", "response": "def _combine_indexers(old_key, shape, new_key):\n    \"\"\" Combine two indexers.\n\n    Parameters\n    ----------\n    old_key: ExplicitIndexer\n        The first indexer for the original array\n    shape: tuple of ints\n        Shape of the original array to be indexed by old_key\n    new_key:\n        The second indexer for indexing original[old_key]\n    \"\"\"\n    if not isinstance(old_key, VectorizedIndexer):\n        old_key = _outer_to_vectorized_indexer(old_key, shape)\n    if len(old_key.tuple) == 0:\n        return new_key\n\n    new_shape = np.broadcast(*old_key.tuple).shape\n    if isinstance(new_key, VectorizedIndexer):\n        new_key = _arrayize_vectorized_indexer(new_key, new_shape)\n    else:\n        new_key = _outer_to_vectorized_indexer(new_key, new_shape)\n\n    return VectorizedIndexer(tuple(o[new_key.tuple] for o in\n                                   np.broadcast_arrays(*old_key.tuple)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsupport explicit indexing by delegating to a raw indexing method.", "response": "def explicit_indexing_adapter(\n        key, shape, indexing_support, raw_indexing_method):\n    \"\"\"Support explicit indexing by delegating to a raw indexing method.\n\n    Outer and/or vectorized indexers are supported by indexing a second time\n    with a NumPy array.\n\n    Parameters\n    ----------\n    key : ExplicitIndexer\n        Explicit indexing object.\n    shape : Tuple[int, ...]\n        Shape of the indexed array.\n    indexing_support : IndexingSupport enum\n        Form of indexing supported by raw_indexing_method.\n    raw_indexing_method: callable\n        Function (like ndarray.__getitem__) that when called with indexing key\n        in the form of a tuple returns an indexed array.\n\n    Returns\n    -------\n    Indexing result, in the form of a duck numpy-array.\n    \"\"\"\n    raw_key, numpy_indices = decompose_indexer(key, shape, indexing_support)\n    result = raw_indexing_method(raw_key.tuple)\n    if numpy_indices.tuple:\n        # index the loaded np.ndarray\n        result = NumpyIndexingAdapter(np.asarray(result))[numpy_indices]\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _decompose_slice(key, size):\n    start, stop, step = key.indices(size)\n    if step > 0:\n        # If key already has a positive step, use it as is in the backend\n        return key, slice(None)\n    else:\n        # determine stop precisely for step > 1 case\n        # e.g. [98:2:-2] -> [98:3:-2]\n        stop = start + int((stop - start - 1) / step) * step + 1\n        start, stop = stop + 1, start + 1\n        return slice(start, stop, -step), slice(None, None, -1)", "response": "decompose a slice to successive two slices."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _decompose_vectorized_indexer(indexer, shape, indexing_support):\n    assert isinstance(indexer, VectorizedIndexer)\n\n    if indexing_support is IndexingSupport.VECTORIZED:\n        return indexer, BasicIndexer(())\n\n    backend_indexer = []\n    np_indexer = []\n    # convert negative indices\n    indexer = [np.where(k < 0, k + s, k) if isinstance(k, np.ndarray) else k\n               for k, s in zip(indexer.tuple, shape)]\n\n    for k, s in zip(indexer, shape):\n        if isinstance(k, slice):\n            # If it is a slice, then we will slice it as-is\n            # (but make its step positive) in the backend,\n            # and then use all of it (slice(None)) for the in-memory portion.\n            bk_slice, np_slice = _decompose_slice(k, s)\n            backend_indexer.append(bk_slice)\n            np_indexer.append(np_slice)\n        else:\n            # If it is a (multidimensional) np.ndarray, just pickup the used\n            # keys without duplication and store them as a 1d-np.ndarray.\n            oind, vind = np.unique(k, return_inverse=True)\n            backend_indexer.append(oind)\n            np_indexer.append(vind.reshape(*k.shape))\n\n    backend_indexer = OuterIndexer(tuple(backend_indexer))\n    np_indexer = VectorizedIndexer(tuple(np_indexer))\n\n    if indexing_support is IndexingSupport.OUTER:\n        return backend_indexer, np_indexer\n\n    # If the backend does not support outer indexing,\n    # backend_indexer (OuterIndexer) is also decomposed.\n    backend_indexer, np_indexer1 = _decompose_outer_indexer(\n        backend_indexer, shape, indexing_support)\n    np_indexer = _combine_indexers(np_indexer1, shape, np_indexer)\n    return backend_indexer, np_indexer", "response": "Decomposes vectorized indexer to the successive two indexers where the first one is used to index the backend arrays while the second one is used to index loaded on - memory np. ndarray."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecompose an outer indexer to the successive two indexers where the first one is used to index backend arrays while the second one is used to index backend arrays while the second one is used to index backend arrays.", "response": "def _decompose_outer_indexer(indexer, shape, indexing_support):\n    \"\"\"\n    Decompose outer indexer to the successive two indexers, where the\n    first indexer will be used to index backend arrays, while the second one\n    is used to index the loaded on-memory np.ndarray.\n\n    Parameters\n    ----------\n    indexer: VectorizedIndexer\n    indexing_support: One of the entries of IndexingSupport\n\n    Returns\n    -------\n    backend_indexer: OuterIndexer or BasicIndexer\n    np_indexers: an ExplicitIndexer (OuterIndexer / BasicIndexer)\n\n    Notes\n    -----\n    This function is used to realize the vectorized indexing for the backend\n    arrays that only support basic or outer indexing.\n\n    As an example, let us consider to index a few elements from a backend array\n    with a orthogonal indexer ([0, 3, 1], [2, 3, 2]).\n    Even if the backend array only supports basic indexing, it is more\n    efficient to load a subslice of the array than loading the entire array,\n\n    >>> backend_indexer = BasicIndexer(slice(0, 3), slice(2, 3))\n    >>> array = array[backend_indexer]  # load subslice of the array\n    >>> np_indexer = OuterIndexer([0, 2, 1], [0, 1, 0])\n    >>> array[np_indexer]  # outer indexing for on-memory np.ndarray.\n    \"\"\"\n    if indexing_support == IndexingSupport.VECTORIZED:\n        return indexer, BasicIndexer(())\n    assert isinstance(indexer, (OuterIndexer, BasicIndexer))\n\n    backend_indexer = []\n    np_indexer = []\n    # make indexer positive\n    pos_indexer = []\n    for k, s in zip(indexer.tuple, shape):\n        if isinstance(k, np.ndarray):\n            pos_indexer.append(np.where(k < 0, k + s, k))\n        elif isinstance(k, integer_types) and k < 0:\n            pos_indexer.append(k + s)\n        else:\n            pos_indexer.append(k)\n    indexer = pos_indexer\n\n    if indexing_support is IndexingSupport.OUTER_1VECTOR:\n        # some backends such as h5py supports only 1 vector in indexers\n        # We choose the most efficient axis\n        gains = [(np.max(k) - np.min(k) + 1.0) / len(np.unique(k))\n                 if isinstance(k, np.ndarray) else 0 for k in indexer]\n        array_index = np.argmax(np.array(gains)) if len(gains) > 0 else None\n\n        for i, (k, s) in enumerate(zip(indexer, shape)):\n            if isinstance(k, np.ndarray) and i != array_index:\n                # np.ndarray key is converted to slice that covers the entire\n                # entries of this key.\n                backend_indexer.append(slice(np.min(k), np.max(k) + 1))\n                np_indexer.append(k - np.min(k))\n            elif isinstance(k, np.ndarray):\n                # Remove duplicates and sort them in the increasing order\n                pkey, ekey = np.unique(k, return_inverse=True)\n                backend_indexer.append(pkey)\n                np_indexer.append(ekey)\n            elif isinstance(k, integer_types):\n                backend_indexer.append(k)\n            else:  # slice:  convert positive step slice for backend\n                bk_slice, np_slice = _decompose_slice(k, s)\n                backend_indexer.append(bk_slice)\n                np_indexer.append(np_slice)\n\n        return (OuterIndexer(tuple(backend_indexer)),\n                OuterIndexer(tuple(np_indexer)))\n\n    if indexing_support == IndexingSupport.OUTER:\n        for k, s in zip(indexer, shape):\n            if isinstance(k, slice):\n                # slice:  convert positive step slice for backend\n                bk_slice, np_slice = _decompose_slice(k, s)\n                backend_indexer.append(bk_slice)\n                np_indexer.append(np_slice)\n            elif isinstance(k, integer_types):\n                backend_indexer.append(k)\n            elif isinstance(k, np.ndarray) and (np.diff(k) >= 0).all():\n                backend_indexer.append(k)\n                np_indexer.append(slice(None))\n            else:\n                # Remove duplicates and sort them in the increasing order\n                oind, vind = np.unique(k, return_inverse=True)\n                backend_indexer.append(oind)\n                np_indexer.append(vind.reshape(*k.shape))\n\n        return (OuterIndexer(tuple(backend_indexer)),\n                OuterIndexer(tuple(np_indexer)))\n\n    # basic indexer\n    assert indexing_support == IndexingSupport.BASIC\n\n    for k, s in zip(indexer, shape):\n        if isinstance(k, np.ndarray):\n            # np.ndarray key is converted to slice that covers the entire\n            # entries of this key.\n            backend_indexer.append(slice(np.min(k), np.max(k) + 1))\n            np_indexer.append(k - np.min(k))\n        elif isinstance(k, integer_types):\n            backend_indexer.append(k)\n        else:  # slice:  convert positive step slice for backend\n            bk_slice, np_slice = _decompose_slice(k, s)\n            backend_indexer.append(bk_slice)\n            np_indexer.append(np_slice)\n\n    return (BasicIndexer(tuple(backend_indexer)),\n            OuterIndexer(tuple(np_indexer)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _arrayize_vectorized_indexer(indexer, shape):\n    slices = [v for v in indexer.tuple if isinstance(v, slice)]\n    if len(slices) == 0:\n        return indexer\n\n    arrays = [v for v in indexer.tuple if isinstance(v, np.ndarray)]\n    n_dim = arrays[0].ndim if len(arrays) > 0 else 0\n    i_dim = 0\n    new_key = []\n    for v, size in zip(indexer.tuple, shape):\n        if isinstance(v, np.ndarray):\n            new_key.append(np.reshape(v, v.shape + (1, ) * len(slices)))\n        else:  # slice\n            shape = ((1,) * (n_dim + i_dim) + (-1,) +\n                     (1,) * (len(slices) - i_dim - 1))\n            new_key.append(np.arange(*v.indices(size)).reshape(shape))\n            i_dim += 1\n    return VectorizedIndexer(tuple(new_key))", "response": "Return an identical vindex but slices are replaced by arrays"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _dask_array_with_chunks_hint(array, chunks):\n    import dask.array as da\n    if len(chunks) < array.ndim:\n        raise ValueError('not enough chunks in hint')\n    new_chunks = []\n    for chunk, size in zip(chunks, array.shape):\n        new_chunks.append(chunk if size > 1 else (1,))\n    return da.from_array(array, new_chunks)", "response": "Create a dask array using the chunks hint for dimensions of size > 1."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_mask(indexer, shape, chunks_hint=None):\n    if isinstance(indexer, OuterIndexer):\n        key = _outer_to_vectorized_indexer(indexer, shape).tuple\n        assert not any(isinstance(k, slice) for k in key)\n        mask = _masked_result_drop_slice(key, chunks_hint)\n\n    elif isinstance(indexer, VectorizedIndexer):\n        key = indexer.tuple\n        base_mask = _masked_result_drop_slice(key, chunks_hint)\n        slice_shape = tuple(np.arange(*k.indices(size)).size\n                            for k, size in zip(key, shape)\n                            if isinstance(k, slice))\n        expanded_mask = base_mask[\n            (Ellipsis,) + (np.newaxis,) * len(slice_shape)]\n        mask = duck_array_ops.broadcast_to(\n            expanded_mask, base_mask.shape + slice_shape)\n\n    elif isinstance(indexer, BasicIndexer):\n        mask = any(k == -1 for k in indexer.tuple)\n\n    else:\n        raise TypeError('unexpected key type: {}'.format(type(indexer)))\n\n    return mask", "response": "Create a mask for indexing with a fill - value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting masked indices in a flat array to nearest unmasked index.", "response": "def _posify_mask_subindexer(index):\n    \"\"\"Convert masked indices in a flat array to the nearest unmasked index.\n\n    Parameters\n    ----------\n    index : np.ndarray\n        One dimensional ndarray with dtype=int.\n\n    Returns\n    -------\n    np.ndarray\n        One dimensional ndarray with all values equal to -1 replaced by an\n        adjacent non-masked element.\n    \"\"\"\n    masked = index == -1\n    unmasked_locs = np.flatnonzero(~masked)\n    if not unmasked_locs.size:\n        # indexing unmasked_locs is invalid\n        return np.zeros_like(index)\n    masked_locs = np.flatnonzero(masked)\n    prev_value = np.maximum(0, np.searchsorted(unmasked_locs, masked_locs) - 1)\n    new_index = index.copy()\n    new_index[masked_locs] = index[unmasked_locs[prev_value]]\n    return new_index"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting masked values in an indexer to nearest unmasked values.", "response": "def posify_mask_indexer(indexer):\n    \"\"\"Convert masked values (-1) in an indexer to nearest unmasked values.\n\n    This routine is useful for dask, where it can be much faster to index\n    adjacent points than arbitrary points from the end of an array.\n\n    Parameters\n    ----------\n    indexer : ExplicitIndexer\n        Input indexer.\n\n    Returns\n    -------\n    ExplicitIndexer\n        Same type of input, with all values in ndarray keys equal to -1\n        replaced by an adjacent non-masked element.\n    \"\"\"\n    key = tuple(_posify_mask_subindexer(k.ravel()).reshape(k.shape)\n                if isinstance(k, np.ndarray) else k\n                for k in indexer.tuple)\n    return type(indexer)(key)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nimporting seaborn and handle deprecation of apionly module", "response": "def import_seaborn():\n    '''import seaborn and handle deprecation of apionly module'''\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        try:\n            import seaborn.apionly as sns\n            if (w and issubclass(w[-1].category, UserWarning) and\n                    (\"seaborn.apionly module\" in str(w[-1].message))):\n                raise ImportError\n        except ImportError:\n            import seaborn as sns\n        finally:\n            warnings.resetwarnings()\n    return sns"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a discrete colormap and normalization of the data.", "response": "def _build_discrete_cmap(cmap, levels, extend, filled):\n    \"\"\"\n    Build a discrete colormap and normalization of the data.\n    \"\"\"\n    import matplotlib as mpl\n\n    if not filled:\n        # non-filled contour plots\n        extend = 'max'\n\n    if extend == 'both':\n        ext_n = 2\n    elif extend in ['min', 'max']:\n        ext_n = 1\n    else:\n        ext_n = 0\n\n    n_colors = len(levels) + ext_n - 1\n    pal = _color_palette(cmap, n_colors)\n\n    new_cmap, cnorm = mpl.colors.from_levels_and_colors(\n        levels, pal, extend=extend)\n    # copy the old cmap name, for easier testing\n    new_cmap.name = getattr(cmap, 'name', cmap)\n\n    return new_cmap, cnorm"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _determine_cmap_params(plot_data, vmin=None, vmax=None, cmap=None,\n                           center=None, robust=False, extend=None,\n                           levels=None, filled=True, norm=None):\n    \"\"\"\n    Use some heuristics to set good defaults for colorbar and range.\n\n    Parameters\n    ==========\n    plot_data: Numpy array\n        Doesn't handle xarray objects\n\n    Returns\n    =======\n    cmap_params : dict\n        Use depends on the type of the plotting function\n    \"\"\"\n    import matplotlib as mpl\n\n    calc_data = np.ravel(plot_data[np.isfinite(plot_data)])\n\n    # Handle all-NaN input data gracefully\n    if calc_data.size == 0:\n        # Arbitrary default for when all values are NaN\n        calc_data = np.array(0.0)\n\n    # Setting center=False prevents a divergent cmap\n    possibly_divergent = center is not False\n\n    # Set center to 0 so math below makes sense but remember its state\n    center_is_none = False\n    if center is None:\n        center = 0\n        center_is_none = True\n\n    # Setting both vmin and vmax prevents a divergent cmap\n    if (vmin is not None) and (vmax is not None):\n        possibly_divergent = False\n\n    # Setting vmin or vmax implies linspaced levels\n    user_minmax = (vmin is not None) or (vmax is not None)\n\n    # vlim might be computed below\n    vlim = None\n\n    # save state; needed later\n    vmin_was_none = vmin is None\n    vmax_was_none = vmax is None\n\n    if vmin is None:\n        if robust:\n            vmin = np.percentile(calc_data, ROBUST_PERCENTILE)\n        else:\n            vmin = calc_data.min()\n    elif possibly_divergent:\n        vlim = abs(vmin - center)\n\n    if vmax is None:\n        if robust:\n            vmax = np.percentile(calc_data, 100 - ROBUST_PERCENTILE)\n        else:\n            vmax = calc_data.max()\n    elif possibly_divergent:\n        vlim = abs(vmax - center)\n\n    if possibly_divergent:\n        # kwargs not specific about divergent or not: infer defaults from data\n        divergent = ((vmin < 0) and (vmax > 0)) or not center_is_none\n    else:\n        divergent = False\n\n    # A divergent map should be symmetric around the center value\n    if divergent:\n        if vlim is None:\n            vlim = max(abs(vmin - center), abs(vmax - center))\n        vmin, vmax = -vlim, vlim\n\n    # Now add in the centering value and set the limits\n    vmin += center\n    vmax += center\n\n    # now check norm and harmonize with vmin, vmax\n    if norm is not None:\n        if norm.vmin is None:\n            norm.vmin = vmin\n        else:\n            if not vmin_was_none and vmin != norm.vmin:\n                raise ValueError('Cannot supply vmin and a norm'\n                                 + ' with a different vmin.')\n            vmin = norm.vmin\n\n        if norm.vmax is None:\n            norm.vmax = vmax\n        else:\n            if not vmax_was_none and vmax != norm.vmax:\n                raise ValueError('Cannot supply vmax and a norm'\n                                 + ' with a different vmax.')\n            vmax = norm.vmax\n\n    # if BoundaryNorm, then set levels\n    if isinstance(norm, mpl.colors.BoundaryNorm):\n        levels = norm.boundaries\n\n    # Choose default colormaps if not provided\n    if cmap is None:\n        if divergent:\n            cmap = OPTIONS['cmap_divergent']\n        else:\n            cmap = OPTIONS['cmap_sequential']\n\n    # Handle discrete levels\n    if levels is not None and norm is None:\n        if is_scalar(levels):\n            if user_minmax:\n                levels = np.linspace(vmin, vmax, levels)\n            elif levels == 1:\n                levels = np.asarray([(vmin + vmax) / 2])\n            else:\n                # N in MaxNLocator refers to bins, not ticks\n                ticker = mpl.ticker.MaxNLocator(levels - 1)\n                levels = ticker.tick_values(vmin, vmax)\n        vmin, vmax = levels[0], levels[-1]\n\n    if extend is None:\n        extend = _determine_extend(calc_data, vmin, vmax)\n\n    if levels is not None or isinstance(norm, mpl.colors.BoundaryNorm):\n        cmap, newnorm = _build_discrete_cmap(cmap, levels, extend, filled)\n        norm = newnorm if norm is None else norm\n\n    return dict(vmin=vmin, vmax=vmax, cmap=cmap, extend=extend,\n                levels=levels, norm=norm)", "response": "Determine the parameters of a colorbar and range of a colorbar."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _infer_xy_labels_3d(darray, x, y, rgb):\n    assert rgb is None or rgb != x\n    assert rgb is None or rgb != y\n    # Start by detecting and reporting invalid combinations of arguments\n    assert darray.ndim == 3\n    not_none = [a for a in (x, y, rgb) if a is not None]\n    if len(set(not_none)) < len(not_none):\n        raise ValueError(\n            'Dimension names must be None or unique strings, but imshow was '\n            'passed x=%r, y=%r, and rgb=%r.' % (x, y, rgb))\n    for label in not_none:\n        if label not in darray.dims:\n            raise ValueError('%r is not a dimension' % (label,))\n\n    # Then calculate rgb dimension if certain and check validity\n    could_be_color = [label for label in darray.dims\n                      if darray[label].size in (3, 4) and label not in (x, y)]\n    if rgb is None and not could_be_color:\n        raise ValueError(\n            'A 3-dimensional array was passed to imshow(), but there is no '\n            'dimension that could be color.  At least one dimension must be '\n            'of size 3 (RGB) or 4 (RGBA), and not given as x or y.')\n    if rgb is None and len(could_be_color) == 1:\n        rgb = could_be_color[0]\n    if rgb is not None and darray[rgb].size not in (3, 4):\n        raise ValueError('Cannot interpret dim %r of size %s as RGB or RGBA.'\n                         % (rgb, darray[rgb].size))\n\n    # If rgb dimension is still unknown, there must be two or three dimensions\n    # in could_be_color.  We therefore warn, and use a heuristic to break ties.\n    if rgb is None:\n        assert len(could_be_color) in (2, 3)\n        rgb = could_be_color[-1]\n        warnings.warn(\n            'Several dimensions of this array could be colors.  Xarray '\n            'will use the last possible dimension (%r) to match '\n            'matplotlib.pyplot.imshow.  You can pass names of x, y, '\n            'and/or rgb dimensions to override this guess.' % rgb)\n    assert rgb is not None\n\n    # Finally, we pick out the red slice and delegate to the 2D version:\n    return _infer_xy_labels(darray.isel(**{rgb: 0}), x, y)", "response": "Infer x and y labels for showing RGB images."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninfers x and y labels for a 2D data array.", "response": "def _infer_xy_labels(darray, x, y, imshow=False, rgb=None):\n    \"\"\"\n    Determine x and y labels. For use in _plot2d\n\n    darray must be a 2 dimensional data array, or 3d for imshow only.\n    \"\"\"\n    assert x is None or x != y\n    if imshow and darray.ndim == 3:\n        return _infer_xy_labels_3d(darray, x, y, rgb)\n\n    if x is None and y is None:\n        if darray.ndim != 2:\n            raise ValueError('DataArray must be 2d')\n        y, x = darray.dims\n    elif x is None:\n        if y not in darray.dims and y not in darray.coords:\n            raise ValueError('y must be a dimension name if x is not supplied')\n        x = darray.dims[0] if y == darray.dims[1] else darray.dims[1]\n    elif y is None:\n        if x not in darray.dims and x not in darray.coords:\n            raise ValueError('x must be a dimension name if y is not supplied')\n        y = darray.dims[0] if x == darray.dims[1] else darray.dims[1]\n    elif any(k not in darray.coords and k not in darray.dims for k in (x, y)):\n        raise ValueError('x and y must be coordinate variables')\n    return x, y"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef label_from_attrs(da, extra=''):\n    ''' Makes informative labels if variable metadata (attrs) follows\n        CF conventions. '''\n\n    if da.attrs.get('long_name'):\n        name = da.attrs['long_name']\n    elif da.attrs.get('standard_name'):\n        name = da.attrs['standard_name']\n    elif da.name is not None:\n        name = da.name\n    else:\n        name = ''\n\n    if da.attrs.get('units'):\n        units = ' [{}]'.format(da.attrs['units'])\n    else:\n        units = ''\n\n    return '\\n'.join(textwrap.wrap(name + extra + units, 30))", "response": "Makes informative labels if variable metadata ( attrs ) follows\n        CF conventions."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _interval_to_bound_points(array):\n\n    array_boundaries = np.array([x.left for x in array])\n    array_boundaries = np.concatenate(\n        (array_boundaries, np.array([array[-1].right])))\n\n    return array_boundaries", "response": "Helper function which returns an array containing the Intervals boundaries."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _resolve_intervals_2dplot(val, func_name):\n    label_extra = ''\n    if _valid_other_type(val, [pd.Interval]):\n        if func_name == 'pcolormesh':\n            val = _interval_to_bound_points(val)\n        else:\n            val = _interval_to_mid_points(val)\n            label_extra = '_center'\n\n    return val, label_extra", "response": "Helper function to resolve the values of a coordinate array containing pd. Interval with their mid - points."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if x has a type from types?", "response": "def _valid_other_type(x, types):\n    \"\"\"\n    Do all elements of x have a type from types?\n    \"\"\"\n    return all(any(isinstance(el, t) for t in types) for el in np.ravel(x))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _valid_numpy_subdtype(x, numpy_types):\n    # If any of the types given in numpy_types is understood as numpy.generic,\n    # all possible x will be considered valid.  This is probably unwanted.\n    for t in numpy_types:\n        assert not np.issubdtype(np.generic, t)\n\n    return any(np.issubdtype(x.dtype, t) for t in numpy_types)", "response": "Check if x is a subdtype of numpy_types."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ensure_plottable(*args):\n    numpy_types = [np.floating, np.integer, np.timedelta64, np.datetime64]\n    other_types = [datetime]\n    try:\n        import cftime\n        cftime_datetime = [cftime.datetime]\n    except ImportError:\n        cftime_datetime = []\n    other_types = other_types + cftime_datetime\n    for x in args:\n        if not (_valid_numpy_subdtype(np.array(x), numpy_types)\n                or _valid_other_type(np.array(x), other_types)):\n            raise TypeError('Plotting requires coordinates to be numeric '\n                            'or dates of type np.datetime64, '\n                            'datetime.datetime, cftime.datetime or '\n                            'pd.Interval.')\n        if (_valid_other_type(np.array(x), cftime_datetime)\n                and not nc_time_axis_available):\n            raise ImportError('Plotting of arrays of cftime.datetime '\n                              'objects or arrays indexed by '\n                              'cftime.datetime objects requires the '\n                              'optional `nc-time-axis` (v1.2.0 or later) '\n                              'package.')", "response": "Raise exception if any of the arguments are not plottable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating axes with provided parameters.", "response": "def _update_axes(ax, xincrease, yincrease,\n                 xscale=None, yscale=None,\n                 xticks=None, yticks=None,\n                 xlim=None, ylim=None):\n    \"\"\"\n    Update axes with provided parameters\n    \"\"\"\n    if xincrease is None:\n        pass\n    elif xincrease and ax.xaxis_inverted():\n        ax.invert_xaxis()\n    elif not xincrease and not ax.xaxis_inverted():\n        ax.invert_xaxis()\n\n    if yincrease is None:\n        pass\n    elif yincrease and ax.yaxis_inverted():\n        ax.invert_yaxis()\n    elif not yincrease and not ax.yaxis_inverted():\n        ax.invert_yaxis()\n\n    # The default xscale, yscale needs to be None.\n    # If we set a scale it resets the axes formatters,\n    # This means that set_xscale('linear') on a datetime axis\n    # will remove the date labels. So only set the scale when explicitly\n    # asked to. https://github.com/matplotlib/matplotlib/issues/8740\n    if xscale is not None:\n        ax.set_xscale(xscale)\n    if yscale is not None:\n        ax.set_yscale(yscale)\n\n    if xticks is not None:\n        ax.set_xticks(xticks)\n    if yticks is not None:\n        ax.set_yticks(yticks)\n\n    if xlim is not None:\n        ax.set_xlim(xlim)\n    if ylim is not None:\n        ax.set_ylim(ylim)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _is_monotonic(coord, axis=0):\n    if coord.shape[axis] < 3:\n        return True\n    else:\n        n = coord.shape[axis]\n        delta_pos = (coord.take(np.arange(1, n), axis=axis) >=\n                     coord.take(np.arange(0, n - 1), axis=axis))\n        delta_neg = (coord.take(np.arange(1, n), axis=axis) <=\n                     coord.take(np.arange(0, n - 1), axis=axis))\n        return np.all(delta_pos) or np.all(delta_neg)", "response": "Return True if the given array is monotonic."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _infer_interval_breaks(coord, axis=0, check_monotonic=False):\n    coord = np.asarray(coord)\n\n    if check_monotonic and not _is_monotonic(coord, axis=axis):\n        raise ValueError(\"The input coordinate is not sorted in increasing \"\n                         \"order along axis %d. This can lead to unexpected \"\n                         \"results. Consider calling the `sortby` method on \"\n                         \"the input DataArray. To plot data with categorical \"\n                         \"axes, consider using the `heatmap` function from \"\n                         \"the `seaborn` statistical plotting library.\" % axis)\n\n    deltas = 0.5 * np.diff(coord, axis=axis)\n    if deltas.size == 0:\n        deltas = np.array(0.0)\n    first = np.take(coord, [0], axis=axis) - np.take(deltas, [0], axis=axis)\n    last = np.take(coord, [-1], axis=axis) + np.take(deltas, [-1], axis=axis)\n    trim_last = tuple(slice(None, -1) if n == axis else slice(None)\n                      for n in range(coord.ndim))\n    return np.concatenate([first, coord[trim_last] + deltas, last], axis=axis)", "response": "Infer the interval breaks of a given coordinate."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _process_cmap_cbar_kwargs(func, kwargs, data):\n\n    cmap = kwargs.pop('cmap', None)\n    colors = kwargs.pop('colors', None)\n\n    cbar_kwargs = kwargs.pop('cbar_kwargs', {})\n    cbar_kwargs = {} if cbar_kwargs is None else dict(cbar_kwargs)\n\n    levels = kwargs.pop('levels', None)\n    if 'contour' in func.__name__ and levels is None:\n        levels = 7  # this is the matplotlib default\n\n    # colors is mutually exclusive with cmap\n    if cmap and colors:\n        raise ValueError(\"Can't specify both cmap and colors.\")\n\n    # colors is only valid when levels is supplied or the plot is of type\n    # contour or contourf\n    if colors and (('contour' not in func.__name__) and (not levels)):\n        raise ValueError(\"Can only specify colors with contour or levels\")\n\n    # we should not be getting a list of colors in cmap anymore\n    # is there a better way to do this test?\n    if isinstance(cmap, (list, tuple)):\n        warnings.warn(\"Specifying a list of colors in cmap is deprecated. \"\n                      \"Use colors keyword instead.\",\n                      DeprecationWarning, stacklevel=3)\n\n    cmap_kwargs = {'plot_data': data,\n                   'levels': levels,\n                   'cmap': colors if colors else cmap,\n                   'filled': func.__name__ != 'contour'}\n\n    cmap_args = getfullargspec(_determine_cmap_params).args\n    cmap_kwargs.update((a, kwargs[a]) for a in cmap_args if a in kwargs)\n    cmap_params = _determine_cmap_params(**cmap_kwargs)\n\n    return cmap_params, cbar_kwargs", "response": "Process kwargs that need to be parsed from kwargs and return cmap_params cbar_kwargs"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots a DataArray using matplotlib. pyplot.", "response": "def plot(darray, row=None, col=None, col_wrap=None, ax=None, hue=None,\n         rtol=0.01, subplot_kws=None, **kwargs):\n    \"\"\"\n    Default plot of DataArray using matplotlib.pyplot.\n\n    Calls xarray plotting function based on the dimensions of\n    darray.squeeze()\n\n    =============== ===========================\n    Dimensions      Plotting function\n    --------------- ---------------------------\n    1               :py:func:`xarray.plot.line`\n    2               :py:func:`xarray.plot.pcolormesh`\n    Anything else   :py:func:`xarray.plot.hist`\n    =============== ===========================\n\n    Parameters\n    ----------\n    darray : DataArray\n    row : string, optional\n        If passed, make row faceted plots on this dimension name\n    col : string, optional\n        If passed, make column faceted plots on this dimension name\n    hue : string, optional\n        If passed, make faceted line plots with hue on this dimension name\n    col_wrap : integer, optional\n        Use together with ``col`` to wrap faceted plots\n    ax : matplotlib axes, optional\n        If None, uses the current axis. Not applicable when using facets.\n    rtol : number, optional\n        Relative tolerance used to determine if the indexes\n        are uniformly spaced. Usually a small positive number.\n    subplot_kws : dict, optional\n        Dictionary of keyword arguments for matplotlib subplots. Only applies\n        to FacetGrid plotting.\n    **kwargs : optional\n        Additional keyword arguments to matplotlib\n\n    \"\"\"\n    darray = darray.squeeze()\n\n    plot_dims = set(darray.dims)\n    plot_dims.discard(row)\n    plot_dims.discard(col)\n    plot_dims.discard(hue)\n\n    ndims = len(plot_dims)\n\n    error_msg = ('Only 1d and 2d plots are supported for facets in xarray. '\n                 'See the package `Seaborn` for more options.')\n\n    if ndims in [1, 2]:\n        if row or col:\n            kwargs['row'] = row\n            kwargs['col'] = col\n            kwargs['col_wrap'] = col_wrap\n            kwargs['subplot_kws'] = subplot_kws\n        if ndims == 1:\n            plotfunc = line\n            kwargs['hue'] = hue\n        elif ndims == 2:\n            if hue:\n                plotfunc = line\n                kwargs['hue'] = hue\n            else:\n                plotfunc = pcolormesh\n    else:\n        if row or col or hue:\n            raise ValueError(error_msg)\n        plotfunc = hist\n\n    kwargs['ax'] = ax\n\n    return plotfunc(darray, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfunction to plot a single line of the given DataArray index against values", "response": "def line(darray, *args, **kwargs):\n    \"\"\"\n    Line plot of DataArray index against values\n\n    Wraps :func:`matplotlib:matplotlib.pyplot.plot`\n\n    Parameters\n    ----------\n    darray : DataArray\n        Must be 1 dimensional\n    figsize : tuple, optional\n        A tuple (width, height) of the figure in inches.\n        Mutually exclusive with ``size`` and ``ax``.\n    aspect : scalar, optional\n        Aspect ratio of plot, so that ``aspect * size`` gives the width in\n        inches. Only used if a ``size`` is provided.\n    size : scalar, optional\n        If provided, create a new figure for the plot with the given size.\n        Height (in inches) of each plot. See also: ``aspect``.\n    ax : matplotlib axes object, optional\n        Axis on which to plot this figure. By default, use the current axis.\n        Mutually exclusive with ``size`` and ``figsize``.\n    hue : string, optional\n        Dimension or coordinate for which you want multiple lines plotted.\n        If plotting against a 2D coordinate, ``hue`` must be a dimension.\n    x, y : string, optional\n        Dimensions or coordinates for x, y axis.\n        Only one of these may be specified.\n        The other coordinate plots values from the DataArray on which this\n        plot method is called.\n    xscale, yscale : 'linear', 'symlog', 'log', 'logit', optional\n        Specifies scaling for the x- and y-axes respectively\n    xticks, yticks : Specify tick locations for x- and y-axes\n    xlim, ylim : Specify x- and y-axes limits\n    xincrease : None, True, or False, optional\n        Should the values on the x axes be increasing from left to right?\n        if None, use the default for the matplotlib function.\n    yincrease : None, True, or False, optional\n        Should the values on the y axes be increasing from top to bottom?\n        if None, use the default for the matplotlib function.\n    add_legend : boolean, optional\n        Add legend with y axis coordinates (2D inputs only).\n    *args, **kwargs : optional\n        Additional arguments to matplotlib.pyplot.plot\n\n    \"\"\"\n\n    # Handle facetgrids first\n    row = kwargs.pop('row', None)\n    col = kwargs.pop('col', None)\n    if row or col:\n        allargs = locals().copy()\n        allargs.update(allargs.pop('kwargs'))\n        allargs.pop('darray')\n        return _easy_facetgrid(darray, line, kind='line', **allargs)\n\n    ndims = len(darray.dims)\n    if ndims > 2:\n        raise ValueError('Line plots are for 1- or 2-dimensional DataArrays. '\n                         'Passed DataArray has {ndims} '\n                         'dimensions'.format(ndims=ndims))\n\n    # Ensures consistency with .plot method\n    figsize = kwargs.pop('figsize', None)\n    aspect = kwargs.pop('aspect', None)\n    size = kwargs.pop('size', None)\n    ax = kwargs.pop('ax', None)\n    hue = kwargs.pop('hue', None)\n    x = kwargs.pop('x', None)\n    y = kwargs.pop('y', None)\n    xincrease = kwargs.pop('xincrease', None)  # default needs to be None\n    yincrease = kwargs.pop('yincrease', None)\n    xscale = kwargs.pop('xscale', None)  # default needs to be None\n    yscale = kwargs.pop('yscale', None)\n    xticks = kwargs.pop('xticks', None)\n    yticks = kwargs.pop('yticks', None)\n    xlim = kwargs.pop('xlim', None)\n    ylim = kwargs.pop('ylim', None)\n    add_legend = kwargs.pop('add_legend', True)\n    _labels = kwargs.pop('_labels', True)\n    if args is ():\n        args = kwargs.pop('args', ())\n\n    ax = get_axis(figsize, size, aspect, ax)\n    xplt, yplt, hueplt, xlabel, ylabel, huelabel = \\\n        _infer_line_data(darray, x, y, hue)\n\n    # Remove pd.Intervals if contained in xplt.values.\n    if _valid_other_type(xplt.values, [pd.Interval]):\n        # Is it a step plot? (see matplotlib.Axes.step)\n        if kwargs.get('linestyle', '').startswith('steps-'):\n            xplt_val, yplt_val = _interval_to_double_bound_points(xplt.values,\n                                                                  yplt.values)\n            # Remove steps-* to be sure that matplotlib is not confused\n            kwargs['linestyle'] = (kwargs['linestyle']\n                                   .replace('steps-pre', '')\n                                   .replace('steps-post', '')\n                                   .replace('steps-mid', ''))\n            if kwargs['linestyle'] == '':\n                kwargs.pop('linestyle')\n        else:\n            xplt_val = _interval_to_mid_points(xplt.values)\n            yplt_val = yplt.values\n            xlabel += '_center'\n    else:\n        xplt_val = xplt.values\n        yplt_val = yplt.values\n\n    _ensure_plottable(xplt_val, yplt_val)\n\n    primitive = ax.plot(xplt_val, yplt_val, *args, **kwargs)\n\n    if _labels:\n        if xlabel is not None:\n            ax.set_xlabel(xlabel)\n\n        if ylabel is not None:\n            ax.set_ylabel(ylabel)\n\n        ax.set_title(darray._title_for_slice())\n\n    if darray.ndim == 2 and add_legend:\n        ax.legend(handles=primitive,\n                  labels=list(hueplt.values),\n                  title=huelabel)\n\n    # Rotate dates on xlabels\n    # Do this without calling autofmt_xdate so that x-axes ticks\n    # on other subplots (if any) are not deleted.\n    # https://stackoverflow.com/questions/17430105/autofmt-xdate-deletes-x-axis-labels-of-all-subplots\n    if np.issubdtype(xplt.dtype, np.datetime64):\n        for xlabels in ax.get_xticklabels():\n            xlabels.set_rotation(30)\n            xlabels.set_ha('right')\n\n    _update_axes(ax, xincrease, yincrease, xscale, yscale,\n                 xticks, yticks, xlim, ylim)\n\n    return primitive"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef step(darray, *args, **kwargs):\n    if ('ls' in kwargs.keys()) and ('linestyle' not in kwargs.keys()):\n        kwargs['linestyle'] = kwargs.pop('ls')\n\n    where = kwargs.pop('where', 'pre')\n\n    if where not in ('pre', 'post', 'mid'):\n        raise ValueError(\"'where' argument to step must be \"\n                         \"'pre', 'post' or 'mid'\")\n\n    kwargs['linestyle'] = 'steps-' + where + kwargs.get('linestyle', '')\n\n    return line(darray, *args, **kwargs)", "response": "This function is a convenience function for plotting a single step plot of DataArray index against values\nFormula"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hist(darray, figsize=None, size=None, aspect=None, ax=None, **kwargs):\n    ax = get_axis(figsize, size, aspect, ax)\n\n    xincrease = kwargs.pop('xincrease', None)  # default needs to be None\n    yincrease = kwargs.pop('yincrease', None)\n    xscale = kwargs.pop('xscale', None)  # default needs to be None\n    yscale = kwargs.pop('yscale', None)\n    xticks = kwargs.pop('xticks', None)\n    yticks = kwargs.pop('yticks', None)\n    xlim = kwargs.pop('xlim', None)\n    ylim = kwargs.pop('ylim', None)\n\n    no_nan = np.ravel(darray.values)\n    no_nan = no_nan[pd.notnull(no_nan)]\n\n    primitive = ax.hist(no_nan, **kwargs)\n\n    ax.set_title('Histogram')\n    ax.set_xlabel(label_from_attrs(darray))\n\n    _update_axes(ax, xincrease, yincrease, xscale, yscale,\n                 xticks, yticks, xlim, ylim)\n\n    return primitive", "response": "Plots a histogram of the given DataArray."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _plot2d(plotfunc):\n    commondoc = \"\"\"\n    Parameters\n    ----------\n    darray : DataArray\n        Must be 2 dimensional, unless creating faceted plots\n    x : string, optional\n        Coordinate for x axis. If None use darray.dims[1]\n    y : string, optional\n        Coordinate for y axis. If None use darray.dims[0]\n    figsize : tuple, optional\n        A tuple (width, height) of the figure in inches.\n        Mutually exclusive with ``size`` and ``ax``.\n    aspect : scalar, optional\n        Aspect ratio of plot, so that ``aspect * size`` gives the width in\n        inches. Only used if a ``size`` is provided.\n    size : scalar, optional\n        If provided, create a new figure for the plot with the given size.\n        Height (in inches) of each plot. See also: ``aspect``.\n    ax : matplotlib axes object, optional\n        Axis on which to plot this figure. By default, use the current axis.\n        Mutually exclusive with ``size`` and ``figsize``.\n    row : string, optional\n        If passed, make row faceted plots on this dimension name\n    col : string, optional\n        If passed, make column faceted plots on this dimension name\n    col_wrap : integer, optional\n        Use together with ``col`` to wrap faceted plots\n    xscale, yscale : 'linear', 'symlog', 'log', 'logit', optional\n        Specifies scaling for the x- and y-axes respectively\n    xticks, yticks : Specify tick locations for x- and y-axes\n    xlim, ylim : Specify x- and y-axes limits\n    xincrease : None, True, or False, optional\n        Should the values on the x axes be increasing from left to right?\n        if None, use the default for the matplotlib function.\n    yincrease : None, True, or False, optional\n        Should the values on the y axes be increasing from top to bottom?\n        if None, use the default for the matplotlib function.\n    add_colorbar : Boolean, optional\n        Adds colorbar to axis\n    add_labels : Boolean, optional\n        Use xarray metadata to label axes\n    norm : ``matplotlib.colors.Normalize`` instance, optional\n        If the ``norm`` has vmin or vmax specified, the corresponding kwarg\n        must be None.\n    vmin, vmax : floats, optional\n        Values to anchor the colormap, otherwise they are inferred from the\n        data and other keyword arguments. When a diverging dataset is inferred,\n        setting one of these values will fix the other by symmetry around\n        ``center``. Setting both values prevents use of a diverging colormap.\n        If discrete levels are provided as an explicit list, both of these\n        values are ignored.\n    cmap : matplotlib colormap name or object, optional\n        The mapping from data values to color space. If not provided, this\n        will be either be ``viridis`` (if the function infers a sequential\n        dataset) or ``RdBu_r`` (if the function infers a diverging dataset).\n        When `Seaborn` is installed, ``cmap`` may also be a `seaborn`\n        color palette. If ``cmap`` is seaborn color palette and the plot type\n        is not ``contour`` or ``contourf``, ``levels`` must also be specified.\n    colors : discrete colors to plot, optional\n        A single color or a list of colors. If the plot type is not ``contour``\n        or ``contourf``, the ``levels`` argument is required.\n    center : float, optional\n        The value at which to center the colormap. Passing this value implies\n        use of a diverging colormap. Setting it to ``False`` prevents use of a\n        diverging colormap.\n    robust : bool, optional\n        If True and ``vmin`` or ``vmax`` are absent, the colormap range is\n        computed with 2nd and 98th percentiles instead of the extreme values.\n    extend : {'neither', 'both', 'min', 'max'}, optional\n        How to draw arrows extending the colorbar beyond its limits. If not\n        provided, extend is inferred from vmin, vmax and the data limits.\n    levels : int or list-like object, optional\n        Split the colormap (cmap) into discrete color intervals. If an integer\n        is provided, \"nice\" levels are chosen based on the data range: this can\n        imply that the final number of levels is not exactly the expected one.\n        Setting ``vmin`` and/or ``vmax`` with ``levels=N`` is equivalent to\n        setting ``levels=np.linspace(vmin, vmax, N)``.\n    infer_intervals : bool, optional\n        Only applies to pcolormesh. If True, the coordinate intervals are\n        passed to pcolormesh. If False, the original coordinates are used\n        (this can be useful for certain map projections). The default is to\n        always infer intervals, unless the mesh is irregular and plotted on\n        a map projection.\n    subplot_kws : dict, optional\n        Dictionary of keyword arguments for matplotlib subplots. Only applies\n        to FacetGrid plotting.\n    cbar_ax : matplotlib Axes, optional\n        Axes in which to draw the colorbar.\n    cbar_kwargs : dict, optional\n        Dictionary of keyword arguments to pass to the colorbar.\n    **kwargs : optional\n        Additional arguments to wrapped matplotlib function\n\n    Returns\n    -------\n    artist :\n        The same type of primitive artist that the wrapped matplotlib\n        function returns\n    \"\"\"\n\n    # Build on the original docstring\n    plotfunc.__doc__ = '%s\\n%s' % (plotfunc.__doc__, commondoc)\n\n    @functools.wraps(plotfunc)\n    def newplotfunc(darray, x=None, y=None, figsize=None, size=None,\n                    aspect=None, ax=None, row=None, col=None,\n                    col_wrap=None, xincrease=True, yincrease=True,\n                    add_colorbar=None, add_labels=True, vmin=None, vmax=None,\n                    cmap=None, center=None, robust=False, extend=None,\n                    levels=None, infer_intervals=None, colors=None,\n                    subplot_kws=None, cbar_ax=None, cbar_kwargs=None,\n                    xscale=None, yscale=None, xticks=None, yticks=None,\n                    xlim=None, ylim=None, norm=None, **kwargs):\n        # All 2d plots in xarray share this function signature.\n        # Method signature below should be consistent.\n\n        # Decide on a default for the colorbar before facetgrids\n        if add_colorbar is None:\n            add_colorbar = plotfunc.__name__ != 'contour'\n        imshow_rgb = (\n            plotfunc.__name__ == 'imshow' and\n            darray.ndim == (3 + (row is not None) + (col is not None)))\n        if imshow_rgb:\n            # Don't add a colorbar when showing an image with explicit colors\n            add_colorbar = False\n            # Matplotlib does not support normalising RGB data, so do it here.\n            # See eg. https://github.com/matplotlib/matplotlib/pull/10220\n            if robust or vmax is not None or vmin is not None:\n                darray = _rescale_imshow_rgb(darray, vmin, vmax, robust)\n                vmin, vmax, robust = None, None, False\n\n        # Handle facetgrids first\n        if row or col:\n            allargs = locals().copy()\n            allargs.pop('imshow_rgb')\n            allargs.update(allargs.pop('kwargs'))\n            allargs.pop('darray')\n            # Need the decorated plotting function\n            allargs['plotfunc'] = globals()[plotfunc.__name__]\n            return _easy_facetgrid(darray, kind='dataarray', **allargs)\n\n        plt = import_matplotlib_pyplot()\n\n        rgb = kwargs.pop('rgb', None)\n        if rgb is not None and plotfunc.__name__ != 'imshow':\n            raise ValueError('The \"rgb\" keyword is only valid for imshow()')\n        elif rgb is not None and not imshow_rgb:\n            raise ValueError('The \"rgb\" keyword is only valid for imshow()'\n                             'with a three-dimensional array (per facet)')\n\n        xlab, ylab = _infer_xy_labels(\n            darray=darray, x=x, y=y, imshow=imshow_rgb, rgb=rgb)\n\n        # better to pass the ndarrays directly to plotting functions\n        xval = darray[xlab].values\n        yval = darray[ylab].values\n\n        # check if we need to broadcast one dimension\n        if xval.ndim < yval.ndim:\n            xval = np.broadcast_to(xval, yval.shape)\n\n        if yval.ndim < xval.ndim:\n            yval = np.broadcast_to(yval, xval.shape)\n\n        # May need to transpose for correct x, y labels\n        # xlab may be the name of a coord, we have to check for dim names\n        if imshow_rgb:\n            # For RGB[A] images, matplotlib requires the color dimension\n            # to be last.  In Xarray the order should be unimportant, so\n            # we transpose to (y, x, color) to make this work.\n            yx_dims = (ylab, xlab)\n            dims = yx_dims + tuple(d for d in darray.dims if d not in yx_dims)\n            if dims != darray.dims:\n                darray = darray.transpose(*dims)\n        elif darray[xlab].dims[-1] == darray.dims[0]:\n            darray = darray.transpose()\n\n        # Pass the data as a masked ndarray too\n        zval = darray.to_masked_array(copy=False)\n\n        # Replace pd.Intervals if contained in xval or yval.\n        xplt, xlab_extra = _resolve_intervals_2dplot(xval, plotfunc.__name__)\n        yplt, ylab_extra = _resolve_intervals_2dplot(yval, plotfunc.__name__)\n\n        _ensure_plottable(xplt, yplt)\n\n        cmap_params, cbar_kwargs = _process_cmap_cbar_kwargs(\n            plotfunc, locals(), zval.data)\n\n        if 'contour' in plotfunc.__name__:\n            # extend is a keyword argument only for contour and contourf, but\n            # passing it to the colorbar is sufficient for imshow and\n            # pcolormesh\n            kwargs['extend'] = cmap_params['extend']\n            kwargs['levels'] = cmap_params['levels']\n            # if colors == a single color, matplotlib draws dashed negative\n            # contours. we lose this feature if we pass cmap and not colors\n            if isinstance(colors, str):\n                cmap_params['cmap'] = None\n                kwargs['colors'] = colors\n\n        if 'pcolormesh' == plotfunc.__name__:\n            kwargs['infer_intervals'] = infer_intervals\n\n        if 'imshow' == plotfunc.__name__ and isinstance(aspect, str):\n            # forbid usage of mpl strings\n            raise ValueError(\"plt.imshow's `aspect` kwarg is not available \"\n                             \"in xarray\")\n\n        ax = get_axis(figsize, size, aspect, ax)\n        primitive = plotfunc(xplt, yplt, zval, ax=ax, cmap=cmap_params['cmap'],\n                             vmin=cmap_params['vmin'],\n                             vmax=cmap_params['vmax'],\n                             norm=cmap_params['norm'],\n                             **kwargs)\n\n        # Label the plot with metadata\n        if add_labels:\n            ax.set_xlabel(label_from_attrs(darray[xlab], xlab_extra))\n            ax.set_ylabel(label_from_attrs(darray[ylab], ylab_extra))\n            ax.set_title(darray._title_for_slice())\n\n        if add_colorbar:\n            if add_labels and 'label' not in cbar_kwargs:\n                cbar_kwargs['label'] = label_from_attrs(darray)\n            cbar = _add_colorbar(primitive, ax, cbar_ax, cbar_kwargs,\n                                 cmap_params)\n\n        elif (cbar_ax is not None or cbar_kwargs):\n            # inform the user about keywords which aren't used\n            raise ValueError(\"cbar_ax and cbar_kwargs can't be used with \"\n                             \"add_colorbar=False.\")\n\n        # origin kwarg overrides yincrease\n        if 'origin' in kwargs:\n            yincrease = None\n\n        _update_axes(ax, xincrease, yincrease, xscale, yscale,\n                     xticks, yticks, xlim, ylim)\n\n        # Rotate dates on xlabels\n        # Do this without calling autofmt_xdate so that x-axes ticks\n        # on other subplots (if any) are not deleted.\n        # https://stackoverflow.com/questions/17430105/autofmt-xdate-deletes-x-axis-labels-of-all-subplots\n        if np.issubdtype(xplt.dtype, np.datetime64):\n            for xlabels in ax.get_xticklabels():\n                xlabels.set_rotation(30)\n                xlabels.set_ha('right')\n\n        return primitive\n\n    # For use as DataArray.plot.plotmethod\n    @functools.wraps(newplotfunc)\n    def plotmethod(_PlotMethods_obj, x=None, y=None, figsize=None, size=None,\n                   aspect=None, ax=None, row=None, col=None, col_wrap=None,\n                   xincrease=True, yincrease=True, add_colorbar=None,\n                   add_labels=True, vmin=None, vmax=None, cmap=None,\n                   colors=None, center=None, robust=False, extend=None,\n                   levels=None, infer_intervals=None, subplot_kws=None,\n                   cbar_ax=None, cbar_kwargs=None,\n                   xscale=None, yscale=None, xticks=None, yticks=None,\n                   xlim=None, ylim=None, norm=None, **kwargs):\n        \"\"\"\n        The method should have the same signature as the function.\n\n        This just makes the method work on Plotmethods objects,\n        and passes all the other arguments straight through.\n        \"\"\"\n        allargs = locals()\n        allargs['darray'] = _PlotMethods_obj._da\n        allargs.update(kwargs)\n        for arg in ['_PlotMethods_obj', 'newplotfunc', 'kwargs']:\n            del allargs[arg]\n        return newplotfunc(**allargs)\n\n    # Add to class _PlotMethods\n    setattr(_PlotMethods, plotmethod.__name__, plotmethod)\n\n    return newplotfunc", "response": "Decorator for common 2d plotting logic"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplotting a 2D array of pixels on a matplotlib axes.", "response": "def imshow(x, y, z, ax, **kwargs):\n    \"\"\"\n    Image plot of 2d DataArray using matplotlib.pyplot\n\n    Wraps :func:`matplotlib:matplotlib.pyplot.imshow`\n\n    While other plot methods require the DataArray to be strictly\n    two-dimensional, ``imshow`` also accepts a 3D array where some\n    dimension can be interpreted as RGB or RGBA color channels and\n    allows this dimension to be specified via the kwarg ``rgb=``.\n\n    Unlike matplotlib, Xarray can apply ``vmin`` and ``vmax`` to RGB or RGBA\n    data, by applying a single scaling factor and offset to all bands.\n    Passing  ``robust=True`` infers ``vmin`` and ``vmax``\n    :ref:`in the usual way <robust-plotting>`.\n\n    .. note::\n        This function needs uniformly spaced coordinates to\n        properly label the axes. Call DataArray.plot() to check.\n\n    The pixels are centered on the coordinates values. Ie, if the coordinate\n    value is 3.2 then the pixels for those coordinates will be centered on 3.2.\n    \"\"\"\n\n    if x.ndim != 1 or y.ndim != 1:\n        raise ValueError('imshow requires 1D coordinates, try using '\n                         'pcolormesh or contour(f)')\n\n    # Centering the pixels- Assumes uniform spacing\n    try:\n        xstep = (x[1] - x[0]) / 2.0\n    except IndexError:\n        # Arbitrary default value, similar to matplotlib behaviour\n        xstep = .1\n    try:\n        ystep = (y[1] - y[0]) / 2.0\n    except IndexError:\n        ystep = .1\n    left, right = x[0] - xstep, x[-1] + xstep\n    bottom, top = y[-1] + ystep, y[0] - ystep\n\n    defaults = {'origin': 'upper',\n                'interpolation': 'nearest'}\n\n    if not hasattr(ax, 'projection'):\n        # not for cartopy geoaxes\n        defaults['aspect'] = 'auto'\n\n    # Allow user to override these defaults\n    defaults.update(kwargs)\n\n    if defaults['origin'] == 'upper':\n        defaults['extent'] = [left, right, bottom, top]\n    else:\n        defaults['extent'] = [left, right, top, bottom]\n\n    if z.ndim == 3:\n        # matplotlib imshow uses black for missing data, but Xarray makes\n        # missing data transparent.  We therefore add an alpha channel if\n        # there isn't one, and set it to transparent where data is masked.\n        if z.shape[-1] == 3:\n            alpha = np.ma.ones(z.shape[:2] + (1,), dtype=z.dtype)\n            if np.issubdtype(z.dtype, np.integer):\n                alpha *= 255\n            z = np.ma.concatenate((z, alpha), axis=2)\n        else:\n            z = z.copy()\n        z[np.any(z.mask, axis=-1), -1] = 0\n\n    primitive = ax.imshow(z, **defaults)\n\n    return primitive"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef contour(x, y, z, ax, **kwargs):\n    primitive = ax.contour(x, y, z, **kwargs)\n    return primitive", "response": "Contour plot of 2d DataArray"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef contourf(x, y, z, ax, **kwargs):\n    primitive = ax.contourf(x, y, z, **kwargs)\n    return primitive", "response": "Wrapper around matplotlib. pyplot. contourf"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pcolormesh(x, y, z, ax, infer_intervals=None, **kwargs):\n\n    # decide on a default for infer_intervals (GH781)\n    x = np.asarray(x)\n    if infer_intervals is None:\n        if hasattr(ax, 'projection'):\n            if len(x.shape) == 1:\n                infer_intervals = True\n            else:\n                infer_intervals = False\n        else:\n            infer_intervals = True\n\n    if (infer_intervals and\n            ((np.shape(x)[0] == np.shape(z)[1]) or\n             ((x.ndim > 1) and (np.shape(x)[1] == np.shape(z)[1])))):\n        if len(x.shape) == 1:\n            x = _infer_interval_breaks(x, check_monotonic=True)\n        else:\n            # we have to infer the intervals on both axes\n            x = _infer_interval_breaks(x, axis=1)\n            x = _infer_interval_breaks(x, axis=0)\n\n    if (infer_intervals and\n            (np.shape(y)[0] == np.shape(z)[0])):\n        if len(y.shape) == 1:\n            y = _infer_interval_breaks(y, check_monotonic=True)\n        else:\n            # we have to infer the intervals on both axes\n            y = _infer_interval_breaks(y, axis=1)\n            y = _infer_interval_breaks(y, axis=0)\n\n    primitive = ax.pcolormesh(x, y, z, **kwargs)\n\n    # by default, pcolormesh picks \"round\" values for bounds\n    # this results in ugly looking plots with lots of surrounding whitespace\n    if not hasattr(ax, 'projection') and x.ndim == 1 and y.ndim == 1:\n        # not a cartopy geoaxis\n        ax.set_xlim(x[0], x[-1])\n        ax.set_ylim(y[0], y[-1])\n\n    return primitive", "response": "A wrapper around matplotlib. pyplot. pcolormesh that takes a 2d array x y z and returns a 2d array x y z."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_scheduler(get=None, collection=None):\n    try:\n        # dask 0.18.1 and later\n        from dask.base import get_scheduler\n        actual_get = get_scheduler(get, collection)\n    except ImportError:\n        try:\n            from dask.utils import effective_get\n            actual_get = effective_get(get, collection)\n        except ImportError:\n            return None\n\n    try:\n        from dask.distributed import Client\n        if isinstance(actual_get.__self__, Client):\n            return 'distributed'\n    except (ImportError, AttributeError):\n        try:\n            import dask.multiprocessing\n            if actual_get == dask.multiprocessing.get:\n                return 'multiprocessing'\n            else:\n                return 'threaded'\n        except ImportError:\n            return 'threaded'", "response": "Determine the dask scheduler that is being used."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef acquire(lock, blocking=True):\n    if blocking:\n        # no arguments needed\n        return lock.acquire()\n    elif DistributedLock is not None and isinstance(lock, DistributedLock):\n        # distributed.Lock doesn't support the blocking argument yet:\n        # https://github.com/dask/distributed/pull/2412\n        return lock.acquire(timeout=0)\n    else:\n        # \"blocking\" keyword argument not supported for:\n        # - threading.Lock on Python 2.\n        # - dask.SerializableLock with dask v1.0.0 or earlier.\n        # - multiprocessing.Lock calls the argument \"block\" instead.\n        return lock.acquire(blocking)", "response": "Acquire a lock possibly in a non - blocking fashion."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncombining a sequence of locks into a single lock.", "response": "def combine_locks(locks):\n    \"\"\"Combine a sequence of locks into a single lock.\"\"\"\n    all_locks = []\n    for lock in locks:\n        if isinstance(lock, CombinedLock):\n            all_locks.extend(lock.locks)\n        elif lock is not None:\n            all_locks.append(lock)\n\n    num_locks = len(all_locks)\n    if num_locks > 1:\n        return CombinedLock(all_locks)\n    elif num_locks == 1:\n        return all_locks[0]\n    else:\n        return DummyLock()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a list of dimensions to squeeze out.", "response": "def get_squeeze_dims(xarray_obj,\n                     dim: Union[Hashable, Iterable[Hashable], None] = None,\n                     axis: Union[int, Iterable[int], None] = None\n                     ) -> List[Hashable]:\n    \"\"\"Get a list of dimensions to squeeze out.\n    \"\"\"\n    if dim is not None and axis is not None:\n        raise ValueError('cannot use both parameters `axis` and `dim`')\n    if dim is None and axis is None:\n        return [d for d, s in xarray_obj.sizes.items() if s == 1]\n\n    if isinstance(dim, Iterable) and not isinstance(dim, str):\n        dim = list(dim)\n    elif dim is not None:\n        dim = [dim]\n    else:\n        assert axis is not None\n        if isinstance(axis, int):\n            axis = [axis]\n        axis = list(axis)\n        if any(not isinstance(a, int) for a in axis):\n            raise TypeError(\n                'parameter `axis` must be int or iterable of int.')\n        alldims = list(xarray_obj.sizes.keys())\n        dim = [alldims[a] for a in axis]\n\n    if any(xarray_obj.sizes[k] > 1 for k in dim):\n        raise ValueError('cannot select a dimension to squeeze out '\n                         'which has length greater than one')\n    return dim"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef full_like(other, fill_value, dtype: Union[str, np.dtype, None] = None):\n    from .dataarray import DataArray\n    from .dataset import Dataset\n    from .variable import Variable\n\n    if isinstance(other, Dataset):\n        data_vars = OrderedDict(\n            (k, _full_like_variable(v, fill_value, dtype))\n            for k, v in other.data_vars.items())\n        return Dataset(data_vars, coords=other.coords, attrs=other.attrs)\n    elif isinstance(other, DataArray):\n        return DataArray(\n            _full_like_variable(other.variable, fill_value, dtype),\n            dims=other.dims, coords=other.coords, attrs=other.attrs,\n            name=other.name)\n    elif isinstance(other, Variable):\n        return _full_like_variable(other, fill_value, dtype)\n    else:\n        raise TypeError(\"Expected DataArray, Dataset, or Variable\")", "response": "Return a new object with the same shape and type as a given object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if a dtype is a subclass of the numpy datetime types", "response": "def is_np_datetime_like(dtype: Union[str, np.dtype]) -> bool:\n    \"\"\"Check if a dtype is a subclass of the numpy datetime types\n    \"\"\"\n    return (np.issubdtype(dtype, np.datetime64) or\n            np.issubdtype(dtype, np.timedelta64))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _contains_cftime_datetimes(array) -> bool:\n    try:\n        from cftime import datetime as cftime_datetime\n    except ImportError:\n        return False\n    else:\n        if array.dtype == np.dtype('O') and array.size > 0:\n            sample = array.ravel()[0]\n            if isinstance(sample, dask_array_type):\n                sample = sample.compute()\n                if isinstance(sample, np.ndarray):\n                    sample = sample.item()\n            return isinstance(sample, cftime_datetime)\n        else:\n            return False", "response": "Check if an array contains cftime. datetime objects\nInvitements"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_axis_num(self, dim: Union[Hashable, Iterable[Hashable]]\n                     ) -> Union[int, Tuple[int, ...]]:\n        \"\"\"Return axis number(s) corresponding to dimension(s) in this array.\n\n        Parameters\n        ----------\n        dim : str or iterable of str\n            Dimension name(s) for which to lookup axes.\n\n        Returns\n        -------\n        int or tuple of int\n            Axis number or numbers corresponding to the given dimensions.\n        \"\"\"\n        if isinstance(dim, Iterable) and not isinstance(dim, str):\n            return tuple(self._get_axis_num(d) for d in dim)\n        else:\n            return self._get_axis_num(dim)", "response": "Returns the axis number or tuple corresponding to the given dimension."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sizes(self: Any) -> Mapping[Hashable, int]:\n        return Frozen(OrderedDict(zip(self.dims, self.shape)))", "response": "Ordered mapping from dimension names to lengths."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprovides method for the key - autocompletions in IPython.", "response": "def _ipython_key_completions_(self) -> List[str]:\n        \"\"\"Provide method for the key-autocompletions in IPython.\n        See http://ipython.readthedocs.io/en/stable/config/integrating.html#tab-completion\n        For the details.\n        \"\"\"  # noqa\n        item_lists = [item\n                      for sublist in self._item_sources\n                      for item in sublist\n                      if isinstance(item, str)]\n        return list(set(item_lists))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new object with squeezed data.", "response": "def squeeze(self, dim: Union[Hashable, Iterable[Hashable], None] = None,\n                drop: bool = False,\n                axis: Union[int, Iterable[int], None] = None):\n        \"\"\"Return a new object with squeezed data.\n\n        Parameters\n        ----------\n        dim : None or Hashable or iterable of Hashable, optional\n            Selects a subset of the length one dimensions. If a dimension is\n            selected with length greater than one, an error is raised. If\n            None, all length one dimensions are squeezed.\n        drop : bool, optional\n            If ``drop=True``, drop squeezed coordinates instead of making them\n            scalar.\n        axis : None or int or iterable of int, optional\n            Like dim, but positional.\n\n        Returns\n        -------\n        squeezed : same type as caller\n            This object, but with with all or a subset of the dimensions of\n            length 1 removed.\n\n        See Also\n        --------\n        numpy.squeeze\n        \"\"\"\n        dims = get_squeeze_dims(self, dim, axis)\n        return self.isel(drop=drop, **{d: 0 for d in dims})"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_index(self, key: Hashable) -> pd.Index:\n        if key not in self.dims:\n            raise KeyError(key)\n\n        try:\n            return self.indexes[key]\n        except KeyError:\n            # need to ensure dtype=int64 in case range is empty on Python 2\n            return pd.Index(range(self.sizes[key]), name=key, dtype=np.int64)", "response": "Get an index for a dimension with fall - back to a default RangeIndex\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nassigns new coordinates to the current object.", "response": "def assign_coords(self, **kwargs):\n        \"\"\"Assign new coordinates to this object.\n\n        Returns a new object with all the original data in addition to the new\n        coordinates.\n\n        Parameters\n        ----------\n        kwargs : keyword, value pairs\n            keywords are the variables names. If the values are callable, they\n            are computed on this object and assigned to new coordinate\n            variables. If the values are not callable, (e.g. a DataArray,\n            scalar, or array), they are simply assigned.\n\n        Returns\n        -------\n        assigned : same type as caller\n            A new object with the new coordinates in addition to the existing\n            data.\n\n        Examples\n        --------\n\n        Convert longitude coordinates from 0-359 to -180-179:\n\n        >>> da = xr.DataArray(np.random.rand(4),\n        ...                   coords=[np.array([358, 359, 0, 1])],\n        ...                   dims='lon')\n        >>> da\n        <xarray.DataArray (lon: 4)>\n        array([0.28298 , 0.667347, 0.657938, 0.177683])\n        Coordinates:\n          * lon      (lon) int64 358 359 0 1\n        >>> da.assign_coords(lon=(((da.lon + 180) % 360) - 180))\n        <xarray.DataArray (lon: 4)>\n        array([0.28298 , 0.667347, 0.657938, 0.177683])\n        Coordinates:\n          * lon      (lon) int64 -2 -1 0 1\n\n        Notes\n        -----\n        Since ``kwargs`` is a dictionary, the order of your arguments may not\n        be preserved, and so the order of the new variables is not well\n        defined. Assigning multiple variables within the same ``assign_coords``\n        is possible, but you cannot reference other variables created within\n        the same ``assign_coords`` call.\n\n        See also\n        --------\n        Dataset.assign\n        Dataset.swap_dims\n        \"\"\"\n        data = self.copy(deep=False)\n        results = self._calc_assign_results(kwargs)\n        data.coords.update(results)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nassign new attrs to this object.", "response": "def assign_attrs(self, *args, **kwargs):\n        \"\"\"Assign new attrs to this object.\n\n        Returns a new object equivalent to self.attrs.update(*args, **kwargs).\n\n        Parameters\n        ----------\n        args : positional arguments passed into ``attrs.update``.\n        kwargs : keyword arguments passed into ``attrs.update``.\n\n        Returns\n        -------\n        assigned : same type as caller\n            A new object with the new attrs in addition to the existing data.\n\n        See also\n        --------\n        Dataset.assign\n        \"\"\"\n        out = self.copy(deep=False)\n        out.attrs.update(*args, **kwargs)\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply func(self, *args, **kwargs) This method replicates the pandas method of the same name. Parameters ---------- func : function function to apply to this xarray object (Dataset/DataArray). ``args``, and ``kwargs`` are passed into ``func``. Alternatively a ``(callable, data_keyword)`` tuple where ``data_keyword`` is a string indicating the keyword of ``callable`` that expects the xarray object. args : positional arguments passed into ``func``. kwargs : a dictionary of keyword arguments passed into ``func``. Returns ------- object : the return type of ``func``. Notes ----- Use ``.pipe`` when chaining together functions that expect xarray or pandas objects, e.g., instead of writing >>> f(g(h(ds), arg1=a), arg2=b, arg3=c) You can write >>> (ds.pipe(h) ... .pipe(g, arg1=a) ... .pipe(f, arg2=b, arg3=c) ... ) If you have a function that takes the data as (say) the second argument, pass a tuple indicating which keyword expects the data. For example, suppose ``f`` takes its data as ``arg2``: >>> (ds.pipe(h) ... .pipe(g, arg1=a) ... .pipe((f, 'arg2'), arg1=a, arg3=c) ... ) See Also -------- pandas.DataFrame.pipe", "response": "def pipe(self, func: Union[Callable[..., T], Tuple[Callable[..., T], str]],\n             *args, **kwargs) -> T:\n        \"\"\"\n        Apply func(self, *args, **kwargs)\n\n        This method replicates the pandas method of the same name.\n\n        Parameters\n        ----------\n        func : function\n            function to apply to this xarray object (Dataset/DataArray).\n            ``args``, and ``kwargs`` are passed into ``func``.\n            Alternatively a ``(callable, data_keyword)`` tuple where\n            ``data_keyword`` is a string indicating the keyword of\n            ``callable`` that expects the xarray object.\n        args : positional arguments passed into ``func``.\n        kwargs : a dictionary of keyword arguments passed into ``func``.\n\n        Returns\n        -------\n        object : the return type of ``func``.\n\n        Notes\n        -----\n\n        Use ``.pipe`` when chaining together functions that expect\n        xarray or pandas objects, e.g., instead of writing\n\n        >>> f(g(h(ds), arg1=a), arg2=b, arg3=c)\n\n        You can write\n\n        >>> (ds.pipe(h)\n        ...    .pipe(g, arg1=a)\n        ...    .pipe(f, arg2=b, arg3=c)\n        ... )\n\n        If you have a function that takes the data as (say) the second\n        argument, pass a tuple indicating which keyword expects the\n        data. For example, suppose ``f`` takes its data as ``arg2``:\n\n        >>> (ds.pipe(h)\n        ...    .pipe(g, arg1=a)\n        ...    .pipe((f, 'arg2'), arg1=a, arg3=c)\n        ...  )\n\n        See Also\n        --------\n        pandas.DataFrame.pipe\n        \"\"\"\n        if isinstance(func, tuple):\n            func, target = func\n            if target in kwargs:\n                raise ValueError('%s is both the pipe target and a keyword '\n                                 'argument' % target)\n            kwargs[target] = self\n            return func(*args, **kwargs)\n        else:\n            return func(self, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef groupby(self, group, squeeze: bool = True):\n        return self._groupby_cls(self, group, squeeze=squeeze)", "response": "Returns a GroupBy object for performing grouped operations on the a\n            array."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef groupby_bins(self, group, bins, right: bool = True, labels=None,\n                     precision: int = 3, include_lowest: bool = False,\n                     squeeze: bool = True):\n        \"\"\"Returns a GroupBy object for performing grouped operations.\n\n        Rather than using all unique values of `group`, the values are discretized\n        first by applying `pandas.cut` [1]_ to `group`.\n\n        Parameters\n        ----------\n        group : str, DataArray or IndexVariable\n            Array whose binned values should be used to group this array. If a\n            string, must be the name of a variable contained in this dataset.\n        bins : int or array of scalars\n            If bins is an int, it defines the number of equal-width bins in the\n            range of x. However, in this case, the range of x is extended by .1%\n            on each side to include the min or max values of x. If bins is a\n            sequence it defines the bin edges allowing for non-uniform bin\n            width. No extension of the range of x is done in this case.\n        right : boolean, optional\n            Indicates whether the bins include the rightmost edge or not. If\n            right == True (the default), then the bins [1,2,3,4] indicate\n            (1,2], (2,3], (3,4].\n        labels : array or boolean, default None\n            Used as labels for the resulting bins. Must be of the same length as\n            the resulting bins. If False, string bin labels are assigned by\n            `pandas.cut`.\n        precision : int\n            The precision at which to store and display the bins labels.\n        include_lowest : bool\n            Whether the first interval should be left-inclusive or not.\n        squeeze : boolean, optional\n            If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n            controls whether the subarrays have a dimension of length 1 along\n            that dimension or if the dimension is squeezed out.\n\n        Returns\n        -------\n        grouped : GroupBy\n            A `GroupBy` object patterned after `pandas.GroupBy` that can be\n            iterated over in the form of `(unique_value, grouped_array)` pairs.\n            The name of the group has the added suffix `_bins` in order to\n            distinguish it from the original variable.\n\n        References\n        ----------\n        .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n        \"\"\"  # noqa\n        return self._groupby_cls(self, group, squeeze=squeeze, bins=bins,\n                                 cut_kwargs={'right': right, 'labels': labels,\n                                             'precision': precision,\n                                             'include_lowest': include_lowest})", "response": "Returns a GroupBy object for performing grouped operations on the avec of unique values of the avec of the avec of the avec of the avec of the avec of the avec of the avec of the avec of the avec of the avec of the avec of the avec of the avec of the avec of the avec of the avec of the avec of the avec of the avec of the avec of the values."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a rolling window object for the given date and time.", "response": "def rolling(self, dim: Optional[Mapping[Hashable, int]] = None,\n                min_periods: Optional[int] = None, center: bool = False,\n                **dim_kwargs: int):\n        \"\"\"\n        Rolling window object.\n\n        Parameters\n        ----------\n        dim: dict, optional\n            Mapping from the dimension name to create the rolling iterator\n            along (e.g. `time`) to its moving window size.\n        min_periods : int, default None\n            Minimum number of observations in window required to have a value\n            (otherwise result is NA). The default, None, is equivalent to\n            setting min_periods equal to the size of the window.\n        center : boolean, default False\n            Set the labels at the center of the window.\n        **dim_kwargs : optional\n            The keyword arguments form of ``dim``.\n            One of dim or dim_kwargs must be provided.\n\n        Returns\n        -------\n        Rolling object (core.rolling.DataArrayRolling for DataArray,\n        core.rolling.DatasetRolling for Dataset.)\n\n        Examples\n        --------\n        Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:\n\n        >>> da = xr.DataArray(np.linspace(0, 11, num=12),\n        ...                   coords=[pd.date_range('15/12/1999',\n        ...                           periods=12, freq=pd.DateOffset(months=1))],\n        ...                   dims='time')\n        >>> da\n        <xarray.DataArray (time: 12)>\n        array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7., 8.,   9.,  10.,  11.])\n        Coordinates:\n          * time     (time) datetime64[ns] 1999-12-15 2000-01-15 2000-02-15 ...\n        >>> da.rolling(time=3, center=True).mean()\n        <xarray.DataArray (time: 12)>\n        array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 1999-12-15 2000-01-15 2000-02-15 ...\n\n        Remove the NaNs using ``dropna()``:\n\n        >>> da.rolling(time=3, center=True).mean().dropna('time')\n        <xarray.DataArray (time: 10)>\n        array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n        Coordinates:\n          * time     (time) datetime64[ns] 2000-01-15 2000-02-15 2000-03-15 ...\n\n        See Also\n        --------\n        core.rolling.DataArrayRolling\n        core.rolling.DatasetRolling\n        \"\"\"  # noqa\n        dim = either_dict_or_kwargs(dim, dim_kwargs, 'rolling')\n        return self._rolling_cls(self, dim, min_periods=min_periods,\n                                 center=center)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef coarsen(self, dim: Optional[Mapping[Hashable, int]] = None,\n                boundary: str = 'exact',\n                side: Union[str, Mapping[Hashable, str]] = 'left',\n                coord_func: str = 'mean',\n                **dim_kwargs: int):\n        \"\"\"\n        Coarsen object.\n\n        Parameters\n        ----------\n        dim: dict, optional\n            Mapping from the dimension name to the window size.\n            dim : str\n                Name of the dimension to create the rolling iterator\n                along (e.g., `time`).\n            window : int\n                Size of the moving window.\n        boundary : 'exact' | 'trim' | 'pad'\n            If 'exact', a ValueError will be raised if dimension size is not a\n            multiple of the window size. If 'trim', the excess entries are\n            dropped. If 'pad', NA will be padded.\n        side : 'left' or 'right' or mapping from dimension to 'left' or 'right'\n        coord_func: function (name) that is applied to the coordintes,\n            or a mapping from coordinate name to function (name).\n\n        Returns\n        -------\n        Coarsen object (core.rolling.DataArrayCoarsen for DataArray,\n        core.rolling.DatasetCoarsen for Dataset.)\n\n        Examples\n        --------\n        Coarsen the long time series by averaging over every four days.\n\n        >>> da = xr.DataArray(np.linspace(0, 364, num=364),\n        ...                   dims='time',\n        ...                   coords={'time': pd.date_range(\n        ...                       '15/12/1999', periods=364)})\n        >>> da\n        <xarray.DataArray (time: 364)>\n        array([  0.      ,   1.002755,   2.00551 , ..., 361.99449 , 362.997245,\n               364.      ])\n        Coordinates:\n          * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-12-12\n        >>>\n        >>> da.coarsen(time=3, boundary='trim').mean()\n        <xarray.DataArray (time: 121)>\n        array([  1.002755,   4.011019,   7.019284,  ...,  358.986226,\n               361.99449 ])\n        Coordinates:\n          * time     (time) datetime64[ns] 1999-12-16 1999-12-19 ... 2000-12-10\n        >>>\n\n        See Also\n        --------\n        core.rolling.DataArrayCoarsen\n        core.rolling.DatasetCoarsen\n        \"\"\"\n        dim = either_dict_or_kwargs(dim, dim_kwargs, 'coarsen')\n        return self._coarsen_cls(\n            self, dim, boundary=boundary, side=side,\n            coord_func=coord_func)", "response": "Returns a new object that coarsen the long time series."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a Resample object for performing downsampling and upsampling operations.", "response": "def resample(self, indexer: Optional[Mapping[Hashable, str]] = None,\n                 skipna=None, closed: Optional[str] = None,\n                 label: Optional[str] = None,\n                 base: int = 0, keep_attrs: Optional[bool] = None,\n                 loffset=None,\n                 **indexer_kwargs: str):\n        \"\"\"Returns a Resample object for performing resampling operations.\n\n        Handles both downsampling and upsampling. If any intervals contain no\n        values from the original object, they will be given the value ``NaN``.\n\n        Parameters\n        ----------\n        indexer : {dim: freq}, optional\n            Mapping from the dimension name to resample frequency.\n        skipna : bool, optional\n            Whether to skip missing values when aggregating in downsampling.\n        closed : 'left' or 'right', optional\n            Side of each interval to treat as closed.\n        label : 'left or 'right', optional\n            Side of each interval to use for labeling.\n        base : int, optional\n            For frequencies that evenly subdivide 1 day, the \"origin\" of the\n            aggregated intervals. For example, for '24H' frequency, base could\n            range from 0 through 23.\n        loffset : timedelta or str, optional\n            Offset used to adjust the resampled time labels. Some pandas date\n            offset strings are supported.\n        keep_attrs : bool, optional\n            If True, the object's attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n        **indexer_kwargs : {dim: freq}\n            The keyword arguments form of ``indexer``.\n            One of indexer or indexer_kwargs must be provided.\n\n        Returns\n        -------\n        resampled : same type as caller\n            This object resampled.\n\n        Examples\n        --------\n        Downsample monthly time-series data to seasonal data:\n\n        >>> da = xr.DataArray(np.linspace(0, 11, num=12),\n        ...                   coords=[pd.date_range('15/12/1999',\n        ...                           periods=12, freq=pd.DateOffset(months=1))],\n        ...                   dims='time')\n        >>> da\n        <xarray.DataArray (time: 12)>\n        array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7., 8.,   9.,  10.,  11.])\n        Coordinates:\n          * time     (time) datetime64[ns] 1999-12-15 2000-01-15 2000-02-15 ...\n        >>> da.resample(time=\"QS-DEC\").mean()\n        <xarray.DataArray (time: 4)>\n        array([ 1.,  4.,  7., 10.])\n        Coordinates:\n          * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01\n\n        Upsample monthly time-series data to daily data:\n\n        >>> da.resample(time='1D').interpolate('linear')\n        <xarray.DataArray (time: 337)>\n        array([ 0.      ,  0.032258,  0.064516, ..., 10.935484, 10.967742, 11.      ])\n        Coordinates:\n          * time     (time) datetime64[ns] 1999-12-15 1999-12-16 1999-12-17 ...\n\n        Limit scope of upsampling method\n        >>> da.resample(time='1D').nearest(tolerance='1D')\n        <xarray.DataArray (time: 337)>\n        array([ 0.,  0., nan, ..., nan, 11., 11.])\n        Coordinates:\n          * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15\n\n        References\n        ----------\n\n        .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n        \"\"\"  # noqa\n        # TODO support non-string indexer after removing the old API.\n\n        from .dataarray import DataArray\n        from .resample import RESAMPLE_DIM\n        from ..coding.cftimeindex import CFTimeIndex\n\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n\n        # note: the second argument (now 'skipna') use to be 'dim'\n        if ((skipna is not None and not isinstance(skipna, bool))\n                or ('how' in indexer_kwargs and 'how' not in self.dims)\n                or ('dim' in indexer_kwargs and 'dim' not in self.dims)):\n            raise TypeError(\n                'resample() no longer supports the `how` or '\n                '`dim` arguments. Instead call methods on resample '\n                \"objects, e.g., data.resample(time='1D').mean()\")\n\n        indexer = either_dict_or_kwargs(indexer, indexer_kwargs, 'resample')\n        if len(indexer) != 1:\n            raise ValueError(\n                \"Resampling only supported along single dimensions.\"\n            )\n        dim, freq = next(iter(indexer.items()))\n\n        dim_name = dim\n        dim_coord = self[dim]\n\n        if isinstance(self.indexes[dim_name], CFTimeIndex):\n            from .resample_cftime import CFTimeGrouper\n            grouper = CFTimeGrouper(freq, closed, label, base, loffset)\n        else:\n            # TODO: to_offset() call required for pandas==0.19.2\n            grouper = pd.Grouper(freq=freq, closed=closed, label=label,\n                                 base=base,\n                                 loffset=pd.tseries.frequencies.to_offset(\n                                     loffset))\n        group = DataArray(dim_coord, coords=dim_coord.coords,\n                          dims=dim_coord.dims, name=RESAMPLE_DIM)\n        resampler = self._resample_cls(self, group=group, dim=dim_name,\n                                       grouper=grouper,\n                                       resample_dim=RESAMPLE_DIM)\n\n        return resampler"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef where(self, cond, other=dtypes.NA, drop: bool = False):\n        from .alignment import align\n        from .dataarray import DataArray\n        from .dataset import Dataset\n\n        if drop:\n            if other is not dtypes.NA:\n                raise ValueError('cannot set `other` if drop=True')\n\n            if not isinstance(cond, (Dataset, DataArray)):\n                raise TypeError(\"cond argument is %r but must be a %r or %r\" %\n                                (cond, Dataset, DataArray))\n\n            # align so we can use integer indexing\n            self, cond = align(self, cond)\n\n            # get cond with the minimal size needed for the Dataset\n            if isinstance(cond, Dataset):\n                clipcond = cond.to_array().any('variable')\n            else:\n                clipcond = cond\n\n            # clip the data corresponding to coordinate dims that are not used\n            nonzeros = zip(clipcond.dims, np.nonzero(clipcond.values))\n            indexers = {k: np.unique(v) for k, v in nonzeros}\n\n            self = self.isel(**indexers)\n            cond = cond.isel(**indexers)\n\n        return ops.where_method(self, cond, other)", "response": "Filter elements from this object according to a condition."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncloses any files linked to this object.", "response": "def close(self: Any) -> None:\n        \"\"\"Close any files linked to this object\n        \"\"\"\n        if self._file_obj is not None:\n            self._file_obj.close()\n        self._file_obj = None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef isin(self, test_elements):\n        from .computation import apply_ufunc\n        from .dataset import Dataset\n        from .dataarray import DataArray\n        from .variable import Variable\n\n        if isinstance(test_elements, Dataset):\n            raise TypeError(\n                'isin() argument must be convertible to an array: {}'\n                .format(test_elements))\n        elif isinstance(test_elements, (Variable, DataArray)):\n            # need to explicitly pull out data to support dask arrays as the\n            # second argument\n            test_elements = test_elements.data\n\n        return apply_ufunc(\n            duck_array_ops.isin,\n            self,\n            kwargs=dict(test_elements=test_elements),\n            dask='allowed',\n        )", "response": "Tests each value in the array for whether it is in test elements."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomparing two docker versions", "response": "def compare_version(v1, v2):\n    \"\"\"Compare docker versions\n\n    >>> v1 = '1.9'\n    >>> v2 = '1.10'\n    >>> compare_version(v1, v2)\n    1\n    >>> compare_version(v2, v1)\n    -1\n    >>> compare_version(v2, v2)\n    0\n    \"\"\"\n    s1 = StrictVersion(v1)\n    s2 = StrictVersion(v2)\n    if s1 == s2:\n        return 0\n    elif s1 > s2:\n        return -1\n    else:\n        return 1"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef datetime_to_timestamp(dt):\n    delta = dt - datetime.utcfromtimestamp(0)\n    return delta.seconds + delta.days * 24 * 3600", "response": "Convert a UTC datetime to a Unix timestamp"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading a line - separated environment file.", "response": "def parse_env_file(env_file):\n    \"\"\"\n    Reads a line-separated environment file.\n    The format of each line should be \"key=value\".\n    \"\"\"\n    environment = {}\n\n    with open(env_file, 'r') as f:\n        for line in f:\n\n            if line[0] == '#':\n                continue\n\n            line = line.strip()\n            if not line:\n                continue\n\n            parse_line = line.split('=', 1)\n            if len(parse_line) == 2:\n                k, v = parse_line\n                environment[k] = v\n            else:\n                raise errors.DockerException(\n                    'Invalid line in environment file {0}:\\n{1}'.format(\n                        env_file, line))\n\n    return environment"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconfigures a plugin. Args: name (string): The name of the plugin. The ``:latest`` tag is optional, and is the default if omitted. options (dict): A key-value mapping of options Returns: ``True`` if successful", "response": "def configure_plugin(self, name, options):\n        \"\"\"\n            Configure a plugin.\n\n            Args:\n                name (string): The name of the plugin. The ``:latest`` tag is\n                    optional, and is the default if omitted.\n                options (dict): A key-value mapping of options\n\n            Returns:\n                ``True`` if successful\n        \"\"\"\n        url = self._url('/plugins/{0}/set', name)\n        data = options\n        if isinstance(data, dict):\n            data = ['{0}={1}'.format(k, v) for k, v in six.iteritems(data)]\n        res = self._post_json(url, data=data)\n        self._raise_for_status(res)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new plugin.", "response": "def create_plugin(self, name, plugin_data_dir, gzip=False):\n        \"\"\"\n            Create a new plugin.\n\n            Args:\n                name (string): The name of the plugin. The ``:latest`` tag is\n                    optional, and is the default if omitted.\n                plugin_data_dir (string): Path to the plugin data directory.\n                    Plugin data directory must contain the ``config.json``\n                    manifest file and the ``rootfs`` directory.\n                gzip (bool): Compress the context using gzip. Default: False\n\n            Returns:\n                ``True`` if successful\n        \"\"\"\n        url = self._url('/plugins/create')\n\n        with utils.create_archive(\n            root=plugin_data_dir, gzip=gzip,\n            files=set(utils.build.walk(plugin_data_dir, []))\n        ) as archv:\n            res = self._post(url, params={'name': name}, data=archv)\n        self._raise_for_status(res)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef disable_plugin(self, name):\n        url = self._url('/plugins/{0}/disable', name)\n        res = self._post(url)\n        self._raise_for_status(res)\n        return True", "response": "Disable an installed plugin."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nenables an installed plugin.", "response": "def enable_plugin(self, name, timeout=0):\n        \"\"\"\n            Enable an installed plugin.\n\n            Args:\n                name (string): The name of the plugin. The ``:latest`` tag is\n                    optional, and is the default if omitted.\n                timeout (int): Operation timeout (in seconds). Default: 0\n\n            Returns:\n                ``True`` if successful\n        \"\"\"\n        url = self._url('/plugins/{0}/enable', name)\n        params = {'timeout': timeout}\n        res = self._post(url, params=params)\n        self._raise_for_status(res)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef inspect_plugin(self, name):\n        url = self._url('/plugins/{0}/json', name)\n        return self._result(self._get(url), True)", "response": "Retrieve plugin metadata.\n\n            Args:\n                name (string): The name of the plugin. The ``:latest`` tag is\n                    optional, and is the default if omitted.\n\n            Returns:\n                A dict containing plugin info"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plugins(self):\n        url = self._url('/plugins')\n        return self._result(self._get(url), True)", "response": "Retrieve a list of installed plugins."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve list of privileges granted to a remote plugin.", "response": "def plugin_privileges(self, name):\n        \"\"\"\n            Retrieve list of privileges to be granted to a plugin.\n\n            Args:\n                name (string): Name of the remote plugin to examine. The\n                    ``:latest`` tag is optional, and is the default if omitted.\n\n            Returns:\n                A list of dictionaries representing the plugin's\n                permissions\n\n        \"\"\"\n        params = {\n            'remote': name,\n        }\n\n        headers = {}\n        registry, repo_name = auth.resolve_repository_name(name)\n        header = auth.get_config_header(self, registry)\n        if header:\n            headers['X-Registry-Auth'] = header\n\n        url = self._url('/plugins/privileges')\n        return self._result(\n            self._get(url, params=params, headers=headers), True\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npushing a new plugin to the registry.", "response": "def push_plugin(self, name):\n        \"\"\"\n            Push a plugin to the registry.\n\n            Args:\n                name (string): Name of the plugin to upload. The ``:latest``\n                    tag is optional, and is the default if omitted.\n\n            Returns:\n                ``True`` if successful\n        \"\"\"\n        url = self._url('/plugins/{0}/pull', name)\n\n        headers = {}\n        registry, repo_name = auth.resolve_repository_name(name)\n        header = auth.get_config_header(self, registry)\n        if header:\n            headers['X-Registry-Auth'] = header\n        res = self._post(url, headers=headers)\n        self._raise_for_status(res)\n        return self._stream_helper(res, decode=True)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove an installed plugin.", "response": "def remove_plugin(self, name, force=False):\n        \"\"\"\n            Remove an installed plugin.\n\n            Args:\n                name (string): Name of the plugin to remove. The ``:latest``\n                    tag is optional, and is the default if omitted.\n                force (bool): Disable the plugin before removing. This may\n                    result in issues if the plugin is in use by a container.\n\n            Returns:\n                ``True`` if successful\n        \"\"\"\n        url = self._url('/plugins/{0}', name)\n        res = self._delete(url, params={'force': force})\n        self._raise_for_status(res)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef upgrade_plugin(self, name, remote, privileges):\n\n        url = self._url('/plugins/{0}/upgrade', name)\n        params = {\n            'remote': remote,\n        }\n\n        headers = {}\n        registry, repo_name = auth.resolve_repository_name(remote)\n        header = auth.get_config_header(self, registry)\n        if header:\n            headers['X-Registry-Auth'] = header\n        response = self._post_json(\n            url, params=params, headers=headers, data=privileges,\n            stream=True\n        )\n        self._raise_for_status(response)\n        return self._stream_helper(response, decode=True)", "response": "Upgrade an installed plugin."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_env(cls, **kwargs):\n        timeout = kwargs.pop('timeout', DEFAULT_TIMEOUT_SECONDS)\n        version = kwargs.pop('version', None)\n        return cls(\n            timeout=timeout, version=version, **kwargs_from_env(**kwargs)\n        )", "response": "Returns a new instance of the class from environment variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef close(self):\n\n        if not self._response.raw.closed:\n            # find the underlying socket object\n            # based on api.client._get_raw_response_socket\n\n            sock_fp = self._response.raw._fp.fp\n\n            if hasattr(sock_fp, 'raw'):\n                sock_raw = sock_fp.raw\n\n                if hasattr(sock_raw, 'sock'):\n                    sock = sock_raw.sock\n\n                elif hasattr(sock_raw, '_sock'):\n                    sock = sock_raw._sock\n\n            elif hasattr(sock_fp, 'channel'):\n                # We're working with a paramiko (SSH) channel, which doesn't\n                # support cancelable streams with the current implementation\n                raise DockerException(\n                    'Cancellable streams not supported for the SSH protocol'\n                )\n            else:\n                sock = sock_fp._sock\n\n            if hasattr(urllib3.contrib, 'pyopenssl') and isinstance(\n                    sock, urllib3.contrib.pyopenssl.WrappedSocket):\n                sock = sock.socket\n\n            sock.shutdown(socket.SHUT_RDWR)\n            sock.close()", "response": "Closes the event streaming."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread at most n bytes from socket returning a tuple of the n - byte integers.", "response": "def read(socket, n=4096):\n    \"\"\"\n    Reads at most n bytes from socket\n    \"\"\"\n\n    recoverable_errors = (errno.EINTR, errno.EDEADLK, errno.EWOULDBLOCK)\n\n    if six.PY3 and not isinstance(socket, NpipeSocket):\n        select.select([socket], [], [])\n\n    try:\n        if hasattr(socket, 'recv'):\n            return socket.recv(n)\n        if six.PY3 and isinstance(socket, getattr(pysocket, 'SocketIO')):\n            return socket.read(n)\n        return os.read(socket.fileno(), n)\n    except EnvironmentError as e:\n        if e.errno not in recoverable_errors:\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads exactly n bytes from socket returning a binary string.", "response": "def read_exactly(socket, n):\n    \"\"\"\n    Reads exactly n bytes from socket\n    Raises SocketError if there isn't enough data\n    \"\"\"\n    data = six.binary_type()\n    while len(data) < n:\n        next_data = read(socket, n - len(data))\n        if not next_data:\n            raise SocketError(\"Unexpected EOF\")\n        data += next_data\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef next_frame_header(socket):\n    try:\n        data = read_exactly(socket, 8)\n    except SocketError:\n        return (-1, -1)\n\n    stream, actual = struct.unpack('>BxxxL', data)\n    return (stream, actual)", "response": "Reads the next frame of data from the socket and returns the stream and size of the next frame."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef frames_iter(socket, tty):\n    if tty:\n        return ((STDOUT, frame) for frame in frames_iter_tty(socket))\n    else:\n        return frames_iter_no_tty(socket)", "response": "Return an iterator of frames read from socket."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef frames_iter_no_tty(socket):\n    while True:\n        (stream, n) = next_frame_header(socket)\n        if n < 0:\n            break\n        while n > 0:\n            result = read(socket, n)\n            if result is None:\n                continue\n            data_length = len(result)\n            if data_length == 0:\n                # We have reached EOF\n                return\n            n -= data_length\n            yield (stream, result)", "response": "Returns a generator of data read from the socket when tty setting is\n    is not enabled."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\niterating through frames read from the socket and return the result. Args: demux (bool): If False, stdout and stderr are multiplexed, and the result is the concatenation of all the frames. If True, the streams are demultiplexed, and the result is a 2-tuple where each item is the concatenation of frames belonging to the same stream.", "response": "def consume_socket_output(frames, demux=False):\n    \"\"\"\n    Iterate through frames read from the socket and return the result.\n\n    Args:\n\n        demux (bool):\n            If False, stdout and stderr are multiplexed, and the result is the\n            concatenation of all the frames. If True, the streams are\n            demultiplexed, and the result is a 2-tuple where each item is the\n            concatenation of frames belonging to the same stream.\n    \"\"\"\n    if demux is False:\n        # If the streams are multiplexed, the generator returns strings, that\n        # we just need to concatenate.\n        return six.binary_type().join(frames)\n\n    # If the streams are demultiplexed, the generator yields tuples\n    # (stdout, stderr)\n    out = [None, None]\n    for frame in frames:\n        # It is guaranteed that for each frame, one and only one stream\n        # is not None.\n        assert frame != (None, None)\n        if frame[0] is not None:\n            if out[0] is None:\n                out[0] = frame[0]\n            else:\n                out[0] += frame[0]\n        else:\n            if out[1] is None:\n                out[1] = frame[1]\n            else:\n                out[1] += frame[1]\n    return tuple(out)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a new instance of a new node.", "response": "def build(self, path=None, tag=None, quiet=False, fileobj=None,\n              nocache=False, rm=False, timeout=None,\n              custom_context=False, encoding=None, pull=False,\n              forcerm=False, dockerfile=None, container_limits=None,\n              decode=False, buildargs=None, gzip=False, shmsize=None,\n              labels=None, cache_from=None, target=None, network_mode=None,\n              squash=None, extra_hosts=None, platform=None, isolation=None,\n              use_config_proxy=False):\n        \"\"\"\n        Similar to the ``docker build`` command. Either ``path`` or ``fileobj``\n        needs to be set. ``path`` can be a local path (to a directory\n        containing a Dockerfile) or a remote URL. ``fileobj`` must be a\n        readable file-like object to a Dockerfile.\n\n        If you have a tar file for the Docker build context (including a\n        Dockerfile) already, pass a readable file-like object to ``fileobj``\n        and also pass ``custom_context=True``. If the stream is compressed\n        also, set ``encoding`` to the correct value (e.g ``gzip``).\n\n        Example:\n            >>> from io import BytesIO\n            >>> from docker import APIClient\n            >>> dockerfile = '''\n            ... # Shared Volume\n            ... FROM busybox:buildroot-2014.02\n            ... VOLUME /data\n            ... CMD [\"/bin/sh\"]\n            ... '''\n            >>> f = BytesIO(dockerfile.encode('utf-8'))\n            >>> cli = APIClient(base_url='tcp://127.0.0.1:2375')\n            >>> response = [line for line in cli.build(\n            ...     fileobj=f, rm=True, tag='yourname/volume'\n            ... )]\n            >>> response\n            ['{\"stream\":\" ---\\\\u003e a9eb17255234\\\\n\"}',\n             '{\"stream\":\"Step 1 : VOLUME /data\\\\n\"}',\n             '{\"stream\":\" ---\\\\u003e Running in abdc1e6896c6\\\\n\"}',\n             '{\"stream\":\" ---\\\\u003e 713bca62012e\\\\n\"}',\n             '{\"stream\":\"Removing intermediate container abdc1e6896c6\\\\n\"}',\n             '{\"stream\":\"Step 2 : CMD [\\\\\"/bin/sh\\\\\"]\\\\n\"}',\n             '{\"stream\":\" ---\\\\u003e Running in dba30f2a1a7e\\\\n\"}',\n             '{\"stream\":\" ---\\\\u003e 032b8b2855fc\\\\n\"}',\n             '{\"stream\":\"Removing intermediate container dba30f2a1a7e\\\\n\"}',\n             '{\"stream\":\"Successfully built 032b8b2855fc\\\\n\"}']\n\n        Args:\n            path (str): Path to the directory containing the Dockerfile\n            fileobj: A file object to use as the Dockerfile. (Or a file-like\n                object)\n            tag (str): A tag to add to the final image\n            quiet (bool): Whether to return the status\n            nocache (bool): Don't use the cache when set to ``True``\n            rm (bool): Remove intermediate containers. The ``docker build``\n                command now defaults to ``--rm=true``, but we have kept the old\n                default of `False` to preserve backward compatibility\n            timeout (int): HTTP timeout\n            custom_context (bool): Optional if using ``fileobj``\n            encoding (str): The encoding for a stream. Set to ``gzip`` for\n                compressing\n            pull (bool): Downloads any updates to the FROM image in Dockerfiles\n            forcerm (bool): Always remove intermediate containers, even after\n                unsuccessful builds\n            dockerfile (str): path within the build context to the Dockerfile\n            buildargs (dict): A dictionary of build arguments\n            container_limits (dict): A dictionary of limits applied to each\n                container created by the build process. Valid keys:\n\n                - memory (int): set memory limit for build\n                - memswap (int): Total memory (memory + swap), -1 to disable\n                    swap\n                - cpushares (int): CPU shares (relative weight)\n                - cpusetcpus (str): CPUs in which to allow execution, e.g.,\n                    ``\"0-3\"``, ``\"0,1\"``\n            decode (bool): If set to ``True``, the returned stream will be\n                decoded into dicts on the fly. Default ``False``\n            shmsize (int): Size of `/dev/shm` in bytes. The size must be\n                greater than 0. If omitted the system uses 64MB\n            labels (dict): A dictionary of labels to set on the image\n            cache_from (:py:class:`list`): A list of images used for build\n                cache resolution\n            target (str): Name of the build-stage to build in a multi-stage\n                Dockerfile\n            network_mode (str): networking mode for the run commands during\n                build\n            squash (bool): Squash the resulting images layers into a\n                single layer.\n            extra_hosts (dict): Extra hosts to add to /etc/hosts in building\n                containers, as a mapping of hostname to IP address.\n            platform (str): Platform in the format ``os[/arch[/variant]]``\n            isolation (str): Isolation technology used during build.\n                Default: `None`.\n            use_config_proxy (bool): If ``True``, and if the docker client\n                configuration file (``~/.docker/config.json`` by default)\n                contains a proxy configuration, the corresponding environment\n                variables will be set in the container being built.\n\n        Returns:\n            A generator for the build output.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n            ``TypeError``\n                If neither ``path`` nor ``fileobj`` is specified.\n        \"\"\"\n        remote = context = None\n        headers = {}\n        container_limits = container_limits or {}\n        buildargs = buildargs or {}\n        if path is None and fileobj is None:\n            raise TypeError(\"Either path or fileobj needs to be provided.\")\n        if gzip and encoding is not None:\n            raise errors.DockerException(\n                'Can not use custom encoding if gzip is enabled'\n            )\n\n        for key in container_limits.keys():\n            if key not in constants.CONTAINER_LIMITS_KEYS:\n                raise errors.DockerException(\n                    'Invalid container_limits key {0}'.format(key)\n                )\n\n        if custom_context:\n            if not fileobj:\n                raise TypeError(\"You must specify fileobj with custom_context\")\n            context = fileobj\n        elif fileobj is not None:\n            context = utils.mkbuildcontext(fileobj)\n        elif path.startswith(('http://', 'https://',\n                              'git://', 'github.com/', 'git@')):\n            remote = path\n        elif not os.path.isdir(path):\n            raise TypeError(\"You must specify a directory to build in path\")\n        else:\n            dockerignore = os.path.join(path, '.dockerignore')\n            exclude = None\n            if os.path.exists(dockerignore):\n                with open(dockerignore, 'r') as f:\n                    exclude = list(filter(\n                        lambda x: x != '' and x[0] != '#',\n                        [l.strip() for l in f.read().splitlines()]\n                    ))\n            dockerfile = process_dockerfile(dockerfile, path)\n            context = utils.tar(\n                path, exclude=exclude, dockerfile=dockerfile, gzip=gzip\n            )\n            encoding = 'gzip' if gzip else encoding\n\n        u = self._url('/build')\n        params = {\n            't': tag,\n            'remote': remote,\n            'q': quiet,\n            'nocache': nocache,\n            'rm': rm,\n            'forcerm': forcerm,\n            'pull': pull,\n            'dockerfile': dockerfile,\n        }\n        params.update(container_limits)\n\n        if use_config_proxy:\n            proxy_args = self._proxy_configs.get_environment()\n            for k, v in proxy_args.items():\n                buildargs.setdefault(k, v)\n        if buildargs:\n            params.update({'buildargs': json.dumps(buildargs)})\n\n        if shmsize:\n            if utils.version_gte(self._version, '1.22'):\n                params.update({'shmsize': shmsize})\n            else:\n                raise errors.InvalidVersion(\n                    'shmsize was only introduced in API version 1.22'\n                )\n\n        if labels:\n            if utils.version_gte(self._version, '1.23'):\n                params.update({'labels': json.dumps(labels)})\n            else:\n                raise errors.InvalidVersion(\n                    'labels was only introduced in API version 1.23'\n                )\n\n        if cache_from:\n            if utils.version_gte(self._version, '1.25'):\n                params.update({'cachefrom': json.dumps(cache_from)})\n            else:\n                raise errors.InvalidVersion(\n                    'cache_from was only introduced in API version 1.25'\n                )\n\n        if target:\n            if utils.version_gte(self._version, '1.29'):\n                params.update({'target': target})\n            else:\n                raise errors.InvalidVersion(\n                    'target was only introduced in API version 1.29'\n                )\n\n        if network_mode:\n            if utils.version_gte(self._version, '1.25'):\n                params.update({'networkmode': network_mode})\n            else:\n                raise errors.InvalidVersion(\n                    'network_mode was only introduced in API version 1.25'\n                )\n\n        if squash:\n            if utils.version_gte(self._version, '1.25'):\n                params.update({'squash': squash})\n            else:\n                raise errors.InvalidVersion(\n                    'squash was only introduced in API version 1.25'\n                )\n\n        if extra_hosts is not None:\n            if utils.version_lt(self._version, '1.27'):\n                raise errors.InvalidVersion(\n                    'extra_hosts was only introduced in API version 1.27'\n                )\n\n            if isinstance(extra_hosts, dict):\n                extra_hosts = utils.format_extra_hosts(extra_hosts)\n            params.update({'extrahosts': extra_hosts})\n\n        if platform is not None:\n            if utils.version_lt(self._version, '1.32'):\n                raise errors.InvalidVersion(\n                    'platform was only introduced in API version 1.32'\n                )\n            params['platform'] = platform\n\n        if isolation is not None:\n            if utils.version_lt(self._version, '1.24'):\n                raise errors.InvalidVersion(\n                    'isolation was only introduced in API version 1.24'\n                )\n            params['isolation'] = isolation\n\n        if context is not None:\n            headers = {'Content-Type': 'application/tar'}\n            if encoding:\n                headers['Content-Encoding'] = encoding\n\n        self._set_auth_headers(headers)\n\n        response = self._post(\n            u,\n            data=context,\n            params=params,\n            headers=headers,\n            stream=True,\n            timeout=timeout,\n        )\n\n        if context is not None and not custom_context:\n            context.close()\n\n        return self._stream_helper(response, decode=decode)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes the builder cache", "response": "def prune_builds(self):\n        \"\"\"\n        Delete the builder cache\n\n        Returns:\n            (dict): A dictionary containing information about the operation's\n                    result. The ``SpaceReclaimed`` key indicates the amount of\n                    bytes of disk space reclaimed.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        url = self._url(\"/build/prune\")\n        return self._result(self._post(url), True)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_swarm_spec(self, *args, **kwargs):\n        ext_ca = kwargs.pop('external_ca', None)\n        if ext_ca:\n            kwargs['external_cas'] = [ext_ca]\n        return types.SwarmSpec(self._version, *args, **kwargs)", "response": "Creates a SwarmSpec instance that can be used as the swarm_spec argument."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes a new Swarm using the current connected engine as the first Swarm node.", "response": "def init_swarm(self, advertise_addr=None, listen_addr='0.0.0.0:2377',\n                   force_new_cluster=False, swarm_spec=None,\n                   default_addr_pool=None, subnet_size=None):\n        \"\"\"\n        Initialize a new Swarm using the current connected engine as the first\n        node.\n\n        Args:\n            advertise_addr (string): Externally reachable address advertised\n                to other nodes. This can either be an address/port combination\n                in the form ``192.168.1.1:4567``, or an interface followed by a\n                port number, like ``eth0:4567``. If the port number is omitted,\n                the port number from the listen address is used. If\n                ``advertise_addr`` is not specified, it will be automatically\n                detected when possible. Default: None\n            listen_addr (string): Listen address used for inter-manager\n                communication, as well as determining the networking interface\n                used for the VXLAN Tunnel Endpoint (VTEP). This can either be\n                an address/port combination in the form ``192.168.1.1:4567``,\n                or an interface followed by a port number, like ``eth0:4567``.\n                If the port number is omitted, the default swarm listening port\n                is used. Default: '0.0.0.0:2377'\n            force_new_cluster (bool): Force creating a new Swarm, even if\n                already part of one. Default: False\n            swarm_spec (dict): Configuration settings of the new Swarm. Use\n                ``APIClient.create_swarm_spec`` to generate a valid\n                configuration. Default: None\n            default_addr_pool (list of strings): Default Address Pool specifies\n                default subnet pools for global scope networks. Each pool\n                should be specified as a CIDR block, like '10.0.0.0/8'.\n                Default: None\n            subnet_size (int): SubnetSize specifies the subnet size of the\n                networks created from the default subnet pool. Default: None\n\n        Returns:\n            ``True`` if successful.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n\n        url = self._url('/swarm/init')\n        if swarm_spec is not None and not isinstance(swarm_spec, dict):\n            raise TypeError('swarm_spec must be a dictionary')\n\n        if default_addr_pool is not None:\n            if utils.version_lt(self._version, '1.39'):\n                raise errors.InvalidVersion(\n                    'Address pool is only available for API version >= 1.39'\n                )\n            # subnet_size becomes 0 if not set with default_addr_pool\n            if subnet_size is None:\n                subnet_size = DEFAULT_SWARM_SUBNET_SIZE\n\n        if subnet_size is not None:\n            if utils.version_lt(self._version, '1.39'):\n                raise errors.InvalidVersion(\n                    'Subnet size is only available for API version >= 1.39'\n                )\n            # subnet_size is ignored if set without default_addr_pool\n            if default_addr_pool is None:\n                default_addr_pool = DEFAULT_SWARM_ADDR_POOL\n\n        data = {\n            'AdvertiseAddr': advertise_addr,\n            'ListenAddr': listen_addr,\n            'DefaultAddrPool': default_addr_pool,\n            'SubnetSize': subnet_size,\n            'ForceNewCluster': force_new_cluster,\n            'Spec': swarm_spec,\n        }\n        response = self._post_json(url, data=data)\n        self._raise_for_status(response)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef inspect_swarm(self):\n        url = self._url('/swarm')\n        return self._result(self._get(url), True)", "response": "Retrieve low - level information about the current swarm."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve low - level information about a swarm node.", "response": "def inspect_node(self, node_id):\n        \"\"\"\n        Retrieve low-level information about a swarm node\n\n        Args:\n            node_id (string): ID of the node to be inspected.\n\n        Returns:\n            A dictionary containing data about this node.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        url = self._url('/nodes/{0}', node_id)\n        return self._result(self._get(url), True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes this Engine join a swarm that has already been created. Args: remote_addrs (:py:class:`list`): Addresses of one or more manager nodes already participating in the Swarm to join. join_token (string): Secret token for joining this Swarm. listen_addr (string): Listen address used for inter-manager communication if the node gets promoted to manager, as well as determining the networking interface used for the VXLAN Tunnel Endpoint (VTEP). Default: ``'0.0.0.0:2377`` advertise_addr (string): Externally reachable address advertised to other nodes. This can either be an address/port combination in the form ``192.168.1.1:4567``, or an interface followed by a port number, like ``eth0:4567``. If the port number is omitted, the port number from the listen address is used. If AdvertiseAddr is not specified, it will be automatically detected when possible. Default: ``None`` Returns: ``True`` if the request went through. Raises: :py:class:`docker.errors.APIError` If the server returns an error.", "response": "def join_swarm(self, remote_addrs, join_token, listen_addr='0.0.0.0:2377',\n                   advertise_addr=None):\n        \"\"\"\n        Make this Engine join a swarm that has already been created.\n\n        Args:\n            remote_addrs (:py:class:`list`): Addresses of one or more manager\n                nodes already participating in the Swarm to join.\n            join_token (string): Secret token for joining this Swarm.\n            listen_addr (string): Listen address used for inter-manager\n                communication if the node gets promoted to manager, as well as\n                determining the networking interface used for the VXLAN Tunnel\n                Endpoint (VTEP). Default: ``'0.0.0.0:2377``\n            advertise_addr (string): Externally reachable address advertised\n                to other nodes. This can either be an address/port combination\n                in the form ``192.168.1.1:4567``, or an interface followed by a\n                port number, like ``eth0:4567``. If the port number is omitted,\n                the port number from the listen address is used. If\n                AdvertiseAddr is not specified, it will be automatically\n                detected when possible. Default: ``None``\n\n        Returns:\n            ``True`` if the request went through.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        data = {\n            \"RemoteAddrs\": remote_addrs,\n            \"ListenAddr\": listen_addr,\n            \"JoinToken\": join_token,\n            \"AdvertiseAddr\": advertise_addr,\n        }\n        url = self._url('/swarm/join')\n        response = self._post_json(url, data=data)\n        self._raise_for_status(response)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nleaves a swarm. Args: force (bool): Leave the swarm even if this node is a manager. Default: ``False`` Returns: ``True`` if the request went through. Raises: :py:class:`docker.errors.APIError` If the server returns an error.", "response": "def leave_swarm(self, force=False):\n        \"\"\"\n        Leave a swarm.\n\n        Args:\n            force (bool): Leave the swarm even if this node is a manager.\n                Default: ``False``\n\n        Returns:\n            ``True`` if the request went through.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        url = self._url('/swarm/leave')\n        response = self._post(url, params={'force': force})\n        # Ignore \"this node is not part of a swarm\" error\n        if force and response.status_code == http_client.NOT_ACCEPTABLE:\n            return True\n        # FIXME: Temporary workaround for 1.13.0-rc bug\n        # https://github.com/docker/docker/issues/29192\n        if force and response.status_code == http_client.SERVICE_UNAVAILABLE:\n            return True\n        self._raise_for_status(response)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_node(self, node_id, force=False):\n        url = self._url('/nodes/{0}', node_id)\n        params = {\n            'force': force\n        }\n        res = self._delete(url, params=params)\n        self._raise_for_status(res)\n        return True", "response": "Removes a node from the swarm."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nunlock a locked swarm.", "response": "def unlock_swarm(self, key):\n        \"\"\"\n            Unlock a locked swarm.\n\n            Args:\n                key (string): The unlock key as provided by\n                    :py:meth:`get_unlock_key`\n\n            Raises:\n                :py:class:`docker.errors.InvalidArgument`\n                    If the key argument is in an incompatible format\n\n                :py:class:`docker.errors.APIError`\n                    If the server returns an error.\n\n            Returns:\n                `True` if the request was successful.\n\n            Example:\n\n                >>> key = client.get_unlock_key()\n                >>> client.unlock_node(key)\n\n        \"\"\"\n        if isinstance(key, dict):\n            if 'UnlockKey' not in key:\n                raise errors.InvalidArgument('Invalid unlock key format')\n        else:\n            key = {'UnlockKey': key}\n\n        url = self._url('/swarm/unlock')\n        res = self._post_json(url, data=key)\n        self._raise_for_status(res)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_node(self, node_id, version, node_spec=None):\n        url = self._url('/nodes/{0}/update?version={1}', node_id, str(version))\n        res = self._post_json(url, data=node_spec)\n        self._raise_for_status(res)\n        return True", "response": "Update the node s configuration with the new version number."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_swarm(self, version, swarm_spec=None, rotate_worker_token=False,\n                     rotate_manager_token=False):\n        \"\"\"\n        Update the Swarm's configuration\n\n        Args:\n            version (int): The version number of the swarm object being\n                updated. This is required to avoid conflicting writes.\n            swarm_spec (dict): Configuration settings to update. Use\n                :py:meth:`~docker.api.swarm.SwarmApiMixin.create_swarm_spec` to\n                generate a valid configuration. Default: ``None``.\n            rotate_worker_token (bool): Rotate the worker join token. Default:\n                ``False``.\n            rotate_manager_token (bool): Rotate the manager join token.\n                Default: ``False``.\n\n        Returns:\n            ``True`` if the request went through.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n\n        url = self._url('/swarm/update')\n        response = self._post_json(url, data=swarm_spec, params={\n            'rotateWorkerToken': rotate_worker_token,\n            'rotateManagerToken': rotate_manager_token,\n            'version': version\n        })\n        self._raise_for_status(response)\n        return True", "response": "Update the Swarm s configuration with the new version number."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a secret. Args: secret_id (str): Secret ID. Returns: (:py:class:`Secret`): The secret. Raises: :py:class:`docker.errors.NotFound` If the secret does not exist. :py:class:`docker.errors.APIError` If the server returns an error.", "response": "def get(self, secret_id):\n        \"\"\"\n        Get a secret.\n\n        Args:\n            secret_id (str): Secret ID.\n\n        Returns:\n            (:py:class:`Secret`): The secret.\n\n        Raises:\n            :py:class:`docker.errors.NotFound`\n                If the secret does not exist.\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        return self.prepare_model(self.client.api.inspect_secret(secret_id))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting secrets. Similar to the docker secret ls command.", "response": "def list(self, **kwargs):\n        \"\"\"\n        List secrets. Similar to the ``docker secret ls`` command.\n\n        Args:\n            filters (dict): Server-side list filtering options.\n\n        Returns:\n            (list of :py:class:`Secret`): The secrets.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        resp = self.client.api.secrets(**kwargs)\n        return [self.prepare_model(obj) for obj in resp]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist the names and ids of all networks in the cluster.", "response": "def networks(self, names=None, ids=None, filters=None):\n        \"\"\"\n        List networks. Similar to the ``docker networks ls`` command.\n\n        Args:\n            names (:py:class:`list`): List of names to filter by\n            ids (:py:class:`list`): List of ids to filter by\n            filters (dict): Filters to be processed on the network list.\n                Available filters:\n                - ``driver=[<driver-name>]`` Matches a network's driver.\n                - ``label=[<key>]`` or ``label=[<key>=<value>]``.\n                - ``type=[\"custom\"|\"builtin\"]`` Filters networks by type.\n\n        Returns:\n            (dict): List of network objects.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n\n        if filters is None:\n            filters = {}\n        if names:\n            filters['name'] = names\n        if ids:\n            filters['id'] = ids\n        params = {'filters': utils.convert_filters(filters)}\n        url = self._url(\"/networks\")\n        res = self._get(url, params=params)\n        return self._result(res, json=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a network using the given parameters.", "response": "def create_network(self, name, driver=None, options=None, ipam=None,\n                       check_duplicate=None, internal=False, labels=None,\n                       enable_ipv6=False, attachable=None, scope=None,\n                       ingress=None):\n        \"\"\"\n        Create a network. Similar to the ``docker network create``.\n\n        Args:\n            name (str): Name of the network\n            driver (str): Name of the driver used to create the network\n            options (dict): Driver options as a key-value dictionary\n            ipam (IPAMConfig): Optional custom IP scheme for the network.\n            check_duplicate (bool): Request daemon to check for networks with\n                same name. Default: ``None``.\n            internal (bool): Restrict external access to the network. Default\n                ``False``.\n            labels (dict): Map of labels to set on the network. Default\n                ``None``.\n            enable_ipv6 (bool): Enable IPv6 on the network. Default ``False``.\n            attachable (bool): If enabled, and the network is in the global\n                scope,  non-service containers on worker nodes will be able to\n                connect to the network.\n            scope (str): Specify the network's scope (``local``, ``global`` or\n                ``swarm``)\n            ingress (bool): If set, create an ingress network which provides\n                the routing-mesh in swarm mode.\n\n        Returns:\n            (dict): The created network reference object\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n\n        Example:\n            A network using the bridge driver:\n\n                >>> client.create_network(\"network1\", driver=\"bridge\")\n\n            You can also create more advanced networks with custom IPAM\n            configurations. For example, setting the subnet to\n            ``192.168.52.0/24`` and gateway address to ``192.168.52.254``.\n\n            .. code-block:: python\n\n                >>> ipam_pool = docker.types.IPAMPool(\n                    subnet='192.168.52.0/24',\n                    gateway='192.168.52.254'\n                )\n                >>> ipam_config = docker.types.IPAMConfig(\n                    pool_configs=[ipam_pool]\n                )\n                >>> docker_client.create_network(\"network1\", driver=\"bridge\",\n                                                 ipam=ipam_config)\n        \"\"\"\n        if options is not None and not isinstance(options, dict):\n            raise TypeError('options must be a dictionary')\n\n        data = {\n            'Name': name,\n            'Driver': driver,\n            'Options': options,\n            'IPAM': ipam,\n            'CheckDuplicate': check_duplicate,\n        }\n\n        if labels is not None:\n            if version_lt(self._version, '1.23'):\n                raise InvalidVersion(\n                    'network labels were introduced in API 1.23'\n                )\n            if not isinstance(labels, dict):\n                raise TypeError('labels must be a dictionary')\n            data[\"Labels\"] = labels\n\n        if enable_ipv6:\n            if version_lt(self._version, '1.23'):\n                raise InvalidVersion(\n                    'enable_ipv6 was introduced in API 1.23'\n                )\n            data['EnableIPv6'] = True\n\n        if internal:\n            if version_lt(self._version, '1.22'):\n                raise InvalidVersion('Internal networks are not '\n                                     'supported in API version < 1.22')\n            data['Internal'] = True\n\n        if attachable is not None:\n            if version_lt(self._version, '1.24'):\n                raise InvalidVersion(\n                    'attachable is not supported in API version < 1.24'\n                )\n            data['Attachable'] = attachable\n\n        if ingress is not None:\n            if version_lt(self._version, '1.29'):\n                raise InvalidVersion(\n                    'ingress is not supported in API version < 1.29'\n                )\n\n            data['Ingress'] = ingress\n\n        if scope is not None:\n            if version_lt(self._version, '1.30'):\n                raise InvalidVersion(\n                    'scope is not supported in API version < 1.30'\n                )\n            data['Scope'] = scope\n\n        url = self._url(\"/networks/create\")\n        res = self._post_json(url, data=data)\n        return self._result(res, json=True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_network(self, net_id):\n        url = self._url(\"/networks/{0}\", net_id)\n        res = self._delete(url)\n        self._raise_for_status(res)", "response": "Remove a network from the cache."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef inspect_network(self, net_id, verbose=None, scope=None):\n        params = {}\n        if verbose is not None:\n            if version_lt(self._version, '1.28'):\n                raise InvalidVersion('verbose was introduced in API 1.28')\n            params['verbose'] = verbose\n        if scope is not None:\n            if version_lt(self._version, '1.31'):\n                raise InvalidVersion('scope was introduced in API 1.31')\n            params['scope'] = scope\n\n        url = self._url(\"/networks/{0}\", net_id)\n        res = self._get(url, params=params)\n        return self._result(res, json=True)", "response": "Get detailed information about a network."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconnect a container to a network.", "response": "def connect_container_to_network(self, container, net_id,\n                                     ipv4_address=None, ipv6_address=None,\n                                     aliases=None, links=None,\n                                     link_local_ips=None):\n        \"\"\"\n        Connect a container to a network.\n\n        Args:\n            container (str): container-id/name to be connected to the network\n            net_id (str): network id\n            aliases (:py:class:`list`): A list of aliases for this endpoint.\n                Names in that list can be used within the network to reach the\n                container. Defaults to ``None``.\n            links (:py:class:`list`): A list of links for this endpoint.\n                Containers declared in this list will be linked to this\n                container. Defaults to ``None``.\n            ipv4_address (str): The IP address of this container on the\n                network, using the IPv4 protocol. Defaults to ``None``.\n            ipv6_address (str): The IP address of this container on the\n                network, using the IPv6 protocol. Defaults to ``None``.\n            link_local_ips (:py:class:`list`): A list of link-local\n                (IPv4/IPv6) addresses.\n        \"\"\"\n        data = {\n            \"Container\": container,\n            \"EndpointConfig\": self.create_endpoint_config(\n                aliases=aliases, links=links, ipv4_address=ipv4_address,\n                ipv6_address=ipv6_address, link_local_ips=link_local_ips\n            ),\n        }\n\n        url = self._url(\"/networks/{0}/connect\", net_id)\n        res = self._post_json(url, data=data)\n        self._raise_for_status(res)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef disconnect_container_from_network(self, container, net_id,\n                                          force=False):\n        \"\"\"\n        Disconnect a container from a network.\n\n        Args:\n            container (str): container ID or name to be disconnected from the\n                network\n            net_id (str): network ID\n            force (bool): Force the container to disconnect from a network.\n                Default: ``False``\n        \"\"\"\n        data = {\"Container\": container}\n        if force:\n            if version_lt(self._version, '1.22'):\n                raise InvalidVersion(\n                    'Forced disconnect was introduced in API 1.22'\n                )\n            data['Force'] = force\n        url = self._url(\"/networks/{0}/disconnect\", net_id)\n        res = self._post_json(url, data=data)\n        self._raise_for_status(res)", "response": "Disconnect a container from a network."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef init(self, advertise_addr=None, listen_addr='0.0.0.0:2377',\n             force_new_cluster=False, default_addr_pool=None,\n             subnet_size=None, **kwargs):\n        \"\"\"\n        Initialize a new swarm on this Engine.\n\n        Args:\n            advertise_addr (str): Externally reachable address advertised to\n                other nodes. This can either be an address/port combination in\n                the form ``192.168.1.1:4567``, or an interface followed by a\n                port number, like ``eth0:4567``. If the port number is omitted,\n                the port number from the listen address is used.\n\n                If not specified, it will be automatically detected when\n                possible.\n            listen_addr (str): Listen address used for inter-manager\n                communication, as well as determining the networking interface\n                used for the VXLAN Tunnel Endpoint (VTEP). This can either be\n                an address/port combination in the form ``192.168.1.1:4567``,\n                or an interface followed by a port number, like ``eth0:4567``.\n                If the port number is omitted, the default swarm listening port\n                is used. Default: ``0.0.0.0:2377``\n            force_new_cluster (bool): Force creating a new Swarm, even if\n                already part of one. Default: False\n            default_addr_pool (list of str): Default Address Pool specifies\n                default subnet pools for global scope networks. Each pool\n                should be specified as a CIDR block, like '10.0.0.0/8'.\n                Default: None\n            subnet_size (int): SubnetSize specifies the subnet size of the\n                networks created from the default subnet pool. Default: None\n            task_history_retention_limit (int): Maximum number of tasks\n                history stored.\n            snapshot_interval (int): Number of logs entries between snapshot.\n            keep_old_snapshots (int): Number of snapshots to keep beyond the\n                current snapshot.\n            log_entries_for_slow_followers (int): Number of log entries to\n                keep around to sync up slow followers after a snapshot is\n                created.\n            heartbeat_tick (int): Amount of ticks (in seconds) between each\n                heartbeat.\n            election_tick (int): Amount of ticks (in seconds) needed without a\n                leader to trigger a new election.\n            dispatcher_heartbeat_period (int):  The delay for an agent to send\n                a heartbeat to the dispatcher.\n            node_cert_expiry (int): Automatic expiry for nodes certificates.\n            external_ca (dict): Configuration for forwarding signing requests\n                to an external certificate authority. Use\n                ``docker.types.SwarmExternalCA``.\n            name (string): Swarm's name\n            labels (dict): User-defined key/value metadata.\n            signing_ca_cert (str): The desired signing CA certificate for all\n                swarm node TLS leaf certificates, in PEM format.\n            signing_ca_key (str): The desired signing CA key for all swarm\n                node TLS leaf certificates, in PEM format.\n            ca_force_rotate (int): An integer whose purpose is to force swarm\n                to generate a new signing CA certificate and key, if none have\n                been specified.\n            autolock_managers (boolean): If set, generate a key and use it to\n                lock data stored on the managers.\n            log_driver (DriverConfig): The default log driver to use for tasks\n                created in the orchestrator.\n\n        Returns:\n            ``True`` if the request went through.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n\n        Example:\n\n            >>> client.swarm.init(\n                advertise_addr='eth0', listen_addr='0.0.0.0:5000',\n                force_new_cluster=False, default_addr_pool=['10.20.0.0/16],\n                subnet_size=24, snapshot_interval=5000,\n                log_entries_for_slow_followers=1200\n            )\n\n        \"\"\"\n        init_kwargs = {\n            'advertise_addr': advertise_addr,\n            'listen_addr': listen_addr,\n            'force_new_cluster': force_new_cluster,\n            'default_addr_pool': default_addr_pool,\n            'subnet_size': subnet_size\n        }\n        init_kwargs['swarm_spec'] = self.client.api.create_swarm_spec(**kwargs)\n        self.client.api.init_swarm(**init_kwargs)\n        self.reload()\n        return True", "response": "Initialize a new Swarm on this Engine."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self, rotate_worker_token=False, rotate_manager_token=False,\n               **kwargs):\n        \"\"\"\n        Update the swarm's configuration.\n\n        It takes the same arguments as :py:meth:`init`, except\n        ``advertise_addr``, ``listen_addr``, and ``force_new_cluster``. In\n        addition, it takes these arguments:\n\n        Args:\n            rotate_worker_token (bool): Rotate the worker join token. Default:\n                ``False``.\n            rotate_manager_token (bool): Rotate the manager join token.\n                Default: ``False``.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n\n        \"\"\"\n        # this seems to have to be set\n        if kwargs.get('node_cert_expiry') is None:\n            kwargs['node_cert_expiry'] = 7776000000000000\n\n        return self.client.api.update_swarm(\n            version=self.version,\n            swarm_spec=self.client.api.create_swarm_spec(**kwargs),\n            rotate_worker_token=rotate_worker_token,\n            rotate_manager_token=rotate_manager_token\n        )", "response": "Update the current configuration of the swarm."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_volume(self, name=None, driver=None, driver_opts=None,\n                      labels=None):\n        \"\"\"\n        Create and register a named volume\n\n        Args:\n            name (str): Name of the volume\n            driver (str): Name of the driver used to create the volume\n            driver_opts (dict): Driver options as a key-value dictionary\n            labels (dict): Labels to set on the volume\n\n        Returns:\n            (dict): The created volume reference object\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n\n        Example:\n\n            >>> volume = cli.create_volume(name='foobar', driver='local',\n                    driver_opts={'foo': 'bar', 'baz': 'false'},\n                    labels={\"key\": \"value\"})\n            >>> print(volume)\n            {u'Driver': u'local',\n             u'Labels': {u'key': u'value'},\n             u'Mountpoint': u'/var/lib/docker/volumes/foobar/_data',\n             u'Name': u'foobar',\n             u'Scope': u'local'}\n\n        \"\"\"\n        url = self._url('/volumes/create')\n        if driver_opts is not None and not isinstance(driver_opts, dict):\n            raise TypeError('driver_opts must be a dictionary')\n\n        data = {\n            'Name': name,\n            'Driver': driver,\n            'DriverOpts': driver_opts,\n        }\n\n        if labels is not None:\n            if utils.compare_version('1.23', self._version) < 0:\n                raise errors.InvalidVersion(\n                    'volume labels were introduced in API 1.23'\n                )\n            if not isinstance(labels, dict):\n                raise TypeError('labels must be a dictionary')\n            data[\"Labels\"] = labels\n\n        return self._result(self._post_json(url, data=data), True)", "response": "Create and register a named volume in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef inspect_volume(self, name):\n        url = self._url('/volumes/{0}', name)\n        return self._result(self._get(url), True)", "response": "Retrieve the information of a specific volume."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting unused volumes from the server.", "response": "def prune_volumes(self, filters=None):\n        \"\"\"\n        Delete unused volumes\n\n        Args:\n            filters (dict): Filters to process on the prune list.\n\n        Returns:\n            (dict): A dict containing a list of deleted volume names and\n                the amount of disk space reclaimed in bytes.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        params = {}\n        if filters:\n            params['filters'] = utils.convert_filters(filters)\n        url = self._url('/volumes/prune')\n        return self._result(self._post(url, params=params), True)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_volume(self, name, force=False):\n        params = {}\n        if force:\n            if utils.version_lt(self._version, '1.25'):\n                raise errors.InvalidVersion(\n                    'force removal was introduced in API 1.25'\n                )\n            params = {'force': force}\n\n        url = self._url('/volumes/{0}', name, params=params)\n        resp = self._delete(url)\n        self._raise_for_status(resp)", "response": "Removes a specific image volume from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new config in the neccesary way.", "response": "def create_config(self, name, data, labels=None):\n        \"\"\"\n            Create a config\n\n            Args:\n                name (string): Name of the config\n                data (bytes): Config data to be stored\n                labels (dict): A mapping of labels to assign to the config\n\n            Returns (dict): ID of the newly created config\n        \"\"\"\n        if not isinstance(data, bytes):\n            data = data.encode('utf-8')\n\n        data = base64.b64encode(data)\n        if six.PY3:\n            data = data.decode('ascii')\n        body = {\n            'Data': data,\n            'Name': name,\n            'Labels': labels\n        }\n\n        url = self._url('/configs/create')\n        return self._result(\n            self._post_json(url, data=body), True\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef inspect_config(self, id):\n        url = self._url('/configs/{0}', id)\n        return self._result(self._get(url), True)", "response": "Retrieve config metadata\n\n            Args:\n                id (string): Full ID of the config to inspect\n\n            Returns (dict): A dictionary of metadata\n\n            Raises:\n                :py:class:`docker.errors.NotFound`\n                    if no config with that ID exists"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_config(self, id):\n        url = self._url('/configs/{0}', id)\n        res = self._delete(url)\n        self._raise_for_status(res)\n        return True", "response": "Removes a config from the cache"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef configs(self, filters=None):\n        url = self._url('/configs')\n        params = {}\n        if filters:\n            params['filters'] = utils.convert_filters(filters)\n        return self._result(self._get(url, params=params), True)", "response": "Returns a list of all the available configuration items for the current user."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a secret in the secret store", "response": "def create_secret(self, name, data, labels=None, driver=None):\n        \"\"\"\n            Create a secret\n\n            Args:\n                name (string): Name of the secret\n                data (bytes): Secret data to be stored\n                labels (dict): A mapping of labels to assign to the secret\n                driver (DriverConfig): A custom driver configuration. If\n                    unspecified, the default ``internal`` driver will be used\n\n            Returns (dict): ID of the newly created secret\n        \"\"\"\n        if not isinstance(data, bytes):\n            data = data.encode('utf-8')\n\n        data = base64.b64encode(data)\n        if six.PY3:\n            data = data.decode('ascii')\n        body = {\n            'Data': data,\n            'Name': name,\n            'Labels': labels\n        }\n\n        if driver is not None:\n            if utils.version_lt(self._version, '1.31'):\n                raise errors.InvalidVersion(\n                    'Secret driver is only available for API version > 1.31'\n                )\n\n            body['Driver'] = driver\n\n        url = self._url('/secrets/create')\n        return self._result(\n            self._post_json(url, data=body), True\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving secret metadata Args: id (string): Full ID of the secret to remove Returns (dict): A dictionary of metadata Raises: :py:class:`docker.errors.NotFound` if no secret with that ID exists", "response": "def inspect_secret(self, id):\n        \"\"\"\n            Retrieve secret metadata\n\n            Args:\n                id (string): Full ID of the secret to remove\n\n            Returns (dict): A dictionary of metadata\n\n            Raises:\n                :py:class:`docker.errors.NotFound`\n                    if no secret with that ID exists\n        \"\"\"\n        url = self._url('/secrets/{0}', id)\n        return self._result(self._get(url), True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update(self, node_spec):\n        return self.client.api.update_node(self.id, self.version, node_spec)", "response": "Update the node s configuration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving this node from the swarm.", "response": "def remove(self, force=False):\n        \"\"\"\n        Remove this node from the swarm.\n\n        Args:\n            force (bool): Force remove an active node. Default: `False`\n\n        Returns:\n            `True` if the request was successful.\n\n        Raises:\n            :py:class:`docker.errors.NotFound`\n                If the node doesn't exist in the swarm.\n\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        return self.client.api.remove_node(self.id, force=force)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(self, node_id):\n        return self.prepare_model(self.client.api.inspect_node(node_id))", "response": "Get a node s related attributes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list(self, *args, **kwargs):\n        return [\n            self.prepare_model(n)\n            for n in self.client.api.nodes(*args, **kwargs)\n        ]", "response": "List swarm nodes.\n\n        Args:\n            filters (dict): Filters to process on the nodes list. Valid\n                filters: ``id``, ``name``, ``membership`` and ``role``.\n                Default: ``None``\n\n        Returns:\n            A list of :py:class:`Node` objects.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n\n        Example:\n\n            >>> client.nodes.list(filters={'role': 'manager'})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nattempt to parse a json object from a buffer.", "response": "def json_splitter(buffer):\n    \"\"\"Attempt to parse a json object from a buffer. If there is at least one\n    object, return it and the rest of the buffer, otherwise return None.\n    \"\"\"\n    buffer = buffer.strip()\n    try:\n        obj, index = json_decoder.raw_decode(buffer)\n        rest = buffer[json.decoder.WHITESPACE.match(buffer, index).end():]\n        return obj, rest\n    except ValueError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconfigures the current node s settings.", "response": "def configure(self, options):\n        \"\"\"\n            Update the plugin's settings.\n\n            Args:\n                options (dict): A key-value mapping of options.\n\n            Raises:\n                :py:class:`docker.errors.APIError`\n                    If the server returns an error.\n        \"\"\"\n        self.client.api.configure_plugin(self.name, options)\n        self.reload()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndisable the plugin. Raises: :py:class:`docker.errors.APIError` If the server returns an error.", "response": "def disable(self):\n        \"\"\"\n            Disable the plugin.\n\n            Raises:\n                :py:class:`docker.errors.APIError`\n                    If the server returns an error.\n        \"\"\"\n\n        self.client.api.disable_plugin(self.name)\n        self.reload()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef enable(self, timeout=0):\n        self.client.api.enable_plugin(self.name, timeout)\n        self.reload()", "response": "Enable the plugin.\n\n            Args:\n                timeout (int): Timeout in seconds. Default: 0\n\n            Raises:\n                :py:class:`docker.errors.APIError`\n                    If the server returns an error."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving the plugin from the server.", "response": "def remove(self, force=False):\n        \"\"\"\n            Remove the plugin from the server.\n\n            Args:\n                force (bool): Remove even if the plugin is enabled.\n                    Default: False\n\n            Raises:\n                :py:class:`docker.errors.APIError`\n                    If the server returns an error.\n        \"\"\"\n        return self.client.api.remove_plugin(self.name, force=force)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef upgrade(self, remote=None):\n        if self.enabled:\n            raise errors.DockerError(\n                'Plugin must be disabled before upgrading.'\n            )\n\n        if remote is None:\n            remote = self.name\n        privileges = self.client.api.plugin_privileges(remote)\n        for d in self.client.api.upgrade_plugin(self.name, remote, privileges):\n            yield d\n        self._reload()", "response": "Upgrade the plugin.\n\n            Args:\n                remote (string): Remote reference to upgrade to. The\n                    ``:latest`` tag is optional and is the default if omitted.\n                    Default: this plugin's name.\n\n            Returns:\n                A generator streaming the decoded API logs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create(self, name, plugin_data_dir, gzip=False):\n        self.client.api.create_plugin(name, plugin_data_dir, gzip)\n        return self.get(name)", "response": "Create a new plugin."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(self, name):\n        return self.prepare_model(self.client.api.inspect_plugin(name))", "response": "Gets a plugin s internal cache id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninstalling a plugin from the remote server.", "response": "def install(self, remote_name, local_name=None):\n        \"\"\"\n            Pull and install a plugin.\n\n            Args:\n                remote_name (string): Remote reference for the plugin to\n                    install. The ``:latest`` tag is optional, and is the\n                    default if omitted.\n                local_name (string): Local name for the pulled plugin.\n                    The ``:latest`` tag is optional, and is the default if\n                    omitted. Optional.\n\n            Returns:\n                (:py:class:`Plugin`): The installed plugin\n            Raises:\n                :py:class:`docker.errors.APIError`\n                    If the server returns an error.\n        \"\"\"\n        privileges = self.client.api.plugin_privileges(remote_name)\n        it = self.client.api.pull_plugin(remote_name, privileges, local_name)\n        for data in it:\n            pass\n        return self.get(local_name or remote_name)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlist the installed plugins on the server.", "response": "def list(self):\n        \"\"\"\n        List plugins installed on the server.\n\n        Returns:\n            (list of :py:class:`Plugin`): The plugins.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        resp = self.client.api.plugins()\n        return [self.prepare_model(r) for r in resp]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting data usage information.", "response": "def df(self):\n        \"\"\"\n        Get data usage information.\n\n        Returns:\n            (dict): A dictionary representing different resource categories\n            and their respective data usage.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        url = self._url('/system/df')\n        return self._result(self._get(url), True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting real - time events from the server. Similar to the docker events command.", "response": "def events(self, since=None, until=None, filters=None, decode=None):\n        \"\"\"\n        Get real-time events from the server. Similar to the ``docker events``\n        command.\n\n        Args:\n            since (UTC datetime or int): Get events from this point\n            until (UTC datetime or int): Get events until this point\n            filters (dict): Filter the events by event time, container or image\n            decode (bool): If set to true, stream will be decoded into dicts on\n                the fly. False by default.\n\n        Returns:\n            A :py:class:`docker.types.daemon.CancellableStream` generator\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n\n        Example:\n\n            >>> for event in client.events(decode=True)\n            ...   print(event)\n            {u'from': u'image/with:tag',\n             u'id': u'container-id',\n             u'status': u'start',\n             u'time': 1423339459}\n            ...\n\n            or\n\n            >>> events = client.events()\n            >>> for event in events:\n            ...   print(event)\n            >>> # and cancel from another thread\n            >>> events.close()\n        \"\"\"\n\n        if isinstance(since, datetime):\n            since = utils.datetime_to_timestamp(since)\n\n        if isinstance(until, datetime):\n            until = utils.datetime_to_timestamp(until)\n\n        if filters:\n            filters = utils.convert_filters(filters)\n\n        params = {\n            'since': since,\n            'until': until,\n            'filters': filters\n        }\n        url = self._url('/events')\n\n        response = self._get(url, params=params, stream=True, timeout=None)\n        stream = self._stream_helper(response, decode=decode)\n\n        return types.CancellableStream(stream, response)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef login(self, username, password=None, email=None, registry=None,\n              reauth=False, dockercfg_path=None):\n        \"\"\"\n        Authenticate with a registry. Similar to the ``docker login`` command.\n\n        Args:\n            username (str): The registry username\n            password (str): The plaintext password\n            email (str): The email for the registry account\n            registry (str): URL to the registry.  E.g.\n                ``https://index.docker.io/v1/``\n            reauth (bool): Whether or not to refresh existing authentication on\n                the Docker server.\n            dockercfg_path (str): Use a custom path for the Docker config file\n                (default ``$HOME/.docker/config.json`` if present,\n                otherwise``$HOME/.dockercfg``)\n\n        Returns:\n            (dict): The response from the login request\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n\n        # If we don't have any auth data so far, try reloading the config file\n        # one more time in case anything showed up in there.\n        # If dockercfg_path is passed check to see if the config file exists,\n        # if so load that config.\n        if dockercfg_path and os.path.exists(dockercfg_path):\n            self._auth_configs = auth.load_config(\n                dockercfg_path, credstore_env=self.credstore_env\n            )\n        elif not self._auth_configs or self._auth_configs.is_empty:\n            self._auth_configs = auth.load_config(\n                credstore_env=self.credstore_env\n            )\n\n        authcfg = self._auth_configs.resolve_authconfig(registry)\n        # If we found an existing auth config for this registry and username\n        # combination, we can return it immediately unless reauth is requested.\n        if authcfg and authcfg.get('username', None) == username \\\n                and not reauth:\n            return authcfg\n\n        req_data = {\n            'username': username,\n            'password': password,\n            'email': email,\n            'serveraddress': registry,\n        }\n\n        response = self._post_json(self._url('/auth'), data=req_data)\n        if response.status_code == 200:\n            self._auth_configs.add_auth(registry or auth.INDEX_NAME, req_data)\n        return self._result(response, json=True)", "response": "Authenticate with a registry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef version(self, api_version=True):\n        url = self._url(\"/version\", versioned_api=api_version)\n        return self._result(self._get(url), json=True)", "response": "Returns version information from the server. Similar to the docker\n            version command."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_service(\n            self, task_template, name=None, labels=None, mode=None,\n            update_config=None, networks=None, endpoint_config=None,\n            endpoint_spec=None, rollback_config=None\n    ):\n        \"\"\"\n        Create a service.\n\n        Args:\n            task_template (TaskTemplate): Specification of the task to start as\n                part of the new service.\n            name (string): User-defined name for the service. Optional.\n            labels (dict): A map of labels to associate with the service.\n                Optional.\n            mode (ServiceMode): Scheduling mode for the service (replicated\n                or global). Defaults to replicated.\n            update_config (UpdateConfig): Specification for the update strategy\n                of the service. Default: ``None``\n            rollback_config (RollbackConfig): Specification for the rollback\n                strategy of the service. Default: ``None``\n            networks (:py:class:`list`): List of network names or IDs to attach\n                the service to. Default: ``None``.\n            endpoint_spec (EndpointSpec): Properties that can be configured to\n                access and load balance a service. Default: ``None``.\n\n        Returns:\n            A dictionary containing an ``ID`` key for the newly created\n            service.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n\n        _check_api_features(\n            self._version, task_template, update_config, endpoint_spec,\n            rollback_config\n        )\n\n        url = self._url('/services/create')\n        headers = {}\n        image = task_template.get('ContainerSpec', {}).get('Image', None)\n        if image is None:\n            raise errors.DockerException(\n                'Missing mandatory Image key in ContainerSpec'\n            )\n        if mode and not isinstance(mode, dict):\n            mode = ServiceMode(mode)\n\n        registry, repo_name = auth.resolve_repository_name(image)\n        auth_header = auth.get_config_header(self, registry)\n        if auth_header:\n            headers['X-Registry-Auth'] = auth_header\n        if utils.version_lt(self._version, '1.25'):\n            networks = networks or task_template.pop('Networks', None)\n        data = {\n            'Name': name,\n            'Labels': labels,\n            'TaskTemplate': task_template,\n            'Mode': mode,\n            'Networks': utils.convert_service_networks(networks),\n            'EndpointSpec': endpoint_spec\n        }\n\n        if update_config is not None:\n            data['UpdateConfig'] = update_config\n\n        if rollback_config is not None:\n            data['RollbackConfig'] = rollback_config\n\n        return self._result(\n            self._post_json(url, data=data, headers=headers), True\n        )", "response": "Creates a new service in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef inspect_service(self, service, insert_defaults=None):\n        url = self._url('/services/{0}', service)\n        params = {}\n        if insert_defaults is not None:\n            if utils.version_lt(self._version, '1.29'):\n                raise errors.InvalidVersion(\n                    'insert_defaults is not supported in API version < 1.29'\n                )\n            params['insertDefaults'] = insert_defaults\n\n        return self._result(self._get(url, params=params), True)", "response": "Return information about a service."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving information about a task.", "response": "def inspect_task(self, task):\n        \"\"\"\n        Retrieve information about a task.\n\n        Args:\n            task (str): Task ID\n\n        Returns:\n            (dict): Information about the task.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        url = self._url('/tasks/{0}', task)\n        return self._result(self._get(url), True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_service(self, service):\n\n        url = self._url('/services/{0}', service)\n        resp = self._delete(url)\n        self._raise_for_status(resp)\n        return True", "response": "Stop and remove a service."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef service_logs(self, service, details=False, follow=False, stdout=False,\n                     stderr=False, since=0, timestamps=False, tail='all',\n                     is_tty=None):\n        \"\"\"\n            Get log stream for a service.\n            Note: This endpoint works only for services with the ``json-file``\n            or ``journald`` logging drivers.\n\n            Args:\n                service (str): ID or name of the service\n                details (bool): Show extra details provided to logs.\n                    Default: ``False``\n                follow (bool): Keep connection open to read logs as they are\n                    sent by the Engine. Default: ``False``\n                stdout (bool): Return logs from ``stdout``. Default: ``False``\n                stderr (bool): Return logs from ``stderr``. Default: ``False``\n                since (int): UNIX timestamp for the logs staring point.\n                    Default: 0\n                timestamps (bool): Add timestamps to every log line.\n                tail (string or int): Number of log lines to be returned,\n                    counting from the current end of the logs. Specify an\n                    integer or ``'all'`` to output all log lines.\n                    Default: ``all``\n                is_tty (bool): Whether the service's :py:class:`ContainerSpec`\n                    enables the TTY option. If omitted, the method will query\n                    the Engine for the information, causing an additional\n                    roundtrip.\n\n            Returns (generator): Logs for the service.\n        \"\"\"\n        params = {\n            'details': details,\n            'follow': follow,\n            'stdout': stdout,\n            'stderr': stderr,\n            'since': since,\n            'timestamps': timestamps,\n            'tail': tail\n        }\n\n        url = self._url('/services/{0}/logs', service)\n        res = self._get(url, params=params, stream=True)\n        if is_tty is None:\n            is_tty = self.inspect_service(\n                service\n            )['Spec']['TaskTemplate']['ContainerSpec'].get('TTY', False)\n        return self._get_result_tty(True, res, is_tty)", "response": "Returns a generator that returns the logs for a given service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating a service in the cluster.", "response": "def update_service(self, service, version, task_template=None, name=None,\n                       labels=None, mode=None, update_config=None,\n                       networks=None, endpoint_config=None,\n                       endpoint_spec=None, fetch_current_spec=False,\n                       rollback_config=None):\n        \"\"\"\n        Update a service.\n\n        Args:\n            service (string): A service identifier (either its name or service\n                ID).\n            version (int): The version number of the service object being\n                updated. This is required to avoid conflicting writes.\n            task_template (TaskTemplate): Specification of the updated task to\n                start as part of the service.\n            name (string): New name for the service. Optional.\n            labels (dict): A map of labels to associate with the service.\n                Optional.\n            mode (ServiceMode): Scheduling mode for the service (replicated\n                or global). Defaults to replicated.\n            update_config (UpdateConfig): Specification for the update strategy\n                of the service. Default: ``None``.\n            rollback_config (RollbackConfig): Specification for the rollback\n                strategy of the service. Default: ``None``\n            networks (:py:class:`list`): List of network names or IDs to attach\n                the service to. Default: ``None``.\n            endpoint_spec (EndpointSpec): Properties that can be configured to\n                access and load balance a service. Default: ``None``.\n            fetch_current_spec (boolean): Use the undefined settings from the\n                current specification of the service. Default: ``False``\n\n        Returns:\n            A dictionary containing a ``Warnings`` key.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n\n        _check_api_features(\n            self._version, task_template, update_config, endpoint_spec,\n            rollback_config\n        )\n\n        if fetch_current_spec:\n            inspect_defaults = True\n            if utils.version_lt(self._version, '1.29'):\n                inspect_defaults = None\n            current = self.inspect_service(\n                service, insert_defaults=inspect_defaults\n            )['Spec']\n\n        else:\n            current = {}\n\n        url = self._url('/services/{0}/update', service)\n        data = {}\n        headers = {}\n\n        data['Name'] = current.get('Name') if name is None else name\n\n        data['Labels'] = current.get('Labels') if labels is None else labels\n\n        if mode is not None:\n            if not isinstance(mode, dict):\n                mode = ServiceMode(mode)\n            data['Mode'] = mode\n        else:\n            data['Mode'] = current.get('Mode')\n\n        data['TaskTemplate'] = _merge_task_template(\n            current.get('TaskTemplate', {}), task_template\n        )\n\n        container_spec = data['TaskTemplate'].get('ContainerSpec', {})\n        image = container_spec.get('Image', None)\n        if image is not None:\n            registry, repo_name = auth.resolve_repository_name(image)\n            auth_header = auth.get_config_header(self, registry)\n            if auth_header:\n                headers['X-Registry-Auth'] = auth_header\n\n        if update_config is not None:\n            data['UpdateConfig'] = update_config\n        else:\n            data['UpdateConfig'] = current.get('UpdateConfig')\n\n        if rollback_config is not None:\n            data['RollbackConfig'] = rollback_config\n        else:\n            data['RollbackConfig'] = current.get('RollbackConfig')\n\n        if networks is not None:\n            converted_networks = utils.convert_service_networks(networks)\n            if utils.version_lt(self._version, '1.25'):\n                data['Networks'] = converted_networks\n            else:\n                data['TaskTemplate']['Networks'] = converted_networks\n        elif utils.version_lt(self._version, '1.25'):\n            data['Networks'] = current.get('Networks')\n        elif data['TaskTemplate'].get('Networks') is None:\n            current_task_template = current.get('TaskTemplate', {})\n            current_networks = current_task_template.get('Networks')\n            if current_networks is None:\n                current_networks = current.get('Networks')\n            if current_networks is not None:\n                data['TaskTemplate']['Networks'] = current_networks\n\n        if endpoint_spec is not None:\n            data['EndpointSpec'] = endpoint_spec\n        else:\n            data['EndpointSpec'] = current.get('EndpointSpec')\n\n        resp = self._post_json(\n            url, data=data, params={'version': version}, headers=headers\n        )\n        return self._result(resp, json=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a suitable APIError from requests. exceptions. HTTPError.", "response": "def create_api_error_from_http_exception(e):\n    \"\"\"\n    Create a suitable APIError from requests.exceptions.HTTPError.\n    \"\"\"\n    response = e.response\n    try:\n        explanation = response.json()['message']\n    except ValueError:\n        explanation = (response.content or '').strip()\n    cls = APIError\n    if response.status_code == 404:\n        if explanation and ('No such image' in str(explanation) or\n                            'not found: does not exist or no pull access'\n                            in str(explanation) or\n                            'repository does not exist' in str(explanation)):\n            cls = ImageNotFound\n        else:\n            cls = NotFound\n    raise cls(e, response=response, explanation=explanation)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_dict(config):\n        '''\n        Instantiate a new ProxyConfig from a dictionary that represents a\n        client configuration, as described in `the documentation`_.\n\n        .. _the documentation:\n            https://docs.docker.com/network/proxy/#configure-the-docker-client\n        '''\n        return ProxyConfig(\n            http=config.get('httpProxy'),\n            https=config.get('httpsProxy'),\n            ftp=config.get('ftpProxy'),\n            no_proxy=config.get('noProxy'),\n        )", "response": "Instantiate a new ProxyConfig from a dictionary that represents a\n        client configuration."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dictionary representing the environment variables used to get the proxy settings.", "response": "def get_environment(self):\n        '''\n        Return a dictionary representing the environment variables used to\n        set the proxy settings.\n        '''\n        env = {}\n        if self.http:\n            env['http_proxy'] = env['HTTP_PROXY'] = self.http\n        if self.https:\n            env['https_proxy'] = env['HTTPS_PROXY'] = self.https\n        if self.ftp:\n            env['ftp_proxy'] = env['FTP_PROXY'] = self.ftp\n        if self.no_proxy:\n            env['no_proxy'] = env['NO_PROXY'] = self.no_proxy\n        return env"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives a list of strings representing environment variables prepend the environment variables corresponding to the proxy settings.", "response": "def inject_proxy_environment(self, environment):\n        '''\n        Given a list of strings representing environment variables, prepend the\n        environment variables corresponding to the proxy settings.\n        '''\n        if not self:\n            return environment\n\n        proxy_env = format_environment(self.get_environment())\n        if not environment:\n            return proxy_env\n        # It is important to prepend our variables, because we want the\n        # variables defined in \"environment\" to take precedence.\n        return proxy_env + environment"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef order(self):\n        # non-GA releases should appear before GA releases\n        # Order: tp -> beta -> rc -> GA\n        if self.stage:\n            for st in STAGES:\n                if st in self.stage:\n                    stage = (STAGES.index(st), self.stage)\n                    break\n        else:\n            stage = (len(STAGES),)\n\n        return (int(self.major), int(self.minor), int(self.patch)) + stage", "response": "Return a representation that allows this object to be sorted\n        correctly with the default comparator."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove(self, force=False):\n        return self.client.api.remove_volume(self.id, force=force)", "response": "Remove this volume.\n\n        Args:\n            force (bool): Force removal of volumes that were already removed\n                out of band by the volume driver plugin.\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If volume failed to remove."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(self, name=None, **kwargs):\n        obj = self.client.api.create_volume(name, **kwargs)\n        return self.prepare_model(obj)", "response": "Create a new volume."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, volume_id):\n        return self.prepare_model(self.client.api.inspect_volume(volume_id))", "response": "Get a specific volume."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list(self, **kwargs):\n        resp = self.client.api.volumes(**kwargs)\n        if not resp.get('Volumes'):\n            return []\n        return [self.prepare_model(obj) for obj in resp['Volumes']]", "response": "List volumes. Similar to the docker volume ls command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fnmatch(name, pat):\n\n    name = name.lower()\n    pat = pat.lower()\n    return fnmatchcase(name, pat)", "response": "Test whether FILENAME matches PATTERN."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, config_id):\n        return self.prepare_model(self.client.api.inspect_config(config_id))", "response": "Get a config.\n\n        Args:\n            config_id (str): Config ID.\n\n        Returns:\n            (:py:class:`Config`): The config.\n\n        Raises:\n            :py:class:`docker.errors.NotFound`\n                If the config does not exist.\n            :py:class:`docker.errors.APIError`\n                If the server returns an error."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_connection(self, *args, **kwargs):\n        conn = super(SSLHTTPAdapter, self).get_connection(*args, **kwargs)\n        if conn.assert_hostname != self.assert_hostname:\n            conn.assert_hostname = self.assert_hostname\n        return conn", "response": "Override the default get_connection method to set the assert_hostname to the value of the assert_hostname attribute."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef exclude_paths(root, patterns, dockerfile=None):\n\n    if dockerfile is None:\n        dockerfile = 'Dockerfile'\n\n    patterns.append('!' + dockerfile)\n    pm = PatternMatcher(patterns)\n    return set(pm.walk(root))", "response": "Returns a set of all paths that do not match any of the patterns in the root directory path."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_image(self, image, chunk_size=DEFAULT_DATA_CHUNK_SIZE):\n        res = self._get(self._url(\"/images/{0}/get\", image), stream=True)\n        return self._stream_raw_result(res, chunk_size, False)", "response": "Get a tarball of an image. Similar to the docker get command."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef history(self, image):\n        res = self._get(self._url(\"/images/{0}/history\", image))\n        return self._result(res, True)", "response": "Show the history of an image."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef images(self, name=None, quiet=False, all=False, filters=None):\n        params = {\n            'filter': name,\n            'only_ids': 1 if quiet else 0,\n            'all': 1 if all else 0,\n        }\n        if filters:\n            params['filters'] = utils.convert_filters(filters)\n        res = self._result(self._get(self._url(\"/images/json\"), params=params),\n                           True)\n        if quiet:\n            return [x['Id'] for x in res]\n        return res", "response": "List images belonging to a repository."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nimport an image from a file or URL.", "response": "def import_image(self, src=None, repository=None, tag=None, image=None,\n                     changes=None, stream_src=False):\n        \"\"\"\n        Import an image. Similar to the ``docker import`` command.\n\n        If ``src`` is a string or unicode string, it will first be treated as a\n        path to a tarball on the local system. If there is an error reading\n        from that file, ``src`` will be treated as a URL instead to fetch the\n        image from. You can also pass an open file handle as ``src``, in which\n        case the data will be read from that file.\n\n        If ``src`` is unset but ``image`` is set, the ``image`` parameter will\n        be taken as the name of an existing image to import from.\n\n        Args:\n            src (str or file): Path to tarfile, URL, or file-like object\n            repository (str): The repository to create\n            tag (str): The tag to apply\n            image (str): Use another image like the ``FROM`` Dockerfile\n                parameter\n        \"\"\"\n        if not (src or image):\n            raise errors.DockerException(\n                'Must specify src or image to import from'\n            )\n        u = self._url('/images/create')\n\n        params = _import_image_params(\n            repository, tag, image,\n            src=(src if isinstance(src, six.string_types) else None),\n            changes=changes\n        )\n        headers = {'Content-Type': 'application/tar'}\n\n        if image or params.get('fromSrc') != '-':  # from image or URL\n            return self._result(\n                self._post(u, data=None, params=params)\n            )\n        elif isinstance(src, six.string_types):  # from file path\n            with open(src, 'rb') as f:\n                return self._result(\n                    self._post(\n                        u, data=f, params=params, headers=headers, timeout=None\n                    )\n                )\n        else:  # from raw data\n            if stream_src:\n                headers['Transfer-Encoding'] = 'chunked'\n            return self._result(\n                self._post(u, data=src, params=params, headers=headers)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nliking :py:meth:`~docker.api.image.ImageApiMixin.import_image`, but allows importing in-memory bytes data. Args: data (bytes collection): Bytes collection containing valid tar data repository (str): The repository to create tag (str): The tag to apply", "response": "def import_image_from_data(self, data, repository=None, tag=None,\n                               changes=None):\n        \"\"\"\n        Like :py:meth:`~docker.api.image.ImageApiMixin.import_image`, but\n        allows importing in-memory bytes data.\n\n        Args:\n            data (bytes collection): Bytes collection containing valid tar data\n            repository (str): The repository to create\n            tag (str): The tag to apply\n        \"\"\"\n\n        u = self._url('/images/create')\n        params = _import_image_params(\n            repository, tag, src='-', changes=changes\n        )\n        headers = {'Content-Type': 'application/tar'}\n        return self._result(\n            self._post(\n                u, data=data, params=params, headers=headers, timeout=None\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_image_from_file(self, filename, repository=None, tag=None,\n                               changes=None):\n        \"\"\"\n        Like :py:meth:`~docker.api.image.ImageApiMixin.import_image`, but only\n        supports importing from a tar file on disk.\n\n        Args:\n            filename (str): Full path to a tar file.\n            repository (str): The repository to create\n            tag (str): The tag to apply\n\n        Raises:\n            IOError: File does not exist.\n        \"\"\"\n\n        return self.import_image(\n            src=filename, repository=repository, tag=tag, changes=changes\n        )", "response": "Imports an image from a tar file on disk."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef import_image_from_url(self, url, repository=None, tag=None,\n                              changes=None):\n        \"\"\"\n        Like :py:meth:`~docker.api.image.ImageApiMixin.import_image`, but only\n        supports importing from a URL.\n\n        Args:\n            url (str): A URL pointing to a tar file.\n            repository (str): The repository to create\n            tag (str): The tag to apply\n        \"\"\"\n        return self.import_image(\n            src=url, repository=repository, tag=tag, changes=changes\n        )", "response": "Imports an image from a URL."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlike :py:meth:`~docker.api.image.ImageApiMixin.import_image`, but only supports importing from another image, like the ``FROM`` Dockerfile parameter. Args: image (str): Image name to import from repository (str): The repository to create tag (str): The tag to apply", "response": "def import_image_from_image(self, image, repository=None, tag=None,\n                                changes=None):\n        \"\"\"\n        Like :py:meth:`~docker.api.image.ImageApiMixin.import_image`, but only\n        supports importing from another image, like the ``FROM`` Dockerfile\n        parameter.\n\n        Args:\n            image (str): Image name to import from\n            repository (str): The repository to create\n            tag (str): The tag to apply\n        \"\"\"\n        return self.import_image(\n            image=image, repository=repository, tag=tag, changes=changes\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef inspect_image(self, image):\n        return self._result(\n            self._get(self._url(\"/images/{0}/json\", image)), True\n        )", "response": "Get detailed information about an image. Similar to the docker inspect command but only for images."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets image digest and platform information by contacting the registry.", "response": "def inspect_distribution(self, image, auth_config=None):\n        \"\"\"\n        Get image digest and platform information by contacting the registry.\n\n        Args:\n            image (str): The image name to inspect\n            auth_config (dict): Override the credentials that are found in the\n                config for this request.  ``auth_config`` should contain the\n                ``username`` and ``password`` keys to be valid.\n\n        Returns:\n            (dict): A dict containing distribution data\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        registry, _ = auth.resolve_repository_name(image)\n\n        headers = {}\n        if auth_config is None:\n            header = auth.get_config_header(self, registry)\n            if header:\n                headers['X-Registry-Auth'] = header\n        else:\n            log.debug('Sending supplied auth config')\n            headers['X-Registry-Auth'] = auth.encode_header(auth_config)\n\n        url = self._url(\"/distribution/{0}/json\", image)\n\n        return self._result(\n            self._get(url, headers=headers), True\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_image(self, data, quiet=None):\n        params = {}\n\n        if quiet is not None:\n            if utils.version_lt(self._version, '1.23'):\n                raise errors.InvalidVersion(\n                    'quiet is not supported in API version < 1.23'\n                )\n            params['quiet'] = quiet\n\n        res = self._post(\n            self._url(\"/images/load\"), data=data, params=params, stream=True\n        )\n        if utils.version_gte(self._version, '1.23'):\n            return self._stream_helper(res, decode=True)\n\n        self._raise_for_status(res)", "response": "Load an image from the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pull(self, repository, tag=None, stream=False, auth_config=None,\n             decode=False, platform=None):\n        \"\"\"\n        Pulls an image. Similar to the ``docker pull`` command.\n\n        Args:\n            repository (str): The repository to pull\n            tag (str): The tag to pull\n            stream (bool): Stream the output as a generator. Make sure to\n                consume the generator, otherwise pull might get cancelled.\n            auth_config (dict): Override the credentials that are found in the\n                config for this request.  ``auth_config`` should contain the\n                ``username`` and ``password`` keys to be valid.\n            decode (bool): Decode the JSON data from the server into dicts.\n                Only applies with ``stream=True``\n            platform (str): Platform in the format ``os[/arch[/variant]]``\n\n        Returns:\n            (generator or str): The output\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n\n        Example:\n\n            >>> for line in cli.pull('busybox', stream=True, decode=True):\n            ...     print(json.dumps(line, indent=4))\n            {\n                \"status\": \"Pulling image (latest) from busybox\",\n                \"progressDetail\": {},\n                \"id\": \"e72ac664f4f0\"\n            }\n            {\n                \"status\": \"Pulling image (latest) from busybox, endpoint: ...\",\n                \"progressDetail\": {},\n                \"id\": \"e72ac664f4f0\"\n            }\n\n        \"\"\"\n        if not tag:\n            repository, tag = utils.parse_repository_tag(repository)\n        registry, repo_name = auth.resolve_repository_name(repository)\n\n        params = {\n            'tag': tag,\n            'fromImage': repository\n        }\n        headers = {}\n\n        if auth_config is None:\n            header = auth.get_config_header(self, registry)\n            if header:\n                headers['X-Registry-Auth'] = header\n        else:\n            log.debug('Sending supplied auth config')\n            headers['X-Registry-Auth'] = auth.encode_header(auth_config)\n\n        if platform is not None:\n            if utils.version_lt(self._version, '1.32'):\n                raise errors.InvalidVersion(\n                    'platform was only introduced in API version 1.32'\n                )\n            params['platform'] = platform\n\n        response = self._post(\n            self._url('/images/create'), params=params, headers=headers,\n            stream=stream, timeout=None\n        )\n\n        self._raise_for_status(response)\n\n        if stream:\n            return self._stream_helper(response, decode=decode)\n\n        return self._result(response)", "response": "Pulls an image from the specified repository."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef push(self, repository, tag=None, stream=False, auth_config=None,\n             decode=False):\n        \"\"\"\n        Push an image or a repository to the registry. Similar to the ``docker\n        push`` command.\n\n        Args:\n            repository (str): The repository to push to\n            tag (str): An optional tag to push\n            stream (bool): Stream the output as a blocking generator\n            auth_config (dict): Override the credentials that are found in the\n                config for this request.  ``auth_config`` should contain the\n                ``username`` and ``password`` keys to be valid.\n            decode (bool): Decode the JSON data from the server into dicts.\n                Only applies with ``stream=True``\n\n        Returns:\n            (generator or str): The output from the server.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n\n        Example:\n            >>> for line in cli.push('yourname/app', stream=True, decode=True):\n            ...   print(line)\n            {'status': 'Pushing repository yourname/app (1 tags)'}\n            {'status': 'Pushing','progressDetail': {}, 'id': '511136ea3c5a'}\n            {'status': 'Image already pushed, skipping', 'progressDetail':{},\n             'id': '511136ea3c5a'}\n            ...\n\n        \"\"\"\n        if not tag:\n            repository, tag = utils.parse_repository_tag(repository)\n        registry, repo_name = auth.resolve_repository_name(repository)\n        u = self._url(\"/images/{0}/push\", repository)\n        params = {\n            'tag': tag\n        }\n        headers = {}\n\n        if auth_config is None:\n            header = auth.get_config_header(self, registry)\n            if header:\n                headers['X-Registry-Auth'] = header\n        else:\n            log.debug('Sending supplied auth config')\n            headers['X-Registry-Auth'] = auth.encode_header(auth_config)\n\n        response = self._post_json(\n            u, None, headers=headers, stream=stream, params=params\n        )\n\n        self._raise_for_status(response)\n\n        if stream:\n            return self._stream_helper(response, decode=decode)\n\n        return self._result(response)", "response": "Push an image or a repository to the registry."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_image(self, image, force=False, noprune=False):\n        params = {'force': force, 'noprune': noprune}\n        res = self._delete(self._url(\"/images/{0}\", image), params=params)\n        return self._result(res, True)", "response": "Remove an image from the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef search(self, term):\n        return self._result(\n            self._get(self._url(\"/images/search\"), params={'term': term}),\n            True\n        )", "response": "Search for images on Docker Hub. Similar to the docker search image command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntagging an image into a repository.", "response": "def tag(self, image, repository, tag=None, force=False):\n        \"\"\"\n        Tag an image into a repository. Similar to the ``docker tag`` command.\n\n        Args:\n            image (str): The image to tag\n            repository (str): The repository to set for the tag\n            tag (str): The tag name\n            force (bool): Force\n\n        Returns:\n            (bool): ``True`` if successful\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n\n        Example:\n\n            >>> client.tag('ubuntu', 'localhost:5000/ubuntu', 'latest',\n                           force=True)\n        \"\"\"\n        params = {\n            'tag': tag,\n            'repo': repository,\n            'force': 1 if force else 0\n        }\n        url = self._url(\"/images/{0}/tag\", image)\n        res = self._post(url, params=params)\n        self._raise_for_status(res)\n        return res.status_code == 201"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef attach(self, container, stdout=True, stderr=True,\n               stream=False, logs=False, demux=False):\n        \"\"\"\n        Attach to a container.\n\n        The ``.logs()`` function is a wrapper around this method, which you can\n        use instead if you want to fetch/stream container output without first\n        retrieving the entire backlog.\n\n        Args:\n            container (str): The container to attach to.\n            stdout (bool): Include stdout.\n            stderr (bool): Include stderr.\n            stream (bool): Return container output progressively as an iterator\n                of strings, rather than a single string.\n            logs (bool): Include the container's previous output.\n            demux (bool): Keep stdout and stderr separate.\n\n        Returns:\n            By default, the container's output as a single string (two if\n            ``demux=True``: one for stdout and one for stderr).\n\n            If ``stream=True``, an iterator of output strings. If\n            ``demux=True``, two iterators are returned: one for stdout and one\n            for stderr.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        params = {\n            'logs': logs and 1 or 0,\n            'stdout': stdout and 1 or 0,\n            'stderr': stderr and 1 or 0,\n            'stream': stream and 1 or 0\n        }\n\n        headers = {\n            'Connection': 'Upgrade',\n            'Upgrade': 'tcp'\n        }\n\n        u = self._url(\"/containers/{0}/attach\", container)\n        response = self._post(u, headers=headers, params=params, stream=True)\n\n        output = self._read_from_socket(\n            response, stream, self._check_is_tty(container), demux=demux)\n\n        if stream:\n            return CancellableStream(output, response)\n        else:\n            return output", "response": "Attaches a container to a specific container."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlikes attach but returns the underlying socket - like object for the HTTP request.", "response": "def attach_socket(self, container, params=None, ws=False):\n        \"\"\"\n        Like ``attach``, but returns the underlying socket-like object for the\n        HTTP request.\n\n        Args:\n            container (str): The container to attach to.\n            params (dict): Dictionary of request parameters (e.g. ``stdout``,\n                ``stderr``, ``stream``).\n                For ``detachKeys``, ~/.docker/config.json is used by default.\n            ws (bool): Use websockets instead of raw HTTP.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        if params is None:\n            params = {\n                'stdout': 1,\n                'stderr': 1,\n                'stream': 1\n            }\n\n        if 'detachKeys' not in params \\\n                and 'detachKeys' in self._general_configs:\n\n            params['detachKeys'] = self._general_configs['detachKeys']\n\n        if ws:\n            return self._attach_websocket(container, params)\n\n        headers = {\n            'Connection': 'Upgrade',\n            'Upgrade': 'tcp'\n        }\n\n        u = self._url(\"/containers/{0}/attach\", container)\n        return self._get_raw_response_socket(\n            self.post(\n                u, None, params=self._attach_params(params), stream=True,\n                headers=headers\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef commit(self, container, repository=None, tag=None, message=None,\n               author=None, changes=None, conf=None):\n        \"\"\"\n        Commit a container to an image. Similar to the ``docker commit``\n        command.\n\n        Args:\n            container (str): The image hash of the container\n            repository (str): The repository to push the image to\n            tag (str): The tag to push\n            message (str): A commit message\n            author (str): The name of the author\n            changes (str): Dockerfile instructions to apply while committing\n            conf (dict): The configuration for the container. See the\n                `Engine API documentation\n                <https://docs.docker.com/reference/api/docker_remote_api/>`_\n                for full details.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        params = {\n            'container': container,\n            'repo': repository,\n            'tag': tag,\n            'comment': message,\n            'author': author,\n            'changes': changes\n        }\n        u = self._url(\"/commit\")\n        return self._result(\n            self._post_json(u, data=conf, params=params), json=True\n        )", "response": "Commits a container to an image. Similar to the docker commit command."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef containers(self, quiet=False, all=False, trunc=False, latest=False,\n                   since=None, before=None, limit=-1, size=False,\n                   filters=None):\n        \"\"\"\n        List containers. Similar to the ``docker ps`` command.\n\n        Args:\n            quiet (bool): Only display numeric Ids\n            all (bool): Show all containers. Only running containers are shown\n                by default\n            trunc (bool): Truncate output\n            latest (bool): Show only the latest created container, include\n                non-running ones.\n            since (str): Show only containers created since Id or Name, include\n                non-running ones\n            before (str): Show only container created before Id or Name,\n                include non-running ones\n            limit (int): Show `limit` last created containers, include\n                non-running ones\n            size (bool): Display sizes\n            filters (dict): Filters to be processed on the image list.\n                Available filters:\n\n                - `exited` (int): Only containers with specified exit code\n                - `status` (str): One of ``restarting``, ``running``,\n                    ``paused``, ``exited``\n                - `label` (str): format either ``\"key\"`` or ``\"key=value\"``\n                - `id` (str): The id of the container.\n                - `name` (str): The name of the container.\n                - `ancestor` (str): Filter by container ancestor. Format of\n                    ``<image-name>[:tag]``, ``<image-id>``, or\n                    ``<image@digest>``.\n                - `before` (str): Only containers created before a particular\n                    container. Give the container name or id.\n                - `since` (str): Only containers created after a particular\n                    container. Give container name or id.\n\n                A comprehensive list can be found in the documentation for\n                `docker ps\n                <https://docs.docker.com/engine/reference/commandline/ps>`_.\n\n        Returns:\n            A list of dicts, one per container\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        params = {\n            'limit': 1 if latest else limit,\n            'all': 1 if all else 0,\n            'size': 1 if size else 0,\n            'trunc_cmd': 1 if trunc else 0,\n            'since': since,\n            'before': before\n        }\n        if filters:\n            params['filters'] = utils.convert_filters(filters)\n        u = self._url(\"/containers/json\")\n        res = self._result(self._get(u, params=params), True)\n\n        if quiet:\n            return [{'Id': x['Id']} for x in res]\n        if trunc:\n            for x in res:\n                x['Id'] = x['Id'][:12]\n        return res", "response": "List containers in the system."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a Docker container.", "response": "def create_container(self, image, command=None, hostname=None, user=None,\n                         detach=False, stdin_open=False, tty=False, ports=None,\n                         environment=None, volumes=None,\n                         network_disabled=False, name=None, entrypoint=None,\n                         working_dir=None, domainname=None, host_config=None,\n                         mac_address=None, labels=None, stop_signal=None,\n                         networking_config=None, healthcheck=None,\n                         stop_timeout=None, runtime=None,\n                         use_config_proxy=False):\n        \"\"\"\n        Creates a container. Parameters are similar to those for the ``docker\n        run`` command except it doesn't support the attach options (``-a``).\n\n        The arguments that are passed directly to this function are\n        host-independent configuration options. Host-specific configuration\n        is passed with the `host_config` argument. You'll normally want to\n        use this method in combination with the :py:meth:`create_host_config`\n        method to generate ``host_config``.\n\n        **Port bindings**\n\n        Port binding is done in two parts: first, provide a list of ports to\n        open inside the container with the ``ports`` parameter, then declare\n        bindings with the ``host_config`` parameter. For example:\n\n        .. code-block:: python\n\n            container_id = cli.create_container(\n                'busybox', 'ls', ports=[1111, 2222],\n                host_config=cli.create_host_config(port_bindings={\n                    1111: 4567,\n                    2222: None\n                })\n            )\n\n\n        You can limit the host address on which the port will be exposed like\n        such:\n\n        .. code-block:: python\n\n            cli.create_host_config(port_bindings={1111: ('127.0.0.1', 4567)})\n\n        Or without host port assignment:\n\n        .. code-block:: python\n\n            cli.create_host_config(port_bindings={1111: ('127.0.0.1',)})\n\n        If you wish to use UDP instead of TCP (default), you need to declare\n        ports as such in both the config and host config:\n\n        .. code-block:: python\n\n            container_id = cli.create_container(\n                'busybox', 'ls', ports=[(1111, 'udp'), 2222],\n                host_config=cli.create_host_config(port_bindings={\n                    '1111/udp': 4567, 2222: None\n                })\n            )\n\n        To bind multiple host ports to a single container port, use the\n        following syntax:\n\n        .. code-block:: python\n\n            cli.create_host_config(port_bindings={\n                1111: [1234, 4567]\n            })\n\n        You can also bind multiple IPs to a single container port:\n\n        .. code-block:: python\n\n            cli.create_host_config(port_bindings={\n                1111: [\n                    ('192.168.0.100', 1234),\n                    ('192.168.0.101', 1234)\n                ]\n            })\n\n        **Using volumes**\n\n        Volume declaration is done in two parts. Provide a list of\n        paths to use as mountpoints inside the container with the\n        ``volumes`` parameter, and declare mappings from paths on the host\n        in the ``host_config`` section.\n\n        .. code-block:: python\n\n            container_id = cli.create_container(\n                'busybox', 'ls', volumes=['/mnt/vol1', '/mnt/vol2'],\n                host_config=cli.create_host_config(binds={\n                    '/home/user1/': {\n                        'bind': '/mnt/vol2',\n                        'mode': 'rw',\n                    },\n                    '/var/www': {\n                        'bind': '/mnt/vol1',\n                        'mode': 'ro',\n                    }\n                })\n            )\n\n        You can alternatively specify binds as a list. This code is equivalent\n        to the example above:\n\n        .. code-block:: python\n\n            container_id = cli.create_container(\n                'busybox', 'ls', volumes=['/mnt/vol1', '/mnt/vol2'],\n                host_config=cli.create_host_config(binds=[\n                    '/home/user1/:/mnt/vol2',\n                    '/var/www:/mnt/vol1:ro',\n                ])\n            )\n\n        **Networking**\n\n        You can specify networks to connect the container to by using the\n        ``networking_config`` parameter. At the time of creation, you can\n        only connect a container to a single networking, but you\n        can create more connections by using\n        :py:meth:`~connect_container_to_network`.\n\n        For example:\n\n        .. code-block:: python\n\n            networking_config = docker_client.create_networking_config({\n                'network1': docker_client.create_endpoint_config(\n                    ipv4_address='172.28.0.124',\n                    aliases=['foo', 'bar'],\n                    links=['container2']\n                )\n            })\n\n            ctnr = docker_client.create_container(\n                img, command, networking_config=networking_config\n            )\n\n        Args:\n            image (str): The image to run\n            command (str or list): The command to be run in the container\n            hostname (str): Optional hostname for the container\n            user (str or int): Username or UID\n            detach (bool): Detached mode: run container in the background and\n                return container ID\n            stdin_open (bool): Keep STDIN open even if not attached\n            tty (bool): Allocate a pseudo-TTY\n            ports (list of ints): A list of port numbers\n            environment (dict or list): A dictionary or a list of strings in\n                the following format ``[\"PASSWORD=xxx\"]`` or\n                ``{\"PASSWORD\": \"xxx\"}``.\n            volumes (str or list): List of paths inside the container to use\n                as volumes.\n            network_disabled (bool): Disable networking\n            name (str): A name for the container\n            entrypoint (str or list): An entrypoint\n            working_dir (str): Path to the working directory\n            domainname (str): The domain name to use for the container\n            host_config (dict): A dictionary created with\n                :py:meth:`create_host_config`.\n            mac_address (str): The Mac Address to assign the container\n            labels (dict or list): A dictionary of name-value labels (e.g.\n                ``{\"label1\": \"value1\", \"label2\": \"value2\"}``) or a list of\n                names of labels to set with empty values (e.g.\n                ``[\"label1\", \"label2\"]``)\n            stop_signal (str): The stop signal to use to stop the container\n                (e.g. ``SIGINT``).\n            stop_timeout (int): Timeout to stop the container, in seconds.\n                Default: 10\n            networking_config (dict): A networking configuration generated\n                by :py:meth:`create_networking_config`.\n            runtime (str): Runtime to use with this container.\n            healthcheck (dict): Specify a test to perform to check that the\n                container is healthy.\n            use_config_proxy (bool): If ``True``, and if the docker client\n                configuration file (``~/.docker/config.json`` by default)\n                contains a proxy configuration, the corresponding environment\n                variables will be set in the container being created.\n\n        Returns:\n            A dictionary with an image 'Id' key and a 'Warnings' key.\n\n        Raises:\n            :py:class:`docker.errors.ImageNotFound`\n                If the specified image does not exist.\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        if isinstance(volumes, six.string_types):\n            volumes = [volumes, ]\n\n        if isinstance(environment, dict):\n            environment = utils.utils.format_environment(environment)\n\n        if use_config_proxy:\n            environment = self._proxy_configs.inject_proxy_environment(\n                environment\n            )\n\n        config = self.create_container_config(\n            image, command, hostname, user, detach, stdin_open, tty,\n            ports, environment, volumes,\n            network_disabled, entrypoint, working_dir, domainname,\n            host_config, mac_address, labels,\n            stop_signal, networking_config, healthcheck,\n            stop_timeout, runtime\n        )\n        return self.create_container_from_config(config, name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_host_config(self, *args, **kwargs):\n        if not kwargs:\n            kwargs = {}\n        if 'version' in kwargs:\n            raise TypeError(\n                \"create_host_config() got an unexpected \"\n                \"keyword argument 'version'\"\n            )\n        kwargs['version'] = self._version\n        return HostConfig(*args, **kwargs)", "response": "Create a dictionary for the host_config argument to\n           ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef diff(self, container):\n        return self._result(\n            self._get(self._url(\"/containers/{0}/changes\", container)), True\n        )", "response": "Inspect changes on a container s filesystem."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef export(self, container, chunk_size=DEFAULT_DATA_CHUNK_SIZE):\n        res = self._get(\n            self._url(\"/containers/{0}/export\", container), stream=True\n        )\n        return self._stream_raw_result(res, chunk_size, False)", "response": "Export the contents of a container as a tar archive."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_archive(self, container, path, chunk_size=DEFAULT_DATA_CHUNK_SIZE):\n        params = {\n            'path': path\n        }\n        url = self._url('/containers/{0}/archive', container)\n        res = self._get(url, params=params, stream=True)\n        self._raise_for_status(res)\n        encoded_stat = res.headers.get('x-docker-container-path-stat')\n        return (\n            self._stream_raw_result(res, chunk_size, False),\n            utils.decode_json_header(encoded_stat) if encoded_stat else None\n        )", "response": "Retrieve a file or folder from a container in the form of a tar archive."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef inspect_container(self, container):\n        return self._result(\n            self._get(self._url(\"/containers/{0}/json\", container)), True\n        )", "response": "Returns a dictionary containing information about the container s attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef kill(self, container, signal=None):\n        url = self._url(\"/containers/{0}/kill\", container)\n        params = {}\n        if signal is not None:\n            if not isinstance(signal, six.string_types):\n                signal = int(signal)\n            params['signal'] = signal\n        res = self._post(url, params=params)\n\n        self._raise_for_status(res)", "response": "Kill a container or send a signal to a container."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef logs(self, container, stdout=True, stderr=True, stream=False,\n             timestamps=False, tail='all', since=None, follow=None,\n             until=None):\n        \"\"\"\n        Get logs from a container. Similar to the ``docker logs`` command.\n\n        The ``stream`` parameter makes the ``logs`` function return a blocking\n        generator you can iterate over to retrieve log output as it happens.\n\n        Args:\n            container (str): The container to get logs from\n            stdout (bool): Get ``STDOUT``. Default ``True``\n            stderr (bool): Get ``STDERR``. Default ``True``\n            stream (bool): Stream the response. Default ``False``\n            timestamps (bool): Show timestamps. Default ``False``\n            tail (str or int): Output specified number of lines at the end of\n                logs. Either an integer of number of lines or the string\n                ``all``. Default ``all``\n            since (datetime or int): Show logs since a given datetime or\n                integer epoch (in seconds)\n            follow (bool): Follow log output. Default ``False``\n            until (datetime or int): Show logs that occurred before the given\n                datetime or integer epoch (in seconds)\n\n        Returns:\n            (generator or str)\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        if follow is None:\n            follow = stream\n        params = {'stderr': stderr and 1 or 0,\n                  'stdout': stdout and 1 or 0,\n                  'timestamps': timestamps and 1 or 0,\n                  'follow': follow and 1 or 0,\n                  }\n        if tail != 'all' and (not isinstance(tail, int) or tail < 0):\n            tail = 'all'\n        params['tail'] = tail\n\n        if since is not None:\n            if isinstance(since, datetime):\n                params['since'] = utils.datetime_to_timestamp(since)\n            elif (isinstance(since, int) and since > 0):\n                params['since'] = since\n            else:\n                raise errors.InvalidArgument(\n                    'since value should be datetime or positive int, '\n                    'not {}'.format(type(since))\n                )\n\n        if until is not None:\n            if utils.version_lt(self._version, '1.35'):\n                raise errors.InvalidVersion(\n                    'until is not supported for API version < 1.35'\n                )\n            if isinstance(until, datetime):\n                params['until'] = utils.datetime_to_timestamp(until)\n            elif (isinstance(until, int) and until > 0):\n                params['until'] = until\n            else:\n                raise errors.InvalidArgument(\n                    'until value should be datetime or positive int, '\n                    'not {}'.format(type(until))\n                )\n\n        url = self._url(\"/containers/{0}/logs\", container)\n        res = self._get(url, params=params, stream=stream)\n        output = self._get_result(container, stream, res)\n\n        if stream:\n            return CancellableStream(output, res)\n        else:\n            return output", "response": "Get logs from a container."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pause(self, container):\n        url = self._url('/containers/{0}/pause', container)\n        res = self._post(url)\n        self._raise_for_status(res)", "response": "Pauses all processes within a container."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the public - facing port that is NAT - ed to private_port.", "response": "def port(self, container, private_port):\n        \"\"\"\n        Lookup the public-facing port that is NAT-ed to ``private_port``.\n        Identical to the ``docker port`` command.\n\n        Args:\n            container (str): The container to look up\n            private_port (int): The private port to inspect\n\n        Returns:\n            (list of dict): The mapping for the host ports\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n\n        Example:\n            .. code-block:: bash\n\n                $ docker run -d -p 80:80 ubuntu:14.04 /bin/sleep 30\n                7174d6347063a83f412fad6124c99cffd25ffe1a0807eb4b7f9cec76ac8cb43b\n\n            .. code-block:: python\n\n                >>> cli.port('7174d6347063', 80)\n                [{'HostIp': '0.0.0.0', 'HostPort': '80'}]\n        \"\"\"\n        res = self._get(self._url(\"/containers/{0}/json\", container))\n        self._raise_for_status(res)\n        json_ = res.json()\n        private_port = str(private_port)\n        h_ports = None\n\n        # Port settings is None when the container is running with\n        # network_mode=host.\n        port_settings = json_.get('NetworkSettings', {}).get('Ports')\n        if port_settings is None:\n            return None\n\n        if '/' in private_port:\n            return port_settings.get(private_port)\n\n        for protocol in ['tcp', 'udp', 'sctp']:\n            h_ports = port_settings.get(private_port + '/' + protocol)\n            if h_ports:\n                break\n\n        return h_ports"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef put_archive(self, container, path, data):\n        params = {'path': path}\n        url = self._url('/containers/{0}/archive', container)\n        res = self._put(url, params=params, data=data)\n        self._raise_for_status(res)\n        return res.status_code == 200", "response": "Insert a file or folder in an existing container using a tar archive as\n            source."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_container(self, container, v=False, link=False, force=False):\n        params = {'v': v, 'link': link, 'force': force}\n        res = self._delete(\n            self._url(\"/containers/{0}\", container), params=params\n        )\n        self._raise_for_status(res)", "response": "Removes a container from the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrename a container. Similar to the docker rename command.", "response": "def rename(self, container, name):\n        \"\"\"\n        Rename a container. Similar to the ``docker rename`` command.\n\n        Args:\n            container (str): ID of the container to rename\n            name (str): New name for the container\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        url = self._url(\"/containers/{0}/rename\", container)\n        params = {'name': name}\n        res = self._post(url, params=params)\n        self._raise_for_status(res)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resize(self, container, height, width):\n        params = {'h': height, 'w': width}\n        url = self._url(\"/containers/{0}/resize\", container)\n        res = self._post(url, params=params)\n        self._raise_for_status(res)", "response": "Resize the tty session."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef restart(self, container, timeout=10):\n        params = {'t': timeout}\n        url = self._url(\"/containers/{0}/restart\", container)\n        conn_timeout = self.timeout\n        if conn_timeout is not None:\n            conn_timeout += timeout\n        res = self._post(url, params=params, timeout=conn_timeout)\n        self._raise_for_status(res)", "response": "Restarts a container. Similar to the docker restart command."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start(self, container, *args, **kwargs):\n        if args or kwargs:\n            raise errors.DeprecatedMethod(\n                'Providing configuration in the start() method is no longer '\n                'supported. Use the host_config param in create_container '\n                'instead.'\n            )\n        url = self._url(\"/containers/{0}/start\", container)\n        res = self._post(url)\n        self._raise_for_status(res)", "response": "Start a container. Similar to the ``docker start`` command, but\n        doesn't support attach options.\n\n        **Deprecation warning:** Passing configuration options in ``start`` is\n        no longer supported. Users are expected to provide host config options\n        in the ``host_config`` parameter of\n        :py:meth:`~ContainerApiMixin.create_container`.\n\n\n        Args:\n            container (str): The container to start\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n            :py:class:`docker.errors.DeprecatedMethod`\n                If any argument besides ``container`` are provided.\n\n        Example:\n\n            >>> container = cli.create_container(\n            ...     image='busybox:latest',\n            ...     command='/bin/sleep 30')\n            >>> cli.start(container=container.get('Id'))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stats(self, container, decode=None, stream=True):\n        url = self._url(\"/containers/{0}/stats\", container)\n        if stream:\n            return self._stream_helper(self._get(url, stream=True),\n                                       decode=decode)\n        else:\n            if decode:\n                raise errors.InvalidArgument(\n                    \"decode is only available in conjuction with stream=True\"\n                )\n            return self._result(self._get(url, params={'stream': False}),\n                                json=True)", "response": "Returns a dictionary of the current statistics for a specific container."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndisplaying the running processes of a container.", "response": "def top(self, container, ps_args=None):\n        \"\"\"\n        Display the running processes of a container.\n\n        Args:\n            container (str): The container to inspect\n            ps_args (str): An optional arguments passed to ps (e.g. ``aux``)\n\n        Returns:\n            (str): The output of the top\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        u = self._url(\"/containers/{0}/top\", container)\n        params = {}\n        if ps_args is not None:\n            params['ps_args'] = ps_args\n        return self._result(self._get(u, params=params), True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unpause(self, container):\n        url = self._url('/containers/{0}/unpause', container)\n        res = self._post(url)\n        self._raise_for_status(res)", "response": "Unpause all processes within a container."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_container(\n        self, container, blkio_weight=None, cpu_period=None, cpu_quota=None,\n        cpu_shares=None, cpuset_cpus=None, cpuset_mems=None, mem_limit=None,\n        mem_reservation=None, memswap_limit=None, kernel_memory=None,\n        restart_policy=None\n    ):\n        \"\"\"\n        Update resource configs of one or more containers.\n\n        Args:\n            container (str): The container to inspect\n            blkio_weight (int): Block IO (relative weight), between 10 and 1000\n            cpu_period (int): Limit CPU CFS (Completely Fair Scheduler) period\n            cpu_quota (int): Limit CPU CFS (Completely Fair Scheduler) quota\n            cpu_shares (int): CPU shares (relative weight)\n            cpuset_cpus (str): CPUs in which to allow execution\n            cpuset_mems (str): MEMs in which to allow execution\n            mem_limit (int or str): Memory limit\n            mem_reservation (int or str): Memory soft limit\n            memswap_limit (int or str): Total memory (memory + swap), -1 to\n                disable swap\n            kernel_memory (int or str): Kernel memory limit\n            restart_policy (dict): Restart policy dictionary\n\n        Returns:\n            (dict): Dictionary containing a ``Warnings`` key.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        url = self._url('/containers/{0}/update', container)\n        data = {}\n        if blkio_weight:\n            data['BlkioWeight'] = blkio_weight\n        if cpu_period:\n            data['CpuPeriod'] = cpu_period\n        if cpu_shares:\n            data['CpuShares'] = cpu_shares\n        if cpu_quota:\n            data['CpuQuota'] = cpu_quota\n        if cpuset_cpus:\n            data['CpusetCpus'] = cpuset_cpus\n        if cpuset_mems:\n            data['CpusetMems'] = cpuset_mems\n        if mem_limit:\n            data['Memory'] = utils.parse_bytes(mem_limit)\n        if mem_reservation:\n            data['MemoryReservation'] = utils.parse_bytes(mem_reservation)\n        if memswap_limit:\n            data['MemorySwap'] = utils.parse_bytes(memswap_limit)\n        if kernel_memory:\n            data['KernelMemory'] = utils.parse_bytes(kernel_memory)\n        if restart_policy:\n            if utils.version_lt(self._version, '1.23'):\n                raise errors.InvalidVersion(\n                    'restart policy update is not supported '\n                    'for API version < 1.23'\n                )\n            data['RestartPolicy'] = restart_policy\n\n        res = self._post_json(url, data=data)\n        return self._result(res, True)", "response": "Updates the resource configs of one or more containers."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wait(self, container, timeout=None, condition=None):\n        url = self._url(\"/containers/{0}/wait\", container)\n        params = {}\n        if condition is not None:\n            if utils.version_lt(self._version, '1.30'):\n                raise errors.InvalidVersion(\n                    'wait condition is not supported for API version < 1.30'\n                )\n            params['condition'] = condition\n\n        res = self._post(url, timeout=timeout, params=params)\n        return self._result(res, True)", "response": "Block until a container stops."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tags(self):\n        tags = self.attrs.get('RepoTags')\n        if tags is None:\n            tags = []\n        return [tag for tag in tags if tag != '<none>:<none>']", "response": "A list of the tags of this image."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a tarball of an image. Similar to the docker save command.", "response": "def save(self, chunk_size=DEFAULT_DATA_CHUNK_SIZE, named=False):\n        \"\"\"\n        Get a tarball of an image. Similar to the ``docker save`` command.\n\n        Args:\n            chunk_size (int): The generator will return up to that much data\n                per iteration, but may return less. If ``None``, data will be\n                streamed as it is received. Default: 2 MB\n            named (str or bool): If ``False`` (default), the tarball will not\n                retain repository and tag information for this image. If set\n                to ``True``, the first tag in the :py:attr:`~tags` list will\n                be used to identify the image. Alternatively, any element of\n                the :py:attr:`~tags` list can be used as an argument to use\n                that specific tag as the saved identifier.\n\n        Returns:\n            (generator): A stream of raw archive data.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n\n        Example:\n\n            >>> image = cli.get_image(\"busybox:latest\")\n            >>> f = open('/tmp/busybox-latest.tar', 'wb')\n            >>> for chunk in image:\n            >>>   f.write(chunk)\n            >>> f.close()\n        \"\"\"\n        img = self.id\n        if named:\n            img = self.tags[0] if self.tags else img\n            if isinstance(named, six.string_types):\n                if named not in self.tags:\n                    raise InvalidArgument(\n                        \"{} is not a valid tag for this image\".format(named)\n                    )\n                img = named\n\n        return self.client.api.get_image(img, chunk_size)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tag(self, repository, tag=None, **kwargs):\n        return self.client.api.tag(self.id, repository, tag=tag, **kwargs)", "response": "Tag this image into a repository."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npull the image digest.", "response": "def pull(self, platform=None):\n        \"\"\"\n        Pull the image digest.\n\n        Args:\n            platform (str): The platform to pull the image for.\n            Default: ``None``\n\n        Returns:\n            (:py:class:`Image`): A reference to the pulled image.\n        \"\"\"\n        repository, _ = parse_repository_tag(self.image_name)\n        return self.collection.pull(repository, tag=self.id, platform=platform)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks whether the given platform identifier is available for this resource.", "response": "def has_platform(self, platform):\n        \"\"\"\n        Check whether the given platform identifier is available for this\n        digest.\n\n        Args:\n            platform (str or dict): A string using the ``os[/arch[/variant]]``\n                format, or a platform dictionary.\n\n        Returns:\n            (bool): ``True`` if the platform is recognized as available,\n            ``False`` otherwise.\n\n        Raises:\n            :py:class:`docker.errors.InvalidArgument`\n                If the platform argument is not a valid descriptor.\n        \"\"\"\n        if platform and not isinstance(platform, dict):\n            parts = platform.split('/')\n            if len(parts) > 3 or len(parts) < 1:\n                raise InvalidArgument(\n                    '\"{0}\" is not a valid platform descriptor'.format(platform)\n                )\n            platform = {'os': parts[0]}\n            if len(parts) > 2:\n                platform['variant'] = parts[2]\n            if len(parts) > 1:\n                platform['architecture'] = parts[1]\n        return normalize_platform(\n            platform, self.client.version()\n        ) in self.attrs['Platforms']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build(self, **kwargs):\n        resp = self.client.api.build(**kwargs)\n        if isinstance(resp, six.string_types):\n            return self.get(resp)\n        last_event = None\n        image_id = None\n        result_stream, internal_stream = itertools.tee(json_stream(resp))\n        for chunk in internal_stream:\n            if 'error' in chunk:\n                raise BuildError(chunk['error'], result_stream)\n            if 'stream' in chunk:\n                match = re.search(\n                    r'(^Successfully built |sha256:)([0-9a-f]+)$',\n                    chunk['stream']\n                )\n                if match:\n                    image_id = match.group(2)\n            last_event = chunk\n        if image_id:\n            return (self.get(image_id), result_stream)\n        raise BuildError(last_event or 'Unknown', result_stream)", "response": "Builds an image and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, name):\n        return self.prepare_model(self.client.api.inspect_image(name))", "response": "Gets an image.\n\n        Args:\n            name (str): The name of the image.\n\n        Returns:\n            (:py:class:`Image`): The image.\n\n        Raises:\n            :py:class:`docker.errors.ImageNotFound`\n                If the image does not exist.\n            :py:class:`docker.errors.APIError`\n                If the server returns an error."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_registry_data(self, name, auth_config=None):\n        return RegistryData(\n            image_name=name,\n            attrs=self.client.api.inspect_distribution(name, auth_config),\n            client=self.client,\n            collection=self,\n        )", "response": "Gets the registry data for an image."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist the images in the repository.", "response": "def list(self, name=None, all=False, filters=None):\n        \"\"\"\n        List images on the server.\n\n        Args:\n            name (str): Only show images belonging to the repository ``name``\n            all (bool): Show intermediate image layers. By default, these are\n                filtered out.\n            filters (dict): Filters to be processed on the image list.\n                Available filters:\n                - ``dangling`` (bool)\n                - ``label`` (str): format either ``key`` or ``key=value``\n\n        Returns:\n            (list of :py:class:`Image`): The images.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        resp = self.client.api.images(name=name, all=all, filters=filters)\n        return [self.get(r[\"Id\"]) for r in resp]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load(self, data):\n        resp = self.client.api.load_image(data)\n        images = []\n        for chunk in resp:\n            if 'stream' in chunk:\n                match = re.search(\n                    r'(^Loaded image ID: |^Loaded image: )(.+)$',\n                    chunk['stream']\n                )\n                if match:\n                    image_id = match.group(2)\n                    images.append(image_id)\n            if 'error' in chunk:\n                raise ImageLoadError(chunk['error'])\n\n        return [self.get(i) for i in images]", "response": "Load an image from a binary image stream."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pull(self, repository, tag=None, **kwargs):\n        if not tag:\n            repository, tag = parse_repository_tag(repository)\n\n        if 'stream' in kwargs:\n            warnings.warn(\n                '`stream` is not a valid parameter for this method'\n                ' and will be overridden'\n            )\n            del kwargs['stream']\n\n        pull_log = self.client.api.pull(\n            repository, tag=tag, stream=True, **kwargs\n        )\n        for _ in pull_log:\n            # We don't do anything with the logs, but we need\n            # to keep the connection alive and wait for the image\n            # to be pulled.\n            pass\n        if tag:\n            return self.get('{0}{2}{1}'.format(\n                repository, tag, '@' if tag.startswith('sha256:') else ':'\n            ))\n        return self.list(repository)", "response": "Pull an image of the given name and return it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting arguments to create and create_container.", "response": "def _create_container_args(kwargs):\n    \"\"\"\n    Convert arguments to create() to arguments to create_container().\n    \"\"\"\n    # Copy over kwargs which can be copied directly\n    create_kwargs = {}\n    for key in copy.copy(kwargs):\n        if key in RUN_CREATE_KWARGS:\n            create_kwargs[key] = kwargs.pop(key)\n    host_config_kwargs = {}\n    for key in copy.copy(kwargs):\n        if key in RUN_HOST_CONFIG_KWARGS:\n            host_config_kwargs[key] = kwargs.pop(key)\n\n    # Process kwargs which are split over both create and host_config\n    ports = kwargs.pop('ports', {})\n    if ports:\n        host_config_kwargs['port_bindings'] = ports\n\n    volumes = kwargs.pop('volumes', {})\n    if volumes:\n        host_config_kwargs['binds'] = volumes\n\n    network = kwargs.pop('network', None)\n    if network:\n        create_kwargs['networking_config'] = {network: None}\n        host_config_kwargs['network_mode'] = network\n\n    # All kwargs should have been consumed by this point, so raise\n    # error if any are left\n    if kwargs:\n        raise create_unexpected_kwargs_error('run', kwargs)\n\n    create_kwargs['host_config'] = HostConfig(**host_config_kwargs)\n\n    # Fill in any kwargs which need processing by create_host_config first\n    port_bindings = create_kwargs['host_config'].get('PortBindings')\n    if port_bindings:\n        # sort to make consistent for tests\n        create_kwargs['ports'] = [tuple(p.split('/', 1))\n                                  for p in sorted(port_bindings.keys())]\n    if volumes:\n        if isinstance(volumes, dict):\n            create_kwargs['volumes'] = [\n                v.get('bind') for v in volumes.values()\n            ]\n        else:\n            create_kwargs['volumes'] = [\n                _host_volume_from_bind(v) for v in volumes\n            ]\n    return create_kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef image(self):\n        image_id = self.attrs.get('ImageID', self.attrs['Image'])\n        if image_id is None:\n            return None\n        return self.client.images.get(image_id.split(':')[1])", "response": "Returns the image of the container."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef status(self):\n        if isinstance(self.attrs['State'], dict):\n            return self.attrs['State']['Status']\n        return self.attrs['State']", "response": "Returns the status of the container. For example running or exited."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef attach(self, **kwargs):\n        return self.client.api.attach(self.id, **kwargs)", "response": "Attach to this container."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef attach_socket(self, **kwargs):\n        return self.client.api.attach_socket(self.id, **kwargs)", "response": "Like attach but returns the underlying socket - like object\n        for the HTTP request."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef commit(self, repository=None, tag=None, **kwargs):\n\n        resp = self.client.api.commit(self.id, repository=repository, tag=tag,\n                                      **kwargs)\n        return self.client.images.get(resp['Id'])", "response": "Commits a container to an image. Similar to the docker commit command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef exec_run(self, cmd, stdout=True, stderr=True, stdin=False, tty=False,\n                 privileged=False, user='', detach=False, stream=False,\n                 socket=False, environment=None, workdir=None, demux=False):\n        \"\"\"\n        Run a command inside this container. Similar to\n        ``docker exec``.\n\n        Args:\n            cmd (str or list): Command to be executed\n            stdout (bool): Attach to stdout. Default: ``True``\n            stderr (bool): Attach to stderr. Default: ``True``\n            stdin (bool): Attach to stdin. Default: ``False``\n            tty (bool): Allocate a pseudo-TTY. Default: False\n            privileged (bool): Run as privileged.\n            user (str): User to execute command as. Default: root\n            detach (bool): If true, detach from the exec command.\n                Default: False\n            stream (bool): Stream response data. Default: False\n            socket (bool): Return the connection socket to allow custom\n                read/write operations. Default: False\n            environment (dict or list): A dictionary or a list of strings in\n                the following format ``[\"PASSWORD=xxx\"]`` or\n                ``{\"PASSWORD\": \"xxx\"}``.\n            workdir (str): Path to working directory for this exec session\n            demux (bool): Return stdout and stderr separately\n\n        Returns:\n            (ExecResult): A tuple of (exit_code, output)\n                exit_code: (int):\n                    Exit code for the executed command or ``None`` if\n                    either ``stream`` or ``socket`` is ``True``.\n                output: (generator, bytes, or tuple):\n                    If ``stream=True``, a generator yielding response chunks.\n                    If ``socket=True``, a socket object for the connection.\n                    If ``demux=True``, a tuple of two bytes: stdout and stderr.\n                    A bytestring containing response data otherwise.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        resp = self.client.api.exec_create(\n            self.id, cmd, stdout=stdout, stderr=stderr, stdin=stdin, tty=tty,\n            privileged=privileged, user=user, environment=environment,\n            workdir=workdir,\n        )\n        exec_output = self.client.api.exec_start(\n            resp['Id'], detach=detach, tty=tty, stream=stream, socket=socket,\n            demux=demux\n        )\n        if socket or stream:\n            return ExecResult(None, exec_output)\n\n        return ExecResult(\n            self.client.api.exec_inspect(resp['Id'])['ExitCode'],\n            exec_output\n        )", "response": "Runs a command inside this container. Similar to docker exec."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef export(self, chunk_size=DEFAULT_DATA_CHUNK_SIZE):\n        return self.client.api.export(self.id, chunk_size)", "response": "Export the contents of the container s filesystem as a tar archive."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_archive(self, path, chunk_size=DEFAULT_DATA_CHUNK_SIZE):\n        return self.client.api.get_archive(self.id, path, chunk_size)", "response": "Retrieve a file or folder from the container in the form of a tar archive."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef kill(self, signal=None):\n\n        return self.client.api.kill(self.id, signal=signal)", "response": "Kill or send a signal to the container."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef logs(self, **kwargs):\n        return self.client.api.logs(self.id, **kwargs)", "response": "Returns a generator that returns the logs from the container."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef put_archive(self, path, data):\n        return self.client.api.put_archive(self.id, path, data)", "response": "Insert a file or folder in this container using a tar archive as\n            source."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove(self, **kwargs):\n        return self.client.api.remove_container(self.id, **kwargs)", "response": "Remove this container. Similar to the docker rm command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrename this container. Similar to the docker rename command.", "response": "def rename(self, name):\n        \"\"\"\n        Rename this container. Similar to the ``docker rename`` command.\n\n        Args:\n            name (str): New name for the container\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        return self.client.api.rename(self.id, name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resize(self, height, width):\n        return self.client.api.resize(self.id, height, width)", "response": "Resize the tty session."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef restart(self, **kwargs):\n        return self.client.api.restart(self.id, **kwargs)", "response": "Restarts this container. Similar to the docker restart command."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts this container. Similar to the docker start command but causes an error if the server returns an error.", "response": "def start(self, **kwargs):\n        \"\"\"\n        Start this container. Similar to the ``docker start`` command, but\n        doesn't support attach options.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        return self.client.api.start(self.id, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the current statistics for this container.", "response": "def stats(self, **kwargs):\n        \"\"\"\n        Stream statistics for this container. Similar to the\n        ``docker stats`` command.\n\n        Args:\n            decode (bool): If set to true, stream will be decoded into dicts\n                on the fly. Only applicable if ``stream`` is True.\n                False by default.\n            stream (bool): If set to false, only the current stats will be\n                returned instead of a stream. True by default.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        return self.client.api.stats(self.id, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stop(self, **kwargs):\n        return self.client.api.stop(self.id, **kwargs)", "response": "Stops a container. Similar to the docker stop command."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndisplays the running processes of the container.", "response": "def top(self, **kwargs):\n        \"\"\"\n        Display the running processes of the container.\n\n        Args:\n            ps_args (str): An optional arguments passed to ps (e.g. ``aux``)\n\n        Returns:\n            (str): The output of the top\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        return self.client.api.top(self.id, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the resource configuration of the containers.", "response": "def update(self, **kwargs):\n        \"\"\"\n        Update resource configuration of the containers.\n\n        Args:\n            blkio_weight (int): Block IO (relative weight), between 10 and 1000\n            cpu_period (int): Limit CPU CFS (Completely Fair Scheduler) period\n            cpu_quota (int): Limit CPU CFS (Completely Fair Scheduler) quota\n            cpu_shares (int): CPU shares (relative weight)\n            cpuset_cpus (str): CPUs in which to allow execution\n            cpuset_mems (str): MEMs in which to allow execution\n            mem_limit (int or str): Memory limit\n            mem_reservation (int or str): Memory soft limit\n            memswap_limit (int or str): Total memory (memory + swap), -1 to\n                disable swap\n            kernel_memory (int or str): Kernel memory limit\n            restart_policy (dict): Restart policy dictionary\n\n        Returns:\n            (dict): Dictionary containing a ``Warnings`` key.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        return self.client.api.update_container(self.id, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wait(self, **kwargs):\n        return self.client.api.wait(self.id, **kwargs)", "response": "Block until the container stops then return its exit code."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self, image, command=None, stdout=True, stderr=False,\n            remove=False, **kwargs):\n        \"\"\"\n        Run a container. By default, it will wait for the container to finish\n        and return its logs, similar to ``docker run``.\n\n        If the ``detach`` argument is ``True``, it will start the container\n        and immediately return a :py:class:`Container` object, similar to\n        ``docker run -d``.\n\n        Example:\n            Run a container and get its output:\n\n            >>> import docker\n            >>> client = docker.from_env()\n            >>> client.containers.run('alpine', 'echo hello world')\n            b'hello world\\\\n'\n\n            Run a container and detach:\n\n            >>> container = client.containers.run('bfirsh/reticulate-splines',\n                                                  detach=True)\n            >>> container.logs()\n            'Reticulating spline 1...\\\\nReticulating spline 2...\\\\n'\n\n        Args:\n            image (str): The image to run.\n            command (str or list): The command to run in the container.\n            auto_remove (bool): enable auto-removal of the container on daemon\n                side when the container's process exits.\n            blkio_weight_device: Block IO weight (relative device weight) in\n                the form of: ``[{\"Path\": \"device_path\", \"Weight\": weight}]``.\n            blkio_weight: Block IO weight (relative weight), accepts a weight\n                value between 10 and 1000.\n            cap_add (list of str): Add kernel capabilities. For example,\n                ``[\"SYS_ADMIN\", \"MKNOD\"]``.\n            cap_drop (list of str): Drop kernel capabilities.\n            cgroup_parent (str): Override the default parent cgroup.\n            cpu_count (int): Number of usable CPUs (Windows only).\n            cpu_percent (int): Usable percentage of the available CPUs\n                (Windows only).\n            cpu_period (int): The length of a CPU period in microseconds.\n            cpu_quota (int): Microseconds of CPU time that the container can\n                get in a CPU period.\n            cpu_rt_period (int): Limit CPU real-time period in microseconds.\n            cpu_rt_runtime (int): Limit CPU real-time runtime in microseconds.\n            cpu_shares (int): CPU shares (relative weight).\n            cpuset_cpus (str): CPUs in which to allow execution (``0-3``,\n                ``0,1``).\n            cpuset_mems (str): Memory nodes (MEMs) in which to allow execution\n                (``0-3``, ``0,1``). Only effective on NUMA systems.\n            detach (bool): Run container in the background and return a\n                :py:class:`Container` object.\n            device_cgroup_rules (:py:class:`list`): A list of cgroup rules to\n                apply to the container.\n            device_read_bps: Limit read rate (bytes per second) from a device\n                in the form of: `[{\"Path\": \"device_path\", \"Rate\": rate}]`\n            device_read_iops: Limit read rate (IO per second) from a device.\n            device_write_bps: Limit write rate (bytes per second) from a\n                device.\n            device_write_iops: Limit write rate (IO per second) from a device.\n            devices (:py:class:`list`): Expose host devices to the container,\n                as a list of strings in the form\n                ``<path_on_host>:<path_in_container>:<cgroup_permissions>``.\n\n                For example, ``/dev/sda:/dev/xvda:rwm`` allows the container\n                to have read-write access to the host's ``/dev/sda`` via a\n                node named ``/dev/xvda`` inside the container.\n            dns (:py:class:`list`): Set custom DNS servers.\n            dns_opt (:py:class:`list`): Additional options to be added to the\n                container's ``resolv.conf`` file.\n            dns_search (:py:class:`list`): DNS search domains.\n            domainname (str or list): Set custom DNS search domains.\n            entrypoint (str or list): The entrypoint for the container.\n            environment (dict or list): Environment variables to set inside\n                the container, as a dictionary or a list of strings in the\n                format ``[\"SOMEVARIABLE=xxx\"]``.\n            extra_hosts (dict): Additional hostnames to resolve inside the\n                container, as a mapping of hostname to IP address.\n            group_add (:py:class:`list`): List of additional group names and/or\n                IDs that the container process will run as.\n            healthcheck (dict): Specify a test to perform to check that the\n                container is healthy.\n            hostname (str): Optional hostname for the container.\n            init (bool): Run an init inside the container that forwards\n                signals and reaps processes\n            init_path (str): Path to the docker-init binary\n            ipc_mode (str): Set the IPC mode for the container.\n            isolation (str): Isolation technology to use. Default: `None`.\n            kernel_memory (int or str): Kernel memory limit\n            labels (dict or list): A dictionary of name-value labels (e.g.\n                ``{\"label1\": \"value1\", \"label2\": \"value2\"}``) or a list of\n                names of labels to set with empty values (e.g.\n                ``[\"label1\", \"label2\"]``)\n            links (dict): Mapping of links using the\n                ``{'container': 'alias'}`` format. The alias is optional.\n                Containers declared in this dict will be linked to the new\n                container using the provided alias. Default: ``None``.\n            log_config (LogConfig): Logging configuration.\n            lxc_conf (dict): LXC config.\n            mac_address (str): MAC address to assign to the container.\n            mem_limit (int or str): Memory limit. Accepts float values\n                (which represent the memory limit of the created container in\n                bytes) or a string with a units identification char\n                (``100000b``, ``1000k``, ``128m``, ``1g``). If a string is\n                specified without a units character, bytes are assumed as an\n                intended unit.\n            mem_reservation (int or str): Memory soft limit\n            mem_swappiness (int): Tune a container's memory swappiness\n                behavior. Accepts number between 0 and 100.\n            memswap_limit (str or int): Maximum amount of memory + swap a\n                container is allowed to consume.\n            mounts (:py:class:`list`): Specification for mounts to be added to\n                the container. More powerful alternative to ``volumes``. Each\n                item in the list is expected to be a\n                :py:class:`docker.types.Mount` object.\n            name (str): The name for this container.\n            nano_cpus (int):  CPU quota in units of 1e-9 CPUs.\n            network (str): Name of the network this container will be connected\n                to at creation time. You can connect to additional networks\n                using :py:meth:`Network.connect`. Incompatible with\n                ``network_mode``.\n            network_disabled (bool): Disable networking.\n            network_mode (str): One of:\n\n                - ``bridge`` Create a new network stack for the container on\n                  on the bridge network.\n                - ``none`` No networking for this container.\n                - ``container:<name|id>`` Reuse another container's network\n                  stack.\n                - ``host`` Use the host network stack.\n\n                Incompatible with ``network``.\n            oom_kill_disable (bool): Whether to disable OOM killer.\n            oom_score_adj (int): An integer value containing the score given\n                to the container in order to tune OOM killer preferences.\n            pid_mode (str): If set to ``host``, use the host PID namespace\n                inside the container.\n            pids_limit (int): Tune a container's pids limit. Set ``-1`` for\n                unlimited.\n            platform (str): Platform in the format ``os[/arch[/variant]]``.\n                Only used if the method needs to pull the requested image.\n            ports (dict): Ports to bind inside the container.\n\n                The keys of the dictionary are the ports to bind inside the\n                container, either as an integer or a string in the form\n                ``port/protocol``, where the protocol is either ``tcp``,\n                ``udp``, or ``sctp``.\n\n                The values of the dictionary are the corresponding ports to\n                open on the host, which can be either:\n\n                - The port number, as an integer. For example,\n                  ``{'2222/tcp': 3333}`` will expose port 2222 inside the\n                  container as port 3333 on the host.\n                - ``None``, to assign a random host port. For example,\n                  ``{'2222/tcp': None}``.\n                - A tuple of ``(address, port)`` if you want to specify the\n                  host interface. For example,\n                  ``{'1111/tcp': ('127.0.0.1', 1111)}``.\n                - A list of integers, if you want to bind multiple host ports\n                  to a single container port. For example,\n                  ``{'1111/tcp': [1234, 4567]}``.\n\n            privileged (bool): Give extended privileges to this container.\n            publish_all_ports (bool): Publish all ports to the host.\n            read_only (bool): Mount the container's root filesystem as read\n                only.\n            remove (bool): Remove the container when it has finished running.\n                Default: ``False``.\n            restart_policy (dict): Restart the container when it exits.\n                Configured as a dictionary with keys:\n\n                - ``Name`` One of ``on-failure``, or ``always``.\n                - ``MaximumRetryCount`` Number of times to restart the\n                  container on failure.\n\n                For example:\n                ``{\"Name\": \"on-failure\", \"MaximumRetryCount\": 5}``\n\n            runtime (str): Runtime to use with this container.\n            security_opt (:py:class:`list`): A list of string values to\n                customize labels for MLS systems, such as SELinux.\n            shm_size (str or int): Size of /dev/shm (e.g. ``1G``).\n            stdin_open (bool): Keep ``STDIN`` open even if not attached.\n            stdout (bool): Return logs from ``STDOUT`` when ``detach=False``.\n                Default: ``True``.\n            stderr (bool): Return logs from ``STDERR`` when ``detach=False``.\n                Default: ``False``.\n            stop_signal (str): The stop signal to use to stop the container\n                (e.g. ``SIGINT``).\n            storage_opt (dict): Storage driver options per container as a\n                key-value mapping.\n            stream (bool): If true and ``detach`` is false, return a log\n                generator instead of a string. Ignored if ``detach`` is true.\n                Default: ``False``.\n            sysctls (dict): Kernel parameters to set in the container.\n            tmpfs (dict): Temporary filesystems to mount, as a dictionary\n                mapping a path inside the container to options for that path.\n\n                For example:\n\n                .. code-block:: python\n\n                    {\n                        '/mnt/vol2': '',\n                        '/mnt/vol1': 'size=3G,uid=1000'\n                    }\n\n            tty (bool): Allocate a pseudo-TTY.\n            ulimits (:py:class:`list`): Ulimits to set inside the container,\n                as a list of :py:class:`docker.types.Ulimit` instances.\n            use_config_proxy (bool): If ``True``, and if the docker client\n                configuration file (``~/.docker/config.json`` by default)\n                contains a proxy configuration, the corresponding environment\n                variables will be set in the container being built.\n            user (str or int): Username or UID to run commands as inside the\n                container.\n            userns_mode (str): Sets the user namespace mode for the container\n                when user namespace remapping option is enabled. Supported\n                values are: ``host``\n            uts_mode (str): Sets the UTS namespace mode for the container.\n                Supported values are: ``host``\n            version (str): The version of the API to use. Set to ``auto`` to\n                automatically detect the server's version. Default: ``1.35``\n            volume_driver (str): The name of a volume driver/plugin.\n            volumes (dict or list): A dictionary to configure volumes mounted\n                inside the container. The key is either the host path or a\n                volume name, and the value is a dictionary with the keys:\n\n                - ``bind`` The path to mount the volume inside the container\n                - ``mode`` Either ``rw`` to mount the volume read/write, or\n                  ``ro`` to mount it read-only.\n\n                For example:\n\n                .. code-block:: python\n\n                    {'/home/user1/': {'bind': '/mnt/vol2', 'mode': 'rw'},\n                     '/var/www': {'bind': '/mnt/vol1', 'mode': 'ro'}}\n\n            volumes_from (:py:class:`list`): List of container names or IDs to\n                get volumes from.\n            working_dir (str): Path to the working directory.\n\n        Returns:\n            The container logs, either ``STDOUT``, ``STDERR``, or both,\n            depending on the value of the ``stdout`` and ``stderr`` arguments.\n\n            ``STDOUT`` and ``STDERR`` may be read only if either ``json-file``\n            or ``journald`` logging driver used. Thus, if you are using none of\n            these drivers, a ``None`` object is returned instead. See the\n            `Engine API documentation\n            <https://docs.docker.com/engine/api/v1.30/#operation/ContainerLogs/>`_\n            for full details.\n\n            If ``detach`` is ``True``, a :py:class:`Container` object is\n            returned instead.\n\n        Raises:\n            :py:class:`docker.errors.ContainerError`\n                If the container exits with a non-zero exit code and\n                ``detach`` is ``False``.\n            :py:class:`docker.errors.ImageNotFound`\n                If the specified image does not exist.\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        if isinstance(image, Image):\n            image = image.id\n        stream = kwargs.pop('stream', False)\n        detach = kwargs.pop('detach', False)\n        platform = kwargs.pop('platform', None)\n\n        if detach and remove:\n            if version_gte(self.client.api._version, '1.25'):\n                kwargs[\"auto_remove\"] = True\n            else:\n                raise RuntimeError(\"The options 'detach' and 'remove' cannot \"\n                                   \"be used together in api versions < 1.25.\")\n\n        if kwargs.get('network') and kwargs.get('network_mode'):\n            raise RuntimeError(\n                'The options \"network\" and \"network_mode\" can not be used '\n                'together.'\n            )\n\n        try:\n            container = self.create(image=image, command=command,\n                                    detach=detach, **kwargs)\n        except ImageNotFound:\n            self.client.images.pull(image, platform=platform)\n            container = self.create(image=image, command=command,\n                                    detach=detach, **kwargs)\n\n        container.start()\n\n        if detach:\n            return container\n\n        logging_driver = container.attrs['HostConfig']['LogConfig']['Type']\n\n        out = None\n        if logging_driver == 'json-file' or logging_driver == 'journald':\n            out = container.logs(\n                stdout=stdout, stderr=stderr, stream=True, follow=True\n            )\n\n        exit_status = container.wait()['StatusCode']\n        if exit_status != 0:\n            out = None\n            if not kwargs.get('auto_remove'):\n                out = container.logs(stdout=False, stderr=True)\n\n        if remove:\n            container.remove()\n        if exit_status != 0:\n            raise ContainerError(\n                container, exit_status, command, image, out\n            )\n\n        return out if stream or out is None else b''.join(\n            [line for line in out]\n        )", "response": "Runs a container and returns its output."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create(self, image, command=None, **kwargs):\n        if isinstance(image, Image):\n            image = image.id\n        kwargs['image'] = image\n        kwargs['command'] = command\n        kwargs['version'] = self.client.api._version\n        create_kwargs = _create_container_args(kwargs)\n        resp = self.client.api.create_container(**create_kwargs)\n        return self.get(resp['Id'])", "response": "Create a container without starting it."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a container by name or ID.", "response": "def get(self, container_id):\n        \"\"\"\n        Get a container by name or ID.\n\n        Args:\n            container_id (str): Container name or ID.\n\n        Returns:\n            A :py:class:`Container` object.\n\n        Raises:\n            :py:class:`docker.errors.NotFound`\n                If the container does not exist.\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        resp = self.client.api.inspect_container(container_id)\n        return self.prepare_model(resp)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list(self, all=False, before=None, filters=None, limit=-1, since=None,\n             sparse=False, ignore_removed=False):\n        \"\"\"\n        List containers. Similar to the ``docker ps`` command.\n\n        Args:\n            all (bool): Show all containers. Only running containers are shown\n                by default\n            since (str): Show only containers created since Id or Name, include\n                non-running ones\n            before (str): Show only container created before Id or Name,\n                include non-running ones\n            limit (int): Show `limit` last created containers, include\n                non-running ones\n            filters (dict): Filters to be processed on the image list.\n                Available filters:\n\n                - `exited` (int): Only containers with specified exit code\n                - `status` (str): One of ``restarting``, ``running``,\n                    ``paused``, ``exited``\n                - `label` (str): format either ``\"key\"`` or ``\"key=value\"``\n                - `id` (str): The id of the container.\n                - `name` (str): The name of the container.\n                - `ancestor` (str): Filter by container ancestor. Format of\n                    ``<image-name>[:tag]``, ``<image-id>``, or\n                    ``<image@digest>``.\n                - `before` (str): Only containers created before a particular\n                    container. Give the container name or id.\n                - `since` (str): Only containers created after a particular\n                    container. Give container name or id.\n\n                A comprehensive list can be found in the documentation for\n                `docker ps\n                <https://docs.docker.com/engine/reference/commandline/ps>`_.\n\n            sparse (bool): Do not inspect containers. Returns partial\n                information, but guaranteed not to block. Use\n                :py:meth:`Container.reload` on resulting objects to retrieve\n                all attributes. Default: ``False``\n            ignore_removed (bool): Ignore failures due to missing containers\n                when attempting to inspect containers from the original list.\n                Set to ``True`` if race conditions are likely. Has no effect\n                if ``sparse=True``. Default: ``False``\n\n        Returns:\n            (list of :py:class:`Container`)\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        resp = self.client.api.containers(all=all, before=before,\n                                          filters=filters, limit=limit,\n                                          since=since)\n        if sparse:\n            return [self.prepare_model(r) for r in resp]\n        else:\n            containers = []\n            for r in resp:\n                try:\n                    containers.append(self.get(r['Id']))\n                # a container may have been removed while iterating\n                except NotFound:\n                    if not ignore_removed:\n                        raise\n            return containers", "response": "List the containers in the system."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of tasks in this service.", "response": "def tasks(self, filters=None):\n        \"\"\"\n        List the tasks in this service.\n\n        Args:\n            filters (dict): A map of filters to process on the tasks list.\n                Valid filters: ``id``, ``name``, ``node``,\n                ``label``, and ``desired-state``.\n\n        Returns:\n            :py:class:`list`: List of task dictionaries.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        if filters is None:\n            filters = {}\n        filters['service'] = self.id\n        return self.client.api.tasks(filters=filters)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(self, **kwargs):\n        # Image is required, so if it hasn't been set, use current image\n        if 'image' not in kwargs:\n            spec = self.attrs['Spec']['TaskTemplate']['ContainerSpec']\n            kwargs['image'] = spec['Image']\n\n        if kwargs.get('force_update') is True:\n            task_template = self.attrs['Spec']['TaskTemplate']\n            current_value = int(task_template.get('ForceUpdate', 0))\n            kwargs['force_update'] = current_value + 1\n\n        create_kwargs = _get_create_service_kwargs('update', kwargs)\n\n        return self.client.api.update_service(\n            self.id,\n            self.version,\n            **create_kwargs\n        )", "response": "Updates the service s configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef logs(self, **kwargs):\n        is_tty = self.attrs['Spec']['TaskTemplate']['ContainerSpec'].get(\n            'TTY', False\n        )\n        return self.client.api.service_logs(self.id, is_tty=is_tty, **kwargs)", "response": "Get the logs for the service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nscaling this service container.", "response": "def scale(self, replicas):\n        \"\"\"\n        Scale service container.\n\n        Args:\n            replicas (int): The number of containers that should be running.\n\n        Returns:\n            bool: ``True`` if successful.\n        \"\"\"\n\n        if 'Global' in self.attrs['Spec']['Mode'].keys():\n            raise InvalidArgument('Cannot scale a global container')\n\n        service_mode = ServiceMode('replicated', replicas)\n        return self.client.api.update_service(self.id, self.version,\n                                              mode=service_mode,\n                                              fetch_current_spec=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new service in the container system.", "response": "def create(self, image, command=None, **kwargs):\n        \"\"\"\n        Create a service. Similar to the ``docker service create`` command.\n\n        Args:\n            image (str): The image name to use for the containers.\n            command (list of str or str): Command to run.\n            args (list of str): Arguments to the command.\n            constraints (list of str): :py:class:`~docker.types.Placement`\n                constraints.\n            preferences (list of tuple): :py:class:`~docker.types.Placement`\n                preferences.\n            platforms (list of tuple): A list of platform constraints\n                expressed as ``(arch, os)`` tuples.\n            container_labels (dict): Labels to apply to the container.\n            endpoint_spec (EndpointSpec): Properties that can be configured to\n                access and load balance a service. Default: ``None``.\n            env (list of str): Environment variables, in the form\n                ``KEY=val``.\n            hostname (string): Hostname to set on the container.\n            init (boolean): Run an init inside the container that forwards\n                signals and reaps processes\n            isolation (string): Isolation technology used by the service's\n                containers. Only used for Windows containers.\n            labels (dict): Labels to apply to the service.\n            log_driver (str): Log driver to use for containers.\n            log_driver_options (dict): Log driver options.\n            mode (ServiceMode): Scheduling mode for the service.\n                Default:``None``\n            mounts (list of str): Mounts for the containers, in the form\n                ``source:target:options``, where options is either\n                ``ro`` or ``rw``.\n            name (str): Name to give to the service.\n            networks (list of str): List of network names or IDs to attach\n                the service to. Default: ``None``.\n            resources (Resources): Resource limits and reservations.\n            restart_policy (RestartPolicy): Restart policy for containers.\n            secrets (list of :py:class:`docker.types.SecretReference`): List\n                of secrets accessible to containers for this service.\n            stop_grace_period (int): Amount of time to wait for\n                containers to terminate before forcefully killing them.\n            update_config (UpdateConfig): Specification for the update strategy\n                of the service. Default: ``None``\n            rollback_config (RollbackConfig): Specification for the rollback\n                strategy of the service. Default: ``None``\n            user (str): User to run commands as.\n            workdir (str): Working directory for commands to run.\n            tty (boolean): Whether a pseudo-TTY should be allocated.\n            groups (:py:class:`list`): A list of additional groups that the\n                container process will run as.\n            open_stdin (boolean): Open ``stdin``\n            read_only (boolean): Mount the container's root filesystem as read\n                only.\n            stop_signal (string): Set signal to stop the service's containers\n            healthcheck (Healthcheck): Healthcheck\n                configuration for this service.\n            hosts (:py:class:`dict`): A set of host to IP mappings to add to\n                the container's `hosts` file.\n            dns_config (DNSConfig): Specification for DNS\n                related configurations in resolver configuration file.\n            configs (:py:class:`list`): List of :py:class:`ConfigReference`\n                that will be exposed to the service.\n            privileges (Privileges): Security options for the service's\n                containers.\n\n        Returns:\n            :py:class:`Service`: The created service.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        kwargs['image'] = image\n        kwargs['command'] = command\n        create_kwargs = _get_create_service_kwargs('create', kwargs)\n        service_id = self.client.api.create_service(**create_kwargs)\n        return self.get(service_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a service s metadata.", "response": "def get(self, service_id, insert_defaults=None):\n        \"\"\"\n        Get a service.\n\n        Args:\n            service_id (str): The ID of the service.\n            insert_defaults (boolean): If true, default values will be merged\n                into the output.\n\n        Returns:\n            :py:class:`Service`: The service.\n\n        Raises:\n            :py:class:`docker.errors.NotFound`\n                If the service does not exist.\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n            :py:class:`docker.errors.InvalidVersion`\n                If one of the arguments is not supported with the current\n                API version.\n        \"\"\"\n        return self.prepare_model(\n            self.client.api.inspect_service(service_id, insert_defaults)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists the services in the cluster.", "response": "def list(self, **kwargs):\n        \"\"\"\n        List services.\n\n        Args:\n            filters (dict): Filters to process on the nodes list. Valid\n                filters: ``id``, ``name`` , ``label`` and ``mode``.\n                Default: ``None``.\n\n        Returns:\n            list of :py:class:`Service`: The services.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        return [\n            self.prepare_model(s)\n            for s in self.client.api.services(**kwargs)\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reload(self):\n        new_model = self.collection.get(self.id)\n        self.attrs = new_model.attrs", "response": "Reload the object from the server and update the attrs attribute with the new data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a model from a set of attributes.", "response": "def prepare_model(self, attrs):\n        \"\"\"\n        Create a model from a set of attributes.\n        \"\"\"\n        if isinstance(attrs, Model):\n            attrs.client = self.client\n            attrs.collection = self\n            return attrs\n        elif isinstance(attrs, dict):\n            return self.model(attrs=attrs, client=self.client, collection=self)\n        else:\n            raise Exception(\"Can't create %s from %s\" %\n                            (self.model.__name__, attrs))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of all containers that are connected to the network as a list of container objects.", "response": "def containers(self):\n        \"\"\"\n        The containers that are connected to the network, as a list of\n        :py:class:`~docker.models.containers.Container` objects.\n        \"\"\"\n        return [\n            self.client.containers.get(cid) for cid in\n            (self.attrs.get('Containers') or {}).keys()\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connect(self, container, *args, **kwargs):\n        if isinstance(container, Container):\n            container = container.id\n        return self.client.api.connect_container_to_network(\n            container, self.id, *args, **kwargs\n        )", "response": "Connects a container to this network."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef disconnect(self, container, *args, **kwargs):\n        if isinstance(container, Container):\n            container = container.id\n        return self.client.api.disconnect_container_from_network(\n            container, self.id, *args, **kwargs\n        )", "response": "Disconnect a container from this network."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new network in the cluster.", "response": "def create(self, name, *args, **kwargs):\n        \"\"\"\n        Create a network. Similar to the ``docker network create``.\n\n        Args:\n            name (str): Name of the network\n            driver (str): Name of the driver used to create the network\n            options (dict): Driver options as a key-value dictionary\n            ipam (IPAMConfig): Optional custom IP scheme for the network.\n            check_duplicate (bool): Request daemon to check for networks with\n                same name. Default: ``None``.\n            internal (bool): Restrict external access to the network. Default\n                ``False``.\n            labels (dict): Map of labels to set on the network. Default\n                ``None``.\n            enable_ipv6 (bool): Enable IPv6 on the network. Default ``False``.\n            attachable (bool): If enabled, and the network is in the global\n                scope,  non-service containers on worker nodes will be able to\n                connect to the network.\n            scope (str): Specify the network's scope (``local``, ``global`` or\n                ``swarm``)\n            ingress (bool): If set, create an ingress network which provides\n                the routing-mesh in swarm mode.\n\n        Returns:\n            (:py:class:`Network`): The network that was created.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n\n        Example:\n            A network using the bridge driver:\n\n                >>> client.networks.create(\"network1\", driver=\"bridge\")\n\n            You can also create more advanced networks with custom IPAM\n            configurations. For example, setting the subnet to\n            ``192.168.52.0/24`` and gateway address to ``192.168.52.254``.\n\n            .. code-block:: python\n\n                >>> ipam_pool = docker.types.IPAMPool(\n                    subnet='192.168.52.0/24',\n                    gateway='192.168.52.254'\n                )\n                >>> ipam_config = docker.types.IPAMConfig(\n                    pool_configs=[ipam_pool]\n                )\n                >>> client.networks.create(\n                    \"network1\",\n                    driver=\"bridge\",\n                    ipam=ipam_config\n                )\n\n        \"\"\"\n        resp = self.client.api.create_network(name, *args, **kwargs)\n        return self.get(resp['Id'])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(self, network_id, *args, **kwargs):\n        return self.prepare_model(\n            self.client.api.inspect_network(network_id, *args, **kwargs)\n        )", "response": "Get a specific network by its ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list(self, *args, **kwargs):\n        greedy = kwargs.pop('greedy', False)\n        resp = self.client.api.networks(*args, **kwargs)\n        networks = [self.prepare_model(item) for item in resp]\n        if greedy and version_gte(self.client.api._version, '1.28'):\n            for net in networks:\n                net.reload()\n        return networks", "response": "List the networks on the server."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the authentication entries and returns a dictionary of authentication entries.", "response": "def parse_auth(cls, entries, raise_on_error=False):\n        \"\"\"\n        Parses authentication entries\n\n        Args:\n          entries:        Dict of authentication entries.\n          raise_on_error: If set to true, an invalid format will raise\n                          InvalidConfigFile\n\n        Returns:\n          Authentication registry.\n        \"\"\"\n\n        conf = {}\n        for registry, entry in six.iteritems(entries):\n            if not isinstance(entry, dict):\n                log.debug(\n                    'Config entry for key {0} is not auth config'.format(\n                        registry\n                    )\n                )\n                # We sometimes fall back to parsing the whole config as if it\n                # was the auth config by itself, for legacy purposes. In that\n                # case, we fail silently and return an empty conf if any of the\n                # keys is not formatted properly.\n                if raise_on_error:\n                    raise errors.InvalidConfigFile(\n                        'Invalid configuration for registry {0}'.format(\n                            registry\n                        )\n                    )\n                return {}\n            if 'identitytoken' in entry:\n                log.debug(\n                    'Found an IdentityToken entry for registry {0}'.format(\n                        registry\n                    )\n                )\n                conf[registry] = {\n                    'IdentityToken': entry['identitytoken']\n                }\n                continue  # Other values are irrelevant if we have a token\n\n            if 'auth' not in entry:\n                # Starting with engine v1.11 (API 1.23), an empty dictionary is\n                # a valid value in the auths config.\n                # https://github.com/docker/compose/issues/3265\n                log.debug(\n                    'Auth data for {0} is absent. Client might be using a '\n                    'credentials store instead.'.format(registry)\n                )\n                conf[registry] = {}\n                continue\n\n            username, password = decode_auth(entry['auth'])\n            log.debug(\n                'Found entry (registry={0}, username={1})'\n                .format(repr(registry), repr(username))\n            )\n\n            conf[registry] = {\n                'username': username,\n                'password': password,\n                'email': entry.get('email'),\n                'serveraddress': registry,\n            }\n        return conf"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nresolve the authentication configuration for a specific registry.", "response": "def resolve_authconfig(self, registry=None):\n        \"\"\"\n        Returns the authentication data from the given auth configuration for a\n        specific registry. As with the Docker client, legacy entries in the\n        config with full URLs are stripped down to hostnames before checking\n        for a match. Returns None if no match was found.\n        \"\"\"\n\n        if self.creds_store or self.cred_helpers:\n            store_name = self.get_credential_store(registry)\n            if store_name is not None:\n                log.debug(\n                    'Using credentials store \"{0}\"'.format(store_name)\n                )\n                cfg = self._resolve_authconfig_credstore(registry, store_name)\n                if cfg is not None:\n                    return cfg\n                log.debug('No entry in credstore - fetching from auth dict')\n\n        # Default to the public index server\n        registry = resolve_index_name(registry) if registry else INDEX_NAME\n        log.debug(\"Looking for auth entry for {0}\".format(repr(registry)))\n\n        if registry in self.auths:\n            log.debug(\"Found {0}\".format(repr(registry)))\n            return self.auths[registry]\n\n        for key, conf in six.iteritems(self.auths):\n            if resolve_index_name(key) == registry:\n                log.debug(\"Found {0}\".format(repr(key)))\n                return conf\n\n        log.debug(\"No entry found\")\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate an exec instance in a running container.", "response": "def exec_create(self, container, cmd, stdout=True, stderr=True,\n                    stdin=False, tty=False, privileged=False, user='',\n                    environment=None, workdir=None, detach_keys=None):\n        \"\"\"\n        Sets up an exec instance in a running container.\n\n        Args:\n            container (str): Target container where exec instance will be\n                created\n            cmd (str or list): Command to be executed\n            stdout (bool): Attach to stdout. Default: ``True``\n            stderr (bool): Attach to stderr. Default: ``True``\n            stdin (bool): Attach to stdin. Default: ``False``\n            tty (bool): Allocate a pseudo-TTY. Default: False\n            privileged (bool): Run as privileged.\n            user (str): User to execute command as. Default: root\n            environment (dict or list): A dictionary or a list of strings in\n                the following format ``[\"PASSWORD=xxx\"]`` or\n                ``{\"PASSWORD\": \"xxx\"}``.\n            workdir (str): Path to working directory for this exec session\n            detach_keys (str): Override the key sequence for detaching\n                a container. Format is a single character `[a-Z]`\n                or `ctrl-<value>` where `<value>` is one of:\n                `a-z`, `@`, `^`, `[`, `,` or `_`.\n                ~/.docker/config.json is used by default.\n\n        Returns:\n            (dict): A dictionary with an exec ``Id`` key.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n\n        if environment is not None and utils.version_lt(self._version, '1.25'):\n            raise errors.InvalidVersion(\n                'Setting environment for exec is not supported in API < 1.25'\n            )\n\n        if isinstance(cmd, six.string_types):\n            cmd = utils.split_command(cmd)\n\n        if isinstance(environment, dict):\n            environment = utils.utils.format_environment(environment)\n\n        data = {\n            'Container': container,\n            'User': user,\n            'Privileged': privileged,\n            'Tty': tty,\n            'AttachStdin': stdin,\n            'AttachStdout': stdout,\n            'AttachStderr': stderr,\n            'Cmd': cmd,\n            'Env': environment,\n        }\n\n        if workdir is not None:\n            if utils.version_lt(self._version, '1.35'):\n                raise errors.InvalidVersion(\n                    'workdir is not supported for API version < 1.35'\n                )\n            data['WorkingDir'] = workdir\n\n        if detach_keys:\n            data['detachKeys'] = detach_keys\n        elif 'detachKeys' in self._general_configs:\n            data['detachKeys'] = self._general_configs['detachKeys']\n\n        url = self._url('/containers/{0}/exec', container)\n        res = self._post_json(url, data=data)\n        return self._result(res, True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns low - level information about an exec command.", "response": "def exec_inspect(self, exec_id):\n        \"\"\"\n        Return low-level information about an exec command.\n\n        Args:\n            exec_id (str): ID of the exec instance\n\n        Returns:\n            (dict): Dictionary of values returned by the endpoint.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        if isinstance(exec_id, dict):\n            exec_id = exec_id.get('Id')\n        res = self._get(self._url(\"/exec/{0}/json\", exec_id))\n        return self._result(res, True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart a previously set up exec command.", "response": "def exec_start(self, exec_id, detach=False, tty=False, stream=False,\n                   socket=False, demux=False):\n        \"\"\"\n        Start a previously set up exec instance.\n\n        Args:\n            exec_id (str): ID of the exec instance\n            detach (bool): If true, detach from the exec command.\n                Default: False\n            tty (bool): Allocate a pseudo-TTY. Default: False\n            stream (bool): Stream response data. Default: False\n            socket (bool): Return the connection socket to allow custom\n                read/write operations.\n            demux (bool): Return stdout and stderr separately\n\n        Returns:\n\n            (generator or str or tuple): If ``stream=True``, a generator\n            yielding response chunks. If ``socket=True``, a socket object for\n            the connection. A string containing response data otherwise. If\n            ``demux=True``, a tuple with two elements of type byte: stdout and\n            stderr.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        # we want opened socket if socket == True\n\n        data = {\n            'Tty': tty,\n            'Detach': detach\n        }\n\n        headers = {} if detach else {\n            'Connection': 'Upgrade',\n            'Upgrade': 'tcp'\n        }\n\n        res = self._post_json(\n            self._url('/exec/{0}/start', exec_id),\n            headers=headers,\n            data=data,\n            stream=True\n        )\n        if detach:\n            return self._result(res)\n        if socket:\n            return self._get_raw_response_socket(res)\n        return self._read_from_socket(res, stream, tty=tty, demux=demux)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _raise_for_status(self, response):\n        try:\n            response.raise_for_status()\n        except requests.exceptions.HTTPError as e:\n            raise create_api_error_from_http_exception(e)", "response": "Raises stored : class : APIError if one occurred."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstreams result for TTY - enabled container and raw binary data", "response": "def _stream_raw_result(self, response, chunk_size=1, decode=True):\n        ''' Stream result for TTY-enabled container and raw binary data'''\n        self._raise_for_status(response)\n        for out in response.iter_content(chunk_size, decode):\n            yield out"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _disable_socket_timeout(self, socket):\n        sockets = [socket, getattr(socket, '_sock', None)]\n\n        for s in sockets:\n            if not hasattr(s, 'settimeout'):\n                continue\n\n            timeout = -1\n\n            if hasattr(s, 'gettimeout'):\n                timeout = s.gettimeout()\n\n            # Don't change the timeout if it is already disabled.\n            if timeout is None or timeout == 0.0:\n                continue\n\n            s.settimeout(None)", "response": "Disables the socket timeout."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reload_config(self, dockercfg_path=None):\n        self._auth_configs = auth.load_config(\n            dockercfg_path, credstore_env=self.credstore_env\n        )", "response": "Reloads the auth configuration for the current user"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef acquire(self, blocking=None, blocking_timeout=None, token=None):\n        sleep = self.sleep\n        if token is None:\n            token = uuid.uuid1().hex.encode()\n        else:\n            encoder = self.redis.connection_pool.get_encoder()\n            token = encoder.encode(token)\n        if blocking is None:\n            blocking = self.blocking\n        if blocking_timeout is None:\n            blocking_timeout = self.blocking_timeout\n        stop_trying_at = None\n        if blocking_timeout is not None:\n            stop_trying_at = mod_time.time() + blocking_timeout\n        while True:\n            if self.do_acquire(token):\n                self.local.token = token\n                return True\n            if not blocking:\n                return False\n            if stop_trying_at is not None and mod_time.time() > stop_trying_at:\n                return False\n            mod_time.sleep(sleep)", "response": "Acquire a shared distributed lock."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef owned(self):\n        stored_token = self.redis.get(self.name)\n        # need to always compare bytes to bytes\n        # TODO: this can be simplified when the context manager is finished\n        if stored_token and not isinstance(stored_token, bytes):\n            encoder = self.redis.connection_pool.get_encoder()\n            stored_token = encoder.encode(stored_token)\n        return self.local.token is not None and \\\n            stored_token == self.local.token", "response": "Returns True if this key is owned by this lock otherwise False."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresets a TTL of an already acquired lock back to a timeout value.", "response": "def reacquire(self):\n        \"\"\"\n        Resets a TTL of an already acquired lock back to a timeout value.\n        \"\"\"\n        if self.local.token is None:\n            raise LockError(\"Cannot reacquire an unlocked lock\")\n        if self.timeout is None:\n            raise LockError(\"Cannot reacquire a lock with no timeout\")\n        return self.do_reacquire()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a unix timestamp to a Python datetime object", "response": "def timestamp_to_datetime(response):\n    \"Converts a unix timestamp to a Python datetime object\"\n    if not response:\n        return None\n    try:\n        response = int(response)\n    except ValueError:\n        return None\n    return datetime.datetime.fromtimestamp(response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pairs_to_dict(response, decode_keys=False):\n    \"Create a dict given a list of key/value pairs\"\n    if response is None:\n        return {}\n    if decode_keys:\n        # the iter form is faster, but I don't know how to make that work\n        # with a nativestr() map\n        return dict(izip(imap(nativestr, response[::2]), response[1::2]))\n    else:\n        it = iter(response)\n        return dict(izip(it, it))", "response": "Create a dict given a list of key / value pairs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a response to a list of score pairs.", "response": "def zset_score_pairs(response, **options):\n    \"\"\"\n    If ``withscores`` is specified in the options, return the response as\n    a list of (value, score) pairs\n    \"\"\"\n    if not response or not options.get('withscores'):\n        return response\n    score_cast_func = options.get('score_cast_func', float)\n    it = iter(response)\n    return list(izip(it, imap(score_cast_func, it)))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsorting the response as a list of n - element tuples with n being the value found in options. groups.", "response": "def sort_return_tuples(response, **options):\n    \"\"\"\n    If ``groups`` is specified, return the response as a list of\n    n-element tuples with n being the value found in options['groups']\n    \"\"\"\n    if not response or not options.get('groups'):\n        return response\n    n = options['groups']\n    return list(izip(*[response[i::n] for i in range(n)]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_url(cls, url, db=None, **kwargs):\n        connection_pool = ConnectionPool.from_url(url, db=db, **kwargs)\n        return cls(connection_pool=connection_pool)", "response": "Returns a Redis client object configured from the given URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lock(self, name, timeout=None, sleep=0.1, blocking_timeout=None,\n             lock_class=None, thread_local=True):\n        \"\"\"\n        Return a new Lock object using key ``name`` that mimics\n        the behavior of threading.Lock.\n\n        If specified, ``timeout`` indicates a maximum life for the lock.\n        By default, it will remain locked until release() is called.\n\n        ``sleep`` indicates the amount of time to sleep per loop iteration\n        when the lock is in blocking mode and another client is currently\n        holding the lock.\n\n        ``blocking_timeout`` indicates the maximum amount of time in seconds to\n        spend trying to acquire the lock. A value of ``None`` indicates\n        continue trying forever. ``blocking_timeout`` can be specified as a\n        float or integer, both representing the number of seconds to wait.\n\n        ``lock_class`` forces the specified lock implementation.\n\n        ``thread_local`` indicates whether the lock token is placed in\n        thread-local storage. By default, the token is placed in thread local\n        storage so that a thread only sees its token, not a token set by\n        another thread. Consider the following timeline:\n\n            time: 0, thread-1 acquires `my-lock`, with a timeout of 5 seconds.\n                     thread-1 sets the token to \"abc\"\n            time: 1, thread-2 blocks trying to acquire `my-lock` using the\n                     Lock instance.\n            time: 5, thread-1 has not yet completed. redis expires the lock\n                     key.\n            time: 5, thread-2 acquired `my-lock` now that it's available.\n                     thread-2 sets the token to \"xyz\"\n            time: 6, thread-1 finishes its work and calls release(). if the\n                     token is *not* stored in thread local storage, then\n                     thread-1 would see the token value as \"xyz\" and would be\n                     able to successfully release the thread-2's lock.\n\n        In some use cases it's necessary to disable thread local storage. For\n        example, if you have code where one thread acquires a lock and passes\n        that lock instance to a worker thread to release later. If thread\n        local storage isn't disabled in this case, the worker thread won't see\n        the token set by the thread that acquired the lock. Our assumption\n        is that these cases aren't common and as such default to using\n        thread local storage.        \"\"\"\n        if lock_class is None:\n            lock_class = Lock\n        return lock_class(self, name, timeout=timeout, sleep=sleep,\n                          blocking_timeout=blocking_timeout,\n                          thread_local=thread_local)", "response": "This method returns a new Lock object with the specified key name and timeout and sleep."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute a command and return a parsed response", "response": "def execute_command(self, *args, **options):\n        \"Execute a command and return a parsed response\"\n        pool = self.connection_pool\n        command_name = args[0]\n        connection = pool.get_connection(command_name, **options)\n        try:\n            connection.send_command(*args)\n            return self.parse_response(connection, command_name, **options)\n        except (ConnectionError, TimeoutError) as e:\n            connection.disconnect()\n            if not (connection.retry_on_timeout and\n                    isinstance(e, TimeoutError)):\n                raise\n            connection.send_command(*args)\n            return self.parse_response(connection, command_name, **options)\n        finally:\n            pool.release(connection)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_response(self, connection, command_name, **options):\n        \"Parses a response from the Redis server\"\n        try:\n            response = connection.read_response()\n        except ResponseError:\n            if EMPTY_RESPONSE in options:\n                return options[EMPTY_RESPONSE]\n            raise\n        if command_name in self.response_callbacks:\n            return self.response_callbacks[command_name](response, **options)\n        return response", "response": "Parses a response from the Redis server"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef client_kill_filter(self, _id=None, _type=None, addr=None, skipme=None):\n        args = []\n        if _type is not None:\n            client_types = ('normal', 'master', 'slave', 'pubsub')\n            if str(_type).lower() not in client_types:\n                raise DataError(\"CLIENT KILL type must be one of %r\" % (\n                                client_types,))\n            args.extend((Token.get_token('TYPE'), _type))\n        if skipme is not None:\n            if not isinstance(skipme, bool):\n                raise DataError(\"CLIENT KILL skipme must be a bool\")\n            if skipme:\n                args.extend((Token.get_token('SKIPME'),\n                             Token.get_token('YES')))\n            else:\n                args.extend((Token.get_token('SKIPME'),\n                             Token.get_token('NO')))\n        if _id is not None:\n            args.extend((Token.get_token('ID'), _id))\n        if addr is not None:\n            args.extend((Token.get_token('ADDR'), addr))\n        if not args:\n            raise DataError(\"CLIENT KILL <filter> <value> ... ... <filter> \"\n                            \"<value> must specify at least one filter\")\n        return self.execute_command('CLIENT KILL', *args)", "response": "Kills a client by a variety of filter options."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of currently connected clients.", "response": "def client_list(self, _type=None):\n        \"\"\"\n        Returns a list of currently connected clients.\n        If type of client specified, only that type will be returned.\n        :param _type: optional. one of the client types (normal, master,\n         replica, pubsub)\n        \"\"\"\n        \"Returns a list of currently connected clients\"\n        if _type is not None:\n            client_types = ('normal', 'master', 'replica', 'pubsub')\n            if str(_type).lower() not in client_types:\n                raise DataError(\"CLIENT LIST _type must be one of %r\" % (\n                                client_types,))\n            return self.execute_command('CLIENT LIST', Token.get_token('TYPE'),\n                                        _type)\n        return self.execute_command('CLIENT LIST')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef client_unblock(self, client_id, error=False):\n        args = ['CLIENT UNBLOCK', int(client_id)]\n        if error:\n            args.append(Token.get_token('ERROR'))\n        return self.execute_command(*args)", "response": "Unblocks a connection by its client id."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef client_pause(self, timeout):\n        if not isinstance(timeout, (int, long)):\n            raise DataError(\"CLIENT PAUSE timeout must be an integer\")\n        return self.execute_command('CLIENT PAUSE', str(timeout))", "response": "Suspend all the Redis clients for the specified amount of time."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef flushall(self, asynchronous=False):\n        args = []\n        if asynchronous:\n            args.append(Token.get_token('ASYNC'))\n        return self.execute_command('FLUSHALL', *args)", "response": "Delete all keys in all databases on the current host."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef migrate(self, host, port, keys, destination_db, timeout,\n                copy=False, replace=False, auth=None):\n        \"\"\"\n        Migrate 1 or more keys from the current Redis server to a different\n        server specified by the ``host``, ``port`` and ``destination_db``.\n\n        The ``timeout``, specified in milliseconds, indicates the maximum\n        time the connection between the two servers can be idle before the\n        command is interrupted.\n\n        If ``copy`` is True, the specified ``keys`` are NOT deleted from\n        the source server.\n\n        If ``replace`` is True, this operation will overwrite the keys\n        on the destination server if they exist.\n\n        If ``auth`` is specified, authenticate to the destination server with\n        the password provided.\n        \"\"\"\n        keys = list_or_args(keys, [])\n        if not keys:\n            raise DataError('MIGRATE requires at least one key')\n        pieces = []\n        if copy:\n            pieces.append(Token.get_token('COPY'))\n        if replace:\n            pieces.append(Token.get_token('REPLACE'))\n        if auth:\n            pieces.append(Token.get_token('AUTH'))\n            pieces.append(auth)\n        pieces.append(Token.get_token('KEYS'))\n        pieces.extend(keys)\n        return self.execute_command('MIGRATE', host, port, '', destination_db,\n                                    timeout, *pieces)", "response": "Migrate one or more keys from the current Redis server to a different Redis server."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef object(self, infotype, key):\n        \"Return the encoding, idletime, or refcount about the key\"\n        return self.execute_command('OBJECT', infotype, key, infotype=infotype)", "response": "Return the encoding idletime or refcount about the key"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the total memory usage for the specified key and its value and associated administrative overheads.", "response": "def memory_usage(self, key, samples=None):\n        \"\"\"\n        Return the total memory usage for key, its value and associated\n        administrative overheads.\n\n        For nested data structures, ``samples`` is the number of elements to\n        sample. If left unspecified, the server's default is 5. Use 0 to sample\n        all elements.\n        \"\"\"\n        args = []\n        if isinstance(samples, int):\n            args.extend([Token.get_token('SAMPLES'), samples])\n        return self.execute_command('MEMORY USAGE', key, *args)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef shutdown(self, save=False, nosave=False):\n        if save and nosave:\n            raise DataError('SHUTDOWN save and nosave cannot both be set')\n        args = ['SHUTDOWN']\n        if save:\n            args.append('SAVE')\n        if nosave:\n            args.append('NOSAVE')\n        try:\n            self.execute_command(*args)\n        except ConnectionError:\n            # a ConnectionError here is expected\n            return\n        raise RedisError(\"SHUTDOWN seems to have failed.\")", "response": "Shutdown the Redis server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef slaveof(self, host=None, port=None):\n        if host is None and port is None:\n            return self.execute_command('SLAVEOF', Token.get_token('NO'),\n                                        Token.get_token('ONE'))\n        return self.execute_command('SLAVEOF', host, port)", "response": "Set the server to be a replicated slave of the instance identified by the host and port."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms a bitwise operation between keys and store the result in dest.", "response": "def bitop(self, operation, dest, *keys):\n        \"\"\"\n        Perform a bitwise operation using ``operation`` between ``keys`` and\n        store the result in ``dest``.\n        \"\"\"\n        return self.execute_command('BITOP', operation, dest, *keys)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the position of the first set bit in a string.", "response": "def bitpos(self, key, bit, start=None, end=None):\n        \"\"\"\n        Return the position of the first bit set to 1 or 0 in a string.\n        ``start`` and ``end`` difines search range. The range is interpreted\n        as a range of bytes and not a range of bits, so start=0 and end=2\n        means to look at the first three bytes.\n        \"\"\"\n        if bit not in (0, 1):\n            raise DataError('bit must be 0 or 1')\n        params = [key, bit]\n\n        start is not None and params.append(start)\n\n        if start is not None and end is not None:\n            params.append(end)\n        elif start is None and end is not None:\n            raise DataError(\"start argument is not set, \"\n                            \"when end is specified\")\n        return self.execute_command('BITPOS', *params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef expireat(self, name, when):\n        if isinstance(when, datetime.datetime):\n            when = int(mod_time.mktime(when.timetuple()))\n        return self.execute_command('EXPIREAT', name, when)", "response": "Set an expire flag on key name. When can be represented as an integer indicating unix time. When can be represented as an integer indicating unix time."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of values ordered identically to keys", "response": "def mget(self, keys, *args):\n        \"\"\"\n        Returns a list of values ordered identically to ``keys``\n        \"\"\"\n        args = list_or_args(keys, args)\n        options = {}\n        if not args:\n            options[EMPTY_RESPONSE] = []\n        return self.execute_command('MGET', *args, **options)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef msetnx(self, mapping):\n        items = []\n        for pair in iteritems(mapping):\n            items.extend(pair)\n        return self.execute_command('MSETNX', *items)", "response": "Sets the key - value pairs for the specified key - value pairs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset an expire flag on key name for time milliseconds.", "response": "def pexpire(self, name, time):\n        \"\"\"\n        Set an expire flag on key ``name`` for ``time`` milliseconds.\n        ``time`` can be represented by an integer or a Python timedelta\n        object.\n        \"\"\"\n        if isinstance(time, datetime.timedelta):\n            time = int(time.total_seconds() * 1000)\n        return self.execute_command('PEXPIRE', name, time)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pexpireat(self, name, when):\n        if isinstance(when, datetime.datetime):\n            ms = int(when.microsecond / 1000)\n            when = int(mod_time.mktime(when.timetuple())) * 1000 + ms\n        return self.execute_command('PEXPIREAT', name, when)", "response": "Set an expire flag on key name when time. When can be represented as an integer representing unix time in milliseconds."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the value of key name to value that expires in time_ms milliseconds.", "response": "def psetex(self, name, time_ms, value):\n        \"\"\"\n        Set the value of key ``name`` to ``value`` that expires in ``time_ms``\n        milliseconds. ``time_ms`` can be represented by an integer or a Python\n        timedelta object\n        \"\"\"\n        if isinstance(time_ms, datetime.timedelta):\n            time_ms = int(time_ms.total_seconds() * 1000)\n        return self.execute_command('PSETEX', name, time_ms, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef restore(self, name, ttl, value, replace=False):\n        params = [name, ttl, value]\n        if replace:\n            params.append('REPLACE')\n        return self.execute_command('RESTORE', *params)", "response": "Restores a key from the given serialized value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set(self, name, value, ex=None, px=None, nx=False, xx=False):\n        pieces = [name, value]\n        if ex is not None:\n            pieces.append('EX')\n            if isinstance(ex, datetime.timedelta):\n                ex = int(ex.total_seconds())\n            pieces.append(ex)\n        if px is not None:\n            pieces.append('PX')\n            if isinstance(px, datetime.timedelta):\n                px = int(px.total_seconds() * 1000)\n            pieces.append(px)\n\n        if nx:\n            pieces.append('NX')\n        if xx:\n            pieces.append('XX')\n        return self.execute_command('SET', *pieces)", "response": "Set the value at key name to value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setbit(self, name, offset, value):\n        value = value and 1 or 0\n        return self.execute_command('SETBIT', name, offset, value)", "response": "Set the value of the bit at offset in name to value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setex(self, name, time, value):\n        if isinstance(time, datetime.timedelta):\n            time = int(time.total_seconds())\n        return self.execute_command('SETEX', name, time, value)", "response": "Set the value of key name to value that expires in time."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the value of the key name to value starting at offset.", "response": "def setrange(self, name, offset, value):\n        \"\"\"\n        Overwrite bytes in the value of ``name`` starting at ``offset`` with\n        ``value``. If ``offset`` plus the length of ``value`` exceeds the\n        length of the original value, the new value will be larger than before.\n        If ``offset`` exceeds the length of the original value, null bytes\n        will be used to pad between the end of the previous value and the start\n        of what's being injected.\n\n        Returns the length of the new string.\n        \"\"\"\n        return self.execute_command('SETRANGE', name, offset, value)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npopping and returns the first element of the list named in keys. If timeout is not specified then block indefinitely for timeout seconds.", "response": "def blpop(self, keys, timeout=0):\n        \"\"\"\n        LPOP a value off of the first non-empty list\n        named in the ``keys`` list.\n\n        If none of the lists in ``keys`` has a value to LPOP, then block\n        for ``timeout`` seconds, or until a value gets pushed on to one\n        of the lists.\n\n        If timeout is 0, then block indefinitely.\n        \"\"\"\n        if timeout is None:\n            timeout = 0\n        keys = list_or_args(keys, None)\n        keys.append(timeout)\n        return self.execute_command('BLPOP', *keys)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npops and returns the first element of the list named in keys. If timeout is not specified then block indefinitely for timeout seconds.", "response": "def brpop(self, keys, timeout=0):\n        \"\"\"\n        RPOP a value off of the first non-empty list\n        named in the ``keys`` list.\n\n        If none of the lists in ``keys`` has a value to RPOP, then block\n        for ``timeout`` seconds, or until a value gets pushed on to one\n        of the lists.\n\n        If timeout is 0, then block indefinitely.\n        \"\"\"\n        if timeout is None:\n            timeout = 0\n        keys = list_or_args(keys, None)\n        keys.append(timeout)\n        return self.execute_command('BRPOP', *keys)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lrange(self, name, start, end):\n        return self.execute_command('LRANGE', name, start, end)", "response": "Return a slice of the list name between start and end"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lrem(self, name, count, value):\n        return self.execute_command('LREM', name, count, value)", "response": "Remove the first count occurrences of elements equal to value from the list stored at name."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset position of list name to value", "response": "def lset(self, name, index, value):\n        \"Set ``position`` of list ``name`` to ``value``\"\n        return self.execute_command('LSET', name, index, value)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsorting and return the list set or sorted set at name.", "response": "def sort(self, name, start=None, num=None, by=None, get=None,\n             desc=False, alpha=False, store=None, groups=False):\n        \"\"\"\n        Sort and return the list, set or sorted set at ``name``.\n\n        ``start`` and ``num`` allow for paging through the sorted data\n\n        ``by`` allows using an external key to weight and sort the items.\n            Use an \"*\" to indicate where in the key the item value is located\n\n        ``get`` allows for returning items from external keys rather than the\n            sorted data itself.  Use an \"*\" to indicate where int he key\n            the item value is located\n\n        ``desc`` allows for reversing the sort\n\n        ``alpha`` allows for sorting lexicographically rather than numerically\n\n        ``store`` allows for storing the result of the sort into\n            the key ``store``\n\n        ``groups`` if set to True and if ``get`` contains at least two\n            elements, sort will return a list of tuples, each containing the\n            values fetched from the arguments to ``get``.\n\n        \"\"\"\n        if (start is not None and num is None) or \\\n                (num is not None and start is None):\n            raise DataError(\"``start`` and ``num`` must both be specified\")\n\n        pieces = [name]\n        if by is not None:\n            pieces.append(Token.get_token('BY'))\n            pieces.append(by)\n        if start is not None and num is not None:\n            pieces.append(Token.get_token('LIMIT'))\n            pieces.append(start)\n            pieces.append(num)\n        if get is not None:\n            # If get is a string assume we want to get a single value.\n            # Otherwise assume it's an interable and we want to get multiple\n            # values. We can't just iterate blindly because strings are\n            # iterable.\n            if isinstance(get, (bytes, basestring)):\n                pieces.append(Token.get_token('GET'))\n                pieces.append(get)\n            else:\n                for g in get:\n                    pieces.append(Token.get_token('GET'))\n                    pieces.append(g)\n        if desc:\n            pieces.append(Token.get_token('DESC'))\n        if alpha:\n            pieces.append(Token.get_token('ALPHA'))\n        if store is not None:\n            pieces.append(Token.get_token('STORE'))\n            pieces.append(store)\n\n        if groups:\n            if not get or isinstance(get, (bytes, basestring)) or len(get) < 2:\n                raise DataError('when using \"groups\" the \"get\" argument '\n                                'must be specified and contain at least '\n                                'two keys')\n\n        options = {'groups': len(get) if groups else None}\n        return self.execute_command('SORT', *pieces, **options)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an iterator over the items in the cache for the specified pattern and number of returns.", "response": "def scan_iter(self, match=None, count=None):\n        \"\"\"\n        Make an iterator using the SCAN command so that the client doesn't\n        need to remember the cursor position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` allows for hint the minimum number of returns\n        \"\"\"\n        cursor = '0'\n        while cursor != 0:\n            cursor, data = self.scan(cursor=cursor, match=match, count=count)\n            for item in data:\n                yield item"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef smove(self, src, dst, value):\n        \"Move ``value`` from set ``src`` to set ``dst`` atomically\"\n        return self.execute_command('SMOVE', src, dst, value)", "response": "Move value from set src to set dst atomically"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef spop(self, name, count=None):\n        \"Remove and return a random member of set ``name``\"\n        args = (count is not None) and [count] or []\n        return self.execute_command('SPOP', name, *args)", "response": "Remove and return a random member of set name"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nacknowledging the successful processing of one or more messages.", "response": "def xack(self, name, groupname, *ids):\n        \"\"\"\n        Acknowledges the successful processing of one or more messages.\n        name: name of the stream.\n        groupname: name of the consumer group.\n        *ids: message ids to acknowlege.\n        \"\"\"\n        return self.execute_command('XACK', name, groupname, *ids)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef xadd(self, name, fields, id='*', maxlen=None, approximate=True):\n        pieces = []\n        if maxlen is not None:\n            if not isinstance(maxlen, (int, long)) or maxlen < 1:\n                raise DataError('XADD maxlen must be a positive integer')\n            pieces.append(Token.get_token('MAXLEN'))\n            if approximate:\n                pieces.append(Token.get_token('~'))\n            pieces.append(str(maxlen))\n        pieces.append(id)\n        if not isinstance(fields, dict) or len(fields) == 0:\n            raise DataError('XADD fields must be a non-empty dict')\n        for pair in iteritems(fields):\n            pieces.extend(pair)\n        return self.execute_command('XADD', name, *pieces)", "response": "Add a new entry to a stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef xclaim(self, name, groupname, consumername, min_idle_time, message_ids,\n               idle=None, time=None, retrycount=None, force=False,\n               justid=False):\n        \"\"\"\n        Changes the ownership of a pending message.\n        name: name of the stream.\n        groupname: name of the consumer group.\n        consumername: name of a consumer that claims the message.\n        min_idle_time: filter messages that were idle less than this amount of\n        milliseconds\n        message_ids: non-empty list or tuple of message IDs to claim\n        idle: optional. Set the idle time (last time it was delivered) of the\n         message in ms\n        time: optional integer. This is the same as idle but instead of a\n         relative amount of milliseconds, it sets the idle time to a specific\n         Unix time (in milliseconds).\n        retrycount: optional integer. set the retry counter to the specified\n         value. This counter is incremented every time a message is delivered\n         again.\n        force: optional boolean, false by default. Creates the pending message\n         entry in the PEL even if certain specified IDs are not already in the\n         PEL assigned to a different client.\n        justid: optional boolean, false by default. Return just an array of IDs\n         of messages successfully claimed, without returning the actual message\n        \"\"\"\n        if not isinstance(min_idle_time, (int, long)) or min_idle_time < 0:\n            raise DataError(\"XCLAIM min_idle_time must be a non negative \"\n                            \"integer\")\n        if not isinstance(message_ids, (list, tuple)) or not message_ids:\n            raise DataError(\"XCLAIM message_ids must be a non empty list or \"\n                            \"tuple of message IDs to claim\")\n\n        kwargs = {}\n        pieces = [name, groupname, consumername, str(min_idle_time)]\n        pieces.extend(list(message_ids))\n\n        if idle is not None:\n            if not isinstance(idle, (int, long)):\n                raise DataError(\"XCLAIM idle must be an integer\")\n            pieces.extend((Token.get_token('IDLE'), str(idle)))\n        if time is not None:\n            if not isinstance(time, (int, long)):\n                raise DataError(\"XCLAIM time must be an integer\")\n            pieces.extend((Token.get_token('TIME'), str(time)))\n        if retrycount is not None:\n            if not isinstance(retrycount, (int, long)):\n                raise DataError(\"XCLAIM retrycount must be an integer\")\n            pieces.extend((Token.get_token('RETRYCOUNT'), str(retrycount)))\n\n        if force:\n            if not isinstance(force, bool):\n                raise DataError(\"XCLAIM force must be a boolean\")\n            pieces.append(Token.get_token('FORCE'))\n        if justid:\n            if not isinstance(justid, bool):\n                raise DataError(\"XCLAIM justid must be a boolean\")\n            pieces.append(Token.get_token('JUSTID'))\n            kwargs['parse_justid'] = True\n        return self.execute_command('XCLAIM', *pieces, **kwargs)", "response": "This method is used to claim a pending message in a consumer group."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new consumer group associated with a stream.", "response": "def xgroup_create(self, name, groupname, id='$', mkstream=False):\n        \"\"\"\n        Create a new consumer group associated with a stream.\n        name: name of the stream.\n        groupname: name of the consumer group.\n        id: ID of the last item in the stream to consider already delivered.\n        \"\"\"\n        pieces = ['XGROUP CREATE', name, groupname, id]\n        if mkstream:\n            pieces.append(Token.get_token('MKSTREAM'))\n        return self.execute_command(*pieces)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef xgroup_delconsumer(self, name, groupname, consumername):\n        return self.execute_command('XGROUP DELCONSUMER', name, groupname,\n                                    consumername)", "response": "Remove a specific consumer from a consumer group."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the last item in the consumer group to something else.", "response": "def xgroup_setid(self, name, groupname, id):\n        \"\"\"\n        Set the consumer group last delivered ID to something else.\n        name: name of the stream.\n        groupname: name of the consumer group.\n        id: ID of the last item in the stream to consider already delivered.\n        \"\"\"\n        return self.execute_command('XGROUP SETID', name, groupname, id)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning information about pending messages in a range.", "response": "def xpending_range(self, name, groupname, min, max, count,\n                       consumername=None):\n        \"\"\"\n        Returns information about pending messages, in a range.\n        name: name of the stream.\n        groupname: name of the consumer group.\n        min: minimum stream ID.\n        max: maximum stream ID.\n        count: number of messages to return\n        consumername: name of a consumer to filter by (optional).\n        \"\"\"\n        pieces = [name, groupname]\n        if min is not None or max is not None or count is not None:\n            if min is None or max is None or count is None:\n                raise DataError(\"XPENDING must be provided with min, max \"\n                                \"and count parameters, or none of them. \")\n            if not isinstance(count, (int, long)) or count < -1:\n                raise DataError(\"XPENDING count must be a integer >= -1\")\n            pieces.extend((min, max, str(count)))\n        if consumername is not None:\n            if min is None or max is None or count is None:\n                raise DataError(\"if XPENDING is provided with consumername,\"\n                                \" it must be provided with min, max and\"\n                                \" count parameters\")\n            pieces.append(consumername)\n        return self.execute_command('XPENDING', *pieces, parse_detail=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading stream values within an interval.", "response": "def xrange(self, name, min='-', max='+', count=None):\n        \"\"\"\n        Read stream values within an interval.\n        name: name of the stream.\n        start: first stream ID. defaults to '-',\n               meaning the earliest available.\n        finish: last stream ID. defaults to '+',\n                meaning the latest available.\n        count: if set, only return this many items, beginning with the\n               earliest available.\n        \"\"\"\n        pieces = [min, max]\n        if count is not None:\n            if not isinstance(count, (int, long)) or count < 1:\n                raise DataError('XRANGE count must be a positive integer')\n            pieces.append(Token.get_token('COUNT'))\n            pieces.append(str(count))\n\n        return self.execute_command('XRANGE', name, *pieces)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef xread(self, streams, count=None, block=None):\n        pieces = []\n        if block is not None:\n            if not isinstance(block, (int, long)) or block < 0:\n                raise DataError('XREAD block must be a non-negative integer')\n            pieces.append(Token.get_token('BLOCK'))\n            pieces.append(str(block))\n        if count is not None:\n            if not isinstance(count, (int, long)) or count < 1:\n                raise DataError('XREAD count must be a positive integer')\n            pieces.append(Token.get_token('COUNT'))\n            pieces.append(str(count))\n        if not isinstance(streams, dict) or len(streams) == 0:\n            raise DataError('XREAD streams must be a non empty dict')\n        pieces.append(Token.get_token('STREAMS'))\n        keys, values = izip(*iteritems(streams))\n        pieces.extend(keys)\n        pieces.extend(values)\n        return self.execute_command('XREAD', *pieces)", "response": "Read many items from a set of streams."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads from a consumer group via a consumer group.", "response": "def xreadgroup(self, groupname, consumername, streams, count=None,\n                   block=None, noack=False):\n        \"\"\"\n        Read from a stream via a consumer group.\n        groupname: name of the consumer group.\n        consumername: name of the requesting consumer.\n        streams: a dict of stream names to stream IDs, where\n               IDs indicate the last ID already seen.\n        count: if set, only return this many items, beginning with the\n               earliest available.\n        block: number of milliseconds to wait, if nothing already present.\n        noack: do not add messages to the PEL\n        \"\"\"\n        pieces = [Token.get_token('GROUP'), groupname, consumername]\n        if count is not None:\n            if not isinstance(count, (int, long)) or count < 1:\n                raise DataError(\"XREADGROUP count must be a positive integer\")\n            pieces.append(Token.get_token(\"COUNT\"))\n            pieces.append(str(count))\n        if block is not None:\n            if not isinstance(block, (int, long)) or block < 0:\n                raise DataError(\"XREADGROUP block must be a non-negative \"\n                                \"integer\")\n            pieces.append(Token.get_token(\"BLOCK\"))\n            pieces.append(str(block))\n        if noack:\n            pieces.append(Token.get_token(\"NOACK\"))\n        if not isinstance(streams, dict) or len(streams) == 0:\n            raise DataError('XREADGROUP streams must be a non empty dict')\n        pieces.append(Token.get_token('STREAMS'))\n        pieces.extend(streams.keys())\n        pieces.extend(streams.values())\n        return self.execute_command('XREADGROUP', *pieces)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntrimming old messages from a stream.", "response": "def xtrim(self, name, maxlen, approximate=True):\n        \"\"\"\n        Trims old messages from a stream.\n        name: name of the stream.\n        maxlen: truncate old stream messages beyond this size\n        approximate: actual stream length may be slightly more than maxlen\n        \"\"\"\n        pieces = [Token.get_token('MAXLEN')]\n        if approximate:\n            pieces.append(Token.get_token('~'))\n        pieces.append(maxlen)\n        return self.execute_command('XTRIM', name, *pieces)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds any number of elements to the specified key.", "response": "def zadd(self, name, mapping, nx=False, xx=False, ch=False, incr=False):\n        \"\"\"\n        Set any number of element-name, score pairs to the key ``name``. Pairs\n        are specified as a dict of element-names keys to score values.\n\n        ``nx`` forces ZADD to only create new elements and not to update\n        scores for elements that already exist.\n\n        ``xx`` forces ZADD to only update scores of elements that already\n        exist. New elements will not be added.\n\n        ``ch`` modifies the return value to be the numbers of elements changed.\n        Changed elements include new elements that were added and elements\n        whose scores changed.\n\n        ``incr`` modifies ZADD to behave like ZINCRBY. In this mode only a\n        single element/score pair can be specified and the score is the amount\n        the existing score will be incremented by. When using this mode the\n        return value of ZADD will be the new score of the element.\n\n        The return value of ZADD varies based on the mode specified. With no\n        options, ZADD returns the number of new elements added to the sorted\n        set.\n        \"\"\"\n        if not mapping:\n            raise DataError(\"ZADD requires at least one element/score pair\")\n        if nx and xx:\n            raise DataError(\"ZADD allows either 'nx' or 'xx', not both\")\n        if incr and len(mapping) != 1:\n            raise DataError(\"ZADD option 'incr' only works when passing a \"\n                            \"single element/score pair\")\n        pieces = []\n        options = {}\n        if nx:\n            pieces.append(Token.get_token('NX'))\n        if xx:\n            pieces.append(Token.get_token('XX'))\n        if ch:\n            pieces.append(Token.get_token('CH'))\n        if incr:\n            pieces.append(Token.get_token('INCR'))\n            options['as_score'] = True\n        for pair in iteritems(mapping):\n            pieces.append(pair[1])\n            pieces.append(pair[0])\n        return self.execute_command('ZADD', name, *pieces, **options)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the number of elements in the sorted set at key name with a score between min and max.", "response": "def zcount(self, name, min, max):\n        \"\"\"\n        Returns the number of elements in the sorted set at key ``name`` with\n        a score between ``min`` and ``max``.\n        \"\"\"\n        return self.execute_command('ZCOUNT', name, min, max)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nincrements the score of value in sorted set name by amount", "response": "def zincrby(self, name, amount, value):\n        \"Increment the score of ``value`` in sorted set ``name`` by ``amount``\"\n        return self.execute_command('ZINCRBY', name, amount, value)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nintersecting multiple sorted sets specified by keys into a new sorted set dest.", "response": "def zinterstore(self, dest, keys, aggregate=None):\n        \"\"\"\n        Intersect multiple sorted sets specified by ``keys`` into\n        a new sorted set, ``dest``. Scores in the destination will be\n        aggregated based on the ``aggregate``, or SUM if none is provided.\n        \"\"\"\n        return self._zaggregate('ZINTERSTORE', dest, keys, aggregate)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the number of items in the sorted set name between the lexicographical range min and max.", "response": "def zlexcount(self, name, min, max):\n        \"\"\"\n        Return the number of items in the sorted set ``name`` between the\n        lexicographical range ``min`` and ``max``.\n        \"\"\"\n        return self.execute_command('ZLEXCOUNT', name, min, max)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef zpopmax(self, name, count=None):\n        args = (count is not None) and [count] or []\n        options = {\n            'withscores': True\n        }\n        return self.execute_command('ZPOPMAX', name, *args, **options)", "response": "Remove and return up to count members with the highest scores\n            from the sorted set name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef zpopmin(self, name, count=None):\n        args = (count is not None) and [count] or []\n        options = {\n            'withscores': True\n        }\n        return self.execute_command('ZPOPMIN', name, *args, **options)", "response": "Remove and return up to count members with the lowest scores\n            from the sorted set name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bzpopmax(self, keys, timeout=0):\n        if timeout is None:\n            timeout = 0\n        keys = list_or_args(keys, None)\n        keys.append(timeout)\n        return self.execute_command('BZPOPMAX', *keys)", "response": "Remove and return the highest priority set from the sorted set at the given keys. If timeout is not set then block indefinitely until a member gets added to the first non - empty set at the given keys. If timeout is set then block indefinitely until a member gets added to the first non - empty set at the given keys."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef zrangebylex(self, name, min, max, start=None, num=None):\n        if (start is not None and num is None) or \\\n                (num is not None and start is None):\n            raise DataError(\"``start`` and ``num`` must both be specified\")\n        pieces = ['ZRANGEBYLEX', name, min, max]\n        if start is not None and num is not None:\n            pieces.extend([Token.get_token('LIMIT'), start, num])\n        return self.execute_command(*pieces)", "response": "Return the lexicographical range of values from sorted set name between min and max."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving all elements in the sorted set name between the lexicographical range specified by min and max.", "response": "def zremrangebylex(self, name, min, max):\n        \"\"\"\n        Remove all elements in the sorted set ``name`` between the\n        lexicographical range specified by ``min`` and ``max``.\n\n        Returns the number of elements removed.\n        \"\"\"\n        return self.execute_command('ZREMRANGEBYLEX', name, min, max)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef zremrangebyrank(self, name, min, max):\n        return self.execute_command('ZREMRANGEBYRANK', name, min, max)", "response": "Remove all elements in the sorted set name with ranks between min and max."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a range of values from the sorted set name with scores between min and max in descending order.", "response": "def zrevrangebyscore(self, name, max, min, start=None, num=None,\n                         withscores=False, score_cast_func=float):\n        \"\"\"\n        Return a range of values from the sorted set ``name`` with scores\n        between ``min`` and ``max`` in descending order.\n\n        If ``start`` and ``num`` are specified, then return a slice\n        of the range.\n\n        ``withscores`` indicates to return the scores along with the values.\n        The return type is a list of (value, score) pairs\n\n        ``score_cast_func`` a callable used to cast the score return value\n        \"\"\"\n        if (start is not None and num is None) or \\\n                (num is not None and start is None):\n            raise DataError(\"``start`` and ``num`` must both be specified\")\n        pieces = ['ZREVRANGEBYSCORE', name, max, min]\n        if start is not None and num is not None:\n            pieces.extend([Token.get_token('LIMIT'), start, num])\n        if withscores:\n            pieces.append(Token.get_token('WITHSCORES'))\n        options = {\n            'withscores': withscores,\n            'score_cast_func': score_cast_func\n        }\n        return self.execute_command(*pieces, **options)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef zunionstore(self, dest, keys, aggregate=None):\n        return self._zaggregate('ZUNIONSTORE', dest, keys, aggregate)", "response": "Union multiple sorted sets specified by keys into a new sorted set dest."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hmget(self, name, keys, *args):\n        \"Returns a list of values ordered identically to ``keys``\"\n        args = list_or_args(keys, args)\n        return self.execute_command('HMGET', name, *args)", "response": "Returns a list of values ordered identically to keys"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting a Lua script and return the result.", "response": "def eval(self, script, numkeys, *keys_and_args):\n        \"\"\"\n        Execute the Lua ``script``, specifying the ``numkeys`` the script\n        will touch and the key names and argument values in ``keys_and_args``.\n        Returns the result of the script.\n\n        In practice, use the object returned by ``register_script``. This\n        function exists purely for Redis API completion.\n        \"\"\"\n        return self.execute_command('EVAL', script, numkeys, *keys_and_args)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef evalsha(self, sha, numkeys, *keys_and_args):\n        return self.execute_command('EVALSHA', sha, numkeys, *keys_and_args)", "response": "Execute a Lua script with the given sha."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef geoadd(self, name, *values):\n        if len(values) % 3 != 0:\n            raise DataError(\"GEOADD requires places with lon, lat and name\"\n                            \" values\")\n        return self.execute_command('GEOADD', name, *values)", "response": "Add the specified geospatial items to the specified key identified by the name argument."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef geodist(self, name, place1, place2, unit=None):\n        pieces = [name, place1, place2]\n        if unit and unit not in ('m', 'km', 'mi', 'ft'):\n            raise DataError(\"GEODIST invalid unit\")\n        elif unit:\n            pieces.append(unit)\n        return self.execute_command('GEODIST', *pieces)", "response": "Return the distance between place1 and place2 members of the key name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the response from a publish / subscribe command", "response": "def parse_response(self, block=True, timeout=0):\n        \"Parse the response from a publish/subscribe command\"\n        connection = self.connection\n        if connection is None:\n            raise RuntimeError(\n                'pubsub connection not set: '\n                'did you forget to call subscribe() or psubscribe()?')\n        if not block and not connection.can_read(timeout=timeout):\n            return None\n        return self._execute(connection, connection.read_response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nnormalizes channel names to be either bytes or strings based on whether responses are automatically decoded.", "response": "def _normalize_keys(self, data):\n        \"\"\"\n        normalize channel/pattern names to be either bytes or strings\n        based on whether responses are automatically decoded. this saves us\n        from coercing the value for each message coming in.\n        \"\"\"\n        encode = self.encoder.encode\n        decode = self.encoder.decode\n        return {decode(encode(k)): v for k, v in iteritems(data)}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsubscribe to channel patterns.", "response": "def psubscribe(self, *args, **kwargs):\n        \"\"\"\n        Subscribe to channel patterns. Patterns supplied as keyword arguments\n        expect a pattern name as the key and a callable as the value. A\n        pattern's callable will be invoked automatically when a message is\n        received on that pattern rather than producing a message via\n        ``listen()``.\n        \"\"\"\n        if args:\n            args = list_or_args(args[0], args[1:])\n        new_patterns = dict.fromkeys(args)\n        new_patterns.update(kwargs)\n        ret_val = self.execute_command('PSUBSCRIBE', *iterkeys(new_patterns))\n        # update the patterns dict AFTER we send the command. we don't want to\n        # subscribe twice to these patterns, once for the command and again\n        # for the reconnection.\n        new_patterns = self._normalize_keys(new_patterns)\n        self.patterns.update(new_patterns)\n        self.pending_unsubscribe_patterns.difference_update(new_patterns)\n        return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef punsubscribe(self, *args):\n        if args:\n            args = list_or_args(args[0], args[1:])\n            patterns = self._normalize_keys(dict.fromkeys(args))\n        else:\n            patterns = self.patterns\n        self.pending_unsubscribe_patterns.update(patterns)\n        return self.execute_command('PUNSUBSCRIBE', *args)", "response": "Unsubscribe from the supplied patterns."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef subscribe(self, *args, **kwargs):\n        if args:\n            args = list_or_args(args[0], args[1:])\n        new_channels = dict.fromkeys(args)\n        new_channels.update(kwargs)\n        ret_val = self.execute_command('SUBSCRIBE', *iterkeys(new_channels))\n        # update the channels dict AFTER we send the command. we don't want to\n        # subscribe twice to these channels, once for the command and again\n        # for the reconnection.\n        new_channels = self._normalize_keys(new_channels)\n        self.channels.update(new_channels)\n        self.pending_unsubscribe_channels.difference_update(new_channels)\n        return ret_val", "response": "Subscribe to channels. Channels supplied as keyword arguments expect\n        a channel name as the key and a callable as the value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unsubscribe(self, *args):\n        if args:\n            args = list_or_args(args[0], args[1:])\n            channels = self._normalize_keys(dict.fromkeys(args))\n        else:\n            channels = self.channels\n        self.pending_unsubscribe_channels.update(channels)\n        return self.execute_command('UNSUBSCRIBE', *args)", "response": "Unsubscribe from the supplied channels."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_message(self, response, ignore_subscribe_messages=False):\n        message_type = nativestr(response[0])\n        if message_type == 'pmessage':\n            message = {\n                'type': message_type,\n                'pattern': response[1],\n                'channel': response[2],\n                'data': response[3]\n            }\n        elif message_type == 'pong':\n            message = {\n                'type': message_type,\n                'pattern': None,\n                'channel': None,\n                'data': response[1]\n            }\n        else:\n            message = {\n                'type': message_type,\n                'pattern': None,\n                'channel': response[1],\n                'data': response[2]\n            }\n\n        # if this is an unsubscribe message, remove it from memory\n        if message_type in self.UNSUBSCRIBE_MESSAGE_TYPES:\n            if message_type == 'punsubscribe':\n                pattern = response[1]\n                if pattern in self.pending_unsubscribe_patterns:\n                    self.pending_unsubscribe_patterns.remove(pattern)\n                    self.patterns.pop(pattern, None)\n            else:\n                channel = response[1]\n                if channel in self.pending_unsubscribe_channels:\n                    self.pending_unsubscribe_channels.remove(channel)\n                    self.channels.pop(channel, None)\n\n        if message_type in self.PUBLISH_MESSAGE_TYPES:\n            # if there's a message handler, invoke it\n            if message_type == 'pmessage':\n                handler = self.patterns.get(message['pattern'], None)\n            else:\n                handler = self.channels.get(message['channel'], None)\n            if handler:\n                handler(message)\n                return None\n        elif message_type != 'pong':\n            # this is a subscribe/unsubscribe message. ignore if we don't\n            # want them\n            if ignore_subscribe_messages or self.ignore_subscribe_messages:\n                return None\n\n        return message", "response": "Parses a pub / sub message and returns a parsed version of the message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef multi(self):\n        if self.explicit_transaction:\n            raise RedisError('Cannot issue nested calls to MULTI')\n        if self.command_stack:\n            raise RedisError('Commands without an initial WATCH have already '\n                             'been issued')\n        self.explicit_transaction = True", "response": "This method is used to issue multiple commands at once."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pipeline_execute_command(self, *args, **options):\n        self.command_stack.append((args, options))\n        return self", "response": "Execute a command in the pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reset(self):\n        self.operations = []\n        self._last_overflow = 'WRAP'\n        self.overflow(self._default_overflow or self._last_overflow)", "response": "Reset the state of the instance to when it was constructed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the overflow algorithm of successive INCRBY operations", "response": "def overflow(self, overflow):\n        \"\"\"\n        Update the overflow algorithm of successive INCRBY operations\n        :param overflow: Overflow algorithm, one of WRAP, SAT, FAIL. See the\n            Redis docs for descriptions of these algorithmsself.\n        :returns: a :py:class:`BitFieldOperation` instance.\n        \"\"\"\n        overflow = overflow.upper()\n        if overflow != self._last_overflow:\n            self._last_overflow = overflow\n            self.operations.append(('OVERFLOW', overflow))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(self, fmt, offset):\n        self.operations.append(('GET', fmt, offset))\n        return self", "response": "Adds a GET operation to the bitfield."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the value of a given bitfield at the given offset.", "response": "def set(self, fmt, offset, value):\n        \"\"\"\n        Set the value of a given bitfield.\n        :param fmt: format-string for the bitfield being read, e.g. 'u8' for\n            an unsigned 8-bit integer.\n        :param offset: offset (in number of bits). If prefixed with a\n            '#', this is an offset multiplier, e.g. given the arguments\n            fmt='u8', offset='#2', the offset will be 16.\n        :param int value: value to set at the given position.\n        :returns: a :py:class:`BitFieldOperation` instance.\n        \"\"\"\n        self.operations.append(('SET', fmt, offset, value))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef execute(self):\n        command = self.command\n        self.reset()\n        return self.client.execute_command(*command)", "response": "Execute the command in a single BITFIELD command."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a cached token object or creates a new one if not already cached", "response": "def get_token(cls, value):\n        \"Gets a cached token object or creates a new one if not already cached\"\n\n        # Use try/except because after running for a short time most tokens\n        # should already be cached\n        try:\n            return cls._cache[value]\n        except KeyError:\n            token = Token(value)\n            cls._cache[value] = token\n            return token"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a bytestring representation of the value", "response": "def encode(self, value):\n        \"Return a bytestring representation of the value\"\n        if isinstance(value, Token):\n            return value.encoded_value\n        elif isinstance(value, bytes):\n            return value\n        elif isinstance(value, bool):\n            # special case bool since it is a subclass of int\n            raise DataError(\"Invalid input of type: 'bool'. Convert to a \"\n                            \"byte, string or number first.\")\n        elif isinstance(value, float):\n            value = repr(value).encode()\n        elif isinstance(value, (int, long)):\n            # python 2 repr() on longs is '123L', so use str() instead\n            value = str(value).encode()\n        elif not isinstance(value, basestring):\n            # a value we don't know how to deal with. throw an error\n            typename = type(value).__name__\n            raise DataError(\"Invalid input of type: '%s'. Convert to a \"\n                            \"byte, string or number first.\" % typename)\n        if isinstance(value, unicode):\n            value = value.encode(self.encoding, self.encoding_errors)\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a unicode string from the byte representation", "response": "def decode(self, value, force=False):\n        \"Return a unicode string from the byte representation\"\n        if (self.decode_responses or force) and isinstance(value, bytes):\n            value = value.decode(self.encoding, self.encoding_errors)\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse an error response", "response": "def parse_error(self, response):\n        \"Parse an error response\"\n        error_code = response.split(' ')[0]\n        if error_code in self.EXCEPTION_CLASSES:\n            response = response[len(error_code) + 1:]\n            exception_class = self.EXCEPTION_CLASSES[error_code]\n            if isinstance(exception_class, dict):\n                exception_class = exception_class.get(response, ResponseError)\n            return exception_class(response)\n        return ResponseError(response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_connect(self, connection):\n        \"Called when the socket connects\"\n        self._sock = connection._sock\n        self._buffer = SocketBuffer(self._sock, self.socket_read_size)\n        self.encoder = connection.encoder", "response": "Called when the socket connects"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on_disconnect(self):\n        \"Called when the socket disconnects\"\n        self._sock = None\n        if self._buffer is not None:\n            self._buffer.close()\n            self._buffer = None\n        self.encoder = None", "response": "Called when the socket disconnects"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconnect to the Redis server if not already connected", "response": "def connect(self):\n        \"Connects to the Redis server if not already connected\"\n        if self._sock:\n            return\n        try:\n            sock = self._connect()\n        except socket.timeout:\n            raise TimeoutError(\"Timeout connecting to server\")\n        except socket.error:\n            e = sys.exc_info()[1]\n            raise ConnectionError(self._error_message(e))\n\n        self._sock = sock\n        self._selector = DefaultSelector(sock)\n        try:\n            self.on_connect()\n        except RedisError:\n            # clean up after any error in on_connect\n            self.disconnect()\n            raise\n\n        # run any user callbacks. right now the only internal callback\n        # is for pubsub channel/pattern resubscription\n        for callback in self._connect_callbacks:\n            callback(self)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndisconnecting from the Redis server", "response": "def disconnect(self):\n        \"Disconnects from the Redis server\"\n        self._parser.on_disconnect()\n        if self._sock is None:\n            return\n        if self._selector is not None:\n            self._selector.close()\n            self._selector = None\n        try:\n            if os.getpid() == self.pid:\n                self._sock.shutdown(socket.SHUT_RDWR)\n            self._sock.close()\n        except socket.error:\n            pass\n        self._sock = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends an already packed command to the Redis server", "response": "def send_packed_command(self, command):\n        \"Send an already packed command to the Redis server\"\n        if not self._sock:\n            self.connect()\n        try:\n            if isinstance(command, str):\n                command = [command]\n            for item in command:\n                self._sock.sendall(item)\n        except socket.timeout:\n            self.disconnect()\n            raise TimeoutError(\"Timeout writing to socket\")\n        except socket.error:\n            e = sys.exc_info()[1]\n            self.disconnect()\n            if len(e.args) == 1:\n                errno, errmsg = 'UNKNOWN', e.args[0]\n            else:\n                errno = e.args[0]\n                errmsg = e.args[1]\n            raise ConnectionError(\"Error %s while writing to socket. %s.\" %\n                                  (errno, errmsg))\n        except:  # noqa: E722\n            self.disconnect()\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npoll the socket to see if there s data that can be read.", "response": "def can_read(self, timeout=0):\n        \"Poll the socket to see if there's data that can be read.\"\n        sock = self._sock\n        if not sock:\n            self.connect()\n            sock = self._sock\n        return self._parser.can_read() or self._selector.can_read(timeout)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_response(self):\n        \"Read the response from a previously sent command\"\n        try:\n            response = self._parser.read_response()\n        except socket.timeout:\n            self.disconnect()\n            raise TimeoutError(\"Timeout reading from %s:%s\" %\n                               (self.host, self.port))\n        except socket.error:\n            self.disconnect()\n            e = sys.exc_info()[1]\n            raise ConnectionError(\"Error while reading from %s:%s : %s\" %\n                                  (self.host, self.port, e.args))\n        except:  # noqa: E722\n            self.disconnect()\n            raise\n        if isinstance(response, ResponseError):\n            raise response\n        return response", "response": "Read the response from a previously sent command"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pack_command(self, *args):\n        \"Pack a series of arguments into the Redis protocol\"\n        output = []\n        # the client might have included 1 or more literal arguments in\n        # the command name, e.g., 'CONFIG GET'. The Redis server expects these\n        # arguments to be sent separately, so split the first argument\n        # manually. All of these arguements get wrapped in the Token class\n        # to prevent them from being encoded.\n        command = args[0]\n        if ' ' in command:\n            args = tuple(Token.get_token(s)\n                         for s in command.split()) + args[1:]\n        else:\n            args = (Token.get_token(command),) + args[1:]\n\n        buff = SYM_EMPTY.join((SYM_STAR, str(len(args)).encode(), SYM_CRLF))\n\n        buffer_cutoff = self._buffer_cutoff\n        for arg in imap(self.encoder.encode, args):\n            # to avoid large string mallocs, chunk the command into the\n            # output list if we're sending large values\n            if len(buff) > buffer_cutoff or len(arg) > buffer_cutoff:\n                buff = SYM_EMPTY.join(\n                    (buff, SYM_DOLLAR, str(len(arg)).encode(), SYM_CRLF))\n                output.append(buff)\n                output.append(arg)\n                buff = SYM_CRLF\n            else:\n                buff = SYM_EMPTY.join(\n                    (buff, SYM_DOLLAR, str(len(arg)).encode(),\n                     SYM_CRLF, arg, SYM_CRLF))\n        output.append(buff)\n        return output", "response": "Pack a series of arguments into the Redis protocol"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _connect(self):\n        \"Wrap the socket with SSL support\"\n        sock = super(SSLConnection, self)._connect()\n        if hasattr(ssl, \"create_default_context\"):\n            context = ssl.create_default_context()\n            context.check_hostname = False\n            context.verify_mode = self.cert_reqs\n            if self.certfile and self.keyfile:\n                context.load_cert_chain(certfile=self.certfile,\n                                        keyfile=self.keyfile)\n            if self.ca_certs:\n                context.load_verify_locations(self.ca_certs)\n            sock = context.wrap_socket(sock, server_hostname=self.host)\n        else:\n            # In case this code runs in a version which is older than 2.7.9,\n            # we want to fall back to old code\n            sock = ssl.wrap_socket(sock,\n                                   cert_reqs=self.cert_reqs,\n                                   keyfile=self.keyfile,\n                                   certfile=self.certfile,\n                                   ca_certs=self.ca_certs)\n        return sock", "response": "Wrap the socket with SSL support"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a Unix domain socket connection", "response": "def _connect(self):\n        \"Create a Unix domain socket connection\"\n        sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n        sock.settimeout(self.socket_timeout)\n        sock.connect(self.path)\n        return sock"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new ConnectionPool configured from a given URL.", "response": "def from_url(cls, url, db=None, decode_components=False, **kwargs):\n        \"\"\"\n        Return a connection pool configured from the given URL.\n\n        For example::\n\n            redis://[:password]@localhost:6379/0\n            rediss://[:password]@localhost:6379/0\n            unix://[:password]@/path/to/socket.sock?db=0\n\n        Three URL schemes are supported:\n\n        - ```redis://``\n          <https://www.iana.org/assignments/uri-schemes/prov/redis>`_ creates a\n          normal TCP socket connection\n        - ```rediss://``\n          <https://www.iana.org/assignments/uri-schemes/prov/rediss>`_ creates\n          a SSL wrapped TCP socket connection\n        - ``unix://`` creates a Unix Domain Socket connection\n\n        There are several ways to specify a database number. The parse function\n        will return the first specified option:\n            1. A ``db`` querystring option, e.g. redis://localhost?db=0\n            2. If using the redis:// scheme, the path argument of the url, e.g.\n               redis://localhost/0\n            3. The ``db`` argument to this function.\n\n        If none of these options are specified, db=0 is used.\n\n        The ``decode_components`` argument allows this function to work with\n        percent-encoded URLs. If this argument is set to ``True`` all ``%xx``\n        escapes will be replaced by their single-character equivalents after\n        the URL has been parsed. This only applies to the ``hostname``,\n        ``path``, and ``password`` components.\n\n        Any additional querystring arguments and keyword arguments will be\n        passed along to the ConnectionPool class's initializer. The querystring\n        arguments ``socket_connect_timeout`` and ``socket_timeout`` if supplied\n        are parsed as float values. The arguments ``socket_keepalive`` and\n        ``retry_on_timeout`` are parsed to boolean values that accept\n        True/False, Yes/No values to indicate state. Invalid types cause a\n        ``UserWarning`` to be raised. In the case of conflicting arguments,\n        querystring arguments always win.\n\n        \"\"\"\n        url = urlparse(url)\n        url_options = {}\n\n        for name, value in iteritems(parse_qs(url.query)):\n            if value and len(value) > 0:\n                parser = URL_QUERY_ARGUMENT_PARSERS.get(name)\n                if parser:\n                    try:\n                        url_options[name] = parser(value[0])\n                    except (TypeError, ValueError):\n                        warnings.warn(UserWarning(\n                            \"Invalid value for `%s` in connection URL.\" % name\n                        ))\n                else:\n                    url_options[name] = value[0]\n\n        if decode_components:\n            password = unquote(url.password) if url.password else None\n            path = unquote(url.path) if url.path else None\n            hostname = unquote(url.hostname) if url.hostname else None\n        else:\n            password = url.password\n            path = url.path\n            hostname = url.hostname\n\n        # We only support redis://, rediss:// and unix:// schemes.\n        if url.scheme == 'unix':\n            url_options.update({\n                'password': password,\n                'path': path,\n                'connection_class': UnixDomainSocketConnection,\n            })\n\n        elif url.scheme in ('redis', 'rediss'):\n            url_options.update({\n                'host': hostname,\n                'port': int(url.port or 6379),\n                'password': password,\n            })\n\n            # If there's a path argument, use it as the db argument if a\n            # querystring value wasn't specified\n            if 'db' not in url_options and path:\n                try:\n                    url_options['db'] = int(path.replace('/', ''))\n                except (AttributeError, ValueError):\n                    pass\n\n            if url.scheme == 'rediss':\n                url_options['connection_class'] = SSLConnection\n        else:\n            valid_schemes = ', '.join(('redis://', 'rediss://', 'unix://'))\n            raise ValueError('Redis URL must specify one of the following'\n                             'schemes (%s)' % valid_schemes)\n\n        # last shot at the db value\n        url_options['db'] = int(url_options.get('db', db or 0))\n\n        # update the arguments from the URL values\n        kwargs.update(url_options)\n\n        # backwards compatability\n        if 'charset' in kwargs:\n            warnings.warn(DeprecationWarning(\n                '\"charset\" is deprecated. Use \"encoding\" instead'))\n            kwargs['encoding'] = kwargs.pop('charset')\n        if 'errors' in kwargs:\n            warnings.warn(DeprecationWarning(\n                '\"errors\" is deprecated. Use \"encoding_errors\" instead'))\n            kwargs['encoding_errors'] = kwargs.pop('errors')\n\n        return cls(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a connection from the pool", "response": "def get_connection(self, command_name, *keys, **options):\n        \"Get a connection from the pool\"\n        self._checkpid()\n        try:\n            connection = self._available_connections.pop()\n        except IndexError:\n            connection = self.make_connection()\n        self._in_use_connections.add(connection)\n        try:\n            # ensure this connection is connected to Redis\n            connection.connect()\n            # connections that the pool provides should be ready to send\n            # a command. if not, the connection was either returned to the\n            # pool before all data has been read or the socket has been\n            # closed. either way, reconnect and verify everything is good.\n            if not connection.is_ready_for_command():\n                connection.disconnect()\n                connection.connect()\n                if not connection.is_ready_for_command():\n                    raise ConnectionError('Connection not ready')\n        except:  # noqa: E722\n            # release the connection back to the pool so that we don't leak it\n            self.release(connection)\n            raise\n\n        return connection"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_encoder(self):\n        \"Return an encoder based on encoding settings\"\n        kwargs = self.connection_kwargs\n        return Encoder(\n            encoding=kwargs.get('encoding', 'utf-8'),\n            encoding_errors=kwargs.get('encoding_errors', 'strict'),\n            decode_responses=kwargs.get('decode_responses', False)\n        )", "response": "Return an encoder based on encoding settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_connection(self):\n        \"Create a new connection\"\n        if self._created_connections >= self.max_connections:\n            raise ConnectionError(\"Too many connections\")\n        self._created_connections += 1\n        return self.connection_class(**self.connection_kwargs)", "response": "Create a new connection"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndisconnect all connections in the pool", "response": "def disconnect(self):\n        \"Disconnects all connections in the pool\"\n        self._checkpid()\n        all_conns = chain(self._available_connections,\n                          self._in_use_connections)\n        for connection in all_conns:\n            connection.disconnect()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_connection(self):\n        \"Make a fresh connection.\"\n        connection = self.connection_class(**self.connection_kwargs)\n        self._connections.append(connection)\n        return connection", "response": "Make a fresh connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a connection from the pool.", "response": "def get_connection(self, command_name, *keys, **options):\n        \"\"\"\n        Get a connection, blocking for ``self.timeout`` until a connection\n        is available from the pool.\n\n        If the connection returned is ``None`` then creates a new connection.\n        Because we use a last-in first-out queue, the existing connections\n        (having been returned to the pool after the initial ``None`` values\n        were added) will be returned before ``None`` values. This means we only\n        create new connections when we need to, i.e.: the actual number of\n        connections will only increase in response to demand.\n        \"\"\"\n        # Make sure we haven't changed process.\n        self._checkpid()\n\n        # Try and get a connection from the pool. If one isn't available within\n        # self.timeout then raise a ``ConnectionError``.\n        connection = None\n        try:\n            connection = self.pool.get(block=True, timeout=self.timeout)\n        except Empty:\n            # Note that this is not caught by the redis client and will be\n            # raised unless handled by application code. If you want never to\n            raise ConnectionError(\"No connection available.\")\n\n        # If the ``connection`` is actually ``None`` then that's a cue to make\n        # a new connection to add to the pool.\n        if connection is None:\n            connection = self.make_connection()\n\n        try:\n            # ensure this connection is connected to Redis\n            connection.connect()\n            # connections that the pool provides should be ready to send\n            # a command. if not, the connection was either returned to the\n            # pool before all data has been read or the socket has been\n            # closed. either way, reconnect and verify everything is good.\n            if not connection.is_ready_for_command():\n                connection.disconnect()\n                connection.connect()\n                if not connection.is_ready_for_command():\n                    raise ConnectionError('Connection not ready')\n        except:  # noqa: E722\n            # release the connection back to the pool so that we don't leak it\n            self.release(connection)\n            raise\n\n        return connection"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrelease the connection back to the pool.", "response": "def release(self, connection):\n        \"Releases the connection back to the pool.\"\n        # Make sure we haven't changed process.\n        self._checkpid()\n        if connection.pid != self.pid:\n            return\n\n        # Put the connection back into the pool.\n        try:\n            self.pool.put_nowait(connection)\n        except Full:\n            # perhaps the pool has been reset() after a fork? regardless,\n            # we don't want this connection\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npack a series of arguments into a value Redis command", "response": "def pack_command(self, *args):\n        \"Pack a series of arguments into a value Redis command\"\n        args_output = SYM_EMPTY.join([\n            SYM_EMPTY.join(\n                (SYM_DOLLAR, str(len(k)).encode(), SYM_CRLF, k, SYM_CRLF))\n            for k in imap(self.encoder.encode, args)])\n        output = SYM_EMPTY.join(\n            (SYM_STAR, str(len(args)).encode(), SYM_CRLF, args_output))\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine if the current platform has the selector available", "response": "def has_selector(selector):\n    \"Determine if the current platform has the selector available\"\n    try:\n        if selector == 'poll':\n            # the select module offers the poll selector even if the platform\n            # doesn't support it. Attempt to poll for nothing to make sure\n            # poll is available\n            p = select.poll()\n            p.poll(0)\n        else:\n            # the other selectors will fail when instantiated\n            getattr(select, selector)().close()\n        return True\n    except (OSError, AttributeError):\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef DefaultSelector(sock):\n    \"Return the best selector for the platform\"\n    global _DEFAULT_SELECTOR\n    if _DEFAULT_SELECTOR is None:\n        if has_selector('poll'):\n            _DEFAULT_SELECTOR = PollSelector\n        elif hasattr(select, 'select'):\n            _DEFAULT_SELECTOR = SelectSelector\n        else:\n            raise RedisError('Platform does not support any selectors')\n    return _DEFAULT_SELECTOR(sock)", "response": "Return the best selector for the platform"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if data is ready to be read from the socket False otherwise.", "response": "def can_read(self, timeout=0):\n        \"\"\"\n        Return True if data is ready to be read from the socket,\n        otherwise False.\n\n        This doesn't guarentee that the socket is still connected, just that\n        there is data to read.\n\n        Automatically retries EINTR errors based on PEP 475.\n        \"\"\"\n        while True:\n            try:\n                return self.check_can_read(timeout)\n            except (select.error, IOError) as ex:\n                if self.errno_from_exception(ex) == errno.EINTR:\n                    continue\n                return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the socket is ready to send a command.", "response": "def is_ready_for_command(self, timeout=0):\n        \"\"\"\n        Return True if the socket is ready to send a command,\n        otherwise False.\n\n        Automatically retries EINTR errors based on PEP 475.\n        \"\"\"\n        while True:\n            try:\n                return self.check_is_ready_for_command(timeout)\n            except (select.error, IOError) as ex:\n                if self.errno_from_exception(ex) == errno.EINTR:\n                    continue\n                return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the error number from an exception object", "response": "def errno_from_exception(self, ex):\n        \"\"\"\n        Get the error number from an exception\n        \"\"\"\n        if hasattr(ex, 'errno'):\n            return ex.errno\n        elif ex.args:\n            return ex.args[0]\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_url(url, db=None, **kwargs):\n    from redis.client import Redis\n    return Redis.from_url(url, db, **kwargs)", "response": "Returns an active Redis client generated from the given database URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rotate_slaves(self):\n        \"Round-robin slave balancer\"\n        slaves = self.sentinel_manager.discover_slaves(self.service_name)\n        if slaves:\n            if self.slave_rr_counter is None:\n                self.slave_rr_counter = random.randint(0, len(slaves) - 1)\n            for _ in xrange(len(slaves)):\n                self.slave_rr_counter = (\n                    self.slave_rr_counter + 1) % len(slaves)\n                slave = slaves[self.slave_rr_counter]\n                yield slave\n        # Fallback to the master connection\n        try:\n            yield self.get_master_address()\n        except MasterNotFoundError:\n            pass\n        raise SlaveNotFoundError('No slave found for %r' % (self.service_name))", "response": "Round - robin slave balancer"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndiscovering the master s address corresponding to the service labeled service_name.", "response": "def discover_master(self, service_name):\n        \"\"\"\n        Asks sentinel servers for the Redis master's address corresponding\n        to the service labeled ``service_name``.\n\n        Returns a pair (address, port) or raises MasterNotFoundError if no\n        master is found.\n        \"\"\"\n        for sentinel_no, sentinel in enumerate(self.sentinels):\n            try:\n                masters = sentinel.sentinel_masters()\n            except (ConnectionError, TimeoutError):\n                continue\n            state = masters.get(service_name)\n            if state and self.check_master_state(state, service_name):\n                # Put this sentinel at the top of the list\n                self.sentinels[0], self.sentinels[sentinel_no] = (\n                    sentinel, self.sentinels[0])\n                return state['ip'], state['port']\n        raise MasterNotFoundError(\"No master found for %r\" % (service_name,))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves slaves that are in an ODOWN or SDOWN state", "response": "def filter_slaves(self, slaves):\n        \"Remove slaves that are in an ODOWN or SDOWN state\"\n        slaves_alive = []\n        for slave in slaves:\n            if slave['is_odown'] or slave['is_sdown']:\n                continue\n            slaves_alive.append((slave['ip'], slave['port']))\n        return slaves_alive"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef discover_slaves(self, service_name):\n        \"Returns a list of alive slaves for service ``service_name``\"\n        for sentinel in self.sentinels:\n            try:\n                slaves = sentinel.sentinel_slaves(service_name)\n            except (ConnectionError, ResponseError, TimeoutError):\n                continue\n            slaves = self.filter_slaves(slaves)\n            if slaves:\n                return slaves\n        return []", "response": "Returns a list of alive slaves for service service_name"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef explain_weights_lightning(estimator, vec=None, top=20, target_names=None,\n                              targets=None, feature_names=None,\n                              coef_scale=None):\n    \"\"\" Return an explanation of a lightning estimator weights \"\"\"\n    return explain_weights_lightning_not_supported(estimator)", "response": "Return an explanation of a lightning estimator weights"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef explain_prediction_lightning(estimator, doc, vec=None, top=None,\n                                 target_names=None, targets=None,\n                                 feature_names=None, vectorized=False,\n                                 coef_scale=None):\n    \"\"\" Return an explanation of a lightning estimator predictions \"\"\"\n    return explain_weights_lightning_not_supported(estimator, doc)", "response": "Return an explanation of a lightning estimator prediction"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_as_html(explanation,  # type: Explanation\n                   include_styles=True,  # type: bool\n                   force_weights=True,  # type: bool\n                   show=fields.ALL,\n                   preserve_density=None,  # type: Optional[bool]\n                   highlight_spaces=None,  # type: Optional[bool]\n                   horizontal_layout=True,  # type: bool\n                   show_feature_values=False  # type: bool\n                   ):\n    # type: (...) -> str\n    \"\"\" Format explanation as html.\n    Most styles are inline, but some are included separately in <style> tag,\n    you can omit them by passing ``include_styles=False`` and call\n    ``format_html_styles`` to render them separately (or just omit them).\n    With ``force_weights=False``, weights will not be displayed in a table for\n    predictions where it is possible to show feature weights highlighted\n    in the document.\n    If ``highlight_spaces`` is None (default), spaces will be highlighted in\n    feature names only if there are any spaces at the start or at the end of the\n    feature. Setting it to True forces space highlighting, and setting it to\n    False turns it off.\n    If ``horizontal_layout`` is True (default), multiclass classifier\n    weights are laid out horizontally.\n    If ``show_feature_values`` is True, feature values are shown if present.\n    Default is False.\n    \"\"\"\n    template = template_env.get_template('explain.html')\n    if highlight_spaces is None:\n        highlight_spaces = should_highlight_spaces(explanation)\n    targets = explanation.targets or []\n    if len(targets) == 1:\n        horizontal_layout = False\n    explaining_prediction = has_any_values_for_weights(explanation)\n    show_feature_values = show_feature_values and explaining_prediction\n\n    rendered_weighted_spans = render_targets_weighted_spans(\n        targets, preserve_density)\n    weighted_spans_others = [\n        t.weighted_spans.other if t.weighted_spans else None for t in targets]\n\n    return template.render(\n        include_styles=include_styles,\n        force_weights=force_weights,\n        target_table_styles=\n        'border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;',\n        tr_styles='border: none;',\n        # Weight (th and td)\n        td1_styles='padding: 0 1em 0 0.5em; text-align: right; border: none;',\n        # N more positive/negative\n        tdm_styles='padding: 0 0.5em 0 0.5em; text-align: center; border: none; '\n                   'white-space: nowrap;',\n        # Feature (th and td)\n        td2_styles='padding: 0 0.5em 0 0.5em; text-align: left; border: none;',\n        # Value (th and td)\n        td3_styles='padding: 0 0.5em 0 1em; text-align: right; border: none;',\n        horizontal_layout_table_styles=\n        'border-collapse: collapse; border: none; margin-bottom: 1.5em;',\n        horizontal_layout_td_styles=\n        'padding: 0px; border: 1px solid black; vertical-align: top;',\n        horizontal_layout_header_styles=\n        'padding: 0.5em; border: 1px solid black; text-align: center;',\n        show=show,\n        expl=explanation,\n        hl_spaces=highlight_spaces,\n        horizontal_layout=horizontal_layout,\n        any_weighted_spans=any(t.weighted_spans for t in targets),\n        feat_imp_weight_range=max_or_0(\n            abs(fw.weight) for fw in explanation.feature_importances.importances)\n        if explanation.feature_importances else 0,\n        target_weight_range=max_or_0(\n            get_weight_range(t.feature_weights) for t in targets),\n        other_weight_range=max_or_0(\n            get_weight_range(other)\n            for other in weighted_spans_others if other),\n        targets_with_weighted_spans=list(\n            zip(targets, rendered_weighted_spans, weighted_spans_others)),\n        show_feature_values=show_feature_values,\n        weights_table_span=3 if show_feature_values else 2,\n        explaining_prediction=explaining_prediction,\n        weight_help=html_escape(WEIGHT_HELP),\n        contribution_help=html_escape(CONTRIBUTION_HELP),\n    )", "response": "Formats the explanation as html."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of rendered weighted spans for the given list of targets.", "response": "def render_targets_weighted_spans(\n        targets,  # type: List[TargetExplanation]\n        preserve_density,  # type: Optional[bool]\n    ):\n    # type: (...) -> List[Optional[str]]\n    \"\"\" Return a list of rendered weighted spans for targets.\n    Function must accept a list in order to select consistent weight\n    ranges across all targets.\n    \"\"\"\n    prepared_weighted_spans = prepare_weighted_spans(\n        targets, preserve_density)\n\n    def _fmt_pws(pws):\n        # type: (PreparedWeightedSpans) -> str\n        name = ('<b>{}:</b> '.format(pws.doc_weighted_spans.vec_name)\n                if pws.doc_weighted_spans.vec_name else '')\n        return '{}{}'.format(name, render_weighted_spans(pws))\n\n    def _fmt_pws_list(pws_lst):\n        # type: (List[PreparedWeightedSpans]) -> str\n        return '<br/>'.join(_fmt_pws(pws) for pws in pws_lst)\n\n    return [_fmt_pws_list(pws_lst) if pws_lst else None\n            for pws_lst in prepared_weighted_spans]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _colorize(token,  # type: str\n              weight,  # type: float\n              weight_range,  # type: float\n              ):\n    # type: (...) -> str\n    \"\"\" Return token wrapped in a span with some styles\n    (calculated from weight and weight_range) applied.\n    \"\"\"\n    token = html_escape(token)\n    if np.isclose(weight, 0.):\n        return (\n            '<span '\n            'style=\"opacity: {opacity}\"'\n            '>{token}</span>'.format(\n                opacity=_weight_opacity(weight, weight_range),\n                token=token)\n        )\n    else:\n        return (\n            '<span '\n            'style=\"background-color: {color}; opacity: {opacity}\" '\n            'title=\"{weight:.3f}\"'\n            '>{token}</span>'.format(\n                color=format_hsl(\n                    weight_color_hsl(weight, weight_range, min_lightness=0.6)),\n                opacity=_weight_opacity(weight, weight_range),\n                weight=weight,\n                token=token)\n        )", "response": "Return a token wrapped in a span with some styles applied from weight and weight_range."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _weight_opacity(weight, weight_range):\n    # type: (float, float) -> str\n    \"\"\" Return opacity value for given weight as a string.\n    \"\"\"\n    min_opacity = 0.8\n    if np.isclose(weight, 0) and np.isclose(weight_range, 0):\n        rel_weight = 0.0\n    else:\n        rel_weight = abs(weight) / weight_range\n    return '{:.2f}'.format(min_opacity + (1 - min_opacity) * rel_weight)", "response": "Return opacity value for given weight as a string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn HSL color components for given weight.", "response": "def weight_color_hsl(weight, weight_range, min_lightness=0.8):\n    # type: (float, float, float) -> _HSL_COLOR\n    \"\"\" Return HSL color components for given weight,\n    where the max absolute weight is given by weight_range.\n    \"\"\"\n    hue = _hue(weight)\n    saturation = 1\n    rel_weight = (abs(weight) / weight_range) ** 0.7\n    lightness = 1.0 - (1 - min_lightness) * rel_weight\n    return hue, saturation, lightness"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nformats hsl color as css color string.", "response": "def format_hsl(hsl_color):\n    # type: (_HSL_COLOR) -> str\n    \"\"\" Format hsl color as css color string.\n    \"\"\"\n    hue, saturation, lightness = hsl_color\n    return 'hsl({}, {:.2%}, {:.2%})'.format(hue, saturation, lightness)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the maximum absolute feature for pos and neg weights.", "response": "def get_weight_range(weights):\n    # type: (FeatureWeights) -> float\n    \"\"\" Max absolute feature for pos and neg weights.\n    \"\"\"\n    return max_or_0(abs(fw.weight)\n                    for lst in [weights.pos, weights.neg]\n                    for fw in lst or [])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncolor for remaining row.", "response": "def remaining_weight_color_hsl(\n        ws,  # type: List[FeatureWeight]\n        weight_range,  # type: float\n        pos_neg,  # type: str\n    ):\n    # type: (...) -> _HSL_COLOR\n    \"\"\" Color for \"remaining\" row.\n    Handles a number of edge cases: if there are no weights in ws or weight_range\n    is zero, assume the worst (most intensive positive or negative color).\n    \"\"\"\n    sign = {'pos': 1.0, 'neg': -1.0}[pos_neg]\n    if not ws and not weight_range:\n        weight = sign\n        weight_range = 1.0\n    elif not ws:\n        weight = sign * weight_range\n    else:\n        weight = min((fw.weight for fw in ws), key=abs)\n    return weight_color_hsl(weight, weight_range)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nformatting unhashed feature: show first (most probable) candidate, display other candidates in title attribute.", "response": "def _format_unhashed_feature(feature, weight, hl_spaces):\n    # type: (...) -> str\n    \"\"\" Format unhashed feature: show first (most probable) candidate,\n    display other candidates in title attribute.\n    \"\"\"\n    if not feature:\n        return ''\n    else:\n        first, rest = feature[0], feature[1:]\n        html = format_signed(\n            first, lambda x: _format_single_feature(x, weight, hl_spaces))\n        if rest:\n            html += ' <span title=\"{}\">&hellip;</span>'.format(\n                '\\n'.join(html_escape(format_signed(f)) for f in rest))\n        return html"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an explanation of a LightGBM estimator or LGBMRegressor.", "response": "def explain_weights_lightgbm(lgb,\n                             vec=None,\n                             top=20,\n                             target_names=None,  # ignored\n                             targets=None,  # ignored\n                             feature_names=None,\n                             feature_re=None,\n                             feature_filter=None,\n                             importance_type='gain',\n                             ):\n    \"\"\"\n    Return an explanation of an LightGBM estimator (via scikit-learn wrapper\n    LGBMClassifier or LGBMRegressor) as feature importances.\n\n    See :func:`eli5.explain_weights` for description of\n    ``top``, ``feature_names``,\n    ``feature_re`` and ``feature_filter`` parameters.\n\n    ``target_names`` and ``targets`` parameters are ignored.\n    \n    Parameters\n    ----------\n    importance_type : str, optional\n        A way to get feature importance. Possible values are:\n\n        - 'gain' - the average gain of the feature when it is used in trees\n          (default)\n        - 'split' - the number of times a feature is used to split the data\n          across all trees\n        - 'weight' - the same as 'split', for compatibility with xgboost\n    \"\"\"\n    coef = _get_lgb_feature_importances(lgb, importance_type)\n    lgb_feature_names = lgb.booster_.feature_name()\n    return get_feature_importance_explanation(lgb, vec, coef,\n        feature_names=feature_names,\n        estimator_feature_names=lgb_feature_names,\n        feature_filter=feature_filter,\n        feature_re=feature_re,\n        top=top,\n        description=DESCRIPTION_LIGHTGBM,\n        num_features=coef.shape[-1],\n        is_regression=isinstance(lgb, lightgbm.LGBMRegressor),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef explain_prediction_lightgbm(\n        lgb, doc,\n        vec=None,\n        top=None,\n        top_targets=None,\n        target_names=None,\n        targets=None,\n        feature_names=None,\n        feature_re=None,\n        feature_filter=None,\n        vectorized=False,\n        ):\n    \"\"\" Return an explanation of LightGBM prediction (via scikit-learn wrapper\n    LGBMClassifier or LGBMRegressor) as feature weights.\n\n    See :func:`eli5.explain_prediction` for description of\n    ``top``, ``top_targets``, ``target_names``, ``targets``,\n    ``feature_names``, ``feature_re`` and ``feature_filter`` parameters.\n\n    ``vec`` is a vectorizer instance used to transform\n    raw features to the input of the estimator ``xgb``\n    (e.g. a fitted CountVectorizer instance); you can pass it\n    instead of ``feature_names``.\n\n    ``vectorized`` is a flag which tells eli5 if ``doc`` should be\n    passed through ``vec`` or not. By default it is False, meaning that\n    if ``vec`` is not None, ``vec.transform([doc])`` is passed to the\n    estimator. Set it to True if you're passing ``vec``,\n    but ``doc`` is already vectorized.\n\n    Method for determining feature importances follows an idea from\n    http://blog.datadive.net/interpreting-random-forests/.\n    Feature weights are calculated by following decision paths in trees\n    of an ensemble.\n    Each leaf has an output score, and expected scores can also be assigned\n    to parent nodes.\n    Contribution of one feature on the decision path is how much expected score\n    changes from parent to child.\n    Weights of all features sum to the output score of the estimator.\n    \"\"\"\n\n    vec, feature_names = handle_vec(lgb, doc, vec, vectorized, feature_names)\n    if feature_names.bias_name is None:\n        # LightGBM estimators do not have an intercept, but here we interpret\n        # them as having an intercept\n        feature_names.bias_name = '<BIAS>'\n    X = get_X(doc, vec, vectorized=vectorized)\n\n    proba = predict_proba(lgb, X)\n    weight_dicts = _get_prediction_feature_weights(lgb, X, _lgb_n_targets(lgb))\n    x = get_X0(add_intercept(X))\n\n    is_regression = isinstance(lgb, lightgbm.LGBMRegressor)\n    is_multiclass = _lgb_n_targets(lgb) > 2\n    names = lgb.classes_ if not is_regression else ['y']\n\n    def get_score_weights(_label_id):\n        _weights = _target_feature_weights(\n            weight_dicts[_label_id],\n            num_features=len(feature_names),\n            bias_idx=feature_names.bias_idx,\n        )\n        _score = _get_score(weight_dicts[_label_id])\n        return _score, _weights\n\n    return get_decision_path_explanation(\n        lgb, doc, vec,\n        x=x,\n        feature_names=feature_names,\n        feature_filter=feature_filter,\n        feature_re=feature_re,\n        top=top,\n        vectorized=vectorized,\n        original_display_names=names,\n        target_names=target_names,\n        targets=targets,\n        top_targets=top_targets,\n        is_regression=is_regression,\n        is_multiclass=is_multiclass,\n        proba=proba,\n        get_score_weights=get_score_weights,\n     )", "response": "Return an explanation of the prediction of a light - gauss - M model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the node value for all leaf nodes.", "response": "def _compute_node_values(tree_info):\n    \"\"\" Add node_value key with an expected value for non-leaf nodes \"\"\"\n    def walk(tree):\n        if 'leaf_value' in tree:\n            return tree['leaf_value'], tree.get('leaf_count', 0)\n        left_value, left_count = walk(tree['left_child'])\n        right_value, right_count = walk(tree['right_child'])\n        count = left_count + right_count\n        if tree['split_gain'] <= 0:\n            assert left_value == right_value\n            tree['_node_value'] = left_value\n        else:\n            tree['_node_value'] = (left_value * left_count +\n                                  right_value * right_count) / count\n        return tree['_node_value'], count\n\n    for tree in tree_info:\n        walk(tree['tree_structure'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _changes(path):\n    res = [path[0]]\n    res += [p - p_prev for p, p_prev in zip(path[1:], path)]\n    return res", "response": "Return a list of changes in a path."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_prediction_feature_weights(lgb, X, n_targets):\n    if n_targets == 2:\n        n_targets = 1\n    dump = lgb.booster_.dump_model()\n    tree_info = dump['tree_info']\n    _compute_node_values(tree_info)\n    pred_leafs = lgb.booster_.predict(X, pred_leaf=True).reshape(-1, n_targets)\n    tree_info = np.array(tree_info).reshape(-1, n_targets)\n    assert pred_leafs.shape == tree_info.shape\n\n    res = []\n    for target in range(n_targets):\n        feature_weights = defaultdict(float)  # type: DefaultDict[Optional[str], float]\n        for info, leaf_id in zip(tree_info[:, target], pred_leafs[:, target]):\n            leaf_index, split_index = _get_leaf_split_indices(\n                info['tree_structure']\n            )\n            bias, path = _get_decision_path(leaf_index, split_index, leaf_id)\n            feature_weights[None] += bias\n            for feat, value in path:\n                feature_weights[feat] += value\n        res.append(dict(feature_weights))\n    return res", "response": "Returns a list of feature weights dicts with feature weights."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreplacing spaces in a string.", "response": "def replace_spaces(s, replacer):\n    # type: (str, Callable[[int, str], str]) -> str\n    \"\"\"\n    >>> replace_spaces('ab', lambda n, l: '_' * n)\n    'ab'\n    >>> replace_spaces('a b', lambda n, l: '_' * n)\n    'a_b'\n    >>> replace_spaces(' ab', lambda n, l: '_' * n)\n    '_ab'\n    >>> replace_spaces('  a b ', lambda n, s: s * n)\n    'leftleftacenterbright'\n    >>> replace_spaces(' a b  ', lambda n, _: '0 0' * n)\n    '0 0a0 0b0 00 0'\n    \"\"\"\n    def replace(m):\n        # type: (Match[str]) -> str\n        if m.start() == 0:\n            side = 'left'\n        elif m.end() == len(s):\n            side = 'right'\n        else:\n            side = 'center'\n        return replacer(len(m.group()), side)\n\n    return re.sub(r'[ ]+', replace, s)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nformatting unhashed feature with sign.", "response": "def format_signed(feature,  # type: Dict[str, Any]\n                  formatter=None,  # type: Callable[..., str]\n                  **kwargs\n                  ):\n    # type: (...) -> str\n    \"\"\"\n    Format unhashed feature with sign.\n\n    >>> format_signed({'name': 'foo', 'sign': 1})\n    'foo'\n    >>> format_signed({'name': 'foo', 'sign': -1})\n    '(-)foo'\n    >>> format_signed({'name': ' foo', 'sign': -1}, lambda x: '\"{}\"'.format(x))\n    '(-)\" foo\"'\n    \"\"\"\n    txt = '' if feature['sign'] > 0 else '(-)'\n    name = feature['name']  # type: str\n    if formatter is not None:\n        name = formatter(name, **kwargs)\n    return '{}{}'.format(txt, name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tabulate(data,  # type: List[List[Any]]\n             header=None,  # type: Optional[List[Any]]\n             col_align=None,  # type: Union[str, List[str]]\n             ):\n    # type: (...) -> List[str]\n    \"\"\" Format data as a table without any fancy features.\n    col_align: l/r/c or a list/string of l/r/c. l = left, r = right, c = center\n    Return a list of strings (lines of the table).\n    \"\"\"\n    if not data and not header:\n        return []\n    if data:\n        n_cols = len(data[0])\n    else:\n        assert header is not None\n        n_cols = len(header)\n    if not all(len(row) == n_cols for row in data):\n        raise ValueError('data is not rectangular')\n\n    if col_align is None:\n        col_align = ['l'] * n_cols\n    elif isinstance(col_align, six.string_types) and len(col_align) == 1:\n        col_align = [col_align] * n_cols\n    else:\n        col_align = list(col_align)\n        if len(col_align) != n_cols:\n            raise ValueError('col_align length does not match number of columns')\n\n    if header and len(header) != n_cols:\n        raise ValueError('header length does not match number of columns')\n\n    if header:\n        data = [header] + data\n    data = [[six.text_type(x) for x in row] for row in data]\n    col_width = [max(len(row[col_i]) for row in data) for col_i in range(n_cols)]\n    if header:\n        data.insert(1, ['-' * width for width in col_width])\n\n    line_tpl = u'  '.join(\n        u'{:%s%s}' % ({'l': '', 'r': '>', 'c': '^'}[align], width)\n        for align, width in zip(col_align, col_width))\n    return [line_tpl.format(*row) for row in data]", "response": "Return a list of strings that can be used to display a table of data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef explain_prediction_sklearn(estimator, doc,\n                               vec=None,\n                               top=None,\n                               top_targets=None,\n                               target_names=None,\n                               targets=None,\n                               feature_names=None,\n                               feature_re=None,\n                               feature_filter=None,\n                               vectorized=False):\n    \"\"\" Return an explanation of a scikit-learn estimator \"\"\"\n    return explain_prediction_sklearn_not_supported(estimator, doc)", "response": "Return an explanation of a scikit - learn estimator."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef explain_prediction_linear_classifier(clf, doc,\n                                         vec=None,\n                                         top=None,\n                                         top_targets=None,\n                                         target_names=None,\n                                         targets=None,\n                                         feature_names=None,\n                                         feature_re=None,\n                                         feature_filter=None,\n                                         vectorized=False,\n                                         ):\n    \"\"\"\n    Explain prediction of a linear classifier.\n\n    See :func:`eli5.explain_prediction` for description of\n    ``top``, ``top_targets``, ``target_names``, ``targets``,\n    ``feature_names``, ``feature_re`` and ``feature_filter`` parameters.\n\n    ``vec`` is a vectorizer instance used to transform\n    raw features to the input of the classifier ``clf``\n    (e.g. a fitted CountVectorizer instance); you can pass it\n    instead of ``feature_names``.\n\n    ``vectorized`` is a flag which tells eli5 if ``doc`` should be\n    passed through ``vec`` or not. By default it is False, meaning that\n    if ``vec`` is not None, ``vec.transform([doc])`` is passed to the\n    classifier. Set it to True if you're passing ``vec``, but ``doc``\n    is already vectorized.\n    \"\"\"\n    vec, feature_names = handle_vec(clf, doc, vec, vectorized, feature_names)\n    X = get_X(doc, vec=vec, vectorized=vectorized, to_dense=True)\n\n    proba = predict_proba(clf, X)\n    score, = clf.decision_function(X)\n\n    if has_intercept(clf):\n        X = add_intercept(X)\n    x = get_X0(X)\n\n    feature_names, flt_indices = feature_names.handle_filter(\n        feature_filter, feature_re, x)\n\n    res = Explanation(\n        estimator=repr(clf),\n        method='linear model',\n        targets=[],\n    )\n    assert res.targets is not None\n\n    _weights = _linear_weights(clf, x, top, feature_names, flt_indices)\n    classes = getattr(clf, \"classes_\", [\"-1\", \"1\"])  # OneClassSVM support\n    display_names = get_target_display_names(classes, target_names,\n                                             targets, top_targets, score)\n\n    if is_multiclass_classifier(clf):\n        for label_id, label in display_names:\n            target_expl = TargetExplanation(\n                target=label,\n                feature_weights=_weights(label_id),\n                score=score[label_id],\n                proba=proba[label_id] if proba is not None else None,\n            )\n            add_weighted_spans(doc, vec, vectorized, target_expl)\n            res.targets.append(target_expl)\n    else:\n        if len(display_names) == 1:  # target is passed explicitly\n            label_id, target = display_names[0]\n        else:\n            label_id = 1 if score >= 0 else 0\n            target = display_names[label_id][1]\n        scale = -1 if label_id == 0 else 1\n\n        target_expl = TargetExplanation(\n            target=target,\n            feature_weights=_weights(0, scale=scale),\n            score=score,\n            proba=proba[label_id] if proba is not None else None,\n        )\n        add_weighted_spans(doc, vec, vectorized, target_expl)\n        res.targets.append(target_expl)\n\n    return res", "response": "Explain prediction of a linear classifier."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef explain_prediction_linear_regressor(reg, doc,\n                                        vec=None,\n                                        top=None,\n                                        top_targets=None,\n                                        target_names=None,\n                                        targets=None,\n                                        feature_names=None,\n                                        feature_re=None,\n                                        feature_filter=None,\n                                        vectorized=False):\n    \"\"\"\n    Explain prediction of a linear regressor.\n\n    See :func:`eli5.explain_prediction` for description of\n    ``top``, ``top_targets``, ``target_names``, ``targets``,\n    ``feature_names``, ``feature_re`` and ``feature_filter`` parameters.\n\n    ``vec`` is a vectorizer instance used to transform\n    raw features to the input of the classifier ``clf``;\n    you can pass it instead of ``feature_names``.\n\n    ``vectorized`` is a flag which tells eli5 if ``doc`` should be\n    passed through ``vec`` or not. By default it is False, meaning that\n    if ``vec`` is not None, ``vec.transform([doc])`` is passed to the\n    regressor ``reg``. Set it to True if you're passing ``vec``,\n    but ``doc`` is already vectorized.\n    \"\"\"\n    if isinstance(reg, (SVR, NuSVR)) and reg.kernel != 'linear':\n        return explain_prediction_sklearn_not_supported(reg, doc)\n\n    vec, feature_names = handle_vec(reg, doc, vec, vectorized, feature_names)\n    X = get_X(doc, vec=vec, vectorized=vectorized, to_dense=True)\n\n    score, = reg.predict(X)\n\n    if has_intercept(reg):\n        X = add_intercept(X)\n    x = get_X0(X)\n\n    feature_names, flt_indices = feature_names.handle_filter(\n        feature_filter, feature_re, x)\n\n    res = Explanation(\n        estimator=repr(reg),\n        method='linear model',\n        targets=[],\n        is_regression=True,\n    )\n    assert res.targets is not None\n\n    _weights = _linear_weights(reg, x, top, feature_names, flt_indices)\n    names = get_default_target_names(reg)\n    display_names = get_target_display_names(names, target_names, targets,\n                                             top_targets, score)\n\n    if is_multitarget_regressor(reg):\n        for label_id, label in display_names:\n            target_expl = TargetExplanation(\n                target=label,\n                feature_weights=_weights(label_id),\n                score=score[label_id],\n            )\n            add_weighted_spans(doc, vec, vectorized, target_expl)\n            res.targets.append(target_expl)\n    else:\n        target_expl = TargetExplanation(\n            target=display_names[0][1],\n            feature_weights=_weights(0),\n            score=score,\n        )\n        add_weighted_spans(doc, vec, vectorized, target_expl)\n        res.targets.append(target_expl)\n\n    return res", "response": "Explain prediction of a linear regressor."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexplains prediction of a tree classifier.", "response": "def explain_prediction_tree_classifier(\n        clf, doc,\n        vec=None,\n        top=None,\n        top_targets=None,\n        target_names=None,\n        targets=None,\n        feature_names=None,\n        feature_re=None,\n        feature_filter=None,\n        vectorized=False):\n    \"\"\" Explain prediction of a tree classifier.\n\n    See :func:`eli5.explain_prediction` for description of\n    ``top``, ``top_targets``, ``target_names``, ``targets``,\n    ``feature_names``, ``feature_re`` and ``feature_filter`` parameters.\n\n    ``vec`` is a vectorizer instance used to transform\n    raw features to the input of the classifier ``clf``\n    (e.g. a fitted CountVectorizer instance); you can pass it\n    instead of ``feature_names``.\n\n    ``vectorized`` is a flag which tells eli5 if ``doc`` should be\n    passed through ``vec`` or not. By default it is False, meaning that\n    if ``vec`` is not None, ``vec.transform([doc])`` is passed to the\n    classifier. Set it to True if you're passing ``vec``,\n    but ``doc`` is already vectorized.\n\n    Method for determining feature importances follows an idea from\n    http://blog.datadive.net/interpreting-random-forests/.\n    Feature weights are calculated by following decision paths in trees\n    of an ensemble (or a single tree for DecisionTreeClassifier).\n    Each node of the tree has an output score, and contribution of a feature\n    on the decision path is how much the score changes from parent to child.\n    Weights of all features sum to the output score or proba of the estimator.\n    \"\"\"\n    vec, feature_names = handle_vec(clf, doc, vec, vectorized, feature_names)\n    X = get_X(doc, vec=vec, vectorized=vectorized)\n    if feature_names.bias_name is None:\n        # Tree estimators do not have an intercept, but here we interpret\n        # them as having an intercept\n        feature_names.bias_name = '<BIAS>'\n\n    proba = predict_proba(clf, X)\n    if hasattr(clf, 'decision_function'):\n        score, = clf.decision_function(X)\n    else:\n        score = None\n\n    is_multiclass = clf.n_classes_ > 2\n    feature_weights = _trees_feature_weights(\n        clf, X, feature_names, clf.n_classes_)\n    x = get_X0(add_intercept(X))\n    flt_feature_names, flt_indices = feature_names.handle_filter(\n        feature_filter, feature_re, x)\n\n    def _weights(label_id, scale=1.0):\n        weights = feature_weights[:, label_id]\n        return get_top_features_filtered(x, flt_feature_names, flt_indices,\n                                         weights, top, scale)\n\n    res = Explanation(\n        estimator=repr(clf),\n        method='decision path',\n        targets=[],\n        description=(DESCRIPTION_TREE_CLF_MULTICLASS if is_multiclass\n                     else DESCRIPTION_TREE_CLF_BINARY),\n    )\n    assert res.targets is not None\n\n    display_names = get_target_display_names(\n        clf.classes_, target_names, targets, top_targets,\n        score=score if score is not None else proba)\n\n    if is_multiclass:\n        for label_id, label in display_names:\n            target_expl = TargetExplanation(\n                target=label,\n                feature_weights=_weights(label_id),\n                score=score[label_id] if score is not None else None,\n                proba=proba[label_id] if proba is not None else None,\n            )\n            add_weighted_spans(doc, vec, vectorized, target_expl)\n            res.targets.append(target_expl)\n    else:\n        target, scale, label_id = get_binary_target_scale_label_id(\n            score, display_names, proba)\n        target_expl = TargetExplanation(\n            target=target,\n            feature_weights=_weights(label_id, scale=scale),\n            score=score if score is not None else None,\n            proba=proba[label_id] if proba is not None else None,\n        )\n        add_weighted_spans(doc, vec, vectorized, target_expl)\n        res.targets.append(target_expl)\n\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef explain_prediction_tree_regressor(\n        reg, doc,\n        vec=None,\n        top=None,\n        top_targets=None,\n        target_names=None,\n        targets=None,\n        feature_names=None,\n        feature_re=None,\n        feature_filter=None,\n        vectorized=False):\n    \"\"\" Explain prediction of a tree regressor.\n\n    See :func:`eli5.explain_prediction` for description of\n    ``top``, ``top_targets``, ``target_names``, ``targets``,\n    ``feature_names``, ``feature_re`` and ``feature_filter`` parameters.\n\n    ``vec`` is a vectorizer instance used to transform\n    raw features to the input of the regressor ``reg``\n    (e.g. a fitted CountVectorizer instance); you can pass it\n    instead of ``feature_names``.\n\n    ``vectorized`` is a flag which tells eli5 if ``doc`` should be\n    passed through ``vec`` or not. By default it is False, meaning that\n    if ``vec`` is not None, ``vec.transform([doc])`` is passed to the\n    regressor. Set it to True if you're passing ``vec``,\n    but ``doc`` is already vectorized.\n\n    Method for determining feature importances follows an idea from\n    http://blog.datadive.net/interpreting-random-forests/.\n    Feature weights are calculated by following decision paths in trees\n    of an ensemble (or a single tree for DecisionTreeRegressor).\n    Each node of the tree has an output score, and contribution of a feature\n    on the decision path is how much the score changes from parent to child.\n    Weights of all features sum to the output score of the estimator.\n    \"\"\"\n    vec, feature_names = handle_vec(reg, doc, vec, vectorized, feature_names)\n    X = get_X(doc, vec=vec, vectorized=vectorized)\n    if feature_names.bias_name is None:\n        # Tree estimators do not have an intercept, but here we interpret\n        # them as having an intercept\n        feature_names.bias_name = '<BIAS>'\n\n    score, = reg.predict(X)\n    num_targets = getattr(reg, 'n_outputs_', 1)\n    is_multitarget = num_targets > 1\n    feature_weights = _trees_feature_weights(reg, X, feature_names, num_targets)\n    x = get_X0(add_intercept(X))\n    flt_feature_names, flt_indices = feature_names.handle_filter(\n        feature_filter, feature_re, x)\n\n    def _weights(label_id, scale=1.0):\n        weights = feature_weights[:, label_id]\n        return get_top_features_filtered(x, flt_feature_names, flt_indices,\n                                         weights, top, scale)\n\n    res = Explanation(\n        estimator=repr(reg),\n        method='decision path',\n        description=(DESCRIPTION_TREE_REG_MULTITARGET if is_multitarget\n                     else DESCRIPTION_TREE_REG),\n        targets=[],\n        is_regression=True,\n    )\n    assert res.targets is not None\n\n    names = get_default_target_names(reg, num_targets=num_targets)\n    display_names = get_target_display_names(names, target_names, targets,\n                                             top_targets, score)\n\n    if is_multitarget:\n        for label_id, label in display_names:\n            target_expl = TargetExplanation(\n                target=label,\n                feature_weights=_weights(label_id),\n                score=score[label_id],\n            )\n            add_weighted_spans(doc, vec, vectorized, target_expl)\n            res.targets.append(target_expl)\n    else:\n        target_expl = TargetExplanation(\n            target=display_names[0][1],\n            feature_weights=_weights(0),\n            score=score,\n        )\n        add_weighted_spans(doc, vec, vectorized, target_expl)\n        res.targets.append(target_expl)\n\n    return res", "response": "Explain prediction of a tree regressor."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _trees_feature_weights(clf, X, feature_names, num_targets):\n    feature_weights = np.zeros([len(feature_names), num_targets])\n    if hasattr(clf, 'tree_'):\n        _update_tree_feature_weights(X, feature_names, clf, feature_weights)\n    else:\n        if isinstance(clf, (\n                GradientBoostingClassifier, GradientBoostingRegressor)):\n            weight = clf.learning_rate\n        else:\n            weight = 1. / len(clf.estimators_)\n        for _clfs in clf.estimators_:\n            _update = partial(_update_tree_feature_weights, X, feature_names)\n            if isinstance(_clfs, np.ndarray):\n                if len(_clfs) == 1:\n                    _update(_clfs[0], feature_weights)\n                else:\n                    for idx, _clf in enumerate(_clfs):\n                        _update(_clf, feature_weights[:, idx])\n            else:\n                _update(_clfs, feature_weights)\n        feature_weights *= weight\n        if hasattr(clf, 'init_'):\n            feature_weights[feature_names.bias_idx] += clf.init_.predict(X)[0]\n    return feature_weights", "response": "Return feature weights for a tree or a tree ensemble."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating tree feature weights using decision path method.", "response": "def _update_tree_feature_weights(X, feature_names, clf, feature_weights):\n    \"\"\" Update tree feature weights using decision path method.\n    \"\"\"\n    tree_value = clf.tree_.value\n    if tree_value.shape[1] == 1:\n        squeeze_axis = 1\n    else:\n        assert tree_value.shape[2] == 1\n        squeeze_axis = 2\n    tree_value = np.squeeze(tree_value, axis=squeeze_axis)\n    tree_feature = clf.tree_.feature\n    _, indices = clf.decision_path(X).nonzero()\n    if isinstance(clf, DecisionTreeClassifier):\n        norm = lambda x: x / x.sum()\n    else:\n        norm = lambda x: x\n    feature_weights[feature_names.bias_idx] += norm(tree_value[0])\n    for parent_idx, child_idx in zip(indices, indices[1:]):\n        assert tree_feature[parent_idx] >= 0\n        feature_idx = tree_feature[parent_idx]\n        diff = norm(tree_value[child_idx]) - norm(tree_value[parent_idx])\n        feature_weights[feature_idx] += diff"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _multiply(X, coef):\n    if sp.issparse(X):\n        return X.multiply(sp.csr_matrix(coef))\n    else:\n        return np.multiply(X, coef)", "response": "Multiplies X by coef element - wise preserving sparsity."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns linear weights getter for label_id.", "response": "def _linear_weights(clf, x, top, flt_feature_names, flt_indices):\n    \"\"\" Return top weights getter for label_id.\n    \"\"\"\n    def _weights(label_id, scale=1.0):\n        coef = get_coef(clf, label_id)\n        scores = _multiply(x, coef)\n        return get_top_features_filtered(x, flt_feature_names, flt_indices,\n                                         scores, top, scale)\n    return _weights"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlike attr. s but with slots = True and attributes extracted from the class s __init__ method signature.", "response": "def attrs(class_):\n    \"\"\" Like attr.s with slots=True,\n    but with attributes extracted from __init__ method signature.\n    slots=True ensures that signature matches what really happens\n    (we can't define different attributes on self).\n    It is useful if we still want __init__ for proper type-checking and\n    do not want to repeat attribute definitions in the class body.\n    \"\"\"\n    attrs_kwargs = {}\n    for method, kw_name in [\n            ('__repr__', 'repr'),\n            ('__eq__', 'cmp'),\n            ('__hash__', 'hash'),\n            ]:\n        if method in class_.__dict__:\n            # Allow to redefine a special method (or else attr.s will do it)\n            attrs_kwargs[kw_name] = False\n    init_args = inspect.getargspec(class_.__init__)\n    defaults_shift = len(init_args.args) - len(init_args.defaults or []) - 1\n    these = {}\n    for idx, arg in enumerate(init_args.args[1:]):\n        attrib_kwargs = {}\n        if idx >= defaults_shift:\n            attrib_kwargs['default'] = init_args.defaults[idx - defaults_shift]\n        these[arg] = attr.ib(**attrib_kwargs)\n    return attr.s(class_, these=these, init=False, slots=True, **attrs_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if a classifier can return probabilities.", "response": "def is_probabilistic_classifier(clf):\n    # type: (Any) -> bool\n    \"\"\" Return True if a classifier can return probabilities \"\"\"\n    if not hasattr(clf, 'predict_proba'):\n        return False\n    if isinstance(clf, OneVsRestClassifier):\n        # It currently has a predict_proba method, but does not check if\n        # wrapped estimator has a predict_proba method.\n        return hasattr(clf.estimator, 'predict_proba')\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npredicts the probability of X in the node.", "response": "def predict_proba(estimator, X):\n    # type: (Any, Any) -> Optional[np.ndarray]\n    \"\"\" Return result of predict_proba, if an estimator supports it, or None.\n    \"\"\"\n    if is_probabilistic_classifier(estimator):\n        try:\n            proba, = estimator.predict_proba(X)\n            return proba\n        except NotImplementedError:\n            return None\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef has_intercept(estimator):\n    # type: (Any) -> bool\n    \"\"\" Return True if an estimator has intercept fit. \"\"\"\n    if hasattr(estimator, 'fit_intercept'):\n        return estimator.fit_intercept\n    if hasattr(estimator, 'intercept_'):\n        if estimator.intercept_ is None:\n            return False\n        # scikit-learn sets intercept to zero vector if it is not fit\n        return np.any(estimator.intercept_)\n    return False", "response": "Return True if an estimator has intercept fit."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_feature_names(clf, vec=None, bias_name='<BIAS>', feature_names=None,\n                      num_features=None, estimator_feature_names=None):\n    # type: (Any, Any, Optional[str], Any, int, Any) -> FeatureNames\n    \"\"\"\n    Return a FeatureNames instance that holds all feature names\n    and a bias feature.\n    If vec is None or doesn't have get_feature_names() method,\n    features are named x0, x1, x2, etc.\n    \"\"\"\n    if not has_intercept(clf):\n        bias_name = None\n\n    if feature_names is None:\n        if vec and hasattr(vec, 'get_feature_names'):\n            return FeatureNames(vec.get_feature_names(), bias_name=bias_name)\n        else:\n            if estimator_feature_names is None:\n                num_features = num_features or get_num_features(clf)\n                return FeatureNames(\n                    n_features=num_features,\n                    unkn_template='x%d',\n                    bias_name=bias_name\n                )\n            return FeatureNames(estimator_feature_names, bias_name=bias_name)\n\n    num_features = num_features or get_num_features(clf)\n    if isinstance(feature_names, FeatureNames):\n        if feature_names.n_features != num_features:\n            raise ValueError(\"feature_names has a wrong n_features: \"\n                             \"expected=%d, got=%d\" % (num_features,\n                                                      feature_names.n_features))\n        # Make a shallow copy setting proper bias_name\n        return FeatureNames(\n            feature_names.feature_names,\n            n_features=num_features,\n            bias_name=bias_name,\n            unkn_template=feature_names.unkn_template)\n    else:\n        if len(feature_names) != num_features:\n            raise ValueError(\"feature_names has a wrong length: \"\n                             \"expected=%d, got=%d\" % (num_features,\n                                                      len(feature_names)))\n        return FeatureNames(feature_names, bias_name=bias_name)", "response": "Returns a list of feature names for a given class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_default_target_names(estimator, num_targets=None):\n    if num_targets is None:\n        if len(estimator.coef_.shape) <= 1:\n            num_targets = 1\n        else:\n            num_targets, _ = estimator.coef_.shape\n    if num_targets == 1:\n        target_names = ['y']\n    else:\n        target_names = ['y%d' % i for i in range(num_targets)]\n    return np.array(target_names)", "response": "Return a vector of target names for the default estimator."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a vector of coefficients for a given label.", "response": "def get_coef(clf, label_id, scale=None):\n    \"\"\"\n    Return a vector of coefficients for a given label,\n    including bias feature.\n\n    ``scale`` (optional) is a scaling vector; coef_[i] => coef[i] * scale[i] if\n    scale[i] is not nan. Intercept is not scaled.\n    \"\"\"\n    if len(clf.coef_.shape) == 2:\n        # Most classifiers (even in binary case) and regressors\n        coef = _dense_1d(clf.coef_[label_id])\n    elif len(clf.coef_.shape) == 1:\n        # SGDRegressor stores coefficients in a 1D array\n        if label_id != 0:\n            raise ValueError(\n                'Unexpected label_id %s for 1D coefficient' % label_id)\n        coef = _dense_1d(clf.coef_)\n    elif len(clf.coef_.shape) == 0:\n        # Lasso with one feature: 0D array\n        coef = np.array([clf.coef_])\n    else:\n        raise ValueError('Unexpected clf.coef_ shape: %s' % clf.coef_.shape)\n\n    if scale is not None:\n        if coef.shape != scale.shape:\n            raise ValueError(\"scale shape is incorrect: expected %s, got %s\" % (\n                coef.shape, scale.shape,\n            ))\n        # print(\"shape is ok\")\n        not_nan = ~np.isnan(scale)\n        coef = coef.copy()\n        coef[not_nan] *= scale[not_nan]\n\n    if not has_intercept(clf):\n        return coef\n    if label_id == 0 and not isinstance(clf.intercept_, np.ndarray):\n        bias = clf.intercept_\n    else:\n        bias = clf.intercept_[label_id]\n    return np.hstack([coef, bias])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the size of a feature vector estimator expects as an input.", "response": "def get_num_features(estimator):\n    \"\"\" Return size of a feature vector estimator expects as an input. \"\"\"\n    if hasattr(estimator, 'coef_'):  # linear models\n        if len(estimator.coef_.shape) == 0:\n            return 1\n        return estimator.coef_.shape[-1]\n    elif hasattr(estimator, 'feature_importances_'):  # ensembles\n        return estimator.feature_importances_.shape[-1]\n    elif hasattr(estimator, 'feature_count_'):  # naive bayes\n        return estimator.feature_count_.shape[-1]\n    elif hasattr(estimator, 'theta_'):\n        return estimator.theta_.shape[-1]\n    elif hasattr(estimator, 'estimators_') and len(estimator.estimators_):\n        # OvR\n        return get_num_features(estimator.estimators_[0])\n    else:\n        raise ValueError(\"Can't figure out feature vector size for %s\" %\n                         estimator)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_X0(X):\n    if pandas_available and isinstance(X, pd.DataFrame):\n        assert len(X) == 1\n        x = np.array(X.iloc[0])\n    else:\n        x, = X\n    return x", "response": "Return zero - th element of a one - element data container."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding intercept column to X", "response": "def add_intercept(X):\n    \"\"\" Add intercept column to X \"\"\"\n    intercept = np.ones((X.shape[0], 1))\n    if sp.issparse(X):\n        return sp.hstack([X, intercept]).tocsr()\n    else:\n        return np.hstack([X, intercept])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexplaining sklearn_crfsuite. CRF weights.", "response": "def explain_weights_sklearn_crfsuite(crf,\n                                     top=20,\n                                     target_names=None,\n                                     targets=None,\n                                     feature_re=None,\n                                     feature_filter=None):\n    \"\"\" Explain sklearn_crfsuite.CRF weights.\n\n    See :func:`eli5.explain_weights` for description of\n    ``top``, ``target_names``, ``targets``,\n    ``feature_re`` and ``feature_filter`` parameters.\n    \"\"\"\n    feature_names = np.array(crf.attributes_)\n    state_coef = crf_state_coef(crf).todense().A\n    transition_coef = crf_transition_coef(crf)\n\n    if feature_filter is not None or feature_re is not None:\n        state_feature_names, flt_indices = (\n            FeatureNames(feature_names).handle_filter(feature_filter, feature_re))\n        state_feature_names = np.array(state_feature_names.feature_names)\n        state_coef = state_coef[:, flt_indices]\n    else:\n        state_feature_names = feature_names\n\n    def _features(label_id):\n        return get_top_features(state_feature_names, state_coef[label_id], top)\n\n    if targets is None:\n        targets = sorted_for_ner(crf.classes_)\n\n    display_names = get_target_display_names(crf.classes_, target_names,\n                                             targets)\n    indices, names = zip(*display_names)\n    transition_coef = filter_transition_coefs(transition_coef, indices)\n\n    return Explanation(\n        targets=[\n            TargetExplanation(\n                target=label,\n                feature_weights=_features(label_id)\n            )\n            for label_id, label in zip(indices, names)\n        ],\n        transition_features=TransitionFeatureWeights(\n            class_names=names,\n            coef=transition_coef,\n        ),\n        estimator=repr(crf),\n        method='CRF',\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfilters the transition coefficients by indices.", "response": "def filter_transition_coefs(transition_coef, indices):\n    \"\"\"\n    >>> coef = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n    >>> filter_transition_coefs(coef, [0])\n    array([[0]])\n    >>> filter_transition_coefs(coef, [1, 2])\n    array([[4, 5],\n           [7, 8]])\n    >>> filter_transition_coefs(coef, [2, 0])\n    array([[8, 6],\n           [2, 0]])\n    >>> filter_transition_coefs(coef, [0, 1, 2])\n    array([[0, 1, 2],\n           [3, 4, 5],\n           [6, 7, 8]])\n    \"\"\"\n    indices = np.array(indices)\n    rows = transition_coef[indices]\n    return rows[:,indices]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of crf_classes sorted in a default order suitable for NER tasks.", "response": "def sorted_for_ner(crf_classes):\n    \"\"\"\n    Return labels sorted in a default order suitable for NER tasks:\n\n    >>> sorted_for_ner(['B-ORG', 'B-PER', 'O', 'I-PER'])\n    ['O', 'B-ORG', 'B-PER', 'I-PER']\n    \"\"\"\n    def key(cls):\n        if len(cls) > 2 and cls[1] == '-':\n            # group names like B-ORG and I-ORG together\n            return cls.split('-', 1)[1], cls\n        return '', cls\n    return sorted(crf_classes, key=key)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a numpy object to their python equivalents. Return converted object.", "response": "def _numpy_to_python(obj):\n    \"\"\" Convert an nested dict/list/tuple that might contain numpy objects\n    to their python equivalents. Return converted object.\n    \"\"\"\n    if isinstance(obj, dict):\n        return {k: _numpy_to_python(v) for k, v in obj.items()}\n    elif isinstance(obj, (list, tuple, np.ndarray)):\n        return [_numpy_to_python(x) for x in obj]\n    elif isinstance(obj, FormattedFeatureName):\n        return obj.value\n    elif isinstance(obj, _numpy_string_types):\n        return six.text_type(obj)\n    elif hasattr(obj, 'dtype') and np.isscalar(obj):\n        if np.issubdtype(obj, np.floating):\n            return float(obj)\n        elif np.issubdtype(obj, np.integer):\n            return int(obj)\n        elif np.issubdtype(obj, np.bool_):\n            return bool(obj)\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _sampler_n_samples(self, n_samples):\n        sampler_indices = self.rng_.choice(range(len(self.samplers)),\n                                           size=n_samples,\n                                           replace=True,\n                                           p=self.weights)\n        return [\n            (self.samplers[idx], freq)\n            for idx, freq in itemfreq(sampler_indices)\n        ]", "response": "Return n_samplers sampler tuples"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsample near the document by replacing some of its features with values sampled from distribution found by KDE.", "response": "def sample_near(self, doc, n_samples=1):\n        \"\"\"\n        Sample near the document by replacing some of its features\n        with values sampled from distribution found by KDE.\n        \"\"\"\n        doc = np.asarray(doc)\n        num_features = len(self.kdes_)\n        sizes = self.rng_.randint(low=1, high=num_features + 1, size=n_samples)\n        samples = []\n        for size in sizes:\n            to_change = self.rng_.choice(num_features, size, replace=False)\n            new_doc = doc.copy()\n            for i in to_change:\n                kde = self.kdes_[i]\n                new_doc[i] = kde.sample(random_state=self.rng_).ravel()\n            samples.append(new_doc)\n        samples = np.asarray(samples)\n        return samples, self._similarity(doc, samples)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of all feature names for a given feature.", "response": "def _all_feature_names(name):\n    # type: (Union[str, bytes, List[Dict]]) -> List[str]\n    \"\"\" All feature names for a feature: usually just the feature itself,\n    but can be several features for unhashed features with collisions.\n    \"\"\"\n    if isinstance(name, bytes):\n        return [name.decode('utf8')]\n    elif isinstance(name, list):\n        return [x['name'] for x in name]\n    else:\n        return [name]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filtered(self, feature_filter, x=None):\n        # type: (Callable, Any) -> Tuple[FeatureNames, List[int]]\n        \"\"\" Return feature names filtered by a regular expression \n        ``feature_re``, and indices of filtered elements.\n        \"\"\"\n        indices = []\n        filtered_feature_names = []\n        indexed_names = None  # type: Optional[Iterable[Tuple[int, Any]]]\n        if isinstance(self.feature_names, (np.ndarray, list)):\n            indexed_names = enumerate(self.feature_names)\n        elif isinstance(self.feature_names, dict):\n            indexed_names = six.iteritems(self.feature_names)\n        elif self.feature_names is None:\n            indexed_names = []\n        assert indexed_names is not None\n\n        if x is not None:\n            if sp.issparse(x) and len(x.shape) == 2:\n                assert x.shape[0] == 1\n                flt = lambda nm, i: feature_filter(nm, x[0, i])\n            else:\n                # FIXME: mypy warns about x[i] because it thinks x can be None\n                flt = lambda nm, i: feature_filter(nm, x[i])  # type: ignore\n        else:\n            flt = lambda nm, i: feature_filter(nm)\n\n        for idx, name in indexed_names:\n            if any(flt(nm, idx) for nm in _all_feature_names(name)):\n                indices.append(idx)\n                filtered_feature_names.append(name)\n        if self.has_bias and flt(self.bias_name, self.bias_idx):\n            assert self.bias_idx is not None  # for mypy\n            bias_name = self.bias_name\n            indices.append(self.bias_idx)\n        else:\n            bias_name = None\n        return (\n            FeatureNames(\n                filtered_feature_names,\n                bias_name=bias_name,\n                unkn_template=self.unkn_template,\n            ),\n            indices)", "response": "Return a list of feature names filtered by a regular expression \n        feature_re and indices of filtered elements."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_feature(self, feature):\n        # type: (Any) -> int\n        \"\"\" Add a new feature name, return it's index.\n        \"\"\"\n        # A copy of self.feature_names is always made, because it might be\n        # \"owned\" by someone else.\n        # It's possible to make the copy only at the first call to\n        # self.add_feature to improve performance.\n        idx = self.n_features\n        if isinstance(self.feature_names, (list, np.ndarray)):\n            self.feature_names = list(self.feature_names)\n            self.feature_names.append(feature)\n        elif isinstance(self.feature_names, dict):\n            self.feature_names = dict(self.feature_names)\n            self.feature_names[idx] = feature\n        elif self.feature_names is None:\n            self.feature_names = {idx: feature}\n        self.n_features += 1\n        return idx", "response": "Add a new feature name to the internal list of features. Return the index of the new feature name."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef format_as_text(expl,  # type: Explanation\n                   show=fields.ALL,\n                   highlight_spaces=None,  # type: Optional[bool]\n                   show_feature_values=False,  # type: bool\n                   ):\n    # type: (...) -> str\n    \"\"\" Format explanation as text.\n\n    Parameters\n    ----------\n    expl : eli5.base.Explanation\n        Explanation returned by ``eli5.explain_weights`` or\n        ``eli5.explain_prediction`` functions.\n\n    highlight_spaces : bool or None, optional\n        Whether to highlight spaces in feature names. This is useful if\n        you work with text and have ngram features which may include spaces\n        at left or right. Default is None, meaning that the value used\n        is set automatically based on vectorizer and feature values.\n\n    show_feature_values : bool\n        When True, feature values are shown along with feature contributions.\n        Default is False.\n\n    show : List[str], optional\n        List of sections to show. Allowed values:\n\n        * 'targets' - per-target feature weights;\n        * 'transition_features' - transition features of a CRF model;\n        * 'feature_importances' - feature importances of a decision tree or\n          an ensemble-based estimator;\n        * 'decision_tree' - decision tree in a graphical form;\n        * 'method' - a string with explanation method;\n        * 'description' - description of explanation method and its caveats.\n\n        ``eli5.formatters.fields`` provides constants that cover common cases:\n        ``INFO`` (method and description), ``WEIGHTS`` (all the rest),\n        and ``ALL`` (all).\n    \"\"\"\n    lines = []  # type: List[str]\n\n    if highlight_spaces is None:\n        highlight_spaces = should_highlight_spaces(expl)\n\n    if expl.error:  # always shown\n        lines.extend(_error_lines(expl))\n\n    explaining_prediction = has_any_values_for_weights(expl)\n    show_feature_values = show_feature_values and explaining_prediction\n\n    for key in show:\n        if not getattr(expl, key, None):\n            continue\n\n        if key == 'method':\n            lines.extend(_method_lines(expl))\n\n        if key == 'description':\n            lines.extend(_description_lines(expl))\n\n        if key == 'transition_features':\n            lines.extend(_transition_features_lines(expl))\n\n        if key == 'targets':\n            lines.extend(_targets_lines(\n                expl,\n                hl_spaces=highlight_spaces,\n                show_feature_values=show_feature_values,\n                explaining_prediction=explaining_prediction,\n            ))\n\n        if key == 'feature_importances':\n            lines.extend(_feature_importances_lines(\n                expl, hl_spaces=highlight_spaces))\n\n        if key == 'decision_tree':\n            lines.extend(_decision_tree_lines(expl))\n\n    return '\\n'.join(lines)", "response": "Format an explanation as text."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nformats feature name for hashed features.", "response": "def _format_unhashed_feature(name, hl_spaces, sep=' | '):\n    # type: (List, bool, str) -> str\n    \"\"\"\n    Format feature name for hashed features.\n    \"\"\"\n    return sep.join(\n        format_signed(n, _format_single_feature, hl_spaces=hl_spaces)\n        for n in name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a ``(pos, neg)`` tuple. ``pos`` and ``neg`` are lists of ``(name, value)`` tuples for features with positive and negative coefficients. Parameters: * ``feature_names`` - a vector of feature names; * ``coef`` - coefficient vector; coef.shape must be equal to feature_names.shape; * ``top`` can be either a number or a ``(num_pos, num_neg)`` tuple. If ``top`` is a number, ``top`` features with largest absolute coefficients are returned. If it is a ``(num_pos, num_neg)`` tuple, the function returns no more than ``num_pos`` positive features and no more than ``num_neg`` negative features. ``None`` value means 'no limit'. * ``x`` is a vector of feature values, passed to FeatureWeight.value.", "response": "def _get_top_features(feature_names, coef, top, x):\n    \"\"\"\n    Return a ``(pos, neg)`` tuple. ``pos`` and ``neg`` are lists of\n    ``(name, value)`` tuples for features with positive and negative\n    coefficients.\n\n    Parameters:\n\n    * ``feature_names`` - a vector of feature names;\n    * ``coef`` - coefficient vector; coef.shape must be equal to\n      feature_names.shape;\n    * ``top`` can be either a number or a ``(num_pos, num_neg)`` tuple.\n      If ``top`` is a number, ``top`` features with largest absolute\n      coefficients are returned. If it is a ``(num_pos, num_neg)`` tuple,\n      the function returns no more than ``num_pos`` positive features and\n      no more than ``num_neg`` negative features. ``None`` value means\n      'no limit'.\n    * ``x`` is a vector of feature values, passed to FeatureWeight.value.\n    \"\"\"\n    if isinstance(top, (list, tuple)):\n        num_pos, num_neg = list(top)  # \"list\" is just for mypy\n        pos = _get_top_positive_features(feature_names, coef, num_pos, x)\n        neg = _get_top_negative_features(feature_names, coef, num_neg, x)\n    else:\n        pos, neg = _get_top_abs_features(feature_names, coef, top, x)\n    return pos, neg"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_char_weights(doc_weighted_spans, preserve_density=None):\n    # type: (DocWeightedSpans, Optional[bool]) -> np.ndarray\n    \"\"\" Return character weights for a text document with highlighted features.\n    If preserve_density is True, then color for longer fragments will be\n    less intensive than for shorter fragments, so that \"sum\" of intensities\n    will correspond to feature weight.\n    If preserve_density is None, then it's value is taken from\n    the corresponding attribute of doc_weighted_spans.\n    \"\"\"\n    if preserve_density is None:\n        preserve_density = doc_weighted_spans.preserve_density\n    char_weights = np.zeros(len(doc_weighted_spans.document))\n    feature_counts = Counter(f for f, _, __ in doc_weighted_spans.spans)\n    for feature, spans, weight in doc_weighted_spans.spans:\n        for start, end in spans:\n            # start can be -1 for char_wb at the start of the document.\n            start = max(0, start)\n            if preserve_density:\n                weight /= (end - start)\n            weight /= feature_counts[feature]\n            char_weights[start:end] += weight\n    return char_weights", "response": "Return the character weights for a text document with highlighted features."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprepare weighted spans for rendering.", "response": "def prepare_weighted_spans(targets,  # type: List[TargetExplanation]\n                           preserve_density=None,  # type: Optional[bool]\n                           ):\n    # type: (...) -> List[Optional[List[PreparedWeightedSpans]]]\n    \"\"\" Return weighted spans prepared for rendering.\n    Calculate a separate weight range for each different weighted\n    span (for each different index): each target has the same number\n    of weighted spans.\n    \"\"\"\n    targets_char_weights = [\n        [get_char_weights(ws, preserve_density=preserve_density)\n         for ws in t.weighted_spans.docs_weighted_spans]\n         if t.weighted_spans else None\n         for t in targets]  # type: List[Optional[List[np.ndarray]]]\n    max_idx = max_or_0(len(ch_w or []) for ch_w in targets_char_weights)\n\n    targets_char_weights_not_None = [\n        cw for cw in targets_char_weights\n        if cw is not None]  # type: List[List[np.ndarray]]\n\n    spans_weight_ranges = [\n        max_or_0(\n            abs(x) for char_weights in targets_char_weights_not_None\n            for x in char_weights[idx])\n        for idx in range(max_idx)]\n    return [\n        [PreparedWeightedSpans(ws, char_weights, weight_range)\n         for ws, char_weights, weight_range in zip(\n            t.weighted_spans.docs_weighted_spans,  # type: ignore\n            t_char_weights,\n            spans_weight_ranges)]\n        if t_char_weights is not None else None\n        for t, t_char_weights in zip(targets, targets_char_weights)]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_span_analyzer(document, vec):\n    preprocessed_doc = vec.build_preprocessor()(vec.decode(document))\n    analyzer = None\n    if vec.analyzer == 'word' and vec.tokenizer is None:\n        stop_words = vec.get_stop_words()\n        tokenize = _build_tokenizer(vec)\n        analyzer = lambda doc: _word_ngrams(vec, tokenize(doc), stop_words)\n    elif vec.analyzer == 'char':\n        preprocessed_doc = vec._white_spaces.sub(' ', preprocessed_doc)\n        analyzer = lambda doc: _char_ngrams(vec, doc)\n    elif vec.analyzer == 'char_wb':\n        preprocessed_doc = vec._white_spaces.sub(' ', preprocessed_doc)\n        analyzer = lambda doc: _char_wb_ngrams(vec, doc)\n    return analyzer, preprocessed_doc", "response": "Build an analyzer and preprocessed doc."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an explanation of an XGBoost estimator or XGBRegressor or XGBClassifier or XGBClassifier or XGBRegressor.", "response": "def explain_weights_xgboost(xgb,\n                            vec=None,\n                            top=20,\n                            target_names=None,  # ignored\n                            targets=None,  # ignored\n                            feature_names=None,\n                            feature_re=None,  # type: Pattern[str]\n                            feature_filter=None,\n                            importance_type='gain',\n                            ):\n    \"\"\"\n    Return an explanation of an XGBoost estimator (via scikit-learn wrapper\n    XGBClassifier or XGBRegressor, or via xgboost.Booster)\n    as feature importances.\n\n    See :func:`eli5.explain_weights` for description of\n    ``top``, ``feature_names``,\n    ``feature_re`` and ``feature_filter`` parameters.\n\n    ``target_names`` and ``targets`` parameters are ignored.\n\n    Parameters\n    ----------\n    importance_type : str, optional\n        A way to get feature importance. Possible values are:\n\n        - 'gain' - the average gain of the feature when it is used in trees\n          (default)\n        - 'weight' - the number of times a feature is used to split the data\n          across all trees\n        - 'cover' - the average coverage of the feature when it is used in trees\n    \"\"\"\n    booster, is_regression = _check_booster_args(xgb)\n    xgb_feature_names = booster.feature_names\n    coef = _xgb_feature_importances(booster, importance_type=importance_type)\n    return get_feature_importance_explanation(\n        xgb, vec, coef,\n        feature_names=feature_names,\n        estimator_feature_names=xgb_feature_names,\n        feature_filter=feature_filter,\n        feature_re=feature_re,\n        top=top,\n        description=DESCRIPTION_XGBOOST,\n        is_regression=is_regression,\n        num_features=coef.shape[-1],\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an explanation of XGBoost prediction for a given document.", "response": "def explain_prediction_xgboost(\n        xgb, doc,\n        vec=None,\n        top=None,\n        top_targets=None,\n        target_names=None,\n        targets=None,\n        feature_names=None,\n        feature_re=None,  # type: Pattern[str]\n        feature_filter=None,\n        vectorized=False,  # type: bool\n        is_regression=None,  # type: bool\n        missing=None,  # type: bool\n        ):\n    \"\"\" Return an explanation of XGBoost prediction (via scikit-learn wrapper\n    XGBClassifier or XGBRegressor, or via xgboost.Booster) as feature weights.\n\n    See :func:`eli5.explain_prediction` for description of\n    ``top``, ``top_targets``, ``target_names``, ``targets``,\n    ``feature_names``, ``feature_re`` and ``feature_filter`` parameters.\n\n    Parameters\n    ----------\n    vec : vectorizer, optional\n        A vectorizer instance used to transform\n        raw features to the input of the estimator ``xgb``\n        (e.g. a fitted CountVectorizer instance); you can pass it\n        instead of ``feature_names``.\n\n    vectorized : bool, optional\n        A flag which tells eli5 if ``doc`` should be\n        passed through ``vec`` or not. By default it is False, meaning that\n        if ``vec`` is not None, ``vec.transform([doc])`` is passed to the\n        estimator. Set it to True if you're passing ``vec``,\n        but ``doc`` is already vectorized.\n\n    is_regression : bool, optional\n        Pass if an ``xgboost.Booster`` is passed as the first argument.\n        True if solving a regression problem (\"objective\" starts with \"reg\")\n        and False for a classification problem.\n        If not set, regression is assumed for a single target estimator\n        and proba will not be shown.\n\n    missing : optional\n        Pass if an ``xgboost.Booster`` is passed as the first argument.\n        Set it to the same value as the ``missing`` argument to\n        ``xgboost.DMatrix``.\n        Matters only if sparse values are used. Default is ``np.nan``.\n\n    Method for determining feature importances follows an idea from\n    http://blog.datadive.net/interpreting-random-forests/.\n    Feature weights are calculated by following decision paths in trees\n    of an ensemble.\n    Each leaf has an output score, and expected scores can also be assigned\n    to parent nodes.\n    Contribution of one feature on the decision path is how much expected score\n    changes from parent to child.\n    Weights of all features sum to the output score of the estimator.\n    \"\"\"\n    booster, is_regression = _check_booster_args(xgb, is_regression)\n    xgb_feature_names = booster.feature_names\n    vec, feature_names = handle_vec(\n        xgb, doc, vec, vectorized, feature_names,\n        num_features=len(xgb_feature_names))\n    if feature_names.bias_name is None:\n        # XGBoost estimators do not have an intercept, but here we interpret\n        # them as having an intercept\n        feature_names.bias_name = '<BIAS>'\n\n    X = get_X(doc, vec, vectorized=vectorized)\n    if sp.issparse(X):\n        # Work around XGBoost issue:\n        # https://github.com/dmlc/xgboost/issues/1238#issuecomment-243872543\n        X = X.tocsc()\n\n    if missing is None:\n        missing = np.nan if isinstance(xgb, Booster) else xgb.missing\n    dmatrix = DMatrix(X, missing=missing)\n\n    if isinstance(xgb, Booster):\n        prediction = xgb.predict(dmatrix)\n        n_targets = prediction.shape[-1]  # type: int\n        if is_regression is None:\n            # When n_targets is 1, this can be classification too,\n            # but it's safer to assume regression.\n            # If n_targets > 1, it must be classification.\n            is_regression = n_targets == 1\n        if is_regression:\n            proba = None\n        else:\n            if n_targets == 1:\n                p, = prediction\n                proba = np.array([1 - p, p])\n            else:\n                proba, = prediction\n    else:\n        proba = predict_proba(xgb, X)\n        n_targets = _xgb_n_targets(xgb)\n\n    if is_regression:\n        names = ['y']\n    elif isinstance(xgb, Booster):\n        names = np.arange(max(2, n_targets))\n    else:\n        names = xgb.classes_\n\n    scores_weights = _prediction_feature_weights(\n        booster, dmatrix, n_targets, feature_names, xgb_feature_names)\n\n    x = get_X0(add_intercept(X))\n    x = _missing_values_set_to_nan(x, missing, sparse_missing=True)\n\n    return get_decision_path_explanation(\n        xgb, doc, vec,\n        x=x,\n        feature_names=feature_names,\n        feature_filter=feature_filter,\n        feature_re=feature_re,\n        top=top,\n        vectorized=vectorized,\n        original_display_names=names,\n        target_names=target_names,\n        targets=targets,\n        top_targets=top_targets,\n        is_regression=is_regression,\n        is_multiclass=n_targets > 1,\n        proba=proba,\n        get_score_weights=lambda label_id: scores_weights[label_id],\n     )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _prediction_feature_weights(booster, dmatrix, n_targets,\n                                feature_names, xgb_feature_names):\n    \"\"\" For each target, return score and numpy array with feature weights\n    on this prediction, following an idea from\n    http://blog.datadive.net/interpreting-random-forests/\n    \"\"\"\n    # XGBClassifier does not have pred_leaf argument, so use booster\n    leaf_ids, = booster.predict(dmatrix, pred_leaf=True)\n    xgb_feature_names = {f: i for i, f in enumerate(xgb_feature_names)}\n    tree_dumps = booster.get_dump(with_stats=True)\n    assert len(tree_dumps) == len(leaf_ids)\n\n    target_feature_weights = partial(\n        _target_feature_weights,\n        feature_names=feature_names, xgb_feature_names=xgb_feature_names)\n    if n_targets > 1:\n        # For multiclass, XGBoost stores dumps and leaf_ids in a 1d array,\n        # so we need to split them.\n        scores_weights = [\n            target_feature_weights(\n                leaf_ids[target_idx::n_targets],\n                tree_dumps[target_idx::n_targets],\n            ) for target_idx in range(n_targets)]\n    else:\n        scores_weights = [target_feature_weights(leaf_ids, tree_dumps)]\n    return scores_weights", "response": "Predicts the feature weights on the given tree."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary with the leaf nodeid and leaf value added to all nodes.", "response": "def _indexed_leafs(parent):\n    \"\"\" Return a leaf nodeid -> node dictionary with\n    \"parent\" and \"leaf\" (average child \"leaf\" value) added to all nodes.\n    \"\"\"\n    if not parent.get('children'):\n        return {parent['nodeid']: parent}\n    indexed = {}\n    for child in parent['children']:\n        child['parent'] = parent\n        if 'leaf' in child:\n            indexed[child['nodeid']] = child\n        else:\n            indexed.update(_indexed_leafs(child))\n    parent['leaf'] = _parent_value(parent['children'])\n    return indexed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the value of the parent node.", "response": "def _parent_value(children):\n    # type: (...) -> int\n    \"\"\" Value of the parent node: a weighted sum of child values.\n    \"\"\"\n    covers = np.array([child['cover'] for child in children])\n    covers /= np.sum(covers)\n    leafs = np.array([child['leaf'] for child in children])\n    return np.sum(leafs * covers)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the text tree dump into a dict that will be used by next XGBoost release release.", "response": "def _parse_tree_dump(text_dump):\n    # type: (str) -> Optional[Dict[str, Any]]\n    \"\"\" Parse text tree dump (one item of a list returned by Booster.get_dump())\n    into json format that will be used by next XGBoost release.\n    \"\"\"\n    result = None\n    stack = []  # type: List[Dict]\n    for line in text_dump.split('\\n'):\n        if line:\n            depth, node = _parse_dump_line(line)\n            if depth == 0:\n                assert not stack\n                result = node\n                stack.append(node)\n            elif depth > len(stack):\n                raise ValueError('Unexpected dump structure')\n            else:\n                if depth < len(stack):\n                    stack = stack[:depth]\n                stack[-1].setdefault('children', []).append(node)\n                stack.append(node)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a copy of values where missing values are replaced to nan according.", "response": "def _missing_values_set_to_nan(values, missing_value, sparse_missing):\n    \"\"\" Return a copy of values where missing values (equal to missing_value)\n    are replaced to nan according. If sparse_missing is True,\n    entries missing in a sparse matrix will also be set to nan.\n    Sparse matrices will be converted to dense format.\n    \"\"\"\n    if sp.issparse(values):\n        assert values.shape[0] == 1\n    if sparse_missing and sp.issparse(values) and missing_value != 0:\n        # Nothing special needs to be done for missing.value == 0 because\n        # missing values are assumed to be zero in sparse matrices.\n        values_coo = values.tocoo()\n        values = values.toarray()[0]\n        missing_mask = values == 0\n        # fix for possible zero values\n        missing_mask[values_coo.col] = False\n        values[missing_mask] = np.nan\n    elif is_sparse_vector(values):\n        values = values.toarray()[0]\n    else:\n        values = values.copy()\n    if not np.isnan(missing_value):\n        values[values == missing_value] = np.nan\n    return values"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns no more than k indices of smallest values.", "response": "def argsort_k_smallest(x, k):\n    \"\"\" Return no more than ``k`` indices of smallest values. \"\"\"\n    if k == 0:\n        return np.array([], dtype=np.intp)\n    if k is None or k >= len(x):\n        return np.argsort(x)\n    indices = np.argpartition(x, k)[:k]\n    values = x[indices]\n    return indices[np.argsort(values)]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_sparse_vector(x):\n    return sp.issparse(x) and len(x.shape) == 2 and x.shape[0] == 1", "response": "Check if x is a 2D sparse matrix with first shape equal to 1."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts indices to a boolean mask.", "response": "def indices_to_bool_mask(indices, size):\n    \"\"\" Convert indices to a boolean (integer) mask.\n\n    >>> list(indices_to_bool_mask(np.array([2, 3]), 4))\n    [False, False, True, True]\n\n    >>> list(indices_to_bool_mask([2, 3], 4))\n    [False, False, True, True]\n\n    >>> indices_to_bool_mask(np.array([5]), 2)\n    Traceback (most recent call last):\n    ...\n    IndexError: index 5 is out of bounds ...\n    \"\"\"\n    mask = np.zeros(size, dtype=bool)\n    mask[indices] = 1\n    return mask"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_target_display_names(original_names=None, target_names=None,\n                             targets=None, top_targets=None, score=None):\n    \"\"\"\n    Return a list of (target_id, display_name) tuples.\n\n    By default original names are passed as-is, only indices are added:\n    >>> get_target_display_names(['x', 'y'])\n    [(0, 'x'), (1, 'y')]\n\n    ``targets`` can be written using both names from ``target_names` and\n    from ``original_names``:\n    >>> get_target_display_names(['x', 'y'], targets=['y', 'X'],\n    ...                   target_names={'x': 'X'})\n    [(1, 'y'), (0, 'X')]\n\n    Provide display names:\n    >>> get_target_display_names([0, 2], target_names=['foo', 'bar'])\n    [(0, 'foo'), (1, 'bar')]\n\n    Change order of labels:\n    >>> get_target_display_names(['x', 'y'], targets=['y', 'x'])\n    [(1, 'y'), (0, 'x')]\n\n    Provide display names, choose only a subset of labels:\n    >>> get_target_display_names([0, 2], target_names=['foo', 'bar'], targets=[2])\n    [(1, 'bar')]\n\n    >>> get_target_display_names([False, True], targets=[True])\n    [(1, True)]\n\n    >>> get_target_display_names([False, True], targets=[False])\n    [(0, False)]\n\n    target_names can be a dictionary with {old_name: new_name} labels:\n    >>> get_target_display_names(['x', 'y'], targets=['y', 'x'],\n    ...                   target_names={'x': 'X'})\n    [(1, 'y'), (0, 'X')]\n\n    Error is raised when target_names format is invalid:\n    >>> get_target_display_names(['x', 'y'], target_names=['foo'])\n    Traceback (most recent call last):\n    ...\n    ValueError: target_names must have the same length as original names (expected 2, got 1)\n\n    Top target selection by score:\n    >>> get_target_display_names(['x', 'y', 'z'], score=[1, 2, 1.5], top_targets=2)\n    [(1, 'y'), (2, 'z')]\n\n    Top target selection by score, negative:\n    >>> get_target_display_names(['x', 'y', 'z'], score=[1, 2, 1.5], top_targets=-3)\n    [(0, 'x'), (2, 'z'), (1, 'y')]\n\n    Error is raised if both top_targets and targets are passed:\n    >>> get_target_display_names(['x', 'y'], targets=['x'], score=[1, 2], top_targets=1)\n    Traceback (most recent call last):\n    ...\n    ValueError: Pass either \"targets\" or \"top_targets\", not both\n    \"\"\"\n    if isinstance(target_names, (list, tuple, np.ndarray)):\n        if original_names is not None:\n            if len(target_names) != len(original_names):\n                raise ValueError(\"target_names must have the same length as \"\n                                 \"original names (expected {}, got {})\".format(\n                                     len(original_names), len(target_names)\n                                 ))\n        display_names = target_names\n    elif isinstance(target_names, dict):\n        display_names = [target_names.get(name, name)\n                         for name in original_names]\n    else:\n        display_names = original_names\n\n    if targets is None:\n        if top_targets is not None:\n            assert len(score) == len(original_names)\n            if top_targets < 0:\n                reverse = False\n                top_targets = -top_targets\n            else:\n                reverse = True\n            targets = [\n                target for _, target in sorted(\n                    enumerate(original_names),\n                    key=lambda x: score[x[0]],\n                    reverse=reverse,\n                )][:top_targets]\n        else:\n            targets = original_names\n    elif top_targets is not None:\n        raise ValueError('Pass either \"targets\" or \"top_targets\", not both')\n\n    target_indices = _get_value_indices(original_names, display_names, targets)\n    names = [display_names[i] for i in target_indices]\n    return list(zip(target_indices, names))", "response": "Get a list of target_id and display_name tuples."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_binary_target_scale_label_id(score, display_names, proba=None):\n    if score is not None:\n        label_id = 1 if score >= 0 else 0\n        scale = -1 if label_id == 0 else 1\n    else:\n        # Only probability is available - this is the case for\n        # DecisionTreeClassifier. As contributions sum to the probability\n        # (not to the score), they shouldn't be inverted.\n        label_id = 1 if proba[1] >= 0.5 else 0\n        scale = 1\n\n    if len(display_names) == 1:  # target is passed explicitly\n        predicted_label_id = label_id\n        label_id, target = display_names[0]\n        scale *= -1 if label_id != predicted_label_id else 1\n    else:\n        target = display_names[label_id][1]\n\n    return target, scale, label_id", "response": "Return the target scale and label_id tuple for a binary classifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_value_indices(names1, names2, lookups):\n    positions = {name: idx for idx, name in enumerate(names2)}\n    positions.update({name: idx for idx, name in enumerate(names1)})\n    return [positions[name] for name in lookups]", "response": "Get the indices of the values in the sequence names1 and names2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrender Graphviz data to SVG", "response": "def dot2svg(dot):\n    # type: (str) -> str\n    \"\"\" Render Graphviz data to SVG \"\"\"\n    svg = graphviz.Source(dot).pipe(format='svg').decode('utf8')  # type: str\n    # strip doctype and xml declaration\n    svg = svg[svg.index('<svg'):]\n    return svg"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an explanation of an estimator", "response": "def explain_weights_sklearn(estimator, vec=None, top=_TOP,\n                            target_names=None,\n                            targets=None,\n                            feature_names=None, coef_scale=None,\n                            feature_re=None, feature_filter=None):\n    \"\"\" Return an explanation of an estimator \"\"\"\n    return explain_weights_sklearn_not_supported(estimator)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef explain_linear_classifier_weights(clf,\n                                      vec=None,\n                                      top=_TOP,\n                                      target_names=None,\n                                      targets=None,\n                                      feature_names=None,\n                                      coef_scale=None,\n                                      feature_re=None,\n                                      feature_filter=None,\n                                      ):\n    \"\"\"\n    Return an explanation of a linear classifier weights.\n\n    See :func:`eli5.explain_weights` for description of\n    ``top``, ``target_names``, ``targets``, ``feature_names``,\n    ``feature_re`` and ``feature_filter`` parameters.\n\n    ``vec`` is a vectorizer instance used to transform\n    raw features to the input of the classifier ``clf``\n    (e.g. a fitted CountVectorizer instance); you can pass it\n    instead of ``feature_names``.\n\n    ``coef_scale`` is a 1D np.ndarray with a scaling coefficient\n    for each feature; coef[i] = coef[i] * coef_scale[i] if\n    coef_scale[i] is not nan. Use it if you want to scale coefficients\n    before displaying them, to take input feature sign or scale in account.\n    \"\"\"\n    feature_names, coef_scale = handle_hashing_vec(vec, feature_names,\n                                                   coef_scale)\n    feature_names, flt_indices = get_feature_names_filtered(\n        clf, vec,\n        feature_names=feature_names,\n        feature_filter=feature_filter,\n        feature_re=feature_re,\n    )\n\n    _extra_caveats = \"\\n\" + HASHING_CAVEATS if is_invhashing(vec) else ''\n\n    def _features(label_id):\n        coef = get_coef(clf, label_id, scale=coef_scale)\n        if flt_indices is not None:\n            coef = coef[flt_indices]\n        return get_top_features(feature_names, coef, top)\n\n    classes = getattr(clf, \"classes_\", [\"-1\", \"1\"])  # OneClassSVM support\n    display_names = get_target_display_names(classes, target_names, targets)\n    if is_multiclass_classifier(clf):\n        return Explanation(\n            targets=[\n                TargetExplanation(\n                    target=label,\n                    feature_weights=_features(label_id)\n                )\n                for label_id, label in display_names\n                ],\n            description=DESCRIPTION_CLF_MULTICLASS + _extra_caveats,\n            estimator=repr(clf),\n            method='linear model',\n        )\n    else:\n        # for binary classifiers scikit-learn stores a single coefficient\n        # vector, which corresponds to clf.classes_[1].\n        return Explanation(\n            targets=[\n                TargetExplanation(\n                    target=display_names[1][1],\n                    feature_weights=_features(0),\n                )\n            ],\n            description=DESCRIPTION_CLF_BINARY + _extra_caveats,\n            estimator=repr(clf),\n            method='linear model',\n        )", "response": "Return an explanation of a linear classifier weights."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an explanation of an RF feature importance estimator.", "response": "def explain_rf_feature_importance(estimator,\n                                  vec=None,\n                                  top=_TOP,\n                                  target_names=None,  # ignored\n                                  targets=None,  # ignored\n                                  feature_names=None,\n                                  feature_re=None,\n                                  feature_filter=None,\n                                  ):\n    \"\"\"\n    Return an explanation of a tree-based ensemble estimator.\n\n    See :func:`eli5.explain_weights` for description of\n    ``top``, ``feature_names``, ``feature_re`` and ``feature_filter``\n    parameters.\n\n    ``target_names`` and ``targets`` parameters are ignored.\n\n    ``vec`` is a vectorizer instance used to transform\n    raw features to the input of the estimator (e.g. a fitted\n    CountVectorizer instance); you can pass it instead of ``feature_names``.\n    \"\"\"\n    coef = estimator.feature_importances_\n    trees = np.array(estimator.estimators_).ravel()\n    coef_std = np.std([tree.feature_importances_ for tree in trees], axis=0)\n    return get_feature_importance_explanation(estimator, vec, coef,\n        coef_std=coef_std,\n        feature_names=feature_names,\n        feature_filter=feature_filter,\n        feature_re=feature_re,\n        top=top,\n        description=DESCRIPTION_RANDOM_FOREST,\n        is_regression=isinstance(estimator, RegressorMixin),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef explain_decision_tree(estimator,\n                          vec=None,\n                          top=_TOP,\n                          target_names=None,\n                          targets=None,  # ignored\n                          feature_names=None,\n                          feature_re=None,\n                          feature_filter=None,\n                          **export_graphviz_kwargs):\n    \"\"\"\n    Return an explanation of a decision tree.\n\n    See :func:`eli5.explain_weights` for description of\n    ``top``, ``target_names``, ``feature_names``,\n    ``feature_re`` and ``feature_filter`` parameters.\n\n    ``targets`` parameter is ignored.\n\n    ``vec`` is a vectorizer instance used to transform\n    raw features to the input of the estimator (e.g. a fitted\n    CountVectorizer instance); you can pass it instead of ``feature_names``.\n\n    All other keyword arguments are passed to\n    `sklearn.tree.export_graphviz`_ function.\n\n    .. _sklearn.tree.export_graphviz: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n    \"\"\"\n    feature_names = get_feature_names(estimator, vec,\n                                      feature_names=feature_names)\n    tree_feature_names = feature_names\n    feature_names, flt_indices = feature_names.handle_filter(\n        feature_filter, feature_re)\n    feature_importances = get_feature_importances_filtered(\n        estimator.feature_importances_, feature_names, flt_indices, top)\n\n    export_graphviz_kwargs.setdefault(\"proportion\", True)\n    tree_info = get_tree_info(\n        estimator,\n        feature_names=tree_feature_names,\n        class_names=target_names,\n        **export_graphviz_kwargs)\n\n    return Explanation(\n        feature_importances=feature_importances,\n        decision_tree=tree_info,\n        description=DESCRIPTION_DECISION_TREE,\n        estimator=repr(estimator),\n        method='decision tree',\n    )", "response": "Return an explanation of a decision tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef explain_linear_regressor_weights(reg,\n                                     vec=None,\n                                     top=_TOP,\n                                     target_names=None,\n                                     targets=None,\n                                     feature_names=None,\n                                     coef_scale=None,\n                                     feature_re=None,\n                                     feature_filter=None,\n                                     ):\n    \"\"\"\n    Return an explanation of a linear regressor weights.\n\n    See :func:`eli5.explain_weights` for description of\n    ``top``, ``target_names``, ``targets``, ``feature_names``,\n    ``feature_re`` and ``feature_filter`` parameters.\n\n    ``vec`` is a vectorizer instance used to transform\n    raw features to the input of the regressor ``reg``; you can\n    pass it instead of ``feature_names``.\n\n    ``coef_scale`` is a 1D np.ndarray with a scaling coefficient\n    for each feature; coef[i] = coef[i] * coef_scale[i] if\n    coef_scale[i] is not nan. Use it if you want to scale coefficients\n    before displaying them, to take input feature sign or scale in account.\n    \"\"\"\n    if isinstance(reg, (SVR, NuSVR)) and reg.kernel != 'linear':\n        return explain_weights_sklearn_not_supported(reg)\n\n    feature_names, coef_scale = handle_hashing_vec(vec, feature_names,\n                                                   coef_scale)\n    feature_names, flt_indices = get_feature_names_filtered(\n        reg, vec,\n        feature_names=feature_names,\n        feature_filter=feature_filter,\n        feature_re=feature_re,\n    )\n    _extra_caveats = \"\\n\" + HASHING_CAVEATS if is_invhashing(vec) else ''\n\n    def _features(target_id):\n        coef = get_coef(reg, target_id, scale=coef_scale)\n        if flt_indices is not None:\n            coef = coef[flt_indices]\n        return get_top_features(feature_names, coef, top)\n\n    display_names = get_target_display_names(get_default_target_names(reg),\n                                             target_names, targets)\n    if is_multitarget_regressor(reg):\n        return Explanation(\n            targets=[\n                TargetExplanation(\n                    target=target_name,\n                    feature_weights=_features(target_id)\n                )\n                for target_id, target_name in display_names\n                ],\n            description=DESCRIPTION_REGRESSION_MULTITARGET + _extra_caveats,\n            estimator=repr(reg),\n            method='linear model',\n            is_regression=True,\n        )\n    else:\n        return Explanation(\n            targets=[TargetExplanation(\n                target=display_names[0][1],\n                feature_weights=_features(0),\n            )],\n            description=DESCRIPTION_REGRESSION + _extra_caveats,\n            estimator=repr(reg),\n            method='linear model',\n            is_regression=True,\n        )", "response": "Return an explanation of a linear regressor weights."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an explanation of PermutationImportance.", "response": "def explain_permutation_importance(estimator,\n                                   vec=None,\n                                   top=_TOP,\n                                   target_names=None,  # ignored\n                                   targets=None,  # ignored\n                                   feature_names=None,\n                                   feature_re=None,\n                                   feature_filter=None,\n                                   ):\n    \"\"\"\n    Return an explanation of PermutationImportance.\n\n    See :func:`eli5.explain_weights` for description of\n    ``top``, ``feature_names``, ``feature_re`` and ``feature_filter``\n    parameters.\n\n    ``target_names`` and ``targets`` parameters are ignored.\n\n    ``vec`` is a vectorizer instance used to transform\n    raw features to the input of the estimator (e.g. a fitted\n    CountVectorizer instance); you can pass it instead of ``feature_names``.\n    \"\"\"\n    coef = estimator.feature_importances_\n    coef_std = estimator.feature_importances_std_\n    return get_feature_importance_explanation(estimator, vec, coef,\n        coef_std=coef_std,\n        feature_names=feature_names,\n        feature_filter=feature_filter,\n        feature_re=feature_re,\n        top=top,\n        description=DESCRIPTION_SCORE_DECREASE + estimator.caveats_,\n        is_regression=isinstance(estimator.wrapped_estimator_, RegressorMixin),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_collisions(indices):\n    # type: (...) -> Dict[int, List[int]]\n    \"\"\"\n    Return a dict ``{column_id: [possible term ids]}``\n    with collision information.\n    \"\"\"\n    collisions = defaultdict(list)  # type: Dict[int, List[int]]\n    for term_id, hash_id in enumerate(indices):\n        collisions[hash_id].append(term_id)\n    return dict(collisions)", "response": "Returns a dict of column_id = > list of possible term ids."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the column index and sign of each term in terms.", "response": "def _get_indices_and_signs(hasher, terms):\n    \"\"\"\n    For each term from ``terms`` return its column index and sign,\n    as assigned by FeatureHasher ``hasher``.\n    \"\"\"\n    X = _transform_terms(hasher, terms)\n    indices = X.nonzero()[1]\n    signs = X.sum(axis=1).A.ravel()\n    return indices, signs"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns feature_names and coef_scale for invhashing vectorizers.", "response": "def handle_hashing_vec(vec, feature_names, coef_scale, with_coef_scale=True):\n    \"\"\" Return feature_names and coef_scale (if with_coef_scale is True),\n    calling .get_feature_names for invhashing vectorizers.\n    \"\"\"\n    needs_coef_scale = with_coef_scale and coef_scale is None\n    if is_invhashing(vec):\n        if feature_names is None:\n            feature_names = vec.get_feature_names(always_signed=False)\n        if needs_coef_scale:\n            coef_scale = vec.column_signs_\n    elif (isinstance(vec, FeatureUnion) and\n              any(is_invhashing(v) for _, v in vec.transformer_list) and\n              (needs_coef_scale or feature_names is None)):\n        _feature_names, _coef_scale = _invhashing_union_feature_names_scale(vec)\n        if feature_names is None:\n            feature_names = _feature_names\n        if needs_coef_scale:\n            coef_scale = _coef_scale\n    return (feature_names, coef_scale) if with_coef_scale else feature_names"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninvert the given vectorizer vec on the given docs.", "response": "def invert_hashing_and_fit(\n        vec,  # type: Union[FeatureUnion, HashingVectorizer]\n        docs\n    ):\n    # type: (...) -> Union[FeatureUnion, InvertableHashingVectorizer]\n    \"\"\" Create an :class:`~.InvertableHashingVectorizer` from hashing\n    vectorizer vec and fit it on docs. If vec is a FeatureUnion, do it for all\n    hashing vectorizers in the union.\n    Return an :class:`~.InvertableHashingVectorizer`, or a FeatureUnion,\n    or an unchanged vectorizer.\n    \"\"\"\n    if isinstance(vec, HashingVectorizer):\n        vec = InvertableHashingVectorizer(vec)\n        vec.fit(docs)\n    elif (isinstance(vec, FeatureUnion) and\n              any(isinstance(v, HashingVectorizer)\n                  for _, v in vec.transformer_list)):\n        vec = _fit_invhashing_union(vec, docs)\n    return vec"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfit InvertableHashingVectorizer on a list of docs inside a FeatureUnion.", "response": "def _fit_invhashing_union(vec_union, docs):\n    # type: (FeatureUnion, Any) -> FeatureUnion\n    \"\"\" Fit InvertableHashingVectorizer on doc inside a FeatureUnion.\n    \"\"\"\n    return FeatureUnion(\n        [(name, invert_hashing_and_fit(v, docs))\n         for name, v in vec_union.transformer_list],\n        transformer_weights=vec_union.transformer_weights,\n        n_jobs=vec_union.n_jobs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfitting the unhasher to the set of possible terms.", "response": "def fit(self, X, y=None):\n        \"\"\" Extract possible terms from documents \"\"\"\n        self.unhasher.fit(self._get_terms_iter(X))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of feature names for the current class entry.", "response": "def get_feature_names(self, always_signed=True):\n        # type: (bool) -> FeatureNames\n        \"\"\"\n        Return feature names.\n        This is a best-effort function which tries to reconstruct feature\n        names based on what it has seen so far.\n\n        HashingVectorizer uses a signed hash function. If always_signed is True,\n        each term in feature names is prepended with its sign. If it is False,\n        signs are only shown in case of possible collisions of different sign.\n\n        You probably want always_signed=True if you're checking\n        unprocessed classifier coefficients, and always_signed=False\n        if you've taken care of :attr:`column_signs_`.\n        \"\"\"\n        return self.unhasher.get_feature_names(\n            always_signed=always_signed,\n            always_positive=self._always_positive(),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a numpy array with expected signs of features.", "response": "def column_signs_(self):\n        \"\"\"\n        Return a numpy array with expected signs of features.\n        Values are\n\n        * +1 when all known terms which map to the column have positive sign;\n        * -1 when all known terms which map to the column have negative sign;\n        * ``nan`` when there are both positive and negative known terms\n          for this column, or when there is no known term which maps to this\n          column.\n        \"\"\"\n        if self._always_positive():\n            return np.ones(self.n_features)\n        self.unhasher.recalculate_attributes()\n        return self.unhasher.column_signs_"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef recalculate_attributes(self, force=False):\n        # type: (bool) -> None\n        \"\"\"\n        Update all computed attributes. It is only needed if you need to access\n        computed attributes after :meth:`patrial_fit` was called.\n        \"\"\"\n        if not self._attributes_dirty and not force:\n            return\n        terms = [term for term, _ in self._term_counts.most_common()]\n        if six.PY2:\n            terms = np.array(terms, dtype=np.object)\n        else:\n            terms = np.array(terms)\n        if len(terms):\n            indices, signs = _get_indices_and_signs(self.hasher, terms)\n        else:\n            indices, signs = np.array([]), np.array([])\n        self.terms_ = terms  # type: np.ndarray\n        self.term_columns_ = indices\n        self.term_signs_ = signs\n        self.collisions_ = _get_collisions(indices)\n        self.column_signs_ = self._get_column_signs()\n        self._attributes_dirty = False", "response": "Recalculate all computed attributes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tree2text(tree_obj, indent=4):\n    # type: (TreeInfo, int) -> str\n    \"\"\"\n    Return text representation of a decision tree.\n    \"\"\"\n    parts = []\n\n    def _format_node(node, depth=0):\n        # type: (NodeInfo, int) -> None\n        def p(*args):\n            # type: (*str) -> None\n            parts.append(\" \" * depth * indent)\n            parts.extend(args)\n\n        if node.is_leaf:\n            value_repr = _format_leaf_value(tree_obj, node)\n            parts.append(\"  ---> {}\".format(value_repr))\n        else:\n            assert node.left is not None\n            assert node.right is not None\n            feat_name = node.feature_name\n\n            if depth > 0:\n                parts.append(\"\\n\")\n            left_samples = node.left.sample_ratio\n            p(\"{feat_name} <= {threshold:0.3f}  ({left_samples:0.1%})\".format(\n                left_samples=left_samples,\n                feat_name=feat_name,\n                threshold=node.threshold,\n            ))\n            _format_node(node.left, depth=depth + 1)\n\n            parts.append(\"\\n\")\n            right_samples = node.right.sample_ratio\n            p(\"{feat_name} > {threshold:0.3f}  ({right_samples:0.1%})\".format(\n                right_samples=right_samples,\n                feat_name=feat_name,\n                threshold=node.threshold,\n                ))\n            _format_node(node.right, depth=depth + 1)\n\n    _format_node(tree_obj.tree)\n    return \"\".join(parts)", "response": "Return text representation of a decision tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nformatting an array of values.", "response": "def _format_array(x, fmt):\n    # type: (Any, str) -> str\n    \"\"\"\n    >>> _format_array([0, 1.0], \"{:0.3f}\")\n    '[0.000, 1.000]'\n    \"\"\"\n    value_repr = \", \".join(fmt.format(v) for v in x)\n    return \"[{}]\".format(value_repr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef explain_weights_df(estimator, **kwargs):\n    # type: (...) -> pd.DataFrame\n    \"\"\" Explain weights and export them to ``pandas.DataFrame``.\n    All keyword arguments are passed to :func:`eli5.explain_weights`.\n    Weights of all features are exported by default.\n    \"\"\"\n    kwargs = _set_defaults(kwargs)\n    return format_as_dataframe(\n        eli5.explain_weights(estimator, **kwargs))", "response": "Explain weights and export them to pandas. DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef explain_weights_dfs(estimator, **kwargs):\n    # type: (...) -> Dict[str, pd.DataFrame]\n    \"\"\" Explain weights and export them to a dict with ``pandas.DataFrame``\n    values (as :func:`eli5.formatters.as_dataframe.format_as_dataframes` does).\n    All keyword arguments are passed to :func:`eli5.explain_weights`.\n    Weights of all features are exported by default.\n    \"\"\"\n    kwargs = _set_defaults(kwargs)\n    return format_as_dataframes(\n        eli5.explain_weights(estimator, **kwargs))", "response": "Explain weights and export them to a dict with pandas. DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef explain_prediction_df(estimator, doc, **kwargs):\n    # type: (...) -> pd.DataFrame\n    \"\"\" Explain prediction and export explanation to ``pandas.DataFrame``\n    All keyword arguments are passed to :func:`eli5.explain_prediction`.\n    Weights of all features are exported by default.\n    \"\"\"\n    kwargs = _set_defaults(kwargs)\n    return format_as_dataframe(\n        eli5.explain_prediction(estimator, doc, **kwargs))", "response": "Explain prediction and export explanation to pandas. DataFrame"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexplaining prediction and export explanation to a dict with pandas. DataFrame values.", "response": "def explain_prediction_dfs(estimator, doc, **kwargs):\n    # type: (...) -> Dict[str, pd.DataFrame]\n    \"\"\" Explain prediction and export explanation\n    to a dict with ``pandas.DataFrame`` values\n    (as :func:`eli5.formatters.as_dataframe.format_as_dataframes` does).\n    All keyword arguments are passed to :func:`eli5.explain_prediction`.\n    Weights of all features are exported by default.\n    \"\"\"\n    kwargs = _set_defaults(kwargs)\n    return format_as_dataframes(\n        eli5.explain_prediction(estimator, doc, **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef format_as_dataframes(explanation):\n    # type: (Explanation) -> Dict[str, pd.DataFrame]\n    \"\"\" Export an explanation to a dictionary with ``pandas.DataFrame`` values\n    and string keys that correspond to explanation attributes.\n    Use this method if several dataframes can be exported from a single\n    explanation (e.g. for CRF explanation with has both feature weights\n    and transition matrix).\n    Note that :func:`eli5.explain_weights` limits number of features\n    by default. If you need all features, pass ``top=None`` to\n    :func:`eli5.explain_weights`, or use\n    :func:`explain_weights_dfs`.\n    \"\"\"\n    result = {}\n    for attr in _EXPORTED_ATTRIBUTES:\n        value = getattr(explanation, attr)\n        if value:\n            result[attr] = format_as_dataframe(value)\n    return result", "response": "Export an explanation to a dictionary with pandas. DataFrame values that correspond to the attributes of the explanation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexport an explanation to a single pandas. DataFrame.", "response": "def format_as_dataframe(explanation):\n    # type: (Explanation) -> Optional[pd.DataFrame]\n    \"\"\" Export an explanation to a single ``pandas.DataFrame``.\n    In case several dataframes could be exported by\n    :func:`eli5.formatters.as_dataframe.format_as_dataframes`,\n    a warning is raised. If no dataframe can be exported, ``None`` is returned.\n    This function also accepts some components of the explanation as arguments:\n    feature importances, targets, transition features.\n    Note that :func:`eli5.explain_weights` limits number of features\n    by default. If you need all features, pass ``top=None`` to\n    :func:`eli5.explain_weights`, or use\n    :func:`explain_weights_df`.\n    \"\"\"\n    for attr in _EXPORTED_ATTRIBUTES:\n        value = getattr(explanation, attr)\n        if value:\n            other_attrs = [a for a in _EXPORTED_ATTRIBUTES\n                           if getattr(explanation, a) and a != attr]\n            if other_attrs:\n                warnings.warn('Exporting {} to DataFrame, but also {} could be '\n                              'exported. Consider using eli5.format_as_dataframes.'\n                              .format(attr, ', '.join(other_attrs)))\n            return format_as_dataframe(value)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nyields the matrix X with one or more columns shuffled.", "response": "def iter_shuffled(X, columns_to_shuffle=None, pre_shuffle=False,\n                  random_state=None):\n    \"\"\"\n    Return an iterator of X matrices which have one or more columns shuffled.\n    After each iteration yielded matrix is mutated inplace, so\n    if you want to use multiple of them at the same time, make copies.\n\n    ``columns_to_shuffle`` is a sequence of column numbers to shuffle.\n    By default, all columns are shuffled once, i.e. columns_to_shuffle\n    is ``range(X.shape[1])``.\n\n    If ``pre_shuffle`` is True, a copy of ``X`` is shuffled once, and then\n    result takes shuffled columns from this copy. If it is False,\n    columns are shuffled on fly. ``pre_shuffle = True`` can be faster\n    if there is a lot of columns, or if columns are used multiple times.\n    \"\"\"\n    rng = check_random_state(random_state)\n\n    if columns_to_shuffle is None:\n        columns_to_shuffle = range(X.shape[1])\n\n    if pre_shuffle:\n        X_shuffled = X.copy()\n        rng.shuffle(X_shuffled)\n\n    X_res = X.copy()\n    for columns in columns_to_shuffle:\n        if pre_shuffle:\n            X_res[:, columns] = X_shuffled[:, columns]\n        else:\n            rng.shuffle(X_res[:, columns])\n        yield X_res\n        X_res[:, columns] = X[:, columns]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_score_importances(\n        score_func,  # type: Callable[[Any, Any], float]\n        X,\n        y,\n        n_iter=5,  # type: int\n        columns_to_shuffle=None,\n        random_state=None\n    ):\n    # type: (...) -> Tuple[float, List[np.ndarray]]\n    \"\"\"\n    Return ``(base_score, score_decreases)`` tuple with the base score and\n    score decreases when a feature is not available.\n\n    ``base_score`` is ``score_func(X, y)``; ``score_decreases``\n    is a list of length ``n_iter`` with feature importance arrays\n    (each array is of shape ``n_features``); feature importances are computed\n    as score decrease when a feature is not available.\n\n    ``n_iter`` iterations of the basic algorithm is done, each iteration\n    starting from a different random seed.\n\n    If you just want feature importances, you can take a mean of the result::\n\n        import numpy as np\n        from eli5.permutation_importance import get_score_importances\n\n        base_score, score_decreases = get_score_importances(score_func, X, y)\n        feature_importances = np.mean(score_decreases, axis=0)\n\n    \"\"\"\n    rng = check_random_state(random_state)\n    base_score = score_func(X, y)\n    scores_decreases = []\n    for i in range(n_iter):\n        scores_shuffled = _get_scores_shufled(\n            score_func, X, y, columns_to_shuffle=columns_to_shuffle,\n            random_state=rng\n        )\n        scores_decreases.append(-scores_shuffled + base_score)\n    return base_score, scores_decreases", "response": "Basic algorithm for calculating the score of the set of features in the current language."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an explanation of the classifier weights in an IPython. display. HTML object.", "response": "def show_weights(estimator, **kwargs):\n    \"\"\" Return an explanation of estimator parameters (weights)\n    as an IPython.display.HTML object. Use this function\n    to show classifier weights in IPython.\n\n    :func:`show_weights` accepts all\n    :func:`eli5.explain_weights` arguments and all\n    :func:`eli5.formatters.html.format_as_html`\n    keyword arguments, so it is possible to get explanation and\n    customize formatting in a single call.\n\n    Parameters\n    ----------\n    estimator : object\n        Estimator instance. This argument must be positional.\n\n    top : int or (int, int) tuple, optional\n        Number of features to show. When ``top`` is int, ``top`` features with\n        a highest absolute values are shown. When it is (pos, neg) tuple,\n        no more than ``pos`` positive features and no more than ``neg``\n        negative features is shown. ``None`` value means no limit.\n\n        This argument may be supported or not, depending on estimator type.\n\n    target_names : list[str] or {'old_name': 'new_name'} dict, optional\n        Names of targets or classes. This argument can be used to provide\n        human-readable class/target names for estimators which don't expose\n        clss names themselves. It can be also used to rename estimator-provided\n        classes before displaying them.\n\n        This argument may be supported or not, depending on estimator type.\n\n    targets : list, optional\n        Order of class/target names to show. This argument can be also used\n        to show information only for a subset of classes. It should be a list\n        of class / target names which match either names provided by\n        an estimator or names defined in ``target_names`` parameter.\n\n        This argument may be supported or not, depending on estimator type.\n\n    feature_names : list, optional\n        A list of feature names. It allows to specify feature\n        names when they are not provided by an estimator object.\n\n        This argument may be supported or not, depending on estimator type.\n\n    feature_re : str, optional\n        Only feature names which match ``feature_re`` regex are shown\n        (more precisely, ``re.search(feature_re, x)`` is checked).\n\n    feature_filter : Callable[[str], bool], optional\n        Only feature names for which ``feature_filter`` function returns True\n        are shown.\n\n    show : List[str], optional\n        List of sections to show. Allowed values:\n\n        * 'targets' - per-target feature weights;\n        * 'transition_features' - transition features of a CRF model;\n        * 'feature_importances' - feature importances of a decision tree or\n          an ensemble-based estimator;\n        * 'decision_tree' - decision tree in a graphical form;\n        * 'method' - a string with explanation method;\n        * 'description' - description of explanation method and its caveats.\n\n        ``eli5.formatters.fields`` provides constants that cover common cases:\n        ``INFO`` (method and description), ``WEIGHTS`` (all the rest),\n        and ``ALL`` (all).\n\n    horizontal_layout : bool\n        When True, feature weight tables are printed horizontally\n        (left to right); when False, feature weight tables are printed\n        vertically (top to down). Default is True.\n\n    highlight_spaces : bool or None, optional\n        Whether to highlight spaces in feature names. This is useful if\n        you work with text and have ngram features which may include spaces\n        at left or right. Default is None, meaning that the value used\n        is set automatically based on vectorizer and feature values.\n\n    include_styles : bool\n        Most styles are inline, but some are included separately in <style> tag;\n        you can omit them by passing ``include_styles=False``. Default is True.\n\n    **kwargs: dict\n        Keyword arguments. All keyword arguments are passed to\n        concrete explain_weights... implementations.\n\n    Returns\n    -------\n    IPython.display.HTML\n        The result is printed in IPython notebook as an HTML widget.\n        If you need to display several explanations as an output of a single\n        cell, or if you want to display it from a function then use\n        IPython.display.display::\n\n            from IPython.display import display\n            display(eli5.show_weights(clf1))\n            display(eli5.show_weights(clf2))\n\n    \"\"\"\n    format_kwargs, explain_kwargs = _split_kwargs(kwargs)\n    expl = explain_weights(estimator, **explain_kwargs)\n    html = format_as_html(expl, **format_kwargs)\n    return HTML(html)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef show_prediction(estimator, doc, **kwargs):\n    format_kwargs, explain_kwargs = _split_kwargs(kwargs)\n    expl = explain_prediction(estimator, doc, **explain_kwargs)\n    html = format_as_html(expl, **format_kwargs)\n    return HTML(html)", "response": "Return an explanation of estimator prediction as an IPython. display. HTML object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert DecisionTreeClassifier or DecisionTreeRegressor to an inspectable object.", "response": "def get_tree_info(decision_tree,\n                  feature_names=None,\n                  **export_graphviz_kwargs):\n    # type: (...) -> TreeInfo\n    \"\"\"\n    Convert DecisionTreeClassifier or DecisionTreeRegressor\n    to an inspectable object.\n    \"\"\"\n    return TreeInfo(\n        criterion=decision_tree.criterion,\n        tree=_get_root_node_info(decision_tree, feature_names),\n        graphviz=tree2dot(decision_tree,\n                          feature_names=feature_names,\n                          **export_graphviz_kwargs),\n        is_classification=isinstance(decision_tree, ClassifierMixin),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_samples(text,                # type: TokenizedText\n                     n_samples=500,       # type: int\n                     bow=True,            # type: bool\n                     random_state=None,\n                     replacement='',      # type: str\n                     min_replace=1,       # type: Union[int, float]\n                     max_replace=1.0,     # type: Union[int, float]\n                     group_size=1,        # type: int\n                     ):\n    # type: (...) -> Tuple[List[str], np.ndarray, np.ndarray]\n    \"\"\"\n    Return ``n_samples`` changed versions of text (with some words removed),\n    along with distances between the original text and a generated\n    examples. If ``bow=False``, all tokens are considered unique\n    (i.e. token position matters).\n    \"\"\"\n    kwargs = dict(\n        n_samples=n_samples,\n        replacement=replacement,\n        random_state=random_state,\n        min_replace=min_replace,\n        max_replace=max_replace,\n    )\n    if bow:\n        num_tokens = len(text.vocab)\n        res = text.replace_random_tokens_bow(**kwargs)\n    else:\n        num_tokens = len(text.tokens)\n        res = text.replace_random_tokens(group_size=group_size, **kwargs)\n\n    texts, num_removed_vec, masks = zip(*res)\n    similarity = cosine_similarity_vec(num_tokens, num_removed_vec)\n    return texts, similarity, vstack(masks)", "response": "Generates n_samples changed versions of text and a generated\n   ."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn cosine similarity between a binary vector with all ones of length num_tokens and vectors of the same length with num_removed_vec elements set to zero.", "response": "def cosine_similarity_vec(num_tokens, num_removed_vec):\n    \"\"\"\n    Return cosine similarity between a binary vector with all ones\n    of length ``num_tokens`` and vectors of the same length with\n    ``num_removed_vec`` elements set to zero.\n    \"\"\"\n    remaining = -np.array(num_removed_vec) + num_tokens\n    return remaining / (np.sqrt(num_tokens + 1e-6) * np.sqrt(remaining + 1e-6))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of tuples with n_samples versions of text with some words replaced with a random number of replacement words.", "response": "def replace_random_tokens(self,\n                              n_samples,  # type: int\n                              replacement='',  # type: str\n                              random_state=None,\n                              min_replace=1,  # type: Union[int, float]\n                              max_replace=1.0,  # type: Union[int, float]\n                              group_size=1  # type: int\n                              ):\n        # type: (...) -> List[Tuple[str, int, np.ndarray]]\n        \"\"\" \n        Return a list of ``(text, replaced_count, mask)``\n        tuples with n_samples versions of text with some words replaced.\n        By default words are replaced with '', i.e. removed.\n        \"\"\"\n        n_tokens = len(self.tokens)\n        indices = np.arange(n_tokens)\n        if not n_tokens:\n            nomask = np.array([], dtype=int)\n            return [('', 0, nomask)] * n_samples\n\n        min_replace, max_replace = self._get_min_max(min_replace, max_replace,\n                                                     n_tokens)\n        rng = check_random_state(random_state)\n        replace_sizes = rng.randint(low=min_replace, high=max_replace + 1,\n                                    size=n_samples)\n        res = []\n        for num_to_replace in replace_sizes:\n            idx_to_replace = rng.choice(indices, num_to_replace, replace=False)\n            idx_to_replace = np.array([idx_to_replace] + [\n                idx_to_replace + shift for shift in range(1, group_size)\n            ]).ravel()\n            padded_size = n_tokens + group_size - 1\n            mask = indices_to_bool_mask(idx_to_replace, padded_size)[:n_tokens]\n            s = self.split.masked(mask, replacement)\n            res.append((s.text, num_to_replace, mask))\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef replace_random_tokens_bow(self,\n                                  n_samples,  # type: int\n                                  replacement='',  # type: str\n                                  random_state=None,\n                                  min_replace=1,  # type: Union[int, float]\n                                  max_replace=1.0, # type: Union[int, float]\n                                  ):\n        # type: (...) -> List[Tuple[str, int, np.ndarray]]\n        \"\"\"\n        Return a list of ``(text, replaced_words_count, mask)`` tuples with\n        n_samples versions of text with some words replaced.\n        If a word is replaced, all duplicate words are also replaced\n        from the text. By default words are replaced with '', i.e. removed.\n        \"\"\"\n        if not self.vocab:\n            nomask = np.array([], dtype=int)\n            return [('', 0, nomask)] * n_samples\n\n        min_replace, max_replace = self._get_min_max(min_replace, max_replace,\n                                                     len(self.vocab))\n        rng = check_random_state(random_state)\n        replace_sizes = rng.randint(low=min_replace, high=max_replace + 1,\n                                    size=n_samples)\n        res = []\n        for num_to_replace in replace_sizes:\n            tokens_to_replace = set(rng.choice(self.vocab, num_to_replace,\n                                               replace=False))\n            idx_to_replace = [idx for idx, token in enumerate(self.tokens)\n                              if token in tokens_to_replace]\n            mask = indices_to_bool_mask(idx_to_replace, len(self.tokens))\n            s = self.split.masked(idx_to_replace, replacement)\n            res.append((s.text, num_to_replace, mask))\n        return res", "response": "Replaces random tokens in the vocabulary with some words replaced."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfits the local classification pipeline to get the explanation of the given doc.", "response": "def fit(self,\n            doc,             # type: str\n            predict_proba,   # type: Callable[[Any], Any]\n            ):\n        # type: (...) -> TextExplainer\n        \"\"\"\n        Explain ``predict_proba`` probabilistic classification function\n        for the ``doc`` example. This method fits a local classification\n        pipeline following LIME approach.\n\n        To get the explanation use :meth:`show_prediction`,\n        :meth:`show_weights`, :meth:`explain_prediction` or\n        :meth:`explain_weights`.\n\n        Parameters\n        ----------\n        doc : str\n            Text to explain\n        predict_proba : callable\n            Black-box classification pipeline. ``predict_proba``\n            should be a function which takes a list of strings (documents)\n            and return a matrix of shape ``(n_samples, n_classes)`` with\n            probability values - a row per document and a column per output\n            label.\n        \"\"\"\n        self.doc_ = doc\n\n        if self.position_dependent:\n            samples, sims, mask, text = self.sampler.sample_near_with_mask(\n                doc=doc,\n                n_samples=self.n_samples\n            )\n            self.vec_ = SingleDocumentVectorizer(\n                token_pattern=self.token_pattern\n            ).fit([doc])\n            X = ~mask\n        else:\n            self.vec_ = clone(self.vec).fit([doc])\n            samples, sims = self.sampler.sample_near(\n                doc=doc,\n                n_samples=self.n_samples\n            )\n            X = self.vec_.transform(samples)\n\n        if self.rbf_sigma is not None:\n            sims = rbf(1-sims, sigma=self.rbf_sigma)\n\n        self.samples_ = samples\n        self.similarity_ = sims\n        self.X_ = X\n        self.y_proba_ = predict_proba(samples)\n        self.clf_ = clone(self.clf)\n\n        self.metrics_ = _train_local_classifier(\n            estimator=self.clf_,\n            samples=X,\n            similarity=sims,\n            y_proba=self.y_proba_,\n            expand_factor=self.expand_factor,\n            random_state=self.rng_\n        )\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef show_prediction(self, **kwargs):\n        self._fix_target_names(kwargs)\n        return eli5.show_prediction(self.clf_, self.doc_, vec=self.vec_,\n                                    **kwargs)", "response": "Call eli5. show_prediction for the locally - fit\n        classification pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef show_weights(self, **kwargs):\n        self._fix_target_names(kwargs)\n        return eli5.show_weights(self.clf_, vec=self.vec_, **kwargs)", "response": "Call eli5. show_weights for the locally - fit\n        classification pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the classification weights for the locally - fit .", "response": "def explain_weights(self, **kwargs):\n        \"\"\"\n        Call :func:`eli5.show_weights` for the locally-fit\n        classification pipeline. Keyword arguments are passed\n        to :func:`eli5.show_weights`.\n\n        :func:`fit` must be called before using this method.\n        \"\"\"\n        self._fix_target_names(kwargs)\n        return eli5.explain_weights(self.clf_, vec=self.vec_, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfitting classifier clf to return probabilities close to y_proba.", "response": "def fit_proba(clf, X, y_proba, expand_factor=10, sample_weight=None,\n              shuffle=True, random_state=None,\n              **fit_params):\n    \"\"\"\n    Fit classifier ``clf`` to return probabilities close to ``y_proba``.\n\n    scikit-learn can't optimize cross-entropy directly if target\n    probability values are not indicator vectors. As a workaround this function\n    expands the dataset according to target probabilities.\n    Use expand_factor=None to turn it off\n    (e.g. if probability scores are 0/1 in a first place).\n    \"\"\"\n    X, y, sample_weight = expanded_X_y_sample_weights(X, y_proba,\n        expand_factor=expand_factor,\n        sample_weight=sample_weight,\n        shuffle=shuffle,\n        random_state=random_state,\n    )\n    fit_params = with_sample_weight(clf, sample_weight, fit_params)\n    clf.fit(X, y, **fit_params)\n    return clf"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn fit_params with added sample_weight argument.", "response": "def with_sample_weight(clf, sample_weight, fit_params):\n    \"\"\"\n    Return fit_params with added \"sample_weight\" argument.\n    Unlike `fit_params['sample_weight'] = sample_weight` it\n    handles a case where ``clf`` is a pipeline.\n    \"\"\"\n    param_name = _get_classifier_prefix(clf) + \"sample_weight\"\n    params = {param_name: sample_weight}\n    params.update(fit_params)\n    return params"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds missing columns to predict_proba result. When a multiclass classifier is fit on a dataset which only contains a subset of possible classes its predict_proba result only has columns corresponding to seen classes. This function adds missing columns.", "response": "def fix_multiclass_predict_proba(y_proba,          # type: np.ndarray\n                                 seen_classes,\n                                 complete_classes\n                                 ):\n    # type: (...) -> np.ndarray\n    \"\"\"\n    Add missing columns to predict_proba result.\n\n    When a multiclass classifier is fit on a dataset which only contains\n    a subset of possible classes its predict_proba result only has columns\n    corresponding to seen classes. This function adds missing columns.\n    \"\"\"\n    assert set(complete_classes) >= set(seen_classes)\n    y_proba_fixed = np.zeros(\n        shape=(y_proba.shape[0], len(complete_classes)),\n        dtype=y_proba.dtype,\n    )\n    class_mapping = np.searchsorted(complete_classes, seen_classes)\n    y_proba_fixed[:, class_mapping] = y_proba\n    return y_proba_fixed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexpand the dataset X and y according to the target probabilities y_proba.", "response": "def expanded_X_y_sample_weights(X, y_proba, expand_factor=10,\n                                sample_weight=None, shuffle=True,\n                                random_state=None):\n    \"\"\"\n    scikit-learn can't optimize cross-entropy directly if target\n    probability values are not indicator vectors.\n    As a workaround this function expands the dataset according to\n    target probabilities. ``expand_factor=None`` means no dataset\n    expansion.\n    \"\"\"\n    rng = check_random_state(random_state)\n    if expand_factor:\n        if sample_weight is not None:\n            X, y, sample_weight = zip(*expand_dataset(X, y_proba,\n                                                      factor=expand_factor,\n                                                      random_state=rng,\n                                                      extra_arrays=[\n                                                          sample_weight\n                                                      ]))\n        else:\n            X, y = zip(*expand_dataset(X, y_proba,\n                                       factor=expand_factor,\n                                       random_state=rng))\n    else:\n        y = y_proba.argmax(axis=1)\n\n    if isinstance(X, (list, tuple)) and len(X) and issparse(X[0]):\n        X = vstack(X)\n\n    if shuffle:\n        if sample_weight is not None:\n            X, y, sample_weight = _shuffle(X, y, sample_weight,\n                                           random_state=rng)\n        else:\n            X, y = _shuffle(X, y, random_state=rng)\n    return X, y, sample_weight"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a dataset with float multiclass probabilities to a dataset with indicator probabilities by duplicating X rows and sampling true labels.", "response": "def expand_dataset(X, y_proba, factor=10, random_state=None, extra_arrays=None):\n    \"\"\"\n    Convert a dataset with float multiclass probabilities to a dataset\n    with indicator probabilities by duplicating X rows and sampling\n    true labels.\n    \"\"\"\n    rng = check_random_state(random_state)\n    extra_arrays = extra_arrays or []\n    n_classes = y_proba.shape[1]\n    classes = np.arange(n_classes, dtype=int)\n    for el in zip(X, y_proba, *extra_arrays):\n        x, probs = el[0:2]\n        rest = el[2:]\n        for label in rng.choice(classes, size=factor, p=probs):\n            yield (x, label) + rest"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets feature names for a given transformer.", "response": "def transform_feature_names(transformer, in_names=None):\n    \"\"\"Get feature names for transformer output as a function of input names.\n\n    Used by :func:`explain_weights` when applied to a scikit-learn Pipeline,\n    this ``singledispatch`` should be registered with custom name\n    transformations for each class of transformer.\n    \n    If there is no ``singledispatch`` handler registered for a transformer \n    class, ``transformer.get_feature_names()`` method is called; if there is\n    no such method then feature names are not supported and \n    this function raises an exception.\n\n    Parameters\n    ----------\n    transformer : scikit-learn-compatible transformer\n    in_names : list of str, optional\n        Names for features input to transformer.transform().\n        If not provided, the implementation may generate default feature names\n        if the number of input features is known.\n\n    Returns\n    -------\n    feature_names : list of str\n    \"\"\"\n    if hasattr(transformer, 'get_feature_names'):\n        return transformer.get_feature_names()\n    raise NotImplementedError('transform_feature_names not available for '\n                              '{}'.format(transformer))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_weighted_spans(doc, vec, feature_weights):\n    # type: (Any, Any, FeatureWeights) -> Optional[WeightedSpans]\n    \"\"\" If possible, return a dict with preprocessed document and a list\n    of spans with weights, corresponding to features in the document.\n    \"\"\"\n    if isinstance(vec, FeatureUnion):\n        return _get_weighted_spans_from_union(doc, vec, feature_weights)\n    else:\n        result = _get_doc_weighted_spans(doc, vec, feature_weights)\n        if result is not None:\n            found_features, doc_weighted_spans = result\n            return WeightedSpans(\n                [doc_weighted_spans],\n                other=_get_other(feature_weights, [('', found_features)]),\n            )\n    return None", "response": "Returns a dict with preprocessed document and a list of weighted spans corresponding to features in the document."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute and set target_expl. weighted_spans attribute when possible.", "response": "def add_weighted_spans(doc, vec, vectorized, target_expl):\n    # type: (Any, Any, bool, TargetExplanation) -> None\n    \"\"\"\n    Compute and set ``target_expl.weighted_spans`` attribute, when possible.\n    \"\"\"\n    if vec is None or vectorized:\n        return\n\n    weighted_spans = get_weighted_spans(doc, vec, target_expl.feature_weights)\n    if weighted_spans:\n        target_expl.weighted_spans = weighted_spans"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_feature_weights_dict(feature_weights,  # type: FeatureWeights\n                              feature_fn        # type: Optional[Callable[[str], str]]\n                              ):\n    # type: (...) -> Dict[str, Tuple[float, Tuple[str, int]]]\n    \"\"\" Return {feat_name: (weight, (group, idx))} mapping. \"\"\"\n    return {\n        # (group, idx) is an unique feature identifier, e.g. ('pos', 2)\n        feat_name: (fw.weight, (group, idx))\n        for group in ['pos', 'neg']\n        for idx, fw in enumerate(getattr(feature_weights, group))\n        for feat_name in _get_features(fw.feature, feature_fn)\n    }", "response": "Return a dictionary mapping each feature name to its weight."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fit(self, X, y, groups=None, **fit_params):\n        # type: (...) -> PermutationImportance\n        \"\"\"Compute ``feature_importances_`` attribute and optionally\n        fit the base estimator.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The training input samples.\n\n        y : array-like, shape (n_samples,)\n            The target values (integers that correspond to classes in\n            classification, real numbers in regression).\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        **fit_params : Other estimator specific parameters\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n        self.scorer_ = check_scoring(self.estimator, scoring=self.scoring)\n\n        if pandas_available and isinstance(X, pd.DataFrame):\n            self.scorer_ = self._wrap_scorer(self.scorer_, X.columns)\n\n        if self.cv != \"prefit\" and self.refit:\n            self.estimator_ = clone(self.estimator)\n            self.estimator_.fit(X, y, **fit_params)\n\n        X = check_array(X)\n\n        if self.cv not in (None, \"prefit\"):\n            si = self._cv_scores_importances(X, y, groups=groups, **fit_params)\n        else:\n            si = self._non_cv_scores_importances(X, y)\n        scores, results = si\n        self.scores_ = np.array(scores)\n        self.results_ = results\n        self.feature_importances_ = np.mean(results, axis=0)\n        self.feature_importances_std_ = np.std(results, axis=0)\n        return self", "response": "Fits the base estimator and returns the PermutationImportance object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an explanation of an estimator prediction.", "response": "def explain_prediction(estimator, doc, **kwargs):\n    \"\"\"\n    Return an explanation of an estimator prediction.\n\n    :func:`explain_prediction` is not doing any work itself, it dispatches\n    to a concrete implementation based on estimator type.\n\n    Parameters\n    ----------\n    estimator : object\n        Estimator instance. This argument must be positional.\n\n    doc : object\n        Example to run estimator on. Estimator makes a prediction for this\n        example, and :func:`explain_prediction` tries to show information\n        about this prediction. Pass a single element, not a one-element array:\n        if you fitted your estimator on ``X``, that would be ``X[i]`` for\n        most containers, and ``X.iloc[i]`` for ``pandas.DataFrame``.\n\n    top : int or (int, int) tuple, optional\n        Number of features to show. When ``top`` is int, ``top`` features with\n        a highest absolute values are shown. When it is (pos, neg) tuple,\n        no more than ``pos`` positive features and no more than ``neg``\n        negative features is shown. ``None`` value means no limit (default).\n\n        This argument may be supported or not, depending on estimator type.\n\n    top_targets : int, optional\n        Number of targets to show. When ``top_targets`` is provided,\n        only specified number of targets with highest scores are shown.\n        Negative value means targets with lowest scores are shown.\n        Must not be given with ``targets`` argument.\n        ``None`` value means no limit: all targets are shown (default).\n\n        This argument may be supported or not, depending on estimator type.\n\n    target_names : list[str] or {'old_name': 'new_name'} dict, optional\n        Names of targets or classes. This argument can be used to provide\n        human-readable class/target names for estimators which don't expose\n        clss names themselves. It can be also used to rename estimator-provided\n        classes before displaying them.\n\n        This argument may be supported or not, depending on estimator type.\n\n    targets : list, optional\n        Order of class/target names to show. This argument can be also used\n        to show information only for a subset of classes. It should be a list\n        of class / target names which match either names provided by\n        an estimator or names defined in ``target_names`` parameter.\n        Must not be given with ``top_targets`` argument.\n\n        In case of binary classification you can use this argument to\n        set the class which probability or score should be displayed, with\n        an appropriate explanation. By default a result for predicted class\n        is shown. For example, you can use ``targets=[True]`` to always show\n        result for a positive class, even if the predicted label is False.\n\n        This argument may be supported or not, depending on estimator type.\n\n    feature_names : list, optional\n        A list of feature names. It allows to specify feature\n        names when they are not provided by an estimator object.\n\n        This argument may be supported or not, depending on estimator type.\n\n    feature_re : str, optional\n        Only feature names which match ``feature_re`` regex are returned\n        (more precisely, ``re.search(feature_re, x)`` is checked).\n\n    feature_filter : Callable[[str, float], bool], optional\n        Only feature names for which ``feature_filter`` function returns True\n        are returned. It must accept feature name and feature value.\n        Missing features always have a NaN value.\n\n    **kwargs: dict\n        Keyword arguments. All keyword arguments are passed to\n        concrete explain_prediction... implementations.\n\n    Returns\n    -------\n    Explanation\n        :class:`~.Explanation` result. Use one of the formatting functions from\n        :mod:`eli5.formatters` to print it in a human-readable form.\n\n        Explanation instances have repr which works well with\n        IPython notebook, but it can be a better idea to use\n        :func:`eli5.show_prediction` instead of :func:`eli5.explain_prediction`\n        if you work with IPython: :func:`eli5.show_prediction` allows to\n        customize formatting without a need to import :mod:`eli5.formatters`\n        functions.\n    \"\"\"\n    return Explanation(\n        estimator=repr(estimator),\n        error=\"estimator %r is not supported\" % estimator,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the directions between two locations.", "response": "def directions(client, origin, destination,\n               mode=None, waypoints=None, alternatives=False, avoid=None,\n               language=None, units=None, region=None, departure_time=None,\n               arrival_time=None, optimize_waypoints=False, transit_mode=None,\n               transit_routing_preference=None, traffic_model=None):\n    \"\"\"Get directions between an origin point and a destination point.\n\n    :param origin: The address or latitude/longitude value from which you wish\n        to calculate directions.\n    :type origin: string, dict, list, or tuple\n\n    :param destination: The address or latitude/longitude value from which\n        you wish to calculate directions. You can use a place_id as destination\n        by putting 'place_id:' as a preffix in the passing parameter.\n    :type destination: string, dict, list, or tuple\n\n    :param mode: Specifies the mode of transport to use when calculating\n        directions. One of \"driving\", \"walking\", \"bicycling\" or \"transit\"\n    :type mode: string\n\n    :param waypoints: Specifies an array of waypoints. Waypoints alter a\n        route by routing it through the specified location(s).\n    :type waypoints: a single location, or a list of locations, where a\n        location is a string, dict, list, or tuple\n\n    :param alternatives: If True, more than one route may be returned in the\n        response.\n    :type alternatives: bool\n\n    :param avoid: Indicates that the calculated route(s) should avoid the\n        indicated features.\n    :type avoid: list or string\n\n    :param language: The language in which to return results.\n    :type language: string\n\n    :param units: Specifies the unit system to use when displaying results.\n        \"metric\" or \"imperial\"\n    :type units: string\n\n    :param region: The region code, specified as a ccTLD (\"top-level domain\"\n        two-character value.\n    :type region: string\n\n    :param departure_time: Specifies the desired time of departure.\n    :type departure_time: int or datetime.datetime\n\n    :param arrival_time: Specifies the desired time of arrival for transit\n        directions. Note: you can't specify both departure_time and\n        arrival_time.\n    :type arrival_time: int or datetime.datetime\n\n    :param optimize_waypoints: Optimize the provided route by rearranging the\n        waypoints in a more efficient order.\n    :type optimize_waypoints: bool\n\n    :param transit_mode: Specifies one or more preferred modes of transit.\n        This parameter may only be specified for requests where the mode is\n        transit. Valid values are \"bus\", \"subway\", \"train\", \"tram\", \"rail\".\n        \"rail\" is equivalent to [\"train\", \"tram\", \"subway\"].\n    :type transit_mode: string or list of strings\n\n    :param transit_routing_preference: Specifies preferences for transit\n        requests. Valid values are \"less_walking\" or \"fewer_transfers\"\n    :type transit_routing_preference: string\n\n    :param traffic_model: Specifies the predictive travel time model to use.\n        Valid values are \"best_guess\" or \"optimistic\" or \"pessimistic\".\n        The traffic_model parameter may only be specified for requests where\n        the travel mode is driving, and where the request includes a\n        departure_time.\n    :type units: string\n\n    :rtype: list of routes\n    \"\"\"\n\n    params = {\n        \"origin\": convert.latlng(origin),\n        \"destination\": convert.latlng(destination)\n    }\n\n    if mode:\n        # NOTE(broady): the mode parameter is not validated by the Maps API\n        # server. Check here to prevent silent failures.\n        if mode not in [\"driving\", \"walking\", \"bicycling\", \"transit\"]:\n            raise ValueError(\"Invalid travel mode.\")\n        params[\"mode\"] = mode\n\n    if waypoints:\n        waypoints = convert.location_list(waypoints)\n        if optimize_waypoints:\n            waypoints = \"optimize:true|\" + waypoints\n        params[\"waypoints\"] = waypoints\n\n    if alternatives:\n        params[\"alternatives\"] = \"true\"\n\n    if avoid:\n        params[\"avoid\"] = convert.join_list(\"|\", avoid)\n\n    if language:\n        params[\"language\"] = language\n\n    if units:\n        params[\"units\"] = units\n\n    if region:\n        params[\"region\"] = region\n\n    if departure_time:\n        params[\"departure_time\"] = convert.time(departure_time)\n\n    if arrival_time:\n        params[\"arrival_time\"] = convert.time(arrival_time)\n\n    if departure_time and arrival_time:\n        raise ValueError(\"Should not specify both departure_time and\"\n                         \"arrival_time.\")\n\n    if transit_mode:\n        params[\"transit_mode\"] = convert.join_list(\"|\", transit_mode)\n\n    if transit_routing_preference:\n        params[\"transit_routing_preference\"] = transit_routing_preference\n\n    if traffic_model:\n        params[\"traffic_model\"] = traffic_model\n\n    return client._request(\"/maps/api/directions/json\", params).get(\"routes\", [])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reverse_geocode(client, latlng, result_type=None, location_type=None,\n                    language=None):\n    \"\"\"\n    Reverse geocoding is the process of converting geographic coordinates into a\n    human-readable address.\n\n    :param latlng: The latitude/longitude value or place_id for which you wish\n        to obtain the closest, human-readable address.\n    :type latlng: string, dict, list, or tuple\n\n    :param result_type: One or more address types to restrict results to.\n    :type result_type: string or list of strings\n\n    :param location_type: One or more location types to restrict results to.\n    :type location_type: list of strings\n\n    :param language: The language in which to return results.\n    :type language: string\n\n    :rtype: list of reverse geocoding results.\n    \"\"\"\n\n    # Check if latlng param is a place_id string.\n    #  place_id strings do not contain commas; latlng strings do.\n    if convert.is_string(latlng) and ',' not in latlng:\n        params = {\"place_id\": latlng}\n    else:\n        params = {\"latlng\": convert.latlng(latlng)}\n\n    if result_type:\n        params[\"result_type\"] = convert.join_list(\"|\", result_type)\n\n    if location_type:\n        params[\"location_type\"] = convert.join_list(\"|\", location_type)\n\n    if language:\n        params[\"language\"] = language\n\n    return client._request(\"/maps/api/geocode/json\", params).get(\"results\", [])", "response": "Reverse geocoding for a given location."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nproviding elevation data for a single location in the order they appear.", "response": "def elevation(client, locations):\n    \"\"\"\n    Provides elevation data for locations provided on the surface of the\n    earth, including depth locations on the ocean floor (which return negative\n    values)\n\n    :param locations: List of latitude/longitude values from which you wish\n        to calculate elevation data.\n    :type locations: a single location, or a list of locations, where a\n        location is a string, dict, list, or tuple\n\n    :rtype: list of elevation data responses\n    \"\"\"\n    params = {\"locations\": convert.shortest_path(locations)}\n    return client._request(\"/maps/api/elevation/json\", params).get(\"results\", [])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprovide elevation data sampled along a path on the surface of the earth.", "response": "def elevation_along_path(client, path, samples):\n    \"\"\"\n    Provides elevation data sampled along a path on the surface of the earth.\n\n    :param path: An encoded polyline string, or a list of latitude/longitude\n        values from which you wish to calculate elevation data.\n    :type path: string, dict, list, or tuple\n\n    :param samples: The number of sample points along a path for which to\n        return elevation data.\n    :type samples: int\n\n    :rtype: list of elevation data responses\n    \"\"\"\n\n    if type(path) is str:\n        path = \"enc:%s\" % path\n    else:\n        path = convert.shortest_path(path)\n\n    params = {\n        \"path\": path,\n        \"samples\": samples\n    }\n\n    return client._request(\"/maps/api/elevation/json\", params).get(\"results\", [])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a lat / lon pair to a comma - separated string.", "response": "def latlng(arg):\n    \"\"\"Converts a lat/lon pair to a comma-separated string.\n\n    For example:\n\n    sydney = {\n        \"lat\" : -33.8674869,\n        \"lng\" : 151.2069902\n    }\n\n    convert.latlng(sydney)\n    # '-33.8674869,151.2069902'\n\n    For convenience, also accepts lat/lon pair as a string, in\n    which case it's returned unchanged.\n\n    :param arg: The lat/lon pair.\n    :type arg: string or dict or list or tuple\n    \"\"\"\n    if is_string(arg):\n        return arg\n\n    normalized = normalize_lat_lng(arg)\n    return \"%s,%s\" % (format_float(normalized[0]), format_float(normalized[1]))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef normalize_lat_lng(arg):\n    if isinstance(arg, dict):\n        if \"lat\" in arg and \"lng\" in arg:\n            return arg[\"lat\"], arg[\"lng\"]\n        if \"latitude\" in arg and \"longitude\" in arg:\n            return arg[\"latitude\"], arg[\"longitude\"]\n\n    # List or tuple.\n    if _is_list(arg):\n        return arg[0], arg[1]\n\n    raise TypeError(\n        \"Expected a lat/lng dict or tuple, \"\n        \"but got %s\" % type(arg).__name__)", "response": "Normalizes the various lat and lng representations and return a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef location_list(arg):\n    if isinstance(arg, tuple):\n        # Handle the single-tuple lat/lng case.\n        return latlng(arg)\n    else:\n        return \"|\".join([latlng(location) for location in as_list(arg)])", "response": "Joins a list of locations into a pipe separated string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _is_list(arg):\n    if isinstance(arg, dict):\n        return False\n    if isinstance(arg, str): # Python 3-only, as str has __iter__\n        return False\n    return (not _has_method(arg, \"strip\")\n            and _has_method(arg, \"__getitem__\")\n            or _has_method(arg, \"__iter__\"))", "response": "Checks if arg is list - like. This excludes strings and dicts."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_string(val):\n    try:\n        basestring\n    except NameError:\n        return isinstance(val, str)\n    return isinstance(val, basestring)", "response": "Determines whether the passed value is a string safe for 2 or 3."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting the value into a unix time.", "response": "def time(arg):\n    \"\"\"Converts the value into a unix time (seconds since unix epoch).\n\n    For example:\n        convert.time(datetime.now())\n        # '1409810596'\n\n    :param arg: The time.\n    :type arg: datetime.datetime or int\n    \"\"\"\n    # handle datetime instances.\n    if _has_method(arg, \"timetuple\"):\n        arg = _time.mktime(arg.timetuple())\n\n    if isinstance(arg, float):\n        arg = int(arg)\n\n    return str(arg)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn true if the given object has a method with the given name.", "response": "def _has_method(arg, method):\n    \"\"\"Returns true if the given object has a method with the given name.\n\n    :param arg: the object\n\n    :param method: the method name\n    :type method: string\n\n    :rtype: bool\n    \"\"\"\n    return hasattr(arg, method) and callable(getattr(arg, method))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef components(arg):\n\n    # Components may have multiple values per type, here we\n    # expand them into individual key/value items, eg:\n    # {\"country\": [\"US\", \"AU\"], \"foo\": 1} -> \"country:AU\", \"country:US\", \"foo:1\"\n    def expand(arg):\n        for k, v in arg.items():\n            for item in as_list(v):\n                yield \"%s:%s\" % (k, item)\n\n    if isinstance(arg, dict):\n        return \"|\".join(sorted(expand(arg)))\n\n    raise TypeError(\n        \"Expected a dict for components, \"\n        \"but got %s\" % type(arg).__name__)", "response": "Converts a dict of components to the format expected by the Google Maps\n    server."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a lat - lng bounds dict to a comma - separated string.", "response": "def bounds(arg):\n    \"\"\"Converts a lat/lon bounds to a comma- and pipe-separated string.\n\n    Accepts two representations:\n    1) string: pipe-separated pair of comma-separated lat/lon pairs.\n    2) dict with two entries - \"southwest\" and \"northeast\". See convert.latlng\n    for information on how these can be represented.\n\n    For example:\n\n    sydney_bounds = {\n        \"northeast\" : {\n            \"lat\" : -33.4245981,\n            \"lng\" : 151.3426361\n        },\n        \"southwest\" : {\n            \"lat\" : -34.1692489,\n            \"lng\" : 150.502229\n        }\n    }\n\n    convert.bounds(sydney_bounds)\n    # '-34.169249,150.502229|-33.424598,151.342636'\n\n    :param arg: The bounds.\n    :type arg: dict\n    \"\"\"\n\n    if is_string(arg) and arg.count(\"|\") == 1 and arg.count(\",\") == 2:\n        return arg\n    elif isinstance(arg, dict):\n        if \"southwest\" in arg and \"northeast\" in arg:\n            return \"%s|%s\" % (latlng(arg[\"southwest\"]),\n                              latlng(arg[\"northeast\"]))\n\n    raise TypeError(\n        \"Expected a bounds (southwest/northeast) dict, \"\n        \"but got %s\" % type(arg).__name__)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decode_polyline(polyline):\n    points = []\n    index = lat = lng = 0\n\n    while index < len(polyline):\n        result = 1\n        shift = 0\n        while True:\n            b = ord(polyline[index]) - 63 - 1\n            index += 1\n            result += b << shift\n            shift += 5\n            if b < 0x1f:\n                break\n        lat += (~result >> 1) if (result & 1) != 0 else (result >> 1)\n\n        result = 1\n        shift = 0\n        while True:\n            b = ord(polyline[index]) - 63 - 1\n            index += 1\n            result += b << shift\n            shift += 5\n            if b < 0x1f:\n                break\n        lng += ~(result >> 1) if (result & 1) != 0 else (result >> 1)\n\n        points.append({\"lat\": lat * 1e-5, \"lng\": lng * 1e-5})\n\n    return points", "response": "Decodes a Polyline string into a list of lat / lng dicts."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nencodes a list of points into a polyline string.", "response": "def encode_polyline(points):\n    \"\"\"Encodes a list of points into a polyline string.\n\n    See the developer docs for a detailed description of this encoding:\n    https://developers.google.com/maps/documentation/utilities/polylinealgorithm\n\n    :param points: a list of lat/lng pairs\n    :type points: list of dicts or tuples\n\n    :rtype: string\n    \"\"\"\n    last_lat = last_lng = 0\n    result = \"\"\n\n    for point in points:\n        ll = normalize_lat_lng(point)\n        lat = int(round(ll[0] * 1e5))\n        lng = int(round(ll[1] * 1e5))\n        d_lat = lat - last_lat\n        d_lng = lng - last_lng\n\n        for v in [d_lat, d_lng]:\n            v = ~(v << 1) if v < 0 else v << 1\n            while v >= 0x20:\n                result += (chr((0x20 | (v & 0x1f)) + 63))\n                v >>= 5\n            result += (chr(v + 63))\n\n        last_lat = lat\n        last_lng = lng\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget time zone for a location on the earth and optionally a timestamp.", "response": "def timezone(client, location, timestamp=None, language=None):\n    \"\"\"Get time zone for a location on the earth, as well as that location's\n    time offset from UTC.\n\n    :param location: The latitude/longitude value representing the location to\n        look up.\n    :type location: string, dict, list, or tuple\n\n    :param timestamp: Timestamp specifies the desired time as seconds since\n        midnight, January 1, 1970 UTC. The Time Zone API uses the timestamp to\n        determine whether or not Daylight Savings should be applied. Times\n        before 1970 can be expressed as negative values. Optional. Defaults to\n        ``datetime.utcnow()``.\n    :type timestamp: int or datetime.datetime\n\n    :param language: The language in which to return results.\n    :type language: string\n\n    :rtype: dict\n    \"\"\"\n\n    params = {\n        \"location\": convert.latlng(location),\n        \"timestamp\": convert.time(timestamp or datetime.utcnow())\n    }\n\n    if language:\n        params[\"language\"] = language\n\n    return client._request( \"/maps/api/timezone/json\", params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsnapping a path to the most likely roads travelled along a route.", "response": "def snap_to_roads(client, path, interpolate=False):\n    \"\"\"Snaps a path to the most likely roads travelled.\n\n    Takes up to 100 GPS points collected along a route, and returns a similar\n    set of data with the points snapped to the most likely roads the vehicle\n    was traveling along.\n\n    :param path: The path to be snapped.\n    :type path: a single location, or a list of locations, where a\n        location is a string, dict, list, or tuple\n\n    :param interpolate: Whether to interpolate a path to include all points\n        forming the full road-geometry. When true, additional interpolated\n        points will also be returned, resulting in a path that smoothly follows\n        the geometry of the road, even around corners and through tunnels.\n        Interpolated paths may contain more points than the original path.\n    :type interpolate: bool\n\n    :rtype: A list of snapped points.\n    \"\"\"\n\n    params = {\"path\": convert.location_list(path)}\n\n    if interpolate:\n        params[\"interpolate\"] = \"true\"\n\n    return client._request(\"/v1/snapToRoads\", params,\n                       base_url=_ROADS_BASE_URL,\n                       accepts_clientid=False,\n                       extract_body=_roads_extract).get(\"snappedPoints\", [])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind the closest road segments for each point.", "response": "def nearest_roads(client, points):\n    \"\"\"Find the closest road segments for each point\n\n    Takes up to 100 independent coordinates, and returns the closest road\n    segment for each point. The points passed do not need to be part of a\n    continuous path.\n\n    :param points: The points for which the nearest road segments are to be\n        located.\n    :type points: a single location, or a list of locations, where a\n        location is a string, dict, list, or tuple\n\n    :rtype: A list of snapped points.\n    \"\"\"\n\n    params = {\"points\": convert.location_list(points)}\n\n    return client._request(\"/v1/nearestRoads\", params,\n                       base_url=_ROADS_BASE_URL,\n                       accepts_clientid=False,\n                       extract_body=_roads_extract).get(\"snappedPoints\", [])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef speed_limits(client, place_ids):\n\n    params = [(\"placeId\", place_id) for place_id in convert.as_list(place_ids)]\n\n    return client._request(\"/v1/speedLimits\", params,\n                       base_url=_ROADS_BASE_URL,\n                       accepts_clientid=False,\n                       extract_body=_roads_extract).get(\"speedLimits\", [])", "response": "Returns the posted speed limit for given ROAD segments."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the posted speed limit for the given path.", "response": "def snapped_speed_limits(client, path):\n    \"\"\"Returns the posted speed limit (in km/h) for given road segments.\n\n    The provided points will first be snapped to the most likely roads the\n    vehicle was traveling along.\n\n    :param path: The path of points to be snapped.\n    :type path: a single location, or a list of locations, where a\n        location is a string, dict, list, or tuple\n\n    :rtype: dict with a list of speed limits and a list of the snapped points.\n    \"\"\"\n\n    params = {\"path\": convert.location_list(path)}\n\n    return client._request(\"/v1/speedLimits\", params,\n                       base_url=_ROADS_BASE_URL,\n                       accepts_clientid=False,\n                       extract_body=_roads_extract)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting a result from a Roads API HTTP response.", "response": "def _roads_extract(resp):\n    \"\"\"Extracts a result from a Roads API HTTP response.\"\"\"\n\n    try:\n        j = resp.json()\n    except:\n        if resp.status_code != 200:\n            raise googlemaps.exceptions.HTTPError(resp.status_code)\n\n        raise googlemaps.exceptions.ApiError(\"UNKNOWN_ERROR\",\n                                             \"Received a malformed response.\")\n\n    if \"error\" in j:\n        error = j[\"error\"]\n        status = error[\"status\"]\n\n        if status == \"RESOURCE_EXHAUSTED\":\n            raise googlemaps.exceptions._OverQueryLimit(status,\n                                                        error.get(\"message\"))\n\n        raise googlemaps.exceptions.ApiError(status, error.get(\"message\"))\n\n    if resp.status_code != 200:\n        raise googlemaps.exceptions.HTTPError(resp.status_code)\n\n    return j"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef places(client, query, location=None, radius=None, language=None,\n           min_price=None, max_price=None, open_now=False, type=None, region=None,\n           page_token=None):\n    \"\"\"\n    Places search.\n\n    :param query: The text string on which to search, for example: \"restaurant\".\n    :type query: string\n\n    :param location: The latitude/longitude value for which you wish to obtain the\n        closest, human-readable address.\n    :type location: string, dict, list, or tuple\n\n    :param radius: Distance in meters within which to bias results.\n    :type radius: int\n\n    :param language: The language in which to return results.\n    :type language: string\n\n    :param min_price: Restricts results to only those places with no less than\n        this price level. Valid values are in the range from 0 (most affordable)\n        to 4 (most expensive).\n    :type min_price: int\n\n    :param max_price: Restricts results to only those places with no greater\n        than this price level. Valid values are in the range from 0 (most\n        affordable) to 4 (most expensive).\n    :type max_price: int\n\n    :param open_now: Return only those places that are open for business at\n        the time the query is sent.\n    :type open_now: bool\n\n    :param type: Restricts the results to places matching the specified type.\n        The full list of supported types is available here:\n        https://developers.google.com/places/supported_types\n    :type type: string\n\n    :param region: The region code, optional parameter.\n        See more @ https://developers.google.com/places/web-service/search\n    :type region: string\n\n    :param page_token: Token from a previous search that when provided will\n        returns the next page of results for the same search.\n    :type page_token: string\n\n    :rtype: result dict with the following keys:\n        results: list of places\n        html_attributions: set of attributions which must be displayed\n        next_page_token: token for retrieving the next page of results\n    \"\"\"\n    return _places(client, \"text\", query=query, location=location,\n                   radius=radius, language=language, min_price=min_price,\n                   max_price=max_price, open_now=open_now, type=type, region=region,\n                   page_token=page_token)", "response": "Search for places in a location."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming nearby search for places. :param location: The latitude/longitude value for which you wish to obtain the closest, human-readable address. :type location: string, dict, list, or tuple :param radius: Distance in meters within which to bias results. :type radius: int :param region: The region code, optional parameter. See more @ https://developers.google.com/places/web-service/search :type region: string :param keyword: A term to be matched against all content that Google has indexed for this place. :type keyword: string :param language: The language in which to return results. :type language: string :param min_price: Restricts results to only those places with no less than this price level. Valid values are in the range from 0 (most affordable) to 4 (most expensive). :type min_price: int :param max_price: Restricts results to only those places with no greater than this price level. Valid values are in the range from 0 (most affordable) to 4 (most expensive). :type max_price: int :param name: One or more terms to be matched against the names of places. :type name: string or list of strings :param open_now: Return only those places that are open for business at the time the query is sent. :type open_now: bool :param rank_by: Specifies the order in which results are listed. Possible values are: prominence (default), distance :type rank_by: string :param type: Restricts the results to places matching the specified type. The full list of supported types is available here: https://developers.google.com/places/supported_types :type type: string :param page_token: Token from a previous search that when provided will returns the next page of results for the same search. :type page_token: string :rtype: result dict with the following keys: status: status code results: list of places html_attributions: set of attributions which must be displayed next_page_token: token for retrieving the next page of results", "response": "def places_nearby(client, location=None, radius=None, keyword=None,\n                  language=None, min_price=None, max_price=None, name=None,\n                  open_now=False, rank_by=None, type=None, page_token=None):\n    \"\"\"\n    Performs nearby search for places.\n\n    :param location: The latitude/longitude value for which you wish to obtain the\n                     closest, human-readable address.\n    :type location: string, dict, list, or tuple\n\n    :param radius: Distance in meters within which to bias results.\n    :type radius: int\n\n    :param region: The region code, optional parameter.\n        See more @ https://developers.google.com/places/web-service/search\n    :type region: string\n\n    :param keyword: A term to be matched against all content that Google has\n                    indexed for this place.\n    :type keyword: string\n\n    :param language: The language in which to return results.\n    :type language: string\n\n    :param min_price: Restricts results to only those places with no less than\n                      this price level. Valid values are in the range from 0\n                      (most affordable) to 4 (most expensive).\n    :type min_price: int\n\n    :param max_price: Restricts results to only those places with no greater\n                      than this price level. Valid values are in the range\n                      from 0 (most affordable) to 4 (most expensive).\n    :type max_price: int\n\n    :param name: One or more terms to be matched against the names of places.\n    :type name: string or list of strings\n\n    :param open_now: Return only those places that are open for business at\n                     the time the query is sent.\n    :type open_now: bool\n\n    :param rank_by: Specifies the order in which results are listed.\n                    Possible values are: prominence (default), distance\n    :type rank_by: string\n\n    :param type: Restricts the results to places matching the specified type.\n        The full list of supported types is available here:\n        https://developers.google.com/places/supported_types\n    :type type: string\n\n    :param page_token: Token from a previous search that when provided will\n                       returns the next page of results for the same search.\n    :type page_token: string\n\n    :rtype: result dict with the following keys:\n            status: status code\n            results: list of places\n            html_attributions: set of attributions which must be displayed\n            next_page_token: token for retrieving the next page of results\n\n    \"\"\"\n    if not location and not page_token:\n        raise ValueError(\"either a location or page_token arg is required\")\n    if rank_by == \"distance\":\n        if not (keyword or name or type):\n            raise ValueError(\"either a keyword, name, or type arg is required \"\n                             \"when rank_by is set to distance\")\n        elif radius is not None:\n            raise ValueError(\"radius cannot be specified when rank_by is set to \"\n                             \"distance\")\n\n    return _places(client, \"nearby\", location=location, radius=radius,\n                   keyword=keyword, language=language, min_price=min_price,\n                   max_price=max_price, name=name, open_now=open_now,\n                   rank_by=rank_by, type=type, page_token=page_token)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef places_radar(client, location, radius, keyword=None, min_price=None,\n                 max_price=None, name=None, open_now=False, type=None):\n    \"\"\"\n    Performs radar search for places.\n\n    :param location: The latitude/longitude value for which you wish to obtain the\n                     closest, human-readable address.\n    :type location: string, dict, list, or tuple\n\n    :param radius: Distance in meters within which to bias results.\n    :type radius: int\n\n    :param keyword: A term to be matched against all content that Google has\n                    indexed for this place.\n    :type keyword: string\n\n    :param min_price: Restricts results to only those places with no less than\n                      this price level. Valid values are in the range from 0\n                      (most affordable) to 4 (most expensive).\n    :type min_price: int\n\n    :param max_price: Restricts results to only those places with no greater\n                      than this price level. Valid values are in the range\n                      from 0 (most affordable) to 4 (most expensive).\n    :type max_price: int\n\n    :param name: One or more terms to be matched against the names of places.\n    :type name: string or list of strings\n\n    :param open_now: Return only those places that are open for business at\n                     the time the query is sent.\n    :type open_now: bool\n\n    :param type: Restricts the results to places matching the specified type.\n        The full list of supported types is available here:\n        https://developers.google.com/places/supported_types\n    :type type: string\n\n    :rtype: result dict with the following keys:\n            status: status code\n            results: list of places\n            html_attributions: set of attributions which must be displayed\n\n    \"\"\"\n    if not (keyword or name or type):\n        raise ValueError(\"either a keyword, name, or type arg is required\")\n\n    from warnings import warn\n    warn(\"places_radar is deprecated, see http://goo.gl/BGiumE\",\n         DeprecationWarning)\n\n    return _places(client, \"radar\", location=location, radius=radius,\n                   keyword=keyword, min_price=min_price, max_price=max_price,\n                   name=name, open_now=open_now, type=type)", "response": "Performs a radar search for places."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _places(client, url_part, query=None, location=None, radius=None,\n            keyword=None, language=None, min_price=0, max_price=4, name=None,\n            open_now=False, rank_by=None, type=None, region=None, page_token=None):\n    \"\"\"\n    Internal handler for ``places``, ``places_nearby``, and ``places_radar``.\n    See each method's docs for arg details.\n    \"\"\"\n\n    params = {\"minprice\": min_price, \"maxprice\": max_price}\n\n    if query:\n        params[\"query\"] = query\n    if location:\n        params[\"location\"] = convert.latlng(location)\n    if radius:\n        params[\"radius\"] = radius\n    if keyword:\n        params[\"keyword\"] = keyword\n    if language:\n        params[\"language\"] = language\n    if name:\n        params[\"name\"] = convert.join_list(\" \", name)\n    if open_now:\n        params[\"opennow\"] = \"true\"\n    if rank_by:\n        params[\"rankby\"] = rank_by\n    if type:\n        params[\"type\"] = type\n    if region:\n        params[\"region\"] = region\n    if page_token:\n        params[\"pagetoken\"] = page_token\n\n    url = \"/maps/api/place/%ssearch/json\" % url_part\n    return client._request(url, params)", "response": "Internal method for places search."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of details for a single place.", "response": "def place(client, place_id, session_token=None, fields=None, language=None):\n    \"\"\"\n    Comprehensive details for an individual place.\n\n    :param place_id: A textual identifier that uniquely identifies a place,\n        returned from a Places search.\n    :type place_id: string\n\n    :param session_token: A random string which identifies an autocomplete\n                          session for billing purposes.\n    :type session_token: string\n\n    :param fields: The fields specifying the types of place data to return,\n                   separated by a comma. For full details see:\n                   https://cloud.google.com/maps-platform/user-guide/product-changes/#places\n    :type input: list\n\n    :param language: The language in which to return results.\n    :type language: string\n\n    :rtype: result dict with the following keys:\n        result: dict containing place details\n        html_attributions: set of attributions which must be displayed\n    \"\"\"\n    params = {\"placeid\": place_id}\n\n    if fields:\n        invalid_fields = set(fields) - PLACES_DETAIL_FIELDS\n        if invalid_fields:\n            raise ValueError(\"Valid values for the `fields` param for \"\n                             \"`place` are '%s', these given field(s) \"\n                             \"are invalid: '%s'\" % (\n                                \"', '\".join(PLACES_DETAIL_FIELDS),\n                                \"', '\".join(invalid_fields)))\n        params[\"fields\"] = convert.join_list(\",\", fields)\n\n    if language:\n        params[\"language\"] = language\n    if session_token:\n        params[\"sessiontoken\"] = session_token\n\n    return client._request(\"/maps/api/place/details/json\", params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndownloading a photo from the Places API and returns an iterator of the image data.", "response": "def places_photo(client, photo_reference, max_width=None, max_height=None):\n    \"\"\"\n    Downloads a photo from the Places API.\n\n    :param photo_reference: A string identifier that uniquely identifies a\n        photo, as provided by either a Places search or Places detail request.\n    :type photo_reference: string\n\n    :param max_width: Specifies the maximum desired width, in pixels.\n    :type max_width: int\n\n    :param max_height: Specifies the maximum desired height, in pixels.\n    :type max_height: int\n\n    :rtype: iterator containing the raw image data, which typically can be\n        used to save an image file locally. For example:\n\n        ```\n        f = open(local_filename, 'wb')\n        for chunk in client.places_photo(photo_reference, max_width=100):\n            if chunk:\n                f.write(chunk)\n        f.close()\n        ```\n    \"\"\"\n\n    if not (max_width or max_height):\n        raise ValueError(\"a max_width or max_height arg is required\")\n\n    params = {\"photoreference\": photo_reference}\n\n    if max_width:\n        params[\"maxwidth\"] = max_width\n    if max_height:\n        params[\"maxheight\"] = max_height\n\n    # \"extract_body\" and \"stream\" args here are used to return an iterable\n    # response containing the image file data, rather than converting from\n    # json.\n    response = client._request(\"/maps/api/place/photo\", params,\n                           extract_body=lambda response: response,\n                           requests_kwargs={\"stream\": True})\n    return response.iter_content()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a Place predictions given a textual search string and optional geographic bounds.", "response": "def places_autocomplete(client, input_text, session_token, offset=None,\n                        location=None, radius=None, language=None, types=None,\n                        components=None, strict_bounds=False):\n    \"\"\"\n    Returns Place predictions given a textual search string and optional\n    geographic bounds.\n\n    :param input_text: The text string on which to search.\n    :type input_text: string\n\n    :param session_token: A random string which identifies an autocomplete\n                          session for billing purposes.\n    :type session_token: string\n\n    :param offset: The position, in the input term, of the last character\n                   that the service uses to match predictions. For example,\n                   if the input is 'Google' and the offset is 3, the\n                   service will match on 'Goo'.\n    :type offset: int\n\n    :param location: The latitude/longitude value for which you wish to obtain the\n                     closest, human-readable address.\n    :type location: string, dict, list, or tuple\n\n    :param radius: Distance in meters within which to bias results.\n    :type radius: int\n\n    :param language: The language in which to return results.\n    :type language: string\n\n    :param types: Restricts the results to places matching the specified type.\n        The full list of supported types is available here:\n        https://developers.google.com/places/web-service/autocomplete#place_types\n    :type types: string\n\n    :param components: A component filter for which you wish to obtain a geocode.\n        Currently, you can use components to filter by up to 5 countries for\n        example: ``{'country': ['US', 'AU']}``\n    :type components: dict\n\n    :param strict_bounds: Returns only those places that are strictly within\n        the region defined by location and radius.\n    :type strict_bounds: bool\n\n    :rtype: list of predictions\n\n    \"\"\"\n    return _autocomplete(client, \"\", input_text, session_token=session_token,\n                         offset=offset, location=location, radius=radius,\n                         language=language, types=types, components=components,\n                         strict_bounds=strict_bounds)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn Place predictions given a textual search query such as pizza near New York and optional geographic bounds.", "response": "def places_autocomplete_query(client, input_text, offset=None, location=None,\n                              radius=None, language=None):\n    \"\"\"\n    Returns Place predictions given a textual search query, such as\n    \"pizza near New York\", and optional geographic bounds.\n\n    :param input_text: The text query on which to search.\n    :type input_text: string\n\n    :param offset: The position, in the input term, of the last character\n        that the service uses to match predictions. For example, if the input\n        is 'Google' and the offset is 3, the service will match on 'Goo'.\n    :type offset: int\n\n    :param location: The latitude/longitude value for which you wish to obtain the\n        closest, human-readable address.\n    :type location: string, dict, list, or tuple\n\n    :param radius: Distance in meters within which to bias results.\n    :type radius: number\n\n    :param language: The language in which to return results.\n    :type language: string\n\n    :rtype: list of predictions\n    \"\"\"\n    return _autocomplete(client, \"query\", input_text, offset=offset,\n                         location=location, radius=radius, language=language)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _autocomplete(client, url_part, input_text, session_token=None,\n                  offset=None, location=None, radius=None, language=None,\n                  types=None, components=None, strict_bounds=False):\n    \"\"\"\n    Internal handler for ``autocomplete`` and ``autocomplete_query``.\n    See each method's docs for arg details.\n    \"\"\"\n\n    params = {\"input\": input_text}\n\n    if session_token:\n        params[\"sessiontoken\"] = session_token\n    if offset:\n        params[\"offset\"] = offset\n    if location:\n        params[\"location\"] = convert.latlng(location)\n    if radius:\n        params[\"radius\"] = radius\n    if language:\n        params[\"language\"] = language\n    if types:\n        params[\"types\"] = types\n    if components:\n        if len(components) != 1 or list(components.keys())[0] != \"country\":\n            raise ValueError(\"Only country components are supported\")\n        params[\"components\"] = convert.components(components)\n    if strict_bounds:\n        params[\"strictbounds\"] = \"true\"\n\n    url = \"/maps/api/place/%sautocomplete/json\" % url_part\n    return client._request(url, params).get(\"predictions\", [])", "response": "Internal method for autocomplete_query."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract the data from a geolocation response.", "response": "def _geolocation_extract(response):\n    \"\"\"\n    Mimics the exception handling logic in ``client._get_body``, but\n    for geolocation which uses a different response format.\n    \"\"\"\n    body = response.json()\n    if response.status_code in (200, 404):\n        return body\n\n    try:\n        error = body[\"error\"][\"errors\"][0][\"reason\"]\n    except KeyError:\n        error = None\n\n    if response.status_code == 403:\n        raise exceptions._OverQueryLimit(response.status_code, error)\n    else:\n        raise exceptions.ApiError(response.status_code, error)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef geolocate(client, home_mobile_country_code=None,\n              home_mobile_network_code=None, radio_type=None, carrier=None,\n              consider_ip=None, cell_towers=None, wifi_access_points=None):\n    \"\"\"\n    The Google Maps Geolocation API returns a location and accuracy\n    radius based on information about cell towers and WiFi nodes given.\n\n    See https://developers.google.com/maps/documentation/geolocation/intro\n    for more info, including more detail for each parameter below.\n\n    :param home_mobile_country_code: The mobile country code (MCC) for\n        the device's home network.\n    :type home_mobile_country_code: string\n\n    :param home_mobile_network_code: The mobile network code (MCC) for\n        the device's home network.\n    :type home_mobile_network_code: string\n\n    :param radio_type: The mobile radio type. Supported values are\n        lte, gsm, cdma, and wcdma. While this field is optional, it\n        should be included if a value is available, for more accurate\n        results.\n    :type radio_type: string\n\n    :param carrier: The carrier name.\n    :type carrier: string\n\n    :param consider_ip: Specifies whether to fall back to IP geolocation\n        if wifi and cell tower signals are not available. Note that the\n        IP address in the request header may not be the IP of the device.\n    :type consider_ip: bool\n\n    :param cell_towers: A list of cell tower dicts. See\n        https://developers.google.com/maps/documentation/geolocation/intro#cell_tower_object\n        for more detail.\n    :type cell_towers: list of dicts\n\n    :param wifi_access_points: A list of WiFi access point dicts. See\n        https://developers.google.com/maps/documentation/geolocation/intro#wifi_access_point_object\n        for more detail.\n    :type wifi_access_points: list of dicts\n    \"\"\"\n\n    params = {}\n    if home_mobile_country_code is not None:\n        params[\"homeMobileCountryCode\"] = home_mobile_country_code\n    if home_mobile_network_code is not None:\n        params[\"homeMobileNetworkCode\"] = home_mobile_network_code\n    if radio_type is not None:\n        params[\"radioType\"] = radio_type\n    if carrier is not None:\n        params[\"carrier\"] = carrier\n    if consider_ip is not None:\n        params[\"considerIp\"] = consider_ip\n    if cell_towers is not None:\n        params[\"cellTowers\"] = cell_towers\n    if wifi_access_points is not None:\n        params[\"wifiAccessPoints\"] = wifi_access_points\n\n    return client._request(\"/geolocation/v1/geolocate\", {},  # No GET params\n                           base_url=_GEOLOCATION_BASE_URL,\n                           extract_body=_geolocation_extract,\n                           post_json=params)", "response": "This function returns a location and accuracy of a node given a set of cell towers and WiFi nodes."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget travel distance and time for a set of origins and destinations.", "response": "def distance_matrix(client, origins, destinations,\n                    mode=None, language=None, avoid=None, units=None,\n                    departure_time=None, arrival_time=None, transit_mode=None,\n                    transit_routing_preference=None, traffic_model=None, region=None):\n    \"\"\" Gets travel distance and time for a matrix of origins and destinations.\n\n    :param origins: One or more locations and/or latitude/longitude values,\n        from which to calculate distance and time. If you pass an address as\n        a string, the service will geocode the string and convert it to a\n        latitude/longitude coordinate to calculate directions.\n    :type origins: a single location, or a list of locations, where a\n        location is a string, dict, list, or tuple\n\n    :param destinations: One or more addresses and/or lat/lng values, to\n        which to calculate distance and time. If you pass an address as a\n        string, the service will geocode the string and convert it to a\n        latitude/longitude coordinate to calculate directions.\n    :type destinations: a single location, or a list of locations, where a\n        location is a string, dict, list, or tuple\n\n    :param mode: Specifies the mode of transport to use when calculating\n        directions. Valid values are \"driving\", \"walking\", \"transit\" or\n        \"bicycling\".\n    :type mode: string\n\n    :param language: The language in which to return results.\n    :type language: string\n\n    :param avoid: Indicates that the calculated route(s) should avoid the\n        indicated features. Valid values are \"tolls\", \"highways\" or \"ferries\".\n    :type avoid: string\n\n    :param units: Specifies the unit system to use when displaying results.\n        Valid values are \"metric\" or \"imperial\".\n    :type units: string\n\n    :param departure_time: Specifies the desired time of departure.\n    :type departure_time: int or datetime.datetime\n\n    :param arrival_time: Specifies the desired time of arrival for transit\n        directions. Note: you can't specify both departure_time and\n        arrival_time.\n    :type arrival_time: int or datetime.datetime\n\n    :param transit_mode: Specifies one or more preferred modes of transit.\n        This parameter may only be specified for requests where the mode is\n        transit. Valid values are \"bus\", \"subway\", \"train\", \"tram\", \"rail\".\n        \"rail\" is equivalent to [\"train\", \"tram\", \"subway\"].\n    :type transit_mode: string or list of strings\n\n    :param transit_routing_preference: Specifies preferences for transit\n        requests. Valid values are \"less_walking\" or \"fewer_transfers\".\n    :type transit_routing_preference: string\n\n    :param traffic_model: Specifies the predictive travel time model to use.\n        Valid values are \"best_guess\" or \"optimistic\" or \"pessimistic\".\n        The traffic_model parameter may only be specified for requests where\n        the travel mode is driving, and where the request includes a\n        departure_time.\n\n    :param region: Specifies the prefered region the geocoder should search\n        first, but it will not restrict the results to only this region. Valid\n        values are a ccTLD code.\n    :type region: string\n\n    :rtype: matrix of distances. Results are returned in rows, each row\n        containing one origin paired with each destination.\n    \"\"\"\n\n    params = {\n        \"origins\": convert.location_list(origins),\n        \"destinations\": convert.location_list(destinations)\n    }\n\n    if mode:\n        # NOTE(broady): the mode parameter is not validated by the Maps API\n        # server. Check here to prevent silent failures.\n        if mode not in [\"driving\", \"walking\", \"bicycling\", \"transit\"]:\n            raise ValueError(\"Invalid travel mode.\")\n        params[\"mode\"] = mode\n\n    if language:\n        params[\"language\"] = language\n\n    if avoid:\n        if avoid not in [\"tolls\", \"highways\", \"ferries\"]:\n            raise ValueError(\"Invalid route restriction.\")\n        params[\"avoid\"] = avoid\n\n    if units:\n        params[\"units\"] = units\n\n    if departure_time:\n        params[\"departure_time\"] = convert.time(departure_time)\n\n    if arrival_time:\n        params[\"arrival_time\"] = convert.time(arrival_time)\n\n    if departure_time and arrival_time:\n        raise ValueError(\"Should not specify both departure_time and\"\n                         \"arrival_time.\")\n\n    if transit_mode:\n        params[\"transit_mode\"] = convert.join_list(\"|\", transit_mode)\n\n    if transit_routing_preference:\n        params[\"transit_routing_preference\"] = transit_routing_preference\n\n    if traffic_model:\n        params[\"traffic_model\"] = traffic_model\n\n    if region:\n        params[\"region\"] = region\n\n    return client._request(\"/maps/api/distancematrix/json\", params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprovides a single entry point for modifying all API methods. For now this is limited to allowing the client object to be modified with an `extra_params` keyword arg to each method, that is then used as the params for each web service request. Please note that this is an unsupported feature for advanced use only. It's also currently incompatibile with multiple threads, see GH #160.", "response": "def make_api_method(func):\n    \"\"\"\n    Provides a single entry point for modifying all API methods.\n    For now this is limited to allowing the client object to be modified\n    with an `extra_params` keyword arg to each method, that is then used\n    as the params for each web service request.\n\n    Please note that this is an unsupported feature for advanced use only.\n    It's also currently incompatibile with multiple threads, see GH #160.\n    \"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        args[0]._extra_params = kwargs.pop(\"extra_params\", None)\n        result = func(*args, **kwargs)\n        try:\n            del args[0]._extra_params\n        except AttributeError:\n            pass\n        return result\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sign_hmac(secret, payload):\n    payload = payload.encode('ascii', 'strict')\n    secret = secret.encode('ascii', 'strict')\n    sig = hmac.new(base64.urlsafe_b64decode(secret), payload, hashlib.sha1)\n    out = base64.urlsafe_b64encode(sig.digest())\n    return out.decode('utf-8')", "response": "Returns a base64 - encoded HMAC - SHA1 signature of a given string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef urlencode_params(params):\n    # urlencode does not handle unicode strings in Python 2.\n    # Firstly, normalize the values so they get encoded correctly.\n    params = [(key, normalize_for_urlencode(val)) for key, val in params]\n    # Secondly, unquote unreserved chars which are incorrectly quoted\n    # by urllib.urlencode, causing invalid auth signatures. See GH #72\n    # for more info.\n    return requests.utils.unquote_unreserved(urlencode(params))", "response": "URL encodes the parameters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _request(self, url, params, first_request_time=None, retry_counter=0,\n             base_url=_DEFAULT_BASE_URL, accepts_clientid=True,\n             extract_body=None, requests_kwargs=None, post_json=None):\n        \"\"\"Performs HTTP GET/POST with credentials, returning the body as\n        JSON.\n\n        :param url: URL path for the request. Should begin with a slash.\n        :type url: string\n\n        :param params: HTTP GET parameters.\n        :type params: dict or list of key/value tuples\n\n        :param first_request_time: The time of the first request (None if no\n            retries have occurred).\n        :type first_request_time: datetime.datetime\n\n        :param retry_counter: The number of this retry, or zero for first attempt.\n        :type retry_counter: int\n\n        :param base_url: The base URL for the request. Defaults to the Maps API\n            server. Should not have a trailing slash.\n        :type base_url: string\n\n        :param accepts_clientid: Whether this call supports the client/signature\n            params. Some APIs require API keys (e.g. Roads).\n        :type accepts_clientid: bool\n\n        :param extract_body: A function that extracts the body from the request.\n            If the request was not successful, the function should raise a\n            googlemaps.HTTPError or googlemaps.ApiError as appropriate.\n        :type extract_body: function\n\n        :param requests_kwargs: Same extra keywords arg for requests as per\n            __init__, but provided here to allow overriding internally on a\n            per-request basis.\n        :type requests_kwargs: dict\n\n        :raises ApiError: when the API returns an error.\n        :raises Timeout: if the request timed out.\n        :raises TransportError: when something went wrong while trying to\n            exceute a request.\n        \"\"\"\n\n        if not first_request_time:\n            first_request_time = datetime.now()\n\n        elapsed = datetime.now() - first_request_time\n        if elapsed > self.retry_timeout:\n            raise googlemaps.exceptions.Timeout()\n\n        if retry_counter > 0:\n            # 0.5 * (1.5 ^ i) is an increased sleep time of 1.5x per iteration,\n            # starting at 0.5s when retry_counter=0. The first retry will occur\n            # at 1, so subtract that first.\n            delay_seconds = 0.5 * 1.5 ** (retry_counter - 1)\n\n            # Jitter this value by 50% and pause.\n            time.sleep(delay_seconds * (random.random() + 0.5))\n\n        authed_url = self._generate_auth_url(url, params, accepts_clientid)\n\n        # Default to the client-level self.requests_kwargs, with method-level\n        # requests_kwargs arg overriding.\n        requests_kwargs = requests_kwargs or {}\n        final_requests_kwargs = dict(self.requests_kwargs, **requests_kwargs)\n\n        # Determine GET/POST.\n        requests_method = self.session.get\n        if post_json is not None:\n            requests_method = self.session.post\n            final_requests_kwargs[\"json\"] = post_json\n\n        try:\n            response = requests_method(base_url + authed_url,\n                                       **final_requests_kwargs)\n        except requests.exceptions.Timeout:\n            raise googlemaps.exceptions.Timeout()\n        except Exception as e:\n            raise googlemaps.exceptions.TransportError(e)\n\n        if response.status_code in _RETRIABLE_STATUSES:\n            # Retry request.\n            return self._request(url, params, first_request_time,\n                                 retry_counter + 1, base_url, accepts_clientid,\n                                 extract_body, requests_kwargs, post_json)\n\n        # Check if the time of the nth previous query (where n is\n        # queries_per_second) is under a second ago - if so, sleep for\n        # the difference.\n        if self.sent_times and len(self.sent_times) == self.queries_per_second:\n            elapsed_since_earliest = time.time() - self.sent_times[0]\n            if elapsed_since_earliest < 1:\n                time.sleep(1 - elapsed_since_earliest)\n\n        try:\n            if extract_body:\n                result = extract_body(response)\n            else:\n                result = self._get_body(response)\n            self.sent_times.append(time.time())\n            return result\n        except googlemaps.exceptions._RetriableRequest as e:\n            if isinstance(e, googlemaps.exceptions._OverQueryLimit) and not self.retry_over_query_limit:\n                raise\n\n            # Retry request.\n            return self._request(url, params, first_request_time,\n                                 retry_counter + 1, base_url, accepts_clientid,\n                                 extract_body, requests_kwargs, post_json)", "response": "Performs a HTTP GET or POST request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating the path and query string portion of the request URL.", "response": "def _generate_auth_url(self, path, params, accepts_clientid):\n        \"\"\"Returns the path and query string portion of the request URL, first\n        adding any necessary parameters.\n\n        :param path: The path portion of the URL.\n        :type path: string\n\n        :param params: URL parameters.\n        :type params: dict or list of key/value tuples\n\n        :rtype: string\n\n        \"\"\"\n        # Deterministic ordering through sorting by key.\n        # Useful for tests, and in the future, any caching.\n        extra_params = getattr(self, \"_extra_params\", None) or {}\n        if type(params) is dict:\n            params = sorted(dict(extra_params, **params).items())\n        else:\n            params = sorted(extra_params.items()) + params[:] # Take a copy.\n\n        if accepts_clientid and self.client_id and self.client_secret:\n            if self.channel:\n                params.append((\"channel\", self.channel))\n            params.append((\"client\", self.client_id))\n\n            path = \"?\".join([path, urlencode_params(params)])\n            sig = sign_hmac(self.client_secret, path)\n            return path + \"&signature=\" + sig\n\n        if self.key:\n            params.append((\"key\", self.key))\n            return path + \"?\" + urlencode_params(params)\n\n        raise ValueError(\"Must provide API key for this API. It does not accept \"\n                         \"enterprise credentials.\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning all jobs that are scheduled to run.", "response": "def run_pending(self):\n        \"\"\"\n        Run all jobs that are scheduled to run.\n\n        Please note that it is *intended behavior that run_pending()\n        does not run missed jobs*. For example, if you've registered a job\n        that should run every minute and you only call run_pending()\n        in one hour increments then your job won't be run 60 times in\n        between but only once.\n        \"\"\"\n        runnable_jobs = (job for job in self.jobs if job.should_run)\n        for job in sorted(runnable_jobs):\n            self._run_job(job)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun all jobs in the system.", "response": "def run_all(self, delay_seconds=0):\n        \"\"\"\n        Run all jobs regardless if they are scheduled to run or not.\n\n        A delay of `delay` seconds is added between each job. This helps\n        distribute system load generated by the jobs more evenly\n        over time.\n\n        :param delay_seconds: A delay added between every executed job\n        \"\"\"\n        logger.info('Running *all* %i jobs with %is delay inbetween',\n                    len(self.jobs), delay_seconds)\n        for job in self.jobs[:]:\n            self._run_job(job)\n            time.sleep(delay_seconds)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes all scheduled jobs with the given tag or all jobs with the given tag.", "response": "def clear(self, tag=None):\n        \"\"\"\n        Deletes scheduled jobs marked with the given tag, or all jobs\n        if tag is omitted.\n\n        :param tag: An identifier used to identify a subset of\n                    jobs to delete\n        \"\"\"\n        if tag is None:\n            del self.jobs[:]\n        else:\n            self.jobs[:] = (job for job in self.jobs if tag not in job.tags)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntags the job with one or more unique indentifiers.", "response": "def tag(self, *tags):\n        \"\"\"\n        Tags the job with one or more unique indentifiers.\n\n        Tags must be hashable. Duplicate tags are discarded.\n\n        :param tags: A unique list of ``Hashable`` tags.\n        :return: The invoked job instance\n        \"\"\"\n        if not all(isinstance(tag, collections.Hashable) for tag in tags):\n            raise TypeError('Tags must be hashable')\n        self.tags.update(tags)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the invoked job instance that represents the time that the job should be run at.", "response": "def at(self, time_str):\n        \"\"\"\n        Specify a particular time that the job should be run at.\n\n        :param time_str: A string in one of the following formats: `HH:MM:SS`,\n            `HH:MM`,`:MM`, `:SS`. The format must make sense given how often\n            the job is repeating; for example, a job that repeats every minute\n            should not be given a string in the form `HH:MM:SS`. The difference\n            between `:MM` and `:SS` is inferred from the selected time-unit\n            (e.g. `every().hour.at(':30')` vs. `every().minute.at(':30')`).\n        :return: The invoked job instance\n        \"\"\"\n        if (self.unit not in ('days', 'hours', 'minutes')\n                and not self.start_day):\n            raise ScheduleValueError('Invalid unit')\n        if not isinstance(time_str, str):\n            raise TypeError('at() should be passed a string')\n        if self.unit == 'days' or self.start_day:\n            if not re.match(r'^([0-2]\\d:)?[0-5]\\d:[0-5]\\d$', time_str):\n                raise ScheduleValueError('Invalid time format')\n        if self.unit == 'hours':\n            if not re.match(r'^([0-5]\\d)?:[0-5]\\d$', time_str):\n                raise ScheduleValueError(('Invalid time format for'\n                                          ' an hourly job'))\n        if self.unit == 'minutes':\n            if not re.match(r'^:[0-5]\\d$', time_str):\n                raise ScheduleValueError(('Invalid time format for'\n                                          ' a minutely job'))\n        time_values = time_str.split(':')\n        if len(time_values) == 3:\n            hour, minute, second = time_values\n        elif len(time_values) == 2 and self.unit == 'minutes':\n            hour = 0\n            minute = 0\n            _, second = time_values\n        else:\n            hour, minute = time_values\n            second = 0\n        if self.unit == 'days' or self.start_day:\n            hour = int(hour)\n            if not (0 <= hour <= 23):\n                raise ScheduleValueError('Invalid number of hours')\n        elif self.unit == 'hours':\n            hour = 0\n        elif self.unit == 'minutes':\n            hour = 0\n            minute = 0\n        minute = int(minute)\n        second = int(second)\n        self.at_time = datetime.time(hour, minute, second)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nspecifying the job_func that should be called every time the job runs. Any additional arguments are passed on to job_func when the job runs. :param job_func: The function to be scheduled :return: The invoked job instance", "response": "def do(self, job_func, *args, **kwargs):\n        \"\"\"\n        Specifies the job_func that should be called every time the\n        job runs.\n\n        Any additional arguments are passed on to job_func when\n        the job runs.\n\n        :param job_func: The function to be scheduled\n        :return: The invoked job instance\n        \"\"\"\n        self.job_func = functools.partial(job_func, *args, **kwargs)\n        try:\n            functools.update_wrapper(self.job_func, job_func)\n        except AttributeError:\n            # job_funcs already wrapped by functools.partial won't have\n            # __name__, __module__ or __doc__ and the update_wrapper()\n            # call will fail.\n            pass\n        self._schedule_next_run()\n        self.scheduler.jobs.append(self)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun the job and immediately reschedule it.", "response": "def run(self):\n        \"\"\"\n        Run the job and immediately reschedule it.\n\n        :return: The return value returned by the `job_func`\n        \"\"\"\n        logger.info('Running job %s', self)\n        ret = self.job_func()\n        self.last_run = datetime.datetime.now()\n        self._schedule_next_run()\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _schedule_next_run(self):\n        if self.unit not in ('seconds', 'minutes', 'hours', 'days', 'weeks'):\n            raise ScheduleValueError('Invalid unit')\n\n        if self.latest is not None:\n            if not (self.latest >= self.interval):\n                raise ScheduleError('`latest` is greater than `interval`')\n            interval = random.randint(self.interval, self.latest)\n        else:\n            interval = self.interval\n\n        self.period = datetime.timedelta(**{self.unit: interval})\n        self.next_run = datetime.datetime.now() + self.period\n        if self.start_day is not None:\n            if self.unit != 'weeks':\n                raise ScheduleValueError('`unit` should be \\'weeks\\'')\n            weekdays = (\n                'monday',\n                'tuesday',\n                'wednesday',\n                'thursday',\n                'friday',\n                'saturday',\n                'sunday'\n            )\n            if self.start_day not in weekdays:\n                raise ScheduleValueError('Invalid start day')\n            weekday = weekdays.index(self.start_day)\n            days_ahead = weekday - self.next_run.weekday()\n            if days_ahead <= 0:  # Target day already happened this week\n                days_ahead += 7\n            self.next_run += datetime.timedelta(days_ahead) - self.period\n        if self.at_time is not None:\n            if (self.unit not in ('days', 'hours', 'minutes')\n                    and self.start_day is None):\n                raise ScheduleValueError(('Invalid unit without'\n                                          ' specifying start day'))\n            kwargs = {\n                'second': self.at_time.second,\n                'microsecond': 0\n            }\n            if self.unit == 'days' or self.start_day is not None:\n                kwargs['hour'] = self.at_time.hour\n            if self.unit in ['days', 'hours'] or self.start_day is not None:\n                kwargs['minute'] = self.at_time.minute\n            self.next_run = self.next_run.replace(**kwargs)\n            # If we are running for the first time, make sure we run\n            # at the specified time *today* (or *this hour*) as well\n            if not self.last_run:\n                now = datetime.datetime.now()\n                if (self.unit == 'days' and self.at_time > now.time() and\n                        self.interval == 1):\n                    self.next_run = self.next_run - datetime.timedelta(days=1)\n                elif self.unit == 'hours' \\\n                        and self.at_time.minute > now.minute \\\n                        or (self.at_time.minute == now.minute\n                            and self.at_time.second > now.second):\n                    self.next_run = self.next_run - datetime.timedelta(hours=1)\n                elif self.unit == 'minutes' \\\n                        and self.at_time.second > now.second:\n                    self.next_run = self.next_run - \\\n                                    datetime.timedelta(minutes=1)\n        if self.start_day is not None and self.at_time is not None:\n            # Let's see if we will still make that time we specified today\n            if (self.next_run - datetime.datetime.now()).days >= 7:\n                self.next_run -= self.period", "response": "Compute the instant when this job should run next."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate an Issuer Credential object.", "response": "async def create(source_id: str, attrs: dict, cred_def_handle: int, name: str, price: str):\n        \"\"\"\n            Creates a Class representing an Issuer Credential\n            :param source_id: Tag associated by user of sdk\n            :param attrs: attributes that will form the credential\n            :param cred_def_handle: Handle from previously created credential def object\n            :param name: Name given to the Credential\n            :param price: Price, in tokens, required as payment for the issuance of the credential.\n\n            Example:\n            source_id = '1'\n            cred_def_handle = 1\n            attrs = {'key': 'value', 'key2': 'value2', 'key3': 'value3'}\n            name = 'Credential Name'\n            issuer_did = '8XFh8yBzrpJQmNyZzgoTqB'\n            phone_number = '8019119191'\n            price = 1\n            issuer_credential = await IssuerCredential.create(source_id, attrs, cred_def_handle, name, price)\n        \"\"\"\n        constructor_params = (source_id, attrs, cred_def_handle, name, price)\n\n        c_source_id = c_char_p(source_id.encode('utf-8'))\n        c_cred_def_handle = c_uint32(cred_def_handle)\n        c_price = c_char_p(price.encode('utf-8'))\n        # default institution_did in config is used as issuer_did\n        c_issuer_did = None\n        c_data = c_char_p(json.dumps(attrs).encode('utf-8'))\n        c_name = c_char_p(name.encode('utf-8'))\n        c_params = (c_source_id, c_cred_def_handle, c_issuer_did, c_data, c_name, c_price)\n\n        return await IssuerCredential._create(\"vcx_issuer_create_credential\",\n                                              constructor_params,\n                                              c_params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def deserialize(data: dict):\n        issuer_credential = await IssuerCredential._deserialize(\"vcx_issuer_credential_deserialize\",\n                                                      json.dumps(data),\n                                                      data.get('data').get('source_id'),\n                                                      data.get('data').get('price'),\n                                                      data.get('data').get('credential_attributes'),\n                                                      data.get('data').get('schema_seq_no'),\n                                                      data.get('data').get('credential_request'))\n        return issuer_credential", "response": "Deserialize a dict representing a serialized IssuerCredential object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def send_offer(self, connection: Connection):\n        if not hasattr(IssuerCredential.send_offer, \"cb\"):\n            self.logger.debug(\"vcx_issuer_send_credential_offer: Creating callback\")\n            IssuerCredential.send_offer.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\n\n        c_credential_handle = c_uint32(self.handle)\n        c_connection_handle = c_uint32(connection.handle)\n\n        await do_call('vcx_issuer_send_credential_offer',\n                      c_credential_handle,\n                      c_connection_handle,\n                      IssuerCredential.send_offer.cb)", "response": "Sends an offer to a prover."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def revoke_credential(self):\n        if not hasattr(IssuerCredential.revoke_credential, \"cb\"):\n            self.logger.debug(\"vcx_issuer_revoke_credential: Creating callback\")\n            IssuerCredential.revoke_credential.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\n\n        c_credential_handle = c_uint32(self.handle)\n\n        await do_call('vcx_issuer_revoke_credential',\n                      c_credential_handle,\n                      IssuerCredential.revoke_credential.cb)", "response": "Revokes a credential.\n        :return: None\n            Example:\n            credential.revoke_credential()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def create_wallet(config: str,\n                        credentials: str) -> None:\n    \"\"\"\n    Creates a new secure wallet with the given unique name.\n\n    :param config: Wallet configuration json.\n     {\n       \"id\": string, Identifier of the wallet.\n             Configured storage uses this identifier to lookup exact wallet data placement.\n       \"storage_type\": optional<string>, Type of the wallet storage. Defaults to 'default'.\n                      'Default' storage type allows to store wallet data in the local file.\n                      Custom storage types can be registered with indy_register_wallet_storage call.\n       \"storage_config\": optional<object>, Storage configuration json. Storage type defines set of supported keys.\n                         Can be optional if storage supports default configuration.\n                          For 'default' storage type configuration is:\n       {\n         \"path\": optional<string>, Path to the directory with wallet files.\n                 Defaults to $HOME/.indy_client/wallet.\n                 Wallet will be stored in the file {path}/{id}/sqlite.db\n       }\n     }\n    :param credentials: Wallet credentials json\n     {\n       \"key\": string, Key or passphrase used for wallet key derivation.\n                      Look to key_derivation_method param for information about supported key derivation methods.\n       \"storage_credentials\": optional<object> Credentials for wallet storage. Storage type defines set of supported keys.\n                              Can be optional if storage supports default configuration.\n                               For 'default' storage type should be empty.\n       \"key_derivation_method\": optional<string> Algorithm to use for wallet key derivation:\n                                ARGON2I_MOD - derive secured wallet master key (used by default)\n                                ARGON2I_INT - derive secured wallet master key (less secured but faster)\n                                RAW - raw wallet key master provided (skip derivation).\n                                      RAW keys can be generated with generate_wallet_key call\n     }\n    :return: Error code\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"create_wallet: >>> config: %r, credentials: %r\",\n                 config,\n                 credentials)\n\n    if not hasattr(create_wallet, \"cb\"):\n        logger.debug(\"create_wallet: Creating callback\")\n        create_wallet.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\n\n    c_config = c_char_p(config.encode('utf-8'))\n    c_credentials = c_char_p(credentials.encode('utf-8'))\n\n    await do_call('indy_create_wallet',\n                  c_config,\n                  c_credentials,\n                  create_wallet.cb)\n\n    logger.debug(\"create_wallet: <<<\")", "response": "Create secure wallet with the given unique name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def close_wallet(handle: int) -> None:\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"close_wallet: >>> handle: %i\", handle)\n\n    if not hasattr(close_wallet, \"cb\"):\n        logger.debug(\"close_wallet: Creating callback\")\n        close_wallet.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\n\n    c_handle = c_int32(handle)\n\n    await do_call('indy_close_wallet',\n                  c_handle,\n                  close_wallet.cb)\n\n    logger.debug(\"close_wallet: <<<\")", "response": "Closes opened wallet and frees allocated resources."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def export_wallet(handle: int,\n                        export_config_json: str) -> None:\n    \"\"\"\n    Exports opened wallet to the file.\n\n    :param handle: wallet handle returned by indy_open_wallet.\n    :param export_config_json: JSON containing settings for input operation.\n       {\n          \"path\": path of the file that contains exported wallet content\n          \"key\": string, Key or passphrase used for wallet export key derivation.\n                         Look to key_derivation_method param for information about supported key derivation methods.\n          \"key_derivation_method\": optional<string> algorithm to use for export key derivation:\n                                ARGON2I_MOD - derive secured wallet export key (used by default)\n                                ARGON2I_INT - derive secured wallet export key (less secured but faster)\n                                RAW - raw wallet export key provided (skip derivation).\n                                      RAW keys can be generated with generate_wallet_key call\n       }\n    :return:\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"export_wallet: >>> handle: %r, export_config_json: %r\",\n                 handle,\n                 export_config_json)\n\n    if not hasattr(export_wallet, \"cb\"):\n        logger.debug(\"export_wallet: Creating callback\")\n        export_wallet.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\n\n    c_export_config_json = c_char_p(export_config_json.encode('utf-8'))\n\n    await do_call('indy_export_wallet',\n                  handle,\n                  c_export_config_json,\n                  export_wallet.cb)\n\n    logger.debug(\"export_wallet: <<<\")", "response": "Exports opened wallet to the file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new secure wallet with the given unique name and then imports its content according to fields provided in import_config_json.", "response": "async def import_wallet(config: str,\n                        credentials: str,\n                        import_config_json: str) -> None:\n    \"\"\"\n    Creates a new secure wallet with the given unique name and then imports its content\n    according to fields provided in import_config\n    This can be seen as an indy_create_wallet call with additional content import\n\n    :param config: Wallet configuration json.\n     {\n       \"id\": string, Identifier of the wallet.\n             Configured storage uses this identifier to lookup exact wallet data placement.\n       \"storage_type\": optional<string>, Type of the wallet storage. Defaults to 'default'.\n                      'Default' storage type allows to store wallet data in the local file.\n                      Custom storage types can be registered with indy_register_wallet_storage call.\n       \"storage_config\": optional<object>, Storage configuration json. Storage type defines set of supported keys.\n                         Can be optional if storage supports default configuration.\n                         For 'default' storage type configuration is:\n       {\n         \"path\": optional<string>, Path to the directory with wallet files.\n                 Defaults to $HOME/.indy_client/wallet.\n                 Wallet will be stored in the file {path}/{id}/sqlite.db\n       }\n     }\n    :param credentials: Wallet credentials json\n     {\n       \"key\": string, Key or passphrase used for wallet key derivation.\n                      Look to key_derivation_method param for information about supported key derivation methods.\n       \"storage_credentials\": optional<object> Credentials for wallet storage. Storage type defines set of supported keys.\n                              Can be optional if storage supports default configuration.\n                              For 'default' storage type should be empty.\n       \"key_derivation_method\": optional<string> Algorithm to use for wallet key derivation:\n                                 ARGON2I_MOD - derive secured wallet master key (used by default)\n                                 ARGON2I_INT - derive secured wallet master key (less secured but faster)\n                                 RAW - raw wallet key master provided (skip derivation).\n                                       RAW keys can be generated with generate_wallet_key call\n     }\n    :param import_config_json: JSON containing settings for input operation\u0416 {\n     \"path\": path of the file that contains exported wallet content\n     \"key\": key used for export of the wallet\n   }\n    :return: Error code\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"import_wallet: >>> config: %r, credentials: %r, import_config_json: %r\",\n                 config,\n                 credentials,\n                 import_config_json)\n\n    if not hasattr(import_wallet, \"cb\"):\n        logger.debug(\"import_wallet: Creating callback\")\n        import_wallet.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\n\n    c_config = c_char_p(config.encode('utf-8'))\n    c_credentials = c_char_p(credentials.encode('utf-8'))\n    c_import_config_json = c_char_p(import_config_json.encode('utf-8'))\n\n    await do_call('indy_import_wallet',\n                  c_config,\n                  c_credentials,\n                  c_import_config_json,\n                  import_wallet.cb)\n\n    logger.debug(\"import_wallet: <<<\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def generate_wallet_key(config: Optional[str]) -> str:\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"generate_wallet_key: >>> config: %r\",\n                 config)\n\n    if not hasattr(generate_wallet_key, \"cb\"):\n        logger.debug(\"generate_wallet_key: Creating callback\")\n        generate_wallet_key.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_config = c_char_p(config.encode('utf-8')) if config is not None else None\n\n    key = await do_call('indy_generate_wallet_key',\n                        c_config,\n                        generate_wallet_key.cb)\n\n    res = key.decode()\n\n    logger.debug(\"generate_wallet_key: <<< res: %r\", res)\n    return res", "response": "Generate wallet master key."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates credential schema entity that describes credential attributes list and allows credentials and interoperability.", "response": "async def issuer_create_schema(issuer_did: str,\n                               name: str,\n                               version: str,\n                               attrs: str) -> (str, str):\n    \"\"\"\n    Create credential schema entity that describes credential attributes list and allows credentials\n    interoperability.\n\n    Schema is public and intended to be shared with all anoncreds workflow actors usually by publishing SCHEMA transaction\n    to Indy distributed ledger.\n\n    It is IMPORTANT for current version POST Schema in Ledger and after that GET it from Ledger\n    with correct seq_no to save compatibility with Ledger.\n    After that can call indy_issuer_create_and_store_credential_def to build corresponding Credential Definition.\n\n    :param issuer_did: DID of schema issuer\n    :param name: a name the schema\n    :param version: a version of the schema\n    :param attrs: a list of schema attributes descriptions (the number of attributes should be less or equal than 125)\n    :return:\n        schema_id: identifier of created schema\n        schema_json: schema as json\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"issuer_create_schema: >>> issuer_did: %r, name: %r, version: %r, attrs: %r\",\n                 issuer_did,\n                 name,\n                 version,\n                 attrs)\n\n    if not hasattr(issuer_create_schema, \"cb\"):\n        logger.debug(\"issuer_create_schema: Creating callback\")\n        issuer_create_schema.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p))\n\n    c_issuer_did = c_char_p(issuer_did.encode('utf-8'))\n    c_name = c_char_p(name.encode('utf-8'))\n    c_version = c_char_p(version.encode('utf-8'))\n    c_attrs = c_char_p(attrs.encode('utf-8'))\n\n    (schema_id, schema_json) = await do_call('indy_issuer_create_schema',\n                                             c_issuer_did,\n                                             c_name,\n                                             c_version,\n                                             c_attrs,\n                                             issuer_create_schema.cb)\n\n    res = (schema_id.decode(), schema_json.decode())\n    logger.debug(\"issuer_create_schema: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def issuer_create_and_store_credential_def(wallet_handle: int,\n                                                 issuer_did: str,\n                                                 schema_json: str,\n                                                 tag: str,\n                                                 signature_type: Optional[str],\n                                                 config_json: Optional[str]) -> (str, str):\n    \"\"\"\n    Create credential definition entity that encapsulates credentials issuer DID, credential schema, secrets used for\n    signing credentials and secrets used for credentials revocation.\n\n    Credential definition entity contains private and public parts. Private part will be stored in the wallet.\n    Public part will be returned as json intended to be shared with all anoncreds workflow actors usually by\n    publishing CRED_DEF transaction to Indy distributed ledger.\n\n    It is IMPORTANT for current version GET Schema from Ledger with correct seq_no to save compatibility with Ledger.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param issuer_did: a DID of the issuer signing cred_def transaction to the Ledger\n    :param schema_json: credential schema as a json\n    :param tag: allows to distinct between credential definitions for the same issuer and schema\n    :param signature_type: credential definition type (optional, 'CL' by default) that defines credentials signature and revocation math.\n    Supported types are:\n        - 'CL': Camenisch-Lysyanskaya credential signature type\n    :param  config_json: (optional) type-specific configuration of credential definition as json:\n        - 'CL':\n          - support_revocation: whether to request non-revocation credential (optional, default false)\n    :return: \n        cred_def_id: identifier of created credential definition\n        cred_def_json: public part of created credential definition\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"issuer_create_and_store_credential_def: >>> wallet_handle: %r, issuer_did: %r, schema_json: %r,\"\n                 \" tag: %r, signature_type: %r, config_json: %r\",\n                 wallet_handle,\n                 issuer_did,\n                 schema_json,\n                 tag,\n                 signature_type,\n                 config_json)\n\n    if not hasattr(issuer_create_and_store_credential_def, \"cb\"):\n        logger.debug(\"issuer_create_and_store_credential_def: Creating callback\")\n        issuer_create_and_store_credential_def.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_issuer_did = c_char_p(issuer_did.encode('utf-8'))\n    c_schema_json = c_char_p(schema_json.encode('utf-8'))\n    c_tag = c_char_p(tag.encode('utf-8'))\n    c_signature_type = c_char_p(signature_type.encode('utf-8')) if signature_type is not None else None\n    c_config_json = c_char_p(config_json.encode('utf-8')) if config_json is not None else None\n\n    (credential_def_id, credential_def_json) = await do_call('indy_issuer_create_and_store_credential_def',\n                                                             c_wallet_handle,\n                                                             c_issuer_did,\n                                                             c_schema_json,\n                                                             c_tag,\n                                                             c_signature_type,\n                                                             c_config_json,\n                                                             issuer_create_and_store_credential_def.cb)\n\n    res = (credential_def_id.decode(), credential_def_json.decode())\n    logger.debug(\"issuer_create_and_store_credential_def: <<< res: %r\", res)\n    return res", "response": "Create credential definition entity that encapsulates credentials and secrets used for revocation and revocation math."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def issuer_create_and_store_revoc_reg(wallet_handle: int,\n                                            issuer_did: str,\n                                            revoc_def_type: Optional[str],\n                                            tag: str,\n                                            cred_def_id: str,\n                                            config_json: str,\n                                            tails_writer_handle: int) -> (str, str, str):\n    \"\"\"\n    Create a new revocation registry for the given credential definition as tuple of entities:\n    - Revocation registry definition that encapsulates credentials definition reference, revocation type specific configuration and\n      secrets used for credentials revocation\n    - Revocation registry state that stores the information about revoked entities in a non-disclosing way. The state can be\n      represented as ordered list of revocation registry entries were each entry represents the list of revocation or issuance operations.\n\n    Revocation registry definition entity contains private and public parts. Private part will be stored in the wallet. Public part\n    will be returned as json intended to be shared with all anoncreds workflow actors usually by publishing REVOC_REG_DEF transaction\n    to Indy distributed ledger.\n\n    Revocation registry state is stored on the wallet and also intended to be shared as the ordered list of REVOC_REG_ENTRY transactions.\n    This call initializes the state in the wallet and returns the initial entry.\n\n    Some revocation registry types (for example, 'CL_ACCUM') can require generation of binary blob called tails used to hide information about revoked credentials in public\n    revocation registry and intended to be distributed out of leger (REVOC_REG_DEF transaction will still contain uri and hash of tails).\n    This call requires access to pre-configured blob storage writer instance handle that will allow to write generated tails.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param issuer_did: a DID of the issuer signing transaction to the Ledger\n    :param revoc_def_type: revocation registry type (optional, default value depends on credential definition type). Supported types are:\n        - 'CL_ACCUM': Type-3 pairing based accumulator. Default for 'CL' credential definition type\n    :param tag: allows to distinct between revocation registries for the same issuer and credential definition\n    :param cred_def_id: id of stored in ledger credential definition\n    :param config_json: type-specific configuration of revocation registry as json:\n        - 'CL_ACCUM':\n            \"issuance_type\": (optional) type of issuance. Currently supported:\n                1) ISSUANCE_BY_DEFAULT: all indices are assumed to be issued and initial accumulator is calculated over all indices;\n                   Revocation Registry is updated only during revocation.\n                2) ISSUANCE_ON_DEMAND: nothing is issued initially accumulator is 1 (used by default);\n            \"max_cred_num\": maximum number of credentials the new registry can process (optional, default 100000)\n        }\n    :param tails_writer_handle:\n    :return: \n        revoc_reg_id: identifier of created revocation registry definition\n        revoc_reg_def_json: public part of revocation registry definition\n        revoc_reg_entry_json: revocation registry entry that defines initial state of revocation registry\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"issuer_create_and_store_revoc_reg: >>> wallet_handle: %r, issuer_did: %r, revoc_def_type: %r,\"\n                 \" tag: %r, cred_def_id: %r, config_json: %r, tails_writer_handle: %r\",\n                 wallet_handle,\n                 issuer_did,\n                 revoc_def_type,\n                 tag,\n                 cred_def_id,\n                 config_json,\n                 tails_writer_handle)\n\n    if not hasattr(issuer_create_and_store_revoc_reg, \"cb\"):\n        logger.debug(\"issuer_create_and_store_revoc_reg: Creating callback\")\n        issuer_create_and_store_revoc_reg.cb = create_cb(\n            CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_issuer_did = c_char_p(issuer_did.encode('utf-8'))\n    c_revoc_def_type = c_char_p(revoc_def_type.encode('utf-8')) if revoc_def_type is not None else None\n    c_tag = c_char_p(tag.encode('utf-8'))\n    c_cred_def_id = c_char_p(cred_def_id.encode('utf-8'))\n    c_config_json = c_char_p(config_json.encode('utf-8'))\n    c_tails_writer_handle = c_int32(tails_writer_handle)\n\n    (rev_reg_id, rev_reg_def_json, rev_reg_entry_json) = await do_call('indy_issuer_create_and_store_revoc_reg',\n                                                                       c_wallet_handle,\n                                                                       c_issuer_did,\n                                                                       c_revoc_def_type,\n                                                                       c_tag,\n                                                                       c_cred_def_id,\n                                                                       c_config_json,\n                                                                       c_tails_writer_handle,\n                                                                       issuer_create_and_store_revoc_reg.cb)\n    res = (rev_reg_id.decode(), rev_reg_def_json.decode(), rev_reg_entry_json.decode())\n    logger.debug(\"issuer_create_and_store_revoc_reg: <<< res: %r\", res)\n    return res", "response": "This function creates a revocation registry for the given issuer and stores it in the wallet."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def issuer_create_credential_offer(wallet_handle: int,\n                                         cred_def_id: str) -> str:\n    \"\"\"\n    Create credential offer that will be used by Prover for\n    credential request creation. Offer includes nonce and key correctness proof\n    for authentication between protocol steps and integrity checking.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param cred_def_id: id of credential definition stored in the wallet\n    :return:credential offer json:\n     {\n         \"schema_id\": string,\n         \"cred_def_id\": string,\n         // Fields below can depend on Cred Def type\n         \"nonce\": string,\n         \"key_correctness_proof\" : <key_correctness_proof>\n     }\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"issuer_create_credential_offer: >>> wallet_handle: %r, cred_def_id: %r\",\n                 wallet_handle,\n                 cred_def_id)\n\n    if not hasattr(issuer_create_credential_offer, \"cb\"):\n        logger.debug(\"issuer_create_credential_offer: Creating callback\")\n        issuer_create_credential_offer.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_cred_def_id = c_char_p(cred_def_id.encode('utf-8'))\n\n    credential_offer_json = await do_call('indy_issuer_create_credential_offer',\n                                          c_wallet_handle,\n                                          c_cred_def_id,\n                                          issuer_create_credential_offer.cb)\n\n    res = credential_offer_json.decode()\n    logger.debug(\"issuer_create_credential_offer: <<< res: %r\", res)\n    return res", "response": "Create credential offer that will be used by Prover for authentication between protocol steps and integrity checking."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def issuer_create_credential(wallet_handle: int,\n                                   cred_offer_json: str,\n                                   cred_req_json: str,\n                                   cred_values_json: str,\n                                   rev_reg_id: Optional[str],\n                                   blob_storage_reader_handle: Optional[int]) -> (str, Optional[str], Optional[str]):\n    \"\"\"\n    Check Cred Request for the given Cred Offer and issue Credential for the given Cred Request.\n\n    Cred Request must match Cred Offer. The credential definition and revocation registry definition\n    referenced in Cred Offer and Cred Request must be already created and stored into the wallet.\n\n    Information for this credential revocation will be store in the wallet as part of revocation registry under\n    generated cred_revoc_id local for this wallet.\n\n    This call returns revoc registry delta as json file intended to be shared as REVOC_REG_ENTRY transaction.\n    Note that it is possible to accumulate deltas to reduce ledger load.\n\n    :param wallet_handle: wallet handle (created by open_wallet).\n    :param cred_offer_json: a cred offer created by issuer_create_credential_offer\n    :param cred_req_json: a credential request created by prover_create_credential_req\n    :param cred_values_json: a credential containing attribute values for each of requested attribute names.\n     Example:\n     {\n      \"attr1\" : {\"raw\": \"value1\", \"encoded\": \"value1_as_int\" },\n      \"attr2\" : {\"raw\": \"value1\", \"encoded\": \"value1_as_int\" }\n     }\n    :param rev_reg_id: (Optional) id of revocation registry definition stored in the wallet\n    :param blob_storage_reader_handle: pre-configured blob storage reader instance handle that\n    will allow to read revocation tails\n    :return: \n     cred_json: Credential json containing signed credential values\n     {\n         \"schema_id\": string,\n         \"cred_def_id\": string,\n         \"rev_reg_def_id\", Optional<string>,\n         \"values\": <see cred_values_json above>,\n         // Fields below can depend on Cred Def type\n         \"signature\": <signature>,\n         \"signature_correctness_proof\": <signature_correctness_proof>\n     }\n     cred_revoc_id: local id for revocation info (Can be used for revocation of this cred)\n     revoc_reg_delta_json: Revocation registry delta json with a newly issued credential\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"issuer_create_credential: >>> wallet_handle: %r, cred_offer_json: %r, cred_req_json: %r,\"\n                 \" cred_values_json: %r, rev_reg_id: %r, blob_storage_reader_handle: %r\",\n                 wallet_handle,\n                 cred_offer_json,\n                 cred_req_json,\n                 cred_values_json,\n                 rev_reg_id,\n                 blob_storage_reader_handle)\n\n    if not hasattr(issuer_create_credential, \"cb\"):\n        logger.debug(\"issuer_create_credential: Creating callback\")\n        issuer_create_credential.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_cred_offer_json = c_char_p(cred_offer_json.encode('utf-8'))\n    c_cred_req_json = c_char_p(cred_req_json.encode('utf-8'))\n    c_cred_values_json = c_char_p(cred_values_json.encode('utf-8'))\n    c_rev_reg_id = c_char_p(rev_reg_id.encode('utf-8')) if rev_reg_id is not None else None\n    c_blob_storage_reader_handle = c_int32(blob_storage_reader_handle) if blob_storage_reader_handle else -1\n\n    (cred_json, cred_revoc_id, revoc_reg_delta_json) = await do_call('indy_issuer_create_credential',\n                                                                     c_wallet_handle,\n                                                                     c_cred_offer_json,\n                                                                     c_cred_req_json,\n                                                                     c_cred_values_json,\n                                                                     c_rev_reg_id,\n                                                                     c_blob_storage_reader_handle,\n                                                                     issuer_create_credential.cb)\n    cred_json = cred_json.decode()\n    cred_revoc_id = cred_revoc_id.decode() if cred_revoc_id else None\n    revoc_reg_delta_json = revoc_reg_delta_json.decode() if revoc_reg_delta_json else None\n    res = (cred_json, cred_revoc_id, revoc_reg_delta_json)\n\n    logger.debug(\"issuer_create_credential: <<< res: %r\", res)\n    return res", "response": "Create a credential for the given Cred Request and issue it to the revocation registry."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def issuer_revoke_credential(wallet_handle: int,\n                                   blob_storage_reader_handle: int,\n                                   rev_reg_id: str,\n                                   cred_revoc_id: str) -> str:\n    \"\"\"\n    Revoke a credential identified by a cred_revoc_id (returned by issuer_create_credential).\n\n    The corresponding credential definition and revocation registry must be already\n    created an stored into the wallet.\n\n    This call returns revoc registry delta as json file intended to be shared as REVOC_REG_ENTRY transaction.\n    Note that it is possible to accumulate deltas to reduce ledger load.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param blob_storage_reader_handle: pre-configured blob storage reader instance handle that will allow\n    to read revocation tails\n    :param rev_reg_id: id of revocation registry stored in wallet\n    :param cred_revoc_id: local id for revocation info\n    :return: Revocation registry delta json with a revoked credential.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\n        \"issuer_revoke_credential: >>> wallet_handle: %r, blob_storage_reader_handle: %r, rev_reg_id: %r, \"\n        \"cred_revoc_id: %r\",\n        wallet_handle,\n        blob_storage_reader_handle,\n        rev_reg_id,\n        cred_revoc_id)\n\n    if not hasattr(issuer_revoke_credential, \"cb\"):\n        logger.debug(\"issuer_revoke_credential: Creating callback\")\n        issuer_revoke_credential.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_blob_storage_reader_handle = c_int32(blob_storage_reader_handle)\n    c_rev_reg_id = c_char_p(rev_reg_id.encode('utf-8'))\n    c_cred_revoc_id = c_char_p(cred_revoc_id.encode('utf-8'))\n\n    revoc_reg_delta_json = await do_call('indy_issuer_revoke_credential',\n                                         c_wallet_handle,\n                                         c_blob_storage_reader_handle,\n                                         c_rev_reg_id,\n                                         c_cred_revoc_id,\n                                         issuer_revoke_credential.cb)\n    res = revoc_reg_delta_json.decode()\n    logger.debug(\"issuer_revoke_credential: <<< res: %r\", res)\n    return res", "response": "Revokes a credential identified by a cred_revoc_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmerges two revocation registry deltas into one revocation registry.", "response": "async def issuer_merge_revocation_registry_deltas(rev_reg_delta_json: str,\n                                                  other_rev_reg_delta_json: str) -> str:\n    \"\"\"\n    Merge two revocation registry deltas (returned by issuer_create_credential or issuer_revoke_credential) to accumulate common delta.\n    Send common delta to ledger to reduce the load.\n\n    :param rev_reg_delta_json: revocation registry delta json\n    :param other_rev_reg_delta_json: revocation registry delta for which PrevAccum value  is equal to current accum value of rev_reg_delta_json.\n    :return: Merged revocation registry delta\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\n        \"issuer_merge_revocation_registry_deltas: >>> rev_reg_delta_json: %r, other_rev_reg_delta_json: %r\",\n        rev_reg_delta_json,\n        other_rev_reg_delta_json)\n\n    if not hasattr(issuer_merge_revocation_registry_deltas, \"cb\"):\n        logger.debug(\"issuer_merge_revocation_registry_deltas: Creating callback\")\n        issuer_merge_revocation_registry_deltas.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_rev_reg_delta_json = c_char_p(rev_reg_delta_json.encode('utf-8'))\n    c_other_rev_reg_delta_json = c_char_p(other_rev_reg_delta_json.encode('utf-8'))\n\n    merged_revoc_reg_delta_json = await do_call('indy_issuer_merge_revocation_registry_deltas',\n                                                c_rev_reg_delta_json,\n                                                c_other_rev_reg_delta_json,\n                                                issuer_merge_revocation_registry_deltas.cb)\n    res = merged_revoc_reg_delta_json.decode()\n    logger.debug(\"issuer_merge_revocation_registry_deltas: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def prover_create_master_secret(wallet_handle: int,\n                                      master_secret_name: Optional[str]) -> str:\n    \"\"\"\n    Creates a master secret with a given name and stores it in the wallet.\n    The name must be unique.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param master_secret_name: (optional, if not present random one will be generated) new master id\n    :return: id of generated master secret.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"prover_create_master_secret: >>> wallet_handle: %r, master_secret_name: %r\",\n                 wallet_handle,\n                 master_secret_name)\n\n    if not hasattr(prover_create_master_secret, \"cb\"):\n        logger.debug(\"prover_create_master_secret: Creating callback\")\n        prover_create_master_secret.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_master_secret_name = c_char_p(master_secret_name.encode('utf-8')) if master_secret_name else None\n\n    out_master_secret_id = await do_call('indy_prover_create_master_secret',\n                                         c_wallet_handle,\n                                         c_master_secret_name,\n                                         prover_create_master_secret.cb)\n\n    res = out_master_secret_id.decode()\n    logger.debug(\"prover_create_master_secret: <<< res: %r\", res)\n    return res", "response": "Creates a master secret with a given name and stores it in the wallet."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def prover_create_credential_req(wallet_handle: int,\n                                       prover_did: str,\n                                       cred_offer_json: str,\n                                       cred_def_json: str,\n                                       master_secret_id: str) -> (str, str):\n    \"\"\"\n    Creates a clam request for the given credential offer.\n\n    The method creates a blinded master secret for a master secret identified by a provided name.\n    The master secret identified by the name must be already stored in the secure wallet (see prover_create_master_secret)\n    The blinded master secret is a part of the credential request.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param prover_did: a DID of the prover\n    :param cred_offer_json: credential offer as a json containing information about the issuer and a credential\n    :param cred_def_json: credential definition json related to <cred_def_id> in <cred_offer_json>\n    :param master_secret_id: the id of the master secret stored in the wallet\n    :return: \n     cred_req_json: Credential request json for creation of credential by Issuer\n     {\n      \"prover_did\" : string,\n      \"cred_def_id\" : string,\n         // Fields below can depend on Cred Def type\n      \"blinded_ms\" : <blinded_master_secret>,\n      \"blinded_ms_correctness_proof\" : <blinded_ms_correctness_proof>,\n      \"nonce\": string\n    }\n     cred_req_metadata_json: Credential request metadata json for processing of received form Issuer credential.\n        Note: cred_req_metadata_json mustn't be shared with Issuer.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"prover_create_credential_req: >>> wallet_handle: %r, prover_did: %r, cred_offer_json: %r,\"\n                 \" cred_def_json: %r, master_secret_id: %r\",\n                 wallet_handle,\n                 prover_did,\n                 cred_offer_json,\n                 cred_def_json,\n                 master_secret_id)\n\n    if not hasattr(prover_create_credential_req, \"cb\"):\n        logger.debug(\"prover_create_credential_req: Creating callback\")\n        prover_create_credential_req.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_prover_did = c_char_p(prover_did.encode('utf-8'))\n    c_cred_offer_json = c_char_p(cred_offer_json.encode('utf-8'))\n    c_cred_def_json = c_char_p(cred_def_json.encode('utf-8'))\n    c_master_secret_id = c_char_p(master_secret_id.encode('utf-8'))\n\n    (credential_req_json, credential_req_metadata_json) = await do_call('indy_prover_create_credential_req',\n                                                                        c_wallet_handle,\n                                                                        c_prover_did,\n                                                                        c_cred_offer_json,\n                                                                        c_cred_def_json,\n                                                                        c_master_secret_id,\n                                                                        prover_create_credential_req.cb)\n\n    credential_req_json = credential_req_json.decode()\n    credential_req_metadata_json = credential_req_metadata_json.decode()\n    res = (credential_req_json, credential_req_metadata_json)\n\n    logger.debug(\"prover_create_credential_req: <<< res: %r\", res)\n    return res", "response": "Create a credential request for the given credential offer."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstoring credential in a secure wallet.", "response": "async def prover_store_credential(wallet_handle: int,\n                                  cred_id: Optional[str],\n                                  cred_req_metadata_json: str,\n                                  cred_json: str,\n                                  cred_def_json: str,\n                                  rev_reg_def_json: Optional[str]) -> str:\n    \"\"\"\n    Check credential provided by Issuer for the given credential request,\n    updates the credential by a master secret and stores in a secure wallet.\n    \n    To support efficient search the following tags will be created for stored credential:\n        {\n            \"schema_id\": <credential schema id>,\n            \"schema_issuer_did\": <credential schema issuer did>,\n            \"schema_name\": <credential schema name>,\n            \"schema_version\": <credential schema version>,\n            \"issuer_did\": <credential issuer did>,\n            \"cred_def_id\": <credential definition id>,\n            \"rev_reg_id\": <credential revocation registry id>, # \"None\" as string if not present\n            // for every attribute in <credential values>\n            \"attr::<attribute name>::marker\": \"1\",\n            \"attr::<attribute name>::value\": <attribute raw value>,\n        }\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param cred_id: (optional, default is a random one) identifier by which credential will be stored in the wallet\n    :param cred_req_metadata_json: a credential request metadata created by prover_create_credential_req\n    :param cred_json: credential json received from issuer\n    :param cred_def_json: credential definition json related to <cred_def_id> in <cred_json>\n    :param rev_reg_def_json: revocation registry definition json related to <rev_reg_def_id> in <cred_json>\n    :return: cred_id: identifier by which credential is stored in the wallet\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"prover_store_credential: >>> wallet_handle: %r, cred_id: %r, \"\n                 \"cred_req_metadata_json: %r, cred_json: %r, cred_def_json: %r, rev_reg_def_json: %r\",\n                 wallet_handle,\n                 cred_id,\n                 cred_req_metadata_json,\n                 cred_json,\n                 cred_def_json,\n                 rev_reg_def_json)\n\n    if not hasattr(prover_store_credential, \"cb\"):\n        logger.debug(\"prover_store_credential: Creating callback\")\n        prover_store_credential.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_cred_id = c_char_p(cred_id.encode('utf-8')) if cred_id else None\n    c_cred_req_metadata_json = c_char_p(cred_req_metadata_json.encode('utf-8'))\n    c_cred_json = c_char_p(cred_json.encode('utf-8'))\n    c_cred_def_json = c_char_p(cred_def_json.encode('utf-8'))\n    c_rev_reg_def_json = c_char_p(rev_reg_def_json.encode('utf-8')) if rev_reg_def_json is not None else None\n\n    cred_id = await do_call('indy_prover_store_credential',\n                            c_wallet_handle,\n                            c_cred_id,\n                            c_cred_req_metadata_json,\n                            c_cred_json,\n                            c_cred_def_json,\n                            c_rev_reg_def_json,\n                            prover_store_credential.cb)\n\n    res = cred_id.decode()\n    logger.debug(\"prover_store_credential: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a credential by its id.", "response": "async def prover_get_credential(wallet_handle: int,\n                                cred_id: str) -> str:\n    \"\"\"\n    Gets human readable credential by the given id.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param cred_id: Identifier by which requested credential is stored in the wallet\n    :return:  credential json\n     {\n         \"referent\": string, // cred_id in the wallet\n         \"attrs\": {\"key1\":\"raw_value1\", \"key2\":\"raw_value2\"},\n         \"schema_id\": string,\n         \"cred_def_id\": string,\n         \"rev_reg_id\": Optional<string>,\n         \"cred_rev_id\": Optional<string>\n     }\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"prover_get_credential: >>> wallet_handle: %r, cred_id: %r\",\n                 wallet_handle,\n                 cred_id)\n\n    if not hasattr(prover_get_credential, \"cb\"):\n        logger.debug(\"prover_get_credential: Creating callback\")\n        prover_get_credential.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_cred_id = c_char_p(cred_id.encode('utf-8'))\n\n    credentials_json = await do_call('indy_prover_get_credential',\n                                     c_wallet_handle,\n                                     c_cred_id,\n                                     prover_get_credential.cb)\n\n    res = credentials_json.decode()\n    logger.debug(\"prover_get_credential: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def prover_get_credentials(wallet_handle: int,\n                                 filter_json: str) -> str:\n    \"\"\"\n    Gets human readable credentials according to the filter.\n    If filter is NULL, then all credentials are returned.\n    Credentials can be filtered by tags created during saving of credential.\n\n    NOTE: This method is deprecated because immediately returns all fetched credentials.\n    Use <prover_search_credentials> to fetch records by small batches.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param filter_json: filter for credentials\n        {\n            \"schema_id\": string, (Optional)\n            \"schema_issuer_did\": string, (Optional)\n            \"schema_name\": string, (Optional)\n            \"schema_version\": string, (Optional)\n            \"issuer_did\": string, (Optional)\n            \"cred_def_id\": string, (Optional)\n        }\n    :return:  credentials json\n     [{\n         \"referent\": string, // cred_id in the wallet\n         \"attrs\": {\"key1\":\"raw_value1\", \"key2\":\"raw_value2\"},\n         \"schema_id\": string,\n         \"cred_def_id\": string,\n         \"rev_reg_id\": Optional<string>,\n         \"cred_rev_id\": Optional<string>\n     }]\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"prover_get_credentials: >>> wallet_handle: %r, filter_json: %r\",\n                 wallet_handle,\n                 filter_json)\n\n    if not hasattr(prover_get_credentials, \"cb\"):\n        logger.debug(\"prover_get_credentials: Creating callback\")\n        prover_get_credentials.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_filter_json = c_char_p(filter_json.encode('utf-8'))\n\n    credentials_json = await do_call('indy_prover_get_credentials',\n                                     c_wallet_handle,\n                                     c_filter_json,\n                                     prover_get_credentials.cb)\n\n    res = credentials_json.decode()\n    logger.debug(\"prover_get_credentials: <<< res: %r\", res)\n    return res", "response": "Get human readable credentials according to the filter."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def prover_search_credentials(wallet_handle: int,\n                                    query_json: str) -> (int, int):\n    \"\"\"\n    Search for credentials stored in wallet.\n    Credentials can be filtered by tags created during saving of credential.\n\n    Instead of immediately returning of fetched credentials this call returns search_handle that can be used later\n    to fetch records by small batches (with prover_credentials_search_fetch_records).\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param query_json: wql style filter for credentials searching based on tags.\n        where wql query: indy-sdk/docs/design/011-wallet-query-language/README.md\n    :return:\n        search_handle: Search handle that can be used later to fetch records by small batches\n            (with prover_credentials_search_fetch_records)\n        total_count: Total count of records\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"prover_search_credentials: >>> wallet_handle: %r, query_json: %r\",\n                 wallet_handle,\n                 query_json)\n\n    if not hasattr(prover_search_credentials, \"cb\"):\n        logger.debug(\"prover_search_credentials: Creating callback\")\n        prover_search_credentials.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_int32, c_uint))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_query_json = c_char_p(query_json.encode('utf-8'))\n\n    res = await do_call('indy_prover_search_credentials',\n                        c_wallet_handle,\n                        c_query_json,\n                        prover_search_credentials.cb)\n\n    logger.debug(\"prover_search_credentials: <<< res: %r\", res)\n    return res", "response": "Search for credentials stored in wallet."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def prover_fetch_credentials(search_handle: int,\n                                   count: int) -> str:\n    \"\"\"\n    Fetch next credentials for search.\n\n    :param search_handle: Search handle (created by prover_open_credentials_search)\n    :param count: Count of records to fetch\n    :return: credentials_json: List of credentials:\n    [{\n        \"referent\": string, // cred_id in the wallet\n        \"attrs\": {\"key1\":\"raw_value1\", \"key2\":\"raw_value2\"},\n        \"schema_id\": string,\n        \"cred_def_id\": string,\n        \"rev_reg_id\": Optional<string>,\n        \"cred_rev_id\": Optional<string>\n    }]\n    NOTE: The list of length less than the requested count means credentials search iterator is completed.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"prover_fetch_credentials: >>> search_handle: %r, count: %r\",\n                 search_handle,\n                 count)\n\n    if not hasattr(prover_fetch_credentials, \"cb\"):\n        logger.debug(\"prover_fetch_credentials: Creating callback\")\n        prover_fetch_credentials.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_search_handle = c_int32(search_handle)\n    c_count = c_uint(count)\n\n    credentials_json = await do_call('indy_prover_fetch_credentials',\n                                     c_search_handle,\n                                     c_count,\n                                     prover_fetch_credentials.cb)\n\n    res = credentials_json.decode()\n    logger.debug(\"prover_fetch_credentials: <<< res: %r\", res)\n    return res", "response": "Fetch next credentials for a given wallet."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def prover_close_credentials_search(search_handle: int) -> None:\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"prover_close_credentials_search: >>> search_handle: %r\",\n                 search_handle)\n\n    if not hasattr(prover_close_credentials_search, \"cb\"):\n        logger.debug(\"prover_close_credentials_search: Creating callback\")\n        prover_close_credentials_search.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\n\n    c_search_handle = c_int32(search_handle)\n\n    res = await do_call('indy_prover_close_credentials_search',\n                        c_search_handle,\n                        prover_close_credentials_search.cb)\n\n    logger.debug(\"prover_close_credentials_search: <<< res: %r\", res)\n    return res", "response": "Close credentials search (make search handle invalid)\n\n    :param search_handle: Search handle (created by prover_open_credentials_search)\n    :return: None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def prover_get_credentials_for_proof_req(wallet_handle: int,\n                                               proof_request_json: str) -> str:\n    \"\"\"\n    Gets human readable credentials matching the given proof request.\n\n    NOTE: This method is deprecated because immediately returns all fetched credentials.\n    Use <prover_search_credentials_for_proof_req> to fetch records by small batches.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param proof_request_json: proof request json\n        {\n            \"name\": string,\n            \"version\": string,\n            \"nonce\": string,\n            \"requested_attributes\": { // set of requested attributes\n                 \"<attr_referent>\": <attr_info>, // see below\n                 ...,\n            },\n            \"requested_predicates\": { // set of requested predicates\n                 \"<predicate_referent>\": <predicate_info>, // see below\n                 ...,\n             },\n            \"non_revoked\": Optional<<non_revoc_interval>>, // see below,\n                           // If specified prover must proof non-revocation\n                           // for date in this interval for each attribute\n                           // (can be overridden on attribute level)\n        }\n    where:\n         attr_referent: Proof-request local identifier of requested attribute\n         attr_info: Describes requested attribute\n             {\n                 \"name\": string, // attribute name, (case insensitive and ignore spaces)\n                 \"restrictions\": Optional<[<filter_json>]>, // see above\n                                  // if specified, credential must satisfy to one of the given restriction.\n                 \"non_revoked\": Optional<<non_revoc_interval>>, // see below,\n                                // If specified prover must proof non-revocation\n                                // for date in this interval this attribute\n                                // (overrides proof level interval)\n             }\n         predicate_referent: Proof-request local identifier of requested attribute predicate\n         predicate_info: Describes requested attribute predicate\n             {\n                 \"name\": attribute name, (case insensitive and ignore spaces)\n                 \"p_type\": predicate type (Currently >= only)\n                 \"p_value\": predicate value\n                 \"restrictions\": Optional<[<filter_json>]>, // see above\n                                 // if specified, credential must satisfy to one of the given restriction.\n                 \"non_revoked\": Optional<<non_revoc_interval>>, // see below,\n                                // If specified prover must proof non-revocation\n                                // for date in this interval this attribute\n                                // (overrides proof level interval)\n             }\n         non_revoc_interval: Defines non-revocation interval\n             {\n                 \"from\": Optional<int>, // timestamp of interval beginning\n                 \"to\": Optional<int>, // timestamp of interval ending\n             }\n    :return: json with credentials for the given proof request.\n             {\n                 \"requested_attrs\": {\n                     \"<attr_referent>\": [{ cred_info: <credential_info>, interval: Optional<non_revoc_interval> }],\n                     ...,\n                 },\n                 \"requested_predicates\": {\n                     \"requested_predicates\": [{ cred_info: <credential_info>, timestamp: Optional<integer> }, { cred_info: <credential_2_info>, timestamp: Optional<integer> }],\n                     \"requested_predicate_2_referent\": [{ cred_info: <credential_2_info>, timestamp: Optional<integer> }]\n                 }\n             }, where credential is\n             {\n                 \"referent\": <string>,\n                 \"attrs\": [{\"attr_name\" : \"attr_raw_value\"}],\n                 \"schema_id\": string,\n                 \"cred_def_id\": string,\n                 \"rev_reg_id\": Optional<int>,\n                 \"cred_rev_id\": Optional<int>,\n             }\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"prover_get_credentials_for_proof_req: >>> wallet_handle: %r, proof_request_json: %r\",\n                 wallet_handle,\n                 proof_request_json)\n\n    if not hasattr(prover_get_credentials_for_proof_req, \"cb\"):\n        logger.debug(\"prover_get_credentials_for_proof_req: Creating callback\")\n        prover_get_credentials_for_proof_req.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_proof_request_json = c_char_p(proof_request_json.encode('utf-8'))\n\n    credentials_json = await do_call('indy_prover_get_credentials_for_proof_req',\n                                     c_wallet_handle,\n                                     c_proof_request_json,\n                                     prover_get_credentials_for_proof_req.cb)\n\n    res = credentials_json.decode()\n    logger.debug(\"prover_get_credentials_for_proof_req: <<< res: %r\", res)\n    return res", "response": "Get the human readable credential for a given proof request."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsearches for credentials matching the given proof request.", "response": "async def prover_search_credentials_for_proof_req(wallet_handle: int,\n                                                  proof_request_json: str,\n                                                  extra_query_json: Optional[str]) -> int:\n    \"\"\"\n    Search for credentials matching the given proof request.\n\n    Instead of immediately returning of fetched credentials this call returns search_handle that can be used later\n    to fetch records by small batches (with prover_fetch_credentials_for_proof_req).\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param proof_request_json: proof request json\n        {\n            \"name\": string,\n            \"version\": string,\n            \"nonce\": string,\n            \"requested_attributes\": { // set of requested attributes\n                 \"<attr_referent>\": <attr_info>, // see below\n                 ...,\n            },\n            \"requested_predicates\": { // set of requested predicates\n                 \"<predicate_referent>\": <predicate_info>, // see below\n                 ...,\n             },\n            \"non_revoked\": Optional<<non_revoc_interval>>, // see below,\n                           // If specified prover must proof non-revocation\n                           // for date in this interval for each attribute\n                           // (can be overridden on attribute level)\n        }\n    :param extra_query_json:(Optional) List of extra queries that will be applied to correspondent attribute/predicate:\n        {\n            \"<attr_referent>\": <wql query>,\n            \"<predicate_referent>\": <wql query>,\n        }\n        where wql query: indy-sdk/docs/design/011-wallet-query-language/README.md\n    :return: search_handle: Search handle that can be used later to fetch records by small batches (with prover_fetch_credentials_for_proof_req)\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"prover_search_credentials_for_proof_req: >>> wallet_handle: %r, proof_request_json: %r, \"\n                 \"extra_query_json: %r\",\n                 wallet_handle,\n                 proof_request_json,\n                 extra_query_json)\n\n    if not hasattr(prover_search_credentials_for_proof_req, \"cb\"):\n        logger.debug(\"prover_search_credentials_for_proof_req: Creating callback\")\n        prover_search_credentials_for_proof_req.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_int32))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_proof_request_json = c_char_p(proof_request_json.encode('utf-8'))\n    c_extra_query_json = c_char_p(extra_query_json.encode('utf-8')) if extra_query_json is not None else None\n\n    res = await do_call('indy_prover_search_credentials_for_proof_req',\n                        c_wallet_handle,\n                        c_proof_request_json,\n                        c_extra_query_json,\n                        prover_search_credentials_for_proof_req.cb)\n\n    logger.debug(\"prover_search_credentials_for_proof_req: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a proof according to the given proof request Either a corresponding credential with optionally revealed attributes or self-attested attribute must be provided for each requested attribute (see indy_prover_get_credentials_for_pool_req). A proof request may request multiple credentials from different schemas and different issuers. All required schemas, public keys and revocation registries must be provided. The proof request also contains nonce. The proof contains either proof or self-attested attribute value for each requested attribute. :param wallet_handle: wallet handler (created by open_wallet). :param proof_req_json: proof request json { \"name\": string, \"version\": string, \"nonce\": string, \"requested_attributes\": { // set of requested attributes \"<attr_referent>\": <attr_info>, // see below ..., }, \"requested_predicates\": { // set of requested predicates \"<predicate_referent>\": <predicate_info>, // see below ..., }, \"non_revoked\": Optional<<non_revoc_interval>>, // see below, // If specified prover must proof non-revocation // for date in this interval for each attribute // (can be overridden on attribute level) } :param requested_credentials_json: either a credential or self-attested attribute for each requested attribute { \"self_attested_attributes\": { \"self_attested_attribute_referent\": string }, \"requested_attributes\": { \"requested_attribute_referent_1\": {\"cred_id\": string, \"timestamp\": Optional<number>, revealed: <bool> }}, \"requested_attribute_referent_2\": {\"cred_id\": string, \"timestamp\": Optional<number>, revealed: <bool> }} }, \"requested_predicates\": { \"requested_predicates_referent_1\": {\"cred_id\": string, \"timestamp\": Optional<number> }}, } } :param master_secret_name: the id of the master secret stored in the wallet :param schemas_json: all schemas json participating in the proof request { <schema1_id>: <schema1_json>, <schema2_id>: <schema2_json>, <schema3_id>: <schema3_json>, } :param credential_defs_json: all credential definitions json participating in the proof request { \"cred_def1_id\": <credential_def1_json>, \"cred_def2_id\": <credential_def2_json>, \"cred_def3_id\": <credential_def3_json>, } :param rev_states_json: all revocation states json participating in the proof request { \"rev_reg_def1_id\": { \"timestamp1\": <rev_state1>, \"timestamp2\": <rev_state2>, }, \"rev_reg_def2_id\": { \"timestamp3\": <rev_state3> }, \"rev_reg_def3_id\": { \"timestamp4\": <rev_state4> }, } where wql query: indy-sdk/docs/design/011-wallet-query-language/README.md attr_referent: Proof-request local identifier of requested attribute attr_info: Describes requested attribute { \"name\": string, // attribute name, (case insensitive and ignore spaces) \"restrictions\": Optional<[<wql query>]>, // if specified, credential must satisfy to one of the given restriction. \"non_revoked\": Optional<<non_revoc_interval>>, // see below, // If specified prover must proof non-revocation // for date in this interval this attribute // (overrides proof level interval) } predicate_referent: Proof-request local identifier of requested attribute predicate predicate_info: Describes requested attribute predicate { \"name\": attribute name, (case insensitive and ignore spaces) \"p_type\": predicate type (Currently >= only) \"p_value\": predicate value \"restrictions\": Optional<[<wql query>]>, // if specified, credential must satisfy to one of the given restriction. \"non_revoked\": Optional<<non_revoc_interval>>, // see below, // If specified prover must proof non-revocation // for date in this interval this attribute // (overrides proof level interval) } non_revoc_interval: Defines non-revocation interval { \"from\": Optional<int>, // timestamp of interval beginning \"to\": Optional<int>, // timestamp of interval ending } :return: Proof json For each requested attribute either a proof (with optionally revealed attribute value) or self-attested attribute value is provided. Each proof is associated with a credential and corresponding schema_id, cred_def_id, rev_reg_id and timestamp. There is also aggregated proof part common for all credential proofs. { \"requested_proof\": { \"revealed_attrs\": { \"requested_attr1_id\": {sub_proof_index: number, raw: string, encoded: string}, \"requested_attr4_id\": {sub_proof_index: number: string, encoded: string}, }, \"unrevealed_attrs\": { \"requested_attr3_id\": {sub_proof_index: number} }, \"self_attested_attrs\": { \"requested_attr2_id\": self_attested_value, }, \"requested_predicates\": { \"requested_predicate_1_referent\": {sub_proof_index: int}, \"requested_predicate_2_referent\": {sub_proof_index: int}, } } \"proof\": { \"proofs\": [ <credential_proof>, <credential_proof>, <credential_proof> ], \"aggregated_proof\": <aggregated_proof> } \"identifiers\": [{schema_id, cred_def_id, Optional<rev_reg_id>, Optional<timestamp>}] }", "response": "async def prover_create_proof(wallet_handle: int,\n                              proof_req_json: str,\n                              requested_credentials_json: str,\n                              master_secret_name: str,\n                              schemas_json: str,\n                              credential_defs_json: str,\n                              rev_states_json: str) -> str:\n    \"\"\"\n    Creates a proof according to the given proof request\n    Either a corresponding credential with optionally revealed attributes or self-attested attribute must be provided\n    for each requested attribute (see indy_prover_get_credentials_for_pool_req).\n    A proof request may request multiple credentials from different schemas and different issuers.\n    All required schemas, public keys and revocation registries must be provided.\n    The proof request also contains nonce.\n    The proof contains either proof or self-attested attribute value for each requested attribute.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param proof_req_json: proof request json\n        {\n            \"name\": string,\n            \"version\": string,\n            \"nonce\": string,\n            \"requested_attributes\": { // set of requested attributes\n                 \"<attr_referent>\": <attr_info>, // see below\n                 ...,\n            },\n            \"requested_predicates\": { // set of requested predicates\n                 \"<predicate_referent>\": <predicate_info>, // see below\n                 ...,\n             },\n            \"non_revoked\": Optional<<non_revoc_interval>>, // see below,\n                           // If specified prover must proof non-revocation\n                           // for date in this interval for each attribute\n                           // (can be overridden on attribute level)\n        }\n    :param requested_credentials_json: either a credential or self-attested attribute for each requested attribute\n        {\n            \"self_attested_attributes\": {\n                \"self_attested_attribute_referent\": string\n            },\n            \"requested_attributes\": {\n                \"requested_attribute_referent_1\": {\"cred_id\": string, \"timestamp\": Optional<number>, revealed: <bool> }},\n                \"requested_attribute_referent_2\": {\"cred_id\": string, \"timestamp\": Optional<number>, revealed: <bool> }}\n            },\n            \"requested_predicates\": {\n                \"requested_predicates_referent_1\": {\"cred_id\": string, \"timestamp\": Optional<number> }},\n            }\n        }\n    :param master_secret_name: the id of the master secret stored in the wallet\n    :param schemas_json: all schemas json participating in the proof request\n          {\n              <schema1_id>: <schema1_json>,\n              <schema2_id>: <schema2_json>,\n              <schema3_id>: <schema3_json>,\n          }\n    :param credential_defs_json: all credential definitions json participating in the proof request\n          {\n              \"cred_def1_id\": <credential_def1_json>,\n              \"cred_def2_id\": <credential_def2_json>,\n              \"cred_def3_id\": <credential_def3_json>,\n          }\n    :param rev_states_json: all revocation states json participating in the proof request\n          {\n              \"rev_reg_def1_id\": {\n                  \"timestamp1\": <rev_state1>,\n                  \"timestamp2\": <rev_state2>,\n              },\n              \"rev_reg_def2_id\": {\n                  \"timestamp3\": <rev_state3>\n              },\n              \"rev_reg_def3_id\": {\n                  \"timestamp4\": <rev_state4>\n              },\n          }\n    where\n     wql query: indy-sdk/docs/design/011-wallet-query-language/README.md\n     attr_referent: Proof-request local identifier of requested attribute\n     attr_info: Describes requested attribute\n         {\n             \"name\": string, // attribute name, (case insensitive and ignore spaces)\n             \"restrictions\": Optional<[<wql query>]>,\n                              // if specified, credential must satisfy to one of the given restriction.\n             \"non_revoked\": Optional<<non_revoc_interval>>, // see below,\n                            // If specified prover must proof non-revocation\n                            // for date in this interval this attribute\n                            // (overrides proof level interval)\n         }\n     predicate_referent: Proof-request local identifier of requested attribute predicate\n     predicate_info: Describes requested attribute predicate\n         {\n             \"name\": attribute name, (case insensitive and ignore spaces)\n             \"p_type\": predicate type (Currently >= only)\n             \"p_value\": predicate value\n             \"restrictions\": Optional<[<wql query>]>,\n                             // if specified, credential must satisfy to one of the given restriction.\n             \"non_revoked\": Optional<<non_revoc_interval>>, // see below,\n                            // If specified prover must proof non-revocation\n                            // for date in this interval this attribute\n                            // (overrides proof level interval)\n         }\n     non_revoc_interval: Defines non-revocation interval\n         {\n             \"from\": Optional<int>, // timestamp of interval beginning\n             \"to\": Optional<int>, // timestamp of interval ending\n         }\n\n    :return: Proof json\n      For each requested attribute either a proof (with optionally revealed attribute value) or\n      self-attested attribute value is provided.\n      Each proof is associated with a credential and corresponding schema_id, cred_def_id, rev_reg_id and timestamp.\n      There is also aggregated proof part common for all credential proofs.\n          {\n              \"requested_proof\": {\n                  \"revealed_attrs\": {\n                      \"requested_attr1_id\": {sub_proof_index: number, raw: string, encoded: string},\n                      \"requested_attr4_id\": {sub_proof_index: number: string, encoded: string},\n                  },\n                  \"unrevealed_attrs\": {\n                      \"requested_attr3_id\": {sub_proof_index: number}\n                  },\n                  \"self_attested_attrs\": {\n                      \"requested_attr2_id\": self_attested_value,\n                  },\n                  \"requested_predicates\": {\n                      \"requested_predicate_1_referent\": {sub_proof_index: int},\n                      \"requested_predicate_2_referent\": {sub_proof_index: int},\n                  }\n              }\n              \"proof\": {\n                  \"proofs\": [ <credential_proof>, <credential_proof>, <credential_proof> ],\n                  \"aggregated_proof\": <aggregated_proof>\n              }\n              \"identifiers\": [{schema_id, cred_def_id, Optional<rev_reg_id>, Optional<timestamp>}]\n          }\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"prover_create_proof: >>> wallet_handle: %r, proof_req_json: %r, requested_credentials_json: %r, \"\n                 \"schemas_json: %r, master_secret_name: %r, credential_defs_json: %r, rev_infos_json: %r\",\n                 wallet_handle,\n                 proof_req_json,\n                 requested_credentials_json,\n                 schemas_json,\n                 master_secret_name,\n                 credential_defs_json,\n                 rev_states_json)\n\n    if not hasattr(prover_create_proof, \"cb\"):\n        logger.debug(\"prover_create_proof: Creating callback\")\n        prover_create_proof.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_proof_req_json = c_char_p(proof_req_json.encode('utf-8'))\n    c_requested_credentials_json = c_char_p(requested_credentials_json.encode('utf-8'))\n    c_schemas_json = c_char_p(schemas_json.encode('utf-8'))\n    c_master_secret_name = c_char_p(master_secret_name.encode('utf-8'))\n    c_credential_defs_json = c_char_p(credential_defs_json.encode('utf-8'))\n    c_rev_infos_json = c_char_p(rev_states_json.encode('utf-8'))\n\n    proof_json = await do_call('indy_prover_create_proof',\n                               c_wallet_handle,\n                               c_proof_req_json,\n                               c_requested_credentials_json,\n                               c_master_secret_name,\n                               c_schemas_json,\n                               c_credential_defs_json,\n                               c_rev_infos_json,\n                               prover_create_proof.cb)\n\n    res = proof_json.decode()\n    logger.debug(\"prover_create_proof: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nverifies a proof of multiple credential.", "response": "async def verifier_verify_proof(proof_request_json: str,\n                                proof_json: str,\n                                schemas_json: str,\n                                credential_defs_json: str,\n                                rev_reg_defs_json: str,\n                                rev_regs_json: str) -> bool:\n    \"\"\"\n    Verifies a proof (of multiple credential).\n    All required schemas, public keys and revocation registries must be provided.\n\n    :param proof_request_json: \n         {\n             \"name\": string,\n             \"version\": string,\n             \"nonce\": string,\n             \"requested_attributes\": { // set of requested attributes\n                  \"<attr_referent>\": <attr_info>, // see below\n                  ...,\n             },\n             \"requested_predicates\": { // set of requested predicates\n                  \"<predicate_referent>\": <predicate_info>, // see below\n                  ...,\n              },\n             \"non_revoked\": Optional<<non_revoc_interval>>, // see below,\n                            // If specified prover must proof non-revocation\n                            // for date in this interval for each attribute\n                            // (can be overridden on attribute level)\n         }\n    :param proof_json: created for request proof json\n         {\n             \"requested_proof\": {\n                 \"revealed_attrs\": {\n                     \"requested_attr1_id\": {sub_proof_index: number, raw: string, encoded: string},\n                     \"requested_attr4_id\": {sub_proof_index: number: string, encoded: string},\n                 },\n                 \"unrevealed_attrs\": {\n                     \"requested_attr3_id\": {sub_proof_index: number}\n                 },\n                 \"self_attested_attrs\": {\n                     \"requested_attr2_id\": self_attested_value,\n                 },\n                 \"requested_predicates\": {\n                     \"requested_predicate_1_referent\": {sub_proof_index: int},\n                     \"requested_predicate_2_referent\": {sub_proof_index: int},\n                 }\n             }\n             \"proof\": {\n                 \"proofs\": [ <credential_proof>, <credential_proof>, <credential_proof> ],\n                 \"aggregated_proof\": <aggregated_proof>\n             }\n             \"identifiers\": [{schema_id, cred_def_id, Optional<rev_reg_id>, Optional<timestamp>}]\n         }\n    :param schemas_json: all schema jsons participating in the proof\n         {\n             <schema1_id>: <schema1_json>,\n             <schema2_id>: <schema2_json>,\n             <schema3_id>: <schema3_json>,\n         }\n    :param credential_defs_json: all credential definitions json participating in the proof\n         {\n             \"cred_def1_id\": <credential_def1_json>,\n             \"cred_def2_id\": <credential_def2_json>,\n             \"cred_def3_id\": <credential_def3_json>,\n         }\n    :param rev_reg_defs_json: all revocation registry definitions json participating in the proof\n         {\n             \"rev_reg_def1_id\": <rev_reg_def1_json>,\n             \"rev_reg_def2_id\": <rev_reg_def2_json>,\n             \"rev_reg_def3_id\": <rev_reg_def3_json>,\n         }\n    :param rev_regs_json: all revocation registries json participating in the proof\n         {\n             \"rev_reg_def1_id\": {\n                 \"timestamp1\": <rev_reg1>,\n                 \"timestamp2\": <rev_reg2>,\n             },\n             \"rev_reg_def2_id\": {\n                 \"timestamp3\": <rev_reg3>\n             },\n             \"rev_reg_def3_id\": {\n                 \"timestamp4\": <rev_reg4>\n             },\n         }\n    :return: valid: true - if signature is valid, false - otherwise\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"verifier_verify_proof: >>> proof_request_json: %r, proof_json: %r, schemas_json: %r, \"\n                 \"credential_defs_jsons: %r, rev_reg_defs_json: %r, rev_regs_json: %r\",\n                 proof_request_json,\n                 proof_json,\n                 schemas_json,\n                 credential_defs_json,\n                 rev_reg_defs_json,\n                 rev_regs_json)\n\n    if not hasattr(verifier_verify_proof, \"cb\"):\n        logger.debug(\"verifier_verify_proof: Creating callback\")\n        verifier_verify_proof.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_bool))\n\n    c_proof_request_json = c_char_p(proof_request_json.encode('utf-8'))\n    c_proof_json = c_char_p(proof_json.encode('utf-8'))\n    c_schemas_json = c_char_p(schemas_json.encode('utf-8'))\n    c_credential_defs_json = c_char_p(credential_defs_json.encode('utf-8'))\n    c_rev_reg_defs_json = c_char_p(rev_reg_defs_json.encode('utf-8'))\n    c_rev_regs_json = c_char_p(rev_regs_json.encode('utf-8'))\n\n    res = await do_call('indy_verifier_verify_proof',\n                        c_proof_request_json,\n                        c_proof_json,\n                        c_schemas_json,\n                        c_credential_defs_json,\n                        c_rev_reg_defs_json,\n                        c_rev_regs_json,\n                        verifier_verify_proof.cb)\n\n    logger.debug(\"verifier_verify_proof: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def create_revocation_state(blob_storage_reader_handle: int,\n                                  rev_reg_def_json: str,\n                                  rev_reg_delta_json: str,\n                                  timestamp: int,\n                                  cred_rev_id: str) -> str:\n    \"\"\"\n    Create revocation state for a credential in the particular time moment.\n\n    :param blob_storage_reader_handle: configuration of blob storage reader handle that will allow to read revocation tails\n    :param rev_reg_def_json: revocation registry definition json\n    :param rev_reg_delta_json: revocation registry definition delta json\n    :param timestamp: time represented as a total number of seconds from Unix Epoch\n    :param cred_rev_id: user credential revocation id in revocation registry\n    :return: revocation state json {\n         \"rev_reg\": <revocation registry>,\n         \"witness\": <witness>,\n         \"timestamp\" : integer\n    }\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"create_revocation_info: >>> blob_storage_reader_handle: %r, rev_reg_def_json: %r,\"\n                 \" rev_reg_delta_json: %r, timestamp: %r, cred_rev_id: %r\",\n                 blob_storage_reader_handle,\n                 rev_reg_def_json,\n                 rev_reg_delta_json,\n                 timestamp,\n                 cred_rev_id)\n\n    if not hasattr(create_revocation_state, \"cb\"):\n        logger.debug(\"create_revocation_state: Creating callback\")\n        create_revocation_state.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_blob_storage_reader_handle = c_int32(blob_storage_reader_handle)\n    c_rev_reg_def_json = c_char_p(rev_reg_def_json.encode('utf-8'))\n    c_rev_reg_delta_json = c_char_p(rev_reg_delta_json.encode('utf-8'))\n    c_timestamp = c_uint64(timestamp)\n    c_cred_rev_id = c_char_p(cred_rev_id.encode('utf-8'))\n\n    rev_state_json = await do_call('indy_create_revocation_state',\n                                   c_blob_storage_reader_handle,\n                                   c_rev_reg_def_json,\n                                   c_rev_reg_delta_json,\n                                   c_timestamp,\n                                   c_cred_rev_id,\n                                   create_revocation_state.cb)\n\n    res = rev_state_json.decode()\n    logger.debug(\"create_revocation_state: <<< res: %r\", res)\n    return res", "response": "Create revocation state for a user credential in the particular time moment."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def sign_and_submit_request(pool_handle: int,\n                                  wallet_handle: int,\n                                  submitter_did: str,\n                                  request_json: str) -> str:\n    \"\"\"\n    Signs and submits request message to validator pool.\n\n    Adds submitter information to passed request json, signs it with submitter\n    sign key (see wallet_sign), and sends signed request message\n    to validator pool (see write_request).\n\n    :param pool_handle: pool handle (created by open_pool_ledger).\n    :param wallet_handle: wallet handle (created by open_wallet).\n    :param submitter_did: Id of Identity stored in secured Wallet.\n    :param request_json: Request data json.\n    :return: Request result as json.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"sign_and_submit_request: >>> pool_handle: %r, wallet_handle: %r, submitter_did: %r, request_json: %r\",\n                 pool_handle,\n                 wallet_handle,\n                 submitter_did,\n                 request_json)\n\n    if not hasattr(sign_and_submit_request, \"cb\"):\n        logger.debug(\"sign_and_submit_request: Creating callback\")\n        sign_and_submit_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_pool_handle = c_int32(pool_handle)\n    c_wallet_handle = c_int32(wallet_handle)\n    c_submitter_did = c_char_p(submitter_did.encode('utf-8'))\n    c_request_json = c_char_p(request_json.encode('utf-8'))\n\n    request_result = await do_call('indy_sign_and_submit_request',\n                                   c_pool_handle,\n                                   c_wallet_handle,\n                                   c_submitter_did,\n                                   c_request_json,\n                                   sign_and_submit_request.cb)\n\n    res = request_result.decode()\n    logger.debug(\"sign_and_submit_request: <<< res: %r\", res)\n    return res", "response": "Signs and submits request to validator pool."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def submit_request(pool_handle: int,\n                         request_json: str) -> str:\n    \"\"\"\n    Publishes request message to validator pool (no signing, unlike sign_and_submit_request).\n    The request is sent to the validator pool as is. It's assumed that it's already prepared.\n\n    :param pool_handle: pool handle (created by open_pool_ledger).\n    :param request_json: Request data json.\n    :return: Request result as json.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"submit_request: >>> pool_handle: %r, request_json: %r\",\n                 pool_handle,\n                 request_json)\n\n    if not hasattr(submit_request, \"cb\"):\n        logger.debug(\"submit_request: Creating callback\")\n        submit_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_pool_handle = c_int32(pool_handle)\n    c_request_json = c_char_p(request_json.encode('utf-8'))\n\n    request_result = await do_call('indy_submit_request',\n                                   c_pool_handle,\n                                   c_request_json,\n                                   submit_request.cb)\n\n    res = request_result.decode()\n    logger.debug(\"submit_request: <<< res: %r\", res)\n    return res", "response": "Submit request to validator pool."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend action to specific nodes of validator pool.", "response": "async def submit_action(pool_handle: int,\n                        request_json: str,\n                        nodes: Optional[str],\n                        timeout: Optional[int]) -> str:\n    \"\"\"\n    Send action to particular nodes of validator pool.\n    \n    The list of requests can be send:\n        POOL_RESTART\n        GET_VALIDATOR_INFO\n   \n    The request is sent to the nodes as is. It's assumed that it's already prepared.\n\n    :param pool_handle: pool handle (created by open_pool_ledger).\n    :param request_json: Request data json.\n    :param nodes: (Optional) List of node names to send the request.\n           [\"Node1\", \"Node2\",....\"NodeN\"]\n    :param timeout: (Optional) Time to wait respond from nodes (override the default timeout) (in sec).\n    :return: Request result as json.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"submit_action: >>> pool_handle: %r, request_json: %r, nodes: %r, timeout: %r\",\n                 pool_handle,\n                 request_json,\n                 nodes,\n                 timeout)\n\n    if not hasattr(submit_action, \"cb\"):\n        logger.debug(\"submit_action: Creating callback\")\n        submit_action.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_pool_handle = c_int32(pool_handle)\n    c_request_json = c_char_p(request_json.encode('utf-8'))\n    c_nodes = c_char_p(nodes.encode('utf-8')) if nodes is not None else None\n    c_timeout = c_int32(timeout) if timeout is not None else None\n\n    request_result = await do_call('indy_submit_action',\n                                   c_pool_handle,\n                                   c_request_json,\n                                   c_nodes,\n                                   c_timeout,\n                                   submit_action.cb)\n\n    res = request_result.decode()\n    logger.debug(\"submit_action: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def build_get_ddo_request(submitter_did: Optional[str],\n                                target_did: str) -> str:\n    \"\"\"\n    Builds a request to get a DDO.\n\n    :param submitter_did: (Optional) DID of the read request sender (if not provided then default Libindy DID will be used).\n    :param target_did: Id of Identity stored in secured Wallet.\n    :return: Request result as json.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"build_get_ddo_request: >>> submitter_did: %r, target_did: %r\",\n                 submitter_did,\n                 target_did)\n\n    if not hasattr(build_get_ddo_request, \"cb\"):\n        logger.debug(\"build_get_ddo_request: Creating callback\")\n        build_get_ddo_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_submitter_did = c_char_p(submitter_did.encode('utf-8')) if submitter_did is not None else None\n    c_target_did = c_char_p(target_did.encode('utf-8'))\n\n    request_json = await do_call('indy_build_get_ddo_request',\n                                 c_submitter_did,\n                                 c_target_did,\n                                 build_get_ddo_request.cb)\n\n    res = request_json.decode()\n    logger.debug(\"build_get_ddo_request: <<< res: %r\", res)\n    return res", "response": "Builds a request to get a DDO."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def build_nym_request(submitter_did: str,\n                            target_did: str,\n                            ver_key: Optional[str],\n                            alias: Optional[str],\n                            role: Optional[str]) -> str:\n    \"\"\"\n    Builds a NYM request.\n\n    :param submitter_did: DID of the submitter stored in secured Wallet.\n    :param target_did: Target DID as base58-encoded string for 16 or 32 bit DID value.\n    :param ver_key: Target identity verification key as base58-encoded string.\n    :param alias: NYM's alias.\n    :param role: Role of a user NYM record:\n                             null (common USER)\n                             TRUSTEE\n                             STEWARD\n                             TRUST_ANCHOR\n                             NETWORK_MONITOR\n                             empty string to reset role\n    :return: Request result as json.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"build_nym_request: >>> submitter_did: %r, target_did: %r, ver_key: %r, alias: %r, role: %r\",\n                 submitter_did,\n                 target_did,\n                 ver_key,\n                 alias,\n                 role)\n\n    if not hasattr(build_nym_request, \"cb\"):\n        logger.debug(\"build_nym_request: Creating callback\")\n        build_nym_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_submitter_did = c_char_p(submitter_did.encode('utf-8'))\n    c_target_did = c_char_p(target_did.encode('utf-8'))\n    c_ver_key = c_char_p(ver_key.encode('utf-8')) if ver_key is not None else None\n    c_alias = c_char_p(alias.encode('utf-8')) if alias is not None else None\n    c_role = c_char_p(role.encode('utf-8')) if role is not None else None\n\n    request_json = await do_call('indy_build_nym_request',\n                                 c_submitter_did,\n                                 c_target_did,\n                                 c_ver_key,\n                                 c_alias,\n                                 c_role,\n                                 build_nym_request.cb)\n\n    res = request_json.decode()\n    logger.debug(\"build_nym_request: <<< res: %r\", res)\n    return res", "response": "Builds a NYM request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding an ATTRIB request. Request to add attribute to a NYM record.", "response": "async def build_attrib_request(submitter_did: str,\n                               target_did: str,\n                               xhash: Optional[str],\n                               raw: Optional[str],\n                               enc: Optional[str]) -> str:\n    \"\"\"\n    Builds an ATTRIB request. Request to add attribute to a NYM record.\n\n    :param submitter_did: DID of the submitter stored in secured Wallet.\n    :param target_did: Target DID as base58-encoded string for 16 or 32 bit DID value.\n    :param xhash: (Optional) Hash of attribute data.\n    :param raw: (Optional) Json, where key is attribute name and value is attribute value.\n    :param enc: (Optional) Encrypted value attribute data.\n    :return: Request result as json.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"build_attrib_request: >>> submitter_did: %r, target_did: %r, hash: %r, raw: %r, enc: %r\",\n                 submitter_did,\n                 target_did,\n                 xhash,\n                 raw,\n                 enc)\n\n    if not hasattr(build_attrib_request, \"cb\"):\n        logger.debug(\"build_attrib_request: Creating callback\")\n        build_attrib_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_submitter_did = c_char_p(submitter_did.encode('utf-8'))\n    c_target_did = c_char_p(target_did.encode('utf-8'))\n    c_hash = c_char_p(xhash.encode('utf-8')) if xhash is not None else None\n    c_raw = c_char_p(raw.encode('utf-8')) if raw is not None else None\n    c_enc = c_char_p(enc.encode('utf-8')) if enc is not None else None\n\n    request_json = await do_call('indy_build_attrib_request',\n                                 c_submitter_did,\n                                 c_target_did,\n                                 c_hash,\n                                 c_raw,\n                                 c_enc,\n                                 build_attrib_request.cb)\n\n    res = request_json.decode()\n    logger.debug(\"build_attrib_request: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a GET_SCHEMA response to get Schema in the format compatible with Anoncreds API.", "response": "async def parse_get_schema_response(get_schema_response: str) -> (str, str):\n    \"\"\"\n    Parse a GET_SCHEMA response to get Schema in the format compatible with Anoncreds API\n\n    :param get_schema_response: response of GET_SCHEMA request.\n    :return: Schema Id and Schema json.\n     {\n         id: identifier of schema\n         attrNames: array of attribute name strings\n         name: Schema's name string\n         version: Schema's version string\n         ver: Version of the Schema json\n     }\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"parse_get_schema_response: >>> get_schema_response: %r\", get_schema_response)\n\n    if not hasattr(parse_get_schema_response, \"cb\"):\n        logger.debug(\"parse_get_schema_response: Creating callback\")\n        parse_get_schema_response.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p))\n\n    c_get_schema_response = c_char_p(get_schema_response.encode('utf-8'))\n\n    (schema_id, schema_json) = await do_call('indy_parse_get_schema_response',\n                                             c_get_schema_response,\n                                             parse_get_schema_response.cb)\n\n    res = (schema_id.decode(), schema_json.decode())\n    logger.debug(\"parse_get_schema_response: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def build_get_cred_def_request(submitter_did: Optional[str],\n                                     id_: str) -> str:\n    \"\"\"\n   Builds a GET_CRED_DEF request. Request to get a credential definition (in particular, public key),\n   that Issuer creates for a particular Credential Schema.\n\n    :param submitter_did: (Optional) DID of the read request sender (if not provided then default Libindy DID will be used).\n    :param id_: Credential Definition Id in ledger.\n    :return: Request result as json.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"build_get_cred_def_request: >>> submitter_did: %r, id: %r\",\n                 submitter_did,\n                 id_)\n\n    if not hasattr(build_get_cred_def_request, \"cb\"):\n        logger.debug(\"build_get_cred_def_request: Creating callback\")\n        build_get_cred_def_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_submitter_did = c_char_p(submitter_did.encode('utf-8')) if submitter_did is not None else None\n    c_id = c_char_p(id_.encode('utf-8'))\n\n    request_json = await do_call('indy_build_get_cred_def_request',\n                                 c_submitter_did,\n                                 c_id,\n                                 build_get_cred_def_request.cb)\n\n    res = request_json.decode()\n    logger.debug(\"build_get_cred_def_request: <<< res: %r\", res)\n    return res", "response": "Builds a GET_CRED_DEF request. Request to get a credential definition."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a GET_CRED_DEF response to get Credential Definition Id and Credential Definition json.", "response": "async def parse_get_cred_def_response(get_cred_def_response: str) -> (str, str):\n    \"\"\"\n    Parse a GET_CRED_DEF response to get Credential Definition in the format compatible with Anoncreds API.\n\n    :param get_cred_def_response: response of GET_CRED_DEF request.\n    :return: Credential Definition Id and Credential Definition json.\n      {\n          id: string - identifier of credential definition\n          schemaId: string - identifier of stored in ledger schema\n          type: string - type of the credential definition. CL is the only supported type now.\n          tag: string - allows to distinct between credential definitions for the same issuer and schema\n          value: Dictionary with Credential Definition's data: {\n              primary: primary credential public key,\n              Optional<revocation>: revocation credential public key\n          },\n          ver: Version of the Credential Definition json\n      }\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"parse_get_cred_def_response: >>> get_cred_def_response: %r\", get_cred_def_response)\n\n    if not hasattr(parse_get_cred_def_response, \"cb\"):\n        logger.debug(\"parse_get_cred_def_response: Creating callback\")\n        parse_get_cred_def_response.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p))\n\n    c_get_cred_def_response = c_char_p(get_cred_def_response.encode('utf-8'))\n\n    (cred_def_id, cred_def_json) = await do_call('indy_parse_get_cred_def_response',\n                                                 c_get_cred_def_response,\n                                                 parse_get_cred_def_response.cb)\n\n    res = (cred_def_id.decode(), cred_def_json.decode())\n    logger.debug(\"parse_get_cred_def_response: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a GET_VALIDATOR_INFO request.", "response": "async def build_get_validator_info_request(submitter_did: str) -> str:\n    \"\"\"\n    Builds a GET_VALIDATOR_INFO request.\n    :param submitter_did: Id of Identity stored in secured Wallet.\n    :return: Request result as json.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"build_get_validator_info_request: >>> submitter_did: %r\", submitter_did)\n\n    if not hasattr(build_get_validator_info_request, \"cb\"):\n        logger.debug(\"build_get_validator_info_request: Creating callback\")\n        build_get_validator_info_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_submitter_did = c_char_p(submitter_did.encode('utf-8'))\n\n    request_json = await do_call('indy_build_get_validator_info_request',\n                                 c_submitter_did,\n                                 build_get_validator_info_request.cb)\n\n    res = request_json.decode()\n    logger.debug(\"build_get_validator_info_request: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def build_get_txn_request(submitter_did: Optional[str],\n                                ledger_type: Optional[str],\n                                seq_no: int) -> str:\n    \"\"\"\n    Builds a GET_TXN request. Request to get any transaction by its seq_no.\n\n    :param submitter_did: (Optional) DID of the read request sender (if not provided then default Libindy DID will be used).\n    :param ledger_type: (Optional) type of the ledger the requested transaction belongs to:\n        DOMAIN - used default,\n        POOL,\n        CONFIG\n        any number\n    :param seq_no: requested transaction sequence number as it's stored on Ledger.\n    :return: Request result as json.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"build_get_txn_request: >>> submitter_did: %r, ledger_type: %r, seq_no: %r\",\n                 submitter_did,\n                 ledger_type,\n                 seq_no)\n\n    if not hasattr(build_get_txn_request, \"cb\"):\n        logger.debug(\"build_get_txn_request: Creating callback\")\n        build_get_txn_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_submitter_did = c_char_p(submitter_did.encode('utf-8')) if submitter_did is not None else None\n    c_ledger_type = c_char_p(ledger_type.encode('utf-8')) if ledger_type is not None else None\n    c_seq_no = c_int32(seq_no)\n\n    request_json = await do_call('indy_build_get_txn_request',\n                                 c_submitter_did,\n                                 c_ledger_type,\n                                 c_seq_no,\n                                 build_get_txn_request.cb)\n\n    res = request_json.decode()\n    logger.debug(\"build_get_txn_request: <<< res: %r\", res)\n    return res", "response": "Builds a GET_TXN request. Request to get any transaction by its seq_no."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a POOL_CONFIG request. Request to change Pool s configuration.", "response": "async def build_pool_config_request(submitter_did: str,\n                                    writes: bool,\n                                    force: bool) -> str:\n    \"\"\"\n    Builds a POOL_CONFIG request. Request to change Pool's configuration.\n\n    :param submitter_did: DID of the submitter stored in secured Wallet.\n    :param writes: Whether any write requests can be processed by the pool\n                   (if false, then pool goes to read-only state). True by default.\n    :param force: Whether we should apply transaction (for example, move pool to read-only state)\n                  without waiting for consensus of this transaction\n    :return: Request result as json.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"build_pool_config_request: >>> submitter_did: %r, writes: %r, force: %r\",\n                 submitter_did,\n                 writes,\n                 force)\n\n    if not hasattr(build_pool_config_request, \"cb\"):\n        logger.debug(\"build_pool_config_request: Creating callback\")\n        build_pool_config_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_submitter_did = c_char_p(submitter_did.encode('utf-8'))\n    c_writes = c_bool(writes)\n    c_force = c_bool(force)\n\n    request_json = await do_call('indy_build_pool_config_request',\n                                 c_submitter_did,\n                                 c_writes,\n                                 c_force,\n                                 build_pool_config_request.cb)\n\n    res = request_json.decode()\n    logger.debug(\"build_pool_config_request: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild a POOL_RESTART request.", "response": "async def build_pool_restart_request(submitter_did: str, action: str, datetime: str) -> str:\n    \"\"\"\n    Builds a POOL_RESTART request\n\n    :param submitter_did: Id of Identity that sender transaction\n    :param action       : Action that pool has to do after received transaction.\n                          Can be \"start\" or \"cancel\"\n    :param datetime           : Time when pool must be restarted.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"build_pool_restart_request: >>> submitter_did: %r, action: %r, datetime: %r\")\n\n    if not hasattr(build_pool_restart_request, \"cb\"):\n        logger.debug(\"build_pool_restart_request: Creating callback\")\n        build_pool_restart_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_submitter_did = c_char_p(submitter_did.encode('utf-8'))\n    c_action = c_char_p(action.encode('utf-8'))\n    c_datetime = c_char_p(datetime.encode('utf-8')) if datetime else None\n\n    request_json = await do_call('indy_build_pool_restart_request',\n                                 c_submitter_did,\n                                 c_action,\n                                 c_datetime,\n                                 build_pool_restart_request.cb)\n\n    res = request_json.decode()\n    logger.debug(\"build_pool_upgrade_request: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a POOL_UPGRADE request.", "response": "async def build_pool_upgrade_request(submitter_did: str,\n                                     name: str,\n                                     version: str,\n                                     action: str,\n                                     _sha256: str,\n                                     _timeout: Optional[int],\n                                     schedule: Optional[str],\n                                     justification: Optional[str],\n                                     reinstall: bool,\n                                     force: bool,\n                                     package: Optional[str]) -> str:\n    \"\"\"\n    Builds a POOL_UPGRADE request. Request to upgrade the Pool (sent by Trustee).\n    It upgrades the specified Nodes (either all nodes in the Pool, or some specific ones).\n\n    :param submitter_did: DID of the submitter stored in secured Wallet.\n    :param name: Human-readable name for the upgrade.\n    :param version: The version of indy-node package we perform upgrade to.\n                    Must be greater than existing one (or equal if reinstall flag is True).\n    :param action: Either start or cancel.\n    :param _sha256: sha256 hash of the package.\n    :param _timeout: (Optional) Limits upgrade time on each Node.\n    :param schedule: (Optional) Schedule of when to perform upgrade on each node. Map Node DIDs to upgrade time.\n    :param justification: (Optional) justification string for this particular Upgrade.\n    :param reinstall: Whether it's allowed to re-install the same version. False by default.\n    :param force: Whether we should apply transaction (schedule Upgrade) without waiting\n                  for consensus of this transaction.\n    :param package: (Optional) Package to be upgraded.\n    :return: Request result as json.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"build_pool_upgrade_request: >>> submitter_did: %r, name: %r, version: %r, action: %r, _sha256: %r, \"\n                 \"timeout: %r, schedule: %r, justification: %r, reinstall: %r, force: %r, package: %r\",\n                 submitter_did, name, version, action, _sha256, _timeout, schedule, justification, reinstall, force,\n                 package)\n\n    if not hasattr(build_pool_upgrade_request, \"cb\"):\n        logger.debug(\"build_pool_upgrade_request: Creating callback\")\n        build_pool_upgrade_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_submitter_did = c_char_p(submitter_did.encode('utf-8'))\n    c_name = c_char_p(name.encode('utf-8'))\n    c_version = c_char_p(version.encode('utf-8'))\n    c_action = c_char_p(action.encode('utf-8'))\n    c_sha256 = c_char_p(_sha256.encode('utf-8'))\n    c_timeout = c_int32(_timeout) if _timeout else c_int32(-1)\n    c_schedule = c_char_p(schedule.encode('utf-8')) if schedule is not None else None\n    c_justification = c_char_p(justification.encode('utf-8')) if justification is not None else None\n    c_reinstall = c_bool(reinstall)\n    c_force = c_bool(force)\n    c_package = c_char_p(package.encode('utf-8')) if package is not None else None\n\n    request_json = await do_call('indy_build_pool_upgrade_request',\n                                 c_submitter_did,\n                                 c_name,\n                                 c_version,\n                                 c_action,\n                                 c_sha256,\n                                 c_timeout,\n                                 c_schedule,\n                                 c_justification,\n                                 c_reinstall,\n                                 c_force,\n                                 c_package,\n                                 build_pool_upgrade_request.cb)\n\n    res = request_json.decode()\n    logger.debug(\"build_pool_upgrade_request: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild a GET_REVOC_REG_DEF request. Request to get a revocation registry definition.", "response": "async def build_get_revoc_reg_def_request(submitter_did: Optional[str],\n                                          rev_reg_def_id: str) -> str:\n    \"\"\"\n    Builds a GET_REVOC_REG_DEF request. Request to get a revocation registry definition,\n    that Issuer creates for a particular Credential Definition.\n\n    :param submitter_did: (Optional) DID of the read request sender (if not provided then default Libindy DID will be used).\n    :param rev_reg_def_id: ID of Revocation Registry Definition in ledger.\n\n    :return: Request result as json.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"build_get_revoc_reg_def_request: >>> submitter_did: %r, rev_reg_def_id: %r\", submitter_did,\n                 rev_reg_def_id)\n\n    if not hasattr(build_get_revoc_reg_def_request, \"cb\"):\n        logger.debug(\"build_get_revoc_reg_def_request: Creating callback\")\n        build_get_revoc_reg_def_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_submitter_did = c_char_p(submitter_did.encode('utf-8')) if submitter_did is not None else None\n    c_rev_reg_def_id = c_char_p(rev_reg_def_id.encode('utf-8'))\n\n    request_json = await do_call('indy_build_get_revoc_reg_def_request',\n                                 c_submitter_did,\n                                 c_rev_reg_def_id,\n                                 build_get_revoc_reg_def_request.cb)\n\n    res = request_json.decode()\n    logger.debug(\"build_get_revoc_reg_def_request: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a GET_REVOC_REG_DEF response to get Revocation Registry Definition Id and Revocation Registry Definition json.", "response": "async def parse_get_revoc_reg_def_response(get_revoc_ref_def_response: str) -> (str, str):\n    \"\"\"\n    Parse a GET_REVOC_REG_DEF response to get Revocation Registry Definition in the format compatible with Anoncreds API.\n\n    :param get_revoc_ref_def_response: response of GET_REVOC_REG_DEF request.\n    :return: Revocation Registry Definition Id and Revocation Registry Definition json.\n      {\n          \"id\": string - ID of the Revocation Registry,\n          \"revocDefType\": string - Revocation Registry type (only CL_ACCUM is supported for now),\n          \"tag\": string - Unique descriptive ID of the Registry,\n          \"credDefId\": string - ID of the corresponding CredentialDefinition,\n          \"value\": Registry-specific data {\n              \"issuanceType\": string - Type of Issuance(ISSUANCE_BY_DEFAULT or ISSUANCE_ON_DEMAND),\n              \"maxCredNum\": number - Maximum number of credentials the Registry can serve.\n              \"tailsHash\": string - Hash of tails.\n              \"tailsLocation\": string - Location of tails file.\n              \"publicKeys\": <public_keys> - Registry's public key.\n          },\n          \"ver\": string - version of revocation registry definition json.\n      }\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"parse_get_revoc_reg_def_response: >>> get_revoc_ref_def_response: %r\", get_revoc_ref_def_response)\n\n    if not hasattr(parse_get_revoc_reg_def_response, \"cb\"):\n        logger.debug(\"parse_get_revoc_reg_def_response: Creating callback\")\n        parse_get_revoc_reg_def_response.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p))\n\n    c_get_revoc_ref_def_response = c_char_p(get_revoc_ref_def_response.encode('utf-8'))\n\n    (revoc_reg_def_id, revoc_reg_def_json) = await do_call('indy_parse_get_revoc_reg_def_response',\n                                                           c_get_revoc_ref_def_response,\n                                                           parse_get_revoc_reg_def_response.cb)\n\n    res = (revoc_reg_def_id.decode(), revoc_reg_def_json.decode())\n    logger.debug(\"parse_get_revoc_reg_def_response: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def build_revoc_reg_entry_request(submitter_did: str,\n                                        revoc_reg_def_id: str,\n                                        rev_def_type: str,\n                                        value: str) -> str:\n    \"\"\"\n    Builds a REVOC_REG_ENTRY request.  Request to add the RevocReg entry containing\n    the new accumulator value and issued/revoked indices.\n    This is just a delta of indices, not the whole list. So, it can be sent each time a new credential is issued/revoked.\n\n    :param submitter_did: DID of the submitter stored in secured Wallet.\n    :param revoc_reg_def_id:  ID of the corresponding RevocRegDef.\n    :param rev_def_type:  Revocation Registry type (only CL_ACCUM is supported for now).\n    :param value: Registry-specific data: \n       {\n           value: {\n               prevAccum: string - previous accumulator value.\n               accum: string - current accumulator value.\n               issued: array<number> - an array of issued indices.\n               revoked: array<number> an array of revoked indices.\n           },\n           ver: string - version revocation registry entry json\n      \n       }\n    :return: Request result as json.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"build_revoc_reg_entry_request: >>> submitter_did: %r, rev_def_type: %r, revoc_reg_def_id: %r, \"\n                 \"value: %r\", submitter_did, rev_def_type, revoc_reg_def_id, value)\n\n    if not hasattr(build_revoc_reg_entry_request, \"cb\"):\n        logger.debug(\"build_revoc_reg_entry_request: Creating callback\")\n        build_revoc_reg_entry_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_submitter_did = c_char_p(submitter_did.encode('utf-8'))\n    c_rev_def_type = c_char_p(rev_def_type.encode('utf-8'))\n    c_revoc_reg_def_id = c_char_p(revoc_reg_def_id.encode('utf-8'))\n    c_value = c_char_p(value.encode('utf-8'))\n\n    request_json = await do_call('indy_build_revoc_reg_entry_request',\n                                 c_submitter_did,\n                                 c_revoc_reg_def_id,\n                                 c_rev_def_type,\n                                 c_value,\n                                 build_revoc_reg_entry_request.cb)\n\n    res = request_json.decode()\n    logger.debug(\"build_revoc_reg_entry_request: <<< res: %r\", res)\n    return res", "response": "Builds a REVOC_REG_ENTRY request. Request to add a Revocation Registry entry to the list of revocation registry entries."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def build_get_revoc_reg_request(submitter_did: Optional[str],\n                                      revoc_reg_def_id: str,\n                                      timestamp: int) -> str:\n    \"\"\"\n    Builds a GET_REVOC_REG request. Request to get the accumulated state of the Revocation Registry\n    by ID. The state is defined by the given timestamp.\n\n    :param submitter_did: (Optional) DID of the read request sender (if not provided then default Libindy DID will be used).\n    :param revoc_reg_def_id:  ID of the corresponding Revocation Registry Definition in ledger.\n    :param timestamp: Requested time represented as a total number of seconds from Unix Epoch\n    :return: Request result as json.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"build_get_revoc_reg_request: >>> submitter_did: %r, revoc_reg_def_id: %r, timestamp: %r\",\n                 submitter_did, revoc_reg_def_id, timestamp)\n\n    if not hasattr(build_get_revoc_reg_request, \"cb\"):\n        logger.debug(\"build_get_revoc_reg_request: Creating callback\")\n        build_get_revoc_reg_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_submitter_did = c_char_p(submitter_did.encode('utf-8')) if submitter_did is not None else None\n    c_revoc_reg_def_id = c_char_p(revoc_reg_def_id.encode('utf-8'))\n    c_timestamp = c_int64(timestamp)\n\n    request_json = await do_call('indy_build_get_revoc_reg_request',\n                                 c_submitter_did,\n                                 c_revoc_reg_def_id,\n                                 c_timestamp,\n                                 build_get_revoc_reg_request.cb)\n\n    res = request_json.decode()\n    logger.debug(\"build_get_revoc_reg_request: <<< res: %r\", res)\n    return res", "response": "Builds a GET_REVOC_REG request. Request to get the accumulated state of the Revocation Registry by ID."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a GET_REVOC_REG response to get Revocation Registry in the format compatible with Anoncreds API.", "response": "async def parse_get_revoc_reg_response(get_revoc_reg_response: str) -> (str, str, int):\n    \"\"\"\n    Parse a GET_REVOC_REG response to get Revocation Registry in the format compatible with Anoncreds API.\n\n    :param get_revoc_reg_response: response of GET_REVOC_REG request.\n    :return: Revocation Registry Definition Id, Revocation Registry json and Timestamp.\n      {\n          \"value\": Registry-specific data {\n              \"accum\": string - current accumulator value.\n          },\n          \"ver\": string - version revocation registry json\n      }\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"parse_get_revoc_reg_response: >>> get_revoc_reg_response: %r\", get_revoc_reg_response)\n\n    if not hasattr(parse_get_revoc_reg_response, \"cb\"):\n        logger.debug(\"parse_get_revoc_reg_response: Creating callback\")\n        parse_get_revoc_reg_response.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p, c_uint64))\n\n    c_get_revoc_reg_response = c_char_p(get_revoc_reg_response.encode('utf-8'))\n\n    (revoc_reg_def_id, revoc_reg_json, timestamp) = await do_call('indy_parse_get_revoc_reg_response',\n                                                                  c_get_revoc_reg_response,\n                                                                  parse_get_revoc_reg_response.cb)\n\n    res = (revoc_reg_def_id.decode(), revoc_reg_json.decode(), timestamp)\n    logger.debug(\"parse_get_revoc_reg_response: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def build_get_revoc_reg_delta_request(submitter_did: Optional[str],\n                                            revoc_reg_def_id: str,\n                                            from_: Optional[int],\n                                            to: int) -> str:\n    \"\"\"\n    Builds a GET_REVOC_REG_DELTA request. Request to get the delta of the accumulated state of the Revocation Registry.\n    The Delta is defined by from and to timestamp fields.\n    If from is not specified, then the whole state till to will be returned.\n\n    :param submitter_did: (Optional) DID of the read request sender (if not provided then default Libindy DID will be used).\n    :param revoc_reg_def_id:  ID of the corresponding Revocation Registry Definition in ledger.\n    :param from_: Requested time represented as a total number of seconds from Unix Epoch\n    :param to: Requested time represented as a total number of seconds from Unix Epoch\n    :return: Request result as json.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"build_get_revoc_reg_delta_request: >>> submitter_did: %r, revoc_reg_def_id: %r, from: %r, to: %r\",\n                 submitter_did, revoc_reg_def_id, from_, to)\n\n    if not hasattr(build_get_revoc_reg_delta_request, \"cb\"):\n        logger.debug(\"build_get_revoc_reg_delta_request: Creating callback\")\n        build_get_revoc_reg_delta_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_submitter_did = c_char_p(submitter_did.encode('utf-8')) if submitter_did is not None else None\n    c_revoc_reg_def_id = c_char_p(revoc_reg_def_id.encode('utf-8'))\n    c_from = c_int64(from_) if from_  else -1\n    c_to = c_int64(to)\n\n    request_json = await do_call('indy_build_get_revoc_reg_delta_request',\n                                 c_submitter_did,\n                                 c_revoc_reg_def_id,\n                                 c_from,\n                                 c_to,\n                                 build_get_revoc_reg_delta_request.cb)\n\n    res = request_json.decode()\n    logger.debug(\"build_get_revoc_reg_delta_request: <<< res: %r\", res)\n    return res", "response": "Builds a GET_REVOC_REG_DELTA request. Request to get the delta of the accumulated state of the Revocation Registry."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a GET_REVOC_REG_DELTA response to get Revocation Registry Delta json and Timestamp.", "response": "async def parse_get_revoc_reg_delta_response(get_revoc_reg_delta_response: str) -> (str, str, int):\n    \"\"\"\n    Parse a GET_REVOC_REG_DELTA response to get Revocation Registry Delta in the format compatible with Anoncreds API.\n\n    :param get_revoc_reg_delta_response: response of GET_REVOC_REG_DELTA request.\n    :return: Revocation Registry Definition Id, Revocation Registry Delta json and Timestamp.\n      {\n          \"value\": Registry-specific data {\n              prevAccum: string - previous accumulator value.\n              accum: string - current accumulator value.\n              issued: array<number> - an array of issued indices.\n              revoked: array<number> an array of revoked indices.\n          },\n          \"ver\": string\n      }\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"parse_get_revoc_reg_delta_response: >>> get_revoc_reg_delta_response: %r\",\n                 get_revoc_reg_delta_response)\n\n    if not hasattr(parse_get_revoc_reg_delta_response, \"cb\"):\n        logger.debug(\"parse_get_revoc_reg_delta_response: Creating callback\")\n        parse_get_revoc_reg_delta_response.cb = create_cb(\n            CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p, c_uint64))\n\n    c_get_revoc_reg_delta_response = c_char_p(get_revoc_reg_delta_response.encode('utf-8'))\n\n    (revoc_reg_def_id, revoc_reg_delta_json, timestamp) = await do_call('indy_parse_get_revoc_reg_delta_response',\n                                                                        c_get_revoc_reg_delta_response,\n                                                                        parse_get_revoc_reg_delta_response.cb)\n\n    res = (revoc_reg_def_id.decode(), revoc_reg_delta_json.decode(), timestamp)\n    logger.debug(\"parse_get_revoc_reg_delta_response: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def get_response_metadata(response: str) -> str:\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"get_response_metadata: >>> response: %r\",\n                 response)\n\n    if not hasattr(get_response_metadata, \"cb\"):\n        logger.debug(\"get_response_metadata: Creating callback\")\n        get_response_metadata.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_response = c_char_p(response.encode('utf-8'))\n\n    response_metadata = await do_call('indy_get_response_metadata',\n                                      c_response,\n                                      get_response_metadata.cb)\n\n    res = response_metadata.decode()\n    logger.debug(\"get_response_metadata: <<< res: %r\", res)\n    return res", "response": "Parse response to get metadata."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def build_auth_rule_request(submitter_did: str,\n                                  txn_type: str,\n                                  action: str,\n                                  field: str,\n                                  old_value: Optional[str],\n                                  new_value: Optional[str],\n                                  constraint: str) -> str:\n    \"\"\"\n    Builds a AUTH_RULE request. Request to change authentication rules for a ledger transaction.\n\n    :param submitter_did: DID of the submitter stored in secured Wallet.\n    :param txn_type: ledger transaction alias or associated value.\n    :param action: type of an action.\n       Can be either \"ADD\" (to add a new rule) or \"EDIT\" (to edit an existing one).\n    :param field: transaction field.\n    :param old_value: (Optional) old value of a field, which can be changed to a new_value (mandatory for EDIT action).\n    :param new_value: (Optional) new value that can be used to fill the field.\n    :param constraint: set of constraints required for execution of an action in the following format:\n        {\n            constraint_id - <string> type of a constraint.\n                Can be either \"ROLE\" to specify final constraint or  \"AND\"/\"OR\" to combine constraints.\n            role - <string> role of a user which satisfy to constrain.\n            sig_count - <u32> the number of signatures required to execution action.\n            need_to_be_owner - <bool> if user must be an owner of transaction.\n            metadata - <object> additional parameters of the constraint.\n        }\n      can be combined by\n        {\n            'constraint_id': <\"AND\" or \"OR\">\n            'auth_constraints': [<constraint_1>, <constraint_2>]\n        }\n\n    Default ledger auth rules: https://github.com/hyperledger/indy-node/blob/master/docs/source/auth_rules.md\n\n    More about AUTH_RULE request: https://github.com/hyperledger/indy-node/blob/master/docs/source/requests.md#auth_rule\n\n    :return: Request result as json.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"build_auth_rule_request: >>> submitter_did: %r, txn_type: %r, action: %r, field: %r, \"\n                 \"old_value: %r, new_value: %r, constraint: %r\",\n                 submitter_did,\n                 txn_type,\n                 action,\n                 field,\n                 old_value,\n                 new_value,\n                 constraint)\n\n    if not hasattr(build_auth_rule_request, \"cb\"):\n        logger.debug(\"build_auth_rule_request: Creating callback\")\n        build_auth_rule_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_submitter_did = c_char_p(submitter_did.encode('utf-8'))\n    c_txn_type = c_char_p(txn_type.encode('utf-8'))\n    c_action = c_char_p(action.encode('utf-8'))\n    c_field = c_char_p(field.encode('utf-8'))\n    c_old_value = c_char_p(old_value.encode('utf-8')) if old_value is not None else None\n    c_new_value = c_char_p(new_value.encode('utf-8')) if new_value is not None else None\n    c_constraint = c_char_p(constraint.encode('utf-8'))\n\n    request_json = await do_call('indy_build_auth_rule_request',\n                                 c_submitter_did,\n                                 c_txn_type,\n                                 c_action,\n                                 c_field,\n                                 c_old_value,\n                                 c_new_value,\n                                 c_constraint,\n                                 build_auth_rule_request.cb)\n\n    res = request_json.decode()\n    logger.debug(\"build_auth_rule_request: <<< res: %r\", res)\n    return res", "response": "Builds a AUTH_RULE request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new DID and store it in a secured Wallet.", "response": "async def create_and_store_my_did(wallet_handle: int,\n                                  did_json: str) -> (str, str):\n    \"\"\"\n    Creates keys (signing and encryption keys) for a new\n    DID (owned by the caller of the library).\n    Identity's DID must be either explicitly provided, or taken as the first 16 bit of verkey.\n    Saves the Identity DID with keys in a secured Wallet, so that it can be used to sign\n    and encrypt transactions.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param did_json: Identity information as json. Example:\n        {\n            \"did\": string, (optional;\n                    if not provided and cid param is false then the first 16 bit of the verkey will be\n                    used as a new DID;\n                    if not provided and cid is true then the full verkey will be used as a new DID;\n                    if provided, then keys will be replaced - key rotation use case)\n\t        \"seed\": string, (optional) Seed that allows deterministic key creation (if not set random one will be created).\n\t                                    Can be UTF-8, base64 or hex string.\n            \"crypto_type\": string, (optional; if not set then ed25519 curve is used;\n                      currently only 'ed25519' value is supported for this field)\n            \"cid\": bool, (optional; if not set then false is used;)\n        }\n    :return: DID and verkey (for verification of signature)\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"create_and_store_my_did: >>> wallet_handle: %r, did_json: %r\",\n                 wallet_handle,\n                 did_json)\n\n    if not hasattr(create_and_store_my_did, \"cb\"):\n        logger.debug(\"create_wallet: Creating callback\")\n        create_and_store_my_did.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_did_json = c_char_p(did_json.encode('utf-8'))\n\n    did, verkey = await do_call('indy_create_and_store_my_did',\n                                c_wallet_handle,\n                                c_did_json,\n                                create_and_store_my_did.cb)\n\n    res = (did.decode(), verkey.decode())\n\n    logger.debug(\"create_and_store_my_did: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate new keys (signing and encryption keys) for an existing DID (owned by the caller of the library). :param wallet_handle: wallet handler (created by open_wallet). :param did: signing DID :param identity_json: Identity information as json. Example: { \"seed\": string, (optional) Seed that allows deterministic key creation (if not set random one will be created). Can be UTF-8, base64 or hex string. \"crypto_type\": string, (optional; if not set then ed25519 curve is used; currently only 'ed25519' value is supported for this field) } :return: verkey", "response": "async def replace_keys_start(wallet_handle: int,\n                             did: str,\n                             identity_json: str) -> str:\n    \"\"\"\n    Generated new keys (signing and encryption keys) for an existing\n    DID (owned by the caller of the library).\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param did: signing DID\n    :param identity_json: Identity information as json. Example:\n        {\n\t        \"seed\": string, (optional) Seed that allows deterministic key creation (if not set random one will be created).\n\t                                   Can be UTF-8, base64 or hex string.\n            \"crypto_type\": string, (optional; if not set then ed25519 curve is used;\n                      currently only 'ed25519' value is supported for this field)\n        }\n    :return: verkey\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"replace_keys_start: >>> wallet_handle: %r, did: %r, identity_json: %r\",\n                 wallet_handle,\n                 did,\n                 identity_json)\n\n    if not hasattr(replace_keys_start, \"cb\"):\n        logger.debug(\"replace_keys_start: Creating callback\")\n        replace_keys_start.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_did = c_char_p(did.encode('utf-8'))\n    c_identity_json = c_char_p(identity_json.encode('utf-8'))\n\n    verkey = await do_call('indy_replace_keys_start',\n                           c_wallet_handle,\n                           c_did,\n                           c_identity_json,\n                           replace_keys_start.cb)\n\n    res = verkey.decode()\n\n    logger.debug(\"replace_keys_start: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply temporary keys as main for an existing DID.", "response": "async def replace_keys_apply(wallet_handle: int,\n                             did: str) -> None:\n    \"\"\"\n    Apply temporary keys as main for an existing DID (owned by the caller of the library).\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param did: The DID to resolve key.\n    :return: Error code\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"replace_keys_apply: >>> wallet_handle: %r, did: %r\",\n                 wallet_handle,\n                 did)\n\n    if not hasattr(replace_keys_apply, \"cb\"):\n        logger.debug(\"replace_keys_apply: Creating callback\")\n        replace_keys_apply.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_did = c_char_p(did.encode('utf-8'))\n\n    await do_call('indy_replace_keys_apply',\n                  c_wallet_handle,\n                  c_did,\n                  replace_keys_apply.cb)\n\n    logger.debug(\"replace_keys_apply: <<<\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstore their DID for a pairwise connection in a secured Wallet.", "response": "async def store_their_did(wallet_handle: int,\n                          identity_json: str) -> None:\n    \"\"\"\n    Saves their DID for a pairwise connection in a secured Wallet,\n    so that it can be used to verify transaction.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param identity_json: Identity information as json. Example:\n        {\n           \"did\": string, (required)\n           \"verkey\": string (optional, if only pk is provided),\n           \"crypto_type\": string, (optional; if not set then ed25519 curve is used;\n                  currently only 'ed25519' value is supported for this field)\n        }\n    :return: None\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"store_their_did: >>> wallet_handle: %r, identity_json: %r\",\n                 wallet_handle,\n                 identity_json)\n\n    if not hasattr(store_their_did, \"cb\"):\n        logger.debug(\"store_their_did: Creating callback\")\n        store_their_did.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_identity_json = c_char_p(identity_json.encode('utf-8'))\n\n    res = await do_call('indy_store_their_did',\n                        c_wallet_handle,\n                        c_identity_json,\n                        store_their_did.cb)\n\n    logger.debug(\"store_their_did: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def create_key(wallet_handle: int,\n                     key_json: str) -> str:\n    \"\"\"\n    Creates keys pair and stores in the wallet.\n\n    :param wallet_handle: Wallet handle (created by open_wallet).\n    :param key_json: Key information as json. Example:\n        {\n\t        \"seed\": string, (optional) Seed that allows deterministic key creation (if not set random one will be created).\n\t                                   Can be UTF-8, base64 or hex string.\n            \"crypto_type\": string, // Optional (if not set then ed25519 curve is used);\n                    Currently only 'ed25519' value is supported for this field.\n        }\n    :return: verkey: Ver key of generated key pair, also used as key identifier\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"create_key: >>> wallet_handle: %r, key_json: %r\",\n                 wallet_handle,\n                 key_json)\n\n    if not hasattr(create_key, \"cb\"):\n        logger.debug(\"create_key: Creating callback\")\n        create_key.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_key_json = c_char_p(key_json.encode('utf-8'))\n\n    verkey = await do_call('indy_create_key',\n                           c_wallet_handle,\n                           c_key_json,\n                           create_key.cb)\n\n    res = verkey.decode()\n\n    logger.debug(\"create_key: <<< res: %r\", res)\n    return res", "response": "Create a new key pair and stores it in the wallet."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def set_key_metadata(wallet_handle: int,\n                           verkey: str,\n                           metadata: str) -> None:\n    \"\"\"\n    Creates keys pair and stores in the wallet.\n\n    :param wallet_handle: Wallet handle (created by open_wallet).\n    :param verkey: the key (verkey, key id) to store metadata.\n    :param metadata: the meta information that will be store with the key.\n    :return: Error code\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"set_key_metadata: >>> wallet_handle: %r, verkey: %r, metadata: %r\",\n                 wallet_handle,\n                 verkey,\n                 metadata)\n\n    if not hasattr(set_key_metadata, \"cb\"):\n        logger.debug(\"set_key_metadata: Creating callback\")\n        set_key_metadata.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_verkey = c_char_p(verkey.encode('utf-8'))\n    c_metadata = c_char_p(metadata.encode('utf-8'))\n\n    await do_call('indy_set_key_metadata',\n                  c_wallet_handle,\n                  c_verkey,\n                  c_metadata,\n                  set_key_metadata.cb)\n\n    logger.debug(\"create_key: <<<\")", "response": "Create keys pair and stores in the wallet."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve the meta information for the given key in the wallet.", "response": "async def get_key_metadata(wallet_handle: int,\n                           verkey: str) -> str:\n    \"\"\"\n    Retrieves the meta information for the giving key in the wallet.\n\n    :param wallet_handle: Wallet handle (created by open_wallet).\n    :param verkey: The key (verkey, key id) to retrieve metadata.\n    :return: metadata: The meta information stored with the key; Can be null if no metadata was saved for this key.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"get_key_metadata: >>> wallet_handle: %r, verkey: %r\",\n                 wallet_handle,\n                 verkey)\n\n    if not hasattr(get_key_metadata, \"cb\"):\n        logger.debug(\"get_key_metadata: Creating callback\")\n        get_key_metadata.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_verkey = c_char_p(verkey.encode('utf-8'))\n\n    metadata = await do_call('indy_get_key_metadata',\n                             c_wallet_handle,\n                             c_verkey,\n                             get_key_metadata.cb)\n\n    res = metadata.decode()\n\n    logger.debug(\"get_key_metadata: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def set_endpoint_for_did(wallet_handle: int,\n                               did: str,\n                               address: str,\n                               transport_key: str) -> None:\n    \"\"\"\n    Set/replaces endpoint information for the given DID.\n\n    :param wallet_handle: Wallet handle (created by open_wallet).\n    :param did: The DID to resolve endpoint.\n    :param address: The DIDs endpoint address.\n    :param transport_key: The DIDs transport key (ver key, key id).\n    :return: Error code\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"set_endpoint_for_did: >>> wallet_handle: %r, did: %r, address: %r, transport_key: %r\",\n                 wallet_handle,\n                 did,\n                 address,\n                 transport_key)\n\n    if not hasattr(set_endpoint_for_did, \"cb\"):\n        logger.debug(\"set_endpoint_for_did: Creating callback\")\n        set_endpoint_for_did.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_did = c_char_p(did.encode('utf-8'))\n    c_address = c_char_p(address.encode('utf-8'))\n    c_transport_key = c_char_p(transport_key.encode('utf-8'))\n\n    await do_call('indy_set_endpoint_for_did',\n                  c_wallet_handle,\n                  c_did,\n                  c_address,\n                  c_transport_key,\n                  set_endpoint_for_did.cb)\n\n    logger.debug(\"set_endpoint_for_did: <<<\")", "response": "Set endpoint information for the given DID."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting endpoint information for a given DID.", "response": "async def get_endpoint_for_did(wallet_handle: int,\n                               pool_handle: int,\n                               did: str) -> (str, Optional[str]):\n    \"\"\"\n    Returns endpoint information for the given DID.\n\n    :param wallet_handle: Wallet handle (created by open_wallet).\n    :param pool_handle: Pool handle (created by open_pool).\n    :param did: The DID to resolve endpoint.\n    :return: (endpoint, transport_vk)\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"get_endpoint_for_did: >>> wallet_handle: %r, pool_handle: %r, did: %r\",\n                 wallet_handle,\n                 pool_handle,\n                 did)\n\n    if not hasattr(get_endpoint_for_did, \"cb\"):\n        logger.debug(\"get_endpoint_for_did: Creating callback\")\n        get_endpoint_for_did.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_pool_handle = c_int32(pool_handle)\n    c_did = c_char_p(did.encode('utf-8'))\n\n    endpoint, transport_vk = await do_call('indy_get_endpoint_for_did',\n                                           c_wallet_handle,\n                                           c_pool_handle,\n                                           c_did,\n                                           get_endpoint_for_did.cb)\n\n    endpoint = endpoint.decode()\n    transport_vk = transport_vk.decode() if transport_vk is not None else None\n    res = (endpoint, transport_vk)\n\n    logger.debug(\"get_endpoint_for_did: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def set_did_metadata(wallet_handle: int,\n                           did: str,\n                           metadata: str) -> None:\n    \"\"\"\n    Saves/replaces the meta information for the giving DID in the wallet.\n\n    :param wallet_handle: Wallet handle (created by open_wallet).\n    :param did: the DID to store metadata.\n    :param metadata: the meta information that will be store with the DID.\n    :return: Error code\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"set_did_metadata: >>> wallet_handle: %r, did: %r, metadata: %r\",\n                 wallet_handle,\n                 did,\n                 metadata)\n\n    if not hasattr(set_did_metadata, \"cb\"):\n        logger.debug(\"set_did_metadata: Creating callback\")\n        set_did_metadata.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_did = c_char_p(did.encode('utf-8'))\n    c_metadata = c_char_p(metadata.encode('utf-8'))\n\n    await do_call('indy_set_did_metadata',\n                  c_wallet_handle,\n                  c_did,\n                  c_metadata,\n                  set_did_metadata.cb)\n\n    logger.debug(\"set_did_metadata: <<<\")", "response": "Sets the meta information for the given DID in the wallet."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets DID metadata and verkey stored in the wallet.", "response": "async def get_my_did_with_meta(wallet_handle: int, did: str) -> str:\n    \"\"\"\n    Get DID metadata and verkey stored in the wallet.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param did: The DID to retrieve metadata.\n    :return: DID with verkey and metadata.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"get_my_did_with_meta: >>> wallet_handle: %r, did: %r\",\n                 wallet_handle,\n                 did)\n\n    if not hasattr(get_my_did_with_meta, \"cb\"):\n        logger.debug(\"get_my_did_with_meta: Creating callback\")\n        get_my_did_with_meta.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_did = c_char_p(did.encode('utf-8'))\n\n    did_with_meta = await do_call('indy_get_my_did_with_meta',\n                                  c_wallet_handle,\n                                  c_did,\n                                  get_my_did_with_meta.cb)\n\n    res = did_with_meta.decode()\n\n    logger.debug(\"get_my_did_with_meta: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def abbreviate_verkey(did: str,\n                          full_verkey: str) -> str:\n    \"\"\"\n    Retrieves abbreviated verkey if it is possible otherwise return full verkey.\n\n    :param did: The DID.\n    :param full_verkey: The DIDs verification key,\n    :return: metadata: Either abbreviated or full verkey.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"abbreviate_verkey: >>> did: %r, full_verkey: %r\",\n                 did, full_verkey)\n\n    if not hasattr(abbreviate_verkey, \"cb\"):\n        logger.debug(\"abbreviate_verkey: Creating callback\")\n        abbreviate_verkey.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_did = c_char_p(did.encode('utf-8'))\n    c_full_verkey = c_char_p(full_verkey.encode('utf-8'))\n\n    metadata = await do_call('indy_abbreviate_verkey',\n                             c_did,\n                             c_full_verkey,\n                             abbreviate_verkey.cb)\n\n    res = metadata.decode()\n\n    logger.debug(\"abbreviate_verkey: <<< res: %r\", res)\n    return res", "response": "Returns the full verkey for the given DID."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a credential with an offer.", "response": "async def create(source_id: str, credential_offer: str):\n        \"\"\"\n        Creates a credential with an offer.\n        :param source_id: user defined id of object.\n        :param credential_offer: JSON string representing the offer used as the basis of creation.\n        :return: A created credential\n        Example:\n        offer = [{\n           \"msg_type\": \"CLAIM_OFFER\",\n           \"version\": \"0.1\",\n           \"to_did\": \"8XFh8yBzrpJQmNyZzgoTqB\",\n           \"from_did\": \"8XFh8yBzrpJQmNyZzgoTqB\",\n           \"libindy_offer\": '{}',\n           \"credential_attrs\": {\n              \"address1\": [\n                 \"101 Tela Lane\"\n              ],\n              \"address2\": [\n                 \"101 Wilson Lane\"\n              ],\n              \"city\": [\n                 \"SLC\"\n              ],\n              \"state\": [\n                 \"UT\"\n              ],\n              \"zip\": [\n                 \"87121\"\n              ]\n           },\n           \"schema_seq_no\": 1487,\n           \"cred_def_id\": \"id1\",\n           \"claim_name\": \"Credential\",\n           \"claim_id\": \"defaultCredentialId\",\n           \"msg_ref_id\": None,\n        }]\n        credential = await Credential.create(source_id, offer)\n        \"\"\"\n        constructor_params = (source_id,)\n\n        c_source_id = c_char_p(source_id.encode('utf-8'))\n        c_offer = c_char_p(json.dumps(credential_offer).encode('utf-8'))\n        c_params = (c_source_id, c_offer, )\n\n        return await Credential._create(\"vcx_credential_create_with_offer\",\n                                        constructor_params,\n                                        c_params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new credential based off of a known message id.", "response": "async def create_with_msgid(source_id: str, connection: Connection, msg_id: str):\n        \"\"\"\n        Create a credential based off of a known message id for a given connection.\n        :param source_id: user defined id of object.\n        :param connection: connection handle of connection to receive offer from\n        :param msg_id: message id\n        :return: A created credential\n        Example:\n        credential = await Credential.create_with_msgid(source_id, connection, msg_id)\n        assert await credential.get_state() == State.RequestReceived\n        \"\"\"\n        credential = Credential(source_id,)\n\n        c_source_id = c_char_p(source_id.encode('utf-8'))\n        c_msg_id = c_char_p(json.dumps(msg_id).encode('utf-8'))\n        c_connection_handle = c_uint32(connection.handle)\n\n        if not hasattr(Credential.create_with_msgid, \"cb\"):\n            Credential.create_with_msgid.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_uint32, c_char_p))\n\n        credential.handle, cred_offer = await do_call('vcx_credential_create_with_msgid',\n                                                      c_source_id,\n                                                      c_connection_handle,\n                                                      c_msg_id,\n                                                      Credential.create_with_msgid.cb)\n\n        credential.cred_offer = json.loads(cred_offer.decode())\n\n        return credential"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a credential object from a previously serialized credential object.", "response": "async def deserialize(data: dict):\n        \"\"\"\n        Create a credential object from a previously serialized credential object\n        :param data: JSON data from a serialized object.\n        :return: A created credential\n        Example:\n        credential = await Credential.create(source_id, offer)\n        data = await credential.serialize()\n        credential2 = await Credential.deserialize(data)\n        \"\"\"\n\n        credential = await Credential._deserialize(\"vcx_credential_deserialize\",\n                                                   json.dumps(data),\n                                                   data.get('data').get('source_id'))\n        return credential"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve all pending credential offers for a given connection.", "response": "async def get_offers(connection: Connection) -> dict:\n        \"\"\"\n        Retrieves all pending credential offers for a given connection.\n        :param connection: A connection handle\n        :return: A list of dictionary objects representing offers from a given connection.\n        Example:\n        credential = await Credential.create_with_msgid(source_id, connection, msg_id)\n        offers = await credential.get_offers(connection)\n        \"\"\"\n        if not hasattr(Credential.get_offers, \"cb\"):\n            Credential.get_offers.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\n\n        c_connection_handle = c_uint32(connection.handle)\n\n        data = await do_call('vcx_credential_get_offers',\n                             c_connection_handle,\n                             Credential.get_offers.cb)\n\n        return json.loads(data.decode())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def send_request(self, connection: Connection, payment_handle: int):\n        if not hasattr(Credential.send_request, \"cb\"):\n            self.logger.debug(\"vcx_credential_send_request: Creating callback\")\n            Credential.send_request.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\n\n        c_credential_handle = c_uint32(self.handle)\n        c_connection_handle = c_uint32(connection.handle)\n        c_payment = c_uint32(payment_handle)\n\n        await do_call('vcx_credential_send_request',\n                      c_credential_handle,\n                      c_connection_handle,\n                      c_payment,\n                      Credential.send_request.cb)", "response": "Sends a credential request to the prover s wallet."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving Payment Transaction Information for this Credential.", "response": "async def get_payment_info(self):\n        \"\"\"\n        Retrieve Payment Transaction Information for this Credential. Typically this will include\n        how much payment is requried by the issuer, which needs to be provided by the prover, before the issuer will\n        issue the credential to the prover. Ideally a prover would want to know how much payment is being asked before\n        submitting the credential request (which triggers the payment to be made).\n        Example:\n        info = credential.get_payment_info()\n        :return:\n        \"\"\"\n        if not hasattr(Credential.get_payment_info, \"cb\"):\n            self.logger.debug(\"vcx_credential_get_payment_info: Creating callback\")\n            Credential.get_payment_info.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\n\n        c_credential_handle = c_uint32(self.handle)\n        data = await do_call('vcx_credential_get_payment_info',\n                      c_credential_handle,\n                      Credential.get_payment_info.cb)\n        return json.loads(data.decode())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def get_payment_txn(self):\n        if not hasattr(Credential.get_payment_txn, \"cb\"):\n            self.logger.debug(\"vcx_credential_get_payment_txn: Creating callback\")\n            Credential.get_payment_txn.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\n\n        c_credential_handle = c_uint32(self.handle)\n\n        payment_txn = await do_call('vcx_credential_get_payment_txn',\n                      c_credential_handle,\n                      Credential.get_payment_txn.cb)\n\n        return json.loads(payment_txn.decode())", "response": "Retirieve the payment transaction associated with this credential."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a connection object representing a single endpoint and can be used for sending and receiving and proofs of a connection.", "response": "async def create(source_id: str):\n        \"\"\"\n        Create a connection object, represents a single endpoint and can be used for sending and receiving\n        credentials and proofs\n\n        :param source_id: Institution's unique ID for the connection\n        :return: connection object\n        Example:\n        connection = await Connection.create(source_id)\n        \"\"\"\n        constructor_params = (source_id,)\n\n        c_source_id = c_char_p(source_id.encode('utf-8'))\n        c_params = (c_source_id,)\n\n        return await Connection._create( \"vcx_connection_create\",\n                                        constructor_params,\n                                        c_params)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def create_with_details(source_id: str, invite_details: str):\n        constructor_params = (source_id,)\n\n        c_source_id = c_char_p(source_id.encode('utf-8'))\n        c_invite_details = c_char_p(invite_details.encode('utf-8'))\n\n        c_params = (c_source_id, c_invite_details, )\n\n        return await Connection._create( \"vcx_connection_create_with_invite\",\n                                        constructor_params,\n                                        c_params)", "response": "Create a connection object with a provided invite."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def deserialize(data: dict):\n        return await Connection._deserialize(\"vcx_connection_deserialize\",\n                                             json.dumps(data),\n                                             data.get('source_id'))", "response": "Deserialize a previously serialized object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconnects securely and privately to the endpoint represented by the object.", "response": "async def connect(self, options: str) -> str:\n        \"\"\"\n        Connect securely and privately to the endpoint represented by the object.\n\n        :param options: detailed connection options\n        Example options:\n        {\"connection_type\":\"SMS\",\"phone\":\"5555555555\",\"use_public_did\":true}\n        or:\n        {\"connection_type\":\"QR\"}\n        Example code:\n        connection = await Connection.create('Sally')\n        invite_details = await connection.connect('{\"connection_type\":\"QR\"}')\n        :return: the invite details sent via SMS or ready to be sent via some other mechanism (QR for example)\n        \"\"\"\n        if not hasattr(Connection.connect, \"cb\"):\n            self.logger.debug(\"vcx_connection_connect: Creating callback\")\n            Connection.connect.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\n\n        c_connection_handle = c_uint32(self.handle)\n        c_connection_data = c_char_p(options.encode('utf-8'))\n        invite_details = await do_call('vcx_connection_connect',\n                                       c_connection_handle,\n                                       c_connection_data,\n                                       Connection.connect.cb)\n        return invite_details"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a generic message to the connection.", "response": "async def send_message(self, msg: str, msg_type: str, msg_title: str) -> str:\n        \"\"\"\n            Send a generic message to the connection\n            :param msg:\n            :param msg_type:\n            :param msg_title:\n            :return:\n            \"\"\"\n        if not hasattr(Connection.send_message, \"cb\"):\n            self.logger.debug(\"vcx_connection_send_message: Creating callback\")\n            Connection.send_message.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\n\n        c_connection_handle = c_uint32(self.handle)\n        c_msg = c_char_p(msg.encode('utf-8'))\n        c_msg_type = c_char_p(msg_type.encode('utf-8'))\n        c_msg_title = c_char_p(msg_title.encode('utf-8'))\n\n        result = await do_call('vcx_connection_send_message',\n                               c_connection_handle,\n                               c_msg,\n                               c_msg_type,\n                               c_msg_title,\n                               Connection.send_message.cb)\n\n        self.logger.debug(\"vcx_connection_send_message completed\")\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def sign_data(self, msg: bytes) -> bytes:\n\n        def transform_cb(arr_ptr: POINTER(c_uint8), arr_len: c_uint32):\n            return bytes(arr_ptr[:arr_len]),\n\n        if not hasattr(Connection.sign_data, \"cb\"):\n            self.logger.debug(\"vcx_connection_sign_data: Creating callback\")\n            Connection.sign_data.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, POINTER(c_uint8), c_uint32), transform_cb)\n\n        c_connection_handle = c_uint32(self.handle)\n        c_msg_len = c_uint32(len(msg))\n\n        result = await do_call('vcx_connection_sign_data',\n                               c_connection_handle,\n                               msg,\n                               c_msg_len,\n                               Connection.sign_data.cb)\n\n        self.logger.debug(\"vcx_connection_sign_data completed\")\n        return result", "response": "Signs data using connection s pairwise key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def verify_signature(self, msg: bytes, signature: bytes) -> bool:\n        if not hasattr(Connection.verify_signature, \"cb\"):\n            self.logger.debug(\"vcx_connection_verify_signature: Creating callback\")\n            Connection.verify_signature.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_bool))\n\n        c_connection_handle = c_uint32(self.handle)\n        c_msg_len = c_uint32(len(msg))\n        c_signature_len = c_uint32(len(signature))\n\n        result = await do_call('vcx_connection_verify_signature',\n                               c_connection_handle,\n                               msg,\n                               c_msg_len,\n                               signature,\n                               c_signature_len,\n                               Connection.verify_signature.cb)\n\n        self.logger.debug(\"vcx_connection_verify_signature completed\")\n        return result", "response": "Verify the signature of a message."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the invite details that were sent to the endpoint.", "response": "async def invite_details(self, abbreviated: bool) -> dict:\n        \"\"\"\n        Get the invite details that were sent or can be sent to the endpoint.\n\n        :param abbreviated: abbreviate invite details or not\n        Example:\n        phone_number = '8019119191'\n        connection = await Connection.create('foobar123')\n        invite_details = await connection.connect(phone_number)\n        inivte_details_again = await connection.invite_details()\n        :return: JSON of invite_details sent to connection\n        \"\"\"\n        if not hasattr(Connection.invite_details, \"cb\"):\n            self.logger.debug(\"vcx_connection_invite_details: Creating callback\")\n            Connection.invite_details.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\n\n        c_connection_handle = c_uint32(self.handle)\n        c_abbreviated = c_bool(abbreviated)\n\n        details = await do_call('vcx_connection_invite_details',\n                                c_connection_handle,\n                                c_abbreviated,\n                                Connection.invite_details.cb)\n\n        return json.loads(details.decode())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def create(source_id: str, proof_request: str):\n        constructor_params = (source_id,)\n\n        c_source_id = c_char_p(source_id.encode('utf-8'))\n        c_proof_request = c_char_p(json.dumps(proof_request).encode('utf-8'))\n        c_params = (c_source_id, c_proof_request, )\n\n        return await DisclosedProof._create(\"vcx_disclosed_proof_create_with_request\",\n                                   constructor_params,\n                                   c_params)", "response": "Create a disclosed proof for fulfilling a corresponding proof request."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def deserialize(data: dict):\n        disclosed_proof = await DisclosedProof._deserialize(\"vcx_disclosed_proof_deserialize\",\n                                                      json.dumps(data),\n                                                      data.get('data').get('source_id'))\n        return disclosed_proof", "response": "Deserialize a disclosed proof from a dict."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def get_requests(connection: Connection) -> dict:\n        if not hasattr(DisclosedProof.get_requests, \"cb\"):\n            DisclosedProof.get_requests.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\n\n        c_connection_handle = c_uint32(connection.handle)\n\n        data = await do_call('vcx_disclosed_proof_get_requests',\n                      c_connection_handle,\n                      DisclosedProof.get_requests.cb)\n\n        return json.loads(data.decode())", "response": "Get all requests associated with the connection"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the credentials from a disclosed proof.", "response": "async def get_creds(self) -> dict:\n        \"\"\"\n        Gets the credentials from a disclosed proof\n        Example:\n        msg_id = '1'\n        phone_number = '8019119191'\n        connection = await Connection.create(source_id)\n        await connection.connect(phone_number)\n        disclosed_proof = await DisclosedProof.create_with_msgid(source_id, connection, msg_id)\n        creds = await disclosed_proof.get_creds()\n        :return: credentials\n        \"\"\"\n        if not hasattr(DisclosedProof.get_creds, \"cb\"):\n            self.logger.debug(\"vcx_disclosed_proof_retrieve_credentials: Creating callback\")\n            DisclosedProof.get_creds.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\n\n        c_disclosed_proof_handle = c_uint32(self.handle)\n\n        data = await do_call('vcx_disclosed_proof_retrieve_credentials',\n                             c_disclosed_proof_handle,\n                             DisclosedProof.get_creds.cb)\n        return json.loads(data.decode())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def send_proof(self, connection: Connection):\n        if not hasattr(DisclosedProof.send_proof, \"cb\"):\n            self.logger.debug(\"vcx_disclosed_proof_send_proof: Creating callback\")\n            DisclosedProof.send_proof.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\n\n        c_disclosed_proof_handle = c_uint32(self.handle)\n        c_connection_handle = c_uint32(connection.handle)\n\n        await do_call('vcx_disclosed_proof_send_proof',\n                      c_disclosed_proof_handle,\n                      c_connection_handle,\n                      DisclosedProof.send_proof.cb)", "response": "Sends the proof to the Connection"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating the proof for the selected credentials and self - attested attributes.", "response": "async def generate_proof(self, selected_creds: dict, self_attested_attrs: dict):\n        \"\"\"\n        Generates the proof\n        Example:\n        msg_id = '1'\n        phone_number = '8019119191'\n        connection = await Connection.create(source_id)\n        await connection.connect(phone_number)\n        disclosed_proof = await DisclosedProof.create_with_msgid(source_id, connection, msg_id)\n        await disclosed_proof.generate_proof({}, {})\n        :param selected_creds: Credentials issued\n        :param self_attested_attrs: Self Attested Attributes\n        :return: None\n        \"\"\"\n        if not hasattr(DisclosedProof.generate_proof, \"cb\"):\n            self.logger.debug(\"vcx_disclosed_proof_generate_proof: Creating callback\")\n            DisclosedProof.generate_proof.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\n\n        c_disclosed_proof_handle = c_uint32(self.handle)\n        c_selected_creds = c_char_p(json.dumps(selected_creds).encode('utf-8'))\n        c_self_attested_attrs = c_char_p(json.dumps(self_attested_attrs).encode('utf-8'))\n\n        await do_call('vcx_disclosed_proof_generate_proof',\n                      c_disclosed_proof_handle,\n                      c_selected_creds,\n                      c_self_attested_attrs,\n                      DisclosedProof.generate_proof.cb)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def create_payment_address(wallet_handle: int,\n                                 payment_method: str,\n                                 config: str) -> str:\n    \"\"\"\n     Create the payment address for specified payment method\n\n\n     This method generates private part of payment address\n     and stores it in a secure place. Ideally it should be\n     secret in libindy wallet (see crypto module).\n\n     Note that payment method should be able to resolve this\n     secret by fully resolvable payment address format.\n\n    :param wallet_handle: wallet handle (created by open_wallet).\n    :param payment_method: Payment method to use (for example, 'sov').\n    :param config: payment address config as json:\n       {\n         seed: <str>, // allows deterministic creation of payment address\n       }\n    :return: payment_address: public identifier of payment address in fully resolvable payment address format.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"create_payment_address: >>> wallet_handle: %r, payment_method: %r, config: %r\",\n                 wallet_handle,\n                 payment_method,\n                 config)\n\n    if not hasattr(create_payment_address, \"cb\"):\n        logger.debug(\"create_payment_address: Creating callback\")\n        create_payment_address.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_payment_method = c_char_p(payment_method.encode('utf-8'))\n    config = c_char_p(config.encode('utf-8'))\n\n    request_result = await do_call('indy_create_payment_address',\n                                   c_wallet_handle,\n                                   c_payment_method,\n                                   config,\n                                   create_payment_address.cb)\n\n    res = request_result.decode()\n    logger.debug(\"create_payment_address: <<< res: %r\", res)\n    return res", "response": "Create payment address for specified payment method and config."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def build_get_payment_sources_request(wallet_handle: int,\n                                            submitter_did: str,\n                                            payment_address: str) -> (str, str):\n    \"\"\"\n    Builds Indy request for getting sources list for payment address\n    according to this payment method.\n\n    :param wallet_handle: wallet handle (created by open_wallet).\n    :param submitter_did : (Option) DID of request sender\n    :param payment_address: target payment address\n    :return: get_sources_txn_json: Indy request for getting sources list for payment address\n             payment_method: used payment method\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"build_get_payment_sources_request: >>> wallet_handle: %r, submitter_did: %r, payment_address: %r\",\n                 wallet_handle,\n                 submitter_did,\n                 payment_address)\n\n    if not hasattr(build_get_payment_sources_request, \"cb\"):\n        logger.debug(\"build_get_payment_sources_request: Creating callback\")\n        build_get_payment_sources_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_submitter_did = c_char_p(submitter_did.encode('utf-8')) if submitter_did is not None else None\n    c_payment_address = c_char_p(payment_address.encode('utf-8'))\n\n    (get_sources_txn_json, payment_method) = await do_call('indy_build_get_payment_sources_request',\n                                                           c_wallet_handle,\n                                                           c_submitter_did,\n                                                           c_payment_address,\n                                                           build_get_payment_sources_request.cb)\n    res = (get_sources_txn_json.decode(), payment_method.decode())\n\n    logger.debug(\"build_get_payment_sources_request: <<< res: %r\", res)\n    return res", "response": "Builds get sources list request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def build_payment_req(wallet_handle: int,\n                            submitter_did: str,\n                            inputs_json: str,\n                            outputs_json: str,\n                            extra: Optional[str]) -> (str, str):\n    \"\"\"\n    Builds Indy request for doing payment\n    according to this payment method.\n   \n    This method consumes set of inputs and outputs.\n   \n    Format of inputs is specific for payment method. Usually it should reference payment transaction\n    with at least one output that corresponds to payment address that user owns.\n\n    :param wallet_handle: wallet handle (created by open_wallet).\n    :param submitter_did : (Option) DID of request sender\n    :param inputs_json: The list of payment sources as json array:\n      [\"source1\", ...]\n      Note that each source should reference payment address    \n    :param outputs_json: The list of outputs as json array:\n      [{\n        recipient: <str>, // payment address of recipient\n        amount: <int>, // amount\n      }]\n    :param extra: // optional information for payment operation\n\n    :return: payment_req_json: Indy request for doing payment\n             payment_method: used payment method\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"build_payment_req: >>> wallet_handle: %r, submitter_did: %r, inputs_json: %r, outputs_json: %r,\"\n                 \" extra: %r\",\n                 wallet_handle,\n                 submitter_did,\n                 inputs_json,\n                 outputs_json,\n                 extra)\n\n    if not hasattr(build_payment_req, \"cb\"):\n        logger.debug(\"build_payment_req: Creating callback\")\n        build_payment_req.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_submitter_did = c_char_p(submitter_did.encode('utf-8')) if submitter_did is not None else None\n    c_inputs_json = c_char_p(inputs_json.encode('utf-8'))\n    c_outputs_json = c_char_p(outputs_json.encode('utf-8'))\n    c_extra = c_char_p(extra.encode('utf-8')) if extra is not None else None\n\n    (payment_req_json, payment_method) = await do_call('indy_build_payment_req',\n                                                       c_wallet_handle,\n                                                       c_submitter_did,\n                                                       c_inputs_json,\n                                                       c_outputs_json,\n                                                       c_extra,\n                                                       build_payment_req.cb)\n    res = (payment_req_json.decode(), payment_method.decode())\n\n    logger.debug(\"build_payment_req: <<< res: %r\", res)\n    return res", "response": "Builds Indy request for doing payment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding Indy request for setting fees for transactions in ledger.", "response": "async def build_set_txn_fees_req(wallet_handle: int,\n                                 submitter_did: str,\n                                 payment_method: str,\n                                 fees_json: str) -> str:\n    \"\"\"\n    Builds Indy request for setting fees for transactions in the ledger\n\n    :param wallet_handle: wallet handle (created by open_wallet).\n    :param submitter_did : (Option) DID of request sender\n    :param payment_method: Payment method to use (for example, 'sov').\n    :param fees_json: {\n       txnType1: amount1,\n       txnType2: amount2,\n       .................\n       txnTypeN: amountN,\n     }\n    :return: set_txn_fees_json: Indy request for setting fees for transactions in the ledger\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"build_set_txn_fees_req: >>> wallet_handle: %r, submitter_did: %r, payment_method: %r, fees_json: %r\",\n                 wallet_handle,\n                 submitter_did,\n                 payment_method,\n                 fees_json)\n\n    if not hasattr(build_set_txn_fees_req, \"cb\"):\n        logger.debug(\"build_set_txn_fees_req: Creating callback\")\n        build_set_txn_fees_req.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_submitter_did = c_char_p(submitter_did.encode('utf-8')) if submitter_did is not None else None\n    c_payment_method = c_char_p(payment_method.encode('utf-8'))\n    c_fees_json = c_char_p(fees_json.encode('utf-8'))\n\n    set_txn_fees_json = await do_call('indy_build_set_txn_fees_req',\n                                      c_wallet_handle,\n                                      c_submitter_did,\n                                      c_payment_method,\n                                      c_fees_json,\n                                      build_set_txn_fees_req.cb)\n\n    res = set_txn_fees_json.decode()\n    logger.debug(\"build_set_txn_fees_req: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def build_get_txn_fees_req(wallet_handle: int,\n                                 submitter_did: str,\n                                 payment_method: str) -> str:\n    \"\"\"\n    Builds Indy request for getting fees for transactions in the ledger\n\n    :param wallet_handle: wallet handle (created by open_wallet).\n    :param submitter_did : (Option) DID of request sender\n    :param payment_method: Payment method to use (for example, 'sov').\n    :return: set_txn_fees_json: Indy request for setting fees for transactions in the ledger\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"build_get_txn_fees_req: >>> wallet_handle: %r, submitter_did: %r, payment_method: %r\",\n                 wallet_handle,\n                 submitter_did,\n                 payment_method)\n\n    if not hasattr(build_get_txn_fees_req, \"cb\"):\n        logger.debug(\"build_get_txn_fees_req: Creating callback\")\n        build_get_txn_fees_req.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_submitter_did = c_char_p(submitter_did.encode('utf-8')) if submitter_did is not None else None\n    c_payment_method = c_char_p(payment_method.encode('utf-8'))\n\n    get_txn_fees_json = await do_call('indy_build_get_txn_fees_req',\n                                      c_wallet_handle,\n                                      c_submitter_did,\n                                      c_payment_method,\n                                      build_get_txn_fees_req.cb)\n\n    res = get_txn_fees_json.decode()\n    logger.debug(\"build_get_txn_fees_req: <<< res: %r\", res)\n    return res", "response": "Builds Indy request for getting fees for transactions in ledger."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def parse_get_txn_fees_response(payment_method: str,\n                                      resp_json: str) -> str:\n    \"\"\"\n    Parses response for Indy request for getting fees\n\n    :param payment_method: Payment method to use (for example, 'sov').\n    :param resp_json: response for Indy request for getting fees\n    :return: fees_json: {\n       txnType1: amount1,\n       txnType2: amount2,\n       .................\n       txnTypeN: amountN,\n     }\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"parse_get_txn_fees_response: >>> payment_method: %r, resp_json: %r\",\n                 payment_method,\n                 resp_json)\n\n    if not hasattr(parse_get_txn_fees_response, \"cb\"):\n        logger.debug(\"parse_get_txn_fees_response: Creating callback\")\n        parse_get_txn_fees_response.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_payment_method = c_char_p(payment_method.encode('utf-8'))\n    c_resp_json = c_char_p(resp_json.encode('utf-8'))\n\n    fees_json = await do_call('indy_parse_get_txn_fees_response',\n                              c_payment_method,\n                              c_resp_json,\n                              parse_get_txn_fees_response.cb)\n\n    res = fees_json.decode()\n    logger.debug(\"parse_get_txn_fees_response: <<< res: %r\", res)\n    return res", "response": "Parses response for Indy request for getting fees."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding Indy request for information to verify the payment receipt for transactions in ledger .", "response": "async def build_verify_payment_req(wallet_handle: int,\n                                   submitter_did: str,\n                                   receipt: str) -> (str, str):\n    \"\"\"\n    Builds Indy request for information to verify the payment receipt\n\n    :param wallet_handle: wallet handle (created by open_wallet).\n    :param submitter_did : (Option) DID of request sender\n    :param receipt: payment receipt to verify\n\n    :return: verify_txn_json: Indy request for verification receipt for transactions in the ledger\n             payment_method: used payment method\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"build_verify_payment_req: >>> wallet_handle: %r, submitter_did: %r, receipt: %r\",\n                 wallet_handle,\n                 submitter_did,\n                 receipt)\n\n    if not hasattr(build_verify_payment_req, \"cb\"):\n        logger.debug(\"build_verify_payment_req: Creating callback\")\n        build_verify_payment_req.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_submitter_did = c_char_p(submitter_did.encode('utf-8')) if submitter_did is not None else None\n    c_receipt = c_char_p(receipt.encode('utf-8'))\n\n    (verify_txn_json, payment_method) = await do_call('indy_build_verify_payment_req',\n                                                      c_wallet_handle,\n                                                      c_submitter_did,\n                                                      c_receipt,\n                                                      build_verify_payment_req.cb)\n    res = (verify_txn_json.decode(), payment_method.decode())\n\n    logger.debug(\"build_verify_payment_req: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def create(source_id: str, name: str, version: str, attrs: list, payment_handle: int):\n        constructor_params = (source_id, name, version, attrs)\n\n        c_source_id = c_char_p(source_id.encode('utf-8'))\n        c_name = c_char_p(name.encode('utf-8'))\n        c_version = c_char_p(version.encode('utf-8'))\n        c_schema_data = c_char_p(json.dumps(attrs).encode('utf-8'))\n        c_payment = c_uint32(payment_handle)\n        c_params = (c_source_id, c_name, c_version, c_schema_data, c_payment)\n\n        schema = await Schema._create(\"vcx_schema_create\", constructor_params, c_params)\n        schema.schema_id = await schema.get_schema_id()\n        return schema", "response": "Creates a new schema object that is written to the ledger"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating the object from a previously serialized object. :param data: The output of the \"serialize\" call Example: source_id = 'foobar123' name = 'Address Schema' version = '1.0' attrs = ['address', 'city', 'state'] payment_handle = 0 schema1 = await Schema.create(source_id, name, version, attrs, payment_handle) data1 = await schema1.serialize() :return: A re-instantiated object", "response": "async def deserialize(data: dict):\n        \"\"\"\n        Create the object from a previously serialized object.\n\n        :param data: The output of the \"serialize\" call\n        Example:\n        source_id = 'foobar123'\n        name = 'Address Schema'\n        version = '1.0'\n        attrs = ['address', 'city', 'state']\n        payment_handle = 0\n        schema1 = await Schema.create(source_id, name, version, attrs, payment_handle)\n        data1 = await schema1.serialize()\n        :return: A re-instantiated object\n        \"\"\"\n        try:\n            # Todo: Find better way to access attr_names. Potential for issues.\n            schema = await Schema._deserialize(\"vcx_schema_deserialize\",\n                                               json.dumps(data),\n                                               data['data']['source_id'],\n                                               data['data']['name'],\n                                               data['data']['version'],\n                                               data['data']['data'])\n\n            schema.schema_id = await schema.get_schema_id()\n            return schema\n        except KeyError:\n            raise VcxError(ErrorCode.InvalidSchema)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def lookup(source_id: str, schema_id: str):\n        try:\n            schema = Schema(source_id, '', '', [])\n\n            if not hasattr(Schema.lookup, \"cb\"):\n                schema.logger.debug(\"vcx_schema_get_attributes: Creating callback\")\n                Schema.lookup.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_uint32, c_char_p))\n\n            c_source_id = c_char_p(source_id.encode('utf-8'))\n            c_schema_id = c_char_p(schema_id.encode('utf-8'))\n\n            handle, data = await do_call('vcx_schema_get_attributes',\n                                         c_source_id,\n                                         c_schema_id,\n                                         Schema.lookup.cb)\n            schema.logger.debug(\"created schema object\")\n\n            schema_result = json.loads(data.decode())\n            schema.attrs = schema_result['data']\n            schema.name = schema_result['name']\n            schema.version = schema_result['version']\n            schema.handle = handle\n            return schema\n        except KeyError:\n            raise VcxError(ErrorCode.InvalidSchema)", "response": "Create a new schema object from an existing ledger schema."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the ledger ID of the object containing the object containing the schema ID", "response": "async def get_schema_id(self):\n        \"\"\"\n        Get the ledger ID of the object\n\n        Example:\n        source_id = 'foobar123'\n        name = 'Address Schema'\n        version = '1.0'\n        attrs = ['address', 'city', 'state']\n        payment_handle = 0\n        schema1 = await Schema.create(source_id, name, version, attrs, payment_handle)\n        id1 = await schema.get_schema_id()\n        :return: ID string\n        \"\"\"\n        cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\n        c_handle = c_uint32(self.handle)\n        id = await do_call('vcx_schema_get_schema_id', c_handle, cb)\n        return id.decode()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def is_pairwise_exists(wallet_handle: int,\n                             their_did: str) -> bool:\n    \"\"\"\n    Check if pairwise is exists.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param their_did: encoded Did.\n    :return: true - if pairwise is exists, false - otherwise\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"is_pairwise_exists: >>> wallet_handle: %r, their_did: %r\",\n                 wallet_handle,\n                 their_did)\n\n    if not hasattr(is_pairwise_exists, \"cb\"):\n        logger.debug(\"is_pairwise_exists: Creating callback\")\n        is_pairwise_exists.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_bool))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_their_did = c_char_p(their_did.encode('utf-8'))\n\n    res = await do_call('indy_is_pairwise_exists',\n                        c_wallet_handle,\n                        c_their_did,\n                        is_pairwise_exists.cb)\n\n    logger.debug(\"is_pairwise_exists: <<< res: %r\", res)\n    return res", "response": "Check if pairwise is exists."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def list_pairwise(wallet_handle: int) -> str:\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"list_pairwise: >>> wallet_handle: %r\", wallet_handle)\n\n    if not hasattr(list_pairwise, \"cb\"):\n        logger.debug(\"list_pairwise: Creating callback\")\n        list_pairwise.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n\n    pairwise_list = await do_call('indy_list_pairwise',\n                                  c_wallet_handle,\n                                  list_pairwise.cb)\n\n    res = pairwise_list.decode()\n    logger.debug(\"list_pairwise: <<< res: %r\", res)\n    return res", "response": "Get list of saved pairwise."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget pairwise information for specific their_did.", "response": "async def get_pairwise(wallet_handle: int,\n                       their_did: str) -> None:\n    \"\"\"\n    Gets pairwise information for specific their_did.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param their_did: encoded Did\n    :return: pairwise_info_json: did info associated with their did\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"get_pairwise: >>> wallet_handle: %r, their_did: %r\",\n                 wallet_handle,\n                 their_did)\n\n    if not hasattr(get_pairwise, \"cb\"):\n        logger.debug(\"get_pairwise: Creating callback\")\n        get_pairwise.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_their_did = c_char_p(their_did.encode('utf-8'))\n\n    pairwise_info_json = await do_call('indy_get_pairwise',\n                                       c_wallet_handle,\n                                       c_their_did,\n                                       get_pairwise.cb)\n\n    res = pairwise_info_json.decode()\n    logger.debug(\"get_pairwise: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def set_pairwise_metadata(wallet_handle: int,\n                                their_did: str,\n                                metadata: Optional[str]) -> None:\n    \"\"\"\n    Save some data in the Wallet for pairwise associated with Did.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param their_did: encoded DID\n    :param metadata: some extra information for pairwise\n    :return: Error code\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"set_pairwise_metadata: >>> wallet_handle: %r, their_did: %r, metadata: %r\",\n                 wallet_handle,\n                 their_did,\n                 metadata)\n\n    if not hasattr(set_pairwise_metadata, \"cb\"):\n        logger.debug(\"set_pairwise_metadata: Creating callback\")\n        set_pairwise_metadata.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_their_did = c_char_p(their_did.encode('utf-8'))\n    c_metadata = c_char_p(metadata.encode('utf-8')) if metadata is not None else None\n\n    await do_call('indy_set_pairwise_metadata',\n                  c_wallet_handle,\n                  c_their_did,\n                  c_metadata,\n                  set_pairwise_metadata.cb)\n\n    logger.debug(\"set_pairwise_metadata: <<<\")", "response": "Set some extra information for pairwise associated with Did."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef print_log(value_color=\"\", value_noncolor=\"\"):\n    HEADER = '\\033[92m'\n    ENDC = '\\033[0m'\n    print(HEADER + value_color + ENDC + str(value_noncolor))", "response": "set the colors for text"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_runtime_config(config: str):\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"set_runtime_config: >>> config: %r\", config)\n\n    c_config = c_char_p(config.encode('utf-8'))\n\n    do_call_sync('indy_set_runtime_config',\n                 c_config)\n\n    logger.debug(\"set_runtime_config: <<<\")", "response": "Set libindy runtime configuration."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def create(source_id: str, name: str, requested_attrs: list, revocation_interval: dict, requested_predicates: list = []):\n        constructor_params = (source_id,)\n\n        c_source_id = c_char_p(source_id.encode('utf-8'))\n        c_name = c_char_p(name.encode('utf-8'))\n        c_req_predicates = c_char_p(json.dumps(requested_predicates).encode('utf-8'))\n        c_req_attrs = c_char_p(json.dumps(requested_attrs).encode('utf-8'))\n        c_revocation_interval = c_char_p(json.dumps(revocation_interval).encode('utf-8'))\n        c_params = (c_source_id, c_req_attrs, c_req_predicates, c_revocation_interval, c_name)\n\n        return await Proof._create(\"vcx_proof_create\",\n                                   constructor_params,\n                                   c_params)", "response": "Creates a generic object containing the specified attributes and revocation intervals."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def deserialize(data: dict):\n        return await Proof._deserialize(\"vcx_proof_deserialize\",\n                                        json.dumps(data),\n                                        data.get('data').get('source_id'))", "response": "Deserialize a Proof object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrequests a new proof from the database.", "response": "async def request_proof(self, connection: Connection):\n        \"\"\"\n        Example:\n        connection = await Connection.create(source_id)\n        await connection.connect(phone_number)\n        name = \"proof name\"\n        requested_attrs = [{\"name\": \"age\", \"restrictions\": [{\"schema_id\": \"6XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"Faber Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"6XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"8XFh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"8XFh8yBzrpJQmNyZzgoTqB:3:CL:1766\" }, { \"schema_id\": \"5XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"BYU Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"5XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"66Fh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"66Fh8yBzrpJQmNyZzgoTqB:3:CL:1766\" } ] }, { \"name\":\"name\", \"restrictions\": [ { \"schema_id\": \"6XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"Faber Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"6XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"8XFh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"8XFh8yBzrpJQmNyZzgoTqB:3:CL:1766\" }, { \"schema_id\": \"5XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"BYU Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"5XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"66Fh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"66Fh8yBzrpJQmNyZzgoTqB:3:CL:1766\"}]}]\n        proof = await Proof.create(source_id, name, requested_attrs)\n        await proof.request_proof(connection)\n        :param connection: Connection\n        :return:\n        \"\"\"\n        if not hasattr(Proof.request_proof, \"cb\"):\n            self.logger.debug(\"vcx_proof_send_request: Creating callback\")\n            Proof.request_proof.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\n\n        c_proof_handle = c_uint32(self.handle)\n        c_connection_handle = c_uint32(connection.handle)\n\n        await do_call('vcx_proof_send_request',\n                      c_proof_handle,\n                      c_connection_handle,\n                      Proof.request_proof.cb)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def get_proof(self, connection: Connection) -> list:\n        if not hasattr(Proof.get_proof, \"cb\"):\n            self.logger.debug(\"vcx_get_proof: Creating callback\")\n            Proof.get_proof.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_uint32, c_char_p))\n\n        c_proof_handle = c_uint32(self.handle)\n        c_connection_handle = c_uint32(connection.handle)\n\n        proof_state, proof = await do_call('vcx_get_proof',\n                                           c_proof_handle,\n                                           c_connection_handle,\n                                           Proof.get_proof.cb)\n        self.proof_state = proof_state\n        return json.loads(proof.decode())", "response": "Get the proof for the given connection."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def vcx_agent_provision(config: str) -> None:\n    logger = logging.getLogger(__name__)\n\n    if not hasattr(vcx_agent_provision, \"cb\"):\n        logger.debug(\"vcx_agent_provision: Creating callback\")\n        vcx_agent_provision.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\n\n    c_config = c_char_p(config.encode('utf-8'))\n\n    result = await do_call('vcx_agent_provision_async',\n                           c_config,\n                           vcx_agent_provision.cb)\n\n    logger.debug(\"vcx_agent_provision completed\")\n    return result.decode()", "response": "Provision an agent in the agency populate configuration and wallet for this agent."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def vcx_ledger_get_fees() -> str:\n    logger = logging.getLogger(__name__)\n\n    if not hasattr(vcx_ledger_get_fees, \"cb\"):\n        logger.debug(\"vcx_ledger_get_fees: Creating callback\")\n        vcx_ledger_get_fees.cb = create_cb(CFUNCTYPE(None, c_uint32))\n\n    result = await do_call('vcx_ledger_get_fees',\n                           vcx_ledger_get_fees.cb)\n\n    logger.debug(\"vcx_ledger_get_fees completed\")\n    return result", "response": "Get ledger fees from the sovrin network\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def vcx_messages_download(status: str = None, uids: str = None, pw_dids: str = None) -> str:\n    logger = logging.getLogger(__name__)\n\n    if not hasattr(vcx_messages_download, \"cb\"):\n        logger.debug(\"vcx_messages_download: Creating callback\")\n        vcx_messages_download.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\n\n    if status:\n        c_status = c_char_p(status.encode('utf-8'))\n    else:\n        c_status = None\n\n    if uids:\n        c_uids = c_char_p(uids.encode('utf-8'))\n    else:\n        c_uids = None\n\n    if pw_dids:\n        c_pw_dids = c_char_p(pw_dids.encode('utf-8'))\n    else:\n        c_pw_dids = None\n\n    result = await do_call('vcx_messages_download',\n                           c_status,\n                           c_uids,\n                           c_pw_dids,\n                           vcx_messages_download.cb)\n\n    logger.debug(\"vcx_messages_download completed\")\n    return result", "response": "Retrieve messages from the specified connection"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the status of messages from the specified connection", "response": "async def vcx_messages_update_status(msg_json: str):\n    \"\"\"\n    Update the status of messages from the specified connection\n    :param msg_json:\n    :return:\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    if not hasattr(vcx_messages_update_status, \"cb\"):\n        logger.debug(\"vcx_messages_update_status: Creating callback\")\n        vcx_messages_update_status.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\n\n    c_msg_json = c_char_p(msg_json.encode('utf-8'))\n    c_status = c_char_p(\"MS-106\".encode('utf-8'))\n\n    result = await do_call('vcx_messages_update_status',\n                           c_status,\n                           c_msg_json,\n                           vcx_messages_update_status.cb)\n\n    logger.debug(\"vcx_messages_update_status completed\")\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsign a message with a key.", "response": "async def crypto_sign(wallet_handle: int,\n                      signer_vk: str,\n                      msg: bytes) -> bytes:\n    \"\"\"\n    Signs a message with a key.\n\n    Note to use DID keys with this function you can call indy_key_for_did to get key id (verkey) for specific DID.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param signer_vk:  id (verkey) of my key. The key must be created by calling create_key or create_and_store_my_did\n    :param msg: a message to be signed\n    :return: a signature string\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"crypto_sign: >>> wallet_handle: %r, signer_vk: %r, msg: %r\",\n                 wallet_handle,\n                 signer_vk,\n                 msg)\n\n    def transform_cb(arr_ptr: POINTER(c_uint8), arr_len: c_uint32):\n        return bytes(arr_ptr[:arr_len]),\n\n    if not hasattr(crypto_sign, \"cb\"):\n        logger.debug(\"crypto_sign: Creating callback\")\n        crypto_sign.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, POINTER(c_uint8), c_uint32), transform_cb)\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_signer_vk = c_char_p(signer_vk.encode('utf-8'))\n    c_msg_len = c_uint32(len(msg))\n\n    signature = await do_call('indy_crypto_sign',\n                              c_wallet_handle,\n                              c_signer_vk,\n                              msg,\n                              c_msg_len,\n                              crypto_sign.cb)\n\n    logger.debug(\"crypto_sign: <<< res: %r\", signature)\n    return signature"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nverify a message with a verkey.", "response": "async def crypto_verify(signer_vk: str,\n                        msg: bytes,\n                        signature: bytes) -> bool:\n    \"\"\"\n    Verify a signature with a verkey.\n\n    Note to use DID keys with this function you can call indy_key_for_did to get key id (verkey) for specific DID.\n\n    :param signer_vk: verkey of signer of the message\n    :param msg: message that has been signed\n    :param signature: a signature to be verified\n    :return: valid: true - if signature is valid, false - otherwise\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"crypto_verify: >>> my_vk: %r, signed_msg: %r, signature: %r\",\n                 signer_vk,\n                 msg,\n                 signature)\n\n    if not hasattr(crypto_verify, \"cb\"):\n        logger.debug(\"crypto_verify: Creating callback\")\n        crypto_verify.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_bool))\n\n    c_signer_vk = c_char_p(signer_vk.encode('utf-8'))\n    c_msg_len = c_uint32(len(msg))\n    c_signature_len = c_uint32(len(signature))\n\n    res = await do_call('indy_crypto_verify',\n                        c_signer_vk,\n                        msg,\n                        c_msg_len,\n                        signature,\n                        c_signature_len,\n                        crypto_verify.cb)\n\n    logger.debug(\"crypto_verify: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def auth_crypt(wallet_handle: int,\n                     sender_vk: str,\n                     recipient_vk: str,\n                     msg: bytes) -> bytes:\n    \"\"\"\n    **** THIS FUNCTION WILL BE DEPRECATED USE pack_message INSTEAD ****\n\n    Encrypt a message by authenticated-encryption scheme.\n\n    Sender can encrypt a confidential message specifically for Recipient, using Sender's public key.\n    Using Recipient's public key, Sender can compute a shared secret key.\n    Using Sender's public key and his secret key, Recipient can compute the exact same shared secret key.\n    That shared secret key can be used to verify that the encrypted message was not tampered with,\n    before eventually decrypting it.\n\n    Note to use DID keys with this function you can call indy_key_for_did to get key id (verkey)\n    for specific DID.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param sender_vk: id (verkey) of my key. The key must be created by calling indy_create_key or\n    indy_create_and_store_my_did\n    :param recipient_vk: id (verkey) of their key\n    :param msg: a message to be signed\n    :return: encrypted message as an array of bytes\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"auth_crypt: >>> wallet_handle: %r,sender_vk: %r, recipient_vk: %r, msg: %r\",\n                 wallet_handle,\n                 sender_vk,\n                 recipient_vk,\n                 msg)\n\n    def transform_cb(arr_ptr: POINTER(c_uint8), arr_len: c_uint32):\n        return bytes(arr_ptr[:arr_len]),\n\n    if not hasattr(auth_crypt, \"cb\"):\n        logger.debug(\"auth_crypt: Creating callback\")\n        auth_crypt.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, POINTER(c_uint8), c_uint32), transform_cb)\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_sender_vk = c_char_p(sender_vk.encode('utf-8'))\n    c_recipient_vk = c_char_p(recipient_vk.encode('utf-8'))\n    c_msg_len = c_uint32(len(msg))\n\n    res = await do_call('indy_crypto_auth_crypt',\n                        c_wallet_handle,\n                        c_sender_vk,\n                        c_recipient_vk,\n                        msg,\n                        c_msg_len,\n                        auth_crypt.cb)\n\n    logger.debug(\"auth_crypt: <<< res: %r\", res)\n    return res", "response": "This function is used to encrypt a message using authenticated - encryption scheme."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def anon_crypt(recipient_vk: str,\n                     msg: bytes) -> bytes:\n    \"\"\"\n    Encrypts a message by anonymous-encryption scheme.\n\n    Sealed boxes are designed to anonymously send messages to a Recipient given its public key.\n    Only the Recipient can decrypt these messages, using its private key.\n    While the Recipient can verify the integrity of the message, it cannot verify the identity of the Sender.\n\n    Note to use DID keys with this function you can call key_for_did to get key id (verkey)\n    for specific DID.\n\n    Note: use pack_message function for A2A goals.\n\n    :param recipient_vk: verkey of message recipient\n    :param msg: a message to be signed\n    :return: an encrypted message as an array of bytes\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"anon_crypt: >>> recipient_vk: %r, msg: %r\",\n                 recipient_vk,\n                 msg)\n\n    def transform_cb(arr_ptr: POINTER(c_uint8), arr_len: c_uint32):\n        return bytes(arr_ptr[:arr_len]),\n\n    if not hasattr(anon_crypt, \"cb\"):\n        logger.debug(\"anon_crypt: Creating callback\")\n        anon_crypt.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, POINTER(c_uint8), c_uint32), transform_cb)\n\n    c_recipient_vk = c_char_p(recipient_vk.encode('utf-8'))\n    c_msg_len = c_uint32(len(msg))\n\n    encrypted_message = await do_call('indy_crypto_anon_crypt',\n                                      c_recipient_vk,\n                                      msg,\n                                      c_msg_len,\n                                      anon_crypt.cb)\n    res = encrypted_message\n    logger.debug(\"anon_crypt: <<< res: %r\", res)\n    return res", "response": "Encrypts a message using anonymous - encryption scheme."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def anon_decrypt(wallet_handle: int,\n                       recipient_vk: str,\n                       encrypted_msg: bytes) -> bytes:\n    \"\"\"\n    Decrypts a message by anonymous-encryption scheme.\n\n    Sealed boxes are designed to anonymously send messages to a Recipient given its public key.\n    Only the Recipient can decrypt these messages, using its private key.\n    While the Recipient can verify the integrity of the message, it cannot verify the identity of the Sender.\n\n    Note to use DID keys with this function you can call key_for_did to get key id (verkey)\n    for specific DID.\n\n    Note: use unpack_message function for A2A goals.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param recipient_vk: id (verkey) of my key. The key must be created by calling indy_create_key or create_and_store_my_did\n    :param encrypted_msg: encrypted message\n    :return: decrypted message as an array of bytes\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"anon_decrypt: >>> wallet_handle: %r, recipient_vk: %r, encrypted_msg: %r\",\n                 wallet_handle,\n                 recipient_vk,\n                 encrypted_msg)\n\n    def transform_cb(arr_ptr: POINTER(c_uint8), arr_len: c_uint32):\n        return bytes(arr_ptr[:arr_len]),\n\n    if not hasattr(anon_decrypt, \"cb\"):\n        logger.debug(\"anon_decrypt: Creating callback\")\n        anon_decrypt.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, POINTER(c_uint8), c_uint32), transform_cb)\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_recipient_vk = c_char_p(recipient_vk.encode('utf-8'))\n    c_encrypted_msg_len = c_uint32(len(encrypted_msg))\n    decrypted_message = await do_call('indy_crypto_anon_decrypt',\n                                      c_wallet_handle,\n                                      c_recipient_vk,\n                                      encrypted_msg,\n                                      c_encrypted_msg_len,\n                                      anon_decrypt.cb)\n\n    logger.debug(\"crypto_box_seal_open: <<< res: %r\", decrypted_message)\n    return decrypted_message", "response": "Decrypts a message using anonymous - encryption scheme."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npacking a message into a JWE - like format.", "response": "async def pack_message(wallet_handle: int,\n                       message: str,\n                       recipient_verkeys: list,\n                       sender_verkey: Optional[str]) -> bytes:\n    \"\"\"\n    Packs a message by encrypting the message and serializes it in a JWE-like format (Experimental)\n\n    Note to use DID keys with this function you can call did.key_for_did to get key id (verkey)\n    for specific DID.\n\n    #Params\n    command_handle: command handle to map callback to user context.\n    wallet_handle: wallet handler (created by open_wallet)\n    message: the message being sent as a string. If it's JSON formatted it should be converted to a string\n    recipient_verkeys: a list of Strings which are recipient verkeys\n    sender_verkey: the sender's verkey as a string. -> When None is passed in this parameter, anoncrypt mode is used\n\n    returns an Agent Wire Message format as a byte array. See HIPE 0028 for detailed formats\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"pack_message: >>> wallet_handle: %r, message: %r, recipient_verkeys: %r, sender_verkey: %r\",\n                 wallet_handle,\n                 message,\n                 recipient_verkeys,\n                 sender_verkey)\n\n    def transform_cb(arr_ptr: POINTER(c_uint8), arr_len: c_uint32):\n        return bytes(arr_ptr[:arr_len]),\n\n    if not hasattr(pack_message, \"cb\"):\n        logger.debug(\"pack_message: Creating callback\")\n        pack_message.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, POINTER(c_uint8), c_uint32), transform_cb)\n\n    c_wallet_handle = c_int32(wallet_handle)\n    msg_bytes = message.encode(\"utf-8\")\n    c_msg_len = c_uint32(len(msg_bytes))\n    c_recipient_verkeys = c_char_p(json.dumps(recipient_verkeys).encode('utf-8'))\n    c_sender_vk = c_char_p(sender_verkey.encode('utf-8')) if sender_verkey is not None else None\n    res = await do_call('indy_pack_message',\n                        c_wallet_handle,\n                        msg_bytes,\n                        c_msg_len,\n                        c_recipient_verkeys,\n                        c_sender_vk,\n                        pack_message.cb)\n    logger.debug(\"pack_message: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def unpack_message(wallet_handle: int,\n                         jwe: bytes) -> bytes:\n    \"\"\"\n    Unpacks a JWE-like formatted message outputted by pack_message (Experimental)\n\n    #Params\n    command_handle: command handle to map callback to user context.\n    wallet_handle: wallet handler (created by open_wallet)\n    message: the output of a pack message\n\n    #Returns -> See HIPE 0028 for details\n    (Authcrypt mode)\n\n    {\n        \"message\": <decrypted message>,\n        \"recipient_verkey\": <recipient verkey used to decrypt>,\n        \"sender_verkey\": <sender verkey used to encrypt>\n    }\n\n    (Anoncrypt mode)\n\n    {\n        \"message\": <decrypted message>,\n        \"recipient_verkey\": <recipient verkey used to decrypt>,\n    }\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"unpack_message: >>> wallet_handle: %r, jwe: %r\",\n                 wallet_handle,\n                 jwe)\n\n    def transform_cb(arr_ptr: POINTER(c_uint8), arr_len: c_uint32):\n        return bytes(arr_ptr[:arr_len]),\n\n    if not hasattr(unpack_message, \"cb\"):\n        logger.debug(\"unpack_message: Creating callback\")\n        unpack_message.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, POINTER(c_uint8), c_uint32), transform_cb)\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_jwe_len = c_uint32(len(jwe))\n    res = await do_call('indy_unpack_message',\n                        c_wallet_handle,\n                        jwe,\n                        c_jwe_len,\n                        unpack_message.cb)\n\n    logger.debug(\"unpack_message: <<< res: %r\", res)\n    return res", "response": "Unpacks a JWE - like formatted message outputted by pack_message."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes VCX with config file.", "response": "async def vcx_init(config_path: str) -> None:\n    \"\"\"\n    Initializes VCX with config file.\n    :param config_path: String\n    Example:\n    await vcx_init('/home/username/vcxconfig.json')\n    :return:\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    if not hasattr(vcx_init, \"cb\"):\n        logger.debug(\"vcx_init: Creating callback\")\n        vcx_init.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\n\n    c_config_path = c_char_p(config_path.encode('utf-8'))\n\n    result = await do_call('vcx_init',\n                           c_config_path,\n                           vcx_init.cb)\n\n    logger.debug(\"vcx_init completed\")\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def create(source_id: str, name: str, schema_id: str, payment_handle: int):\n        constructor_params = (source_id, name, schema_id)\n\n        c_source_id = c_char_p(source_id.encode('utf-8'))\n        c_schema_id = c_char_p(schema_id.encode('utf-8'))\n        c_name = c_char_p(name.encode('utf-8'))\n        # default institution_did in config is used as issuer_did\n        c_issuer_did = None\n        c_payment = c_uint32(payment_handle)\n        # Todo: add params for tag and config\n        c_tag = c_char_p('tag1'.encode('utf-8'))\n        c_config = c_char_p('{\"support_revocation\":false}'.encode('utf-8'))\n        c_params = (c_source_id, c_name, c_schema_id, c_issuer_did, c_tag, c_config, c_payment)\n\n        return await CredentialDef._create(\"vcx_credentialdef_create\",\n                                           constructor_params,\n                                           c_params)", "response": "Creates a new CredentialDef object that is written to the ledger."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def deserialize(data: dict):\n        try:\n            credential_def = await CredentialDef._deserialize(\"vcx_credentialdef_deserialize\",\n                                                              json.dumps(data),\n                                                              data['data']['source_id'],\n                                                              data['data']['name'],\n                                                              data['data']['id'])\n            return credential_def\n        except KeyError:\n            raise VcxError(ErrorCode.InvalidCredentialDef)", "response": "Deserialize a previously serialized credential definition."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def get_cred_def_id(self):\n        cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\n        c_handle = c_uint32(self.handle)\n        cred_def_id = await do_call('vcx_credentialdef_get_cred_def_id', c_handle, cb)\n        return cred_def_id .decode()", "response": "Get the ledger ID of the object containing credential definitions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def create_pool_ledger_config(config_name: str,\n                                    config: Optional[str]) -> None:\n    \"\"\"\n    Creates a new local pool ledger configuration that can be used later to connect pool nodes.\n\n    :param config_name: Name of the pool ledger configuration.\n    :param config: (optional) Pool configuration json. if NULL, then default config will be used. Example:\n        {\n            \"genesis_txn\": string (optional), A path to genesis transaction file. If NULL, then a default one will be used.\n                           If file doesn't exists default one will be created.\n        }\n    :return: Error code\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"create_pool_ledger_config: >>> config_name: %r, config: %r\",\n                 config_name,\n                 config)\n\n    if not hasattr(create_pool_ledger_config, \"cb\"):\n        logger.debug(\"create_pool_ledger_config: Creating callback\")\n        create_pool_ledger_config.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\n\n    c_config_name = c_char_p(config_name.encode('utf-8'))\n    c_config = c_char_p(config.encode('utf-8')) if config is not None else None\n\n    res = await do_call('indy_create_pool_ledger_config',\n                        c_config_name,\n                        c_config,\n                        create_pool_ledger_config.cb)\n\n    logger.debug(\"create_pool_ledger_config: <<< res: %r\", res)\n    return res", "response": "Create a new local pool ledger configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrefreshes a local copy of a pool ledger and updates pool nodes connections.", "response": "async def refresh_pool_ledger(handle: int) -> None:\n    \"\"\"\n    Refreshes a local copy of a pool ledger and updates pool nodes connections.\n\n    :param handle: pool handle returned by indy_open_pool_ledger\n    :return: Error code\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"refresh_pool_ledger: >>> config_name: %r\",\n                 handle)\n\n    if not hasattr(refresh_pool_ledger, \"cb\"):\n        logger.debug(\"refresh_pool_ledger: Creating callback\")\n        refresh_pool_ledger.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\n\n    c_handle = c_int32(handle)\n\n    res = await do_call('indy_refresh_pool_ledger',\n                        c_handle,\n                        refresh_pool_ledger.cb)\n\n    logger.debug(\"refresh_pool_ledger: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting names of created pool ledgers", "response": "async def list_pools() -> None:\n    \"\"\"\n    Lists names of created pool ledgers\n    :return: Error code\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"list_pools: >>> \")\n\n    if not hasattr(list_pools, \"cb\"):\n        logger.debug(\"list_pools: Creating callback\")\n        list_pools.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    res = await do_call('indy_list_pools',\n                        list_pools.cb)\n    res = json.loads(res.decode())\n    logger.debug(\"list_pools: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def delete_pool_ledger_config(config_name: str) -> None:\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"delete_pool_ledger_config: >>> config_name: %r\",\n                 config_name)\n\n    if not hasattr(delete_pool_ledger_config, \"cb\"):\n        logger.debug(\"delete_pool_ledger_config: Creating callback\")\n        delete_pool_ledger_config.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\n\n    c_config_name = c_char_p(config_name.encode('utf-8'))\n\n    res = await do_call('indy_delete_pool_ledger_config',\n                        c_config_name,\n                        delete_pool_ledger_config.cb)\n\n    logger.debug(\"delete_pool_ledger_config: <<< res: %r\", res)\n    return res", "response": "Delete pool ledger configuration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting protocol version of the current node.", "response": "async def set_protocol_version(protocol_version: int) -> None:\n    \"\"\"\n    Set PROTOCOL_VERSION to specific version.\n\n    There is a global property PROTOCOL_VERSION that used in every request to the pool and\n    specified version of Indy Node which Libindy works.\n    By default PROTOCOL_VERSION=1.\n\n    :param protocol_version: Protocol version will be used:\n        1 - for Indy Node 1.3\n        2 - for Indy Node 1.4 and greater\n    :return: Error code\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"set_protocol_version: >>> protocol_version: %r\",\n                 protocol_version)\n\n    if not hasattr(set_protocol_version, \"cb\"):\n        logger.debug(\"set_protocol_version: Creating callback\")\n        set_protocol_version.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\n\n    res = await do_call('indy_set_protocol_version',\n                        protocol_version,\n                        set_protocol_version.cb)\n\n    logger.debug(\"set_protocol_version: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def open_search(type_: str, query: dict, options: dict):\n        logger = logging.getLogger(__name__)\n\n        if not hasattr(Wallet.open_search, \"cb\"):\n            logger.debug(\"vcx_wallet_open_search: Creating callback\")\n            Wallet.open_search.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_uint32))\n\n        c_type_ = c_char_p(type_.encode('utf-8'))\n        c_query = c_char_p(json.dumps(query).encode('utf-8'))\n        c_options = c_char_p(json.dumps(options).encode('utf-8')) if options else None\n\n        data = await do_call('vcx_wallet_open_search',\n                             c_type_,\n                             c_query,\n                             c_options,\n                             Wallet.open_search.cb)\n\n        logger.debug(\"vcx_wallet_open_search completed\")\n        return data", "response": "Opens a search handle within the storage wallet."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsearch next n records from an open search handle.", "response": "async def search_next_records(handle: int, count: int):\n        \"\"\"\n        Searches for next n record from an open search handle\n\n        :param handle: int\n        :param count: int\n         Example:\n        query_json = {\"tagName1\": \"str1\"}\n        type_ = 'TestType'\n        search_handle = await Wallet.open_search(type_, query_json, None)\n        results = await Wallet.search_next_records(search_handle, 5)\n        :return:\n        \"\"\"\n        logger = logging.getLogger(__name__)\n\n        if not hasattr(Wallet.search_next_records, \"cb\"):\n            logger.debug(\"vcx_wallet_search_next_records: Creating callback\")\n            Wallet.search_next_records.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\n        c_handle = c_uint32(handle)\n        c_count = c_uint32(count)\n\n        data = await do_call('vcx_wallet_search_next_records',\n                             c_handle,\n                             c_count,\n                             Wallet.search_next_records.cb)\n\n        logger.debug(\"vcx_wallet_search_next_records completed\")\n        return data.decode()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def get_record(type_: str, id: str, options: str):\n        logger = logging.getLogger(__name__)\n\n        if not hasattr(Wallet.get_record, \"cb\"):\n            logger.debug(\"vcx_wallet_get_record: Creating callback\")\n            Wallet.get_record.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\n\n        c_type_ = c_char_p(type_.encode('utf-8'))\n        c_id = c_char_p(id.encode('utf-8'))\n        c_options = c_char_p(options.encode('utf-8'))\n        data = await do_call('vcx_wallet_get_record',\n                             c_type_,\n                             c_id,\n                             c_options,\n                             Wallet.get_record.cb)\n\n        logger.debug(\"vcx_wallet_get_record completed\")\n        return data.decode()", "response": "Retrieves a record from the wallet storage."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def delete_record(type_: str, id: str):\n        logger = logging.getLogger(__name__)\n\n        if not hasattr(Wallet.delete_record, \"cb\"):\n            logger.debug(\"vcx_wallet_delete_record: Creating callback\")\n            Wallet.delete_record.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\n\n        c_type_ = c_char_p(type_.encode('utf-8'))\n        c_id = c_char_p(id.encode('utf-8'))\n        result = await do_call('vcx_wallet_delete_record',\n                               c_type_,\n                               c_id,\n                               Wallet.delete_record.cb)\n\n        logger.debug(\"vcx_wallet_delete_record completed\")\n        return result", "response": "Delete a record from the storage wallet."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def get_token_info(handle: int) -> str:\n        logger = logging.getLogger(__name__)\n\n        if not hasattr(Wallet.get_token_info, \"cb\"):\n            logger.debug(\"vcx_wallet_get_token_info: Creating callback\")\n            Wallet.get_token_info.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\n\n        c_payment = c_uint32(handle)\n\n        result = await do_call('vcx_wallet_get_token_info',\n                               c_payment,\n                               Wallet.get_token_info.cb)\n\n        logger.debug(\"vcx_wallet_get_token_info completed\")\n        return result", "response": "Retrieves the ledger token info associated with the given handle."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def create_payment_address(seed: str = None) -> str:\n        logger = logging.getLogger(__name__)\n\n        if not hasattr(Wallet.create_payment_address, \"cb\"):\n            logger.debug(\"vcx_wallet_create_payment_address: Creating callback\")\n            Wallet.create_payment_address.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\n\n        if seed:\n            c_seed = c_char_p(seed.encode('utf-8'))\n        else:\n            c_seed = None\n\n        result = await do_call('vcx_wallet_create_payment_address',\n                               c_seed,\n                               Wallet.create_payment_address.cb)\n\n        logger.debug(\"vcx_wallet_create_payment_address completed\")\n        return result", "response": "Creates a payment address inside the wallet."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine whether a payment address is valid or not.", "response": "async def validate_payment_address(address: str) -> None:\n        \"\"\"\n        Determines whether a payment address is valid or not\n        :param address: String\n        Example:\n        address = await Wallet.create_payment_address('00000000000000000000000001234567')\n        b = await Wallet.validate_payment_address(address)\n        :return: Boolean\n        \"\"\"\n\n        logger = logging.getLogger(__name__)\n\n        if not hasattr(Wallet.validate_payment_address, \"cb\"):\n            logger.debug(\"vcx_wallet_validate_payment_address: Creating callback\")\n            Wallet.validate_payment_address.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\n\n        c_address = c_char_p(address.encode('utf-8'))\n        result = await do_call('vcx_wallet_validate_payment_address',\n                               c_address,\n                               Wallet.validate_payment_address.cb)\n\n        logger.debug(\"vcx_wallet_validate_payment_address completed\")\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def send_tokens(payment_handle: int, tokens: int, address: str) -> str:\n        logger = logging.getLogger(__name__)\n\n        if not hasattr(Wallet.send_tokens, \"cb\"):\n            logger.debug(\"vcx_wallet_send_tokens: Creating callback\")\n            Wallet.send_tokens.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\n\n        c_payment_handle = c_uint32(payment_handle)\n        c_tokens = c_char_p(str(tokens).encode('utf-8'))\n        c_address = c_char_p(address.encode('utf-8'))\n\n        result = await do_call('vcx_wallet_send_tokens',\n                               c_payment_handle,\n                               c_tokens,\n                               c_address,\n                               Wallet.send_tokens.cb)\n\n        logger.debug(\"vcx_wallet_send_tokens completed\")\n        return result", "response": "Send tokens to an address."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexporting the wallet to User s File System.", "response": "async def export(path, backup_key):\n        \"\"\"\n        Exports opened wallet\n        :param path: Path to export wallet to User's File System.\n        :param backupKey: String representing the User's Key for securing (encrypting) the exported Wallet.\n        :return:\n        Error code - success indicates that the wallet was successfully exported.\n        \"\"\"\n        logger = logging.getLogger(__name__)\n\n        if not hasattr(Wallet.export, \"cb\"):\n            logger.debug(\"vcx_wallet_export: Creating callback\")\n            Wallet.export.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\n\n        c_backupKey = c_char_p(backup_key.encode('utf-8'))\n        c_path = c_char_p(path.encode('utf-8'))\n\n        result = await do_call('vcx_wallet_export',\n                               c_path,\n                               c_backupKey,\n                               Wallet.export.cb)\n\n        logger.debug(\"vcx_wallet_export completed\")\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nimport wallet from file with given key.", "response": "async def import_wallet(config):\n        \"\"\"\n        Imports wallet from file with given key.\n        Cannot be used if wallet is already opened (Especially if vcx_init has already been used).\n        :param config: Can be same config that is passed to vcx_init.\n        Must include: '{\"wallet_name\":\"\",\"wallet_key\":\"\",\"exported_wallet_path\":\"\",\"backup_key\":\"\"}'\n        :return:\n        Error code - success indicates that the wallet was successfully imported.\n        \"\"\"\n\n        logger = logging.getLogger(__name__)\n\n        if not hasattr(Wallet.import_wallet, \"cb\"):\n            logger.debug(\"vcx_wallet_import: Creating callback\")\n            Wallet.import_wallet.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\n\n        c_config = c_char_p(config.encode('utf-8'))\n\n        result = await do_call('vcx_wallet_import',\n                               c_config,\n                               Wallet.import_wallet.cb)\n\n        logger.debug(\"vcx_wallet_export completed\")\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates a non - secret wallet record value.", "response": "async def update_wallet_record_value(wallet_handle: int,\n                                     type_: str,\n                                     id_: str,\n                                     value: str) -> None:\n    \"\"\"\n    Update a non-secret wallet record value\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param type_: allows to separate different record types collections\n    :param id_: the id of record\n    :param value: the value of record\n    :return: None\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"update_wallet_record_value: >>> wallet_handle: %r, type_: %r, id: %r, value: %r\",\n                 wallet_handle,\n                 type_,\n                 id_,\n                 value)\n\n    if not hasattr(update_wallet_record_value, \"cb\"):\n        logger.debug(\"update_wallet_record_value: Creating callback\")\n        update_wallet_record_value.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_type = c_char_p(type_.encode('utf-8'))\n    c_id = c_char_p(id_.encode('utf-8'))\n    c_value = c_char_p(value.encode('utf-8'))\n\n    res = await do_call('indy_update_wallet_record_value',\n                        c_wallet_handle,\n                        c_type,\n                        c_id,\n                        c_value,\n                        update_wallet_record_value.cb)\n\n    logger.debug(\"update_wallet_record_value: <<< res: %r\", res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def get_wallet_record(wallet_handle: int,\n                            type_: str,\n                            id: str,\n                            options_json: str) -> str:\n    \"\"\"\n    Get an wallet record by id\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param type_: allows to separate different record types collections\n    :param id: the id of record\n    :param options_json: //TODO: FIXME: Think about replacing by bitmask\n      {\n        retrieveType: (optional, false by default) Retrieve record type,\n        retrieveValue: (optional, true by default) Retrieve record value,\n        retrieveTags: (optional, true by default) Retrieve record tags\n      }\n    :return: wallet record json:\n     {\n       id: \"Some id\",\n       type: \"Some type\", // present only if retrieveType set to true\n       value: \"Some value\", // present only if retrieveValue set to true\n       tags: <tags json>, // present only if retrieveTags set to true\n     }\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"get_wallet_record: >>> wallet_handle: %r, type_: %r, id: %r, options_json: %r\",\n                 wallet_handle,\n                 type_,\n                 id,\n                 options_json)\n\n    if not hasattr(get_wallet_record, \"cb\"):\n        logger.debug(\"get_wallet_record: Creating callback\")\n        get_wallet_record.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_type = c_char_p(type_.encode('utf-8'))\n    c_id = c_char_p(id.encode('utf-8'))\n    c_options_json = c_char_p(options_json.encode('utf-8'))\n\n    wallet_record = await do_call('indy_get_wallet_record',\n                                  c_wallet_handle,\n                                  c_type,\n                                  c_id,\n                                  c_options_json,\n                                  get_wallet_record.cb)\n    res = wallet_record.decode()\n\n    logger.debug(\"get_wallet_record: <<< res: %r\", res)\n    return res", "response": "Get an wallet record by id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def open_wallet_search(wallet_handle: int,\n                             type_: str,\n                             query_json: str,\n                             options_json: str) -> int:\n    \"\"\"\n    Search for wallet records\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param type_: allows to separate different record types collections\n    :param query_json: MongoDB style query to wallet record tags:\n      {\n        \"tagName\": \"tagValue\",\n        $or: {\n          \"tagName2\": { $regex: 'pattern' },\n          \"tagName3\": { $gte: '123' },\n        },\n      }\n    :param options_json: //TODO: FIXME: Think about replacing by bitmask\n      {\n        retrieveRecords: (optional, true by default) If false only \"counts\" will be calculated,\n        retrieveTotalCount: (optional, false by default) Calculate total count,\n        retrieveType: (optional, false by default) Retrieve record type,\n        retrieveValue: (optional, true by default) Retrieve record value,\n        retrieveTags: (optional, true by default) Retrieve record tags,\n      }\n    :return: search_handle: Wallet search handle that can be used later\n             to fetch records by small batches (with fetch_wallet_search_next_records)\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"open_wallet_search: >>> wallet_handle: %r, type_: %r, query_json: %r, options_json: %r\",\n                 wallet_handle,\n                 type_,\n                 query_json,\n                 options_json)\n\n    if not hasattr(open_wallet_search, \"cb\"):\n        logger.debug(\"open_wallet_search: Creating callback\")\n        open_wallet_search.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_int32))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_type = c_char_p(type_.encode('utf-8'))\n    c_query_json = c_char_p(query_json.encode('utf-8'))\n    c_options_json = c_char_p(options_json.encode('utf-8'))\n\n    search_handle = await do_call('indy_open_wallet_search',\n                                  c_wallet_handle,\n                                  c_type,\n                                  c_query_json,\n                                  c_options_json,\n                                  open_wallet_search.cb)\n    res = search_handle\n\n    logger.debug(\"open_wallet_search: <<< res: %r\", res)\n    return res", "response": "Search for wallet records by small batches."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def fetch_wallet_search_next_records(wallet_handle: int,\n                                           wallet_search_handle: int,\n                                           count: int) -> str:\n    \"\"\"\n    Fetch next records for wallet search.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param wallet_search_handle: wallet wallet handle (created by open_wallet_search)\n    :param count: Count of records to fetch\n    :return: wallet records json:\n     {\n       totalCount: <str>, // present only if retrieveTotalCount set to true\n       records: [{ // present only if retrieveRecords set to true\n           id: \"Some id\",\n           type: \"Some type\", // present only if retrieveType set to true\n           value: \"Some value\", // present only if retrieveValue set to true\n           tags: <tags json>, // present only if retrieveTags set to true\n       }],\n     }\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"fetch_wallet_search_next_records: >>> wallet_handle: %r, wallet_search_handle: %r, count: %r\",\n                 wallet_handle,\n                 wallet_search_handle,\n                 count)\n\n    if not hasattr(fetch_wallet_search_next_records, \"cb\"):\n        logger.debug(\"fetch_wallet_search_next_records: Creating callback\")\n        fetch_wallet_search_next_records.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_wallet_search_handle = c_int32(wallet_search_handle)\n    c_count = c_uint(count)\n\n    records_json = await do_call('indy_fetch_wallet_search_next_records',\n                                 c_wallet_handle,\n                                 c_wallet_search_handle,\n                                 c_count,\n                                 fetch_wallet_search_next_records.cb)\n    res = records_json.decode()\n\n    logger.debug(\"fetch_wallet_search_next_records: <<< res: %r\", res)\n    return res", "response": "Fetch next records for wallet search."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def close_wallet_search(wallet_search_handle: int) -> None:\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"close_wallet_search: >>> wallet_search_handle: %r\",\n                 wallet_search_handle)\n\n    if not hasattr(close_wallet_search, \"cb\"):\n        logger.debug(\"close_wallet_search: Creating callback\")\n        close_wallet_search.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\n\n    c_wallet_search_handle = c_int32(wallet_search_handle)\n\n    res = await do_call('indy_close_wallet_search',\n                        c_wallet_search_handle,\n                        close_wallet_search.cb)\n\n    logger.debug(\"close_wallet_search: <<< res: %r\", res)\n    return res", "response": "Close wallet search (make search handle invalid)\n\n    :param wallet_search_handle: wallet wallet handle (created by open_wallet_search)\n    :return: None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef register_mark(key=None):\n    def wrap(mark):\n        name = key if key is not None else mark.__module__ + mark.__name__\n        Mark.mark_types[name] = mark\n        return mark\n    return wrap", "response": "Returns a decorator registering a mark class in the mark type registry."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the list of scales corresponding to a given dimension.", "response": "def _get_dimension_scales(self, dimension, preserve_domain=False):\n        \"\"\"\n        Return the list of scales corresponding to a given dimension.\n\n        The preserve_domain optional argument specifies whether one should\n        filter out the scales for which preserve_domain is set to True.\n        \"\"\"\n        if preserve_domain:\n            return [\n                self.scales[k] for k in self.scales if (\n                    k in self.scales_metadata and\n                    self.scales_metadata[k].get('dimension') == dimension and\n                    not self.preserve_domain.get(k)\n                )\n            ]\n        else:\n            return [\n                self.scales[k] for k in self.scales if (\n                    k in self.scales_metadata and\n                    self.scales_metadata[k].get('dimension') == dimension\n                )\n            ]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate_scales(self, proposal):\n        # Validate scales' 'rtype' versus data attribute 'rtype' decoration\n        # At this stage it is already validated that all values in self.scales\n        # are instances of Scale.\n        scales = proposal.value\n        for name in self.trait_names(scaled=True):\n            trait = self.traits()[name]\n            if name not in scales:\n                # Check for missing scale\n                if not trait.allow_none:\n                    raise TraitError(\"Missing scale for data attribute %s.\" %\n                                     name)\n            else:\n                # Check scale range type compatibility\n                if scales[name].rtype != trait.get_metadata('rtype'):\n                    raise TraitError(\"Range type mismatch for scale %s.\" %\n                                     name)\n        return scales", "response": "Validate the scales based on the mark s scaled attributes metadata."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef register_axis(key=None):\n    def wrap(axis):\n        name = key if key is not None else axis.__module__ + axis.__name__\n        BaseAxis.axis_types[name] = axis\n        return axis\n    return wrap", "response": "Returns a decorator registering an axis class in the axis type registry."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a PanZoom interaction with the x and y dimension scales of the specified marks.", "response": "def panzoom(marks):\n    \"\"\"Helper function for panning and zooming over a set of marks.\n\n    Creates and returns a panzoom interaction with the 'x' and 'y' dimension\n    scales of the specified marks.\n    \"\"\"\n    return PanZoom(scales={\n            'x': sum([mark._get_dimension_scales('x', preserve_domain=True) for mark in marks], []),\n            'y': sum([mark._get_dimension_scales('y', preserve_domain=True) for mark in marks], [])\n    })"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef register_scale(key=None):\n    def wrap(scale):\n        label = key if key is not None else scale.__module__ + scale.__name__\n        Scale.scale_types[label] = scale\n        return scale\n    return wrap", "response": "Returns a decorator to register a scale type in the scale registry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef install(user=False, symlink=False, overwrite=False, **kwargs):\n    directory = join(dirname(abspath(__file__)), 'nbextension')\n    install_nbextension(directory, destination='bqplot',\n                        symlink=symlink, user=user, overwrite=overwrite,\n                        **kwargs)", "response": "Install the bqplot nbextension."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hashable(data, v):\n    try:\n        data[v]\n    except (TypeError, KeyError, IndexError):\n        return False\n    return True", "response": "Determine whether v can be hashed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef show(key=None, display_toolbar=True):\n    if key is None:\n        figure = current_figure()\n    else:\n        figure = _context['figure_registry'][key]\n    if display_toolbar:\n        if not hasattr(figure, 'pyplot'):\n            figure.pyplot = Toolbar(figure=figure)\n        display(VBox([figure, figure.pyplot]))\n    else:\n        display(figure)", "response": "Shows the current context figure in the output area."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new context figure and switches between figures.", "response": "def figure(key=None, fig=None, **kwargs):\n    \"\"\"Creates figures and switches between figures.\n\n    If a ``bqplot.Figure`` object is provided via the fig optional argument,\n    this figure becomes the current context figure.\n\n    Otherwise:\n\n    - If no key is provided, a new empty context figure is created.\n    - If a key is provided for which a context already exists, the\n      corresponding context becomes current.\n    - If a key is provided and no corresponding context exists, a new context\n      is reated for that key and becomes current.\n\n    Besides, optional arguments allow to set or modify Attributes\n    of the selected context figure.\n\n    Parameters\n    ----------\n    key: hashable, optional\n        Any variable that can be used as a key for a dictionary\n    fig: Figure, optional\n        A bqplot Figure\n\n    \"\"\"\n    scales_arg = kwargs.pop('scales', {})\n    _context['current_key'] = key\n    if fig is not None:                                     # fig provided\n        _context['figure'] = fig\n        if key is not None:\n            _context['figure_registry'][key] = fig\n        for arg in kwargs:\n            setattr(_context['figure'], arg, kwargs[arg])\n    else:                                                   # no fig provided\n        if key is None:                                     # no key provided\n            _context['figure'] = Figure(**kwargs)\n        else:                                               # a key is provided\n            if key not in _context['figure_registry']:\n                if 'title' not in kwargs:\n                    kwargs['title'] = 'Figure' + ' ' + str(key)\n                _context['figure_registry'][key] = Figure(**kwargs)\n            _context['figure'] = _context['figure_registry'][key]\n            for arg in kwargs:\n                setattr(_context['figure'], arg, kwargs[arg])\n    scales(key, scales=scales_arg)\n    # Set the axis reference dictionary. This dictionary contains the mapping\n    # from the possible dimensions in the figure to the list of scales with\n    # respect to which axes have been drawn for this figure.\n    # Used to automatically generate axis.\n    if(getattr(_context['figure'], 'axis_registry', None) is None):\n        setattr(_context['figure'], 'axis_registry', {})\n    return _context['figure']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nclose and unregister the context figure corresponding to the key.", "response": "def close(key):\n    \"\"\"Closes and unregister the context figure corresponding to the key.\n\n    Parameters\n    ----------\n\n    key: hashable\n        Any variable that can be used as a key for a dictionary\n\n    \"\"\"\n    figure_registry = _context['figure_registry']\n    if key not in figure_registry:\n        return\n    if _context['figure'] == figure_registry[key]:\n        figure()\n    fig = figure_registry[key]\n    if hasattr(fig, 'pyplot'):\n        fig.pyplot.close()\n    fig.close()\n    del figure_registry[key]\n    del _context['scale_registry'][key]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef scales(key=None, scales={}):\n    old_ctxt = _context['scales']\n    if key is None:  # No key provided\n        _context['scales'] = {_get_attribute_dimension(k): scales[k] if scales[k] is not Keep\n                              else old_ctxt[_get_attribute_dimension(k)] for k in scales}\n    else:  # A key is provided\n        if key not in _context['scale_registry']:\n            _context['scale_registry'][key] = {\n                _get_attribute_dimension(k): scales[k]\n                if scales[k] is not Keep\n                else old_ctxt[_get_attribute_dimension(k)]\n                for k in scales\n            }\n        _context['scales'] = _context['scale_registry'][key]", "response": "Creates and switches between context scales."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_lim(min, max, name):\n    scale = _context['scales'][_get_attribute_dimension(name)]\n    scale.min = min\n    scale.max = max\n    return scale", "response": "Sets the domain bounds of the scale associated with the provided key."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndraws axes corresponding to the scales of a given mark.", "response": "def axes(mark=None, options={}, **kwargs):\n    \"\"\"Draws axes corresponding to the scales of a given mark.\n\n    It also returns a dictionary of drawn axes. If the mark is not provided,\n    the last drawn mark is used.\n\n    Parameters\n    ----------\n    mark: Mark or None (default: None)\n        The mark to inspect to create axes. If None, the last mark drawn is\n        used instead.\n    options: dict (default: {})\n        Options for the axes to be created. If a scale labeled 'x' is required\n        for that mark, options['x'] contains optional keyword arguments for the\n        constructor of the corresponding axis type.\n    \"\"\"\n    if mark is None:\n        mark = _context['last_mark']\n    if mark is None:\n        return {}\n    fig = kwargs.get('figure', current_figure())\n    scales = mark.scales\n    fig_axes = [axis for axis in fig.axes]\n    axes = {}\n    for name in scales:\n        if name not in mark.class_trait_names(scaled=True):\n            # The scale is not needed.\n            continue\n        scale_metadata = mark.scales_metadata.get(name, {})\n        dimension = scale_metadata.get('dimension', scales[name])\n        axis_args = dict(scale_metadata,\n                         **(options.get(name, {})))\n\n        axis = _fetch_axis(fig, dimension, scales[name])\n        if axis is not None:\n            # For this figure, an axis exists for the scale in the given\n            # dimension. Apply the properties and return back the object.\n            _apply_properties(axis, options.get(name, {}))\n            axes[name] = axis\n            continue\n\n        # An axis must be created. We fetch the type from the registry\n        # the key being provided in the scaled attribute decoration\n        key = mark.class_traits()[name].get_metadata('atype')\n        if(key is not None):\n            axis_type = Axis.axis_types[key]\n            axis = axis_type(scale=scales[name], **axis_args)\n            axes[name] = axis\n            fig_axes.append(axis)\n            # Update the axis registry of the figure once the axis is added\n            _update_fig_axis_registry(fig, dimension, scales[name], axis)\n    fig.axes = fig_axes\n    return axes"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the value of the grid_lines for the axis.", "response": "def grids(fig=None, value='solid'):\n    \"\"\"Sets the value of the grid_lines for the axis to the passed value.\n    The default value is `solid`.\n\n    Parameters\n    ----------\n    fig: Figure or None(default: None)\n        The figure for which the axes should be edited. If the value is None,\n        the current figure is used.\n    value: {'none', 'solid', 'dashed'}\n        The display of the grid_lines\n    \"\"\"\n\n    if fig is None:\n        fig = current_figure()\n    for a in fig.axes:\n        a.grid_lines = value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the title for the current figure.", "response": "def title(label, style=None):\n    \"\"\"Sets the title for the current figure.\n\n    Parameters\n    ----------\n    label : str\n        The new title for the current figure.\n    style: dict\n        The CSS style to be applied to the figure title\n    \"\"\"\n    fig = current_figure()\n    fig.title = label\n    if style is not None:\n        fig.title_style = style"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hline(level, **kwargs):\n    kwargs.setdefault('colors', ['dodgerblue'])\n    kwargs.setdefault('stroke_width', 1)\n    scales = kwargs.pop('scales', {})\n    fig = kwargs.get('figure', current_figure())\n    scales['x'] = fig.scale_x\n\n    level = array(level)\n    if len(level.shape) == 0:\n        x = [0, 1]\n        y = [level, level]\n    else:\n        x = [0, 1]\n        y = column_stack([level, level])\n    return plot(x, y, scales=scales, preserve_domain={\n        'x': True,\n        'y': kwargs.get('preserve_domain', False)\n    }, axes=False, update_context=False, **kwargs)", "response": "Draws a horizontal line at the given level."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a kwarg dict suitable for a ColorScale", "response": "def _process_cmap(cmap):\n    '''\n    Returns a kwarg dict suitable for a ColorScale\n    '''\n    option = {}\n    if isinstance(cmap, str):\n        option['scheme'] = cmap\n    elif isinstance(cmap, list):\n        option['colors'] = cmap\n    else:\n        raise ValueError('''`cmap` must be a string (name of a color scheme)\n                         or a list of colors, but a value of {} was given\n                         '''.format(cmap))\n    return option"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_cmap(cmap):\n    '''\n    Set the color map of the current 'color' scale.\n    '''\n    scale = _context['scales']['color']\n    for k, v in _process_cmap(cmap).items():\n        setattr(scale, k, v)\n    return scale", "response": "Set the color map of the current color scale."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndraws the data attributes of the specified mark type.", "response": "def _draw_mark(mark_type, options={}, axes_options={}, **kwargs):\n    \"\"\"Draw the mark of specified mark type.\n\n    Parameters\n    ----------\n    mark_type: type\n        The type of mark to be drawn\n    options: dict (default: {})\n        Options for the scales to be created. If a scale labeled 'x' is\n        required for that mark, options['x'] contains optional keyword\n        arguments for the constructor of the corresponding scale type.\n    axes_options: dict (default: {})\n        Options for the axes to be created. If an axis labeled 'x' is required\n        for that mark, axes_options['x'] contains optional keyword arguments\n        for the constructor of the corresponding axis type.\n    figure: Figure or None\n        The figure to which the mark is to be added.\n        If the value is None, the current figure is used.\n    cmap: list or string\n        List of css colors, or name of bqplot color scheme\n    \"\"\"\n    fig = kwargs.pop('figure', current_figure())\n    scales = kwargs.pop('scales', {})\n    update_context = kwargs.pop('update_context', True)\n\n    # Set the color map of the color scale\n    cmap = kwargs.pop('cmap', None)\n    if cmap is not None:\n        # Add the colors or scheme to the color scale options\n        options['color'] = dict(options.get('color', {}),\n                                **_process_cmap(cmap))\n\n    # Going through the list of data attributes\n    for name in mark_type.class_trait_names(scaled=True):\n        dimension = _get_attribute_dimension(name, mark_type)\n        # TODO: the following should also happen if name in kwargs and\n        # scales[name] is incompatible.\n        if name not in kwargs:\n            # The scaled attribute is not being passed to the mark. So no need\n            # create a scale for this.\n            continue\n        elif name in scales:\n            if update_context:\n                _context['scales'][dimension] = scales[name]\n        # Scale has to be fetched from the context or created as it has not\n        # been passed.\n        elif dimension not in _context['scales']:\n            # Creating a scale for the dimension if a matching scale is not\n            # present in _context['scales']\n            traitlet = mark_type.class_traits()[name]\n            rtype = traitlet.get_metadata('rtype')\n            dtype = traitlet.validate(None, kwargs[name]).dtype\n            # Fetching the first matching scale for the rtype and dtype of the\n            # scaled attributes of the mark.\n            compat_scale_types = [\n                    Scale.scale_types[key]\n                    for key in Scale.scale_types\n                    if Scale.scale_types[key].rtype == rtype and\n                    issubdtype(dtype, Scale.scale_types[key].dtype)\n                ]\n            sorted_scales = sorted(compat_scale_types,\n                                   key=lambda x: x.precedence)\n            scales[name] = sorted_scales[-1](**options.get(name, {}))\n            # Adding the scale to the context scales\n            if update_context:\n                _context['scales'][dimension] = scales[name]\n        else:\n            scales[name] = _context['scales'][dimension]\n\n    mark = mark_type(scales=scales, **kwargs)\n    _context['last_mark'] = mark\n    fig.marks = [m for m in fig.marks] + [mark]\n    if kwargs.get('axes', True):\n        axes(mark, options=axes_options)\n    return mark"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _infer_x_for_line(y):\n    array_shape = shape(y)\n\n    if len(array_shape) == 0:\n        return []\n    if len(array_shape) == 1:\n        return arange(array_shape[0])\n    if len(array_shape) > 1:\n        return arange(array_shape[1])", "response": "Infers the x for a line if no x is provided."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot(*args, **kwargs):\n    marker_str = None\n    if len(args) == 1:\n        kwargs['y'] = args[0]\n        if kwargs.get('index_data', None) is not None:\n            kwargs['x'] = kwargs['index_data']\n        else:\n            kwargs['x'] = _infer_x_for_line(args[0])\n    elif len(args) == 2:\n        if type(args[1]) == str:\n            kwargs['y'] = args[0]\n            kwargs['x'] = _infer_x_for_line(args[0])\n            marker_str = args[1].strip()\n        else:\n            kwargs['x'] = args[0]\n            kwargs['y'] = args[1]\n    elif len(args) == 3:\n        kwargs['x'] = args[0]\n        kwargs['y'] = args[1]\n        if type(args[2]) == str:\n            marker_str = args[2].strip()\n\n    if marker_str:\n        line_style, color, marker = _get_line_styles(marker_str)\n\n        # only marker specified => draw scatter\n        if marker and not line_style:\n            kwargs['marker'] = marker\n            if color:\n                kwargs['colors'] = [color]\n            return _draw_mark(Scatter, **kwargs)\n        else:  # draw lines in all other cases\n            kwargs['line_style'] = line_style or 'solid'\n\n            if marker:\n                kwargs['marker'] = marker\n            if color:\n                kwargs['colors'] = [color]\n            return _draw_mark(Lines, **kwargs)\n    else:\n        return _draw_mark(Lines, **kwargs)", "response": "Plots a line in the current context figure."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef imshow(image, format, **kwargs):\n    if format == 'widget':\n        ipyimage = image\n    elif format == 'filename':\n        with open(image, 'rb') as f:\n            data = f.read()\n            ipyimage = ipyImage(value=data)\n    else:\n        ipyimage = ipyImage(value=image, format=format)\n    kwargs['image'] = ipyimage\n\n    kwargs.setdefault('x', [0., 1.])\n    kwargs.setdefault('y', [0., 1.])\n\n    return _draw_mark(Image, **kwargs)", "response": "Draw an image in the current context figure."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ohlc(*args, **kwargs):\n    if len(args) == 2:\n        kwargs['x'] = args[0]\n        kwargs['y'] = args[1]\n    elif len(args) == 1:\n        kwargs['y'] = args[0]\n        length = len(args[0])\n        kwargs['x'] = arange(length)\n    return _draw_mark(OHLC, **kwargs)", "response": "Draw OHLC bars or candle bars in the current context figure."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef scatter(x, y, **kwargs):\n    kwargs['x'] = x\n    kwargs['y'] = y\n    return _draw_mark(Scatter, **kwargs)", "response": "Draw a scatter in the current context figure."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hist(sample, options={}, **kwargs):\n    kwargs['sample'] = sample\n    scales = kwargs.pop('scales', {})\n    if 'count' not in scales:\n        dimension = _get_attribute_dimension('count', Hist)\n        if dimension in _context['scales']:\n            scales['count'] = _context['scales'][dimension]\n        else:\n            scales['count'] = LinearScale(**options.get('count', {}))\n            _context['scales'][dimension] = scales['count']\n    kwargs['scales'] = scales\n    return _draw_mark(Hist, options=options, **kwargs)", "response": "Draw a histogram in the current context figure."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndrawing a histogram in the current context figure.", "response": "def bin(sample, options={}, **kwargs):\n    \"\"\"Draw a histogram in the current context figure.\n    Parameters\n    ----------\n    sample: numpy.ndarray, 1d\n        The sample for which the histogram must be generated.\n    options: dict (default: {})\n        Options for the scales to be created. If a scale labeled 'x'\n        is required for that mark, options['x'] contains optional keyword\n        arguments for the constructor of the corresponding scale type.\n    axes_options: dict (default: {})\n        Options for the axes to be created. If an axis labeled 'x' is\n        required for that mark, axes_options['x'] contains optional\n        keyword arguments for the constructor of the corresponding axis type.\n    \"\"\"\n    kwargs['sample'] = sample\n    scales = kwargs.pop('scales', {})\n    for xy in ['x', 'y']:\n        if xy not in scales:\n            dimension = _get_attribute_dimension(xy, Bars)\n            if dimension in _context['scales']:\n                scales[xy] = _context['scales'][dimension]\n            else:\n                scales[xy] = LinearScale(**options.get(xy, {}))\n                _context['scales'][dimension] = scales[xy]\n    kwargs['scales'] = scales\n    return _draw_mark(Bins, options=options, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndraw a bar chart in the current context figure.", "response": "def bar(x, y, **kwargs):\n    \"\"\"Draws a bar chart in the current context figure.\n\n    Parameters\n    ----------\n\n    x: numpy.ndarray, 1d\n        The x-coordinates of the data points.\n    y: numpy.ndarray, 1d\n        The y-coordinates of the data pints.\n    options: dict (default: {})\n        Options for the scales to be created. If a scale labeled 'x' is\n        required for that mark, options['x'] contains optional keyword\n        arguments for the constructor of the corresponding scale type.\n    axes_options: dict (default: {})\n        Options for the axes to be created. If an axis labeled 'x' is required\n        for that mark, axes_options['x'] contains optional keyword arguments\n        for the constructor of the corresponding axis type.\n    \"\"\"\n    kwargs['x'] = x\n    kwargs['y'] = y\n    return _draw_mark(Bars, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndraws a boxplot in the current context figure.", "response": "def boxplot(x, y, **kwargs):\n    \"\"\"Draws a boxplot in the current context figure.\n\n    Parameters\n    ----------\n\n    x: numpy.ndarray, 1d\n        The x-coordinates of the data points.\n    y: numpy.ndarray, 2d\n        The data from which the boxes are to be created. Each row of the data\n        corresponds to one box drawn in the plot.\n    options: dict (default: {})\n        Options for the scales to be created. If a scale labeled 'x' is\n        required for that mark, options['x'] contains optional keyword\n        arguments for the constructor of the corresponding scale type.\n    axes_options: dict (default: {})\n        Options for the axes to be created. If an axis labeled 'x' is required\n        for that mark, axes_options['x'] contains optional keyword arguments\n        for the constructor of the corresponding axis type.\n    \"\"\"\n    kwargs['x'] = x\n    kwargs['y'] = y\n    return _draw_mark(Boxplot, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef geo(map_data, **kwargs):\n    scales = kwargs.pop('scales', _context['scales'])\n    options = kwargs.get('options', {})\n    if 'projection' not in scales:\n        scales['projection'] = Mercator(**options.get('projection', {}))\n    kwargs['scales'] = scales\n    if isinstance(map_data, string_types):\n        kwargs['map_data'] = topo_load('map_data/' + map_data + '.json')\n    else:\n        kwargs['map_data'] = map_data\n    return _draw_mark(Map, **kwargs)", "response": "Draw a map in the current context figure."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _add_interaction(int_type, **kwargs):\n\n    fig = kwargs.pop('figure', current_figure())\n    marks = kwargs.pop('marks', [_context['last_mark']])\n\n    for name, traitlet in int_type.class_traits().items():\n        dimension = traitlet.get_metadata('dimension')\n        if dimension is not None:\n            # only scales have this attribute in interactions\n            kwargs[name] = _get_context_scale(dimension)\n    kwargs['marks'] = marks\n    interaction = int_type(**kwargs)\n    if fig.interaction is not None:\n        fig.interaction.close()\n    fig.interaction = interaction\n    return interaction", "response": "Add the interaction for the specified type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _create_selector(int_type, func, trait, **kwargs):\n    interaction = _add_interaction(int_type, **kwargs)\n    if func is not None:\n        interaction.on_trait_change(func, trait)\n    return interaction", "response": "Create a new selector of the specified type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clear():\n    fig = _context['figure']\n    if fig is not None:\n        fig.marks = []\n        fig.axes = []\n        setattr(fig, 'axis_registry', {})\n        _context['scales'] = {}\n        key = _context['current_key']\n        if key is not None:\n            _context['scale_registry'][key] = {}", "response": "Clears the current context figure of all marks axes and grid lines."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the current global context dictionary. All the attributes to be set should be set.", "response": "def set_context(context):\n    \"\"\"Sets the current global context dictionary. All the attributes to be set\n    should be set. Otherwise, it will result in unpredictable behavior.\"\"\"\n    global _context\n    _context = {k: v for k, v in context.items()}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_attribute_dimension(trait_name, mark_type=None):\n    if(mark_type is None):\n        return trait_name\n    scale_metadata = mark_type.class_traits()['scales_metadata']\\\n        .default_args[0]\n    return scale_metadata.get(trait_name, {}).get('dimension', None)", "response": "Returns the dimension for the name of the trait for the specified mark."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\napplying the specified properties to the widget.", "response": "def _apply_properties(widget, properties={}):\n    \"\"\"Applies the specified properties to the widget.\n\n    `properties` is a dictionary with key value pairs corresponding\n    to the properties to be applied to the widget.\n    \"\"\"\n    with widget.hold_sync():\n        for key, value in properties.items():\n            setattr(widget, key, value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of line style color and marker type from a given marker string.", "response": "def _get_line_styles(marker_str):\n    \"\"\"Return line style, color and marker type from specified marker string.\n\n    For example, if ``marker_str`` is 'g-o' then the method returns\n    ``('solid', 'green', 'circle')``.\n    \"\"\"\n    def _extract_marker_value(marker_str, code_dict):\n        \"\"\"Extracts the marker value from a given marker string.\n\n        Looks up the `code_dict` and returns the corresponding marker for a\n        specific code.\n\n        For example if `marker_str` is 'g-o' then the method extracts\n        - 'green' if the code_dict is color_codes,\n        - 'circle' if the code_dict is marker_codes etc.\n        \"\"\"\n        val = None\n        for code in code_dict:\n            if code in marker_str:\n                val = code_dict[code]\n                break\n        return val\n\n    return [_extract_marker_value(marker_str, code_dict) for\n            code_dict in [LINE_STYLE_CODES, COLOR_CODES, MARKER_CODES]]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def collect(self):\n        result = helpers.TotalList()\n        async for message in self:\n            result.append(message)\n\n        result.total = self.total\n        return result", "response": "Create a self iterator and collect it into a TotalList."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles a full API request.", "response": "async def handler(event):\n    \"\"\"#full: Advises to read \"Accessing the full API\" in the docs.\"\"\"\n    await asyncio.wait([\n        event.delete(),\n        event.respond(READ_FULL, reply_to=event.reply_to_msg_id)\n    ])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsearching query in the method reference.", "response": "async def handler(event):\n    \"\"\"#search query: Searches for \"query\" in the method reference.\"\"\"\n    query = urllib.parse.quote(event.pattern_match.group(1))\n    await asyncio.wait([\n        event.delete(),\n        event.respond(SEARCH.format(query), reply_to=event.reply_to_msg_id)\n    ])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def handler(event):\n    rtd = RTFD if event.pattern_match.group(1) else RTD\n    await asyncio.wait([\n        event.delete(),\n        event.respond(rtd, reply_to=event.reply_to_msg_id)\n    ])", "response": "#rtd: Tells the user to please read the docs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhandle a message query.", "response": "async def handler(event):\n    \"\"\"#client or #msg query: Looks for the given attribute in RTD.\"\"\"\n    await event.delete()\n\n    await event.respond(\n        get_docs_message(kind=event.pattern_match.group(1),\n                         query=event.pattern_match.group(2)),\n        reply_to=event.reply_to_msg_id\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhandling an OFFTOPIC event.", "response": "async def handler(event):\n    \"\"\"#ot, #offtopic: Tells the user to move to @TelethonOffTopic.\"\"\"\n    await asyncio.wait([\n        event.delete(),\n        event.respond(OFFTOPIC[event.chat_id], reply_to=event.reply_to_msg_id)\n    ])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_reply_markup(self, buttons, inline_only=False):\n        if buttons is None:\n            return None\n\n        try:\n            if buttons.SUBCLASS_OF_ID == 0xe2e10ef2:\n                return buttons  # crc32(b'ReplyMarkup'):\n        except AttributeError:\n            pass\n\n        if not utils.is_list_like(buttons):\n            buttons = [[buttons]]\n        elif not utils.is_list_like(buttons[0]):\n            buttons = [buttons]\n\n        is_inline = False\n        is_normal = False\n        resize = None\n        single_use = None\n        selective = None\n\n        rows = []\n        for row in buttons:\n            current = []\n            for button in row:\n                if isinstance(button, custom.Button):\n                    if button.resize is not None:\n                        resize = button.resize\n                    if button.single_use is not None:\n                        single_use = button.single_use\n                    if button.selective is not None:\n                        selective = button.selective\n\n                    button = button.button\n                elif isinstance(button, custom.MessageButton):\n                    button = button.button\n\n                inline = custom.Button._is_inline(button)\n                is_inline |= inline\n                is_normal |= not inline\n\n                if button.SUBCLASS_OF_ID == 0xbad74a3:\n                    # 0xbad74a3 == crc32(b'KeyboardButton')\n                    current.append(button)\n\n            if current:\n                rows.append(types.KeyboardButtonRow(current))\n\n        if inline_only and is_normal:\n            raise ValueError('You cannot use non-inline buttons here')\n        elif is_inline == is_normal and is_normal:\n            raise ValueError('You cannot mix inline with normal buttons')\n        elif is_inline:\n            return types.ReplyInlineMarkup(rows)\n        # elif is_normal:\n        return types.ReplyKeyboardMarkup(\n            rows, resize=resize, single_use=single_use, selective=selective)", "response": "Builds a reply markup for the given buttons."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pretty_format(obj, indent=None):\n        if indent is None:\n            if isinstance(obj, TLObject):\n                obj = obj.to_dict()\n\n            if isinstance(obj, dict):\n                return '{}({})'.format(obj.get('_', 'dict'), ', '.join(\n                    '{}={}'.format(k, TLObject.pretty_format(v))\n                    for k, v in obj.items() if k != '_'\n                ))\n            elif isinstance(obj, str) or isinstance(obj, bytes):\n                return repr(obj)\n            elif hasattr(obj, '__iter__'):\n                return '[{}]'.format(\n                    ', '.join(TLObject.pretty_format(x) for x in obj)\n                )\n            else:\n                return repr(obj)\n        else:\n            result = []\n            if isinstance(obj, TLObject):\n                obj = obj.to_dict()\n\n            if isinstance(obj, dict):\n                result.append(obj.get('_', 'dict'))\n                result.append('(')\n                if obj:\n                    result.append('\\n')\n                    indent += 1\n                    for k, v in obj.items():\n                        if k == '_':\n                            continue\n                        result.append('\\t' * indent)\n                        result.append(k)\n                        result.append('=')\n                        result.append(TLObject.pretty_format(v, indent))\n                        result.append(',\\n')\n                    result.pop()  # last ',\\n'\n                    indent -= 1\n                    result.append('\\n')\n                    result.append('\\t' * indent)\n                result.append(')')\n\n            elif isinstance(obj, str) or isinstance(obj, bytes):\n                result.append(repr(obj))\n\n            elif hasattr(obj, '__iter__'):\n                result.append('[\\n')\n                indent += 1\n                for x in obj:\n                    result.append('\\t' * indent)\n                    result.append(TLObject.pretty_format(x, indent))\n                    result.append(',\\n')\n                indent -= 1\n                result.append('\\t' * indent)\n                result.append(']')\n\n            else:\n                result.append(repr(obj))\n\n            return ''.join(result)", "response": "Pretty formats the given object as a string which is returned."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nserialize a single object into a byte string.", "response": "def serialize_bytes(data):\n        \"\"\"Write bytes by using Telegram guidelines\"\"\"\n        if not isinstance(data, bytes):\n            if isinstance(data, str):\n                data = data.encode('utf-8')\n            else:\n                raise TypeError(\n                    'bytes or str expected, not {}'.format(type(data)))\n\n        r = []\n        if len(data) < 254:\n            padding = (len(data) + 1) % 4\n            if padding != 0:\n                padding = 4 - padding\n\n            r.append(bytes([len(data)]))\n            r.append(data)\n\n        else:\n            padding = len(data) % 4\n            if padding != 0:\n                padding = 4 - padding\n\n            r.append(bytes([\n                254,\n                len(data) % 256,\n                (len(data) >> 8) % 256,\n                (len(data) >> 16) % 256\n            ]))\n            r.append(data)\n\n        r.append(bytes(padding))\n        return b''.join(r)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrepresent the current TLE as JSON.", "response": "def to_json(self, fp=None, default=_json_default, **kwargs):\n        \"\"\"\n        Represent the current `TLObject` as JSON.\n\n        If ``fp`` is given, the JSON will be dumped to said\n        file pointer, otherwise a JSON string will be returned.\n\n        Note that bytes and datetimes cannot be represented\n        in JSON, so if those are found, they will be base64\n        encoded and ISO-formatted, respectively, by default.\n        \"\"\"\n        d = self.to_dict()\n        if fp:\n            return json.dump(d, fp, default=default, **kwargs)\n        else:\n            return json.dumps(d, default=default, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def _parse_message_text(self, message, parse_mode):\n        if parse_mode is ():\n            parse_mode = self._parse_mode\n        else:\n            parse_mode = utils.sanitize_parse_mode(parse_mode)\n\n        if not parse_mode:\n            return message, []\n\n        message, msg_entities = parse_mode.parse(message)\n        for i in reversed(range(len(msg_entities))):\n            e = msg_entities[i]\n            if isinstance(e, types.MessageEntityTextUrl):\n                m = re.match(r'^@|\\+|tg://user\\?id=(\\d+)', e.url)\n                if m:\n                    user = int(m.group(1)) if m.group(1) else e.url\n                    is_mention = await self._replace_with_mention(msg_entities, i, user)\n                    if not is_mention:\n                        del msg_entities[i]\n            elif isinstance(e, (types.MessageEntityMentionName,\n                                types.InputMessageEntityMentionName)):\n                is_mention = await self._replace_with_mention(msg_entities, i, e.user_id)\n                if not is_mention:\n                    del msg_entities[i]\n\n        return message, msg_entities", "response": "Parses the text of a message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting the response message from the result of a request and Update result.", "response": "def _get_response_message(self, request, result, input_chat):\n        \"\"\"\n        Extracts the response message known a request and Update result.\n        The request may also be the ID of the message to match.\n\n        If ``request is None`` this method returns ``{id: message}``.\n\n        If ``request.random_id`` is a list, this method returns a list too.\n        \"\"\"\n        if isinstance(result, types.UpdateShort):\n            updates = [result.update]\n            entities = {}\n        elif isinstance(result, (types.Updates, types.UpdatesCombined)):\n            updates = result.updates\n            entities = {utils.get_peer_id(x): x\n                        for x in\n                        itertools.chain(result.users, result.chats)}\n        else:\n            return None\n\n        random_to_id = {}\n        id_to_message = {}\n        for update in updates:\n            if isinstance(update, types.UpdateMessageID):\n                random_to_id[update.random_id] = update.id\n\n            elif isinstance(update, (\n                    types.UpdateNewChannelMessage, types.UpdateNewMessage)):\n                update.message._finish_init(self, entities, input_chat)\n                id_to_message[update.message.id] = update.message\n\n            elif (isinstance(update, types.UpdateEditMessage)\n                  and not isinstance(request.peer, types.InputPeerChannel)):\n                if request.id == update.message.id:\n                    update.message._finish_init(self, entities, input_chat)\n                    return update.message\n\n            elif (isinstance(update, types.UpdateEditChannelMessage)\n                  and utils.get_peer_id(request.peer) ==\n                  utils.get_peer_id(update.message.to_id)):\n                if request.id == update.message.id:\n                    update.message._finish_init(self, entities, input_chat)\n                    return update.message\n\n        if request is None:\n            return id_to_message\n\n        random_id = request if isinstance(request, int) else request.random_id\n        if not utils.is_list_like(random_id):\n            if random_id in random_to_id:\n                return id_to_message[random_to_id[random_id]]\n            else:\n                return None\n        else:\n            # ``rnd in random_to_id`` is needed because trying to forward only\n            # deleted messages causes `MESSAGE_ID_INVALID`, but forwarding\n            # valid and invalid messages in the same call makes the call\n            # succeed, although the API won't return those messages thus\n            # `random_to_id[rnd]` would `KeyError`.\n            return [id_to_message[random_to_id[rnd]]\n                    if rnd in random_to_id else None\n                    for rnd in random_id]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def get_sender(self):\n        # ``sender.min`` is present both in :tl:`User` and :tl:`Channel`.\n        # It's a flag that will be set if only minimal information is\n        # available (such as display name, but username may be missing),\n        # in which case we want to force fetch the entire thing because\n        # the user explicitly called a method. If the user is okay with\n        # cached information, they may use the property instead.\n        if (self._sender is None or self._sender.min) \\\n                and await self.get_input_sender():\n            try:\n                self._sender =\\\n                    await self._client.get_entity(self._input_sender)\n            except ValueError:\n                await self._reload_message()\n        return self._sender", "response": "Returns the sender if it s not already cached."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef input_sender(self):\n        if self._input_sender is None and self._sender_id:\n            try:\n                self._input_sender = self._client.session\\\n                    .get_input_entity(self._sender_id)\n            except ValueError:\n                pass\n        return self._input_sender", "response": "This is the input version of the input chat who has the message sent."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def get_input_sender(self):\n        if self.input_sender is None and self._sender_id:\n            await self._refetch_sender()\n        return self._input_sender", "response": "Returns the input sender if it s not already cached."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unregister(callback, event=None):\n    found = 0\n    if event and not isinstance(event, type):\n        event = type(event)\n\n    handlers = getattr(callback, _HANDLERS_ATTRIBUTE, [])\n    handlers.append((event, callback))\n    i = len(handlers)\n    while i:\n        i -= 1\n        ev = handlers[i]\n        if not event or isinstance(ev, event):\n            del handlers[i]\n            found += 1\n\n    return found", "response": "Unregister a callback from the base base."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(message, delimiters=None, url_re=None):\n    if not message:\n        return message, []\n\n    if url_re is None:\n        url_re = DEFAULT_URL_RE\n    elif isinstance(url_re, str):\n        url_re = re.compile(url_re)\n\n    if not delimiters:\n        if delimiters is not None:\n            return message, []\n        delimiters = DEFAULT_DELIMITERS\n\n    # Cannot use a for loop because we need to skip some indices\n    i = 0\n    result = []\n    current = None\n    end_delimiter = None\n\n    # Work on byte level with the utf-16le encoding to get the offsets right.\n    # The offset will just be half the index we're at.\n    message = add_surrogate(message)\n    while i < len(message):\n        if url_re and current is None:\n            # If we're not inside a previous match since Telegram doesn't allow\n            # nested message entities, try matching the URL from the i'th pos.\n            url_match = url_re.match(message, pos=i)\n            if url_match:\n                # Replace the whole match with only the inline URL text.\n                message = ''.join((\n                    message[:url_match.start()],\n                    url_match.group(1),\n                    message[url_match.end():]\n                ))\n\n                result.append(MessageEntityTextUrl(\n                    offset=url_match.start(), length=len(url_match.group(1)),\n                    url=del_surrogate(url_match.group(2))\n                ))\n                i += len(url_match.group(1))\n                # Next loop iteration, don't check delimiters, since\n                # a new inline URL might be right after this one.\n                continue\n\n        if end_delimiter is None:\n            # We're not expecting any delimiter, so check them all\n            for d, m in delimiters.items():\n                # Slice the string at the current i'th position to see if\n                # it matches the current delimiter d, otherwise skip it.\n                if message[i:i + len(d)] != d:\n                    continue\n\n                if message[i + len(d):i + 2 * len(d)] == d:\n                    # The same delimiter can't be right afterwards, if\n                    # this were the case we would match empty strings\n                    # like `` which we don't want to.\n                    continue\n\n                # Get rid of the delimiter by slicing it away\n                message = message[:i] + message[i + len(d):]\n                if m == MessageEntityPre:\n                    # Special case, also has 'lang'\n                    current = m(i, None, '')\n                else:\n                    current = m(i, None)\n\n                end_delimiter = d  # We expect the same delimiter.\n                break\n\n        elif message[i:i + len(end_delimiter)] == end_delimiter:\n            message = message[:i] + message[i + len(end_delimiter):]\n            current.length = i - current.offset\n            result.append(current)\n            current, end_delimiter = None, None\n            # Don't increment i here as we matched a delimiter,\n            # and there may be a new one right after. This is\n            # different than when encountering the first delimiter,\n            # as we already know there won't be the same right after.\n            continue\n\n        # Next iteration\n        i += 1\n\n    # We may have found some a delimiter but not its ending pair.\n    # If this is the case, we want to insert the delimiter character back.\n    if current is not None:\n        message = (\n            message[:current.offset]\n            + end_delimiter\n            + message[current.offset:]\n        )\n\n    message = strip_text(message, result)\n    return del_surrogate(message), result", "response": "Parses a given markdown message and returns its stripped representation and a list of MessageEntities that were found."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nperform the reverse operation to .parse(), effectively returning markdown-like syntax given a normal text and its MessageEntity's. :param text: the text to be reconverted into markdown. :param entities: the MessageEntity's applied to the text. :return: a markdown-like text representing the combination of both inputs.", "response": "def unparse(text, entities, delimiters=None, url_fmt=None):\n    \"\"\"\n    Performs the reverse operation to .parse(), effectively returning\n    markdown-like syntax given a normal text and its MessageEntity's.\n\n    :param text: the text to be reconverted into markdown.\n    :param entities: the MessageEntity's applied to the text.\n    :return: a markdown-like text representing the combination of both inputs.\n    \"\"\"\n    if not text or not entities:\n        return text\n\n    if not delimiters:\n        if delimiters is not None:\n            return text\n        delimiters = DEFAULT_DELIMITERS\n\n    if url_fmt is None:\n        url_fmt = DEFAULT_URL_FORMAT\n\n    if isinstance(entities, TLObject):\n        entities = (entities,)\n    else:\n        entities = tuple(sorted(entities, key=lambda e: e.offset, reverse=True))\n\n    text = add_surrogate(text)\n    delimiters = {v: k for k, v in delimiters.items()}\n    for entity in entities:\n        s = entity.offset\n        e = entity.offset + entity.length\n        delimiter = delimiters.get(type(entity), None)\n        if delimiter:\n            text = text[:s] + delimiter + text[s:e] + delimiter + text[e:]\n        elif url_fmt:\n            url = None\n            if isinstance(entity, MessageEntityTextUrl):\n                url = entity.url\n            elif isinstance(entity, MessageEntityMentionName):\n                url = 'tg://user?id={}'.format(entity.user_id)\n            if url:\n                # It's possible that entities are malformed and end up in the\n                # middle of some character, like emoji, by using malformed\n                # clients or bots. Try decoding the current one to check if\n                # this is the case, and if it is, advance the entity.\n                while e <= len(text):\n                    try:\n                        del_surrogate(text[s:e])\n                        break\n                    except UnicodeDecodeError:\n                        e += 1\n                else:\n                    # Out of bounds, no luck going forward\n                    while e > s:\n                        try:\n                            del_surrogate(text[s:e])\n                            break\n                        except UnicodeDecodeError:\n                            e -= 1\n                    else:\n                        # No luck going backwards either, ignore entity\n                        continue\n\n                text = (\n                    text[:s] +\n                    add_surrogate(url_fmt.format(text[s:e], url)) +\n                    text[e:]\n                )\n\n    return del_surrogate(text)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the new nonce hash based on the current attributes.", "response": "def calc_new_nonce_hash(self, new_nonce, number):\n        \"\"\"\n        Calculates the new nonce hash based on the current attributes.\n\n        :param new_nonce: the new nonce to be hashed.\n        :param number: number to prepend before the hash.\n        :return: the hash for the given new nonce.\n        \"\"\"\n        new_nonce = new_nonce.to_bytes(32, 'little', signed=True)\n        data = new_nonce + struct.pack('<BQ', number, self.aux_hash)\n\n        # Calculates the message key from the given data\n        return int.from_bytes(sha1(data).digest()[4:20], 'little', signed=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the variable length bytes corresponding to the given integer", "response": "def get_byte_array(integer):\n    \"\"\"Return the variable length bytes corresponding to the given int\"\"\"\n    # Operate in big endian (unlike most of Telegram API) since:\n    # > \"...pq is a representation of a natural number\n    #    (in binary *big endian* format)...\"\n    # > \"...current value of dh_prime equals\n    #    (in *big-endian* byte order)...\"\n    # Reference: https://core.telegram.org/mtproto/auth_key\n    return int.to_bytes(\n        integer,\n        (integer.bit_length() + 8 - 1) // 8,  # 8 bits per byte,\n        byteorder='big',\n        signed=False\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compute_fingerprint(key):\n    n = TLObject.serialize_bytes(get_byte_array(key.n))\n    e = TLObject.serialize_bytes(get_byte_array(key.e))\n    # Telegram uses the last 8 bytes as the fingerprint\n    return struct.unpack('<q', sha1(n + e).digest()[-8:])[0]", "response": "Given a RSA key computes its 8 - byte - long fingerprint."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_key(pub):\n    global _server_keys\n    key = rsa.PublicKey.load_pkcs1(pub)\n    _server_keys[_compute_fingerprint(key)] = key", "response": "Adds a new public key to be used when encrypting new data is needed"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef encrypt(fingerprint, data):\n    global _server_keys\n    key = _server_keys.get(fingerprint, None)\n    if not key:\n        return None\n\n    # len(sha1.digest) is always 20, so we're left with 255 - 20 - x padding\n    to_encrypt = sha1(data).digest() + data + os.urandom(235 - len(data))\n\n    # rsa module rsa.encrypt adds 11 bits for padding which we don't want\n    # rsa module uses rsa.transform.bytes2int(to_encrypt), easier way:\n    payload = int.from_bytes(to_encrypt, 'big')\n    encrypted = rsa.core.encrypt_int(payload, key.e, key.n)\n    # rsa module uses transform.int2bytes(encrypted, keylength), easier:\n    block = encrypted.to_bytes(256, 'big')\n    return block", "response": "Encrypts the given data using the RSA key matching the given fingerprint."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def send_message(self, *args, **kwargs):\n        message = await self._client.send_message(\n            self._input_chat, *args, **kwargs)\n\n        self._outgoing.add(message.id)\n        self._last_outgoing = message.id\n        return message", "response": "Sends a message in the context of this conversation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmark the user as read.", "response": "def mark_read(self, message=None):\n        \"\"\"\n        Marks as read the latest received message if ``message is None``.\n        Otherwise, marks as read until the given message (or message ID).\n\n        This is equivalent to calling `client.send_read_acknowledge\n        <telethon.client.messages.MessageMethods.send_read_acknowledge>`.\n        \"\"\"\n        if message is None:\n            if self._incoming:\n                message = self._incoming[-1].id\n            else:\n                message = 0\n        elif not isinstance(message, int):\n            message = message.id\n\n        return self._client.send_read_acknowledge(\n            self._input_chat, max_id=message)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a coroutine that will resolve once a response arrives.", "response": "async def get_response(self, message=None, *, timeout=None):\n        \"\"\"\n        Returns a coroutine that will resolve once a response arrives.\n\n        Args:\n            message (`Message <telethon.tl.custom.message.Message>` | `int`, optional):\n                The message (or the message ID) for which a response\n                is expected. By default this is the last sent message.\n\n            timeout (`int` | `float`, optional):\n                If present, this `timeout` (in seconds) will override the\n                per-action timeout defined for the conversation.\n        \"\"\"\n        return await self._get_message(\n            message, self._response_indices, self._pending_responses, timeout,\n            lambda x, y: True\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def get_reply(self, message=None, *, timeout=None):\n        return await self._get_message(\n            message, self._reply_indices, self._pending_replies, timeout,\n            lambda x, y: x.reply_to_msg_id == y\n        )", "response": "Get a reply from the specified message."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_message(\n            self, target_message, indices, pending, timeout, condition):\n        \"\"\"\n        Gets the next desired message under the desired condition.\n\n        Args:\n            target_message (`object`):\n                The target message for which we want to find another\n                response that applies based on `condition`.\n\n            indices (`dict`):\n                This dictionary remembers the last ID chosen for the\n                input `target_message`.\n\n            pending (`dict`):\n                This dictionary remembers {msg_id: Future} to be set\n                once `condition` is met.\n\n            timeout (`int`):\n                The timeout (in seconds) override to use for this operation.\n\n            condition (`callable`):\n                The condition callable that checks if an incoming\n                message is a valid response.\n        \"\"\"\n        start_time = time.time()\n        target_id = self._get_message_id(target_message)\n\n        # If there is no last-chosen ID, make sure to pick one *after*\n        # the input message, since we don't want responses back in time\n        if target_id not in indices:\n            for i, incoming in enumerate(self._incoming):\n                if incoming.id > target_id:\n                    indices[target_id] = i\n                    break\n            else:\n                indices[target_id] = len(self._incoming)\n\n        # We will always return a future from here, even if the result\n        # can be set immediately. Otherwise, needing to await only\n        # sometimes is an annoying edge case (i.e. we would return\n        # a `Message` but `get_response()` always `await`'s).\n        future = self._client.loop.create_future()\n\n        # If there are enough responses saved return the next one\n        last_idx = indices[target_id]\n        if last_idx < len(self._incoming):\n            incoming = self._incoming[last_idx]\n            if condition(incoming, target_id):\n                indices[target_id] += 1\n                future.set_result(incoming)\n                return future\n\n        # Otherwise the next incoming response will be the one to use\n        pending[target_id] = future\n        return self._get_result(future, start_time, timeout)", "response": "Internal method that returns the next desired message that applies to the target message."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def get_edit(self, message=None, *, timeout=None):\n        start_time = time.time()\n        target_id = self._get_message_id(message)\n\n        target_date = self._edit_dates.get(target_id, 0)\n        earliest_edit = min(\n            (x for x in self._incoming\n             if x.edit_date\n             and x.id > target_id\n             and x.edit_date.timestamp() > target_date\n             ),\n            key=lambda x: x.edit_date.timestamp(),\n            default=None\n        )\n\n        if earliest_edit and earliest_edit.edit_date.timestamp() > target_date:\n            self._edit_dates[target_id] = earliest_edit.edit_date.timestamp()\n            return earliest_edit\n\n        # Otherwise the next incoming response will be the one to use\n        future = asyncio.Future(loop=self._client.loop)\n        self._pending_edits[target_id] = future\n        return await self._get_result(future, start_time, timeout)", "response": "Await for an edit after the last message arrives."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nawait for the sent message to be read.", "response": "async def wait_read(self, message=None, *, timeout=None):\n        \"\"\"\n        Awaits for the sent message to be read. Note that receiving\n        a response doesn't imply the message was read, and this action\n        will also trigger even without a response.\n        \"\"\"\n        start_time = time.time()\n        future = self._client.loop.create_future()\n        target_id = self._get_message_id(message)\n\n        if self._last_read is None:\n            self._last_read = target_id - 1\n\n        if self._last_read >= target_id:\n            return\n\n        self._pending_reads[target_id] = future\n        return await self._get_result(future, start_time, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwaiting for a custom event to occur.", "response": "async def wait_event(self, event, *, timeout=None):\n        \"\"\"\n        Waits for a custom event to occur. Timeouts still apply.\n\n        Unless you're certain that your code will run fast enough,\n        generally you should get a \"handle\" of this special coroutine\n        before acting. Generally, you should do this:\n\n        >>> from telethon import TelegramClient, events\n        >>>\n        >>> client = TelegramClient(...)\n        >>>\n        >>> async def main():\n        >>>     async with client.conversation(...) as conv:\n        >>>         response = conv.wait_event(events.NewMessage(incoming=True))\n        >>>         await conv.send_message('Hi')\n        >>>         response = await response\n\n        This way your event can be registered before acting,\n        since the response may arrive before your event was\n        registered. It depends on your use case since this\n        also means the event can arrive before you send\n        a previous action.\n        \"\"\"\n        start_time = time.time()\n        if isinstance(event, type):\n            event = event()\n\n        await event.resolve(self._client)\n\n        counter = Conversation._custom_counter\n        Conversation._custom_counter += 1\n\n        future = asyncio.Future(loop=self._client.loop)\n\n        # We need the `async def` here because we want to block on the future\n        # from `_get_result` by using `await` on it. If we returned the future\n        # immediately we would `del` from `_custom` too early.\n\n        async def result():\n            try:\n                return await self._get_result(future, start_time, timeout)\n            finally:\n                del self._custom[counter]\n\n        self._custom[counter] = (event, future)\n        return await result()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstarting the event loop.", "response": "def start(\n            self,\n            phone=lambda: input('Please enter your phone (or bot token): '),\n            password=lambda: getpass.getpass('Please enter your password: '),\n            *,\n            bot_token=None, force_sms=False, code_callback=None,\n            first_name='New User', last_name='', max_attempts=3):\n        \"\"\"\n        Convenience method to interactively connect and sign in if required,\n        also taking into consideration that 2FA may be enabled in the account.\n\n        If the phone doesn't belong to an existing account (and will hence\n        `sign_up` for a new one),  **you are agreeing to Telegram's\n        Terms of Service. This is required and your account\n        will be banned otherwise.** See https://telegram.org/tos\n        and https://core.telegram.org/api/terms.\n\n        Example usage:\n            >>> client = ...\n            >>> client.start(phone)\n            Please enter the code you received: 12345\n            Please enter your password: *******\n            (You are now logged in)\n\n        If the event loop is already running, this method returns a\n        coroutine that you should await on your own code; otherwise\n        the loop is ran until said coroutine completes.\n\n        Args:\n            phone (`str` | `int` | `callable`):\n                The phone (or callable without arguments to get it)\n                to which the code will be sent. If a bot-token-like\n                string is given, it will be used as such instead.\n                The argument may be a coroutine.\n\n            password (`str`, `callable`, optional):\n                The password for 2 Factor Authentication (2FA).\n                This is only required if it is enabled in your account.\n                The argument may be a coroutine.\n\n            bot_token (`str`):\n                Bot Token obtained by `@BotFather <https://t.me/BotFather>`_\n                to log in as a bot. Cannot be specified with ``phone`` (only\n                one of either allowed).\n\n            force_sms (`bool`, optional):\n                Whether to force sending the code request as SMS.\n                This only makes sense when signing in with a `phone`.\n\n            code_callback (`callable`, optional):\n                A callable that will be used to retrieve the Telegram\n                login code. Defaults to `input()`.\n                The argument may be a coroutine.\n\n            first_name (`str`, optional):\n                The first name to be used if signing up. This has no\n                effect if the account already exists and you sign in.\n\n            last_name (`str`, optional):\n                Similar to the first name, but for the last. Optional.\n\n            max_attempts (`int`, optional):\n                How many times the code/password callback should be\n                retried or switching between signing in and signing up.\n\n        Returns:\n            This `TelegramClient`, so initialization\n            can be chained with ``.start()``.\n        \"\"\"\n        if code_callback is None:\n            def code_callback():\n                return input('Please enter the code you received: ')\n        elif not callable(code_callback):\n            raise ValueError(\n                'The code_callback parameter needs to be a callable '\n                'function that returns the code you received by Telegram.'\n            )\n\n        if not phone and not bot_token:\n            raise ValueError('No phone number or bot token provided.')\n\n        if phone and bot_token and not callable(phone):\n            raise ValueError('Both a phone and a bot token provided, '\n                             'must only provide one of either')\n\n        coro = self._start(\n            phone=phone,\n            password=password,\n            bot_token=bot_token,\n            force_sms=force_sms,\n            code_callback=code_callback,\n            first_name=first_name,\n            last_name=last_name,\n            max_attempts=max_attempts\n        )\n        return (\n            coro if self.loop.is_running()\n            else self.loop.run_until_complete(coro)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting or completes the sign in process with the given phone number or code.", "response": "async def sign_in(\n            self, phone=None, code=None, *, password=None,\n            bot_token=None, phone_code_hash=None):\n        \"\"\"\n        Starts or completes the sign in process with the given phone number\n        or code that Telegram sent.\n\n        Args:\n            phone (`str` | `int`):\n                The phone to send the code to if no code was provided,\n                or to override the phone that was previously used with\n                these requests.\n\n            code (`str` | `int`):\n                The code that Telegram sent. Note that if you have sent this\n                code through the application itself it will immediately\n                expire. If you want to send the code, obfuscate it somehow.\n                If you're not doing any of this you can ignore this note.\n\n            password (`str`):\n                2FA password, should be used if a previous call raised\n                SessionPasswordNeededError.\n\n            bot_token (`str`):\n                Used to sign in as a bot. Not all requests will be available.\n                This should be the hash the @BotFather gave you.\n\n            phone_code_hash (`str`, optional):\n                The hash returned by `send_code_request`. This can be left as\n                ``None`` to use the last hash known for the phone to be used.\n\n        Returns:\n            The signed in user, or the information about\n            :meth:`send_code_request`.\n        \"\"\"\n        me = await self.get_me()\n        if me:\n            return me\n\n        if phone and not code and not password:\n            return await self.send_code_request(phone)\n        elif code:\n            phone, phone_code_hash = \\\n                self._parse_phone_and_hash(phone, phone_code_hash)\n\n            # May raise PhoneCodeEmptyError, PhoneCodeExpiredError,\n            # PhoneCodeHashEmptyError or PhoneCodeInvalidError.\n            result = await self(functions.auth.SignInRequest(\n                phone, phone_code_hash, str(code)))\n        elif password:\n            pwd = await self(functions.account.GetPasswordRequest())\n            result = await self(functions.auth.CheckPasswordRequest(\n                pwd_mod.compute_check(pwd, password)\n            ))\n        elif bot_token:\n            result = await self(functions.auth.ImportBotAuthorizationRequest(\n                flags=0, bot_auth_token=bot_token,\n                api_id=self.api_id, api_hash=self.api_hash\n            ))\n        else:\n            raise ValueError(\n                'You must provide a phone and a code the first time, '\n                'and a password only if an RPCError was raised before.'\n            )\n\n        return self._on_login(result.user)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsign up to Telegram.", "response": "async def sign_up(self, code, first_name, last_name='',\n                      *, phone=None, phone_code_hash=None):\n        \"\"\"\n        Signs up to Telegram if you don't have an account yet.\n        You must call .send_code_request(phone) first.\n\n        **By using this method you're agreeing to Telegram's\n        Terms of Service. This is required and your account\n        will be banned otherwise.** See https://telegram.org/tos\n        and https://core.telegram.org/api/terms.\n\n        Args:\n            code (`str` | `int`):\n                The code sent by Telegram\n\n            first_name (`str`):\n                The first name to be used by the new account.\n\n            last_name (`str`, optional)\n                Optional last name.\n\n            phone (`str` | `int`, optional):\n                The phone to sign up. This will be the last phone used by\n                default (you normally don't need to set this).\n\n            phone_code_hash (`str`, optional):\n                The hash returned by `send_code_request`. This can be left as\n                ``None`` to use the last hash known for the phone to be used.\n\n        Returns:\n            The new created :tl:`User`.\n        \"\"\"\n        me = await self.get_me()\n        if me:\n            return me\n\n        if self._tos and self._tos.text:\n            if self.parse_mode:\n                t = self.parse_mode.unparse(self._tos.text, self._tos.entities)\n            else:\n                t = self._tos.text\n            sys.stderr.write(\"{}\\n\".format(t))\n            sys.stderr.flush()\n\n        phone, phone_code_hash = \\\n            self._parse_phone_and_hash(phone, phone_code_hash)\n\n        result = await self(functions.auth.SignUpRequest(\n            phone_number=phone,\n            phone_code_hash=phone_code_hash,\n            phone_code=str(code),\n            first_name=first_name,\n            last_name=last_name\n        ))\n\n        if self._tos:\n            await self(\n                functions.help.AcceptTermsOfServiceRequest(self._tos.id))\n\n        return self._on_login(result.user)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _on_login(self, user):\n        self._bot = bool(user.bot)\n        self._self_input_peer = utils.get_input_peer(user, allow_self=False)\n        self._authorized = True\n\n        return user", "response": "Called when the login or sign up process completes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend a code request to the specified phone number.", "response": "async def send_code_request(self, phone, *, force_sms=False):\n        \"\"\"\n        Sends a code request to the specified phone number.\n\n        Args:\n            phone (`str` | `int`):\n                The phone to which the code will be sent.\n\n            force_sms (`bool`, optional):\n                Whether to force sending as SMS.\n\n        Returns:\n            An instance of :tl:`SentCode`.\n        \"\"\"\n        phone = utils.parse_phone(phone) or self._phone\n        phone_hash = self._phone_code_hash.get(phone)\n\n        if not phone_hash:\n            try:\n                result = await self(functions.auth.SendCodeRequest(\n                    phone, self.api_id, self.api_hash, types.CodeSettings()))\n            except errors.AuthRestartError:\n                return self.send_code_request(phone, force_sms=force_sms)\n\n            self._tos = result.terms_of_service\n            self._phone_code_hash[phone] = phone_hash = result.phone_code_hash\n        else:\n            force_sms = True\n\n        self._phone = phone\n\n        if force_sms:\n            result = await self(\n                functions.auth.ResendCodeRequest(phone, phone_hash))\n\n            self._phone_code_hash[phone] = result.phone_code_hash\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def log_out(self):\n        try:\n            await self(functions.auth.LogOutRequest())\n        except errors.RPCError:\n            return False\n\n        self._bot = None\n        self._self_input_peer = None\n        self._authorized = False\n        self._state_cache.reset()\n\n        await self.disconnect()\n        self.session.delete()\n        return True", "response": "Logs out Telegram and deletes the current. session file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def edit_2fa(\n            self, current_password=None, new_password=None,\n            *, hint='', email=None, email_code_callback=None):\n        \"\"\"\n        Changes the 2FA settings of the logged in user, according to the\n        passed parameters. Take note of the parameter explanations.\n\n        Note that this method may be *incredibly* slow depending on the\n        prime numbers that must be used during the process to make sure\n        that everything is safe.\n\n        Has no effect if both current and new password are omitted.\n\n        current_password (`str`, optional):\n            The current password, to authorize changing to ``new_password``.\n            Must be set if changing existing 2FA settings.\n            Must **not** be set if 2FA is currently disabled.\n            Passing this by itself will remove 2FA (if correct).\n\n        new_password (`str`, optional):\n            The password to set as 2FA.\n            If 2FA was already enabled, ``current_password`` **must** be set.\n            Leaving this blank or ``None`` will remove the password.\n\n        hint (`str`, optional):\n            Hint to be displayed by Telegram when it asks for 2FA.\n            Leaving unspecified is highly discouraged.\n            Has no effect if ``new_password`` is not set.\n\n        email (`str`, optional):\n            Recovery and verification email. If present, you must also\n            set `email_code_callback`, else it raises ``ValueError``.\n\n        email_code_callback (`callable`, optional):\n            If an email is provided, a callback that returns the code sent\n            to it must also be set. This callback may be asynchronous.\n            It should return a string with the code. The length of the\n            code will be passed to the callback as an input parameter.\n\n            If the callback returns an invalid code, it will raise\n            ``CodeInvalidError``.\n\n        Returns:\n            ``True`` if successful, ``False`` otherwise.\n        \"\"\"\n        if new_password is None and current_password is None:\n            return False\n\n        if email and not callable(email_code_callback):\n            raise ValueError('email present without email_code_callback')\n\n        pwd = await self(functions.account.GetPasswordRequest())\n        pwd.new_algo.salt1 += os.urandom(32)\n        assert isinstance(pwd, types.account.Password)\n        if not pwd.has_password and current_password:\n            current_password = None\n\n        if current_password:\n            password = pwd_mod.compute_check(pwd, current_password)\n        else:\n            password = types.InputCheckPasswordEmpty()\n\n        if new_password:\n            new_password_hash = pwd_mod.compute_digest(\n                pwd.new_algo, new_password)\n        else:\n            new_password_hash = b''\n\n        try:\n            await self(functions.account.UpdatePasswordSettingsRequest(\n                password=password,\n                new_settings=types.account.PasswordInputSettings(\n                    new_algo=pwd.new_algo,\n                    new_password_hash=new_password_hash,\n                    hint=hint,\n                    email=email,\n                    new_secure_settings=None\n                )\n            ))\n        except errors.EmailUnconfirmedError as e:\n            code = email_code_callback(e.code_length)\n            if inspect.isawaitable(code):\n                code = await code\n\n            code = str(code)\n            await self(functions.account.ConfirmPasswordEmailRequest(code))\n\n        return True", "response": "Edit the 2FA settings of the logged in user."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the class name for the given error code.", "response": "def _get_class_name(error_code):\n    \"\"\"\n    Gets the corresponding class name for the given error code,\n    this either being an integer (thus base error name) or str.\n    \"\"\"\n    if isinstance(error_code, int):\n        return KNOWN_BASE_CLASSES.get(\n            error_code, 'RPCError' + str(error_code).replace('-', 'Neg')\n        )\n\n    return snake_to_camel_case(\n        error_code.replace('FIRSTNAME', 'FIRST_NAME').lower(), suffix='Error')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_errors(csv_file):\n    with csv_file.open(newline='') as f:\n        f = csv.reader(f)\n        next(f, None)  # header\n        for line, tup in enumerate(f, start=2):\n            try:\n                name, codes, description = tup\n            except ValueError:\n                raise ValueError('Columns count mismatch, unquoted comma in '\n                                 'desc? (line {})'.format(line)) from None\n\n            try:\n                codes = [int(x) for x in codes.split()] or [400]\n            except ValueError:\n                raise ValueError('Not all codes are integers '\n                                 '(line {})'.format(line)) from None\n\n            yield Error([int(x) for x in codes], name, description)", "response": "Parses the input CSV file with columns name error codes description and yields Error instances as a result."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def do_authentication(sender):\n    # Step 1 sending: PQ Request, endianness doesn't matter since it's random\n    nonce = int.from_bytes(os.urandom(16), 'big', signed=True)\n    res_pq = await sender.send(ReqPqMultiRequest(nonce))\n    assert isinstance(res_pq, ResPQ), 'Step 1 answer was %s' % res_pq\n\n    if res_pq.nonce != nonce:\n        raise SecurityError('Step 1 invalid nonce from server')\n\n    pq = get_int(res_pq.pq)\n\n    # Step 2 sending: DH Exchange\n    p, q = Factorization.factorize(pq)\n    p, q = rsa.get_byte_array(p), rsa.get_byte_array(q)\n    new_nonce = int.from_bytes(os.urandom(32), 'little', signed=True)\n\n    pq_inner_data = bytes(PQInnerData(\n        pq=rsa.get_byte_array(pq), p=p, q=q,\n        nonce=res_pq.nonce,\n        server_nonce=res_pq.server_nonce,\n        new_nonce=new_nonce\n    ))\n\n    # sha_digest + data + random_bytes\n    cipher_text, target_fingerprint = None, None\n    for fingerprint in res_pq.server_public_key_fingerprints:\n        cipher_text = rsa.encrypt(fingerprint, pq_inner_data)\n        if cipher_text is not None:\n            target_fingerprint = fingerprint\n            break\n\n    if cipher_text is None:\n        raise SecurityError(\n            'Step 2 could not find a valid key for fingerprints: {}'\n            .format(', '.join(\n                [str(f) for f in res_pq.server_public_key_fingerprints])\n            )\n        )\n\n    server_dh_params = await sender.send(ReqDHParamsRequest(\n        nonce=res_pq.nonce,\n        server_nonce=res_pq.server_nonce,\n        p=p, q=q,\n        public_key_fingerprint=target_fingerprint,\n        encrypted_data=cipher_text\n    ))\n\n    assert isinstance(\n        server_dh_params, (ServerDHParamsOk, ServerDHParamsFail)),\\\n        'Step 2.1 answer was %s' % server_dh_params\n\n    if server_dh_params.nonce != res_pq.nonce:\n        raise SecurityError('Step 2 invalid nonce from server')\n\n    if server_dh_params.server_nonce != res_pq.server_nonce:\n        raise SecurityError('Step 2 invalid server nonce from server')\n\n    if isinstance(server_dh_params, ServerDHParamsFail):\n        nnh = int.from_bytes(\n            sha1(new_nonce.to_bytes(32, 'little', signed=True)).digest()[4:20],\n            'little', signed=True\n        )\n        if server_dh_params.new_nonce_hash != nnh:\n            raise SecurityError('Step 2 invalid DH fail nonce from server')\n\n    assert isinstance(server_dh_params, ServerDHParamsOk),\\\n        'Step 2.2 answer was %s' % server_dh_params\n\n    # Step 3 sending: Complete DH Exchange\n    key, iv = helpers.generate_key_data_from_nonce(\n        res_pq.server_nonce, new_nonce\n    )\n    if len(server_dh_params.encrypted_answer) % 16 != 0:\n        # See PR#453\n        raise SecurityError('Step 3 AES block size mismatch')\n\n    plain_text_answer = AES.decrypt_ige(\n        server_dh_params.encrypted_answer, key, iv\n    )\n\n    with BinaryReader(plain_text_answer) as reader:\n        reader.read(20)  # hash sum\n        server_dh_inner = reader.tgread_object()\n        assert isinstance(server_dh_inner, ServerDHInnerData),\\\n            'Step 3 answer was %s' % server_dh_inner\n\n    if server_dh_inner.nonce != res_pq.nonce:\n        raise SecurityError('Step 3 Invalid nonce in encrypted answer')\n\n    if server_dh_inner.server_nonce != res_pq.server_nonce:\n        raise SecurityError('Step 3 Invalid server nonce in encrypted answer')\n\n    dh_prime = get_int(server_dh_inner.dh_prime, signed=False)\n    g_a = get_int(server_dh_inner.g_a, signed=False)\n    time_offset = server_dh_inner.server_time - int(time.time())\n\n    b = get_int(os.urandom(256), signed=False)\n    gb = pow(server_dh_inner.g, b, dh_prime)\n    gab = pow(g_a, b, dh_prime)\n\n    # Prepare client DH Inner Data\n    client_dh_inner = bytes(ClientDHInnerData(\n        nonce=res_pq.nonce,\n        server_nonce=res_pq.server_nonce,\n        retry_id=0,  # TODO Actual retry ID\n        g_b=rsa.get_byte_array(gb)\n    ))\n\n    client_dh_inner_hashed = sha1(client_dh_inner).digest() + client_dh_inner\n\n    # Encryption\n    client_dh_encrypted = AES.encrypt_ige(client_dh_inner_hashed, key, iv)\n\n    # Prepare Set client DH params\n    dh_gen = await sender.send(SetClientDHParamsRequest(\n        nonce=res_pq.nonce,\n        server_nonce=res_pq.server_nonce,\n        encrypted_data=client_dh_encrypted,\n    ))\n\n    nonce_types = (DhGenOk, DhGenRetry, DhGenFail)\n    assert isinstance(dh_gen, nonce_types), 'Step 3.1 answer was %s' % dh_gen\n    name = dh_gen.__class__.__name__\n    if dh_gen.nonce != res_pq.nonce:\n        raise SecurityError('Step 3 invalid {} nonce from server'.format(name))\n\n    if dh_gen.server_nonce != res_pq.server_nonce:\n        raise SecurityError(\n            'Step 3 invalid {} server nonce from server'.format(name))\n\n    auth_key = AuthKey(rsa.get_byte_array(gab))\n    nonce_number = 1 + nonce_types.index(type(dh_gen))\n    new_nonce_hash = auth_key.calc_new_nonce_hash(new_nonce, nonce_number)\n\n    dh_hash = getattr(dh_gen, 'new_nonce_hash{}'.format(nonce_number))\n    if dh_hash != new_nonce_hash:\n        raise SecurityError('Step 3 invalid new nonce hash')\n\n    if not isinstance(dh_gen, DhGenOk):\n        raise AssertionError('Step 3.2 answer was %s' % dh_gen)\n\n    return auth_key, time_offset", "response": "Executes the authentication process with Telegram servers."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_int(byte_array, signed=True):\n    return int.from_bytes(byte_array, byteorder='big', signed=signed)", "response": "Gets the specified integer from its byte array."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def post_init(self):\n        if await self.cl.is_user_authorized():\n            self.set_signed_in(await self.cl.get_me())\n        else:\n            # User is not logged in, configure the button to ask them to login\n            self.sign_in_button.configure(text='Sign in')\n            self.sign_in_label.configure(\n                text='Sign in (phone/token):')", "response": "Complete the initialization of our application."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def sign_in(self, event=None):\n        self.sign_in_label.configure(text='Working...')\n        self.sign_in_entry.configure(state=tkinter.DISABLED)\n        if await self.cl.is_user_authorized():\n            await self.cl.log_out()\n            self.destroy()\n            return\n\n        value = self.sign_in_entry.get().strip()\n        if self.code:\n            self.set_signed_in(await self.cl.sign_in(code=value))\n        elif ':' in value:\n            self.set_signed_in(await self.cl.sign_in(bot_token=value))\n        else:\n            self.code = await self.cl.send_code_request(value)\n            self.sign_in_label.configure(text='Code:')\n            self.sign_in_entry.configure(state=tkinter.NORMAL)\n            self.sign_in_entry.delete(0, tkinter.END)\n            self.sign_in_entry.focus()\n            return", "response": "Sign in the current user."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the application as signed in.", "response": "def set_signed_in(self, me):\n        \"\"\"\n        Configures the application as \"signed in\" (displays user's\n        name and disables the entry to input phone/bot token/code).\n        \"\"\"\n        self.me = me\n        self.sign_in_label.configure(text='Signed in')\n        self.sign_in_entry.configure(state=tkinter.NORMAL)\n        self.sign_in_entry.delete(0, tkinter.END)\n        self.sign_in_entry.insert(tkinter.INSERT, utils.get_display_name(me))\n        self.sign_in_entry.configure(state=tkinter.DISABLED)\n        self.sign_in_button.configure(text='Log out')\n        self.chat.focus()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a message to the user.", "response": "async def send_message(self, event=None):\n        \"\"\"\n        Sends a message. Does nothing if the client is not connected.\n        \"\"\"\n        if not self.cl.is_connected():\n            return\n\n        # The user needs to configure a chat where the message should be sent.\n        #\n        # If the chat ID does not exist, it was not valid and the user must\n        # configure one; hint them by changing the background to red.\n        if not self.chat_id:\n            self.chat.configure(bg='red')\n            self.chat.focus()\n            return\n\n        # Get the message, clear the text field and focus it again\n        text = self.message.get().strip()\n        self.message.delete(0, tkinter.END)\n        self.message.focus()\n        if not text:\n            return\n\n        # NOTE: This part is optional but supports editing messages\n        #       You can remove it if you find it too complicated.\n        #\n        # Check if the edit matches any text\n        m = EDIT.match(text)\n        if m:\n            find = re.compile(m.group(1).lstrip())\n            # Cannot reversed(enumerate(...)), use index\n            for i in reversed(range(len(self.sent_text))):\n                msg_id, msg_text = self.sent_text[i]\n                if find.search(msg_text):\n                    # Found text to replace, so replace it and edit\n                    new = find.sub(m.group(2), msg_text)\n                    self.sent_text[i] = (msg_id, new)\n                    await self.cl.edit_message(self.chat_id, msg_id, new)\n\n                    # Notify that a replacement was made\n                    self.log.insert(tkinter.END, '(message edited: {} -> {})\\n'\n                                    .format(msg_text, new))\n                    self.log.yview(tkinter.END)\n                    return\n\n        # Check if we want to delete the message\n        m = DELETE.match(text)\n        if m:\n            try:\n                delete = self.message_ids.pop(-int(m.group(1)))\n            except IndexError:\n                pass\n            else:\n                await self.cl.delete_messages(self.chat_id, delete)\n                # Notify that a message was deleted\n                self.log.insert(tkinter.END, '(message deleted)\\n')\n                self.log.yview(tkinter.END)\n                return\n\n        # Check if we want to reply to some message\n        reply_to = None\n        m = REPLY.match(text)\n        if m:\n            text = m.group(2)\n            try:\n                reply_to = self.message_ids[-int(m.group(1))]\n            except IndexError:\n                pass\n\n        # NOTE: This part is no longer optional. It sends the message.\n        # Send the message text and get back the sent message object\n        message = await self.cl.send_message(self.chat_id, text,\n                                             reply_to=reply_to)\n\n        # Save the sent message ID and text to allow edits\n        self.sent_text.append((message.id, text))\n\n        # Process the sent message as if it were an event\n        await self.on_message(message)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck the input chat and if the user is logged in and if it has a message that we can send and listen them.", "response": "async def check_chat(self, event=None):\n        \"\"\"\n        Checks the input chat where to send and listen messages from.\n        \"\"\"\n        if self.me is None:\n            return  # Not logged in yet\n\n        chat = self.chat.get().strip()\n        try:\n            chat = int(chat)\n        except ValueError:\n            pass\n\n        try:\n            old = self.chat_id\n            # Valid chat ID, set it and configure the colour back to white\n            self.chat_id = await self.cl.get_peer_id(chat)\n            self.chat.configure(bg='white')\n\n            # If the chat ID changed, clear the\n            # messages that we could edit or reply\n            if self.chat_id != old:\n                self.message_ids.clear()\n                self.sent_text.clear()\n                self.log.delete('1.0', tkinter.END)\n                if not self.me.bot:\n                    for msg in reversed(\n                            await self.cl.get_messages(self.chat_id, 100)):\n                        await self.on_message(msg)\n        except ValueError:\n            # Invalid chat ID, let the user know with a yellow background\n            self.chat_id = None\n            self.chat.configure(bg='yellow')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef old(self):\n        ori = self.original.action\n        if isinstance(ori, (\n                types.ChannelAdminLogEventActionChangeAbout,\n                types.ChannelAdminLogEventActionChangeTitle,\n                types.ChannelAdminLogEventActionChangeUsername\n        )):\n            return ori.prev_value\n        elif isinstance(ori, types.ChannelAdminLogEventActionChangePhoto):\n            return ori.prev_photo\n        elif isinstance(ori, types.ChannelAdminLogEventActionChangeStickerSet):\n            return ori.prev_stickerset\n        elif isinstance(ori, types.ChannelAdminLogEventActionEditMessage):\n            return ori.prev_message\n        elif isinstance(ori, (\n                types.ChannelAdminLogEventActionParticipantToggleAdmin,\n                types.ChannelAdminLogEventActionParticipantToggleBan\n        )):\n            return ori.prev_participant\n        elif isinstance(ori, (\n                types.ChannelAdminLogEventActionToggleInvites,\n                types.ChannelAdminLogEventActionTogglePreHistoryHidden,\n                types.ChannelAdminLogEventActionToggleSignatures\n        )):\n            return not ori.new_value\n        elif isinstance(ori, types.ChannelAdminLogEventActionDeleteMessage):\n            return ori.message\n        elif isinstance(ori, types.ChannelAdminLogEventActionDefaultBannedRights):\n            return ori.prev_banned_rights", "response": "Returns the old value from the event."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef new(self):\n        ori = self.original.action\n        if isinstance(ori, (\n                types.ChannelAdminLogEventActionChangeAbout,\n                types.ChannelAdminLogEventActionChangeTitle,\n                types.ChannelAdminLogEventActionChangeUsername,\n                types.ChannelAdminLogEventActionToggleInvites,\n                types.ChannelAdminLogEventActionTogglePreHistoryHidden,\n                types.ChannelAdminLogEventActionToggleSignatures\n        )):\n            return ori.new_value\n        elif isinstance(ori, types.ChannelAdminLogEventActionChangePhoto):\n            return ori.new_photo\n        elif isinstance(ori, types.ChannelAdminLogEventActionChangeStickerSet):\n            return ori.new_stickerset\n        elif isinstance(ori, types.ChannelAdminLogEventActionEditMessage):\n            return ori.new_message\n        elif isinstance(ori, (\n                types.ChannelAdminLogEventActionParticipantToggleAdmin,\n                types.ChannelAdminLogEventActionParticipantToggleBan\n        )):\n            return ori.new_participant\n        elif isinstance(ori, types.ChannelAdminLogEventActionParticipantInvite):\n            return ori.participant\n        elif isinstance(ori, types.ChannelAdminLogEventActionDefaultBannedRights):\n            return ori.new_banned_rights\n        elif isinstance(ori, types.ChannelAdminLogEventActionStopPoll):\n            return ori.message", "response": "Returns the new value present in the event."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nencrypting the given text in 16 - byte blocks by using the given key and 32 - byte initialization vector.", "response": "def encrypt_ige(plain_text, key, iv):\n        \"\"\"\n        Encrypts the given text in 16-bytes blocks by using the\n        given key and 32-bytes initialization vector.\n        \"\"\"\n        padding = len(plain_text) % 16\n        if padding:\n            plain_text += os.urandom(16 - padding)\n\n        if cryptg:\n            return cryptg.encrypt_ige(plain_text, key, iv)\n        if libssl.encrypt_ige:\n            return libssl.encrypt_ige(plain_text, key, iv)\n\n        iv1 = iv[:len(iv) // 2]\n        iv2 = iv[len(iv) // 2:]\n\n        aes = pyaes.AES(key)\n\n        cipher_text = []\n        blocks_count = len(plain_text) // 16\n\n        for block_index in range(blocks_count):\n            plain_text_block = list(\n                plain_text[block_index * 16:block_index * 16 + 16]\n            )\n            for i in range(16):\n                plain_text_block[i] ^= iv1[i]\n\n            cipher_text_block = aes.encrypt(plain_text_block)\n\n            for i in range(16):\n                cipher_text_block[i] ^= iv2[i]\n\n            iv1 = cipher_text_block\n            iv2 = plain_text[block_index * 16:block_index * 16 + 16]\n\n            cipher_text.extend(cipher_text_block)\n\n        return bytes(cipher_text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinishes the initialization of the current object.", "response": "def _finish_init(self, client, entities, input_chat):\n        \"\"\"\n        Finishes the initialization of this message by setting\n        the client that sent the message and making use of the\n        known entities.\n        \"\"\"\n        self._client = client\n        self._sender = entities.get(self._sender_id)\n        if self._sender:\n            try:\n                self._input_sender = utils.get_input_peer(self._sender)\n            except TypeError:\n                self._input_sender = None\n\n        self._chat = entities.get(self.chat_id)\n        self._input_chat = input_chat\n        if not self._input_chat and self._chat:\n            try:\n                self._input_chat = utils.get_input_peer(self._chat)\n            except TypeError:\n                self._input_chat = None\n\n        self._via_bot = entities.get(self.via_bot_id)\n        if self._via_bot:\n            try:\n                self._via_input_bot = utils.get_input_peer(self._via_bot)\n            except TypeError:\n                self._via_input_bot = None\n\n        if self.fwd_from:\n            self._forward = Forward(self._client, self.fwd_from, entities)\n\n        if self.action:\n            if isinstance(self.action, (types.MessageActionChatAddUser,\n                                        types.MessageActionChatCreate)):\n                self._action_entities = [entities.get(i)\n                                         for i in self.action.users]\n            elif isinstance(self.action, types.MessageActionChatDeleteUser):\n                self._action_entities = [entities.get(self.action.user_id)]\n            elif isinstance(self.action, types.MessageActionChatJoinedByLink):\n                self._action_entities = [entities.get(self.action.inviter_id)]\n            elif isinstance(self.action, types.MessageActionChatMigrateTo):\n                self._action_entities = [entities.get(utils.get_peer_id(\n                    types.PeerChannel(self.action.channel_id)))]\n            elif isinstance(\n                    self.action, types.MessageActionChannelMigrateFrom):\n                self._action_entities = [entities.get(utils.get_peer_id(\n                    types.PeerChat(self.action.chat_id)))]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a matrix containing all buttons of the message as MessageButton instances.", "response": "def buttons(self):\n        \"\"\"\n        Returns a matrix (list of lists) containing all buttons of the message\n        as `MessageButton <telethon.tl.custom.messagebutton.MessageButton>`\n        instances.\n        \"\"\"\n        if self._buttons is None and self.reply_markup:\n            if not self.input_chat:\n                return\n            try:\n                bot = self._needed_markup_bot()\n            except ValueError:\n                return\n            else:\n                self._set_buttons(self._input_chat, bot)\n\n        return self._buttons"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def get_buttons(self):\n        if not self.buttons and self.reply_markup:\n            chat = await self.get_input_chat()\n            if not chat:\n                return\n            try:\n                bot = self._needed_markup_bot()\n            except ValueError:\n                await self._reload_message()\n                bot = self._needed_markup_bot()  # TODO use via_input_bot\n\n            self._set_buttons(chat, bot)\n\n        return self._buttons", "response": "Returns the buttons for the current user."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the total number of buttons in the reply markup.", "response": "def button_count(self):\n        \"\"\"\n        Returns the total button count.\n        \"\"\"\n        if self._buttons_count is None:\n            if isinstance(self.reply_markup, (\n                    types.ReplyInlineMarkup, types.ReplyKeyboardMarkup)):\n                self._buttons_count = sum(\n                    len(row.buttons) for row in self.reply_markup.rows)\n            else:\n                self._buttons_count = 0\n\n        return self._buttons_count"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef photo(self):\n        if isinstance(self.media, types.MessageMediaPhoto):\n            if isinstance(self.media.photo, types.Photo):\n                return self.media.photo\n        elif isinstance(self.action, types.MessageActionChatEditPhoto):\n            return self.action.photo\n        else:\n            web = self.web_preview\n            if web and isinstance(web.photo, types.Photo):\n                return web.photo", "response": "Returns the photo object for this message."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the document object.", "response": "def document(self):\n        \"\"\"\n        If the message media is a document,\n        this returns the :tl:`Document` object.\n        \"\"\"\n        if isinstance(self.media, types.MessageMediaDocument):\n            if isinstance(self.media.document, types.Document):\n                return self.media.document\n        else:\n            web = self.web_preview\n            if web and isinstance(web.photo, types.Document):\n                return web.photo"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef web_preview(self):\n        if isinstance(self.media, types.MessageMediaWebPage):\n            if isinstance(self.media.webpage, types.WebPage):\n                return self.media.webpage", "response": "Returns the WebPage object that represents the web preview of the message."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the game id of the message media.", "response": "def game(self):\n        \"\"\"\n        If the message media is a game, this returns the :tl:`Game`.\n        \"\"\"\n        if isinstance(self.media, types.MessageMediaGame):\n            return self.media.game"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef geo(self):\n        if isinstance(self.media, (types.MessageMediaGeo,\n                                   types.MessageMediaGeoLive,\n                                   types.MessageMediaVenue)):\n            return self.media.geo", "response": "If the message media is geo, geo live or a venue,\n        this returns the :tl:`GeoPoint`."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of tuples containing the inner text of the message entity.", "response": "def get_entities_text(self, cls=None):\n        \"\"\"\n        Returns a list of tuples [(:tl:`MessageEntity`, `str`)], the string\n        being the inner text of the message entity (like bold, italics, etc).\n\n        Args:\n            cls (`type`):\n                Returns entities matching this type only. For example,\n                the following will print the text for all ``code`` entities:\n\n                >>> from telethon.tl.types import MessageEntityCode\n                >>>\n                >>> m = ...  # get the message\n                >>> for _, inner_text in m.get_entities_text(MessageEntityCode):\n                >>>     print(inner_text)\n        \"\"\"\n        ent = self.entities\n        if not ent:\n            return []\n\n        if cls:\n            ent = [c for c in ent if isinstance(c, cls)]\n\n        texts = utils.get_inner_text(self.message, ent)\n        return list(zip(ent, texts))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def get_reply_message(self):\n        if self._reply_message is None:\n            if not self.reply_to_msg_id:\n                return None\n\n            # Bots cannot access other bots' messages by their ID.\n            # However they can access them through replies...\n            self._reply_message = await self._client.get_messages(\n                await self.get_input_chat() if self.is_channel else None,\n                ids=types.InputMessageReplyTo(self.id)\n            )\n            if not self._reply_message:\n                # ...unless the current message got deleted.\n                #\n                # If that's the case, give it a second chance accessing\n                # directly by its ID.\n                self._reply_message = await self._client.get_messages(\n                    self._input_chat if self.is_channel else None,\n                    ids=self.reply_to_msg_id\n                )\n\n        return self._reply_message", "response": "Gets the reply message for this message."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def respond(self, *args, **kwargs):\n        return await self._client.send_message(\n            await self.get_input_chat(), *args, **kwargs)", "response": "Respond to the message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def reply(self, *args, **kwargs):\n        kwargs['reply_to'] = self.id\n        return await self._client.send_message(\n            await self.get_input_chat(), *args, **kwargs)", "response": "Reply to the message."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def forward_to(self, *args, **kwargs):\n        kwargs['messages'] = self.id\n        kwargs['from_peer'] = await self.get_input_chat()\n        return await self._client.forward_messages(*args, **kwargs)", "response": "Forward the message to the input chat."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def edit(self, *args, **kwargs):\n        if self.fwd_from or not self.out:\n            return None  # We assume self.out was patched for our chat\n\n        if 'link_preview' not in kwargs:\n            kwargs['link_preview'] = bool(self.web_preview)\n\n        if 'buttons' not in kwargs:\n            kwargs['buttons'] = self.reply_markup\n\n        return await self._client.edit_message(\n            await self.get_input_chat(), self.id,\n            *args, **kwargs\n        )", "response": "Edits the message iff it s outgoing."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete the message. You're responsible for checking whether you have the permission to do so, or to except the error otherwise. Shorthand for `telethon.client.messages.MessageMethods.delete_messages` with ``entity`` and ``message_ids`` already set. If you need to delete more than one message at once, don't use this `delete` method. Use a `telethon.client.telegramclient.TelegramClient` instance directly.", "response": "async def delete(self, *args, **kwargs):\n        \"\"\"\n        Deletes the message. You're responsible for checking whether you\n        have the permission to do so, or to except the error otherwise.\n        Shorthand for\n        `telethon.client.messages.MessageMethods.delete_messages` with\n        ``entity`` and ``message_ids`` already set.\n\n        If you need to delete more than one message at once, don't use\n        this `delete` method. Use a\n        `telethon.client.telegramclient.TelegramClient` instance directly.\n        \"\"\"\n        return await self._client.delete_messages(\n            await self.get_input_chat(), [self.id],\n            *args, **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndownloads the media contained in the message.", "response": "async def download_media(self, *args, **kwargs):\n        \"\"\"\n        Downloads the media contained in the message, if any. Shorthand\n        for `telethon.client.downloads.DownloadMethods.download_media`\n        with the ``message`` already set.\n        \"\"\"\n        return await self._client.download_media(self, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nclicks the specified button on the specified i th column and text.", "response": "async def click(self, i=None, j=None,\n                    *, text=None, filter=None, data=None):\n        \"\"\"\n        Calls `telethon.tl.custom.messagebutton.MessageButton.click`\n        for the specified button.\n\n        Does nothing if the message has no buttons.\n\n        Args:\n            i (`int`):\n                Clicks the i'th button (starting from the index 0).\n                Will ``raise IndexError`` if out of bounds. Example:\n\n                >>> message = ...  # get the message somehow\n                >>> # Clicking the 3rd button\n                >>> # [button1] [button2]\n                >>> # [     button3     ]\n                >>> # [button4] [button5]\n                >>> message.click(2)  # index\n\n            j (`int`):\n                Clicks the button at position (i, j), these being the\n                indices for the (row, column) respectively. Example:\n\n                >>> # Clicking the 2nd button on the 1st row.\n                >>> # [button1] [button2]\n                >>> # [     button3     ]\n                >>> # [button4] [button5]\n                >>> message.click(0, 1)  # (row, column)\n\n                This is equivalent to ``message.buttons[0][1].click()``.\n\n            text (`str` | `callable`):\n                Clicks the first button with the text \"text\". This may\n                also be a callable, like a ``re.compile(...).match``,\n                and the text will be passed to it.\n\n            filter (`callable`):\n                Clicks the first button for which the callable\n                returns ``True``. The callable should accept a single\n                `telethon.tl.custom.messagebutton.MessageButton` argument.\n\n            data (`bytes`):\n                This argument overrides the rest and will not search any\n                buttons. Instead, it will directly send the request to\n                behave as if it clicked a button with said data. Note\n                that if the message does not have this data, it will\n                ``raise DataInvalidError``.\n        \"\"\"\n        if data:\n            if not await self.get_input_chat():\n                return None\n\n            try:\n                return await self._client(\n                    functions.messages.GetBotCallbackAnswerRequest(\n                        peer=self._input_chat,\n                        msg_id=self.id,\n                        data=data\n                    )\n                )\n            except errors.BotTimeout:\n                return None\n\n        if sum(int(x is not None) for x in (i, text, filter)) >= 2:\n            raise ValueError('You can only set either of i, text or filter')\n\n        if not await self.get_buttons():\n            return  # Accessing the property sets self._buttons[_flat]\n\n        if text is not None:\n            if callable(text):\n                for button in self._buttons_flat:\n                    if text(button.text):\n                        return await button.click()\n            else:\n                for button in self._buttons_flat:\n                    if button.text == text:\n                        return await button.click()\n            return\n\n        if filter is not None:\n            for button in self._buttons_flat:\n                if filter(button):\n                    return await button.click()\n            return\n\n        if i is None:\n            i = 0\n        if j is None:\n            return await self._buttons_flat[i].click()\n        else:\n            return await self._buttons[i][j].click()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def _reload_message(self):\n        try:\n            chat = await self.get_input_chat() if self.is_channel else None\n            msg = await self._client.get_messages(chat, ids=self.id)\n        except ValueError:\n            return  # We may not have the input chat/get message failed\n        if not msg:\n            return  # The message may be deleted and it will be None\n\n        self._sender = msg._sender\n        self._input_sender = msg._input_sender\n        self._chat = msg._chat\n        self._input_chat = msg._input_chat\n        self._via_bot = msg._via_bot\n        self._via_input_bot = msg._via_input_bot\n        self._forward = msg._forward\n        self._action_entities = msg._action_entities", "response": "Re - fetches this message and re - saves the sender and chat entities along with their input versions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _set_buttons(self, chat, bot):\n        if isinstance(self.reply_markup, (\n                types.ReplyInlineMarkup, types.ReplyKeyboardMarkup)):\n            self._buttons = [[\n                MessageButton(self._client, button, chat, bot, self.id)\n                for button in row.buttons\n            ] for row in self.reply_markup.rows]\n            self._buttons_flat = [x for row in self._buttons for x in row]", "response": "Sets the buttons for this message."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _needed_markup_bot(self):\n        if not isinstance(self.reply_markup, (\n                types.ReplyInlineMarkup, types.ReplyKeyboardMarkup)):\n            return None\n\n        for row in self.reply_markup.rows:\n            for button in row.buttons:\n                if isinstance(button, types.KeyboardButtonSwitchInline):\n                    if button.same_peer:\n                        bot = self.input_sender\n                        if not bot:\n                            raise ValueError('No input sender')\n                    else:\n                        try:\n                            return self._client._entity_cache[self.via_bot_id]\n                        except KeyError:\n                            raise ValueError('No input sender') from None", "response": "Returns the input peer of the bot that s needed for the reply markup."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a link node to the TL reference.", "response": "def make_link_node(rawtext, app, name, options):\n    \"\"\"\n    Create a link to the TL reference.\n\n    :param rawtext: Text being replaced with link node.\n    :param app: Sphinx application context\n    :param name: Name of the object to link to\n    :param options: Options dictionary passed to role func.\n    \"\"\"\n    try:\n        base = app.config.tl_ref_url\n        if not base:\n            raise AttributeError\n    except AttributeError as e:\n        raise ValueError('tl_ref_url config value is not set') from e\n\n    if base[-1] != '/':\n        base += '/'\n\n    set_classes(options)\n    node = nodes.reference(rawtext, utils.unescape(name),\n                           refuri='{}?q={}'.format(base, name),\n                           **options)\n    return node"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tl_role(name, rawtext, text, lineno, inliner, options=None, content=None):\n    if options is None:\n        options = {}\n    if content is None:\n        content = []\n\n    # TODO Report error on type not found?\n    # Usage:\n    #   msg = inliner.reporter.error(..., line=lineno)\n    #   return [inliner.problematic(rawtext, rawtext, msg)], [msg]\n    app = inliner.document.settings.env.app\n    node = make_link_node(rawtext, app, text, options)\n    return [node], []", "response": "Link to the TL reference."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def inline_query(self, bot, query, *, offset=None, geo_point=None):\n        bot = await self.get_input_entity(bot)\n        result = await self(functions.messages.GetInlineBotResultsRequest(\n            bot=bot,\n            peer=types.InputPeerEmpty(),\n            query=query,\n            offset=offset or '',\n            geo_point=geo_point\n        ))\n\n        return custom.InlineResults(self, result)", "response": "Makes a given inline query to the specified bot."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def get_me(self, input_peer=False):\n        if input_peer and self._self_input_peer:\n            return self._self_input_peer\n\n        try:\n            me = (await self(\n                functions.users.GetUsersRequest([types.InputUserSelf()])))[0]\n\n            self._bot = me.bot\n            if not self._self_input_peer:\n                self._self_input_peer = utils.get_input_peer(\n                    me, allow_self=False\n                )\n\n            return self._self_input_peer if input_peer else me\n        except errors.UnauthorizedError:\n            return None", "response": "Gets the ID of the current user."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def is_bot(self):\n        if self._bot is None:\n            self._bot = (await self.get_me()).bot\n\n        return self._bot", "response": "Return True if the signed - in user is a bot False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if the user is authorized.", "response": "async def is_user_authorized(self):\n        \"\"\"\n        Returns ``True`` if the user is authorized.\n        \"\"\"\n        if self._authorized is None:\n            try:\n                # Any request that requires authorization will work\n                await self(functions.updates.GetStateRequest())\n                self._authorized = True\n            except errors.RPCError:\n                self._authorized = False\n\n        return self._authorized"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def get_entity(self, entity):\n        single = not utils.is_list_like(entity)\n        if single:\n            entity = (entity,)\n\n        # Group input entities by string (resolve username),\n        # input users (get users), input chat (get chats) and\n        # input channels (get channels) to get the most entities\n        # in the less amount of calls possible.\n        inputs = []\n        for x in entity:\n            if isinstance(x, str):\n                inputs.append(x)\n            else:\n                inputs.append(await self.get_input_entity(x))\n\n        users = [x for x in inputs\n                 if isinstance(x, (types.InputPeerUser, types.InputPeerSelf))]\n        chats = [x.chat_id for x in inputs\n                 if isinstance(x, types.InputPeerChat)]\n        channels = [x for x in inputs\n                    if isinstance(x, types.InputPeerChannel)]\n        if users:\n            # GetUsersRequest has a limit of 200 per call\n            tmp = []\n            while users:\n                curr, users = users[:200], users[200:]\n                tmp.extend(await self(functions.users.GetUsersRequest(curr)))\n            users = tmp\n        if chats:  # TODO Handle chats slice?\n            chats = (await self(\n                functions.messages.GetChatsRequest(chats))).chats\n        if channels:\n            channels = (await self(\n                functions.channels.GetChannelsRequest(channels))).chats\n\n        # Merge users, chats and channels into a single dictionary\n        id_entity = {\n            utils.get_peer_id(x): x\n            for x in itertools.chain(users, chats, channels)\n        }\n\n        # We could check saved usernames and put them into the users,\n        # chats and channels list from before. While this would reduce\n        # the amount of ResolveUsername calls, it would fail to catch\n        # username changes.\n        result = []\n        for x in inputs:\n            if isinstance(x, str):\n                result.append(await self._get_entity_from_string(x))\n            elif not isinstance(x, types.InputPeerSelf):\n                result.append(id_entity[utils.get_peer_id(x)])\n            else:\n                result.append(next(\n                    u for u in id_entity.values()\n                    if isinstance(u, types.User) and u.is_self\n                ))\n\n        return result[0] if single else result", "response": "Get the most recent Telegram entity."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the input entity for a given peer.", "response": "async def get_input_entity(self, peer):\n        \"\"\"\n        Turns the given peer into its input entity version. Most requests\n        use this kind of :tl:`InputPeer`, so this is the most suitable call\n        to make for those cases. **Generally you should let the library do\n        its job** and don't worry about getting the input entity first, but\n        if you're going to use an entity often, consider making the call:\n\n        >>> import asyncio\n        >>> rc = asyncio.get_event_loop().run_until_complete\n        >>>\n        >>> from telethon import TelegramClient\n        >>> client = TelegramClient(...)\n        >>> # If you're going to use \"username\" often in your code\n        >>> # (make a lot of calls), consider getting its input entity\n        >>> # once, and then using the \"user\" everywhere instead.\n        >>> user = rc(client.get_input_entity('username'))\n        >>> # The same applies to IDs, chats or channels.\n        >>> chat = rc(client.get_input_entity(-123456789))\n\n        entity (`str` | `int` | :tl:`Peer` | :tl:`InputPeer`):\n            If a username or invite link is given, **the library will\n            use the cache**. This means that it's possible to be using\n            a username that *changed* or an old invite link (this only\n            happens if an invite link for a small group chat is used\n            after it was upgraded to a mega-group).\n\n            If the username or ID from the invite link is not found in\n            the cache, it will be fetched. The same rules apply to phone\n            numbers (``'+34 123456789'``) from people in your contact list.\n\n            If an exact name is given, it must be in the cache too. This\n            is not reliable as different people can share the same name\n            and which entity is returned is arbitrary, and should be used\n            only for quick tests.\n\n            If a positive integer ID is given, the entity will be searched\n            in cached users, chats or channels, without making any call.\n\n            If a negative integer ID is given, the entity will be searched\n            exactly as either a chat (prefixed with ``-``) or as a channel\n            (prefixed with ``-100``).\n\n            If a :tl:`Peer` is given, it will be searched exactly in the\n            cache as either a user, chat or channel.\n\n            If the given object can be turned into an input entity directly,\n            said operation will be done.\n\n            Unsupported types will raise ``TypeError``.\n\n            If the entity can't be found, ``ValueError`` will be raised.\n\n        Returns:\n            :tl:`InputPeerUser`, :tl:`InputPeerChat` or :tl:`InputPeerChannel`\n            or :tl:`InputPeerSelf` if the parameter is ``'me'`` or ``'self'``.\n\n            If you need to get the ID of yourself, you should use\n            `get_me` with ``input_peer=True``) instead.\n        \"\"\"\n        # Short-circuit if the input parameter directly maps to an InputPeer\n        try:\n            return utils.get_input_peer(peer)\n        except TypeError:\n            pass\n\n        # Next in priority is having a peer (or its ID) cached in-memory\n        try:\n            # 0x2d45687 == crc32(b'Peer')\n            if isinstance(peer, int) or peer.SUBCLASS_OF_ID == 0x2d45687:\n                return self._entity_cache[peer]\n        except (AttributeError, KeyError):\n            pass\n\n        # Then come known strings that take precedence\n        if peer in ('me', 'self'):\n            return types.InputPeerSelf()\n\n        # No InputPeer, cached peer, or known string. Fetch from disk cache\n        try:\n            return self.session.get_input_entity(peer)\n        except ValueError:\n            pass\n\n        # Only network left to try\n        if isinstance(peer, str):\n            return utils.get_input_peer(\n                await self._get_entity_from_string(peer))\n\n        # If we're a bot and the user has messaged us privately users.getUsers\n        # will work with access_hash = 0. Similar for channels.getChannels.\n        # If we're not a bot but the user is in our contacts, it seems to work\n        # regardless. These are the only two special-cased requests.\n        peer = utils.get_peer(peer)\n        if isinstance(peer, types.PeerUser):\n            users = await self(functions.users.GetUsersRequest([\n                types.InputUser(peer.user_id, access_hash=0)]))\n            if users and not isinstance(users[0], types.UserEmpty):\n                # If the user passed a valid ID they expect to work for\n                # channels but would be valid for users, we get UserEmpty.\n                # Avoid returning the invalid empty input peer for that.\n                #\n                # We *could* try to guess if it's a channel first, and if\n                # it's not, work as a chat and try to validate it through\n                # another request, but that becomes too much work.\n                return utils.get_input_peer(users[0])\n        elif isinstance(peer, types.PeerChat):\n            return types.InputPeerChat(peer.chat_id)\n        elif isinstance(peer, types.PeerChannel):\n            try:\n                channels = await self(functions.channels.GetChannelsRequest([\n                    types.InputChannel(peer.channel_id, access_hash=0)]))\n                return utils.get_input_peer(channels.chats[0])\n            except errors.ChannelInvalidError:\n                pass\n\n        raise ValueError(\n            'Could not find the input entity for {!r}. Please read https://'\n            'telethon.readthedocs.io/en/latest/extra/basic/entities.html to'\n            ' find out more details.'\n            .format(peer)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the ID for the given peer.", "response": "async def get_peer_id(self, peer, add_mark=True):\n        \"\"\"\n        Gets the ID for the given peer, which may be anything entity-like.\n\n        This method needs to be ``async`` because `peer` supports usernames,\n        invite-links, phone numbers (from people in your contact list), etc.\n\n        If ``add_mark is False``, then a positive ID will be returned\n        instead. By default, bot-API style IDs (signed) are returned.\n        \"\"\"\n        if isinstance(peer, int):\n            return utils.get_peer_id(peer, add_mark=add_mark)\n\n        try:\n            if peer.SUBCLASS_OF_ID not in (0x2d45687, 0xc91c90b6):\n                # 0x2d45687, 0xc91c90b6 == crc32(b'Peer') and b'InputPeer'\n                peer = await self.get_input_entity(peer)\n        except AttributeError:\n            peer = await self.get_input_entity(peer)\n\n        if isinstance(peer, types.InputPeerSelf):\n            peer = await self.get_me(input_peer=True)\n\n        return utils.get_peer_id(peer, add_mark=add_mark)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a full entity from a given string.", "response": "async def _get_entity_from_string(self, string):\n        \"\"\"\n        Gets a full entity from the given string, which may be a phone or\n        a username, and processes all the found entities on the session.\n        The string may also be a user link, or a channel/chat invite link.\n\n        This method has the side effect of adding the found users to the\n        session database, so it can be queried later without API calls,\n        if this option is enabled on the session.\n\n        Returns the found entity, or raises TypeError if not found.\n        \"\"\"\n        phone = utils.parse_phone(string)\n        if phone:\n            try:\n                for user in (await self(\n                        functions.contacts.GetContactsRequest(0))).users:\n                    if user.phone == phone:\n                        return user\n            except errors.BotMethodInvalidError:\n                raise ValueError('Cannot get entity by phone number as a '\n                                 'bot (try using integer IDs, not strings)')\n        elif string.lower() in ('me', 'self'):\n            return await self.get_me()\n        else:\n            username, is_join_chat = utils.parse_username(string)\n            if is_join_chat:\n                invite = await self(\n                    functions.messages.CheckChatInviteRequest(username))\n\n                if isinstance(invite, types.ChatInvite):\n                    raise ValueError(\n                        'Cannot get entity from a channel (or group) '\n                        'that you are not part of. Join the group and retry'\n                    )\n                elif isinstance(invite, types.ChatInviteAlready):\n                    return invite.chat\n            elif username:\n                try:\n                    result = await self(\n                        functions.contacts.ResolveUsernameRequest(username))\n                except errors.UsernameNotOccupiedError as e:\n                    raise ValueError('No user has \"{}\" as username'\n                                     .format(username)) from e\n\n                try:\n                    pid = utils.get_peer_id(result.peer, add_mark=False)\n                    if isinstance(result.peer, types.PeerUser):\n                        return next(x for x in result.users if x.id == pid)\n                    else:\n                        return next(x for x in result.chats if x.id == pid)\n                except StopIteration:\n                    pass\n            try:\n                # Nobody with this username, maybe it's an exact name/title\n                return await self.get_entity(\n                    self.session.get_input_entity(string))\n            except ValueError:\n                pass\n\n        raise ValueError(\n            'Cannot find any entity corresponding to \"{}\"'.format(string)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a : tl. InputDialogPeer or types. InputDialogPeer depending on the dialog s SUBCLASS_OF_ID.", "response": "async def _get_input_dialog(self, dialog):\n        \"\"\"\n        Returns a :tl:`InputDialogPeer`. This is a bit tricky because\n        it may or not need access to the client to convert what's given\n        into an input entity.\n        \"\"\"\n        try:\n            if dialog.SUBCLASS_OF_ID == 0xa21c9795:  # crc32(b'InputDialogPeer')\n                dialog.peer = await self.get_input_entity(dialog.peer)\n                return dialog\n            elif dialog.SUBCLASS_OF_ID == 0xc91c90b6:  # crc32(b'InputPeer')\n                return types.InputDialogPeer(dialog)\n        except AttributeError:\n            pass\n\n        return types.InputDialogPeer(await self.get_input_entity(dialog))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a : tl : InputNotifyPeer or None if the given notify is not an input entity.", "response": "async def _get_input_notify(self, notify):\n        \"\"\"\n        Returns a :tl:`InputNotifyPeer`. This is a bit tricky because\n        it may or not need access to the client to convert what's given\n        into an input entity.\n        \"\"\"\n        try:\n            if notify.SUBCLASS_OF_ID == 0x58981615:\n                if isinstance(notify, types.InputNotifyPeer):\n                    notify.peer = await self.get_input_entity(notify.peer)\n                return notify\n        except AttributeError:\n            return types.InputNotifyPeer(await self.get_input_entity(notify))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef input_entity(self):\n        if not self._input_entity:\n            try:\n                self._input_entity = self._client._entity_cache[self._peer]\n            except KeyError:\n                pass\n\n        return self._input_entity", "response": "Returns the input version of the entity."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the entity if it is not yet set.", "response": "async def get_entity(self):\n        \"\"\"\n        Returns `entity` but will make an API call if necessary.\n        \"\"\"\n        if not self.entity and await self.get_input_entity():\n            try:\n                self._entity =\\\n                    await self._client.get_entity(self._input_entity)\n            except ValueError:\n                pass\n\n        return self._entity"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def set_message(\n            self, text=None, reply_to=0, parse_mode=(),\n            link_preview=None):\n        \"\"\"\n        Changes the draft message on the Telegram servers. The changes are\n        reflected in this object.\n\n        :param str text: New text of the draft.\n                         Preserved if left as None.\n\n        :param int reply_to: Message ID to reply to.\n                             Preserved if left as 0, erased if set to None.\n\n        :param bool link_preview: Whether to attach a web page preview.\n                                  Preserved if left as None.\n\n        :param str parse_mode: The parse mode to be used for the text.\n        :return bool: ``True`` on success.\n        \"\"\"\n        if text is None:\n            text = self._text\n\n        if reply_to == 0:\n            reply_to = self.reply_to_msg_id\n\n        if link_preview is None:\n            link_preview = self.link_preview\n\n        raw_text, entities =\\\n            await self._client._parse_message_text(text, parse_mode)\n\n        result = await self._client(SaveDraftRequest(\n            peer=self._peer,\n            message=raw_text,\n            no_webpage=not link_preview,\n            reply_to_msg_id=reply_to,\n            entities=entities\n        ))\n\n        if result:\n            self._text = text\n            self._raw_text = raw_text\n            self.link_preview = link_preview\n            self.reply_to_msg_id = reply_to\n            self.date = datetime.datetime.now(tz=datetime.timezone.utc)\n\n        return result", "response": "Changes the draft message on the Telegram servers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def send(self, clear=True, parse_mode=()):\n        await self._client.send_message(\n            self._peer, self.text, reply_to=self.reply_to_msg_id,\n            link_preview=self.link_preview, parse_mode=parse_mode,\n            clear_draft=clear\n        )", "response": "Sends the contents of this draft to the dialog."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def download_profile_photo(\n            self, entity, file=None, *, download_big=True):\n        \"\"\"\n        Downloads the profile photo of the given entity (user/chat/channel).\n\n        Args:\n            entity (`entity`):\n                From who the photo will be downloaded.\n\n                .. note::\n\n                    This method expects the full entity (which has the data\n                    to download the photo), not an input variant.\n\n                    It's possible that sometimes you can't fetch the entity\n                    from its input (since you can get errors like\n                    ``ChannelPrivateError``) but you already have it through\n                    another call, like getting a forwarded message from it.\n\n            file (`str` | `file`, optional):\n                The output file path, directory, or stream-like object.\n                If the path exists and is a file, it will be overwritten.\n                If file is the type `bytes`, it will be downloaded in-memory\n                as a bytestring (e.g. ``file=bytes``).\n\n            download_big (`bool`, optional):\n                Whether to use the big version of the available photos.\n\n        Returns:\n            ``None`` if no photo was provided, or if it was Empty. On success\n            the file path is returned since it may differ from the one given.\n        \"\"\"\n        # hex(crc32(x.encode('ascii'))) for x in\n        # ('User', 'Chat', 'UserFull', 'ChatFull')\n        ENTITIES = (0x2da17977, 0xc5af5d94, 0x1f4661b9, 0xd49a2697)\n        # ('InputPeer', 'InputUser', 'InputChannel')\n        INPUTS = (0xc91c90b6, 0xe669bf46, 0x40f202fd)\n        if not isinstance(entity, TLObject) or entity.SUBCLASS_OF_ID in INPUTS:\n            entity = await self.get_entity(entity)\n\n        possible_names = []\n        if entity.SUBCLASS_OF_ID not in ENTITIES:\n            photo = entity\n        else:\n            if not hasattr(entity, 'photo'):\n                # Special case: may be a ChatFull with photo:Photo\n                # This is different from a normal UserProfilePhoto and Chat\n                if not hasattr(entity, 'chat_photo'):\n                    return None\n\n                return await self._download_photo(\n                    entity.chat_photo, file, date=None, progress_callback=None)\n\n            for attr in ('username', 'first_name', 'title'):\n                possible_names.append(getattr(entity, attr, None))\n\n            photo = entity.photo\n\n        if isinstance(photo, (types.UserProfilePhoto, types.ChatPhoto)):\n            dc_id = photo.dc_id\n            which = photo.photo_big if download_big else photo.photo_small\n            loc = types.InputPeerPhotoFileLocation(\n                peer=await self.get_input_entity(entity),\n                local_id=which.local_id,\n                volume_id=which.volume_id,\n                big=download_big\n            )\n        else:\n            # It doesn't make any sense to check if `photo` can be used\n            # as input location, because then this method would be able\n            # to \"download the profile photo of a message\", i.e. its\n            # media which should be done with `download_media` instead.\n            return None\n\n        file = self._get_proper_filename(\n            file, 'profile_photo', '.jpg',\n            possible_names=possible_names\n        )\n\n        try:\n            result = await self.download_file(loc, file, dc_id=dc_id)\n            return result if file is bytes else file\n        except errors.LocationInvalidError:\n            # See issue #500, Android app fails as of v4.6.0 (1155).\n            # The fix seems to be using the full channel chat photo.\n            ie = await self.get_input_entity(entity)\n            if isinstance(ie, types.InputPeerChannel):\n                full = await self(functions.channels.GetFullChannelRequest(ie))\n                return await self._download_photo(\n                    full.full_chat.chat_photo, file,\n                    date=None, progress_callback=None,\n                    thumb=-1 if download_big else 0\n                )\n            else:\n                # Until there's a report for chats, no need to.\n                return None", "response": "Downloads the profile photo of the given entity."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndownloading the given media or the media from a specified Message.", "response": "async def download_media(self, message, file=None,\n                             *, thumb=None, progress_callback=None):\n        \"\"\"\n        Downloads the given media, or the media from a specified Message.\n\n        Note that if the download is too slow, you should consider installing\n        ``cryptg`` (through ``pip install cryptg``) so that decrypting the\n        received data is done in C instead of Python (much faster).\n\n        message (`Message <telethon.tl.custom.message.Message>` | :tl:`Media`):\n            The media or message containing the media that will be downloaded.\n\n        file (`str` | `file`, optional):\n            The output file path, directory, or stream-like object.\n            If the path exists and is a file, it will be overwritten.\n            If file is the type `bytes`, it will be downloaded in-memory\n            as a bytestring (e.g. ``file=bytes``).\n\n        progress_callback (`callable`, optional):\n            A callback function accepting two parameters:\n            ``(received bytes, total)``.\n\n        thumb (`int` | :tl:`PhotoSize`, optional):\n            Which thumbnail size from the document or photo to download,\n            instead of downloading the document or photo itself.\n\n            If it's specified but the file does not have a thumbnail,\n            this method will return ``None``.\n\n            The parameter should be an integer index between ``0`` and\n            ``len(sizes)``. ``0`` will download the smallest thumbnail,\n            and ``len(sizes) - 1`` will download the largest thumbnail.\n            You can also use negative indices.\n\n            You can also pass the :tl:`PhotoSize` instance to use.\n\n            In short, use ``thumb=0`` if you want the smallest thumbnail\n            and ``thumb=-1`` if you want the largest thumbnail.\n\n        Returns:\n            ``None`` if no media was provided, or if it was Empty. On success\n            the file path is returned since it may differ from the one given.\n        \"\"\"\n        # TODO This won't work for messageService\n        if isinstance(message, types.Message):\n            date = message.date\n            media = message.media\n        else:\n            date = datetime.datetime.now()\n            media = message\n\n        if isinstance(media, str):\n            media = utils.resolve_bot_file_id(media)\n\n        if isinstance(media, types.MessageMediaWebPage):\n            if isinstance(media.webpage, types.WebPage):\n                media = media.webpage.document or media.webpage.photo\n\n        if isinstance(media, (types.MessageMediaPhoto, types.Photo)):\n            return await self._download_photo(\n                media, file, date, thumb, progress_callback\n            )\n        elif isinstance(media, (types.MessageMediaDocument, types.Document)):\n            return await self._download_document(\n                media, file, date, thumb, progress_callback\n            )\n        elif isinstance(media, types.MessageMediaContact) and thumb is None:\n            return self._download_contact(\n                media, file\n            )\n        elif isinstance(media, (types.WebDocument, types.WebDocumentNoProxy)) and thumb is None:\n            return await self._download_web_document(\n                media, file, progress_callback\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndownloading the given input location to a file.", "response": "async def download_file(\n            self, input_location, file=None, *, part_size_kb=None,\n            file_size=None, progress_callback=None, dc_id=None):\n        \"\"\"\n        Downloads the given input location to a file.\n\n        Args:\n            input_location (:tl:`InputFileLocation`):\n                The file location from which the file will be downloaded.\n                See `telethon.utils.get_input_location` source for a complete\n                list of supported types.\n\n            file (`str` | `file`, optional):\n                The output file path, directory, or stream-like object.\n                If the path exists and is a file, it will be overwritten.\n\n                If the file path is ``None`` or ``bytes``, then the result\n                will be saved in memory and returned as `bytes`.\n\n            part_size_kb (`int`, optional):\n                Chunk size when downloading files. The larger, the less\n                requests will be made (up to 512KB maximum).\n\n            file_size (`int`, optional):\n                The file size that is about to be downloaded, if known.\n                Only used if ``progress_callback`` is specified.\n\n            progress_callback (`callable`, optional):\n                A callback function accepting two parameters:\n                ``(downloaded bytes, total)``. Note that the\n                ``total`` is the provided ``file_size``.\n\n            dc_id (`int`, optional):\n                The data center the library should connect to in order\n                to download the file. You shouldn't worry about this.\n        \"\"\"\n        if not part_size_kb:\n            if not file_size:\n                part_size_kb = 64  # Reasonable default\n            else:\n                part_size_kb = utils.get_appropriated_part_size(file_size)\n\n        part_size = int(part_size_kb * 1024)\n        # https://core.telegram.org/api/files says:\n        # > part_size % 1024 = 0 (divisible by 1KB)\n        #\n        # But https://core.telegram.org/cdn (more recent) says:\n        # > limit must be divisible by 4096 bytes\n        # So we just stick to the 4096 limit.\n        if part_size % 4096 != 0:\n            raise ValueError(\n                'The part size must be evenly divisible by 4096.')\n\n        in_memory = file is None or file is bytes\n        if in_memory:\n            f = io.BytesIO()\n        elif isinstance(file, str):\n            # Ensure that we'll be able to download the media\n            helpers.ensure_parent_dir_exists(file)\n            f = open(file, 'wb')\n        else:\n            f = file\n\n        old_dc = dc_id\n        dc_id, input_location = utils.get_input_location(input_location)\n        if dc_id is None:\n            dc_id = old_dc\n\n        exported = dc_id and self.session.dc_id != dc_id\n        if exported:\n            try:\n                sender = await self._borrow_exported_sender(dc_id)\n            except errors.DcIdInvalidError:\n                # Can't export a sender for the ID we are currently in\n                config = await self(functions.help.GetConfigRequest())\n                for option in config.dc_options:\n                    if option.ip_address == self.session.server_address:\n                        self.session.set_dc(\n                            option.id, option.ip_address, option.port)\n                        self.session.save()\n                        break\n\n                # TODO Figure out why the session may have the wrong DC ID\n                sender = self._sender\n                exported = False\n        else:\n            # The used sender will also change if ``FileMigrateError`` occurs\n            sender = self._sender\n\n        self._log[__name__].info('Downloading file in chunks of %d bytes',\n                                 part_size)\n        try:\n            offset = 0\n            while True:\n                try:\n                    result = await sender.send(functions.upload.GetFileRequest(\n                        input_location, offset, part_size\n                    ))\n                    if isinstance(result, types.upload.FileCdnRedirect):\n                        # TODO Implement\n                        raise NotImplementedError\n                except errors.FileMigrateError as e:\n                    self._log[__name__].info('File lives in another DC')\n                    sender = await self._borrow_exported_sender(e.new_dc)\n                    exported = True\n                    continue\n\n                offset += part_size\n                if not result.bytes:\n                    if in_memory:\n                        f.flush()\n                        return f.getvalue()\n                    else:\n                        return getattr(result, 'type', '')\n\n                self._log[__name__].debug('Saving %d more bytes',\n                                          len(result.bytes))\n                f.write(result.bytes)\n                if progress_callback:\n                    progress_callback(f.tell(), file_size)\n        finally:\n            if exported:\n                await self._return_exported_sender(sender)\n            elif sender != self._sender:\n                await sender.disconnect()\n            if isinstance(file, str) or in_memory:\n                f.close()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def _download_photo(self, photo, file, date, thumb, progress_callback):\n        # Determine the photo and its largest size\n        if isinstance(photo, types.MessageMediaPhoto):\n            photo = photo.photo\n        if not isinstance(photo, types.Photo):\n            return\n\n        size = self._get_thumb(photo.sizes, thumb)\n        if not size or isinstance(size, types.PhotoSizeEmpty):\n            return\n\n        file = self._get_proper_filename(file, 'photo', '.jpg', date=date)\n        if isinstance(size, (types.PhotoCachedSize, types.PhotoStrippedSize)):\n            return self._download_cached_photo_size(size, file)\n\n        result = await self.download_file(\n            types.InputPhotoFileLocation(\n                id=photo.id,\n                access_hash=photo.access_hash,\n                file_reference=photo.file_reference,\n                thumb_size=size.type\n            ),\n            file,\n            file_size=size.size,\n            progress_callback=progress_callback\n        )\n        return result if file is bytes else file", "response": "Download a photo and return the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_kind_and_names(attributes):\n        kind = 'document'\n        possible_names = []\n        for attr in attributes:\n            if isinstance(attr, types.DocumentAttributeFilename):\n                possible_names.insert(0, attr.file_name)\n\n            elif isinstance(attr, types.DocumentAttributeAudio):\n                kind = 'audio'\n                if attr.performer and attr.title:\n                    possible_names.append('{} - {}'.format(\n                        attr.performer, attr.title\n                    ))\n                elif attr.performer:\n                    possible_names.append(attr.performer)\n                elif attr.title:\n                    possible_names.append(attr.title)\n                elif attr.voice:\n                    kind = 'voice'\n\n        return kind, possible_names", "response": "Gets kind and possible names for a given set of attributes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nspecializing version of. download_media() for documents.", "response": "async def _download_document(\n            self, document, file, date, thumb, progress_callback):\n        \"\"\"Specialized version of .download_media() for documents.\"\"\"\n        if isinstance(document, types.MessageMediaDocument):\n            document = document.document\n        if not isinstance(document, types.Document):\n            return\n\n        kind, possible_names = self._get_kind_and_names(document.attributes)\n        file = self._get_proper_filename(\n            file, kind, utils.get_extension(document),\n            date=date, possible_names=possible_names\n        )\n\n        if thumb is None:\n            size = None\n        else:\n            size = self._get_thumb(document.thumbs, thumb)\n            if isinstance(size, (types.PhotoCachedSize, types.PhotoStrippedSize)):\n                return self._download_cached_photo_size(size, file)\n\n        result = await self.download_file(\n            types.InputDocumentFileLocation(\n                id=document.id,\n                access_hash=document.access_hash,\n                file_reference=document.file_reference,\n                thumb_size=size.type if size else ''\n            ),\n            file,\n            file_size=size.size if size else document.size,\n            progress_callback=progress_callback\n        )\n\n        return result if file is bytes else file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndownloading the vCard contact from the specified file.", "response": "def _download_contact(cls, mm_contact, file):\n        \"\"\"\n        Specialized version of .download_media() for contacts.\n        Will make use of the vCard 4.0 format.\n        \"\"\"\n        first_name = mm_contact.first_name\n        last_name = mm_contact.last_name\n        phone_number = mm_contact.phone_number\n\n        # Remove these pesky characters\n        first_name = first_name.replace(';', '')\n        last_name = (last_name or '').replace(';', '')\n        result = (\n            'BEGIN:VCARD\\n'\n            'VERSION:4.0\\n'\n            'N:{f};{l};;;\\n'\n            'FN:{f} {l}\\n'\n            'TEL;TYPE=cell;VALUE=uri:tel:+{p}\\n'\n            'END:VCARD\\n'\n        ).format(f=first_name, l=last_name, p=phone_number).encode('utf-8')\n\n        if file is bytes:\n            return result\n        elif isinstance(file, str):\n            file = cls._get_proper_filename(\n                file, 'contact', '.vcard',\n                possible_names=[first_name, phone_number, last_name]\n            )\n            f = open(file, 'wb', encoding='utf-8')\n        else:\n            f = file\n\n        try:\n            f.write(result)\n        finally:\n            # Only close the stream if we opened it\n            if isinstance(file, str):\n                f.close()\n\n        return file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def _download_web_document(cls, web, file, progress_callback):\n        if not aiohttp:\n            raise ValueError(\n                'Cannot download web documents without the aiohttp '\n                'dependency install it (pip install aiohttp)'\n            )\n\n        # TODO Better way to get opened handles of files and auto-close\n        in_memory = file is bytes\n        if in_memory:\n            f = io.BytesIO()\n        elif isinstance(file, str):\n            kind, possible_names = cls._get_kind_and_names(web.attributes)\n            file = cls._get_proper_filename(\n                file, kind, utils.get_extension(web),\n                possible_names=possible_names\n            )\n            f = open(file, 'wb')\n        else:\n            f = file\n\n        try:\n            with aiohttp.ClientSession() as session:\n                # TODO Use progress_callback; get content length from response\n                # https://github.com/telegramdesktop/tdesktop/blob/c7e773dd9aeba94e2be48c032edc9a78bb50234e/Telegram/SourceFiles/ui/images.cpp#L1318-L1319\n                async with session.get(web.url) as response:\n                    while True:\n                        chunk = await response.content.read(128 * 1024)\n                        if not chunk:\n                            break\n                        f.write(chunk)\n        finally:\n            if isinstance(file, str) or file is bytes:\n                f.close()\n\n        return f.getvalue() if in_memory else file", "response": "Download a web document."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a proper filename for file.", "response": "def _get_proper_filename(file, kind, extension,\n                             date=None, possible_names=None):\n        \"\"\"Gets a proper filename for 'file', if this is a path.\n\n           'kind' should be the kind of the output file (photo, document...)\n           'extension' should be the extension to be added to the file if\n                       the filename doesn't have any yet\n           'date' should be when this file was originally sent, if known\n           'possible_names' should be an ordered list of possible names\n\n           If no modification is made to the path, any existing file\n           will be overwritten.\n           If any modification is made to the path, this method will\n           ensure that no existing file will be overwritten.\n        \"\"\"\n        if isinstance(file, pathlib.Path):\n            file = str(file.absolute())\n\n        if file is not None and not isinstance(file, str):\n            # Probably a stream-like object, we cannot set a filename here\n            return file\n\n        if file is None:\n            file = ''\n        elif os.path.isfile(file):\n            # Make no modifications to valid existing paths\n            return file\n\n        if os.path.isdir(file) or not file:\n            try:\n                name = None if possible_names is None else next(\n                    x for x in possible_names if x\n                )\n            except StopIteration:\n                name = None\n\n            if not name:\n                if not date:\n                    date = datetime.datetime.now()\n                name = '{}_{}-{:02}-{:02}_{:02}-{:02}-{:02}'.format(\n                    kind,\n                    date.year, date.month, date.day,\n                    date.hour, date.minute, date.second,\n                )\n            file = os.path.join(file, name)\n\n        directory, name = os.path.split(file)\n        name, ext = os.path.splitext(name)\n        if not ext:\n            ext = extension\n\n        result = os.path.join(directory, name + ext)\n        if not os.path.isfile(result):\n            return result\n\n        i = 1\n        while True:\n            result = os.path.join(directory, '{} ({}){}'.format(name, i, ext))\n            if not os.path.isfile(result):\n                return result\n            i += 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetermines whether the given message is in the range or not.", "response": "def _message_in_range(self, message):\n        \"\"\"\n        Determine whether the given message is in the range or\n        it should be ignored (and avoid loading more chunks).\n        \"\"\"\n        # No entity means message IDs between chats may vary\n        if self.entity:\n            if self.reverse:\n                if message.id <= self.last_id or message.id >= self.max_id:\n                    return False\n            else:\n                if message.id >= self.last_id or message.id <= self.min_id:\n                    return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the offset of the request based on the last message.", "response": "def _update_offset(self, last_message):\n        \"\"\"\n        After making the request, update its offset with the last message.\n        \"\"\"\n        self.request.offset_id = last_message.id\n        if self.reverse:\n            # We want to skip the one we already have\n            self.request.offset_id += 1\n\n        if isinstance(self.request, functions.messages.SearchRequest):\n            # Unlike getHistory and searchGlobal that use *offset* date,\n            # this is *max* date. This means that doing a search in reverse\n            # will break it. Since it's not really needed once we're going\n            # (only for the first request), it's safe to just clear it off.\n            self.request.max_date = None\n        else:\n            # getHistory and searchGlobal call it offset_date\n            self.request.offset_date = last_message.date\n\n        if isinstance(self.request, functions.messages.SearchGlobalRequest):\n            self.request.offset_peer = last_message.input_chat"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef iter_messages(\n            self, entity, limit=None, *, offset_date=None, offset_id=0,\n            max_id=0, min_id=0, add_offset=0, search=None, filter=None,\n            from_user=None, wait_time=None, ids=None, reverse=False\n    ):\n        \"\"\"\n        Iterator over the message history for the specified entity.\n        If either `search`, `filter` or `from_user` are provided,\n        :tl:`messages.Search` will be used instead of :tl:`messages.getHistory`.\n\n        Args:\n            entity (`entity`):\n                The entity from whom to retrieve the message history.\n\n                It may be ``None`` to perform a global search, or\n                to get messages by their ID from no particular chat.\n                Note that some of the offsets will not work if this\n                is the case.\n\n                Note that if you want to perform a global search,\n                you **must** set a non-empty `search` string.\n\n            limit (`int` | `None`, optional):\n                Number of messages to be retrieved. Due to limitations with\n                the API retrieving more than 3000 messages will take longer\n                than half a minute (or even more based on previous calls).\n\n                The limit may also be ``None``, which would eventually return\n                the whole history.\n\n            offset_date (`datetime`):\n                Offset date (messages *previous* to this date will be\n                retrieved). Exclusive.\n\n            offset_id (`int`):\n                Offset message ID (only messages *previous* to the given\n                ID will be retrieved). Exclusive.\n\n            max_id (`int`):\n                All the messages with a higher (newer) ID or equal to this will\n                be excluded.\n\n            min_id (`int`):\n                All the messages with a lower (older) ID or equal to this will\n                be excluded.\n\n            add_offset (`int`):\n                Additional message offset (all of the specified offsets +\n                this offset = older messages).\n\n            search (`str`):\n                The string to be used as a search query.\n\n            filter (:tl:`MessagesFilter` | `type`):\n                The filter to use when returning messages. For instance,\n                :tl:`InputMessagesFilterPhotos` would yield only messages\n                containing photos.\n\n            from_user (`entity`):\n                Only messages from this user will be returned.\n                This parameter will be ignored if it is not an user.\n\n            wait_time (`int`):\n                Wait time (in seconds) between different\n                :tl:`GetHistoryRequest`. Use this parameter to avoid hitting\n                the ``FloodWaitError`` as needed. If left to ``None``, it will\n                default to 1 second only if the limit is higher than 3000.\n\n            ids (`int`, `list`):\n                A single integer ID (or several IDs) for the message that\n                should be returned. This parameter takes precedence over\n                the rest (which will be ignored if this is set). This can\n                for instance be used to get the message with ID 123 from\n                a channel. Note that if the message doesn't exist, ``None``\n                will appear in its place, so that zipping the list of IDs\n                with the messages can match one-to-one.\n\n                .. note::\n\n                    At the time of writing, Telegram will **not** return\n                    :tl:`MessageEmpty` for :tl:`InputMessageReplyTo` IDs that\n                    failed (i.e. the message is not replying to any, or is\n                    replying to a deleted message). This means that it is\n                    **not** possible to match messages one-by-one, so be\n                    careful if you use non-integers in this parameter.\n\n            reverse (`bool`, optional):\n                If set to ``True``, the messages will be returned in reverse\n                order (from oldest to newest, instead of the default newest\n                to oldest). This also means that the meaning of `offset_id`\n                and `offset_date` parameters is reversed, although they will\n                still be exclusive. `min_id` becomes equivalent to `offset_id`\n                instead of being `max_id` as well since messages are returned\n                in ascending order.\n\n                You cannot use this if both `entity` and `ids` are ``None``.\n\n        Yields:\n            Instances of `telethon.tl.custom.message.Message`.\n\n        Notes:\n            Telegram's flood wait limit for :tl:`GetHistoryRequest` seems to\n            be around 30 seconds per 10 requests, therefore a sleep of 1\n            second is the default for this limit (or above).\n        \"\"\"\n\n        if ids is not None:\n            return _IDsIter(self, limit, entity=entity, ids=ids)\n\n        return _MessagesIter(\n            client=self,\n            reverse=reverse,\n            wait_time=wait_time,\n            limit=limit,\n            entity=entity,\n            offset_id=offset_id,\n            min_id=min_id,\n            max_id=max_id,\n            from_user=from_user,\n            offset_date=offset_date,\n            add_offset=add_offset,\n            filter=filter,\n            search=search\n        )", "response": "This function returns an iterator over the message history for the specified entity."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def get_messages(self, *args, **kwargs):\n        if len(args) == 1 and 'limit' not in kwargs:\n            if 'min_id' in kwargs and 'max_id' in kwargs:\n                kwargs['limit'] = None\n            else:\n                kwargs['limit'] = 1\n\n        it = self.iter_messages(*args, **kwargs)\n\n        ids = kwargs.get('ids')\n        if ids and not utils.is_list_like(ids):\n            async for message in it:\n                return message\n            else:\n                # Iterator exhausted = empty, to handle InputMessageReplyTo\n                return None\n\n        return await it.collect()", "response": "Same as iter_messages but returns a list of messages."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends a message to the specified entity.", "response": "async def send_message(\n            self, entity, message='', *, reply_to=None,\n            parse_mode=(), link_preview=True, file=None,\n            force_document=False, clear_draft=False, buttons=None,\n            silent=None):\n        \"\"\"\n        Sends the given message to the specified entity (user/chat/channel).\n\n        The default parse mode is the same as the official applications\n        (a custom flavour of markdown). ``**bold**, `code` or __italic__``\n        are available. In addition you can send ``[links](https://example.com)``\n        and ``[mentions](@username)`` (or using IDs like in the Bot API:\n        ``[mention](tg://user?id=123456789)``) and ``pre`` blocks with three\n        backticks.\n\n        Sending a ``/start`` command with a parameter (like ``?start=data``)\n        is also done through this method. Simply send ``'/start data'`` to\n        the bot.\n\n        Args:\n            entity (`entity`):\n                To who will it be sent.\n\n            message (`str` | `Message <telethon.tl.custom.message.Message>`):\n                The message to be sent, or another message object to resend.\n\n                The maximum length for a message is 35,000 bytes or 4,096\n                characters. Longer messages will not be sliced automatically,\n                and you should slice them manually if the text to send is\n                longer than said length.\n\n            reply_to (`int` | `Message <telethon.tl.custom.message.Message>`, optional):\n                Whether to reply to a message or not. If an integer is provided,\n                it should be the ID of the message that it should reply to.\n\n            parse_mode (`object`, optional):\n                See the `TelegramClient.parse_mode\n                <telethon.client.messageparse.MessageParseMethods.parse_mode>`\n                property for allowed values. Markdown parsing will be used by\n                default.\n\n            link_preview (`bool`, optional):\n                Should the link preview be shown?\n\n            file (`file`, optional):\n                Sends a message with a file attached (e.g. a photo,\n                video, audio or document). The ``message`` may be empty.\n\n            force_document (`bool`, optional):\n                Whether to send the given file as a document or not.\n\n            clear_draft (`bool`, optional):\n                Whether the existing draft should be cleared or not.\n                Has no effect when sending a file.\n\n            buttons (`list`, `custom.Button <telethon.tl.custom.button.Button>`, :tl:`KeyboardButton`):\n                The matrix (list of lists), row list or button to be shown\n                after sending the message. This parameter will only work if\n                you have signed in as a bot. You can also pass your own\n                :tl:`ReplyMarkup` here.\n\n                All the following limits apply together:\n\n                * There can be 100 buttons at most (any more are ignored).\n                * There can be 8 buttons per row at most (more are ignored).\n                * The maximum callback data per button is 64 bytes.\n                * The maximum data that can be embedded in total is just\n                  over 4KB, shared between inline callback data and text.\n\n            silent (`bool`, optional):\n                Whether the message should notify people in a broadcast\n                channel or not. Defaults to ``False``, which means it will\n                notify them. Set it to ``True`` to alter this behaviour.\n\n        Returns:\n            The sent `custom.Message <telethon.tl.custom.message.Message>`.\n        \"\"\"\n        if file is not None:\n            return await self.send_file(\n                entity, file, caption=message, reply_to=reply_to,\n                parse_mode=parse_mode, force_document=force_document,\n                buttons=buttons\n            )\n        elif not message:\n            raise ValueError(\n                'The message cannot be empty unless a file is provided'\n            )\n\n        entity = await self.get_input_entity(entity)\n        if isinstance(message, types.Message):\n            if buttons is None:\n                markup = message.reply_markup\n            else:\n                markup = self.build_reply_markup(buttons)\n\n            if silent is None:\n                silent = message.silent\n\n            if (message.media and not isinstance(\n                    message.media, types.MessageMediaWebPage)):\n                return await self.send_file(\n                    entity,\n                    message.media,\n                    caption=message.message,\n                    silent=silent,\n                    reply_to=reply_to,\n                    buttons=markup,\n                    entities=message.entities\n                )\n\n            request = functions.messages.SendMessageRequest(\n                peer=entity,\n                message=message.message or '',\n                silent=silent,\n                reply_to_msg_id=utils.get_message_id(reply_to),\n                reply_markup=markup,\n                entities=message.entities,\n                clear_draft=clear_draft,\n                no_webpage=not isinstance(\n                    message.media, types.MessageMediaWebPage)\n            )\n            message = message.message\n        else:\n            message, msg_ent = await self._parse_message_text(message,\n                                                              parse_mode)\n            request = functions.messages.SendMessageRequest(\n                peer=entity,\n                message=message,\n                entities=msg_ent,\n                no_webpage=not link_preview,\n                reply_to_msg_id=utils.get_message_id(reply_to),\n                clear_draft=clear_draft,\n                silent=silent,\n                reply_markup=self.build_reply_markup(buttons)\n            )\n\n        result = await self(request)\n        if isinstance(result, types.UpdateShortSentMessage):\n            message = types.Message(\n                id=result.id,\n                to_id=utils.get_peer(entity),\n                message=message,\n                date=result.date,\n                out=result.out,\n                media=result.media,\n                entities=result.entities,\n                reply_markup=request.reply_markup\n            )\n            message._finish_init(self, {}, entity)\n            return message\n\n        return self._get_response_message(request, result, entity)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def forward_messages(self, entity, messages, from_peer=None,\n                               *, silent=None, as_album=None):\n        \"\"\"\n        Forwards the given message(s) to the specified entity.\n\n        Args:\n            entity (`entity`):\n                To which entity the message(s) will be forwarded.\n\n            messages (`list` | `int` | `Message <telethon.tl.custom.message.Message>`):\n                The message(s) to forward, or their integer IDs.\n\n            from_peer (`entity`):\n                If the given messages are integer IDs and not instances\n                of the ``Message`` class, this *must* be specified in\n                order for the forward to work. This parameter indicates\n                the entity from which the messages should be forwarded.\n\n            silent (`bool`, optional):\n                Whether the message should notify people in a broadcast\n                channel or not. Defaults to ``False``, which means it will\n                notify them. Set it to ``True`` to alter this behaviour.\n\n            as_album (`bool`, optional):\n                Whether several image messages should be forwarded as an\n                album (grouped) or not. The default behaviour is to treat\n                albums specially and send outgoing requests with\n                ``as_album=True`` only for the albums if message objects\n                are used. If IDs are used it will group by default.\n\n                In short, the default should do what you expect,\n                ``True`` will group always (even converting separate\n                images into albums), and ``False`` will never group.\n\n        Returns:\n            The list of forwarded `telethon.tl.custom.message.Message`,\n            or a single one if a list wasn't provided as input.\n\n            Note that if all messages are invalid (i.e. deleted) the call\n            will fail with ``MessageIdInvalidError``. If only some are\n            invalid, the list will have ``None`` instead of those messages.\n        \"\"\"\n        single = not utils.is_list_like(messages)\n        if single:\n            messages = (messages,)\n\n        entity = await self.get_input_entity(entity)\n\n        if from_peer:\n            from_peer = await self.get_input_entity(from_peer)\n            from_peer_id = await self.get_peer_id(from_peer)\n        else:\n            from_peer_id = None\n\n        def _get_key(m):\n            if isinstance(m, int):\n                if from_peer_id is not None:\n                    return from_peer_id, None\n\n                raise ValueError('from_peer must be given if integer IDs are used')\n            elif isinstance(m, types.Message):\n                return m.chat_id, m.grouped_id\n            else:\n                raise TypeError('Cannot forward messages of type {}'.format(type(m)))\n\n        # We want to group outgoing chunks differently if we are \"smart\"\n        # about sending as album.\n        #\n        # Why? We need separate requests for ``as_album=True/False``, so\n        # if we want that behaviour, when we group messages to create the\n        # chunks, we need to consider the grouped ID too. But if we don't\n        # care about that, we don't need to consider it for creating the\n        # chunks, so we can make less requests.\n        if as_album is None:\n            get_key = _get_key\n        else:\n            def get_key(m):\n                return _get_key(m)[0]  # Ignore grouped_id\n\n        sent = []\n        for chat_id, chunk in itertools.groupby(messages, key=get_key):\n            chunk = list(chunk)\n            if isinstance(chunk[0], int):\n                chat = from_peer\n                grouped = True if as_album is None else as_album\n            else:\n                chat = await chunk[0].get_input_chat()\n                if as_album is None:\n                    grouped = any(m.grouped_id is not None for m in chunk)\n                else:\n                    grouped = as_album\n\n                chunk = [m.id for m in chunk]\n\n            req = functions.messages.ForwardMessagesRequest(\n                from_peer=chat,\n                id=chunk,\n                to_peer=entity,\n                silent=silent,\n                # Trying to send a single message as grouped will cause\n                # GROUPED_MEDIA_INVALID. If more than one message is forwarded\n                # (even without media...), this error goes away.\n                grouped=len(chunk) > 1 and grouped\n            )\n            result = await self(req)\n            sent.extend(self._get_response_message(req, result, entity))\n\n        return sent[0] if single else sent", "response": "Forwards the given list of messages to the specified entity."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def edit_message(\n            self, entity, message=None, text=None,\n            *, parse_mode=(), link_preview=True, file=None,\n            buttons=None):\n        \"\"\"\n        Edits the given message ID (to change its contents or disable preview).\n\n        Args:\n            entity (`entity` | `Message <telethon.tl.custom.message.Message>`):\n                From which chat to edit the message. This can also be\n                the message to be edited, and the entity will be inferred\n                from it, so the next parameter will be assumed to be the\n                message text.\n\n                You may also pass a :tl:`InputBotInlineMessageID`,\n                which is the only way to edit messages that were sent\n                after the user selects an inline query result.\n\n            message (`int` | `Message <telethon.tl.custom.message.Message>` | `str`):\n                The ID of the message (or `Message\n                <telethon.tl.custom.message.Message>` itself) to be edited.\n                If the `entity` was a `Message\n                <telethon.tl.custom.message.Message>`, then this message\n                will be treated as the new text.\n\n            text (`str`, optional):\n                The new text of the message. Does nothing if the `entity`\n                was a `Message <telethon.tl.custom.message.Message>`.\n\n            parse_mode (`object`, optional):\n                See the `TelegramClient.parse_mode\n                <telethon.client.messageparse.MessageParseMethods.parse_mode>`\n                property for allowed values. Markdown parsing will be used by\n                default.\n\n            link_preview (`bool`, optional):\n                Should the link preview be shown?\n\n            file (`str` | `bytes` | `file` | `media`, optional):\n                The file object that should replace the existing media\n                in the message.\n\n            buttons (`list`, `custom.Button <telethon.tl.custom.button.Button>`, :tl:`KeyboardButton`):\n                The matrix (list of lists), row list or button to be shown\n                after sending the message. This parameter will only work if\n                you have signed in as a bot. You can also pass your own\n                :tl:`ReplyMarkup` here.\n\n        Examples:\n\n            >>> client = ...\n            >>> message = client.send_message('username', 'hello')\n            >>>\n            >>> client.edit_message('username', message, 'hello!')\n            >>> # or\n            >>> client.edit_message('username', message.id, 'Hello')\n            >>> # or\n            >>> client.edit_message(message, 'Hello!')\n\n        Raises:\n            ``MessageAuthorRequiredError`` if you're not the author of the\n            message but tried editing it anyway.\n\n            ``MessageNotModifiedError`` if the contents of the message were\n            not modified at all.\n\n        Returns:\n            The edited `telethon.tl.custom.message.Message`, unless\n            `entity` was a :tl:`InputBotInlineMessageID` in which\n            case this method returns a boolean.\n        \"\"\"\n        if isinstance(entity, types.InputBotInlineMessageID):\n            text = message\n            message = entity\n        elif isinstance(entity, types.Message):\n            text = message  # Shift the parameters to the right\n            message = entity\n            entity = entity.to_id\n\n        text, msg_entities = await self._parse_message_text(text, parse_mode)\n        file_handle, media, image = await self._file_to_media(file)\n\n        if isinstance(entity, types.InputBotInlineMessageID):\n            return await self(functions.messages.EditInlineBotMessageRequest(\n                id=entity,\n                message=text,\n                no_webpage=not link_preview,\n                entities=msg_entities,\n                media=media,\n                reply_markup=self.build_reply_markup(buttons)\n            ))\n\n        entity = await self.get_input_entity(entity)\n        request = functions.messages.EditMessageRequest(\n            peer=entity,\n            id=utils.get_message_id(message),\n            message=text,\n            no_webpage=not link_preview,\n            entities=msg_entities,\n            media=media,\n            reply_markup=self.build_reply_markup(buttons)\n        )\n        msg = self._get_response_message(request, await self(request), entity)\n        await self._cache_media(msg, file, file_handle, image=image)\n        return msg", "response": "Edits the given message."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes a message from a chat optionally for everyone.", "response": "async def delete_messages(self, entity, message_ids, *, revoke=True):\n        \"\"\"\n        Deletes a message from a chat, optionally \"for everyone\".\n\n        Args:\n            entity (`entity`):\n                From who the message will be deleted. This can actually\n                be ``None`` for normal chats, but **must** be present\n                for channels and megagroups.\n\n            message_ids (`list` | `int` | `Message <telethon.tl.custom.message.Message>`):\n                The IDs (or ID) or messages to be deleted.\n\n            revoke (`bool`, optional):\n                Whether the message should be deleted for everyone or not.\n                By default it has the opposite behaviour of official clients,\n                and it will delete the message for everyone.\n\n                `Since 24 March 2019\n                <https://telegram.org/blog/unsend-privacy-emoji>`_, you can\n                also revoke messages of any age (i.e. messages sent long in\n                the past) the *other* person sent in private conversations\n                (and of course your messages too).\n\n                Disabling this has no effect on channels or megagroups,\n                since it will unconditionally delete the message for everyone.\n\n        Returns:\n            A list of :tl:`AffectedMessages`, each item being the result\n            for the delete calls of the messages in chunks of 100 each.\n        \"\"\"\n        if not utils.is_list_like(message_ids):\n            message_ids = (message_ids,)\n\n        message_ids = (\n            m.id if isinstance(m, (\n                types.Message, types.MessageService, types.MessageEmpty))\n            else int(m) for m in message_ids\n        )\n\n        entity = await self.get_input_entity(entity) if entity else None\n        if isinstance(entity, types.InputPeerChannel):\n            return await self([functions.channels.DeleteMessagesRequest(\n                         entity, list(c)) for c in utils.chunks(message_ids)])\n        else:\n            return await self([functions.messages.DeleteMessagesRequest(\n                         list(c), revoke) for c in utils.chunks(message_ids)])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend a read acknowledge to the given entity.", "response": "async def send_read_acknowledge(\n            self, entity, message=None, *, max_id=None, clear_mentions=False):\n        \"\"\"\n        Sends a \"read acknowledge\" (i.e., notifying the given peer that we've\n        read their messages, also known as the \"double check\").\n\n        This effectively marks a message as read (or more than one) in the\n        given conversation.\n\n        If neither message nor maximum ID are provided, all messages will be\n        marked as read by assuming that ``max_id = 0``.\n\n        Args:\n            entity (`entity`):\n                The chat where these messages are located.\n\n            message (`list` | `Message <telethon.tl.custom.message.Message>`):\n                Either a list of messages or a single message.\n\n            max_id (`int`):\n                Overrides messages, until which message should the\n                acknowledge should be sent.\n\n            clear_mentions (`bool`):\n                Whether the mention badge should be cleared (so that\n                there are no more mentions) or not for the given entity.\n\n                If no message is provided, this will be the only action\n                taken.\n        \"\"\"\n        if max_id is None:\n            if not message:\n                max_id = 0\n            else:\n                if utils.is_list_like(message):\n                    max_id = max(msg.id for msg in message)\n                else:\n                    max_id = message.id\n\n        entity = await self.get_input_entity(entity)\n        if clear_mentions:\n            await self(functions.messages.ReadMentionsRequest(entity))\n            if max_id is None:\n                return True\n\n        if max_id is not None:\n            if isinstance(entity, types.InputPeerChannel):\n                return await self(functions.channels.ReadHistoryRequest(\n                    entity, max_id=max_id))\n            else:\n                return await self(functions.messages.ReadHistoryRequest(\n                    entity, max_id=max_id))\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend a file to the specified entity.", "response": "async def send_file(\n            self, entity, file, *, caption=None, force_document=False,\n            progress_callback=None, reply_to=None, attributes=None,\n            thumb=None, allow_cache=True, parse_mode=(),\n            voice_note=False, video_note=False, buttons=None, silent=None,\n            supports_streaming=False, **kwargs):\n        \"\"\"\n        Sends a file to the specified entity.\n\n        Args:\n            entity (`entity`):\n                Who will receive the file.\n\n            file (`str` | `bytes` | `file` | `media`):\n                The file to send, which can be one of:\n\n                * A local file path to an in-disk file. The file name\n                  will be the path's base name.\n\n                * A `bytes` byte array with the file's data to send\n                  (for example, by using ``text.encode('utf-8')``).\n                  A default file name will be used.\n\n                * A bytes `io.IOBase` stream over the file to send\n                  (for example, by using ``open(file, 'rb')``).\n                  Its ``.name`` property will be used for the file name,\n                  or a default if it doesn't have one.\n\n                * An external URL to a file over the internet. This will\n                  send the file as \"external\" media, and Telegram is the\n                  one that will fetch the media and send it.\n\n                * A Bot API-like ``file_id``. You can convert previously\n                  sent media to file IDs for later reusing with\n                  `telethon.utils.pack_bot_file_id`.\n\n                * A handle to an existing file (for example, if you sent a\n                  message with media before, you can use its ``message.media``\n                  as a file here).\n\n                * A handle to an uploaded file (from `upload_file`).\n\n                To send an album, you should provide a list in this parameter.\n\n                If a list or similar is provided, the files in it will be\n                sent as an album in the order in which they appear, sliced\n                in chunks of 10 if more than 10 are given.\n\n            caption (`str`, optional):\n                Optional caption for the sent media message. When sending an\n                album, the caption may be a list of strings, which will be\n                assigned to the files pairwise.\n\n            force_document (`bool`, optional):\n                If left to ``False`` and the file is a path that ends with\n                the extension of an image file or a video file, it will be\n                sent as such. Otherwise always as a document.\n\n            progress_callback (`callable`, optional):\n                A callback function accepting two parameters:\n                ``(sent bytes, total)``.\n\n            reply_to (`int` | `Message <telethon.tl.custom.message.Message>`):\n                Same as `reply_to` from `send_message`.\n\n            attributes (`list`, optional):\n                Optional attributes that override the inferred ones, like\n                :tl:`DocumentAttributeFilename` and so on.\n\n            thumb (`str` | `bytes` | `file`, optional):\n                Optional JPEG thumbnail (for documents). **Telegram will\n                ignore this parameter** unless you pass a ``.jpg`` file!\n\n                The file must also be small in dimensions and in-disk size.\n                Successful thumbnails were files below 20kb and 200x200px.\n                Width/height and dimensions/size ratios may be important.\n\n            allow_cache (`bool`, optional):\n                Whether to allow using the cached version stored in the\n                database or not. Defaults to ``True`` to avoid re-uploads.\n                Must be ``False`` if you wish to use different attributes\n                or thumb than those that were used when the file was cached.\n\n            parse_mode (`object`, optional):\n                See the `TelegramClient.parse_mode\n                <telethon.client.messageparse.MessageParseMethods.parse_mode>`\n                property for allowed values. Markdown parsing will be used by\n                default.\n\n            voice_note (`bool`, optional):\n                If ``True`` the audio will be sent as a voice note.\n\n                Set `allow_cache` to ``False`` if you sent the same file\n                without this setting before for it to work.\n\n            video_note (`bool`, optional):\n                If ``True`` the video will be sent as a video note,\n                also known as a round video message.\n\n                Set `allow_cache` to ``False`` if you sent the same file\n                without this setting before for it to work.\n\n            buttons (`list`, `custom.Button <telethon.tl.custom.button.Button>`, :tl:`KeyboardButton`):\n                The matrix (list of lists), row list or button to be shown\n                after sending the message. This parameter will only work if\n                you have signed in as a bot. You can also pass your own\n                :tl:`ReplyMarkup` here.\n\n            silent (`bool`, optional):\n                Whether the message should notify people in a broadcast\n                channel or not. Defaults to ``False``, which means it will\n                notify them. Set it to ``True`` to alter this behaviour.\n\n            supports_streaming (`bool`, optional):\n                Whether the sent video supports streaming or not. Note that\n                Telegram only recognizes as streamable some formats like MP4,\n                and others like AVI or MKV will not work. You should convert\n                these to MP4 before sending if you want them to be streamable.\n                Unsupported formats will result in ``VideoContentTypeError``.\n\n        Notes:\n            If the ``hachoir3`` package (``hachoir`` module) is installed,\n            it will be used to determine metadata from audio and video files.\n\n            If the `pillow` package is installed and you are sending a photo,\n            it will be resized to fit within the maximum dimensions allowed\n            by Telegram to avoid ``errors.PhotoInvalidDimensionsError``. This\n            cannot be done if you are sending :tl:`InputFile`, however.\n\n        Returns:\n            The `telethon.tl.custom.message.Message` (or messages) containing\n            the sent file, or messages if a list of them was passed.\n        \"\"\"\n        # i.e. ``None`` was used\n        if not file:\n            raise TypeError('Cannot use {!r} as file'.format(file))\n\n        if not caption:\n            caption = ''\n\n        # First check if the user passed an iterable, in which case\n        # we may want to send as an album if all are photo files.\n        if utils.is_list_like(file):\n            # TODO Fix progress_callback\n            images = []\n            if force_document:\n                documents = file\n            else:\n                documents = []\n                for x in file:\n                    if utils.is_image(x):\n                        images.append(x)\n                    else:\n                        documents.append(x)\n\n            result = []\n            while images:\n                result += await self._send_album(\n                    entity, images[:10], caption=caption,\n                    progress_callback=progress_callback, reply_to=reply_to,\n                    parse_mode=parse_mode, silent=silent\n                )\n                images = images[10:]\n\n            for x in documents:\n                result.append(await self.send_file(\n                    entity, x, allow_cache=allow_cache,\n                    caption=caption, force_document=force_document,\n                    progress_callback=progress_callback, reply_to=reply_to,\n                    attributes=attributes, thumb=thumb, voice_note=voice_note,\n                    video_note=video_note, buttons=buttons, silent=silent,\n                    supports_streaming=supports_streaming,\n                    **kwargs\n                ))\n\n            return result\n\n        entity = await self.get_input_entity(entity)\n        reply_to = utils.get_message_id(reply_to)\n\n        # Not document since it's subject to change.\n        # Needed when a Message is passed to send_message and it has media.\n        if 'entities' in kwargs:\n            msg_entities = kwargs['entities']\n        else:\n            caption, msg_entities =\\\n                await self._parse_message_text(caption, parse_mode)\n\n        file_handle, media, image = await self._file_to_media(\n            file, force_document=force_document,\n            progress_callback=progress_callback,\n            attributes=attributes,  allow_cache=allow_cache, thumb=thumb,\n            voice_note=voice_note, video_note=video_note,\n            supports_streaming=supports_streaming\n        )\n\n        # e.g. invalid cast from :tl:`MessageMediaWebPage`\n        if not media:\n            raise TypeError('Cannot use {!r} as file'.format(file))\n\n        markup = self.build_reply_markup(buttons)\n        request = functions.messages.SendMediaRequest(\n            entity, media, reply_to_msg_id=reply_to, message=caption,\n            entities=msg_entities, reply_markup=markup, silent=silent\n        )\n        msg = self._get_response_message(request, await self(request), entity)\n        await self._cache_media(msg, file, file_handle, image=image)\n\n        return msg"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nspecializes version of .send_file for albums", "response": "async def _send_album(self, entity, files, caption='',\n                          progress_callback=None, reply_to=None,\n                          parse_mode=(), silent=None):\n        \"\"\"Specialized version of .send_file for albums\"\"\"\n        # We don't care if the user wants to avoid cache, we will use it\n        # anyway. Why? The cached version will be exactly the same thing\n        # we need to produce right now to send albums (uploadMedia), and\n        # cache only makes a difference for documents where the user may\n        # want the attributes used on them to change.\n        #\n        # In theory documents can be sent inside the albums but they appear\n        # as different messages (not inside the album), and the logic to set\n        # the attributes/avoid cache is already written in .send_file().\n        entity = await self.get_input_entity(entity)\n        if not utils.is_list_like(caption):\n            caption = (caption,)\n\n        captions = []\n        for c in reversed(caption):  # Pop from the end (so reverse)\n            captions.append(await self._parse_message_text(c or '', parse_mode))\n\n        reply_to = utils.get_message_id(reply_to)\n\n        # Need to upload the media first, but only if they're not cached yet\n        media = []\n        for file in files:\n            # Albums want :tl:`InputMedia` which, in theory, includes\n            # :tl:`InputMediaUploadedPhoto`. However using that will\n            # make it `raise MediaInvalidError`, so we need to upload\n            # it as media and then convert that to :tl:`InputMediaPhoto`.\n            fh, fm, _ = await self._file_to_media(file)\n            if isinstance(fm, types.InputMediaUploadedPhoto):\n                r = await self(functions.messages.UploadMediaRequest(\n                    entity, media=fm\n                ))\n                self.session.cache_file(\n                    fh.md5, fh.size, utils.get_input_photo(r.photo))\n\n                fm = utils.get_input_media(r.photo)\n\n            if captions:\n                caption, msg_entities = captions.pop()\n            else:\n                caption, msg_entities = '', None\n            media.append(types.InputSingleMedia(\n                fm,\n                message=caption,\n                entities=msg_entities\n            ))\n\n        # Now we can construct the multi-media request\n        result = await self(functions.messages.SendMultiMediaRequest(\n            entity, reply_to_msg_id=reply_to, multi_media=media, silent=silent\n        ))\n\n        # We never sent a `random_id` for the messages that resulted from\n        # the request so we can't pair them up with the `Updates` that we\n        # get from Telegram. However, the sent messages have a photo and\n        # the photo IDs match with those we did send.\n        #\n        # Updates -> {_: message}\n        messages = self._get_response_message(None, result, entity)\n        # {_: message} -> {photo ID: message}\n        messages = {m.photo.id: m for m in messages.values()}\n        # Sent photo IDs -> messages\n        return [messages[m.media.id.id] for m in media]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuploads a file to the specified TeleBot server and returns a handle to the file.", "response": "async def upload_file(\n            self, file, *, part_size_kb=None, file_name=None, use_cache=None,\n            progress_callback=None):\n        \"\"\"\n        Uploads the specified file and returns a handle (an instance of\n        :tl:`InputFile` or :tl:`InputFileBig`, as required) which can be\n        later used before it expires (they are usable during less than a day).\n\n        Uploading a file will simply return a \"handle\" to the file stored\n        remotely in the Telegram servers, which can be later used on. This\n        will **not** upload the file to your own chat or any chat at all.\n\n        Args:\n            file (`str` | `bytes` | `file`):\n                The path of the file, byte array, or stream that will be sent.\n                Note that if a byte array or a stream is given, a filename\n                or its type won't be inferred, and it will be sent as an\n                \"unnamed application/octet-stream\".\n\n            part_size_kb (`int`, optional):\n                Chunk size when uploading files. The larger, the less\n                requests will be made (up to 512KB maximum).\n\n            file_name (`str`, optional):\n                The file name which will be used on the resulting InputFile.\n                If not specified, the name will be taken from the ``file``\n                and if this is not a ``str``, it will be ``\"unnamed\"``.\n\n            use_cache (`type`, optional):\n                The type of cache to use (currently either :tl:`InputDocument`\n                or :tl:`InputPhoto`). If present and the file is small enough\n                to need the MD5, it will be checked against the database,\n                and if a match is found, the upload won't be made. Instead,\n                an instance of type ``use_cache`` will be returned.\n\n            progress_callback (`callable`, optional):\n                A callback function accepting two parameters:\n                ``(sent bytes, total)``.\n\n        Returns:\n            :tl:`InputFileBig` if the file size is larger than 10MB,\n            `telethon.tl.custom.inputsizedfile.InputSizedFile`\n            (subclass of :tl:`InputFile`) otherwise.\n        \"\"\"\n        if isinstance(file, (types.InputFile, types.InputFileBig)):\n            return file  # Already uploaded\n\n        if not file_name and getattr(file, 'name', None):\n            file_name = file.name\n\n        if isinstance(file, str):\n            file_size = os.path.getsize(file)\n        elif isinstance(file, bytes):\n            file_size = len(file)\n        else:\n            if isinstance(file, io.IOBase) and file.seekable():\n                pos = file.tell()\n            else:\n                pos = None\n\n            # TODO Don't load the entire file in memory always\n            data = file.read()\n            if pos is not None:\n                file.seek(pos)\n\n            file = data\n            file_size = len(file)\n\n        # File will now either be a string or bytes\n        if not part_size_kb:\n            part_size_kb = utils.get_appropriated_part_size(file_size)\n\n        if part_size_kb > 512:\n            raise ValueError('The part size must be less or equal to 512KB')\n\n        part_size = int(part_size_kb * 1024)\n        if part_size % 1024 != 0:\n            raise ValueError(\n                'The part size must be evenly divisible by 1024')\n\n        # Set a default file name if None was specified\n        file_id = helpers.generate_random_long()\n        if not file_name:\n            if isinstance(file, str):\n                file_name = os.path.basename(file)\n            else:\n                file_name = str(file_id)\n\n        # If the file name lacks extension, add it if possible.\n        # Else Telegram complains with `PHOTO_EXT_INVALID_ERROR`\n        # even if the uploaded image is indeed a photo.\n        if not os.path.splitext(file_name)[-1]:\n            file_name += utils._get_extension(file)\n\n        # Determine whether the file is too big (over 10MB) or not\n        # Telegram does make a distinction between smaller or larger files\n        is_large = file_size > 10 * 1024 * 1024\n        hash_md5 = hashlib.md5()\n        if not is_large:\n            # Calculate the MD5 hash before anything else.\n            # As this needs to be done always for small files,\n            # might as well do it before anything else and\n            # check the cache.\n            if isinstance(file, str):\n                with open(file, 'rb') as stream:\n                    file = stream.read()\n            hash_md5.update(file)\n            if use_cache:\n                cached = self.session.get_file(\n                    hash_md5.digest(), file_size, cls=_CacheType(use_cache)\n                )\n                if cached:\n                    return cached\n\n        part_count = (file_size + part_size - 1) // part_size\n        self._log[__name__].info('Uploading file of %d bytes in %d chunks of %d',\n                                 file_size, part_count, part_size)\n\n        with open(file, 'rb') if isinstance(file, str) else BytesIO(file)\\\n                as stream:\n            for part_index in range(part_count):\n                # Read the file by in chunks of size part_size\n                part = stream.read(part_size)\n\n                # The SavePartRequest is different depending on whether\n                # the file is too large or not (over or less than 10MB)\n                if is_large:\n                    request = functions.upload.SaveBigFilePartRequest(\n                        file_id, part_index, part_count, part)\n                else:\n                    request = functions.upload.SaveFilePartRequest(\n                        file_id, part_index, part)\n\n                result = await self(request)\n                if result:\n                    self._log[__name__].debug('Uploaded %d/%d',\n                                              part_index + 1, part_count)\n                    if progress_callback:\n                        progress_callback(stream.tell(), file_size)\n                else:\n                    raise RuntimeError(\n                        'Failed to upload file part {}.'.format(part_index))\n\n        if is_large:\n            return types.InputFileBig(file_id, part_count, file_name)\n        else:\n            return custom.InputSizedFile(\n                file_id, part_count, file_name, md5=hash_md5, size=file_size\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add(self, entities):\n        if not utils.is_list_like(entities):\n            # Invariant: all \"chats\" and \"users\" are always iterables,\n            # and \"user\" never is (so we wrap it inside a list).\n            entities = itertools.chain(\n                getattr(entities, 'chats', []),\n                getattr(entities, 'users', []),\n                (hasattr(entities, 'user') and [entities.user]) or []\n            )\n\n        for entity in entities:\n            try:\n                pid = utils.get_peer_id(entity)\n                if pid not in self.__dict__:\n                    # Note: `get_input_peer` already checks for `access_hash`\n                    self.__dict__[pid] = utils.get_input_peer(entity)\n            except TypeError:\n                pass", "response": "Adds the given entities to the cache if they weren t saved before."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_until_disconnected(self):\n        if self.loop.is_running():\n            return self._run_until_disconnected()\n        try:\n            return self.loop.run_until_complete(self.disconnected)\n        except KeyboardInterrupt:\n            pass\n        finally:\n            # No loop.run_until_complete; it's already syncified\n            self.disconnect()", "response": "Runs the event loop until the connection is disconnected."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on(self, event):\n        def decorator(f):\n            self.add_event_handler(f, event)\n            return f\n\n        return decorator", "response": "Decorator for adding event handlers to the internal cache."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_event_handler(self, callback, event=None):\n        builders = events._get_handlers(callback)\n        if builders is not None:\n            for event in builders:\n                self._event_builders.append((event, callback))\n            return\n\n        if isinstance(event, type):\n            event = event()\n        elif not event:\n            event = events.Raw()\n\n        self._event_builders.append((event, callback))", "response": "Adds a callback to be called on the specified event."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_event_handler(self, callback, event=None):\n        found = 0\n        if event and not isinstance(event, type):\n            event = type(event)\n\n        i = len(self._event_builders)\n        while i:\n            i -= 1\n            ev, cb = self._event_builders[i]\n            if cb == callback and (not event or isinstance(ev, event)):\n                del self._event_builders[i]\n                found += 1\n\n        return found", "response": "Removes a callback from the event handlers list."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncatch up on the missed updates while the client was offline.", "response": "async def catch_up(self):\n        \"\"\"\n        \"Catches up\" on the missed updates while the client was offline.\n        You should call this method after registering the event handlers\n        so that the updates it loads can by processed by your script.\n\n        This can also be used to forcibly fetch new updates if there are any.\n        \"\"\"\n        pts, date = self._state_cache[None]\n        self.session.catching_up = True\n        try:\n            while True:\n                d = await self(functions.updates.GetDifferenceRequest(\n                    pts, date, 0\n                ))\n                if isinstance(d, (types.updates.DifferenceSlice,\n                                  types.updates.Difference)):\n                    if isinstance(d, types.updates.Difference):\n                        state = d.state\n                    else:\n                        state = d.intermediate_state\n\n                    pts, date = state.pts, state.date\n                    self._handle_update(types.Updates(\n                        users=d.users,\n                        chats=d.chats,\n                        date=state.date,\n                        seq=state.seq,\n                        updates=d.other_updates + [\n                            types.UpdateNewMessage(m, 0, 0)\n                            for m in d.new_messages\n                        ]\n                    ))\n\n                    # TODO Implement upper limit (max_pts)\n                    # We don't want to fetch updates we already know about.\n                    #\n                    # We may still get duplicates because the Difference\n                    # contains a lot of updates and presumably only has\n                    # the state for the last one, but at least we don't\n                    # unnecessarily fetch too many.\n                    #\n                    # updates.getDifference's pts_total_limit seems to mean\n                    # \"how many pts is the request allowed to return\", and\n                    # if there is more than that, it returns \"too long\" (so\n                    # there would be duplicate updates since we know about\n                    # some). This can be used to detect collisions (i.e.\n                    # it would return an update we have already seen).\n                else:\n                    if isinstance(d, types.updates.DifferenceEmpty):\n                        date = d.date\n                    elif isinstance(d, types.updates.DifferenceTooLong):\n                        pts = d.pts\n                    break\n        except (ConnectionError, asyncio.CancelledError):\n            pass\n        finally:\n            # TODO Save new pts to session\n            self._state_cache._pts_date = (pts, date)\n            self.session.catching_up = False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reset(self):\n        # Session IDs can be random on every connection\n        self.id = struct.unpack('q', os.urandom(8))[0]\n        self._sequence = 0\n        self._last_msg_id = 0", "response": "Resets the state of the object to its initial state."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the key based on the guidelines for MTProto 2 and MTProto 3.", "response": "def _calc_key(auth_key, msg_key, client):\n        \"\"\"\n        Calculate the key based on Telegram guidelines for MTProto 2,\n        specifying whether it's the client or not. See\n        https://core.telegram.org/mtproto/description#defining-aes-key-and-initialization-vector\n        \"\"\"\n        x = 0 if client else 8\n        sha256a = sha256(msg_key + auth_key[x: x + 36]).digest()\n        sha256b = sha256(auth_key[x + 40:x + 76] + msg_key).digest()\n\n        aes_key = sha256a[:8] + sha256b[8:24] + sha256a[24:32]\n        aes_iv = sha256b[:8] + sha256a[8:24] + sha256b[24:32]\n\n        return aes_key, aes_iv"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_data_as_message(self, buffer, data, content_related,\n                              *, after_id=None):\n        \"\"\"\n        Writes a message containing the given data into buffer.\n\n        Returns the message id.\n        \"\"\"\n        msg_id = self._get_new_msg_id()\n        seq_no = self._get_seq_no(content_related)\n        if after_id is None:\n            body = GzipPacked.gzip_if_smaller(content_related, data)\n        else:\n            body = GzipPacked.gzip_if_smaller(content_related,\n                bytes(InvokeAfterMsgRequest(after_id, data)))\n\n        buffer.write(struct.pack('<qii', msg_id, seq_no, len(body)))\n        buffer.write(body)\n        return msg_id", "response": "Writes a message containing the given data into buffer. Returns the message id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nencrypting the given message data using the current authorization key.", "response": "def encrypt_message_data(self, data):\n        \"\"\"\n        Encrypts the given message data using the current authorization key\n        following MTProto 2.0 guidelines core.telegram.org/mtproto/description.\n        \"\"\"\n        data = struct.pack('<qq', self.salt, self.id) + data\n        padding = os.urandom(-(len(data) + 12) % 16 + 12)\n\n        # Being substr(what, offset, length); x = 0 for client\n        # \"msg_key_large = SHA256(substr(auth_key, 88+x, 32) + pt + padding)\"\n        msg_key_large = sha256(\n            self.auth_key.key[88:88 + 32] + data + padding).digest()\n\n        # \"msg_key = substr (msg_key_large, 8, 16)\"\n        msg_key = msg_key_large[8:24]\n        aes_key, aes_iv = self._calc_key(self.auth_key.key, msg_key, True)\n\n        key_id = struct.pack('<Q', self.auth_key.key_id)\n        return (key_id + msg_key +\n                AES.encrypt_ige(data + padding, aes_key, aes_iv))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef decrypt_message_data(self, body):\n        if len(body) < 8:\n            raise InvalidBufferError(body)\n\n        # TODO Check salt, session_id and sequence_number\n        key_id = struct.unpack('<Q', body[:8])[0]\n        if key_id != self.auth_key.key_id:\n            raise SecurityError('Server replied with an invalid auth key')\n\n        msg_key = body[8:24]\n        aes_key, aes_iv = self._calc_key(self.auth_key.key, msg_key, False)\n        body = AES.decrypt_ige(body[24:], aes_key, aes_iv)\n\n        # https://core.telegram.org/mtproto/security_guidelines\n        # Sections \"checking sha256 hash\" and \"message length\"\n        our_key = sha256(self.auth_key.key[96:96 + 32] + body)\n        if msg_key != our_key.digest()[8:24]:\n            raise SecurityError(\n                \"Received msg_key doesn't match with expected one\")\n\n        reader = BinaryReader(body)\n        reader.read_long()  # remote_salt\n        if reader.read_long() != self.id:\n            raise SecurityError('Server replied with a wrong session ID')\n\n        remote_msg_id = reader.read_long()\n        remote_sequence = reader.read_int()\n        reader.read_int()  # msg_len for the inner object, padding ignored\n\n        # We could read msg_len bytes and use those in a new reader to read\n        # the next TLObject without including the padding, but since the\n        # reader isn't used for anything else after this, it's unnecessary.\n        obj = reader.tgread_object()\n\n        return TLMessage(remote_msg_id, remote_sequence, obj)", "response": "Decrypt a message body."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_new_msg_id(self):\n        now = time.time() + self.time_offset\n        nanoseconds = int((now - int(now)) * 1e+9)\n        new_msg_id = (int(now) << 32) | (nanoseconds << 2)\n\n        if self._last_msg_id >= new_msg_id:\n            new_msg_id = self._last_msg_id + 4\n\n        self._last_msg_id = new_msg_id\n        return new_msg_id", "response": "Generates a new unique message ID based on the current time since epoch."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_time_offset(self, correct_msg_id):\n        bad = self._get_new_msg_id()\n        old = self.time_offset\n\n        now = int(time.time())\n        correct = correct_msg_id >> 32\n        self.time_offset = correct - now\n\n        if self.time_offset != old:\n            self._last_msg_id = 0\n            self._log.debug(\n                'Updated time offset (old offset %d, bad %d, good %d, new %d)',\n                old, bad, correct_msg_id, self.time_offset\n            )\n\n        return self.time_offset", "response": "Updates the time offset to the correct message ID."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate the next sequence number depending on whether the current sequence number should be for a content - related query.", "response": "def _get_seq_no(self, content_related):\n        \"\"\"\n        Generates the next sequence number depending on whether\n        it should be for a content-related query or not.\n        \"\"\"\n        if content_related:\n            result = self._sequence * 2 + 1\n            self._sequence += 1\n            return result\n        else:\n            return self._sequence * 2"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_tl(file_path, layer, methods=None, ignored_ids=CORE_TYPES):\n    method_info = {m.name: m for m in (methods or [])}\n    obj_all = []\n    obj_by_name = {}\n    obj_by_type = collections.defaultdict(list)\n    with file_path.open() as file:\n        is_function = False\n        for line in file:\n            comment_index = line.find('//')\n            if comment_index != -1:\n                line = line[:comment_index]\n\n            line = line.strip()\n            if not line:\n                continue\n\n            match = re.match('---(\\w+)---', line)\n            if match:\n                following_types = match.group(1)\n                is_function = following_types == 'functions'\n                continue\n\n            try:\n                result = _from_line(\n                    line, is_function, method_info, layer=layer)\n\n                if result.id in ignored_ids:\n                    continue\n\n                obj_all.append(result)\n                if not result.is_function:\n                    obj_by_name[result.fullname] = result\n                    obj_by_type[result.result].append(result)\n            except ValueError as e:\n                if 'vector#1cb5c415' not in str(e):\n                    raise\n\n    # Once all objects have been parsed, replace the\n    # string type from the arguments with references\n    for obj in obj_all:\n        if obj.id in AUTH_KEY_TYPES:\n            for arg in obj.args:\n                if arg.type == 'string':\n                    arg.type = 'bytes'\n\n        for arg in obj.args:\n            arg.cls = obj_by_type.get(arg.type) or (\n                [obj_by_name[arg.type]] if arg.type in obj_by_name else []\n            )\n\n    yield from obj_all", "response": "This method parses a. tl file and yields TLObjects."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind the layer used on the specified scheme. tl file.", "response": "def find_layer(file_path):\n    \"\"\"Finds the layer used on the specified scheme.tl file.\"\"\"\n    layer_regex = re.compile(r'^//\\s*LAYER\\s*(\\d+)$')\n    with file_path.open('r') as file:\n        for line in file:\n            match = layer_regex.match(line)\n            if match:\n                return int(match.group(1))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef photo(self):\n        if isinstance(self.result, types.BotInlineResult):\n            return self.result.thumb\n        elif isinstance(self.result, types.BotInlineMediaResult):\n            return self.result.photo", "response": "Returns either the WebDocument thumbnail for the current result or the Photo for the current result."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning either the WebDocument content for the current result or the Document for media results.", "response": "def document(self):\n        \"\"\"\n        Returns either the :tl:`WebDocument` content for\n        normal results or the :tl:`Document` for media results.\n        \"\"\"\n        if isinstance(self.result, types.BotInlineResult):\n            return self.result.content\n        elif isinstance(self.result, types.BotInlineMediaResult):\n            return self.result.document"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nclicks this result and sends the associated message.", "response": "async def click(self, entity, reply_to=None,\n                    silent=False, clear_draft=False, hide_via=False):\n        \"\"\"\n        Clicks this result and sends the associated `message`.\n\n        Args:\n            entity (`entity`):\n                The entity to which the message of this result should be sent.\n\n            reply_to (`int` | `Message <telethon.tl.custom.message.Message>`, optional):\n                If present, the sent message will reply to this ID or message.\n\n            silent (`bool`, optional):\n                If ``True``, the sent message will not notify the user(s).\n\n            clear_draft (`bool`, optional):\n                Whether the draft should be removed after sending the\n                message from this result or not. Defaults to ``False``.\n            \n            hide_via (`bool`, optional):\n                Whether the \"via @bot\" should be hidden or not.\n                Only works with certain bots (like @bing or @gif).\n        \"\"\"\n        entity = await self._client.get_input_entity(entity)\n        reply_id = None if reply_to is None else utils.get_message_id(reply_to)\n        req = functions.messages.SendInlineBotResultRequest(\n            peer=entity,\n            query_id=self._query_id,\n            id=self.result.id,\n            silent=silent,\n            clear_draft=clear_draft,\n            hide_via=hide_via,\n            reply_to_msg_id=reply_id\n        )\n        return self._client._get_response_message(\n            req, await self._client(req), entity)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def download_media(self, *args, **kwargs):\n        if self.document or self.photo:\n            return await self._client.download_media(\n                self.document or self.photo, *args, **kwargs)", "response": "Downloads the media in this result."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the state of the given object with the given update.", "response": "def update(\n            self,\n            update,\n            *,\n            channel_id=None,\n            has_pts=(\n                types.UpdateNewMessage,\n                types.UpdateDeleteMessages,\n                types.UpdateReadHistoryInbox,\n                types.UpdateReadHistoryOutbox,\n                types.UpdateWebPage,\n                types.UpdateReadMessagesContents,\n                types.UpdateEditMessage,\n                types.updates.State,\n                types.updates.DifferenceTooLong,\n                types.UpdateShortMessage,\n                types.UpdateShortChatMessage,\n                types.UpdateShortSentMessage\n            ),\n            has_date=(\n                types.UpdateUserPhoto,\n                types.UpdateEncryption,\n                types.UpdateEncryptedMessagesRead,\n                types.UpdateChatParticipantAdd,\n                types.updates.DifferenceEmpty,\n                types.UpdateShortMessage,\n                types.UpdateShortChatMessage,\n                types.UpdateShort,\n                types.UpdatesCombined,\n                types.Updates,\n                types.UpdateShortSentMessage,\n            ),\n            has_channel_pts=(\n                types.UpdateChannelTooLong,\n                types.UpdateNewChannelMessage,\n                types.UpdateDeleteChannelMessages,\n                types.UpdateEditChannelMessage,\n                types.UpdateChannelWebPage,\n                types.updates.ChannelDifferenceEmpty,\n                types.updates.ChannelDifferenceTooLong,\n                types.updates.ChannelDifference\n            )\n    ):\n        \"\"\"\n        Update the state with the given update.\n        \"\"\"\n        has_pts = isinstance(update, has_pts)\n        has_date = isinstance(update, has_date)\n        has_channel_pts = isinstance(update, has_channel_pts)\n        if has_pts and has_date:\n            self._pts_date = update.pts, update.date\n        elif has_pts:\n            self._pts_date = update.pts, self._pts_date[1]\n        elif has_date:\n            self._pts_date = self._pts_date[0], update.date\n\n        if has_channel_pts:\n            if channel_id is None:\n                channel_id = self.get_channel_id(update)\n\n            if channel_id is None:\n                self._logger.info(\n                    'Failed to retrieve channel_id from %s', update)\n            else:\n                self.__dict__[channel_id] = update.pts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an iterator over the dialogs in the system.", "response": "def iter_dialogs(\n            self, limit=None, *, offset_date=None, offset_id=0,\n            offset_peer=types.InputPeerEmpty(), ignore_migrated=False\n    ):\n        \"\"\"\n        Returns an iterator over the dialogs, yielding 'limit' at most.\n        Dialogs are the open \"chats\" or conversations with other people,\n        groups you have joined, or channels you are subscribed to.\n\n        Args:\n            limit (`int` | `None`):\n                How many dialogs to be retrieved as maximum. Can be set to\n                ``None`` to retrieve all dialogs. Note that this may take\n                whole minutes if you have hundreds of dialogs, as Telegram\n                will tell the library to slow down through a\n                ``FloodWaitError``.\n\n            offset_date (`datetime`, optional):\n                The offset date to be used.\n\n            offset_id (`int`, optional):\n                The message ID to be used as an offset.\n\n            offset_peer (:tl:`InputPeer`, optional):\n                The peer to be used as an offset.\n\n            ignore_migrated (`bool`, optional):\n                Whether :tl:`Chat` that have ``migrated_to`` a :tl:`Channel`\n                should be included or not. By default all the chats in your\n                dialogs are returned, but setting this to ``True`` will hide\n                them in the same way official applications do.\n\n        Yields:\n            Instances of `telethon.tl.custom.dialog.Dialog`.\n        \"\"\"\n        return _DialogsIter(\n            self,\n            limit,\n            offset_date=offset_date,\n            offset_id=offset_id,\n            offset_peer=offset_peer,\n            ignore_migrated=ignore_migrated\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new conversation with the given entity and returns the messages.", "response": "def conversation(\n            self, entity,\n            *, timeout=60, total_timeout=None, max_messages=100,\n            exclusive=True, replies_are_responses=True):\n        \"\"\"\n        Creates a `Conversation <telethon.tl.custom.conversation.Conversation>`\n        with the given entity so you can easily send messages and await for\n        responses or other reactions. Refer to its documentation for more.\n\n        Args:\n            entity (`entity`):\n                The entity with which a new conversation should be opened.\n\n            timeout (`int` | `float`, optional):\n                The default timeout (in seconds) *per action* to be used. You\n                may also override this timeout on a per-method basis. By\n                default each action can take up to 60 seconds (the value of\n                this timeout).\n\n            total_timeout (`int` | `float`, optional):\n                The total timeout (in seconds) to use for the whole\n                conversation. This takes priority over per-action\n                timeouts. After these many seconds pass, subsequent\n                actions will result in ``asyncio.TimeoutError``.\n\n            max_messages (`int`, optional):\n                The maximum amount of messages this conversation will\n                remember. After these many messages arrive in the\n                specified chat, subsequent actions will result in\n                ``ValueError``.\n\n            exclusive (`bool`, optional):\n                By default, conversations are exclusive within a single\n                chat. That means that while a conversation is open in a\n                chat, you can't open another one in the same chat, unless\n                you disable this flag.\n\n                If you try opening an exclusive conversation for\n                a chat where it's already open, it will raise\n                ``AlreadyInConversationError``.\n\n            replies_are_responses (`bool`, optional):\n                Whether replies should be treated as responses or not.\n\n                If the setting is enabled, calls to `conv.get_response\n                <telethon.tl.custom.conversation.Conversation.get_response>`\n                and a subsequent call to `conv.get_reply\n                <telethon.tl.custom.conversation.Conversation.get_reply>`\n                will return different messages, otherwise they may return\n                the same message.\n\n                Consider the following scenario with one outgoing message,\n                1, and two incoming messages, the second one replying::\n\n                                        Hello! <1\n                    2> (reply to 1) Hi!\n                    3> (reply to 1) How are you?\n\n                And the following code:\n\n                .. code-block:: python\n\n                    async with client.conversation(chat) as conv:\n                        msg1 = await conv.send_message('Hello!')\n                        msg2 = await conv.get_response()\n                        msg3 = await conv.get_reply()\n\n                With the setting enabled, ``msg2`` will be ``'Hi!'`` and\n                ``msg3`` be ``'How are you?'`` since replies are also\n                responses, and a response was already returned.\n\n                With the setting disabled, both ``msg2`` and ``msg3`` will\n                be ``'Hi!'`` since one is a response and also a reply.\n\n        Returns:\n            A `Conversation <telethon.tl.custom.conversation.Conversation>`.\n        \"\"\"\n        return custom.Conversation(\n            self,\n            entity,\n            timeout=timeout,\n            total_timeout=total_timeout,\n            max_messages=max_messages,\n            exclusive=exclusive,\n            replies_are_responses=replies_are_responses\n\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def connect(self, connection):\n        if self._user_connected:\n            self._log.info('User is already connected!')\n            return\n\n        self._connection = connection\n        await self._connect()\n        self._user_connected = True", "response": "Connects to the specified connection using the given auth key."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconnects to the Telegram server and generates the authorization key if necessary.", "response": "async def _connect(self):\n        \"\"\"\n        Performs the actual connection, retrying, generating the\n        authorization key if necessary, and starting the send and\n        receive loops.\n        \"\"\"\n        self._log.info('Connecting to %s...', self._connection)\n        for attempt in retry_range(self._retries):\n            try:\n                self._log.debug('Connection attempt {}...'.format(attempt))\n                await self._connection.connect(timeout=self._connect_timeout)\n            except (IOError, asyncio.TimeoutError) as e:\n                self._log.warning('Attempt {} at connecting failed: {}: {}'\n                                  .format(attempt, type(e).__name__, e))\n                await asyncio.sleep(self._delay)\n            else:\n                break\n        else:\n            raise ConnectionError('Connection to Telegram failed {} time(s)'\n                                  .format(attempt))\n\n        self._log.debug('Connection success!')\n        if not self.auth_key:\n            plain = MTProtoPlainSender(self._connection, loggers=self._loggers)\n            for attempt in retry_range(self._retries):\n                try:\n                    self._log.debug('New auth_key attempt {}...'\n                                    .format(attempt))\n                    self.auth_key.key, self._state.time_offset =\\\n                        await authenticator.do_authentication(plain)\n\n                    # This is *EXTREMELY* important since we don't control\n                    # external references to the authorization key, we must\n                    # notify whenever we change it. This is crucial when we\n                    # switch to different data centers.\n                    if self._auth_key_callback:\n                        self._auth_key_callback(self.auth_key)\n\n                    break\n                except (SecurityError, AssertionError) as e:\n                    self._log.warning('Attempt {} at new auth_key failed: {}'\n                                      .format(attempt, e))\n                    await asyncio.sleep(self._delay)\n            else:\n                e = ConnectionError('auth_key generation failed {} time(s)'\n                                    .format(attempt))\n                await self._disconnect(error=e)\n                raise e\n\n        self._log.debug('Starting send loop')\n        self._send_loop_handle = self._loop.create_task(self._send_loop())\n\n        self._log.debug('Starting receive loop')\n        self._recv_loop_handle = self._loop.create_task(self._recv_loop())\n\n        # _disconnected only completes after manual disconnection\n        # or errors after which the sender cannot continue such\n        # as failing to reconnect or any unexpected error.\n        if self._disconnected.done():\n            self._disconnected = self._loop.create_future()\n\n        self._log.info('Connection to %s complete!', self._connection)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreconnecting to the current connection.", "response": "async def _reconnect(self, last_error):\n        \"\"\"\n        Cleanly disconnects and then reconnects.\n        \"\"\"\n        self._log.debug('Closing current connection...')\n        await self._connection.disconnect()\n\n        await helpers._cancel(\n            self._log,\n            send_loop_handle=self._send_loop_handle,\n            recv_loop_handle=self._recv_loop_handle\n        )\n\n        # TODO See comment in `_start_reconnect`\n        # Perhaps this should be the last thing to do?\n        # But _connect() creates tasks which may run and,\n        # if they see that reconnecting is True, they will end.\n        # Perhaps that task creation should not belong in connect?\n        self._reconnecting = False\n\n        # Start with a clean state (and thus session ID) to avoid old msgs\n        self._state.reset()\n\n        retries = self._retries if self._auto_reconnect else 0\n        for attempt in retry_range(retries):\n            try:\n                await self._connect()\n            except (IOError, asyncio.TimeoutError) as e:\n                last_error = e\n                self._log.info('Failed reconnection attempt %d with %s',\n                               attempt, e.__class__.__name__)\n\n                await asyncio.sleep(self._delay)\n            except Exception as e:\n                last_error = e\n                self._log.exception('Unexpected exception reconnecting on '\n                                    'attempt %d', attempt)\n\n                await asyncio.sleep(self._delay)\n            else:\n                self._send_queue.extend(self._pending_state.values())\n                self._pending_state.clear()\n\n                if self._auto_reconnect_callback:\n                    self._loop.create_task(self._auto_reconnect_callback())\n\n                break\n        else:\n            self._log.error('Automatic reconnection failed {} time(s)'\n                            .format(attempt))\n            await self._disconnect(error=last_error.with_traceback(None))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts a reconnection in the background.", "response": "def _start_reconnect(self, error):\n        \"\"\"Starts a reconnection in the background.\"\"\"\n        if self._user_connected and not self._reconnecting:\n            # We set reconnecting to True here and not inside the new task\n            # because it may happen that send/recv loop calls this again\n            # while the new task hasn't had a chance to run yet. This race\n            # condition puts `self.connection` in a bad state with two calls\n            # to its `connect` without disconnecting, so it creates a second\n            # receive loop. There can't be two tasks receiving data from\n            # the reader, since that causes an error, and the library just\n            # gets stuck.\n            # TODO It still gets stuck? Investigate where and why.\n            self._reconnecting = True\n            self._loop.create_task(self._reconnect(error))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def _send_loop(self):\n        while self._user_connected and not self._reconnecting:\n            if self._pending_ack:\n                ack = RequestState(MsgsAck(list(self._pending_ack)), self._loop)\n                self._send_queue.append(ack)\n                self._last_acks.append(ack)\n                self._pending_ack.clear()\n\n            self._log.debug('Waiting for messages to send...')\n            # TODO Wait for the connection send queue to be empty?\n            # This means that while it's not empty we can wait for\n            # more messages to be added to the send queue.\n            batch, data = await self._send_queue.get()\n\n            if not data:\n                continue\n\n            self._log.debug('Encrypting %d message(s) in %d bytes for sending',\n                          len(batch), len(data))\n\n            data = self._state.encrypt_message_data(data)\n            try:\n                await self._connection.send(data)\n            except IOError as e:\n                self._log.info('Connection closed while sending data')\n                self._start_reconnect(e)\n                return\n\n            for state in batch:\n                if not isinstance(state, list):\n                    if isinstance(state.request, TLRequest):\n                        self._pending_state[state.msg_id] = state\n                else:\n                    for s in state:\n                        if isinstance(s.request, TLRequest):\n                            self._pending_state[s.msg_id] = s\n\n            self._log.debug('Encrypted messages put in a queue to be sent')", "response": "This method is responsible for sending messages over the network."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def _recv_loop(self):\n        while self._user_connected and not self._reconnecting:\n            self._log.debug('Receiving items from the network...')\n            try:\n                body = await self._connection.recv()\n            except IOError as e:\n                self._log.info('Connection closed while receiving data')\n                self._start_reconnect(e)\n                return\n\n            try:\n                message = self._state.decrypt_message_data(body)\n            except TypeNotFoundError as e:\n                # Received object which we don't know how to deserialize\n                self._log.info('Type %08x not found, remaining data %r',\n                             e.invalid_constructor_id, e.remaining)\n                continue\n            except SecurityError as e:\n                # A step while decoding had the incorrect data. This message\n                # should not be considered safe and it should be ignored.\n                self._log.warning('Security error while unpacking a '\n                                'received message: %s', e)\n                continue\n            except BufferError as e:\n                if isinstance(e, InvalidBufferError) and e.code == 404:\n                    self._log.info('Broken authorization key; resetting')\n                else:\n                    self._log.warning('Invalid buffer %s', e)\n\n                self.auth_key.key = None\n                if self._auth_key_callback:\n                    self._auth_key_callback(None)\n\n                self._start_reconnect(e)\n                return\n            except Exception as e:\n                self._log.exception('Unhandled error while receiving data')\n                self._start_reconnect(e)\n                return\n\n            try:\n                await self._process_message(message)\n            except Exception:\n                self._log.exception('Unhandled error while processing msgs')", "response": "This loop is responsible for reading all incoming responses from the network and handling and dispatching the messages."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses a message received from the broker.", "response": "async def _process_message(self, message):\n        \"\"\"\n        Adds the given message to the list of messages that must be\n        acknowledged and dispatches control to different ``_handle_*``\n        method based on its type.\n        \"\"\"\n        self._pending_ack.add(message.msg_id)\n        handler = self._handlers.get(message.obj.CONSTRUCTOR_ID,\n                                     self._handle_update)\n        await handler(message)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _pop_states(self, msg_id):\n        state = self._pending_state.pop(msg_id, None)\n        if state:\n            return [state]\n\n        to_pop = []\n        for state in self._pending_state.values():\n            if state.container_id == msg_id:\n                to_pop.append(state.msg_id)\n\n        if to_pop:\n            return [self._pending_state.pop(x) for x in to_pop]\n\n        for ack in self._last_acks:\n            if ack.msg_id == msg_id:\n                return [ack]\n\n        return []", "response": "Pops the states known to match the given ID from pending messages."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def _handle_rpc_result(self, message):\n        rpc_result = message.obj\n        state = self._pending_state.pop(rpc_result.req_msg_id, None)\n        self._log.debug('Handling RPC result for message %d',\n                      rpc_result.req_msg_id)\n\n        if not state:\n            # TODO We should not get responses to things we never sent\n            # However receiving a File() with empty bytes is \"common\".\n            # See #658, #759 and #958. They seem to happen in a container\n            # which contain the real response right after.\n            try:\n                with BinaryReader(rpc_result.body) as reader:\n                    if not isinstance(reader.tgread_object(), upload.File):\n                        raise ValueError('Not an upload.File')\n            except (TypeNotFoundError, ValueError):\n                self._log.info('Received response without parent request: {}'\n                               .format(rpc_result.body))\n            return\n\n        if rpc_result.error:\n            error = rpc_message_to_error(rpc_result.error, state.request)\n            self._send_queue.append(\n                RequestState(MsgsAck([state.msg_id]), loop=self._loop))\n\n            if not state.future.cancelled():\n                state.future.set_exception(error)\n        else:\n            with BinaryReader(rpc_result.body) as reader:\n                result = state.request.read_result(reader)\n\n            if not state.future.cancelled():\n                state.future.set_result(result)", "response": "Handle the result of a remote procedure call."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprocesses the inner messages of a container.", "response": "async def _handle_container(self, message):\n        \"\"\"\n        Processes the inner messages of a container with many of them:\n\n            msg_container#73f1f8dc messages:vector<%Message> = MessageContainer;\n        \"\"\"\n        self._log.debug('Handling container')\n        for inner_message in message.obj.messages:\n            await self._process_message(inner_message)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles a gzipped message.", "response": "async def _handle_gzip_packed(self, message):\n        \"\"\"\n        Unpacks the data from a gzipped object and processes it:\n\n            gzip_packed#3072cfa1 packed_data:bytes = Object;\n        \"\"\"\n        self._log.debug('Handling gzipped data')\n        with BinaryReader(message.obj.data) as reader:\n            message.obj = reader.tgread_object()\n            await self._process_message(message)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def _handle_pong(self, message):\n        pong = message.obj\n        self._log.debug('Handling pong for message %d', pong.msg_id)\n        state = self._pending_state.pop(pong.msg_id, None)\n        if state:\n            state.future.set_result(pong)", "response": "Handles pong results, which don't come inside a ``rpc_result``\n        but are still sent through a request:\n\n            pong#347773c5 msg_id:long ping_id:long = Pong;"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def _handle_bad_server_salt(self, message):\n        bad_salt = message.obj\n        self._log.debug('Handling bad salt for message %d', bad_salt.bad_msg_id)\n        self._state.salt = bad_salt.new_server_salt\n        states = self._pop_states(bad_salt.bad_msg_id)\n        self._send_queue.extend(states)\n\n        self._log.debug('%d message(s) will be resent', len(states))", "response": "Handle a bad server salt message."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def _handle_bad_notification(self, message):\n        bad_msg = message.obj\n        states = self._pop_states(bad_msg.bad_msg_id)\n\n        self._log.debug('Handling bad msg %s', bad_msg)\n        if bad_msg.error_code in (16, 17):\n            # Sent msg_id too low or too high (respectively).\n            # Use the current msg_id to determine the right time offset.\n            to = self._state.update_time_offset(\n                correct_msg_id=message.msg_id)\n            self._log.info('System clock is wrong, set time offset to %ds', to)\n        elif bad_msg.error_code == 32:\n            # msg_seqno too low, so just pump it up by some \"large\" amount\n            # TODO A better fix would be to start with a new fresh session ID\n            self._state._sequence += 64\n        elif bad_msg.error_code == 33:\n            # msg_seqno too high never seems to happen but just in case\n            self._state._sequence -= 16\n        else:\n            for state in states:\n                state.future.set_exception(\n                    BadMessageError(state.request, bad_msg.error_code))\n            return\n\n        # Messages are to be re-sent once we've corrected the issue\n        self._send_queue.extend(states)\n        self._log.debug('%d messages will be resent due to bad msg',\n                        len(states))", "response": "Handle a bad message notification."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhandle a detailed info message.", "response": "async def _handle_detailed_info(self, message):\n        \"\"\"\n        Updates the current status with the received detailed information:\n\n            msg_detailed_info#276d3ec6 msg_id:long answer_msg_id:long\n            bytes:int status:int = MsgDetailedInfo;\n        \"\"\"\n        # TODO https://goo.gl/VvpCC6\n        msg_id = message.obj.answer_msg_id\n        self._log.debug('Handling detailed info for message %d', msg_id)\n        self._pending_ack.add(msg_id)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhandle a NewSessionCreated message.", "response": "async def _handle_new_session_created(self, message):\n        \"\"\"\n        Updates the current status with the received session information:\n\n            new_session_created#9ec20908 first_msg_id:long unique_id:long\n            server_salt:long = NewSession;\n        \"\"\"\n        # TODO https://goo.gl/LMyN7A\n        self._log.debug('Handling new session created')\n        self._state.salt = message.obj.server_salt"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def _handle_ack(self, message):\n        ack = message.obj\n        self._log.debug('Handling acknowledge for %s', str(ack.msg_ids))\n        for msg_id in ack.msg_ids:\n            state = self._pending_state.get(msg_id)\n            if state and isinstance(state.request, LogOutRequest):\n                del self._pending_state[msg_id]\n                state.future.set_result(True)", "response": "Handle an ack from the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles future salts from the server.", "response": "async def _handle_future_salts(self, message):\n        \"\"\"\n        Handles future salt results, which don't come inside a\n        ``rpc_result`` but are still sent through a request:\n\n            future_salts#ae500895 req_msg_id:long now:int\n            salts:vector<future_salt> = FutureSalts;\n        \"\"\"\n        # TODO save these salts and automatically adjust to the\n        # correct one whenever the salt in use expires.\n        self._log.debug('Handling future salts for message %d', message.msg_id)\n        state = self._pending_state.pop(message.msg_id, None)\n        if state:\n            state.future.set_result(message.obj)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhandling a state forgotten message.", "response": "async def _handle_state_forgotten(self, message):\n        \"\"\"\n        Handles both :tl:`MsgsStateReq` and :tl:`MsgResendReq` by\n        enqueuing a :tl:`MsgsStateInfo` to be sent at a later point.\n        \"\"\"\n        self._send_queue.append(RequestState(MsgsStateInfo(\n            req_msg_id=message.msg_id, info=chr(1) * len(message.obj.msg_ids)),\n            loop=self._loop))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def article(\n            self, title, description=None,\n            *, url=None, thumb=None, content=None,\n            id=None, text=None, parse_mode=(), link_preview=True,\n            geo=None, period=60, contact=None, game=False, buttons=None\n    ):\n        \"\"\"\n        Creates new inline result of article type.\n\n        Args:\n            title (`str`):\n                The title to be shown for this result.\n\n            description (`str`, optional):\n                Further explanation of what this result means.\n\n            url (`str`, optional):\n                The URL to be shown for this result.\n\n            thumb (:tl:`InputWebDocument`, optional):\n                The thumbnail to be shown for this result.\n                For now it has to be a :tl:`InputWebDocument` if present.\n\n            content (:tl:`InputWebDocument`, optional):\n                The content to be shown for this result.\n                For now it has to be a :tl:`InputWebDocument` if present.\n        \"\"\"\n        # TODO Does 'article' work always?\n        # article, photo, gif, mpeg4_gif, video, audio,\n        # voice, document, location, venue, contact, game\n        result = types.InputBotInlineResult(\n            id=id or '',\n            type='article',\n            send_message=await self._message(\n                text=text, parse_mode=parse_mode, link_preview=link_preview,\n                geo=geo, period=period,\n                contact=contact,\n                game=game,\n                buttons=buttons\n            ),\n            title=title,\n            description=description,\n            url=url,\n            thumb=thumb,\n            content=content\n        )\n        if id is None:\n            result.id = hashlib.sha256(bytes(result)).hexdigest()\n\n        return result", "response": "Create an inline result of article type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def photo(\n            self, file, *, id=None,\n            text=None, parse_mode=(), link_preview=True,\n            geo=None, period=60, contact=None, game=False, buttons=None\n    ):\n        \"\"\"\n        Creates a new inline result of photo type.\n\n        Args:\n            file (`obj`, optional):\n                Same as ``file`` for `client.send_file\n                <telethon.client.uploads.UploadMethods.send_file>`.\n        \"\"\"\n        try:\n            fh = utils.get_input_photo(file)\n        except TypeError:\n            fh = await self._client.upload_file(file, use_cache=types.InputPhoto)\n\n        if not isinstance(fh, types.InputPhoto):\n            r = await self._client(functions.messages.UploadMediaRequest(\n                types.InputPeerSelf(), media=types.InputMediaUploadedPhoto(fh)\n            ))\n            fh = utils.get_input_photo(r.photo)\n\n        result = types.InputBotInlineResultPhoto(\n            id=id or '',\n            type='photo',\n            photo=fh,\n            send_message=await self._message(\n                text=text or '',\n                parse_mode=parse_mode,\n                link_preview=link_preview,\n                geo=geo,\n                period=period,\n                contact=contact,\n                game=game,\n                buttons=buttons\n            )\n        )\n        if id is None:\n            result.id = hashlib.sha256(bytes(result)).hexdigest()\n\n        return result", "response": "Creates a new inline result of photo type."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def document(\n            self, file, title=None, *, description=None, type=None,\n            mime_type=None, attributes=None, force_document=False,\n            voice_note=False, video_note=False, use_cache=True, id=None,\n            text=None, parse_mode=(), link_preview=True,\n            geo=None, period=60, contact=None, game=False, buttons=None\n    ):\n        \"\"\"\n        Creates a new inline result of document type.\n\n        `use_cache`, `mime_type`, `attributes`, `force_document`,\n        `voice_note` and `video_note` are described in `client.send_file\n        <telethon.client.uploads.UploadMethods.send_file>`.\n\n        Args:\n            file (`obj`):\n                Same as ``file`` for `client.send_file\n                <telethon.client.uploads.UploadMethods.send_file>`.\n\n            title (`str`, optional):\n                The title to be shown for this result.\n\n            description (`str`, optional):\n                Further explanation of what this result means.\n\n            type (`str`, optional):\n                The type of the document. May be one of: photo, gif,\n                mpeg4_gif, video, audio, voice, document, sticker.\n\n                See \"Type of the result\" in https://core.telegram.org/bots/api.\n        \"\"\"\n        if type is None:\n            if voice_note:\n                type = 'voice'\n            else:\n                type = 'document'\n\n        try:\n            fh = utils.get_input_document(file)\n        except TypeError:\n            use_cache = types.InputDocument if use_cache else None\n            fh = await self._client.upload_file(file, use_cache=use_cache)\n\n        if not isinstance(fh, types.InputDocument):\n            attributes, mime_type = utils.get_attributes(\n                file,\n                mime_type=mime_type,\n                attributes=attributes,\n                force_document=force_document,\n                voice_note=voice_note,\n                video_note=video_note\n            )\n            r = await self._client(functions.messages.UploadMediaRequest(\n                types.InputPeerSelf(), media=types.InputMediaUploadedDocument(\n                    fh,\n                    mime_type=mime_type,\n                    attributes=attributes,\n                    nosound_video=None,\n                    thumb=None\n            )))\n            fh = utils.get_input_document(r.document)\n\n        result = types.InputBotInlineResultDocument(\n            id=id or '',\n            type=type,\n            document=fh,\n            send_message=await self._message(\n                # Empty string for text if there's media but text is None.\n                # We may want to display a document but send text; however\n                # default to sending the media (without text, i.e. stickers).\n                text=text or '',\n                parse_mode=parse_mode,\n                link_preview=link_preview,\n                geo=geo,\n                period=period,\n                contact=contact,\n                game=game,\n                buttons=buttons\n            ),\n            title=title,\n            description=description\n        )\n        if id is None:\n            result.id = hashlib.sha256(bytes(result)).hexdigest()\n\n        return result", "response": "Creates a new inline result of a document."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new inline result of game type.", "response": "async def game(\n            self, short_name, *, id=None,\n            text=None, parse_mode=(), link_preview=True,\n            geo=None, period=60, contact=None, game=False, buttons=None\n    ):\n        \"\"\"\n        Creates a new inline result of game type.\n\n        Args:\n            short_name (`str`):\n                The short name of the game to use.\n        \"\"\"\n        result = types.InputBotInlineResultGame(\n            id=id or '',\n            short_name=short_name,\n            send_message=await self._message(\n                text=text, parse_mode=parse_mode, link_preview=link_preview,\n                geo=geo, period=period,\n                contact=contact,\n                game=game,\n                buttons=buttons\n            )\n        )\n        if id is None:\n            result.id = hashlib.sha256(bytes(result)).hexdigest()\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a message to this dialog.", "response": "async def send_message(self, *args, **kwargs):\n        \"\"\"\n        Sends a message to this dialog. This is just a wrapper around\n        ``client.send_message(dialog.input_entity, *args, **kwargs)``.\n        \"\"\"\n        return await self._client.send_message(\n            self.input_entity, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def delete(self):\n        if self.is_channel:\n            await self._client(functions.channels.LeaveChannelRequest(\n                self.input_entity))\n        else:\n            if self.is_group:\n                await self._client(functions.messages.DeleteChatUserRequest(\n                    self.entity.id, types.InputPeerSelf()))\n            await self._client(functions.messages.DeleteHistoryRequest(\n                self.input_entity, 0))", "response": "Deletes the dialog from the dialog list."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_int(self, signed=True):\n        return int.from_bytes(self.read(4), byteorder='little', signed=signed)", "response": "Reads an integer ( 4 bytes ) value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread a long integer ( 8 bytes ) value.", "response": "def read_long(self, signed=True):\n        \"\"\"Reads a long integer (8 bytes) value.\"\"\"\n        return int.from_bytes(self.read(8), byteorder='little', signed=signed)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread a n - bits long integer value.", "response": "def read_large_int(self, bits, signed=True):\n        \"\"\"Reads a n-bits long integer value.\"\"\"\n        return int.from_bytes(\n            self.read(bits // 8), byteorder='little', signed=signed)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read(self, length=None):\n        if length is None:\n            return self.reader.read()\n\n        result = self.reader.read(length)\n        if len(result) != length:\n            raise BufferError(\n                'No more data left to read (need {}, got {}: {}); last read {}'\n                .format(length, len(result), repr(result), repr(self._last))\n            )\n\n        self._last = result\n        return result", "response": "Read the given amount of bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads a Telegram - encoded byte array without the need of the need of the need of the need of the of specifying its length.", "response": "def tgread_bytes(self):\n        \"\"\"\n        Reads a Telegram-encoded byte array, without the need of\n        specifying its length.\n        \"\"\"\n        first_byte = self.read_byte()\n        if first_byte == 254:\n            length = self.read_byte() | (self.read_byte() << 8) | (\n                self.read_byte() << 16)\n            padding = length % 4\n        else:\n            length = first_byte\n            padding = (length + 1) % 4\n\n        data = self.read(length)\n        if padding > 0:\n            padding = 4 - padding\n            self.read(padding)\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread a Telegram boolean value.", "response": "def tgread_bool(self):\n        \"\"\"Reads a Telegram boolean value.\"\"\"\n        value = self.read_int(signed=False)\n        if value == 0x997275b5:  # boolTrue\n            return True\n        elif value == 0xbc799737:  # boolFalse\n            return False\n        else:\n            raise RuntimeError('Invalid boolean code {}'.format(hex(value)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tgread_date(self):\n        value = self.read_int()\n        if value == 0:\n            return None\n        else:\n            return datetime.fromtimestamp(value, tz=timezone.utc)", "response": "Reads and converts Unix time to Python datetime object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tgread_object(self):\n        constructor_id = self.read_int(signed=False)\n        clazz = tlobjects.get(constructor_id, None)\n        if clazz is None:\n            # The class was None, but there's still a\n            # chance of it being a manually parsed value like bool!\n            value = constructor_id\n            if value == 0x997275b5:  # boolTrue\n                return True\n            elif value == 0xbc799737:  # boolFalse\n                return False\n            elif value == 0x1cb5c415:  # Vector\n                return [self.tgread_object() for _ in range(self.read_int())]\n\n            clazz = core_objects.get(constructor_id, None)\n            if clazz is None:\n                # If there was still no luck, give up\n                self.seek(-4)  # Go back\n                pos = self.tell_position()\n                error = TypeNotFoundError(constructor_id, self.read())\n                self.set_position(pos)\n                raise error\n\n        return clazz.from_reader(self)", "response": "Reads a Telegram object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread a list of Telegram objects.", "response": "def tgread_vector(self):\n        \"\"\"Reads a vector (a list) of Telegram objects.\"\"\"\n        if 0x1cb5c415 != self.read_int(signed=False):\n            raise RuntimeError('Invalid constructor code, vector was expected')\n\n        count = self.read_int()\n        return [self.tgread_object() for _ in range(count)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef factorize(cls, pq):\n        if pq % 2 == 0:\n            return 2, pq // 2\n\n        y, c, m = randint(1, pq - 1), randint(1, pq - 1), randint(1, pq - 1)\n        g = r = q = 1\n        x = ys = 0\n\n        while g == 1:\n            x = y\n            for i in range(r):\n                y = (pow(y, 2, pq) + c) % pq\n\n            k = 0\n            while k < r and g == 1:\n                ys = y\n                for i in range(min(m, r - k)):\n                    y = (pow(y, 2, pq) + c) % pq\n                    q = q * (abs(x - y)) % pq\n\n                g = cls.gcd(q, pq)\n                k += m\n\n            r *= 2\n\n        if g == pq:\n            while True:\n                ys = (pow(ys, 2, pq) + c) % pq\n                g = cls.gcd(abs(x - ys), pq)\n                if g > 1:\n                    break\n\n        p, q = g, pq // g\n        return (p, q) if p < q else (q, p)", "response": "Factorizes the given large integer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef gcd(a, b):\n        while b:\n            a, b = b, a % b\n\n        return a", "response": "Calculates the Greatest Common Divisor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a proxy object over the current :ref:`TelegramClient` through which making requests will use :tl:`InvokeWithTakeoutRequest` to wrap them. In other words, returns the current client modified so that requests are done as a takeout: >>> from telethon.sync import TelegramClient >>> >>> with TelegramClient(...) as client: >>> with client.takeout() as takeout: >>> client.get_messages('me') # normal call >>> takeout.get_messages('me') # wrapped through takeout Some of the calls made through the takeout session will have lower flood limits. This is useful if you want to export the data from conversations or mass-download media, since the rate limits will be lower. Only some requests will be affected, and you will need to adjust the `wait_time` of methods like `client.iter_messages <telethon.client.messages.MessageMethods.iter_messages>`. By default, all parameters are ``None``, and you need to enable those you plan to use by setting them to either ``True`` or ``False``. You should ``except errors.TakeoutInitDelayError as e``, since this exception will raise depending on the condition of the session. You can then access ``e.seconds`` to know how long you should wait for before calling the method again. There's also a `success` property available in the takeout proxy object, so from the `with` body you can set the boolean result that will be sent back to Telegram. But if it's left ``None`` as by default, then the action is based on the `finalize` parameter. If it's ``True`` then the takeout will be finished, and if no exception occurred during it, then ``True`` will be considered as a result. Otherwise, the takeout will not be finished and its ID will be preserved for future usage as `client.session.takeout_id <telethon.sessions.abstract.Session.takeout_id>`. Args: contacts (`bool`): Set to ``True`` if you plan on downloading contacts. users (`bool`): Set to ``True`` if you plan on downloading information from users and their private conversations with you. chats (`bool`): Set to ``True`` if you plan on downloading information from small group chats, such as messages and media. megagroups (`bool`): Set to ``True`` if you plan on downloading information from megagroups (channels), such as messages and media. channels (`bool`): Set to ``True`` if you plan on downloading information from broadcast channels, such as messages and media. files (`bool`): Set to ``True`` if you plan on downloading media and you don't only wish to export messages. max_file_size (`int`): The maximum file size, in bytes, that you plan to download for each message with media.", "response": "def takeout(\n            self, finalize=True, *, contacts=None, users=None, chats=None,\n            megagroups=None, channels=None, files=None, max_file_size=None):\n        \"\"\"\n        Creates a proxy object over the current :ref:`TelegramClient` through\n        which making requests will use :tl:`InvokeWithTakeoutRequest` to wrap\n        them. In other words, returns the current client modified so that\n        requests are done as a takeout:\n\n        >>> from telethon.sync import TelegramClient\n        >>>\n        >>> with TelegramClient(...) as client:\n        >>>     with client.takeout() as takeout:\n        >>>         client.get_messages('me')  # normal call\n        >>>         takeout.get_messages('me')  # wrapped through takeout\n\n        Some of the calls made through the takeout session will have lower\n        flood limits. This is useful if you want to export the data from\n        conversations or mass-download media, since the rate limits will\n        be lower. Only some requests will be affected, and you will need\n        to adjust the `wait_time` of methods like `client.iter_messages\n        <telethon.client.messages.MessageMethods.iter_messages>`.\n\n        By default, all parameters are ``None``, and you need to enable those\n        you plan to use by setting them to either ``True`` or ``False``.\n\n        You should ``except errors.TakeoutInitDelayError as e``, since this\n        exception will raise depending on the condition of the session. You\n        can then access ``e.seconds`` to know how long you should wait for\n        before calling the method again.\n\n        There's also a `success` property available in the takeout proxy\n        object, so from the `with` body you can set the boolean result that\n        will be sent back to Telegram. But if it's left ``None`` as by\n        default, then the action is based on the `finalize` parameter. If\n        it's ``True`` then the takeout will be finished, and if no exception\n        occurred during it, then ``True`` will be considered as a result.\n        Otherwise, the takeout will not be finished and its ID will be\n        preserved for future usage as `client.session.takeout_id\n        <telethon.sessions.abstract.Session.takeout_id>`.\n\n        Args:\n            contacts (`bool`):\n                Set to ``True`` if you plan on downloading contacts.\n\n            users (`bool`):\n                Set to ``True`` if you plan on downloading information\n                from users and their private conversations with you.\n\n            chats (`bool`):\n                Set to ``True`` if you plan on downloading information\n                from small group chats, such as messages and media.\n\n            megagroups (`bool`):\n                Set to ``True`` if you plan on downloading information\n                from megagroups (channels), such as messages and media.\n\n            channels (`bool`):\n                Set to ``True`` if you plan on downloading information\n                from broadcast channels, such as messages and media.\n\n            files (`bool`):\n                Set to ``True`` if you plan on downloading media and\n                you don't only wish to export messages.\n\n            max_file_size (`int`):\n                The maximum file size, in bytes, that you plan\n                to download for each message with media.\n        \"\"\"\n        request_kwargs = dict(\n            contacts=contacts,\n            message_users=users,\n            message_chats=chats,\n            message_megagroups=megagroups,\n            message_channels=channels,\n            files=files,\n            file_max_size=max_file_size\n        )\n        arg_specified = (arg is not None for arg in request_kwargs.values())\n\n        if self.session.takeout_id is None or any(arg_specified):\n            request = functions.account.InitTakeoutSessionRequest(\n                **request_kwargs)\n        else:\n            request = None\n\n        return _TakeoutClient(finalize, self, request)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nends a takeout with specified result sent back to Telegram.", "response": "async def end_takeout(self, success):\n        \"\"\"\n        Finishes a takeout, with specified result sent back to Telegram.\n\n        Returns:\n            ``True`` if the operation was successful, ``False`` otherwise.\n        \"\"\"\n        try:\n            async with _TakeoutClient(True, self, None) as takeout:\n                takeout.success = success\n        except ValueError:\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts all the methods in the given types into synchronous.", "response": "def syncify(*types):\n    \"\"\"\n    Converts all the methods in the given types (class definitions)\n    into synchronous, which return either the coroutine or the result\n    based on whether ``asyncio's`` event loop is running.\n    \"\"\"\n    # Our asynchronous generators all are `RequestIter`, which already\n    # provide a synchronous iterator variant, so we don't need to worry\n    # about asyncgenfunction's here.\n    for t in types:\n        for name in dir(t):\n            if not name.startswith('_') or name == '__call__':\n                if inspect.iscoroutinefunction(getattr(t, name)):\n                    _syncify_wrap(t, name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def get(self):\n        if not self._deque:\n            self._ready.clear()\n            await self._ready.wait()\n\n        buffer = io.BytesIO()\n        batch = []\n        size = 0\n\n        # Fill a new batch to return while the size is small enough,\n        # as long as we don't exceed the maximum length of messages.\n        while self._deque and len(batch) <= MessageContainer.MAXIMUM_LENGTH:\n            state = self._deque.popleft()\n            size += len(state.data) + TLMessage.SIZE_OVERHEAD\n\n            if size <= MessageContainer.MAXIMUM_SIZE:\n                state.msg_id = self._state.write_data_as_message(\n                    buffer, state.data, isinstance(state.request, TLRequest),\n                    after_id=state.after.msg_id if state.after else None\n                )\n                batch.append(state)\n                self._log.debug('Assigned msg_id = %d to %s (%x)',\n                                state.msg_id, state.request.__class__.__name__,\n                                id(state.request))\n                continue\n\n            if batch:\n                # Put the item back since it can't be sent in this batch\n                self._deque.appendleft(state)\n                break\n\n            # If a single message exceeds the maximum size, then the\n            # message payload cannot be sent. Telegram would forcibly\n            # close the connection; message would never be confirmed.\n            #\n            # We don't put the item back because it can never be sent.\n            # If we did, we would loop again and reach this same path.\n            # Setting the exception twice results in `InvalidStateError`\n            # and this method should never return with error, which we\n            # really want to avoid.\n            self._log.warning(\n                'Message payload for %s is too long (%d) and cannot be sent',\n                state.request.__class__.__name__, len(state.data)\n            )\n            state.future.set_exception(\n                ValueError('Request payload is too big'))\n\n            size = 0\n            continue\n\n        if not batch:\n            return None, None\n\n        if len(batch) > 1:\n            # Inlined code to pack several messages into a container\n            data = struct.pack(\n                '<Ii', MessageContainer.CONSTRUCTOR_ID, len(batch)\n            ) + buffer.getvalue()\n            buffer = io.BytesIO()\n            container_id = self._state.write_data_as_message(\n                buffer, data, content_related=False\n            )\n            for s in batch:\n                s.container_id = container_id\n\n        data = buffer.getvalue()\n        return batch, data", "response": "Get the next available item from the queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nestablishes a connection with the server.", "response": "async def connect(self, timeout=None, ssl=None):\n        \"\"\"\n        Establishes a connection with the server.\n        \"\"\"\n        await self._connect(timeout=timeout, ssl=ssl)\n        self._connected = True\n\n        self._send_task = self._loop.create_task(self._send_loop())\n        self._recv_task = self._loop.create_task(self._recv_loop())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndisconnecting from the server and clears pending outgoing and incoming messages.", "response": "async def disconnect(self):\n        \"\"\"\n        Disconnects from the server, and clears\n        pending outgoing and incoming messages.\n        \"\"\"\n        self._connected = False\n\n        await helpers._cancel(\n            self._log,\n            send_task=self._send_task,\n            recv_task=self._recv_task\n        )\n\n        if self._writer:\n            self._writer.close()\n            if sys.version_info >= (3, 7):\n                await self._writer.wait_closed()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending a packet through this connection mode.", "response": "def send(self, data):\n        \"\"\"\n        Sends a packet of data through this connection mode.\n\n        This method returns a coroutine.\n        \"\"\"\n        if not self._connected:\n            raise ConnectionError('Not connected')\n\n        return self._send_queue.put(data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreceive a packet of data through this connection mode. This method returns a coroutine which will return the value of the first available item in the receive queue.", "response": "async def recv(self):\n        \"\"\"\n        Receives a packet of data through this connection mode.\n\n        This method returns a coroutine.\n        \"\"\"\n        while self._connected:\n            result = await self._recv_queue.get()\n            if result:  # None = sentinel value = keep trying\n                return result\n\n        raise ConnectionError('Not connected')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def _send_loop(self):\n        try:\n            while self._connected:\n                self._send(await self._send_queue.get())\n                await self._writer.drain()\n        except asyncio.CancelledError:\n            pass\n        except Exception as e:\n            if isinstance(e, IOError):\n                self._log.info('The server closed the connection while sending')\n            else:\n                self._log.exception('Unexpected exception in the send loop')\n\n            await self.disconnect()", "response": "This loop is constantly popping items off the send queue and sending them to the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def _recv_loop(self):\n        while self._connected:\n            try:\n                data = await self._recv()\n            except asyncio.CancelledError:\n                break\n            except Exception as e:\n                if isinstance(e, (IOError, asyncio.IncompleteReadError)):\n                    msg = 'The server closed the connection'\n                    self._log.info(msg)\n                elif isinstance(e, InvalidChecksumError):\n                    msg = 'The server response had an invalid checksum'\n                    self._log.info(msg)\n                else:\n                    msg = 'Unexpected exception in the receive loop'\n                    self._log.exception(msg)\n\n                await self.disconnect()\n\n                # Add a sentinel value to unstuck recv\n                if self._recv_queue.empty():\n                    self._recv_queue.put_nowait(None)\n\n                break\n\n            try:\n                await self._recv_queue.put(data)\n            except asyncio.CancelledError:\n                break", "response": "This loop is constantly putting items on the receive queue as they re read."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _init_conn(self):\n        if self._codec.tag:\n            self._writer.write(self._codec.tag)", "response": "Initialize the connection to Telegram."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns gzipped bytes if the gzipped data is smaller than the original byte array.", "response": "def gzip_if_smaller(content_related, data):\n        \"\"\"Calls bytes(request), and based on a certain threshold,\n           optionally gzips the resulting data. If the gzipped data is\n           smaller than the original byte array, this is returned instead.\n\n           Note that this only applies to content related requests.\n        \"\"\"\n        if content_related and len(data) > 512:\n            gzipped = bytes(GzipPacked(data))\n            return gzipped if len(gzipped) < len(data) else data\n        else:\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sorted_args(self):\n        return sorted(self.args,\n                      key=lambda x: x.is_flag or x.can_be_inferred)", "response": "Returns the arguments properly sorted and ready to plug - in\n           into a Python s method header."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nasserts that the connection is open and returns a cursor", "response": "def _cursor(self):\n        \"\"\"Asserts that the connection is open and returns a cursor\"\"\"\n        if self._conn is None:\n            self._conn = sqlite3.connect(self.filename,\n                                         check_same_thread=False)\n        return self._conn.cursor()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _execute(self, stmt, *values):\n        c = self._cursor()\n        try:\n            return c.execute(stmt, values).fetchone()\n        finally:\n            c.close()", "response": "Executes a statement and returns the first row of the result set."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef close(self):\n        if self.filename != ':memory:':\n            if self._conn is not None:\n                self._conn.commit()\n                self._conn.close()\n                self._conn = None", "response": "Closes the connection unless we re working in - memory"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete the current session file", "response": "def delete(self):\n        \"\"\"Deletes the current session file\"\"\"\n        if self.filename == ':memory:':\n            return True\n        try:\n            os.remove(self.filename)\n            return True\n        except OSError:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist all the users who have ever connected using this client and never logged out", "response": "def list_sessions(cls):\n        \"\"\"Lists all the sessions of the users who have ever connected\n           using this client and never logged out\n        \"\"\"\n        return [os.path.splitext(os.path.basename(f))[0]\n                for f in os.listdir('.') if f.endswith(EXTENSION)]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_entities(self, tlo):\n        if not self.save_entities:\n            return\n\n        rows = self._entities_to_rows(tlo)\n        if not rows:\n            return\n\n        c = self._cursor()\n        try:\n            c.executemany(\n                'insert or replace into entities values (?,?,?,?,?)', rows)\n        finally:\n            c.close()", "response": "Processes all the found entities on the given TLObject. Returns True if new input entities were added."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the input CSV file with columns method usability and errors and yields MethodInfo instances as a result.", "response": "def parse_methods(csv_file, errors_dict):\n    \"\"\"\n    Parses the input CSV file with columns (method, usability, errors)\n    and yields `MethodInfo` instances as a result.\n    \"\"\"\n    with csv_file.open(newline='') as f:\n        f = csv.reader(f)\n        next(f, None)  # header\n        for line, (method, usability, errors) in enumerate(f, start=2):\n            try:\n                errors = [errors_dict[x] for x in errors.split()]\n            except KeyError:\n                raise ValueError('Method {} references unknown errors {}'\n                                 .format(method, errors)) from None\n\n            yield MethodInfo(method, usability, errors)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndisconnecting from Telegram. If the event loop is already running, this method returns a coroutine that you should await on your own code; otherwise the loop is ran until said coroutine completes.", "response": "def disconnect(self):\n        \"\"\"\n        Disconnects from Telegram.\n\n        If the event loop is already running, this method returns a\n        coroutine that you should await on your own code; otherwise\n        the loop is ran until said coroutine completes.\n        \"\"\"\n        if self._loop.is_running():\n            return self._disconnect_coro()\n        else:\n            self._loop.run_until_complete(self._disconnect_coro())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def _disconnect(self):\n        await self._sender.disconnect()\n        await helpers._cancel(self._log[__name__],\n                              updates_handle=self._updates_handle)", "response": "Disconnects from the client and cancels all pending updates."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nswitching the current connection to the new data center.", "response": "async def _switch_dc(self, new_dc):\n        \"\"\"\n        Permanently switches the current connection to the new data center.\n        \"\"\"\n        self._log[__name__].info('Reconnecting to new data center %s', new_dc)\n        dc = await self._get_dc(new_dc)\n\n        self.session.set_dc(dc.id, dc.ip_address, dc.port)\n        # auth_key's are associated with a server, which has now changed\n        # so it's not valid anymore. Set to None to force recreating it.\n        self._sender.auth_key.key = None\n        self.session.auth_key = None\n        self.session.save()\n        await self._disconnect()\n        return await self.connect()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _auth_key_callback(self, auth_key):\n        self.session.auth_key = auth_key\n        self.session.save()", "response": "Callback from the sender whenever we need to generate a new authorization key."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the Data Center ( DC ) associated to dc_id", "response": "async def _get_dc(self, dc_id, cdn=False):\n        \"\"\"Gets the Data Center (DC) associated to 'dc_id'\"\"\"\n        cls = self.__class__\n        if not cls._config:\n            cls._config = await self(functions.help.GetConfigRequest())\n\n        if cdn and not self._cdn_config:\n            cls._cdn_config = await self(functions.help.GetCdnConfigRequest())\n            for pk in cls._cdn_config.public_keys:\n                rsa.add_key(pk.public_key)\n\n        return next(\n            dc for dc in cls._config.dc_options\n            if dc.id == dc_id\n            and bool(dc.ipv6) == self._use_ipv6 and bool(dc.cdn) == cdn\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def _create_exported_sender(self, dc_id):\n        # Thanks badoualy/kotlogram on /telegram/api/DefaultTelegramClient.kt\n        # for clearly showing how to export the authorization\n        dc = await self._get_dc(dc_id)\n        # Can't reuse self._sender._connection as it has its own seqno.\n        #\n        # If one were to do that, Telegram would reset the connection\n        # with no further clues.\n        sender = MTProtoSender(None, self._loop, loggers=self._log)\n        await sender.connect(self._connection(\n            dc.ip_address,\n            dc.port,\n            dc.id,\n            loop=self._loop,\n            loggers=self._log,\n            proxy=self._proxy\n        ))\n        self._log[__name__].info('Exporting authorization for data center %s',\n                                 dc)\n        auth = await self(functions.auth.ExportAuthorizationRequest(dc_id))\n        req = self._init_with(functions.auth.ImportAuthorizationRequest(\n            id=auth.id, bytes=auth.bytes\n        ))\n        await sender.send(req)\n        return sender", "response": "Creates a new exported MTProtoSender for the given dc_id and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def _borrow_exported_sender(self, dc_id):\n        async with self._borrow_sender_lock:\n            n, sender = self._borrowed_senders.get(dc_id, (0, None))\n            if not sender:\n                sender = await self._create_exported_sender(dc_id)\n                sender.dc_id = dc_id\n            elif not n:\n                dc = await self._get_dc(dc_id)\n                await sender.connect(self._connection(\n                    dc.ip_address,\n                    dc.port,\n                    dc.id,\n                    loop=self._loop,\n                    loggers=self._log,\n                    proxy=self._proxy\n                ))\n\n            self._borrowed_senders[dc_id] = (n + 1, sender)\n\n        return sender", "response": "Borrows a connected MTProtoSender for the given dc_id. Returns a new instance of the appropriate class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a borrowed exported sender.", "response": "async def _return_exported_sender(self, sender):\n        \"\"\"\n        Returns a borrowed exported sender. If all borrows have\n        been returned, the sender is cleanly disconnected.\n        \"\"\"\n        async with self._borrow_sender_lock:\n            dc_id = sender.dc_id\n            n, _ = self._borrowed_senders[dc_id]\n            n -= 1\n            self._borrowed_senders[dc_id] = (n, sender)\n            if not n:\n                self._log[__name__].info(\n                    'Disconnecting borrowed sender for DC %d', dc_id)\n                await sender.disconnect()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def _get_cdn_client(self, cdn_redirect):\n        # TODO Implement\n        raise NotImplementedError\n        session = self._exported_sessions.get(cdn_redirect.dc_id)\n        if not session:\n            dc = await self._get_dc(cdn_redirect.dc_id, cdn=True)\n            session = self.session.clone()\n            await session.set_dc(dc.id, dc.ip_address, dc.port)\n            self._exported_sessions[cdn_redirect.dc_id] = session\n\n        self._log[__name__].info('Creating new CDN client')\n        client = TelegramBareClient(\n            session, self.api_id, self.api_hash,\n            proxy=self._sender.connection.conn.proxy,\n            timeout=self._sender.connection.get_timeout()\n        )\n\n        # This will make use of the new RSA keys for this specific CDN.\n        #\n        # We won't be calling GetConfigRequest because it's only called\n        # when needed by ._get_dc, and also it's static so it's likely\n        # set already. Avoid invoking non-CDN methods by not syncing updates.\n        client.connect(_sync_updates=False)\n        return client", "response": "Get a new CDN client."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nturns the given iterable into chunks of the specified size.", "response": "def chunks(iterable, size=100):\n    \"\"\"\n    Turns the given iterable into chunks of the specified size,\n    which is 100 by default since that's what Telegram uses the most.\n    \"\"\"\n    it = iter(iterable)\n    size -= 1\n    for head in it:\n        yield itertools.chain([head], itertools.islice(it, size))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the display name for the given entity. Returns an empty string if the given entity is not a user or chat or channel.", "response": "def get_display_name(entity):\n    \"\"\"\n    Gets the display name for the given entity, if it's an :tl:`User`,\n    :tl:`Chat` or :tl:`Channel`. Returns an empty string otherwise.\n    \"\"\"\n    if isinstance(entity, types.User):\n        if entity.last_name and entity.first_name:\n            return '{} {}'.format(entity.first_name, entity.last_name)\n        elif entity.first_name:\n            return entity.first_name\n        elif entity.last_name:\n            return entity.last_name\n        else:\n            return ''\n\n    elif isinstance(entity, (types.Chat, types.Channel)):\n        return entity.title\n\n    return ''"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_extension(media):\n\n    # Photos are always compressed as .jpg by Telegram\n    if isinstance(media, (types.UserProfilePhoto,\n                          types.ChatPhoto, types.MessageMediaPhoto)):\n        return '.jpg'\n\n    # Documents will come with a mime type\n    if isinstance(media, types.MessageMediaDocument):\n        media = media.document\n    if isinstance(media, (\n            types.Document, types.WebDocument, types.WebDocumentNoProxy)):\n        if media.mime_type == 'application/octet-stream':\n            # Octet stream are just bytes, which have no default extension\n            return ''\n        else:\n            return guess_extension(media.mime_type) or ''\n\n    return ''", "response": "Gets the corresponding extension for any Telegram media."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the input peer for the given entity.", "response": "def get_input_peer(entity, allow_self=True, check_hash=True):\n    \"\"\"\n    Gets the input peer for the given \"entity\" (user, chat or channel).\n\n    A ``TypeError`` is raised if the given entity isn't a supported type\n    or if ``check_hash is True`` but the entity's ``access_hash is None``.\n\n    Note that ``check_hash`` **is ignored** if an input peer is already\n    passed since in that case we assume the user knows what they're doing.\n    This is key to getting entities by explicitly passing ``hash = 0``.\n    \"\"\"\n    try:\n        if entity.SUBCLASS_OF_ID == 0xc91c90b6:  # crc32(b'InputPeer')\n            return entity\n    except AttributeError:\n        # e.g. custom.Dialog (can't cyclic import).\n        if allow_self and hasattr(entity, 'input_entity'):\n            return entity.input_entity\n        elif hasattr(entity, 'entity'):\n            return get_input_peer(entity.entity)\n        else:\n            _raise_cast_fail(entity, 'InputPeer')\n\n    if isinstance(entity, types.User):\n        if entity.is_self and allow_self:\n            return types.InputPeerSelf()\n        elif entity.access_hash is not None or not check_hash:\n            return types.InputPeerUser(entity.id, entity.access_hash)\n        else:\n            raise TypeError('User without access_hash cannot be input')\n\n    if isinstance(entity, (types.Chat, types.ChatEmpty, types.ChatForbidden)):\n        return types.InputPeerChat(entity.id)\n\n    if isinstance(entity, (types.Channel, types.ChannelForbidden)):\n        if entity.access_hash is not None or not check_hash:\n            return types.InputPeerChannel(entity.id, entity.access_hash)\n        else:\n            raise TypeError('Channel without access_hash cannot be input')\n\n    if isinstance(entity, types.InputUser):\n        return types.InputPeerUser(entity.user_id, entity.access_hash)\n\n    if isinstance(entity, types.InputChannel):\n        return types.InputPeerChannel(entity.channel_id, entity.access_hash)\n\n    if isinstance(entity, types.InputUserSelf):\n        return types.InputPeerSelf()\n\n    if isinstance(entity, types.UserEmpty):\n        return types.InputPeerEmpty()\n\n    if isinstance(entity, types.UserFull):\n        return get_input_peer(entity.user)\n\n    if isinstance(entity, types.ChatFull):\n        return types.InputPeerChat(entity.id)\n\n    if isinstance(entity, types.PeerChat):\n        return types.InputPeerChat(entity.chat_id)\n\n    _raise_cast_fail(entity, 'InputPeer')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_input_channel(entity):\n    try:\n        if entity.SUBCLASS_OF_ID == 0x40f202fd:  # crc32(b'InputChannel')\n            return entity\n    except AttributeError:\n        _raise_cast_fail(entity, 'InputChannel')\n\n    if isinstance(entity, (types.Channel, types.ChannelForbidden)):\n        return types.InputChannel(entity.id, entity.access_hash or 0)\n\n    if isinstance(entity, types.InputPeerChannel):\n        return types.InputChannel(entity.channel_id, entity.access_hash)\n\n    _raise_cast_fail(entity, 'InputChannel')", "response": "Similar to get_input_peer but for InputChannel s."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_input_photo(photo):\n    try:\n        if photo.SUBCLASS_OF_ID == 0x846363e0:  # crc32(b'InputPhoto'):\n            return photo\n    except AttributeError:\n        _raise_cast_fail(photo, 'InputPhoto')\n\n    if isinstance(photo, types.photos.Photo):\n        photo = photo.photo\n\n    if isinstance(photo, types.Photo):\n        return types.InputPhoto(id=photo.id, access_hash=photo.access_hash,\n                                file_reference=photo.file_reference)\n\n    if isinstance(photo, types.PhotoEmpty):\n        return types.InputPhotoEmpty()\n\n    if isinstance(photo, types.messages.ChatFull):\n        photo = photo.full_chat\n    if isinstance(photo, types.ChannelFull):\n        return get_input_photo(photo.chat_photo)\n    elif isinstance(photo, types.UserFull):\n        return get_input_photo(photo.profile_photo)\n    elif isinstance(photo, (types.Channel, types.Chat, types.User)):\n        return get_input_photo(photo.photo)\n\n    if isinstance(photo, (types.UserEmpty, types.ChatEmpty,\n                          types.ChatForbidden, types.ChannelForbidden)):\n        return types.InputPhotoEmpty()\n\n    _raise_cast_fail(photo, 'InputPhoto')", "response": "Similar to get_input_peer but for photos"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_input_chat_photo(photo):\n    try:\n        if photo.SUBCLASS_OF_ID == 0xd4eb2d74:  # crc32(b'InputChatPhoto')\n            return photo\n        elif photo.SUBCLASS_OF_ID == 0xe7655f1f:  # crc32(b'InputFile'):\n            return types.InputChatUploadedPhoto(photo)\n    except AttributeError:\n        _raise_cast_fail(photo, 'InputChatPhoto')\n\n    photo = get_input_photo(photo)\n    if isinstance(photo, types.InputPhoto):\n        return types.InputChatPhoto(photo)\n    elif isinstance(photo, types.InputPhotoEmpty):\n        return types.InputChatPhotoEmpty()\n\n    _raise_cast_fail(photo, 'InputChatPhoto')", "response": "Similar to get_input_peer but for chat photos"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_input_geo(geo):\n    try:\n        if geo.SUBCLASS_OF_ID == 0x430d225:  # crc32(b'InputGeoPoint'):\n            return geo\n    except AttributeError:\n        _raise_cast_fail(geo, 'InputGeoPoint')\n\n    if isinstance(geo, types.GeoPoint):\n        return types.InputGeoPoint(lat=geo.lat, long=geo.long)\n\n    if isinstance(geo, types.GeoPointEmpty):\n        return types.InputGeoPointEmpty()\n\n    if isinstance(geo, types.MessageMediaGeo):\n        return get_input_geo(geo.geo)\n\n    if isinstance(geo, types.Message):\n        return get_input_geo(geo.media)\n\n    _raise_cast_fail(geo, 'InputGeoPoint')", "response": "Similar to get_input_peer but for GeoPoints"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_input_media(\n        media, *,\n        is_photo=False, attributes=None, force_document=False,\n        voice_note=False, video_note=False, supports_streaming=False\n):\n    \"\"\"\n    Similar to :meth:`get_input_peer`, but for media.\n\n    If the media is :tl:`InputFile` and ``is_photo`` is known to be ``True``,\n    it will be treated as an :tl:`InputMediaUploadedPhoto`. Else, the rest\n    of parameters will indicate how to treat it.\n    \"\"\"\n    try:\n        if media.SUBCLASS_OF_ID == 0xfaf846f4:  # crc32(b'InputMedia')\n            return media\n        elif media.SUBCLASS_OF_ID == 0x846363e0:  # crc32(b'InputPhoto')\n            return types.InputMediaPhoto(media)\n        elif media.SUBCLASS_OF_ID == 0xf33fdb68:  # crc32(b'InputDocument')\n            return types.InputMediaDocument(media)\n    except AttributeError:\n        _raise_cast_fail(media, 'InputMedia')\n\n    if isinstance(media, types.MessageMediaPhoto):\n        return types.InputMediaPhoto(\n            id=get_input_photo(media.photo),\n            ttl_seconds=media.ttl_seconds\n        )\n\n    if isinstance(media, (types.Photo, types.photos.Photo, types.PhotoEmpty)):\n        return types.InputMediaPhoto(\n            id=get_input_photo(media)\n        )\n\n    if isinstance(media, types.MessageMediaDocument):\n        return types.InputMediaDocument(\n            id=get_input_document(media.document),\n            ttl_seconds=media.ttl_seconds\n        )\n\n    if isinstance(media, (types.Document, types.DocumentEmpty)):\n        return types.InputMediaDocument(\n            id=get_input_document(media)\n        )\n\n    if isinstance(media, (types.InputFile, types.InputFileBig)):\n        if is_photo:\n            return types.InputMediaUploadedPhoto(file=media)\n        else:\n            attrs, mime = get_attributes(\n                media,\n                attributes=attributes,\n                force_document=force_document,\n                voice_note=voice_note,\n                video_note=video_note,\n                supports_streaming=supports_streaming\n            )\n            return types.InputMediaUploadedDocument(\n                file=media, mime_type=mime, attributes=attrs)\n\n    if isinstance(media, types.MessageMediaGame):\n        return types.InputMediaGame(id=media.game.id)\n\n    if isinstance(media, types.MessageMediaContact):\n        return types.InputMediaContact(\n            phone_number=media.phone_number,\n            first_name=media.first_name,\n            last_name=media.last_name,\n            vcard=''\n        )\n\n    if isinstance(media, types.MessageMediaGeo):\n        return types.InputMediaGeoPoint(geo_point=get_input_geo(media.geo))\n\n    if isinstance(media, types.MessageMediaVenue):\n        return types.InputMediaVenue(\n            geo_point=get_input_geo(media.geo),\n            title=media.title,\n            address=media.address,\n            provider=media.provider,\n            venue_id=media.venue_id,\n            venue_type=''\n        )\n\n    if isinstance(media, (\n            types.MessageMediaEmpty, types.MessageMediaUnsupported,\n            types.ChatPhotoEmpty, types.UserProfilePhotoEmpty,\n            types.ChatPhoto, types.UserProfilePhoto,\n            types.FileLocationToBeDeprecated)):\n        return types.InputMediaEmpty()\n\n    if isinstance(media, types.Message):\n        return get_input_media(media.media, is_photo=is_photo)\n\n    _raise_cast_fail(media, 'InputMedia')", "response": "Returns a new InputMedia object for the given media."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_input_message(message):\n    try:\n        if isinstance(message, int):  # This case is really common too\n            return types.InputMessageID(message)\n        elif message.SUBCLASS_OF_ID == 0x54b6bcc5:  # crc32(b'InputMessage'):\n            return message\n        elif message.SUBCLASS_OF_ID == 0x790009e3:  # crc32(b'Message'):\n            return types.InputMessageID(message.id)\n    except AttributeError:\n        pass\n\n    _raise_cast_fail(message, 'InputMedia')", "response": "Similar to get_input_peer but for input messages."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_message_id(message):\n    if message is None:\n        return None\n\n    if isinstance(message, int):\n        return message\n\n    try:\n        if message.SUBCLASS_OF_ID == 0x790009e3:\n            # hex(crc32(b'Message')) = 0x790009e3\n            return message.id\n    except AttributeError:\n        pass\n\n    raise TypeError('Invalid message type: {}'.format(type(message)))", "response": "Similar to get_input_peer but for message IDs."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a list of attributes for the given file and mime type.", "response": "def get_attributes(file, *, attributes=None, mime_type=None,\n                   force_document=False, voice_note=False, video_note=False,\n                   supports_streaming=False):\n    \"\"\"\n    Get a list of attributes for the given file and\n    the mime type as a tuple ([attribute], mime_type).\n    \"\"\"\n    # Note: ``file.name`` works for :tl:`InputFile` and some `IOBase` streams\n    name = file if isinstance(file, str) else getattr(file, 'name', 'unnamed')\n    if mime_type is None:\n        mime_type = mimetypes.guess_type(name)[0]\n\n    attr_dict = {types.DocumentAttributeFilename:\n        types.DocumentAttributeFilename(os.path.basename(name))}\n\n    if is_audio(file):\n        m = _get_metadata(file)\n        if m:\n            attr_dict[types.DocumentAttributeAudio] = \\\n                types.DocumentAttributeAudio(\n                    voice=voice_note,\n                    title=m.get('title') if m.has('title') else None,\n                    performer=m.get('author') if m.has('author') else None,\n                    duration=int(m.get('duration').seconds\n                                 if m.has('duration') else 0)\n                )\n\n    if not force_document and is_video(file):\n        m = _get_metadata(file)\n        if m:\n            doc = types.DocumentAttributeVideo(\n                round_message=video_note,\n                w=m.get('width') if m.has('width') else 0,\n                h=m.get('height') if m.has('height') else 0,\n                duration=int(m.get('duration').seconds\n                             if m.has('duration') else 0),\n                supports_streaming=supports_streaming\n            )\n        else:\n            doc = types.DocumentAttributeVideo(\n                0, 1, 1, round_message=video_note,\n                supports_streaming=supports_streaming)\n\n        attr_dict[types.DocumentAttributeVideo] = doc\n\n    if voice_note:\n        if types.DocumentAttributeAudio in attr_dict:\n            attr_dict[types.DocumentAttributeAudio].voice = True\n        else:\n            attr_dict[types.DocumentAttributeAudio] = \\\n                types.DocumentAttributeAudio(0, voice=True)\n\n    # Now override the attributes if any. As we have a dict of\n    # {cls: instance}, we can override any class with the list\n    # of attributes provided by the user easily.\n    if attributes:\n        for a in attributes:\n            attr_dict[type(a)] = a\n\n    # Ensure we have a mime type, any; but it cannot be None\n    # 'The \"octet-stream\" subtype is used to indicate that a body\n    # contains arbitrary binary data.'\n    if not mime_type:\n        mime_type = 'application/octet-stream'\n\n    return list(attr_dict.values()), mime_type"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsanitizes the given parse mode into an object with the appropriate parse and unparse callable properties.", "response": "def sanitize_parse_mode(mode):\n    \"\"\"\n    Converts the given parse mode into an object with\n    ``parse`` and ``unparse`` callable properties.\n    \"\"\"\n    if not mode:\n        return None\n\n    if callable(mode):\n        class CustomMode:\n            @staticmethod\n            def unparse(text, entities):\n                raise NotImplementedError\n\n        CustomMode.parse = mode\n        return CustomMode\n    elif (all(hasattr(mode, x) for x in ('parse', 'unparse'))\n          and all(callable(x) for x in (mode.parse, mode.unparse))):\n        return mode\n    elif isinstance(mode, str):\n        try:\n            return {\n                'md': markdown,\n                'markdown': markdown,\n                'htm': html,\n                'html': html\n            }[mode.lower()]\n        except KeyError:\n            raise ValueError('Unknown parse mode {}'.format(mode))\n    else:\n        raise TypeError('Invalid parse mode type {}'.format(mode))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_input_location(location):\n    try:\n        if location.SUBCLASS_OF_ID == 0x1523d462:\n            return None, location  # crc32(b'InputFileLocation'):\n    except AttributeError:\n        _raise_cast_fail(location, 'InputFileLocation')\n\n    if isinstance(location, types.Message):\n        location = location.media\n\n    if isinstance(location, types.MessageMediaDocument):\n        location = location.document\n    elif isinstance(location, types.MessageMediaPhoto):\n        location = location.photo\n\n    if isinstance(location, types.Document):\n        return (location.dc_id, types.InputDocumentFileLocation(\n            id=location.id,\n            access_hash=location.access_hash,\n            file_reference=location.file_reference,\n            thumb_size=''  # Presumably to download one of its thumbnails\n        ))\n    elif isinstance(location, types.Photo):\n        return (location.dc_id, types.InputPhotoFileLocation(\n            id=location.id,\n            access_hash=location.access_hash,\n            file_reference=location.file_reference,\n            thumb_size=location.sizes[-1].type\n        ))\n\n    if isinstance(location, types.FileLocationToBeDeprecated):\n        raise TypeError('Unavailable location cannot be used as input')\n\n    _raise_cast_fail(location, 'InputFileLocation')", "response": "Returns a tuple of dc_id and location for input messages."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the extension for the given file.", "response": "def _get_extension(file):\n    \"\"\"\n    Gets the extension for the given file, which can be either a\n    str or an ``open()``'ed file (which has a ``.name`` attribute).\n    \"\"\"\n    if isinstance(file, str):\n        return os.path.splitext(file)[-1]\n    elif isinstance(file, bytes):\n        kind = imghdr.what(io.BytesIO(file))\n        return ('.' + kind) if kind else ''\n    elif isinstance(file, io.IOBase) and file.seekable():\n        kind = imghdr.what(file)\n        return ('.' + kind) if kind is not None else ''\n    elif getattr(file, 'name', None):\n        # Note: ``file.name`` works for :tl:`InputFile` and some `IOBase`\n        return _get_extension(file.name)\n    else:\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if the file extension looks like an image file to Telegram.", "response": "def is_image(file):\n    \"\"\"\n    Returns ``True`` if the file extension looks like an image file to Telegram.\n    \"\"\"\n    match = re.match(r'\\.(png|jpe?g)', _get_extension(file), re.IGNORECASE)\n    if match:\n        return True\n    else:\n        return isinstance(resolve_bot_file_id(file), types.Photo)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_phone(phone):\n    if isinstance(phone, int):\n        return str(phone)\n    else:\n        phone = re.sub(r'[+()\\s-]', '', str(phone))\n        if phone.isdigit():\n            return phone", "response": "Parses the given phone number or returns None if it s invalid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the given username or channel access hash given a string username or URL. Returns a tuple consisting of the stripped lowercase username whether it is a joinchat or hash. Returns None if the given username or link is not valid.", "response": "def parse_username(username):\n    \"\"\"\n    Parses the given username or channel access hash, given\n    a string, username or URL. Returns a tuple consisting of\n    both the stripped, lowercase username and whether it is\n    a joinchat/ hash (in which case is not lowercase'd).\n\n    Returns ``(None, False)`` if the ``username`` or link is not valid.\n    \"\"\"\n    username = username.strip()\n    m = USERNAME_RE.match(username) or TG_JOIN_RE.match(username)\n    if m:\n        username = username[m.end():]\n        is_invite = bool(m.group(1))\n        if is_invite:\n            return username, True\n        else:\n            username = username.rstrip('/')\n\n    if VALID_USERNAME_RE.match(username):\n        return username.lower(), False\n    else:\n        return None, False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the inner text that s surrounded by the given entities.", "response": "def get_inner_text(text, entities):\n    \"\"\"\n    Gets the inner text that's surrounded by the given entities.\n    For instance: text = 'hey!', entity = MessageEntityBold(2, 2) -> 'y!'.\n\n    :param text:     the original text.\n    :param entities: the entity or entities that must be matched.\n    :return: a single result or a list of the text surrounded by the entities.\n    \"\"\"\n    text = add_surrogate(text)\n    result = []\n    for e in entities:\n        start = e.offset\n        end = e.offset + e.length\n        result.append(del_surrogate(text[start:end]))\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the ID of the given peer.", "response": "def get_peer_id(peer, add_mark=True):\n    \"\"\"\n    Finds the ID of the given peer, and converts it to the \"bot api\" format\n    so it the peer can be identified back. User ID is left unmodified,\n    chat ID is negated, and channel ID is prefixed with -100.\n\n    The original ID and the peer type class can be returned with\n    a call to :meth:`resolve_id(marked_id)`.\n    \"\"\"\n    # First we assert it's a Peer TLObject, or early return for integers\n    if isinstance(peer, int):\n        return peer if add_mark else resolve_id(peer)[0]\n\n    # Tell the user to use their client to resolve InputPeerSelf if we got one\n    if isinstance(peer, types.InputPeerSelf):\n        _raise_cast_fail(peer, 'int (you might want to use client.get_peer_id)')\n\n    try:\n        peer = get_peer(peer)\n    except TypeError:\n        _raise_cast_fail(peer, 'int')\n\n    if isinstance(peer, types.PeerUser):\n        return peer.user_id\n    elif isinstance(peer, types.PeerChat):\n        # Check in case the user mixed things up to avoid blowing up\n        if not (0 < peer.chat_id <= 0x7fffffff):\n            peer.chat_id = resolve_id(peer.chat_id)[0]\n\n        return -peer.chat_id if add_mark else peer.chat_id\n    else:  # if isinstance(peer, types.PeerChannel):\n        # Check in case the user mixed things up to avoid blowing up\n        if not (0 < peer.channel_id <= 0x7fffffff):\n            peer.channel_id = resolve_id(peer.channel_id)[0]\n\n        if not add_mark:\n            return peer.channel_id\n\n        # Concat -100 through math tricks, .to_supergroup() on\n        # Madeline IDs will be strictly positive -> log works.\n        return -(peer.channel_id + pow(\n            10, math.floor(math.log10(peer.channel_id) + 3)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a marked ID returns the original ID and its peer type.", "response": "def resolve_id(marked_id):\n    \"\"\"Given a marked ID, returns the original ID and its :tl:`Peer` type.\"\"\"\n    if marked_id >= 0:\n        return marked_id, types.PeerUser\n\n    # There have been report of chat IDs being 10000xyz, which means their\n    # marked version is -10000xyz, which in turn looks like a channel but\n    # it becomes 00xyz (= xyz). Hence, we must assert that there are only\n    # two zeroes.\n    m = re.match(r'-100([^0]\\d*)', str(marked_id))\n    if m:\n        return int(m.group(1)), types.PeerChannel\n\n    return -marked_id, types.PeerChat"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _rle_decode(data):\n    if not data:\n        return data\n\n    new = b''\n    last = b''\n    for cur in data:\n        if last == b'\\0':\n            new += last * cur\n            last = b''\n        else:\n            new += last\n            last = bytes([cur])\n\n    return new + last", "response": "Decode a run - length - encoded byte string into a single byte string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndecode a Telegram base64 - encoded string into its bytes .", "response": "def _decode_telegram_base64(string):\n    \"\"\"\n    Decodes an url-safe base64-encoded string into its bytes\n    by first adding the stripped necessary padding characters.\n\n    This is the way Telegram shares binary data as strings,\n    such as Bot API-style file IDs or invite links.\n\n    Returns ``None`` if the input string was not valid.\n    \"\"\"\n    try:\n        return base64.urlsafe_b64decode(string + '=' * (len(string) % 4))\n    except (binascii.Error, ValueError, TypeError):\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _encode_telegram_base64(string):\n    try:\n        return base64.urlsafe_b64encode(string).rstrip(b'=').decode('ascii')\n    except (binascii.Error, ValueError, TypeError):\n        return None", "response": "Decodes a Telegram base64 string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a Bot API - style file_id returns the media it represents.", "response": "def resolve_bot_file_id(file_id):\n    \"\"\"\n    Given a Bot API-style `file_id`, returns the media it represents.\n    If the `file_id` is not valid, ``None`` is returned instead.\n\n    Note that the `file_id` does not have information such as image\n    dimensions or file size, so these will be zero if present.\n\n    For thumbnails, the photo ID and hash will always be zero.\n    \"\"\"\n    data = _rle_decode(_decode_telegram_base64(file_id))\n    if not data or data[-1] == b'\\x02':\n        return None\n\n    data = data[:-1]\n    if len(data) == 24:\n        file_type, dc_id, media_id, access_hash = struct.unpack('<iiqq', data)\n\n        if not (1 <= dc_id <= 5):\n            # Valid `file_id`'s must have valid DC IDs. Since this method is\n            # called when sending a file and the user may have entered a path\n            # they believe is correct but the file doesn't exist, this method\n            # may detect a path as \"valid\" bot `file_id` even when it's not.\n            # By checking the `dc_id`, we greatly reduce the chances of this\n            # happening.\n            return None\n\n        attributes = []\n        if file_type == 3 or file_type == 9:\n            attributes.append(types.DocumentAttributeAudio(\n                duration=0,\n                voice=file_type == 3\n            ))\n        elif file_type == 4 or file_type == 13:\n            attributes.append(types.DocumentAttributeVideo(\n                duration=0,\n                w=0,\n                h=0,\n                round_message=file_type == 13\n            ))\n        # elif file_type == 5:  # other, cannot know which\n        elif file_type == 8:\n            attributes.append(types.DocumentAttributeSticker(\n                alt='',\n                stickerset=types.InputStickerSetEmpty()\n            ))\n        elif file_type == 10:\n            attributes.append(types.DocumentAttributeAnimated())\n\n        return types.Document(\n            id=media_id,\n            access_hash=access_hash,\n            date=None,\n            mime_type='',\n            size=0,\n            thumbs=None,\n            dc_id=dc_id,\n            attributes=attributes,\n            file_reference=b''\n        )\n    elif len(data) == 44:\n        (file_type, dc_id, media_id, access_hash,\n            volume_id, secret, local_id) = struct.unpack('<iiqqqqi', data)\n\n        if not (1 <= dc_id <= 5):\n            return None\n\n        # Thumbnails (small) always have ID 0; otherwise size 'x'\n        photo_size = 's' if media_id or access_hash else 'x'\n        return types.Photo(\n            id=media_id,\n            access_hash=access_hash,\n            file_reference=b'',\n            date=None,\n            sizes=[types.PhotoSize(\n                type=photo_size,\n                location=types.FileLocationToBeDeprecated(\n                    volume_id=volume_id,\n                    local_id=local_id\n                ),\n                w=0,\n                h=0,\n                size=0\n            )],\n            dc_id=dc_id,\n            has_stickers=None\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pack_bot_file_id(file):\n    if isinstance(file, types.MessageMediaDocument):\n        file = file.document\n    elif isinstance(file, types.MessageMediaPhoto):\n        file = file.photo\n\n    if isinstance(file, types.Document):\n        file_type = 5\n        for attribute in file.attributes:\n            if isinstance(attribute, types.DocumentAttributeAudio):\n                file_type = 3 if attribute.voice else 9\n            elif isinstance(attribute, types.DocumentAttributeVideo):\n                file_type = 13 if attribute.round_message else 4\n            elif isinstance(attribute, types.DocumentAttributeSticker):\n                file_type = 8\n            elif isinstance(attribute, types.DocumentAttributeAnimated):\n                file_type = 10\n            else:\n                continue\n            break\n\n        return _encode_telegram_base64(_rle_encode(struct.pack(\n            '<iiqqb', file_type, file.dc_id, file.id, file.access_hash, 2)))\n\n    elif isinstance(file, types.Photo):\n        size = next((x for x in reversed(file.sizes) if isinstance(\n            x, (types.PhotoSize, types.PhotoCachedSize))), None)\n\n        if not size:\n            return None\n\n        size = size.location\n        return _encode_telegram_base64(_rle_encode(struct.pack(\n            '<iiqqqqib', 2, file.dc_id, file.id, file.access_hash,\n            size.volume_id, 0, size.local_id, 2  # 0 = old `secret`\n        )))\n    else:\n        return None", "response": "Packs a bot file id into a base64 string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nresolves the given invite link.", "response": "def resolve_invite_link(link):\n    \"\"\"\n    Resolves the given invite link. Returns a tuple of\n    ``(link creator user id, global chat id, random int)``.\n\n    Note that for broadcast channels, the link creator\n    user ID will be zero to protect their identity.\n    Normal chats and megagroup channels will have such ID.\n\n    Note that the chat ID may not be accurate for chats\n    with a link that were upgraded to megagroup, since\n    the link can remain the same, but the chat ID will\n    be correct once a new link is generated.\n    \"\"\"\n    link_hash, is_link = parse_username(link)\n    if not is_link:\n        # Perhaps the user passed the link hash directly\n        link_hash = link\n\n    try:\n        return struct.unpack('>LLQ', _decode_telegram_base64(link_hash))\n    except (struct.error, TypeError):\n        return None, None, None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resolve_inline_message_id(inline_msg_id):\n    try:\n        dc_id, message_id, pid, access_hash = \\\n            struct.unpack('<iiiq', _decode_telegram_base64(inline_msg_id))\n        peer = types.PeerChannel(-pid) if pid < 0 else types.PeerUser(pid)\n        return message_id, peer, dc_id, access_hash\n    except (struct.error, TypeError):\n        return None, None, None, None", "response": "Resolves an inline message ID."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a stripped photo to a JPG file.", "response": "def stripped_photo_to_jpg(stripped):\n    \"\"\"\n    Adds the JPG header and footer to a stripped image.\n\n    Ported from https://github.com/telegramdesktop/tdesktop/blob/bec39d89e19670eb436dc794a8f20b657cb87c71/Telegram/SourceFiles/ui/image/image.cpp#L225\n    \"\"\"\n    if len(stripped) < 3 or stripped[0] != 1:\n        return stripped\n\n    header = bytearray(b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00(\\x1c\\x1e#\\x1e\\x19(#!#-+(0<dA<77<{X]Id\\x91\\x80\\x99\\x96\\x8f\\x80\\x8c\\x8a\\xa0\\xb4\\xe6\\xc3\\xa0\\xaa\\xda\\xad\\x8a\\x8c\\xc8\\xff\\xcb\\xda\\xee\\xf5\\xff\\xff\\xff\\x9b\\xc1\\xff\\xff\\xff\\xfa\\xff\\xe6\\xfd\\xff\\xf8\\xff\\xdb\\x00C\\x01+--<5<vAAv\\xf8\\xa5\\x8c\\xa5\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xf8\\xff\\xc0\\x00\\x11\\x08\\x00\\x00\\x00\\x00\\x03\\x01\"\\x00\\x02\\x11\\x01\\x03\\x11\\x01\\xff\\xc4\\x00\\x1f\\x00\\x00\\x01\\x05\\x01\\x01\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\t\\n\\x0b\\xff\\xc4\\x00\\xb5\\x10\\x00\\x02\\x01\\x03\\x03\\x02\\x04\\x03\\x05\\x05\\x04\\x04\\x00\\x00\\x01}\\x01\\x02\\x03\\x00\\x04\\x11\\x05\\x12!1A\\x06\\x13Qa\\x07\"q\\x142\\x81\\x91\\xa1\\x08#B\\xb1\\xc1\\x15R\\xd1\\xf0$3br\\x82\\t\\n\\x16\\x17\\x18\\x19\\x1a%&\\'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\\x83\\x84\\x85\\x86\\x87\\x88\\x89\\x8a\\x92\\x93\\x94\\x95\\x96\\x97\\x98\\x99\\x9a\\xa2\\xa3\\xa4\\xa5\\xa6\\xa7\\xa8\\xa9\\xaa\\xb2\\xb3\\xb4\\xb5\\xb6\\xb7\\xb8\\xb9\\xba\\xc2\\xc3\\xc4\\xc5\\xc6\\xc7\\xc8\\xc9\\xca\\xd2\\xd3\\xd4\\xd5\\xd6\\xd7\\xd8\\xd9\\xda\\xe1\\xe2\\xe3\\xe4\\xe5\\xe6\\xe7\\xe8\\xe9\\xea\\xf1\\xf2\\xf3\\xf4\\xf5\\xf6\\xf7\\xf8\\xf9\\xfa\\xff\\xc4\\x00\\x1f\\x01\\x00\\x03\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\t\\n\\x0b\\xff\\xc4\\x00\\xb5\\x11\\x00\\x02\\x01\\x02\\x04\\x04\\x03\\x04\\x07\\x05\\x04\\x04\\x00\\x01\\x02w\\x00\\x01\\x02\\x03\\x11\\x04\\x05!1\\x06\\x12AQ\\x07aq\\x13\"2\\x81\\x08\\x14B\\x91\\xa1\\xb1\\xc1\\t#3R\\xf0\\x15br\\xd1\\n\\x16$4\\xe1%\\xf1\\x17\\x18\\x19\\x1a&\\'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\\x82\\x83\\x84\\x85\\x86\\x87\\x88\\x89\\x8a\\x92\\x93\\x94\\x95\\x96\\x97\\x98\\x99\\x9a\\xa2\\xa3\\xa4\\xa5\\xa6\\xa7\\xa8\\xa9\\xaa\\xb2\\xb3\\xb4\\xb5\\xb6\\xb7\\xb8\\xb9\\xba\\xc2\\xc3\\xc4\\xc5\\xc6\\xc7\\xc8\\xc9\\xca\\xd2\\xd3\\xd4\\xd5\\xd6\\xd7\\xd8\\xd9\\xda\\xe2\\xe3\\xe4\\xe5\\xe6\\xe7\\xe8\\xe9\\xea\\xf2\\xf3\\xf4\\xf5\\xf6\\xf7\\xf8\\xf9\\xfa\\xff\\xda\\x00\\x0c\\x03\\x01\\x00\\x02\\x11\\x03\\x11\\x00?\\x00')\n    footer = b\"\\xff\\xd9\"\n    header[164] = stripped[1]\n    header[166] = stripped[2]\n    return bytes(header) + stripped[3:] + footer"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def send(self, request):\n        body = bytes(request)\n        msg_id = self._state._get_new_msg_id()\n        await self._connection.send(\n            struct.pack('<qqi', 0, msg_id, len(body)) + body\n        )\n\n        body = await self._connection.recv()\n        if len(body) < 8:\n            raise InvalidBufferError(body)\n\n        with BinaryReader(body) as reader:\n            auth_key_id = reader.read_long()\n            assert auth_key_id == 0, 'Bad auth_key_id'\n\n            msg_id = reader.read_long()\n            assert msg_id != 0,  'Bad msg_id'\n            # ^ We should make sure that the read ``msg_id`` is greater\n            # than our own ``msg_id``. However, under some circumstances\n            # (bad system clock/working behind proxies) this seems to not\n            # be the case, which would cause endless assertion errors.\n\n            length = reader.read_int()\n            assert length > 0,  'Bad length'\n            # We could read length bytes and use those in a new reader to read\n            # the next TLObject without including the padding, but since the\n            # reader isn't used for anything else after this, it's unnecessary.\n            return reader.tgread_object()", "response": "Sends and receives the result for the given request."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _rel(self, path):\n        return os.path.relpath(\n            str(path), self._parent).replace(os.path.sep, '/')", "response": "Get the relative path for the given path from the current\n            file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_head(self, title, css_path, default_css):\n        self.title = title\n        self.write(\n            '''<!DOCTYPE html>\n<html>\n<head>\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n    <title>{title}</title>\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <link id=\"style\" href=\"{rel_css}/docs.dark.css\" rel=\"stylesheet\">\n    <script>\n    document.getElementById(\"style\").href = \"{rel_css}/docs.\"\n        + (localStorage.getItem(\"theme\") || \"{def_css}\")\n        + \".css\";\n    </script>\n    <link href=\"https://fonts.googleapis.com/css?family=Nunito|Source+Code+Pro\"\n          rel=\"stylesheet\">\n</head>\n<body>\n<div id=\"main_div\">''',\n            title=title,\n            rel_css=self._rel(css_path),\n            def_css=default_css\n        )", "response": "Writes the head part for the generated document with the given title and CSS."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the menu separator.", "response": "def set_menu_separator(self, img):\n        \"\"\"Sets the menu separator.\n           Must be called before adding entries to the menu\n        \"\"\"\n        if img:\n            self.menu_separator_tag = '<img src=\"{}\" alt=\"/\" />'.format(\n                self._rel(img))\n        else:\n            self.menu_separator_tag = None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a menu entry. Will create it if it doesn t exist yet.", "response": "def add_menu(self, name, link=None):\n        \"\"\"Adds a menu entry, will create it if it doesn't exist yet\"\"\"\n        if self.menu_began:\n            if self.menu_separator_tag:\n                self.write(self.menu_separator_tag)\n        else:\n            # First time, create the menu tag\n            self.write('<ul class=\"horizontal\">')\n            self.menu_began = True\n\n        self.write('<li>')\n        if link:\n            self.write('<a href=\"{}\">', self._rel(link))\n\n        # Write the real menu entry text\n        self.write(name)\n\n        if link:\n            self.write('</a>')\n        self.write('</li>')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite a title header in the document body", "response": "def write_title(self, title, level=1, id=None):\n        \"\"\"Writes a title header in the document body,\n           with an optional depth level\n        \"\"\"\n        if id:\n            self.write('<h{lv} id=\"{id}\">{title}</h{lv}>',\n                       title=title, lv=level, id=id)\n        else:\n            self.write('<h{lv}>{title}</h{lv}>',\n                       title=title, lv=level)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_code(self, tlobject):\n        self.write('<pre>---{}---\\n',\n                   'functions' if tlobject.is_function else 'types')\n\n        # Write the function or type and its ID\n        if tlobject.namespace:\n            self.write(tlobject.namespace)\n            self.write('.')\n\n        self.write('{}#{:08x}', tlobject.name, tlobject.id)\n\n        # Write all the arguments (or do nothing if there's none)\n        for arg in tlobject.args:\n            self.write(' ')\n            add_link = not arg.generic_definition and not arg.is_generic\n\n            # \"Opening\" modifiers\n            if arg.generic_definition:\n                self.write('{')\n\n            # Argument name\n            self.write(arg.name)\n            self.write(':')\n\n            # \"Opening\" modifiers\n            if arg.is_flag:\n                self.write('flags.{}?', arg.flag_index)\n\n            if arg.is_generic:\n                self.write('!')\n\n            if arg.is_vector:\n                self.write('<a href=\"{}\">Vector</a>&lt;',\n                           self.type_to_path('vector'))\n\n            # Argument type\n            if arg.type:\n                if add_link:\n                    self.write('<a href=\"{}\">', self.type_to_path(arg.type))\n                self.write(arg.type)\n                if add_link:\n                    self.write('</a>')\n            else:\n                self.write('#')\n\n            # \"Closing\" modifiers\n            if arg.is_vector:\n                self.write('&gt;')\n\n            if arg.generic_definition:\n                self.write('}')\n\n        # Now write the resulting type (result from a function/type)\n        self.write(' = ')\n        generic_name = next((arg.name for arg in tlobject.args\n                             if arg.generic_definition), None)\n\n        if tlobject.result == generic_name:\n            # Generic results cannot have any link\n            self.write(tlobject.result)\n        else:\n            if re.search('^vector<', tlobject.result, re.IGNORECASE):\n                # Notice that we don't simply make up the \"Vector\" part,\n                # because some requests (as of now, only FutureSalts),\n                # use a lower type name for it (see #81)\n                vector, inner = tlobject.result.split('<')\n                inner = inner.strip('>')\n                self.write('<a href=\"{}\">{}</a>&lt;',\n                           self.type_to_path(vector), vector)\n\n                self.write('<a href=\"{}\">{}</a>&gt;',\n                           self.type_to_path(inner), inner)\n            else:\n                self.write('<a href=\"{}\">{}</a>',\n                           self.type_to_path(tlobject.result), tlobject.result)\n\n        self.write('</pre>')", "response": "Writes the code for the given tlobject properly with hyperlinks\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef begin_table(self, column_count):\n        self.table_columns = column_count\n        self.table_columns_left = 0\n        self.write('<table>')", "response": "Starts a table with the given column_count required to automatically generate the right amount of columns when adding items to the rows"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_copy_button(self, text, text_to_copy):\n        self.write_copy_script = True\n        self.write('<button onclick=\"cp(\\'{}\\');\">{}</button>'\n                   .format(text_to_copy, text))", "response": "Writes a button with text which can be used\n           to copy text_to_copy to clipboard when it s clicked."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef end_body(self):\n        if self.write_copy_script:\n            self.write(\n                '<textarea id=\"c\" class=\"invisible\"></textarea>'\n                '<script>'\n                'function cp(t){'\n                'var c=document.getElementById(\"c\");'\n                'c.value=t;'\n                'c.select();'\n                'try{document.execCommand(\"copy\")}'\n                'catch(e){}}'\n                '</script>'\n            )\n\n        self.write('</div>{}</body></html>', self._script)", "response": "Ends the whole document. This should be called the last."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwrapping around handle. write.", "response": "def write(self, s, *args, **kwargs):\n        \"\"\"Wrapper around handle.write\"\"\"\n        if args or kwargs:\n            self.handle.write(s.format(*args, **kwargs))\n        else:\n            self.handle.write(s)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iter_participants(\n            self, entity, limit=None, *, search='',\n            filter=None, aggressive=False\n    ):\n        \"\"\"\n        Iterator over the participants belonging to the specified chat.\n\n        Args:\n            entity (`entity`):\n                The entity from which to retrieve the participants list.\n\n            limit (`int`):\n                Limits amount of participants fetched.\n\n            search (`str`, optional):\n                Look for participants with this string in name/username.\n\n                If ``aggressive is True``, the symbols from this string will\n                be used.\n\n            filter (:tl:`ChannelParticipantsFilter`, optional):\n                The filter to be used, if you want e.g. only admins\n                Note that you might not have permissions for some filter.\n                This has no effect for normal chats or users.\n\n                .. note::\n\n                    The filter :tl:`ChannelParticipantsBanned` will return\n                    *restricted* users. If you want *banned* users you should\n                    use :tl:`ChannelParticipantsKicked` instead.\n\n            aggressive (`bool`, optional):\n                Aggressively looks for all participants in the chat.\n\n                This is useful for channels since 20 July 2018,\n                Telegram added a server-side limit where only the\n                first 200 members can be retrieved. With this flag\n                set, more than 200 will be often be retrieved.\n\n                This has no effect if a ``filter`` is given.\n\n        Yields:\n            The :tl:`User` objects returned by :tl:`GetParticipantsRequest`\n            with an additional ``.participant`` attribute which is the\n            matched :tl:`ChannelParticipant` type for channels/megagroups\n            or :tl:`ChatParticipants` for normal chats.\n        \"\"\"\n        return _ParticipantsIter(\n            self,\n            limit,\n            entity=entity,\n            filter=filter,\n            search=search,\n            aggressive=aggressive\n        )", "response": "Returns an iterator over the participants in the specified chat."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef action(self, entity, action, *, delay=4, auto_cancel=True):\n        if isinstance(action, str):\n            try:\n                action = _ChatAction._str_mapping[action.lower()]\n            except KeyError:\n                raise ValueError('No such action \"{}\"'.format(action)) from None\n        elif not isinstance(action, types.TLObject) or action.SUBCLASS_OF_ID != 0x20b2cc21:\n            # 0x20b2cc21 = crc32(b'SendMessageAction')\n            if isinstance(action, type):\n                raise ValueError('You must pass an instance, not the class')\n            else:\n                raise ValueError('Cannot use {} as action'.format(action))\n\n        if isinstance(action, types.SendMessageCancelAction):\n            # ``SetTypingRequest.resolve`` will get input peer of ``entity``.\n            return self(functions.messages.SetTypingRequest(\n                entity, types.SendMessageCancelAction()))\n\n        return _ChatAction(\n            self, entity, action, delay=delay, auto_cancel=auto_cancel)", "response": "This function returns a context - manager object that represents a chat action."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def prepare_decrypter(client, cdn_client, cdn_redirect):\n        cdn_aes = AESModeCTR(\n            key=cdn_redirect.encryption_key,\n            # 12 first bytes of the IV..4 bytes of the offset (0, big endian)\n            iv=cdn_redirect.encryption_iv[:12] + bytes(4)\n        )\n\n        # We assume that cdn_redirect.cdn_file_hashes are ordered by offset,\n        # and that there will be enough of these to retrieve the whole file.\n        decrypter = CdnDecrypter(\n            cdn_client, cdn_redirect.file_token,\n            cdn_aes, cdn_redirect.cdn_file_hashes\n        )\n\n        cdn_file = await cdn_client(GetCdnFileRequest(\n            file_token=cdn_redirect.file_token,\n            offset=cdn_redirect.cdn_file_hashes[0].offset,\n            limit=cdn_redirect.cdn_file_hashes[0].limit\n        ))\n        if isinstance(cdn_file, CdnFileReuploadNeeded):\n            # We need to use the original client here\n            await client(ReuploadCdnFileRequest(\n                file_token=cdn_redirect.file_token,\n                request_token=cdn_file.request_token\n            ))\n\n            # We want to always return a valid upload.CdnFile\n            cdn_file = decrypter.get_file()\n        else:\n            cdn_file.bytes = decrypter.cdn_aes.encrypt(cdn_file.bytes)\n            cdn_hash = decrypter.cdn_file_hashes.pop(0)\n            decrypter.check(cdn_file.bytes, cdn_hash)\n\n        return decrypter, cdn_file", "response": "Prepare a new CDN decrypter."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls GetCdnFileRequest and decrypts its bytes.", "response": "def get_file(self):\n        \"\"\"\n        Calls GetCdnFileRequest and decrypts its bytes.\n        Also ensures that the file hasn't been tampered.\n\n        :return: the CdnFile result.\n        \"\"\"\n        if self.cdn_file_hashes:\n            cdn_hash = self.cdn_file_hashes.pop(0)\n            cdn_file = self.client(GetCdnFileRequest(\n                self.file_token, cdn_hash.offset, cdn_hash.limit\n            ))\n            cdn_file.bytes = self.cdn_aes.encrypt(cdn_file.bytes)\n            self.check(cdn_file.bytes, cdn_hash)\n        else:\n            cdn_file = CdnFile(bytes(0))\n\n        return cdn_file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse(html):\n    if not html:\n        return html, []\n\n    parser = HTMLToTelegramParser()\n    parser.feed(_add_surrogate(html))\n    text = helpers.strip_text(parser.text, parser.entities)\n    return _del_surrogate(text), parser.entities", "response": "Parses the given HTML message and returns its stripped representation and a list of the MessageEntity s that were found."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unparse(text, entities):\n    if not text or not entities:\n        return text\n\n    text = _add_surrogate(text)\n    html = []\n    last_offset = 0\n    for entity in entities:\n        if entity.offset > last_offset:\n            html.append(escape(text[last_offset:entity.offset]))\n        elif entity.offset < last_offset:\n            continue\n\n        skip_entity = False\n        entity_text = escape(text[entity.offset:entity.offset + entity.length])\n        entity_type = type(entity)\n\n        if entity_type == MessageEntityBold:\n            html.append('<strong>{}</strong>'.format(entity_text))\n        elif entity_type == MessageEntityItalic:\n            html.append('<em>{}</em>'.format(entity_text))\n        elif entity_type == MessageEntityCode:\n            html.append('<code>{}</code>'.format(entity_text))\n        elif entity_type == MessageEntityPre:\n            if entity.language:\n                html.append(\n                    \"<pre>\\n\"\n                    \"    <code class='language-{}'>\\n\"\n                    \"        {}\\n\"\n                    \"    </code>\\n\"\n                    \"</pre>\".format(entity.language, entity_text))\n            else:\n                html.append('<pre><code>{}</code></pre>'\n                            .format(entity_text))\n        elif entity_type == MessageEntityEmail:\n            html.append('<a href=\"mailto:{0}\">{0}</a>'.format(entity_text))\n        elif entity_type == MessageEntityUrl:\n            html.append('<a href=\"{0}\">{0}</a>'.format(entity_text))\n        elif entity_type == MessageEntityTextUrl:\n            html.append('<a href=\"{}\">{}</a>'\n                        .format(escape(entity.url), entity_text))\n        elif entity_type == MessageEntityMentionName:\n            html.append('<a href=\"tg://user?id={}\">{}</a>'\n                        .format(entity.user_id, entity_text))\n        else:\n            skip_entity = True\n        last_offset = entity.offset + (0 if skip_entity else entity.length)\n    html.append(text[last_offset:])\n    return _del_surrogate(''.join(html))", "response": "This function performs the reverse operation to. parse effectively returning a HTML representation of the text and its MessageEntity s."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef data(self):\n        if isinstance(self.button, types.KeyboardButtonCallback):\n            return self.button.data", "response": "The bytes data for this button."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef inline_query(self):\n        if isinstance(self.button, types.KeyboardButtonSwitchInline):\n            return self.button.query", "response": "The query str for inline buttons."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef url(self):\n        if isinstance(self.button, types.KeyboardButtonUrl):\n            return self.button.url", "response": "The url str for this button."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def click(self):\n        if isinstance(self.button, types.KeyboardButton):\n            return await self._client.send_message(\n                self._chat, self.button.text, reply_to=self._msg_id)\n        elif isinstance(self.button, types.KeyboardButtonCallback):\n            req = functions.messages.GetBotCallbackAnswerRequest(\n                peer=self._chat, msg_id=self._msg_id, data=self.button.data\n            )\n            try:\n                return await self._client(req)\n            except BotTimeout:\n                return None\n        elif isinstance(self.button, types.KeyboardButtonSwitchInline):\n            return await self._client(functions.messages.StartBotRequest(\n                bot=self._bot, peer=self._chat, start_param=self.button.query\n            ))\n        elif isinstance(self.button, types.KeyboardButtonUrl):\n            return webbrowser.open(self.button.url)\n        elif isinstance(self.button, types.KeyboardButtonGame):\n            req = functions.messages.GetBotCallbackAnswerRequest(\n                peer=self._chat, msg_id=self._msg_id, game=True\n            )\n            try:\n                return await self._client(req)\n            except BotTimeout:\n                return None", "response": "Clicks the specified keyboard button."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nresolve the event builders.", "response": "async def resolve(self, client):\n        \"\"\"Helper method to allow event builders to be resolved before usage\"\"\"\n        if self.resolved:\n            return\n\n        if not self._resolve_lock:\n            self._resolve_lock = asyncio.Lock(loop=client.loop)\n\n        async with self._resolve_lock:\n            if not self.resolved:\n                await self._resolve(client)\n                self.resolved = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter(self, event):\n        if not self.resolved:\n            return None\n\n        if self.chats is not None:\n            # Note: the `event.chat_id` property checks if it's `None` for us\n            inside = event.chat_id in self.chats\n            if inside == self.blacklist_chats:\n                # If this chat matches but it's a blacklist ignore.\n                # If it doesn't match but it's a whitelist ignore.\n                return None\n\n        if not self.func or self.func(event):\n            return event", "response": "Filter out events that are not in the set of chats."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the entity and input entity pair for the given entity ID.", "response": "def _get_entity_pair(self, entity_id):\n        \"\"\"\n        Returns ``(entity, input_entity)`` for the given entity ID.\n        \"\"\"\n        entity = self._entities.get(entity_id)\n        try:\n            input_entity = utils.get_input_peer(entity)\n        except TypeError:\n            try:\n                input_entity = self._client._entity_cache[entity_id]\n            except KeyError:\n                input_entity = None\n\n        return entity, input_entity"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _load_entities(self):\n        if not self._chat_peer:\n            return True  # Nothing to load (e.g. MessageDeleted)\n\n        self._chat, self._input_chat = self._get_entity_pair(self.chat_id)\n        return self._input_chat is not None", "response": "Load all the entities from cache and return True if all of them are loaded False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def _get_difference(self, channel_id, pts_date):\n        # Fetch since the last known pts/date before this update arrived,\n        # in order to fetch this update at full, including its entities.\n        self.client._log[__name__].debug('Getting difference for entities')\n        if channel_id:\n            try:\n                where = await self.client.get_input_entity(channel_id)\n            except ValueError:\n                return\n\n            result = await self.client(functions.updates.GetChannelDifferenceRequest(\n                channel=where,\n                filter=types.ChannelMessagesFilterEmpty(),\n                pts=pts_date,  # just pts\n                limit=100,\n                force=True\n            ))\n        else:\n            result = await self.client(functions.updates.GetDifferenceRequest(\n                pts=pts_date[0],\n                date=pts_date[1],\n                qts=0\n            ))\n\n        if isinstance(result, (types.updates.Difference,\n                               types.updates.DifferenceSlice,\n                               types.updates.ChannelDifference,\n                               types.updates.ChannelDifferenceTooLong)):\n            self.original_update._entities.update({\n                utils.get_peer_id(x): x for x in\n                itertools.chain(result.users, result.chats)\n            })\n\n        if not self._load_entities():\n            self.client._log[__name__].info(\n                'Could not find all entities for update.pts = %s',\n                getattr(self.original_update, 'pts', None)\n            )", "response": "Get the difference for this channel_id and pts_date."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the chat if it s not already cached.", "response": "async def get_chat(self):\n        \"\"\"\n        Returns `chat`, but will make an API call to find the\n        chat unless it's already cached.\n        \"\"\"\n        # See `get_sender` for information about 'min'.\n        if (self._chat is None or getattr(self._chat, 'min', None))\\\n                and await self.get_input_chat():\n            try:\n                self._chat =\\\n                    await self._client.get_entity(self._input_chat)\n            except ValueError:\n                await self._refetch_chat()\n        return self._chat"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def get_input_chat(self):\n        if self.input_chat is None and self.chat_id:\n            try:\n                # The chat may be recent, look in dialogs\n                target = self.chat_id\n                async for d in self._client.iter_dialogs(100):\n                    if d.id == target:\n                        self._chat = d.entity\n                        self._input_chat = d.input_entity\n                        break\n            except errors.RPCError:\n                pass\n\n        return self._input_chat", "response": "Returns the input chat if it s not already cached."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_group(self):\n        if self._broadcast is None and self.chat:\n            self._broadcast = getattr(self.chat, 'broadcast', None)\n\n        return (\n            isinstance(self._chat_peer, (types.PeerChat, types.PeerChannel))\n            and not self._broadcast\n        )", "response": "True if the message was sent on a group or megagroup."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_random_long(signed=True):\n    return int.from_bytes(os.urandom(8), signed=signed, byteorder='little')", "response": "Generates a random long integer which is optionally signed"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ensure_parent_dir_exists(file_path):\n    parent = os.path.dirname(file_path)\n    if parent:\n        os.makedirs(parent, exist_ok=True)", "response": "Ensures that the parent directory exists"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstrip whitespace from the given text modifying the provided entities.", "response": "def strip_text(text, entities):\n    \"\"\"\n    Strips whitespace from the given text modifying the provided entities.\n\n    This assumes that there are no overlapping entities, that their length\n    is greater or equal to one, and that their length is not out of bounds.\n    \"\"\"\n    if not entities:\n        return text.strip()\n\n    while text and text[-1].isspace():\n        e = entities[-1]\n        if e.offset + e.length == len(text):\n            if e.length == 1:\n                del entities[-1]\n                if not entities:\n                    return text.strip()\n            else:\n                e.length -= 1\n        text = text[:-1]\n\n    while text and text[0].isspace():\n        for i in reversed(range(len(entities))):\n            e = entities[i]\n            if e.offset != 0:\n                e.offset -= 1\n                continue\n\n            if e.length == 1:\n                del entities[0]\n                if not entities:\n                    return text.lstrip()\n            else:\n                e.length -= 1\n\n        text = text[1:]\n\n    return text"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def _cancel(log, **tasks):\n    for name, task in tasks.items():\n        if not task:\n            continue\n\n        task.cancel()\n        try:\n            await task\n        except asyncio.CancelledError:\n            pass\n        except Exception:\n            log.exception('Unhandled exception from %s after cancel', name)", "response": "Helper to cancel one or more tasks gracefully logging exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhelping to cut boilerplate on async context managers that offer synchronous variants.", "response": "def _sync_enter(self):\n    \"\"\"\n    Helps to cut boilerplate on async context\n    managers that offer synchronous variants.\n    \"\"\"\n    if hasattr(self, 'loop'):\n        loop = self.loop\n    else:\n        loop = self._client.loop\n\n    if loop.is_running():\n        raise RuntimeError(\n            'You must use \"async with\" if the event loop '\n            'is running (i.e. you are inside an \"async def\")'\n        )\n\n    return loop.run_until_complete(self.__aenter__())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_key_data_from_nonce(server_nonce, new_nonce):\n    server_nonce = server_nonce.to_bytes(16, 'little', signed=True)\n    new_nonce = new_nonce.to_bytes(32, 'little', signed=True)\n    hash1 = sha1(new_nonce + server_nonce).digest()\n    hash2 = sha1(server_nonce + new_nonce).digest()\n    hash3 = sha1(new_nonce + new_nonce).digest()\n\n    key = hash1 + hash2[:12]\n    iv = hash2[12:20] + hash3 + new_nonce[:4]\n    return key, iv", "response": "Generates the key data corresponding to the given nonce"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write(self, string, *args, **kwargs):\n        if self.on_new_line:\n            self.on_new_line = False  # We're not on a new line anymore\n            # If the string was not empty, indent; Else probably a new line\n            if string.strip():\n                self.indent()\n\n        if args or kwargs:\n            self.out_stream.write(string.format(*args, **kwargs))\n        else:\n            self.out_stream.write(string)", "response": "Writes a string into the source code with indentation applied if required\n            is True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef writeln(self, string='', *args, **kwargs):\n        self.write(string + '\\n', *args, **kwargs)\n        self.on_new_line = True\n\n        # If we're writing a block, increment indent for the next time\n        if string and string[-1] == ':':\n            self.current_indent += 1\n\n        # Clear state after the user adds a new line\n        self.auto_added_line = False", "response": "Writes a string into the source code _and_ appends a new line"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nend an indentation block leaving an empty line afterwards", "response": "def end_block(self):\n        \"\"\"Ends an indentation block, leaving an empty line afterwards\"\"\"\n        self.current_indent -= 1\n\n        # If we did not add a new line automatically yet, now it's the time!\n        if not self.auto_added_line:\n            self.writeln()\n            self.auto_added_line = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites the source code corresponding to the given TLObject.", "response": "def _write_source_code(tlobject, kind, builder, type_constructors):\n    \"\"\"\n    Writes the source code corresponding to the given TLObject\n    by making use of the ``builder`` `SourceBuilder`.\n\n    Additional information such as file path depth and\n    the ``Type: [Constructors]`` must be given for proper\n    importing and documentation strings.\n    \"\"\"\n    _write_class_init(tlobject, kind, type_constructors, builder)\n    _write_resolve(tlobject, builder)\n    _write_to_dict(tlobject, builder)\n    _write_to_bytes(tlobject, builder)\n    _write_from_reader(tlobject, builder)\n    _write_read_result(tlobject, builder)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _write_arg_to_bytes(builder, arg, args, name=None):\n    if arg.generic_definition:\n        return  # Do nothing, this only specifies a later type\n\n    if name is None:\n        name = 'self.{}'.format(arg.name)\n\n    # The argument may be a flag, only write if it's not None AND\n    # if it's not a True type.\n    # True types are not actually sent, but instead only used to\n    # determine the flags.\n    if arg.is_flag:\n        if arg.type == 'true':\n            return  # Exit, since True type is never written\n        elif arg.is_vector:\n            # Vector flags are special since they consist of 3 values,\n            # so we need an extra join here. Note that empty vector flags\n            # should NOT be sent either!\n            builder.write(\"b'' if {0} is None or {0} is False \"\n                          \"else b''.join((\", name)\n        else:\n            builder.write(\"b'' if {0} is None or {0} is False \"\n                          \"else (\", name)\n\n    if arg.is_vector:\n        if arg.use_vector_id:\n            # vector code, unsigned 0x1cb5c415 as little endian\n            builder.write(r\"b'\\x15\\xc4\\xb5\\x1c',\")\n\n        builder.write(\"struct.pack('<i', len({})),\", name)\n\n        # Cannot unpack the values for the outer tuple through *[(\n        # since that's a Python >3.5 feature, so add another join.\n        builder.write(\"b''.join(\")\n\n        # Temporary disable .is_vector, not to enter this if again\n        # Also disable .is_flag since it's not needed per element\n        old_flag = arg.is_flag\n        arg.is_vector = arg.is_flag = False\n        _write_arg_to_bytes(builder, arg, args, name='x')\n        arg.is_vector = True\n        arg.is_flag = old_flag\n\n        builder.write(' for x in {})', name)\n\n    elif arg.flag_indicator:\n        # Calculate the flags with those items which are not None\n        if not any(f.is_flag for f in args):\n            # There's a flag indicator, but no flag arguments so it's 0\n            builder.write(r\"b'\\0\\0\\0\\0'\")\n        else:\n            builder.write(\"struct.pack('<I', \")\n            builder.write(\n                ' | '.join('(0 if {0} is None or {0} is False else {1})'\n                           .format('self.{}'.format(flag.name),\n                                   1 << flag.flag_index)\n                           for flag in args if flag.is_flag)\n            )\n            builder.write(')')\n\n    elif 'int' == arg.type:\n        # struct.pack is around 4 times faster than int.to_bytes\n        builder.write(\"struct.pack('<i', {})\", name)\n\n    elif 'long' == arg.type:\n        builder.write(\"struct.pack('<q', {})\", name)\n\n    elif 'int128' == arg.type:\n        builder.write(\"{}.to_bytes(16, 'little', signed=True)\", name)\n\n    elif 'int256' == arg.type:\n        builder.write(\"{}.to_bytes(32, 'little', signed=True)\", name)\n\n    elif 'double' == arg.type:\n        builder.write(\"struct.pack('<d', {})\", name)\n\n    elif 'string' == arg.type:\n        builder.write('self.serialize_bytes({})', name)\n\n    elif 'Bool' == arg.type:\n        # 0x997275b5 if boolean else 0xbc799737\n        builder.write(r\"b'\\xb5ur\\x99' if {} else b'7\\x97y\\xbc'\", name)\n\n    elif 'true' == arg.type:\n        pass  # These are actually NOT written! Only used for flags\n\n    elif 'bytes' == arg.type:\n        builder.write('self.serialize_bytes({})', name)\n\n    elif 'date' == arg.type:  # Custom format\n        builder.write('self.serialize_datetime({})', name)\n\n    else:\n        # Else it may be a custom type\n        builder.write('bytes({})', name)\n\n        # If the type is not boxed (i.e. starts with lowercase) we should\n        # not serialize the constructor ID (so remove its first 4 bytes).\n        boxed = arg.type[arg.type.find('.') + 1].isupper()\n        if not boxed:\n            builder.write('[4:]')\n\n    if arg.is_flag:\n        builder.write(')')\n        if arg.is_vector:\n            builder.write(')')  # We were using a tuple\n\n    return True", "response": "Writes the argument to the bytes representation of the given TLObject."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _write_arg_read_code(builder, arg, args, name):\n\n    if arg.generic_definition:\n        return  # Do nothing, this only specifies a later type\n\n    # The argument may be a flag, only write that flag was given!\n    was_flag = False\n    if arg.is_flag:\n        # Treat 'true' flags as a special case, since they're true if\n        # they're set, and nothing else needs to actually be read.\n        if 'true' == arg.type:\n            builder.writeln('{} = bool(flags & {})',\n                            name, 1 << arg.flag_index)\n            return\n\n        was_flag = True\n        builder.writeln('if flags & {}:', 1 << arg.flag_index)\n        # Temporary disable .is_flag not to enter this if\n        # again when calling the method recursively\n        arg.is_flag = False\n\n    if arg.is_vector:\n        if arg.use_vector_id:\n            # We have to read the vector's constructor ID\n            builder.writeln(\"reader.read_int()\")\n\n        builder.writeln('{} = []', name)\n        builder.writeln('for _ in range(reader.read_int()):')\n        # Temporary disable .is_vector, not to enter this if again\n        arg.is_vector = False\n        _write_arg_read_code(builder, arg, args, name='_x')\n        builder.writeln('{}.append(_x)', name)\n        arg.is_vector = True\n\n    elif arg.flag_indicator:\n        # Read the flags, which will indicate what items we should read next\n        builder.writeln('flags = reader.read_int()')\n        builder.writeln()\n\n    elif 'int' == arg.type:\n        builder.writeln('{} = reader.read_int()', name)\n\n    elif 'long' == arg.type:\n        builder.writeln('{} = reader.read_long()', name)\n\n    elif 'int128' == arg.type:\n        builder.writeln('{} = reader.read_large_int(bits=128)', name)\n\n    elif 'int256' == arg.type:\n        builder.writeln('{} = reader.read_large_int(bits=256)', name)\n\n    elif 'double' == arg.type:\n        builder.writeln('{} = reader.read_double()', name)\n\n    elif 'string' == arg.type:\n        builder.writeln('{} = reader.tgread_string()', name)\n\n    elif 'Bool' == arg.type:\n        builder.writeln('{} = reader.tgread_bool()', name)\n\n    elif 'true' == arg.type:\n        # Arbitrary not-None value, don't actually read \"true\" flags\n        builder.writeln('{} = True', name)\n\n    elif 'bytes' == arg.type:\n        builder.writeln('{} = reader.tgread_bytes()', name)\n\n    elif 'date' == arg.type:  # Custom format\n        builder.writeln('{} = reader.tgread_date()', name)\n\n    else:\n        # Else it may be a custom type\n        if not arg.skip_constructor_id:\n            builder.writeln('{} = reader.tgread_object()', name)\n        else:\n            # Import the correct type inline to avoid cyclic imports.\n            # There may be better solutions so that we can just access\n            # all the types before the files have been parsed, but I\n            # don't know of any.\n            sep_index = arg.type.find('.')\n            if sep_index == -1:\n                ns, t = '.', arg.type\n            else:\n                ns, t = '.' + arg.type[:sep_index], arg.type[sep_index+1:]\n            class_name = snake_to_camel_case(t)\n\n            # There would be no need to import the type if we're in the\n            # file with the same namespace, but since it does no harm\n            # and we don't have information about such thing in the\n            # method we just ignore that case.\n            builder.writeln('from {} import {}', ns, class_name)\n            builder.writeln('{} = {}.from_reader(reader)',\n                            name, class_name)\n\n    # End vector and flag blocks if required (if we opened them before)\n    if arg.is_vector:\n        builder.end_block()\n\n    if was_flag:\n        builder.current_indent -= 1\n        builder.writeln('else:')\n        builder.writeln('{} = None', name)\n        builder.current_indent -= 1\n        # Restore .is_flag\n        arg.is_flag = True", "response": "Writes the read code for the given argument."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a Telegram s RPC Error to a Python error.", "response": "def rpc_message_to_error(rpc_error, request):\n    \"\"\"\n    Converts a Telegram's RPC Error to a Python error.\n\n    :param rpc_error: the RpcError instance.\n    :param request: the request that caused this error.\n    :return: the RPCError as a Python exception that represents this error.\n    \"\"\"\n    # Try to get the error by direct look-up, otherwise regex\n    cls = rpc_errors_dict.get(rpc_error.error_message, None)\n    if cls:\n        return cls(request)\n\n    for msg_regex, cls in rpc_errors_re:\n        m = re.match(msg_regex, rpc_error.error_message)\n        if m:\n            capture = int(m.group(1)) if m.groups() else None\n            return cls(request, capture=capture)\n\n    # Some errors are negative:\n    # * -500 for \"No workers running\",\n    # * -503 for \"Timeout\"\n    #\n    # We treat them as if they were positive, so -500 will be treated\n    # as a `ServerError`, etc.\n    cls = base_errors.get(abs(rpc_error.error_code))\n    if cls:\n        return cls(request, rpc_error.error_message)\n\n    return RPCError(request, rpc_error.error_message, rpc_error.error_code)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if the button belongs to an inline keyboard.", "response": "def _is_inline(button):\n        \"\"\"\n        Returns ``True`` if the button belongs to an inline keyboard.\n        \"\"\"\n        return isinstance(button, (\n            types.KeyboardButtonCallback,\n            types.KeyboardButtonSwitchInline,\n            types.KeyboardButtonUrl\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef inline(text, data=None):\n        if not data:\n            data = text.encode('utf-8')\n        elif not isinstance(data, (bytes, bytearray, memoryview)):\n            data = str(data).encode('utf-8')\n\n        if len(data) > 64:\n            raise ValueError('Too many bytes for the data')\n\n        return types.KeyboardButtonCallback(text, data)", "response": "Create a new inline button."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new button to switch to inline query.", "response": "def switch_inline(text, query='', same_peer=False):\n        \"\"\"\n        Creates a new button to switch to inline query.\n\n        If `query` is given, it will be the default text to be used\n        when making the inline query.\n\n        If ``same_peer is True`` the inline query will directly be\n        set under the currently opened chat. Otherwise, the user will\n        have to select a different dialog to make the query.\n        \"\"\"\n        return types.KeyboardButtonSwitchInline(text, query, same_peer)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new keyboard button with the given text.", "response": "def text(cls, text, *, resize=None, single_use=None, selective=None):\n        \"\"\"\n        Creates a new button with the given text.\n\n        Args:\n            resize (`bool`):\n                If present, the entire keyboard will be reconfigured to\n                be resized and be smaller if there are not many buttons.\n\n            single_use (`bool`):\n                If present, the entire keyboard will be reconfigured to\n                be usable only once before it hides itself.\n\n            selective (`bool`):\n                If present, the entire keyboard will be reconfigured to\n                be \"selective\". The keyboard will be shown only to specific\n                users. It will target users that are @mentioned in the text\n                of the message or to the sender of the message you reply to.\n        \"\"\"\n        return cls(types.KeyboardButton(text),\n                   resize=resize, single_use=single_use, selective=selective)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef request_location(cls, text, *,\n                         resize=None, single_use=None, selective=None):\n        \"\"\"\n        Creates a new button that will request\n        the user's location upon being clicked.\n\n        ``resize``, ``single_use`` and ``selective`` are documented in `text`.\n        \"\"\"\n        return cls(types.KeyboardButtonRequestGeoLocation(text),\n                   resize=resize, single_use=single_use, selective=selective)", "response": "Creates a new button that will request the user s location upon being clicked."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new button that will request the user s phone number upon being clicked.", "response": "def request_phone(cls, text, *,\n                      resize=None, single_use=None, selective=None):\n        \"\"\"\n        Creates a new button that will request\n        the user's phone number upon being clicked.\n\n        ``resize``, ``single_use`` and ``selective`` are documented in `text`.\n        \"\"\"\n        return cls(types.KeyboardButtonRequestPhone(text),\n                   resize=resize, single_use=single_use, selective=selective)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sprint(string, *args, **kwargs):\n    try:\n        print(string, *args, **kwargs)\n    except UnicodeEncodeError:\n        string = string.encode('utf-8', errors='ignore')\\\n                       .decode('ascii', errors='ignore')\n        print(string, *args, **kwargs)", "response": "Safe Print (handle UnicodeEncodeErrors on some terminals)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef print_title(title):\n    sprint('\\n')\n    sprint('=={}=='.format('=' * len(title)))\n    sprint('= {} ='.format(title))\n    sprint('=={}=='.format('=' * len(title)))", "response": "Helper function to print titles to the console more nicely"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def async_input(prompt):\n    print(prompt, end='', flush=True)\n    return (await loop.run_in_executor(None, sys.stdin.readline)).rstrip()", "response": "A coroutine function that returns a string from stdin."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def run(self):\n\n        # Once everything is ready, we can add an event handler.\n        #\n        # Events are an abstraction over Telegram's \"Updates\" and\n        # are much easier to use.\n        self.add_event_handler(self.message_handler, events.NewMessage)\n\n        # Enter a while loop to chat as long as the user wants\n        while True:\n            # Retrieve the top dialogs. You can set the limit to None to\n            # retrieve all of them if you wish, but beware that may take\n            # a long time if you have hundreds of them.\n            dialog_count = 15\n\n            # Entities represent the user, chat or channel\n            # corresponding to the dialog on the same index.\n            dialogs = await self.get_dialogs(limit=dialog_count)\n\n            i = None\n            while i is None:\n                print_title('Dialogs window')\n\n                # Display them so the user can choose\n                for i, dialog in enumerate(dialogs, start=1):\n                    sprint('{}. {}'.format(i, get_display_name(dialog.entity)))\n\n                # Let the user decide who they want to talk to\n                print()\n                print('> Who do you want to send messages to?')\n                print('> Available commands:')\n                print('  !q: Quits the dialogs window and exits.')\n                print('  !l: Logs out, terminating this session.')\n                print()\n                i = await async_input('Enter dialog ID or a command: ')\n                if i == '!q':\n                    return\n                if i == '!l':\n                    # Logging out will cause the user to need to reenter the\n                    # code next time they want to use the library, and will\n                    # also delete the *.session file off the filesystem.\n                    #\n                    # This is not the same as simply calling .disconnect(),\n                    # which simply shuts down everything gracefully.\n                    await self.log_out()\n                    return\n\n                try:\n                    i = int(i if i else 0) - 1\n                    # Ensure it is inside the bounds, otherwise retry\n                    if not 0 <= i < dialog_count:\n                        i = None\n                except ValueError:\n                    i = None\n\n            # Retrieve the selected user (or chat, or channel)\n            entity = dialogs[i].entity\n\n            # Show some information\n            print_title('Chat with \"{}\"'.format(get_display_name(entity)))\n            print('Available commands:')\n            print('  !q:  Quits the current chat.')\n            print('  !Q:  Quits the current chat and exits.')\n            print('  !h:  prints the latest messages (message History).')\n            print('  !up  <path>: Uploads and sends the Photo from path.')\n            print('  !uf  <path>: Uploads and sends the File from path.')\n            print('  !d   <msg-id>: Deletes a message by its id')\n            print('  !dm  <msg-id>: Downloads the given message Media (if any).')\n            print('  !dp: Downloads the current dialog Profile picture.')\n            print('  !i:  Prints information about this chat..')\n            print()\n\n            # And start a while loop to chat\n            while True:\n                msg = await async_input('Enter a message: ')\n                # Quit\n                if msg == '!q':\n                    break\n                elif msg == '!Q':\n                    return\n\n                # History\n                elif msg == '!h':\n                    # First retrieve the messages and some information\n                    messages = await self.get_messages(entity, limit=10)\n\n                    # Iterate over all (in reverse order so the latest appear\n                    # the last in the console) and print them with format:\n                    # \"[hh:mm] Sender: Message\"\n                    for msg in reversed(messages):\n                        # Note how we access .sender here. Since we made an\n                        # API call using the self client, it will always have\n                        # information about the sender. This is different to\n                        # events, where Telegram may not always send the user.\n                        name = get_display_name(msg.sender)\n\n                        # Format the message content\n                        if getattr(msg, 'media', None):\n                            self.found_media[msg.id] = msg\n                            content = '<{}> {}'.format(\n                                type(msg.media).__name__, msg.message)\n\n                        elif hasattr(msg, 'message'):\n                            content = msg.message\n                        elif hasattr(msg, 'action'):\n                            content = str(msg.action)\n                        else:\n                            # Unknown message, simply print its class name\n                            content = type(msg).__name__\n\n                        # And print it to the user\n                        sprint('[{}:{}] (ID={}) {}: {}'.format(\n                            msg.date.hour, msg.date.minute, msg.id, name, content))\n\n                # Send photo\n                elif msg.startswith('!up '):\n                    # Slice the message to get the path\n                    path = msg[len('!up '):]\n                    await self.send_photo(path=path, entity=entity)\n\n                # Send file (document)\n                elif msg.startswith('!uf '):\n                    # Slice the message to get the path\n                    path = msg[len('!uf '):]\n                    await self.send_document(path=path, entity=entity)\n\n                # Delete messages\n                elif msg.startswith('!d '):\n                    # Slice the message to get message ID\n                    msg = msg[len('!d '):]\n                    deleted_msg = await self.delete_messages(entity, msg)\n                    print('Deleted {}'.format(deleted_msg))\n\n                # Download media\n                elif msg.startswith('!dm '):\n                    # Slice the message to get message ID\n                    await self.download_media_by_id(msg[len('!dm '):])\n\n                # Download profile photo\n                elif msg == '!dp':\n                    print('Downloading profile picture to usermedia/...')\n                    os.makedirs('usermedia', exist_ok=True)\n                    output = await self.download_profile_photo(entity,\n                                                               'usermedia')\n                    if output:\n                        print('Profile picture downloaded to', output)\n                    else:\n                        print('No profile picture found for this user!')\n\n                elif msg == '!i':\n                    attributes = list(entity.to_dict().items())\n                    pad = max(len(x) for x, _ in attributes)\n                    for name, val in attributes:\n                        print(\"{:<{width}} : {}\".format(name, val, width=pad))\n\n                # Send chat message (if any)\n                elif msg:\n                    await self.send_message(entity, msg, link_preview=False)", "response": "This function is the main loop of the Telegram client. It will wait for the user to enter a new message and send it to the user."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def send_photo(self, path, entity):\n        await self.send_file(\n            entity, path,\n            progress_callback=self.upload_progress_callback\n        )\n        print('Photo sent!')", "response": "Sends the file located at path to the desired entity as a photo"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending the file located at path to the desired entity as a document", "response": "async def send_document(self, path, entity):\n        \"\"\"Sends the file located at path to the desired entity as a document\"\"\"\n        await self.send_file(\n            entity, path,\n            force_document=True,\n            progress_callback=self.upload_progress_callback\n        )\n        print('Document sent!')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def download_media_by_id(self, media_id):\n        try:\n            msg = self.found_media[int(media_id)]\n        except (ValueError, KeyError):\n            # ValueError when parsing, KeyError when accessing dictionary\n            print('Invalid media ID given or message not found!')\n            return\n\n        print('Downloading media to usermedia/...')\n        os.makedirs('usermedia', exist_ok=True)\n        output = await self.download_media(\n            msg.media,\n            file='usermedia/',\n            progress_callback=self.download_progress_callback\n        )\n        print('Media downloaded to {}!'.format(output))", "response": "Given a message ID finds the media this message contained and\n           downloads it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the file name of the given object.", "response": "def _get_file_name(tlobject):\n    \"\"\"``ClassName -> class_name.html``.\"\"\"\n    name = tlobject.name if isinstance(tlobject, TLObject) else tlobject\n    # Courtesy of http://stackoverflow.com/a/1176023/4759433\n    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n    result = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n    return '{}.html'.format(result)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the code to import a TLObject.", "response": "def get_import_code(tlobject):\n    \"\"\"``TLObject -> from ... import ...``.\"\"\"\n    kind = 'functions' if tlobject.is_function else 'types'\n    ns = '.' + tlobject.namespace if tlobject.namespace else ''\n    return 'from telethon.tl.{}{} import {}'\\\n        .format(kind, ns, tlobject.class_name)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_path_for(root, tlobject):\n    out_dir = root / ('methods' if tlobject.is_function else 'constructors')\n    if tlobject.namespace:\n        out_dir /= tlobject.namespace\n\n    return out_dir / _get_file_name(tlobject)", "response": "Creates and returns the path for the given TLObject at root."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding the <title > for the given HTML file or ( Unknown ) if no title is found.", "response": "def _find_title(html_file):\n    \"\"\"Finds the <title> for the given HTML file, or (Unknown).\"\"\"\n    # TODO Is it necessary to read files like this?\n    with html_file.open() as f:\n        for line in f:\n            if '<title>' in line:\n                # + 7 to skip len('<title>')\n                return line[line.index('<title>') + 7:line.index('</title>')]\n\n    return '(Unknown)'"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild the menu used for the current document writer.", "response": "def _build_menu(docs):\n    \"\"\"\n    Builds the menu used for the current ``DocumentWriter``.\n    \"\"\"\n\n    paths = []\n    current = docs.filename\n    while current != docs.root:\n        current = current.parent\n        paths.append(current)\n\n    for path in reversed(paths):\n        docs.add_menu(path.stem.title(), link=path / 'index.html')\n\n    if docs.filename.stem != 'index':\n        docs.add_menu(docs.title, link=docs.filename)\n\n    docs.end_menu()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate the index. html file for the specified folder.", "response": "def _generate_index(root, folder, paths,\n                    bots_index=False, bots_index_paths=()):\n    \"\"\"Generates the index file for the specified folder\"\"\"\n    # Determine the namespaces listed here (as sub folders)\n    # and the files (.html files) that we should link to\n    namespaces = []\n    files = []\n    INDEX = 'index.html'\n    BOT_INDEX = 'botindex.html'\n\n    for item in (bots_index_paths or folder.iterdir()):\n        if item.is_dir():\n            namespaces.append(item)\n        elif item.name not in (INDEX, BOT_INDEX):\n            files.append(item)\n\n    # Now that everything is setup, write the index.html file\n    filename = folder / (BOT_INDEX if bots_index else INDEX)\n    with DocsWriter(root, filename, _get_path_for_type) as docs:\n        # Title should be the current folder name\n        docs.write_head(str(folder).replace(os.path.sep, '/').title(),\n                        css_path=paths['css'],\n                        default_css=paths['default_css'])\n\n        docs.set_menu_separator(paths['arrow'])\n        _build_menu(docs)\n        docs.write_title(str(filename.parent.relative_to(root))\n                         .replace(os.path.sep, '/').title())\n\n        if bots_index:\n            docs.write_text('These are the methods that you may be able to '\n                            'use as a bot. Click <a href=\"{}\">here</a> to '\n                            'view them all.'.format(INDEX))\n        else:\n            docs.write_text('Click <a href=\"{}\">here</a> to view the methods '\n                            'that you can use as a bot.'.format(BOT_INDEX))\n        if namespaces:\n            docs.write_title('Namespaces', level=3)\n            docs.begin_table(4)\n            namespaces.sort()\n            for namespace in namespaces:\n                # For every namespace, also write the index of it\n                namespace_paths = []\n                if bots_index:\n                    for item in bots_index_paths:\n                        if item.parent == namespace:\n                            namespace_paths.append(item)\n\n                _generate_index(root, namespace, paths,\n                                bots_index, namespace_paths)\n\n                docs.add_row(\n                    namespace.stem.title(),\n                    link=namespace / (BOT_INDEX if bots_index else INDEX))\n\n            docs.end_table()\n\n        docs.write_title('Available items')\n        docs.begin_table(2)\n\n        files = [(f, _find_title(f)) for f in files]\n        files.sort(key=lambda t: t[1])\n\n        for file, title in files:\n            docs.add_row(title, link=file)\n\n        docs.end_table()\n        docs.end_body()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_description(arg):\n    desc = []\n    otherwise = False\n    if arg.can_be_inferred:\n        desc.append('If left unspecified, it will be inferred automatically.')\n        otherwise = True\n    elif arg.is_flag:\n        desc.append('This argument defaults to '\n                    '<code>None</code> and can be omitted.')\n        otherwise = True\n\n    if arg.type in {'InputPeer', 'InputUser', 'InputChannel',\n                    'InputNotifyPeer', 'InputDialogPeer'}:\n        desc.append(\n            'Anything entity-like will work if the library can find its '\n            '<code>Input</code> version (e.g., usernames, <code>Peer</code>, '\n            '<code>User</code> or <code>Channel</code> objects, etc.).'\n        )\n\n    if arg.is_vector:\n        if arg.is_generic:\n            desc.append('A list of other Requests must be supplied.')\n        else:\n            desc.append('A list must be supplied.')\n    elif arg.is_generic:\n        desc.append('A different Request must be supplied for this argument.')\n    else:\n        otherwise = False  # Always reset to false if no other text is added\n\n    if otherwise:\n        desc.insert(1, 'Otherwise,')\n        desc[-1] = desc[-1][:1].lower() + desc[-1][1:]\n\n    return ' '.join(desc).replace(\n        'list',\n        '<span class=\"tooltip\" title=\"Any iterable that supports len() '\n        'will work too\">list</span>'\n    )", "response": "Generates a proper description for the given argument."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _copy_replace(src, dst, replacements):\n    with src.open() as infile, dst.open('w') as outfile:\n        outfile.write(re.sub(\n            '|'.join(re.escape(k) for k in replacements),\n            lambda m: str(replacements[m.group(0)]),\n            infile.read()\n        ))", "response": "Copies the src file into dst applying the replacements dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate the documentation HTML files from scheme. tl methods and constructors and layer.", "response": "def _write_html_pages(root, tlobjects, methods, layer, input_res):\n    \"\"\"\n    Generates the documentation HTML files from from ``scheme.tl``\n    to ``/methods`` and ``/constructors``, etc.\n    \"\"\"\n    # Save 'Type: [Constructors]' for use in both:\n    # * Seeing the return type or constructors belonging to the same type.\n    # * Generating the types documentation, showing available constructors.\n    paths = {k: root / v for k, v in (\n        ('css', 'css'),\n        ('arrow', 'img/arrow.svg'),\n        ('search.js', 'js/search.js'),\n        ('404', '404.html'),\n        ('index_all', 'index.html'),\n        ('bot_index', 'botindex.html'),\n        ('index_types', 'types/index.html'),\n        ('index_methods', 'methods/index.html'),\n        ('index_constructors', 'constructors/index.html')\n    )}\n    paths['default_css'] = 'light'  # docs.<name>.css, local path\n    type_to_constructors = defaultdict(list)\n    type_to_functions = defaultdict(list)\n    for tlobject in tlobjects:\n        d = type_to_functions if tlobject.is_function else type_to_constructors\n        d[tlobject.result].append(tlobject)\n\n    for t, cs in type_to_constructors.items():\n        type_to_constructors[t] = list(sorted(cs, key=lambda c: c.name))\n\n    methods = {m.name: m for m in methods}\n\n    # Since the output directory is needed everywhere partially apply it now\n    create_path_for = functools.partial(_get_path_for, root)\n    path_for_type = lambda t: root / _get_path_for_type(t)\n    bot_docs_paths = []\n\n    for tlobject in tlobjects:\n        filename = create_path_for(tlobject)\n        with DocsWriter(root, filename, path_for_type) as docs:\n            docs.write_head(title=tlobject.class_name,\n                            css_path=paths['css'],\n                            default_css=paths['default_css'])\n\n            # Create the menu (path to the current TLObject)\n            docs.set_menu_separator(paths['arrow'])\n            _build_menu(docs)\n\n            # Create the page title\n            docs.write_title(tlobject.class_name)\n\n            if tlobject.is_function:\n                if tlobject.usability == Usability.USER:\n                    start = '<strong>Only users</strong> can'\n                elif tlobject.usability == Usability.BOT:\n                    bot_docs_paths.append(filename)\n                    start = '<strong>Only bots</strong> can'\n                elif tlobject.usability == Usability.BOTH:\n                    bot_docs_paths.append(filename)\n                    start = '<strong>Both users and bots</strong> can'\n                else:\n                    bot_docs_paths.append(filename)\n                    start = \\\n                        'Both users and bots <strong>may</strong> be able to'\n\n                docs.write_text('{} use this method. <a href=\"#examples\">'\n                                'See code examples.</a>'.format(start))\n\n            # Write the code definition for this TLObject\n            docs.write_code(tlobject)\n            docs.write_copy_button('Copy import to the clipboard',\n                                   get_import_code(tlobject))\n\n            # Write the return type (or constructors belonging to the same type)\n            docs.write_title('Returns' if tlobject.is_function\n                             else 'Belongs to', level=3)\n\n            generic_arg = next((arg.name for arg in tlobject.args\n                                if arg.generic_definition), None)\n\n            if tlobject.result == generic_arg:\n                # We assume it's a function returning a generic type\n                generic_arg = next((arg.name for arg in tlobject.args\n                                    if arg.is_generic))\n                docs.write_text('This function returns the result of whatever '\n                                'the result from invoking the request passed '\n                                'through <i>{}</i> is.'.format(generic_arg))\n            else:\n                if re.search('^vector<', tlobject.result, re.IGNORECASE):\n                    docs.write_text('A list of the following type is returned.')\n                    _, inner = tlobject.result.split('<')\n                    inner = inner.strip('>')\n                else:\n                    inner = tlobject.result\n\n                docs.begin_table(column_count=1)\n                docs.add_row(inner, link=path_for_type(inner))\n                docs.end_table()\n\n                cs = type_to_constructors.get(inner, [])\n                if not cs:\n                    docs.write_text('This type has no instances available.')\n                elif len(cs) == 1:\n                    docs.write_text('This type can only be an instance of:')\n                else:\n                    docs.write_text('This type can be an instance of either:')\n\n                docs.begin_table(column_count=2)\n                for constructor in cs:\n                    link = create_path_for(constructor)\n                    docs.add_row(constructor.class_name, link=link)\n                docs.end_table()\n\n            # Return (or similar types) written. Now parameters/members\n            docs.write_title(\n                'Parameters' if tlobject.is_function else 'Members', level=3\n            )\n\n            # Sort the arguments in the same way they're sorted\n            # on the generated code (flags go last)\n            args = [\n                a for a in tlobject.sorted_args()\n                if not a.flag_indicator and not a.generic_definition\n            ]\n\n            if args:\n                # Writing parameters\n                docs.begin_table(column_count=3)\n\n                for arg in args:\n                    # Name row\n                    docs.add_row(arg.name,\n                                 bold=True)\n\n                    # Type row\n                    friendly_type = 'flag' if arg.type == 'true' else arg.type\n                    if arg.is_generic:\n                        docs.add_row('!' + friendly_type, align='center')\n                    else:\n                        docs.add_row(\n                            friendly_type, align='center',\n                            link=path_for_type(arg.type)\n                         )\n\n                    # Add a description for this argument\n                    docs.add_row(_get_description(arg))\n\n                docs.end_table()\n            else:\n                if tlobject.is_function:\n                    docs.write_text('This request takes no input parameters.')\n                else:\n                    docs.write_text('This type has no members.')\n\n            if tlobject.is_function:\n                docs.write_title('Known RPC errors')\n                method_info = methods.get(tlobject.fullname)\n                errors = method_info and method_info.errors\n                if not errors:\n                    docs.write_text(\"This request can't cause any RPC error \"\n                                    \"as far as we know.\")\n                else:\n                    docs.write_text(\n                        'This request can cause {} known error{}:'.format(\n                            len(errors), '' if len(errors) == 1 else 's'\n                    ))\n                    docs.begin_table(column_count=2)\n                    for error in errors:\n                        docs.add_row('<code>{}</code>'.format(error.name))\n                        docs.add_row('{}.'.format(error.description))\n                    docs.end_table()\n                    docs.write_text('You can import these from '\n                                    '<code>telethon.errors</code>.')\n\n                docs.write_title('Example', id='examples')\n                docs.write('''<pre>\\\n<strong>from</strong> telethon.sync <strong>import</strong> TelegramClient\n<strong>from</strong> telethon <strong>import</strong> functions, types\n\n<strong>with</strong> TelegramClient(name, api_id, api_hash) <strong>as</strong> client:\n    result = client(''')\n                tlobject.as_example(docs, indent=1)\n                docs.write(')\\n')\n                if tlobject.result.startswith('Vector'):\n                    docs.write('''\\\n    <strong>for</strong> x <strong>in</strong> result:\n        print(x''')\n                else:\n                    docs.write('    print(result')\n                    if tlobject.result != 'Bool' \\\n                            and not tlobject.result.startswith('Vector'):\n                        docs.write('.stringify()')\n\n                docs.write(')</pre>')\n\n            depth = '../' * (2 if tlobject.namespace else 1)\n            docs.add_script(src='prependPath = \"{}\";'.format(depth))\n            docs.add_script(path=paths['search.js'])\n            docs.end_body()\n\n    # Find all the available types (which are not the same as the constructors)\n    # Each type has a list of constructors associated to it, hence is a map\n    for t, cs in type_to_constructors.items():\n        filename = path_for_type(t)\n        out_dir = filename.parent\n        if out_dir:\n            out_dir.mkdir(parents=True, exist_ok=True)\n\n        # Since we don't have access to the full TLObject, split the type\n        if '.' in t:\n            namespace, name = t.split('.')\n        else:\n            namespace, name = None, t\n\n        with DocsWriter(root, filename, path_for_type) as docs:\n            docs.write_head(title=snake_to_camel_case(name),\n                            css_path=paths['css'],\n                            default_css=paths['default_css'])\n\n            docs.set_menu_separator(paths['arrow'])\n            _build_menu(docs)\n\n            # Main file title\n            docs.write_title(snake_to_camel_case(name))\n\n            # List available constructors for this type\n            docs.write_title('Available constructors', level=3)\n            if not cs:\n                docs.write_text('This type has no constructors available.')\n            elif len(cs) == 1:\n                docs.write_text('This type has one constructor available.')\n            else:\n                docs.write_text('This type has %d constructors available.' %\n                                len(cs))\n\n            docs.begin_table(2)\n            for constructor in cs:\n                # Constructor full name\n                link = create_path_for(constructor)\n                docs.add_row(constructor.class_name, link=link)\n            docs.end_table()\n\n            # List all the methods which return this type\n            docs.write_title('Methods returning this type', level=3)\n            functions = type_to_functions.get(t, [])\n            if not functions:\n                docs.write_text('No method returns this type.')\n            elif len(functions) == 1:\n                docs.write_text('Only the following method returns this type.')\n            else:\n                docs.write_text(\n                    'The following %d methods return this type as a result.' %\n                    len(functions)\n                )\n\n            docs.begin_table(2)\n            for func in functions:\n                link = create_path_for(func)\n                docs.add_row(func.class_name, link=link)\n            docs.end_table()\n\n            # List all the methods which take this type as input\n            docs.write_title('Methods accepting this type as input', level=3)\n            other_methods = sorted(\n                (u for u in tlobjects\n                 if any(a.type == t for a in u.args) and u.is_function),\n                key=lambda u: u.name\n            )\n            if not other_methods:\n                docs.write_text(\n                    'No methods accept this type as an input parameter.')\n            elif len(other_methods) == 1:\n                docs.write_text(\n                    'Only this method has a parameter with this type.')\n            else:\n                docs.write_text(\n                    'The following %d methods accept this type as an input '\n                    'parameter.' % len(other_methods))\n\n            docs.begin_table(2)\n            for ot in other_methods:\n                link = create_path_for(ot)\n                docs.add_row(ot.class_name, link=link)\n            docs.end_table()\n\n            # List every other type which has this type as a member\n            docs.write_title('Other types containing this type', level=3)\n            other_types = sorted(\n                (u for u in tlobjects\n                 if any(a.type == t for a in u.args) and not u.is_function),\n                key=lambda u: u.name\n            )\n\n            if not other_types:\n                docs.write_text(\n                    'No other types have a member of this type.')\n            elif len(other_types) == 1:\n                docs.write_text(\n                    'You can find this type as a member of this other type.')\n            else:\n                docs.write_text(\n                    'You can find this type as a member of any of '\n                    'the following %d types.' % len(other_types))\n\n            docs.begin_table(2)\n            for ot in other_types:\n                link = create_path_for(ot)\n                docs.add_row(ot.class_name, link=link)\n            docs.end_table()\n            docs.end_body()\n\n    # After everything's been written, generate an index.html per folder.\n    # This will be done automatically and not taking into account any extra\n    # information that we have available, simply a file listing all the others\n    # accessible by clicking on their title\n    for folder in ['types', 'methods', 'constructors']:\n        _generate_index(root, root / folder, paths)\n\n    _generate_index(root, root / 'methods', paths, True,\n                    bot_docs_paths)\n\n    # Write the final core index, the main index for the rest of files\n    types = set()\n    methods = []\n    cs = []\n    for tlobject in tlobjects:\n        if tlobject.is_function:\n            methods.append(tlobject)\n        else:\n            cs.append(tlobject)\n\n        if not tlobject.result.lower() in CORE_TYPES:\n            if re.search('^vector<', tlobject.result, re.IGNORECASE):\n                types.add(tlobject.result.split('<')[1].strip('>'))\n            else:\n                types.add(tlobject.result)\n\n    types = sorted(types)\n    methods = sorted(methods, key=lambda m: m.name)\n    cs = sorted(cs, key=lambda c: c.name)\n\n    shutil.copy(str(input_res / '404.html'), str(paths['404']))\n    _copy_replace(input_res / 'core.html', paths['index_all'], {\n        '{type_count}': len(types),\n        '{method_count}': len(methods),\n        '{constructor_count}': len(tlobjects) - len(methods),\n        '{layer}': layer,\n    })\n\n    def fmt(xs):\n        zs = {}  # create a dict to hold those which have duplicated keys\n        for x in xs:\n            zs[x.class_name] = x.class_name in zs\n        return ', '.join(\n            '\"{}.{}\"'.format(x.namespace, x.class_name)\n            if zs[x.class_name] and x.namespace\n            else '\"{}\"'.format(x.class_name) for x in xs\n        )\n\n    request_names = fmt(methods)\n    constructor_names = fmt(cs)\n\n    def fmt(xs, formatter):\n        return ', '.join('\"{}\"'.format(\n            formatter(x)).replace(os.path.sep, '/') for x in xs)\n\n    type_names = fmt(types, formatter=lambda x: x)\n\n    # Local URLs shouldn't rely on the output's root, so set empty root\n    get_path_for = functools.partial(_get_path_for, Path())\n\n    request_urls = fmt(methods, get_path_for)\n    type_urls = fmt(types, _get_path_for_type)\n    constructor_urls = fmt(cs, get_path_for)\n\n    paths['search.js'].parent.mkdir(parents=True, exist_ok=True)\n    _copy_replace(input_res / 'js/search.js', paths['search.js'], {\n        '{request_names}': request_names,\n        '{type_names}': type_names,\n        '{constructor_names}': constructor_names,\n        '{request_urls}': request_urls,\n        '{type_urls}': type_urls,\n        '{constructor_urls}': constructor_urls\n    })"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_structure(tlobjects, output_dir):\n    types_ns = set()\n    method_ns = set()\n    for obj in tlobjects:\n        if obj.namespace:\n            if obj.is_function:\n                method_ns.add(obj.namespace)\n            else:\n                types_ns.add(obj.namespace)\n\n    output_dir.mkdir(exist_ok=True)\n\n    type_dir = output_dir / 'types'\n    type_dir.mkdir(exist_ok=True)\n\n    cons_dir = output_dir / 'constructors'\n    cons_dir.mkdir(exist_ok=True)\n    for ns in types_ns:\n        (type_dir / ns).mkdir(exist_ok=True)\n        (cons_dir / ns).mkdir(exist_ok=True)\n\n    meth_dir = output_dir / 'methods'\n    meth_dir.mkdir(exist_ok=True)\n    for ns in types_ns:\n        (meth_dir / ns).mkdir(exist_ok=True)", "response": "Create the required directory structure\n    in output_dir for the input objects."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_connection(self, *args, **kwargs):\n\n        args = list(args)\n        if len(args) == 0:  # kazoo 2.6.0 slightly changed the way how it calls create_connection method\n            kwargs['timeout'] = max(self._connect_timeout, kwargs.get('timeout', self._connect_timeout*10)/10.0)\n        elif len(args) == 1:\n            args.append(self._connect_timeout)\n        else:\n            args[1] = max(self._connect_timeout, args[1]/10.0)\n        return super(PatroniSequentialThreadingHandler, self).create_connection(*args, **kwargs)", "response": "This method is used to establish a connection with one of the zookeeper nodes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the ttl of the current session.", "response": "def set_ttl(self, ttl):\n        \"\"\"It is not possible to change ttl (session_timeout) in zookeeper without\n        destroying old session and creating the new one. This method returns `!True`\n        if session_timeout has been changed (`restart()` has been called).\"\"\"\n        if self._client._session_timeout != ttl:\n            self._client._session_timeout = ttl\n            self._client.restart()\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntranslating scope name to service name which can be used in dns.", "response": "def service_name_from_scope_name(scope_name):\n    \"\"\"Translate scope name to service name which can be used in dns.\n\n    230 = 253 - len('replica.') - len('.service.consul')\n    \"\"\"\n\n    def replace_char(match):\n        c = match.group(0)\n        return '-' if c in '. _' else \"u{:04d}\".format(ord(c))\n\n    service_name = re.sub(r'[^a-z0-9\\-]', replace_char, scope_name.lower())\n    return service_name[0:230]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _do_refresh_session(self):\n        if self._session and self._last_session_refresh + self._loop_wait > time.time():\n            return False\n\n        if self._session:\n            try:\n                self._client.session.renew(self._session)\n            except NotFound:\n                self._session = None\n        ret = not self._session\n        if ret:\n            try:\n                self._session = self._client.session.create(name=self._scope + '-' + self._name,\n                                                            checks=self.__session_checks,\n                                                            lock_delay=0.001, behavior='delete')\n            except InvalidSessionTTL:\n                logger.exception('session.create')\n                self.adjust_ttl()\n                raise\n\n        self._last_session_refresh = time.time()\n        return ret", "response": "Refreshes the session if it had to create new session"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread and parses postmaster. pid from the data directory", "response": "def _read_postmaster_pidfile(data_dir):\n        \"\"\"Reads and parses postmaster.pid from the data directory\n\n        :returns dictionary of values if successful, empty dictionary otherwise\n        \"\"\"\n        pid_line_names = ['pid', 'data_dir', 'start_time', 'port', 'socket_dir', 'listen_addr', 'shmem_key']\n        try:\n            with open(os.path.join(data_dir, 'postmaster.pid')) as f:\n                return {name: line.rstrip('\\n') for name, line in zip(pid_line_names, f)}\n        except IOError:\n            return {}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef signal_stop(self, mode):\n        if self.is_single_user:\n            logger.warning(\"Cannot stop server; single-user server is running (PID: {0})\".format(self.pid))\n            return False\n        try:\n            self.send_signal(STOP_SIGNALS[mode])\n        except psutil.NoSuchProcess:\n            return True\n        except psutil.AccessDenied as e:\n            logger.warning(\"Could not send stop signal to PostgreSQL (error: {0})\".format(e))\n            return False\n\n        return None", "response": "Signal postmaster process to stop the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a human readable string representation of the size of a resource.", "response": "def repr_size(n_bytes):\n    \"\"\"\n    >>> repr_size(1000)\n    '1000 Bytes'\n    >>> repr_size(8257332324597)\n    '7.5 TiB'\n    \"\"\"\n    if n_bytes < 1024:\n        return '{0} Bytes'.format(n_bytes)\n    i = -1\n    while n_bytes > 1023:\n        n_bytes /= 1024.0\n        i += 1\n    return '{0} {1}iB'.format(round(n_bytes, 1), si_prefixes[i])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the size in bytes for the given prefix.", "response": "def size_as_bytes(size_, prefix):\n    \"\"\"\n    >>> size_as_bytes(7.5, 'T')\n    8246337208320\n    \"\"\"\n    prefix = prefix.upper()\n\n    assert prefix in si_prefixes\n\n    exponent = si_prefixes.index(prefix) + 1\n\n    return int(size_ * (1024.0 ** exponent))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning the WAL - E backup process.", "response": "def run(self):\n        \"\"\"\n        Creates a new replica using WAL-E\n\n        Returns\n        -------\n        ExitCode\n            0 = Success\n            1 = Error, try again\n            2 = Error, don't try again\n\n        \"\"\"\n        if self.init_error:\n            logger.error('init error: %r did not exist at initialization time',\n                         self.wal_e.env_dir)\n            return ExitCode.FAIL\n\n        try:\n            should_use_s3 = self.should_use_s3_to_create_replica()\n            if should_use_s3 is None:  # Need to retry\n                return ExitCode.RETRY_LATER\n            elif should_use_s3:\n                return self.create_replica_with_s3()\n            elif not should_use_s3:\n                return ExitCode.FAIL\n        except Exception:\n            logger.exception(\"Unhandled exception when running WAL-E restore\")\n        return ExitCode.FAIL"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine whether it makes sense to use S3 and not pg_basebackup.", "response": "def should_use_s3_to_create_replica(self):\n        \"\"\" determine whether it makes sense to use S3 and not pg_basebackup \"\"\"\n\n        threshold_megabytes = self.wal_e.threshold_mb\n        threshold_percent = self.wal_e.threshold_pct\n\n        try:\n            cmd = self.wal_e.cmd + ['backup-list', '--detail', 'LATEST']\n\n            logger.debug('calling %r', cmd)\n            wale_output = subprocess.check_output(cmd)\n\n            reader = csv.DictReader(wale_output.decode('utf-8').splitlines(),\n                                    dialect='excel-tab')\n            rows = list(reader)\n            if not len(rows):\n                logger.warning('wal-e did not find any backups')\n                return False\n\n            # This check might not add much, it was performed in the previous\n            # version of this code. since the old version rolled CSV parsing the\n            # check may have been part of the CSV parsing.\n            if len(rows) > 1:\n                logger.warning(\n                    'wal-e returned more than one row of backups: %r',\n                    rows)\n                return False\n\n            backup_info = rows[0]\n        except subprocess.CalledProcessError:\n            logger.exception(\"could not query wal-e latest backup\")\n            return None\n\n        try:\n            backup_size = int(backup_info['expanded_size_bytes'])\n            backup_start_segment = backup_info['wal_segment_backup_start']\n            backup_start_offset = backup_info['wal_segment_offset_backup_start']\n        except KeyError:\n            logger.exception(\"unable to get some of WALE backup parameters\")\n            return None\n\n        # WAL filename is XXXXXXXXYYYYYYYY000000ZZ, where X - timeline, Y - LSN logical log file,\n        # ZZ - 2 high digits of LSN offset. The rest of the offset is the provided decimal offset,\n        # that we have to convert to hex and 'prepend' to the high offset digits.\n\n        lsn_segment = backup_start_segment[8:16]\n        # first 2 characters of the result are 0x and the last one is L\n        lsn_offset = hex((int(backup_start_segment[16:32], 16) << 24) + int(backup_start_offset))[2:-1]\n\n        # construct the LSN from the segment and offset\n        backup_start_lsn = '{0}/{1}'.format(lsn_segment, lsn_offset)\n\n        diff_in_bytes = backup_size\n        attempts_no = 0\n        while True:\n            if self.master_connection:\n                try:\n                    # get the difference in bytes between the current WAL location and the backup start offset\n                    with psycopg2.connect(self.master_connection) as con:\n                        if con.server_version >= 100000:\n                            wal_name = 'wal'\n                            lsn_name = 'lsn'\n                        else:\n                            wal_name = 'xlog'\n                            lsn_name = 'location'\n                        con.autocommit = True\n                        with con.cursor() as cur:\n                            cur.execute((\"SELECT CASE WHEN pg_catalog.pg_is_in_recovery()\"\n                                         \" THEN GREATEST(pg_catalog.pg_{0}_{1}_diff(COALESCE(\"\n                                         \"pg_last_{0}_receive_{1}(), '0/0'), %s)::bigint, \"\n                                         \"pg_catalog.pg_{0}_{1}_diff(pg_catalog.pg_last_{0}_replay_{1}(), %s)::bigint)\"\n                                         \" ELSE pg_catalog.pg_{0}_{1}_diff(pg_catalog.pg_current_{0}_{1}(), %s)::bigint\"\n                                         \" END\").format(wal_name, lsn_name),\n                                        (backup_start_lsn, backup_start_lsn, backup_start_lsn))\n\n                            diff_in_bytes = int(cur.fetchone()[0])\n                except psycopg2.Error:\n                    logger.exception('could not determine difference with the master location')\n                    if attempts_no < self.retries:  # retry in case of a temporarily connection issue\n                        attempts_no = attempts_no + 1\n                        time.sleep(RETRY_SLEEP_INTERVAL)\n                        continue\n                    else:\n                        if not self.no_master:\n                            return False  # do no more retries on the outer level\n                        logger.info(\"continue with base backup from S3 since master is not available\")\n                        diff_in_bytes = 0\n                        break\n            else:\n                # always try to use WAL-E if master connection string is not available\n                diff_in_bytes = 0\n            break\n\n        # if the size of the accumulated WAL segments is more than a certan percentage of the backup size\n        # or exceeds the pre-determined size - pg_basebackup is chosen instead.\n        is_size_thresh_ok = diff_in_bytes < int(threshold_megabytes) * 1048576\n        threshold_pct_bytes = backup_size * threshold_percent / 100.0\n        is_percentage_thresh_ok = float(diff_in_bytes) < int(threshold_pct_bytes)\n        are_thresholds_ok = is_size_thresh_ok and is_percentage_thresh_ok\n\n        class Size(object):\n            def __init__(self, n_bytes, prefix=None):\n                self.n_bytes = n_bytes\n                self.prefix = prefix\n\n            def __repr__(self):\n                if self.prefix is not None:\n                    n_bytes = size_as_bytes(self.n_bytes, self.prefix)\n                else:\n                    n_bytes = self.n_bytes\n                return repr_size(n_bytes)\n\n        class HumanContext(object):\n            def __init__(self, items):\n                self.items = items\n\n            def __repr__(self):\n                return ', '.join('{}={!r}'.format(key, value)\n                                 for key, value in self.items)\n\n        human_context = repr(HumanContext([\n            ('threshold_size', Size(threshold_megabytes, 'M')),\n            ('threshold_percent', threshold_percent),\n            ('threshold_percent_size', Size(threshold_pct_bytes)),\n            ('backup_size', Size(backup_size)),\n            ('backup_diff', Size(diff_in_bytes)),\n            ('is_size_thresh_ok', is_size_thresh_ok),\n            ('is_percentage_thresh_ok', is_percentage_thresh_ok),\n        ]))\n\n        if not are_thresholds_ok:\n            logger.info('wal-e backup size diff is over threshold, falling back '\n                        'to other means of restore: %s', human_context)\n        else:\n            logger.info('Thresholds are OK, using wal-e basebackup: %s', human_context)\n        return are_thresholds_ok"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef watching(w, watch, max_count=None, clear=True):\n\n    if w and not watch:\n        watch = 2\n    if watch and clear:\n        click.clear()\n    yield 0\n\n    if max_count is not None and max_count < 1:\n        return\n\n    counter = 1\n    while watch and counter <= (max_count or counter):\n        time.sleep(watch)\n        counter += 1\n        if clear:\n            click.clear()\n        yield 0", "response": "Yields the list of items that are watching w."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _do_failover_or_switchover(obj, action, cluster_name, master, candidate, force, scheduled=None):\n\n    dcs = get_dcs(obj, cluster_name)\n    cluster = dcs.get_cluster()\n\n    if action == 'switchover' and cluster.leader is None:\n        raise PatroniCtlException('This cluster has no master')\n\n    if master is None:\n        if force or action == 'failover':\n            master = cluster.leader and cluster.leader.name\n        else:\n            master = click.prompt('Master', type=str, default=cluster.leader.member.name)\n\n    if master is not None and cluster.leader and cluster.leader.member.name != master:\n        raise PatroniCtlException('Member {0} is not the leader of cluster {1}'.format(master, cluster_name))\n\n    # excluding members with nofailover tag\n    candidate_names = [str(m.name) for m in cluster.members if m.name != master and not m.nofailover]\n    # We sort the names for consistent output to the client\n    candidate_names.sort()\n\n    if not candidate_names:\n        raise PatroniCtlException('No candidates found to {0} to'.format(action))\n\n    if candidate is None and not force:\n        candidate = click.prompt('Candidate ' + str(candidate_names), type=str, default='')\n\n    if action == 'failover' and not candidate:\n        raise PatroniCtlException('Failover could be performed only to a specific candidate')\n\n    if candidate == master:\n        raise PatroniCtlException(action.title() + ' target and source are the same.')\n\n    if candidate and candidate not in candidate_names:\n        raise PatroniCtlException('Member {0} does not exist in cluster {1}'.format(candidate, cluster_name))\n\n    scheduled_at_str = None\n    scheduled_at = None\n\n    if action == 'switchover':\n        if scheduled is None and not force:\n            scheduled = click.prompt('When should the switchover take place (e.g. 2015-10-01T14:30) ',\n                                     type=str, default='now')\n\n        scheduled_at = parse_scheduled(scheduled)\n        if scheduled_at:\n            if cluster.is_paused():\n                raise PatroniCtlException(\"Can't schedule switchover in the paused state\")\n            scheduled_at_str = scheduled_at.isoformat()\n\n    failover_value = {'leader': master, 'candidate': candidate, 'scheduled_at': scheduled_at_str}\n\n    logging.debug(failover_value)\n\n    # By now we have established that the leader exists and the candidate exists\n    click.echo('Current cluster topology')\n    output_members(dcs.get_cluster(), cluster_name)\n\n    if not force:\n        demote_msg = ', demoting current master ' + master if master else ''\n\n        if not click.confirm('Are you sure you want to {0} cluster {1}{2}?'.format(action, cluster_name, demote_msg)):\n            raise PatroniCtlException('Aborting ' + action)\n\n    r = None\n    try:\n        member = cluster.leader.member if cluster.leader else cluster.get_member(candidate, False)\n\n        r = request_patroni(member, 'post', action, failover_value, auth_header(obj))\n\n        # probably old patroni, which doesn't support switchover yet\n        if r.status_code == 501 and action == 'switchover' and 'Server does not support this operation' in r.text:\n            r = request_patroni(member, 'post', 'failover', failover_value, auth_header(obj))\n\n        if r.status_code in (200, 202):\n            logging.debug(r)\n            cluster = dcs.get_cluster()\n            logging.debug(cluster)\n            click.echo('{0} {1}'.format(timestamp(), r.text))\n        else:\n            click.echo('{0} failed, details: {1}, {2}'.format(action.title(), r.status_code, r.text))\n            return\n    except Exception:\n        logging.exception(r)\n        logging.warning('Failing over to DCS')\n        click.echo('{0} Could not {1} using Patroni api, falling back to DCS'.format(timestamp(), action))\n        dcs.manual_failover(master, candidate, scheduled_at=scheduled_at)\n\n    output_members(cluster, cluster_name)", "response": "Internal helper function to perform failover or switchover actions."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrip - off of the ha. touch_member without inter - class dependencies", "response": "def touch_member(config, dcs):\n    ''' Rip-off of the ha.touch_member without inter-class dependencies '''\n    p = Postgresql(config['postgresql'])\n    p.set_state('running')\n    p.set_role('master')\n\n    def restapi_connection_string(config):\n        protocol = 'https' if config.get('certfile') else 'http'\n        connect_address = config.get('connect_address')\n        listen = config['listen']\n        return '{0}://{1}/patroni'.format(protocol, connect_address or listen)\n\n    data = {\n        'conn_url': p.connection_string,\n        'api_url': restapi_connection_string(config['restapi']),\n        'state': p.state,\n        'role': p.role\n    }\n\n    return dcs.touch_member(data, permanent=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfilling - in some basic configuration parameters if config file is not set", "response": "def set_defaults(config, cluster_name):\n    \"\"\"fill-in some basic configuration parameters if config file is not set \"\"\"\n    config['postgresql'].setdefault('name', cluster_name)\n    config['postgresql'].setdefault('scope', cluster_name)\n    config['postgresql'].setdefault('listen', '127.0.0.1')\n    config['postgresql']['authentication'] = {'replication': None}\n    config['restapi']['listen'] = ':' in config['restapi']['listen'] and config['restapi']['listen'] or '127.0.0.1:8008'"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a temporary file with specified contents that persists for the context.", "response": "def temporary_file(contents, suffix='', prefix='tmp'):\n    \"\"\"Creates a temporary file with specified contents that persists for the context.\n\n    :param contents: binary string that will be written to the file.\n    :param prefix: will be prefixed to the filename.\n    :param suffix: will be appended to the filename.\n    :returns path of the created file.\n    \"\"\"\n    tmp = tempfile.NamedTemporaryFile(suffix=suffix, prefix=prefix, delete=False)\n    with tmp:\n        tmp.write(contents)\n\n    try:\n        yield tmp.name\n    finally:\n        os.unlink(tmp.name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nshows a diff between two strings.", "response": "def show_diff(before_editing, after_editing):\n    \"\"\"Shows a diff between two strings.\n\n    If the output is to a tty the diff will be colored. Inputs are expected to be unicode strings.\n    \"\"\"\n    def listify(string):\n        return [l+'\\n' for l in string.rstrip('\\n').split('\\n')]\n\n    unified_diff = difflib.unified_diff(listify(before_editing), listify(after_editing))\n\n    if sys.stdout.isatty():\n        buf = io.StringIO()\n        for line in unified_diff:\n            # Force cast to unicode as difflib on Python 2.7 returns a mix of unicode and str.\n            buf.write(text_type(line))\n        buf.seek(0)\n\n        class opts:\n            side_by_side = False\n            width = 80\n            tab_width = 8\n        cdiff.markup_to_pager(cdiff.PatchStream(buf), opts)\n    else:\n        for line in unified_diff:\n            click.echo(line.rstrip('\\n'))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef format_config_for_editing(data):\n    return yaml.safe_dump(data, default_flow_style=False, encoding=None, allow_unicode=True)", "response": "Formats the configuration as YAML for human consumption."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef apply_config_changes(before_editing, data, kvpairs):\n    changed_data = copy.deepcopy(data)\n\n    def set_path_value(config, path, value, prefix=()):\n        # Postgresql GUCs can't be nested, but can contain dots so we re-flatten the structure for this case\n        if prefix == ('postgresql', 'parameters'):\n            path = ['.'.join(path)]\n\n        key = path[0]\n        if len(path) == 1:\n            if value is None:\n                config.pop(key, None)\n            else:\n                config[key] = value\n        else:\n            if not isinstance(config.get(key), dict):\n                config[key] = {}\n            set_path_value(config[key], path[1:], value, prefix + (key,))\n            if config[key] == {}:\n                del config[key]\n\n    for pair in kvpairs:\n        if not pair or \"=\" not in pair:\n            raise PatroniCtlException(\"Invalid parameter setting {0}\".format(pair))\n        key_path, value = pair.split(\"=\", 1)\n        set_path_value(changed_data, key_path.strip().split(\".\"), yaml.safe_load(value))\n\n    return format_config_for_editing(changed_data), changed_data", "response": "Applies config changes specified as a list of key - value pairs."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\napplies changes from a YAML file to the configuration datastructure", "response": "def apply_yaml_file(data, filename):\n    \"\"\"Applies changes from a YAML file to configuration\n\n    :param data: configuration datastructure\n    :param filename: name of the YAML file, - is taken to mean standard input\n    :returns tuple of human readable and parsed datastructure after changes\n    \"\"\"\n    changed_data = copy.deepcopy(data)\n\n    if filename == '-':\n        new_options = yaml.safe_load(sys.stdin)\n    else:\n        with open(filename) as fd:\n            new_options = yaml.safe_load(fd)\n\n    patch_config(changed_data, new_options)\n\n    return format_config_for_editing(changed_data), changed_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninvokes editor command to edit configuration in human readable format", "response": "def invoke_editor(before_editing, cluster_name):\n    \"\"\"Starts editor command to edit configuration in human readable format\n\n    :param before_editing: human representation before editing\n    :returns tuple of human readable and parsed datastructure after changes\n    \"\"\"\n    editor_cmd = os.environ.get('EDITOR')\n    if not editor_cmd:\n        raise PatroniCtlException('EDITOR environment variable is not set')\n\n    with temporary_file(contents=before_editing.encode('utf-8'),\n                        suffix='.yaml',\n                        prefix='{0}-config-'.format(cluster_name)) as tmpfile:\n        ret = subprocess.call([editor_cmd, tmpfile])\n        if ret:\n            raise PatroniCtlException(\"Editor exited with return code {0}\".format(ret))\n\n        with codecs.open(tmpfile, encoding='utf-8') as fd:\n            after_editing = fd.read()\n\n        return after_editing, yaml.safe_load(after_editing)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _tag_ebs(self, conn, role):\n        tags = {'Name': 'spilo_' + self.cluster_name, 'Role': role, 'Instance': self.instance_id}\n        volumes = conn.get_all_volumes(filters={'attachment.instance-id': self.instance_id})\n        conn.create_tags([v.id for v in volumes], tags)", "response": "set tags for the EBS storage"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _tag_ec2(self, conn, role):\n        tags = {'Role': role}\n        conn.create_tags([self.instance_id], tags)", "response": "tag the current EC2 instance with a cluster role"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef failover_limitation(self):\n        if not self.reachable:\n            return 'not reachable'\n        if self.tags.get('nofailover', False):\n            return 'not allowed to promote'\n        if self.watchdog_failed:\n            return 'not watchdog capable'\n        return None", "response": "Returns a string describing what the node can promote or None if everything is ok."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_effective_tags(self):\n        tags = self.patroni.tags.copy()\n        # _disable_sync could be modified concurrently, but we don't care as attribute get and set are atomic.\n        if self._disable_sync > 0:\n            tags['nosync'] = True\n        return tags", "response": "Return configuration tags merged with dynamically applied tags."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_sync_replication(self):\n        if self.is_synchronous_mode():\n            current = self.cluster.sync.leader and self.cluster.sync.sync_standby\n            picked, allow_promote = self.state_handler.pick_synchronous_standby(self.cluster)\n            if picked != current:\n                # We need to revoke privilege from current before replacing it in the config\n                if current:\n                    logger.info(\"Removing synchronous privilege from %s\", current)\n                    if not self.dcs.write_sync_state(self.state_handler.name, None, index=self.cluster.sync.index):\n                        logger.info('Synchronous replication key updated by someone else.')\n                        return\n\n                if self.is_synchronous_mode_strict() and picked is None:\n                    picked = '*'\n                    logger.warning(\"No standbys available!\")\n\n                logger.info(\"Assigning synchronous standby status to %s\", picked)\n                self.state_handler.set_synchronous_standby(picked)\n\n                if picked and picked != '*' and not allow_promote:\n                    # Wait for PostgreSQL to enable synchronous mode and see if we can immediately set sync_standby\n                    time.sleep(2)\n                    picked, allow_promote = self.state_handler.pick_synchronous_standby(self.cluster)\n                if allow_promote:\n                    try:\n                        cluster = self.dcs.get_cluster()\n                    except DCSError:\n                        return logger.warning(\"Could not get cluster state from DCS during process_sync_replication()\")\n                    if cluster.sync.leader and cluster.sync.leader != self.state_handler.name:\n                        logger.info(\"Synchronous replication key updated by someone else\")\n                        return\n                    if not self.dcs.write_sync_state(self.state_handler.name, picked, index=cluster.sync.index):\n                        logger.info(\"Synchronous replication key updated by someone else\")\n                        return\n                    logger.info(\"Synchronous standby status assigned to %s\", picked)\n        else:\n            if self.cluster.sync.leader and self.dcs.delete_sync_state(index=self.cluster.sync.index):\n                logger.info(\"Disabled synchronous replication\")\n            self.state_handler.set_synchronous_standby(None)", "response": "Process synchronous standby and update the sync_standby state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef while_not_sync_standby(self, func):\n\n        if not self.is_synchronous_mode() or self.patroni.nosync:\n            return func()\n\n        with self._member_state_lock:\n            self._disable_sync += 1\n        try:\n            if self.touch_member():\n                # Master should notice the updated value during the next cycle. We will wait double that, if master\n                # hasn't noticed the value by then not disabling sync replication is not likely to matter.\n                for _ in polling_loop(timeout=self.dcs.loop_wait*2, interval=2):\n                    try:\n                        if not self.is_sync_standby(self.dcs.get_cluster()):\n                            break\n                    except DCSError:\n                        logger.warning(\"Could not get cluster state, skipping synchronous standby disable\")\n                        break\n                    logger.info(\"Waiting for master to release us from synchronous standby\")\n            else:\n                logger.warning(\"Updating member state failed, skipping synchronous standby disable\")\n\n            return func()\n        finally:\n            with self._member_state_lock:\n                self._disable_sync -= 1", "response": "Runs specified action while the node is not assigned synchronous standby status."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if instance with an wal should be unhealthy due to replication lag.", "response": "def is_lagging(self, wal_position):\n        \"\"\"Returns if instance with an wal should consider itself unhealthy to be promoted due to replication lag.\n\n        :param wal_position: Current wal position.\n        :returns True when node is lagging\n        \"\"\"\n        lag = (self.cluster.last_leader_operation or 0) - wal_position\n        return lag > self.patroni.config.get('maximum_lag_on_failover', 0)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndemote PostgreSQL running as master.", "response": "def demote(self, mode):\n        \"\"\"Demote PostgreSQL running as master.\n\n        :param mode: One of offline, graceful or immediate.\n            offline is used when connection to DCS is not available.\n            graceful is used when failing over to another node due to user request. May only be called running async.\n            immediate is used when we determine that we are not suitable for master and want to failover quickly\n                without regard for data durability. May only be called synchronously.\n            immediate-nolock is used when find out that we have lost the lock to be master. Need to bring down\n                PostgreSQL as quickly as possible without regard for data durability. May only be called synchronously.\n        \"\"\"\n        mode_control = {\n            'offline':          dict(stop='fast', checkpoint=False, release=False, offline=True, async_req=False),\n            'graceful':         dict(stop='fast', checkpoint=True, release=True, offline=False, async_req=False),\n            'immediate':        dict(stop='immediate', checkpoint=False, release=True, offline=False, async_req=True),\n            'immediate-nolock': dict(stop='immediate', checkpoint=False, release=False, offline=False, async_req=True),\n        }[mode]\n\n        self.state_handler.trigger_check_diverged_lsn()\n        self.state_handler.stop(mode_control['stop'], checkpoint=mode_control['checkpoint'],\n                                on_safepoint=self.watchdog.disable if self.watchdog.is_running else None)\n        self.state_handler.set_role('demoted')\n        self.set_is_leader(False)\n\n        if mode_control['release']:\n            with self._async_executor:\n                self.release_leader_key_voluntarily()\n            time.sleep(2)  # Give a time to somebody to take the leader lock\n        if mode_control['offline']:\n            node_to_follow, leader = None, None\n        else:\n            cluster = self.dcs.get_cluster()\n            node_to_follow, leader = self._get_node_to_follow(cluster), cluster.leader\n\n        # FIXME: with mode offline called from DCS exception handler and handle_long_action_in_progress\n        # there could be an async action already running, calling follow from here will lead\n        # to racy state handler state updates.\n        if mode_control['async_req']:\n            self._async_executor.schedule('starting after demotion')\n            self._async_executor.run_async(self.state_handler.follow, (node_to_follow,))\n        else:\n            if self.is_synchronous_mode():\n                self.state_handler.set_synchronous_standby(None)\n            if self.state_handler.rewind_or_reinitialize_needed_and_possible(leader):\n                return False  # do not start postgres, but run pg_rewind on the next iteration\n            self.state_handler.follow(node_to_follow)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_manual_failover_from_leader(self):\n        failover = self.cluster.failover\n        if not failover or (self.is_paused() and not self.state_handler.is_leader()):\n            return\n\n        if (failover.scheduled_at and not\n            self.should_run_scheduled_action(\"failover\", failover.scheduled_at, lambda:\n                                             self.dcs.manual_failover('', '', index=failover.index))):\n            return\n\n        if not failover.leader or failover.leader == self.state_handler.name:\n            if not failover.candidate or failover.candidate != self.state_handler.name:\n                if not failover.candidate and self.is_paused():\n                    logger.warning('Failover is possible only to a specific candidate in a paused state')\n                else:\n                    if self.is_synchronous_mode():\n                        if failover.candidate and not self.cluster.sync.matches(failover.candidate):\n                            logger.warning('Failover candidate=%s does not match with sync_standby=%s',\n                                           failover.candidate, self.cluster.sync.sync_standby)\n                            members = []\n                        else:\n                            members = [m for m in self.cluster.members if self.cluster.sync.matches(m.name)]\n                    else:\n                        members = [m for m in self.cluster.members\n                                   if not failover.candidate or m.name == failover.candidate]\n                    if self.is_failover_possible(members):  # check that there are healthy members\n                        self._async_executor.schedule('manual failover: demote')\n                        self._async_executor.run_async(self.demote, ('graceful',))\n                        return 'manual failover: demoting myself'\n                    else:\n                        logger.warning('manual failover: no healthy members found, failover is not possible')\n            else:\n                logger.warning('manual failover: I am already the leader, no need to failover')\n        else:\n            logger.warning('manual failover: leader name does not match: %s != %s',\n                           failover.leader, self.state_handler.name)\n\n        logger.info('Cleaning up failover key')\n        self.dcs.manual_failover('', '', index=failover.index)", "response": "Checks if manual failover is requested and takes action if appropriate."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nclusters has no leader key", "response": "def process_unhealthy_cluster(self):\n        \"\"\"Cluster has no leader key\"\"\"\n\n        if self.is_healthiest_node():\n            if self.acquire_lock():\n                failover = self.cluster.failover\n                if failover:\n                    if self.is_paused() and failover.leader and failover.candidate:\n                        logger.info('Updating failover key after acquiring leader lock...')\n                        self.dcs.manual_failover('', failover.candidate, failover.scheduled_at, failover.index)\n                    else:\n                        logger.info('Cleaning up failover key after acquiring leader lock...')\n                        self.dcs.manual_failover('', '')\n                self.load_cluster_from_dcs()\n\n                if self.is_standby_cluster():\n                    # standby leader disappeared, and this is a healthiest\n                    # replica, so it should become a new standby leader.\n                    # This imply that we need to start following a remote master\n                    msg = 'promoted self to a standby leader by acquiring session lock'\n                    return self.enforce_follow_remote_master(msg)\n                else:\n                    return self.enforce_master_role(\n                        'acquired session lock as a leader',\n                        'promoted self to leader by acquiring session lock'\n                    )\n            else:\n                return self.follow('demoted self after trying and failing to obtain lock',\n                                   'following new leader after trying and failing to obtain lock')\n        else:\n            # when we are doing manual failover there is no guaranty that new leader is ahead of any other node\n            # node tagged as nofailover can be ahead of the new leader either, but it is always excluded from elections\n            if bool(self.cluster.failover) or self.patroni.nofailover:\n                self.state_handler.trigger_check_diverged_lsn()\n                time.sleep(2)  # Give a time to somebody to take the leader lock\n\n            if self.patroni.nofailover:\n                return self.follow('demoting self because I am not allowed to become master',\n                                   'following a different leader because I am not allowed to promote')\n            return self.follow('demoting self because i am not the healthiest node',\n                               'following a different leader because i am not the healthiest node')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef restart(self, restart_data, run_async=False):\n        assert isinstance(restart_data, dict)\n\n        if (not self.restart_matches(restart_data.get('role'),\n                                     restart_data.get('postgres_version'),\n                                     ('restart_pending' in restart_data))):\n            return (False, \"restart conditions are not satisfied\")\n\n        with self._async_executor:\n            prev = self._async_executor.schedule('restart')\n            if prev is not None:\n                return (False, prev + ' already in progress')\n\n            # Make the main loop to think that we were recovering dead postgres. If we fail\n            # to start postgres after a specified timeout (see below), we need to remove\n            # leader key (if it belong to us) rather than trying to start postgres once again.\n            self.recovering = True\n\n        # Now that restart is scheduled we can set timeout for startup, it will get reset\n        # once async executor runs and main loop notices PostgreSQL as up.\n        timeout = restart_data.get('timeout', self.patroni.config['master_start_timeout'])\n        self.set_start_timeout(timeout)\n\n        # For non async cases we want to wait for restart to complete or timeout before returning.\n        do_restart = functools.partial(self.state_handler.restart, timeout, self._async_executor.critical_task)\n        if self.is_synchronous_mode() and not self.has_lock():\n            do_restart = functools.partial(self.while_not_sync_standby, do_restart)\n\n        if run_async:\n            self._async_executor.run_async(do_restart)\n            return (True, 'restart initiated')\n        else:\n            res = self._async_executor.run(do_restart)\n            if res:\n                return (True, 'restarted successfully')\n            elif res is None:\n                return (False, 'postgres is still starting')\n            else:\n                return (False, 'restart failed')", "response": "Restart the dead postgres."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_starting_instance(self):\n\n        # Check if we are in startup, when paused defer to main loop for manual failovers.\n        if not self.state_handler.check_for_startup() or self.is_paused():\n            self.set_start_timeout(None)\n            if self.is_paused():\n                self.state_handler.set_state(self.state_handler.is_running() and 'running' or 'stopped')\n            return None\n\n        # state_handler.state == 'starting' here\n        if self.has_lock():\n            if not self.update_lock():\n                logger.info(\"Lost lock while starting up. Demoting self.\")\n                self.demote('immediate-nolock')\n                return 'stopped PostgreSQL while starting up because leader key was lost'\n\n            timeout = self._start_timeout or self.patroni.config['master_start_timeout']\n            time_left = timeout - self.state_handler.time_in_state()\n\n            if time_left <= 0:\n                if self.is_failover_possible(self.cluster.members):\n                    logger.info(\"Demoting self because master startup is taking too long\")\n                    self.demote('immediate')\n                    return 'stopped PostgreSQL because of startup timeout'\n                else:\n                    return 'master start has timed out, but continuing to wait because failover is not possible'\n            else:\n                msg = self.process_manual_failover_from_leader()\n                if msg is not None:\n                    return msg\n\n                return 'PostgreSQL is still starting up, {0:.0f} seconds until timeout'.format(time_left)\n        else:\n            # Use normal processing for standbys\n            logger.info(\"Still starting up as a standby.\")\n            return None", "response": "This method handles the starting instance of a PostgreSQL instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_remote_member(self, member=None):\n        cluster_params = self.get_standby_cluster_config()\n\n        if cluster_params:\n            name = member.name if member else 'remote_master:{}'.format(uuid.uuid1())\n\n            data = {k: v for k, v in cluster_params.items() if k in RemoteMember.allowed_keys()}\n            data['no_replication_slot'] = 'primary_slot_name' not in cluster_params\n            conn_kwargs = member.conn_kwargs() if member else \\\n                {k: cluster_params[k] for k in ('host', 'port') if k in cluster_params}\n            if conn_kwargs:\n                data['conn_kwargs'] = conn_kwargs\n\n            return RemoteMember(name, data)", "response": "Returns RemoteMember object for given member."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_machines_cache_from_srv(self, srv):\n\n        ret = []\n        for r in ['-client-ssl', '-client', '-ssl', '', '-server-ssl', '-server']:\n            protocol = 'https' if '-ssl' in r else 'http'\n            endpoint = '/members' if '-server' in r else ''\n            for host, port in self.get_srv_record('_etcd{0}._tcp.{1}'.format(r, srv)):\n                url = uri(protocol, host, port, endpoint)\n                if endpoint:\n                    try:\n                        response = requests.get(url, timeout=self.read_timeout, verify=False)\n                        if response.ok:\n                            for member in response.json():\n                                ret.extend(member['clientURLs'])\n                            break\n                    except RequestException:\n                        logger.exception('GET %s', url)\n                else:\n                    ret.append(url)\n            if ret:\n                self._protocol = protocol\n                break\n        else:\n            logger.warning('Can not resolve SRV for %s', srv)\n        return list(set(ret))", "response": "Fetch list of machines cache entries by resolving _etcd - server. _tcp. SRV record."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a list of machines cache entries from a DNS host and port.", "response": "def _get_machines_cache_from_dns(self, host, port):\n        \"\"\"One host might be resolved into multiple ip addresses. We will make list out of it\"\"\"\n        if self.protocol == 'http':\n            ret = []\n            for af, _, _, _, sa in self._dns_resolver.resolve(host, port):\n                host, port = sa[:2]\n                if af == socket.AF_INET6:\n                    host = '[{0}]'.format(host)\n                ret.append(uri(self.protocol, host, port))\n            if ret:\n                return list(set(ret))\n        return [uri(self.protocol, host, port)]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding and executes pg_ctl command", "response": "def pg_ctl(self, cmd, *args, **kwargs):\n        \"\"\"Builds and executes pg_ctl command\n\n        :returns: `!True` when return_code == 0, otherwise `!False`\"\"\"\n\n        pg_ctl = [self._pgcommand('pg_ctl'), cmd]\n        return subprocess.call(pg_ctl + ['-D', self._data_dir] + list(args), **kwargs) == 0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pg_isready(self):\n\n        cmd = [self._pgcommand('pg_isready'), '-p', self._local_address['port'], '-d', self._database]\n\n        # Host is not set if we are connecting via default unix socket\n        if 'host' in self._local_address:\n            cmd.extend(['-h', self._local_address['host']])\n\n        # We only need the username because pg_isready does not try to authenticate\n        if 'username' in self._superuser:\n            cmd.extend(['-U', self._superuser['username']])\n\n        ret = subprocess.call(cmd)\n        return_codes = {0: STATE_RUNNING,\n                        1: STATE_REJECT,\n                        2: STATE_NO_RESPONSE,\n                        3: STATE_UNKNOWN}\n        return return_codes.get(ret, STATE_UNKNOWN)", "response": "Runs pg_isready to see if PostgreSQL is accepting connections."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef can_rewind(self):\n        # low-hanging fruit: check if pg_rewind configuration is there\n        if not self.config.get('use_pg_rewind'):\n            return False\n\n        cmd = [self._pgcommand('pg_rewind'), '--help']\n        try:\n            ret = subprocess.call(cmd, stdout=open(os.devnull, 'w'), stderr=subprocess.STDOUT)\n            if ret != 0:  # pg_rewind is not there, close up the shop and go home\n                return False\n        except OSError:\n            return False\n        return self.configuration_allows_rewind(self.controldata())", "response": "check if pg_rewind executable is there and that wal_log_hints and checksums turned on\n            return True if it is possible to revert the archive"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes a query and return the cursor.", "response": "def _query(self, sql, *params):\n        \"\"\"We are always using the same cursor, therefore this method is not thread-safe!!!\n        You can call it from different threads only if you are holding explicit `AsyncExecutor` lock,\n        because the main thread is always holding this lock when running HA cycle.\"\"\"\n        cursor = None\n        try:\n            cursor = self._cursor()\n            cursor.execute(sql, params)\n            return cursor\n        except psycopg2.Error as e:\n            if cursor and cursor.connection.closed == 0:\n                # When connected via unix socket, psycopg2 can't recoginze 'connection lost'\n                # and leaves `_cursor_holder.connection.closed == 0`, but psycopg2.OperationalError\n                # is still raised (what is correct). It doesn't make sense to continiue with existing\n                # connection and we will close it, to avoid its reuse by the `_cursor` method.\n                if isinstance(e, psycopg2.OperationalError):\n                    self.close_connection()\n                else:\n                    raise e\n            if self.state == 'restarting':\n                raise RetryFailedError('cluster is being restarted')\n            raise PostgresConnectionException('connection problems')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_bootstrap_post_init(self, config):\n        cmd = config.get('post_bootstrap') or config.get('post_init')\n        if cmd:\n            r = self._local_connect_kwargs\n\n            if 'host' in r:\n                # '/tmp' => '%2Ftmp' for unix socket path\n                host = quote_plus(r['host']) if r['host'].startswith('/') else r['host']\n            else:\n                host = ''\n\n                # https://www.postgresql.org/docs/current/static/libpq-pgpass.html\n                # A host name of localhost matches both TCP (host name localhost) and Unix domain socket\n                # (pghost empty or the default socket directory) connections coming from the local machine.\n                r['host'] = 'localhost'  # set it to localhost to write into pgpass\n\n            if 'user' in r:\n                user = r['user'] + '@'\n            else:\n                user = ''\n                if 'password' in r:\n                    import getpass\n                    r.setdefault('user', os.environ.get('PGUSER', getpass.getuser()))\n\n            connstring = 'postgres://{0}{1}:{2}/{3}'.format(user, host, r['port'], r['database'])\n            env = self.write_pgpass(r) if 'password' in r else None\n\n            try:\n                ret = self.cancellable_subprocess_call(shlex.split(cmd) + [connstring], env=env)\n            except OSError:\n                logger.error('post_init script %s failed', cmd)\n                return False\n            if ret != 0:\n                logger.error('post_init script %s returned non-zero code %d', cmd, ret)\n                return False\n        return True", "response": "Runs a script after initdb or custom bootstrap script is called and waits until completion."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if there is one that does not require a replication connection.", "response": "def can_create_replica_without_replication_connection(self):\n        \"\"\" go through the replication methods to see if there are ones\n            that does not require a working replication connection.\n        \"\"\"\n        replica_methods = self._create_replica_methods\n        return any(self.replica_method_can_work_without_replication_connection(method) for method in replica_methods)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_replica(self, clone_member):\n\n        self.set_state('creating replica')\n        self._sysid = None\n\n        is_remote_master = isinstance(clone_member, RemoteMember)\n        create_replica_methods = is_remote_master and clone_member.create_replica_methods\n\n        # get list of replica methods either from clone member or from\n        # the config. If there is no configuration key, or no value is\n        # specified, use basebackup\n        replica_methods = (\n            create_replica_methods\n            or self._create_replica_methods\n            or ['basebackup']\n        )\n\n        if clone_member and clone_member.conn_url:\n            r = clone_member.conn_kwargs(self._replication)\n            connstring = 'postgres://{user}@{host}:{port}/{database}'.format(**r)\n            # add the credentials to connect to the replica origin to pgpass.\n            env = self.write_pgpass(r)\n        else:\n            connstring = ''\n            env = os.environ.copy()\n            # if we don't have any source, leave only replica methods that work without it\n            replica_methods = \\\n                [r for r in replica_methods if self.replica_method_can_work_without_replication_connection(r)]\n\n        # go through them in priority order\n        ret = 1\n        for replica_method in replica_methods:\n            with self._cancellable_lock:\n                if self._is_cancelled:\n                    break\n            # if the method is basebackup, then use the built-in\n            if replica_method == \"basebackup\":\n                ret = self.basebackup(connstring, env, self.config.get(replica_method, {}))\n                if ret == 0:\n                    logger.info(\"replica has been created using basebackup\")\n                    # if basebackup succeeds, exit with success\n                    break\n            else:\n                if not self.data_directory_empty():\n                    if self.config.get(replica_method, {}).get('keep_data', False):\n                        logger.info('Leaving data directory uncleaned')\n                    else:\n                        self.remove_data_directory()\n\n                cmd = replica_method\n                method_config = {}\n                # user-defined method; check for configuration\n                # not required, actually\n                if self.config.get(replica_method, {}):\n                    method_config = self.config[replica_method].copy()\n                    # look to see if the user has supplied a full command path\n                    # if not, use the method name as the command\n                    cmd = method_config.pop('command', cmd)\n\n                # add the default parameters\n                if not method_config.get('no_params', False):\n                    method_config.update({\"scope\": self.scope,\n                                          \"role\": \"replica\",\n                                          \"datadir\": self._data_dir,\n                                          \"connstring\": connstring})\n                else:\n                    for param in ('no_params', 'no_master', 'keep_data'):\n                        method_config.pop(param, None)\n                params = [\"--{0}={1}\".format(arg, val) for arg, val in method_config.items()]\n                try:\n                    # call script with the full set of parameters\n                    ret = self.cancellable_subprocess_call(shlex.split(cmd) + params, env=env)\n                    # if we succeeded, stop\n                    if ret == 0:\n                        logger.info('replica has been created using %s', replica_method)\n                        break\n                    else:\n                        logger.error('Error creating replica using method %s: %s exited with code=%s',\n                                     replica_method, cmd, ret)\n                except Exception:\n                    logger.exception('Error creating replica using method %s', replica_method)\n                    ret = 1\n\n        self.set_state('stopped')\n        return ret", "response": "create the replica according to the replica_method_can_work_without_replication_connection method."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_running(self):\n        if self._postmaster_proc:\n            if self._postmaster_proc.is_running():\n                return self._postmaster_proc\n            self._postmaster_proc = None\n\n        # we noticed that postgres was restarted, force syncing of replication\n        self._schedule_load_slots = self.use_slots\n\n        self._postmaster_proc = PostmasterProcess.from_pidfile(self._data_dir)\n        return self._postmaster_proc", "response": "Returns the most recently seen process\n        if one is running or None. If no process is running updates the cached process based on pid file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall a callback command without waiting for it to finish", "response": "def call_nowait(self, cb_name):\n        \"\"\" pick a callback command and call it without waiting for it to finish \"\"\"\n        if self.bootstrapping:\n            return\n        if cb_name in (ACTION_ON_START, ACTION_ON_STOP, ACTION_ON_RESTART, ACTION_ON_ROLE_CHANGE):\n            self.__cb_called = True\n\n        if self.callback and cb_name in self.callback:\n            cmd = self.callback[cb_name]\n            try:\n                cmd = shlex.split(self.callback[cb_name]) + [cb_name, self.role, self.scope]\n                self._callback_executor.call(cmd)\n            except Exception:\n                logger.exception('callback %s %s %s %s failed', cmd, cb_name, self.role, self.scope)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wait_for_port_open(self, postmaster, timeout):\n        for _ in polling_loop(timeout):\n            with self._cancellable_lock:\n                if self._is_cancelled:\n                    return False\n\n            if not postmaster.is_running():\n                logger.error('postmaster is not running')\n                self.set_state('start failed')\n                return False\n\n            isready = self.pg_isready()\n            if isready != STATE_NO_RESPONSE:\n                if isready not in [STATE_REJECT, STATE_RUNNING]:\n                    logger.warning(\"Can't determine PostgreSQL startup status, assuming running\")\n                return True\n\n        logger.warning(\"Timed out waiting for PostgreSQL to start\")\n        return False", "response": "Waits until PostgreSQL opens ports. Returns True if successful False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding the effective configuration dictionary from the server parameters and controldata.", "response": "def _build_effective_configuration(self):\n        \"\"\"It might happen that the current value of one (or more) below parameters stored in\n        the controldata is higher than the value stored in the global cluster configuration.\n\n        Example: max_connections in global configuration is 100, but in controldata\n        `Current max_connections setting: 200`. If we try to start postgres with\n        max_connections=100, it will immediately exit.\n        As a workaround we will start it with the values from controldata and set `pending_restart`\n        to true as an indicator that current values of parameters are not matching expectations.\"\"\"\n\n        OPTIONS_MAPPING = {\n            'max_connections': 'max_connections setting',\n            'max_prepared_transactions': 'max_prepared_xacts setting',\n            'max_locks_per_transaction': 'max_locks_per_xact setting'\n        }\n\n        if self._major_version >= 90400:\n            OPTIONS_MAPPING['max_worker_processes'] = 'max_worker_processes setting'\n\n        data = self.controldata()\n        effective_configuration = self._server_parameters.copy()\n\n        for name, cname in OPTIONS_MAPPING.items():\n            value = parse_int(effective_configuration[name])\n            cvalue = parse_int(data[cname])\n            if cvalue > value:\n                effective_configuration[name] = cvalue\n                self._pending_restart = True\n        return effective_configuration"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting a PostgreSQL server.", "response": "def start(self, timeout=None, task=None, block_callbacks=False, role=None):\n        \"\"\"Start PostgreSQL\n\n        Waits for postmaster to open ports or terminate so pg_isready can be used to check startup completion\n        or failure.\n\n        :returns: True if start was initiated and postmaster ports are open, False if start failed\"\"\"\n        # make sure we close all connections established against\n        # the former node, otherwise, we might get a stalled one\n        # after kill -9, which would report incorrect data to\n        # patroni.\n        self.close_connection()\n\n        if self.is_running():\n            logger.error('Cannot start PostgreSQL because one is already running.')\n            self.set_state('starting')\n            return True\n\n        if not block_callbacks:\n            self.__cb_pending = ACTION_ON_START\n\n        self.set_role(role or self.get_postgres_role_from_data_directory())\n\n        self.set_state('starting')\n        self._pending_restart = False\n\n        configuration = self._server_parameters if self.role == 'master' else self._build_effective_configuration()\n        self._write_postgresql_conf(configuration)\n        self.resolve_connection_addresses()\n        self._replace_pg_hba()\n        self._replace_pg_ident()\n\n        options = ['--{0}={1}'.format(p, configuration[p]) for p in self.CMDLINE_OPTIONS\n                   if p in configuration and p != 'wal_keep_segments']\n\n        with self._cancellable_lock:\n            if self._is_cancelled:\n                return False\n\n        with task or null_context():\n            if task and task.is_cancelled:\n                logger.info(\"PostgreSQL start cancelled.\")\n                return False\n\n            self._postmaster_proc = PostmasterProcess.start(self._pgcommand('postgres'),\n                                                            self._data_dir,\n                                                            self._postgresql_conf,\n                                                            options)\n\n            if task:\n                task.complete(self._postmaster_proc)\n\n        start_timeout = timeout\n        if not start_timeout:\n            try:\n                start_timeout = float(self.config.get('pg_ctl_timeout', 60))\n            except ValueError:\n                start_timeout = 60\n\n        # We want postmaster to open ports before we continue\n        if not self._postmaster_proc or not self.wait_for_port_open(self._postmaster_proc, start_timeout):\n            return False\n\n        ret = self.wait_for_startup(start_timeout)\n        if ret is not None:\n            return ret\n        elif timeout is not None:\n            return False\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstops the PostgreSQL node.", "response": "def stop(self, mode='fast', block_callbacks=False, checkpoint=None, on_safepoint=None):\n        \"\"\"Stop PostgreSQL\n\n        Supports a callback when a safepoint is reached. A safepoint is when no user backend can return a successful\n        commit to users. Currently this means we wait for user backends to close. But in the future alternate mechanisms\n        could be added.\n\n        :param on_safepoint: This callback is called when no user backends are running.\n        \"\"\"\n        if checkpoint is None:\n            checkpoint = False if mode == 'immediate' else True\n\n        success, pg_signaled = self._do_stop(mode, block_callbacks, checkpoint, on_safepoint)\n        if success:\n            # block_callbacks is used during restart to avoid\n            # running start/stop callbacks in addition to restart ones\n            if not block_callbacks:\n                self.set_state('stopped')\n                if pg_signaled:\n                    self.call_nowait(ACTION_ON_STOP)\n        else:\n            logger.warning('pg_ctl stop failed')\n            self.set_state('stop failed')\n        return success"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_startup_state_changed(self):\n        ready = self.pg_isready()\n\n        if ready == STATE_REJECT:\n            return False\n        elif ready == STATE_NO_RESPONSE:\n            self.set_state('start failed')\n            self._schedule_load_slots = False  # TODO: can remove this?\n            if not self._running_custom_bootstrap:\n                self.save_configuration_files()  # TODO: maybe remove this?\n            return True\n        else:\n            if ready != STATE_RUNNING:\n                # Bad configuration or unexpected OS error. No idea of PostgreSQL status.\n                # Let the main loop of run cycle clean up the mess.\n                logger.warning(\"%s status returned from pg_isready\",\n                               \"Unknown\" if ready == STATE_UNKNOWN else \"Invalid\")\n            self.set_state('running')\n            self._schedule_load_slots = self.use_slots\n            if not self._running_custom_bootstrap:\n                self.save_configuration_files()\n            # TODO: __cb_pending can be None here after PostgreSQL restarts on its own. Do we want to call the callback?\n            # Previously we didn't even notice.\n            action = self.__cb_pending or ACTION_ON_START\n            self.call_nowait(action)\n            self.__cb_pending = None\n\n            return True", "response": "Checks if PostgreSQL has completed starting up or failed or still starting. Returns True if state was changed from starting False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwait for PostgreSQL startup to complete or fail.", "response": "def wait_for_startup(self, timeout=None):\n        \"\"\"Waits for PostgreSQL startup to complete or fail.\n\n        :returns: True if start was successful, False otherwise\"\"\"\n        if not self.is_starting():\n            # Should not happen\n            logger.warning(\"wait_for_startup() called when not in starting state\")\n\n        while not self.check_startup_state_changed():\n            with self._cancellable_lock:\n                if self._is_cancelled:\n                    return None\n            if timeout and self.time_in_state() > timeout:\n                return None\n            time.sleep(1)\n\n        return self.state == 'running'"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef restart(self, timeout=None, task=None, block_callbacks=False, role=None):\n        self.set_state('restarting')\n        if not block_callbacks:\n            self.__cb_pending = ACTION_ON_RESTART\n        ret = self.stop(block_callbacks=True) and self.start(timeout, task, True, role)\n        if not ret and not self.is_starting():\n            self.set_state('restart failed ({0})'.format(self.state))\n        return ret", "response": "Restarts the PostgreSQL instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _replace_pg_hba(self):\n\n        # when we are doing custom bootstrap we assume that we don't know superuser password\n        # and in order to be able to change it, we are opening trust access from a certain address\n        if self._running_custom_bootstrap:\n            addresses = {'': 'local'}\n            if 'host' in self._local_address and not self._local_address['host'].startswith('/'):\n                for _, _, _, _, sa in socket.getaddrinfo(self._local_address['host'], self._local_address['port'],\n                                                         0, socket.SOCK_STREAM, socket.IPPROTO_TCP):\n                    addresses[sa[0] + '/32'] = 'host'\n\n            with open(self._pg_hba_conf, 'w') as f:\n                f.write(self._CONFIG_WARNING_HEADER)\n                for address, t in addresses.items():\n                    f.write((\n                        '{0}\\treplication\\t{1}\\t{3}\\ttrust\\n'\n                        '{0}\\tall\\t{2}\\t{3}\\ttrust\\n'\n                    ).format(t, self._replication['username'], self._superuser.get('username') or 'all', address))\n        elif not self._server_parameters.get('hba_file') and self.config.get('pg_hba'):\n            with open(self._pg_hba_conf, 'w') as f:\n                f.write(self._CONFIG_WARNING_HEADER)\n                for line in self.config['pg_hba']:\n                    f.write('{0}\\n'.format(line))\n            return True", "response": "Replace pg_hba. conf content in PGDATA if pg_hba. conf is not defined in the postgresql. parameters section and pg_hba is defined in the postgresql. parameters section."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreplace pg_ident. conf content in the PGDATA file.", "response": "def _replace_pg_ident(self):\n        \"\"\"\n        Replace pg_ident.conf content in the PGDATA if ident_file is not defined in the\n        `postgresql.parameters` and pg_ident is defined in the `postgresql` section.\n\n        :returns: True if pg_ident.conf was rewritten.\n        \"\"\"\n\n        if not self._server_parameters.get('ident_file') and self.config.get('pg_ident'):\n            with open(self._pg_ident_conf, 'w') as f:\n                f.write(self._CONFIG_WARNING_HEADER)\n                for line in self.config['pg_ident']:\n                    f.write('{0}\\n'.format(line))\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the contents of pg_controldata or non - True value if pg_controldata call failed", "response": "def controldata(self):\n        \"\"\" return the contents of pg_controldata, or non-True value if pg_controldata call failed \"\"\"\n        result = {}\n        # Don't try to call pg_controldata during backup restore\n        if self._version_file_exists() and self.state != 'creating replica':\n            try:\n                env = {'LANG': 'C', 'LC_ALL': 'C', 'PATH': os.getenv('PATH')}\n                if os.getenv('SYSTEMROOT') is not None:\n                    env['SYSTEMROOT'] = os.getenv('SYSTEMROOT')\n                data = subprocess.check_output([self._pgcommand('pg_controldata'), self._data_dir], env=env)\n                if data:\n                    data = data.decode('utf-8').splitlines()\n                    # pg_controldata output depends on major verion. Some of parameters are prefixed by 'Current '\n                    result = {l.split(':')[0].replace('Current ', '', 1): l.split(':', 1)[1].strip() for l in data\n                              if l and ':' in l}\n            except subprocess.CalledProcessError:\n                logger.exception(\"Error when calling pg_controldata\")\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_configuration_files(self):\n        try:\n            for f in self._configuration_to_save:\n                config_file = os.path.join(self._config_dir, f)\n                backup_file = os.path.join(self._data_dir, f + '.backup')\n                if os.path.isfile(config_file):\n                    shutil.copy(config_file, backup_file)\n        except IOError:\n            logger.exception('unable to create backup copies of configuration files')\n        return True", "response": "save the configuration files to the backup files"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrestores a previously saved postgresql. conf file", "response": "def restore_configuration_files(self):\n        \"\"\" restore a previously saved postgresql.conf \"\"\"\n        try:\n            for f in self._configuration_to_save:\n                config_file = os.path.join(self._config_dir, f)\n                backup_file = os.path.join(self._data_dir, f + '.backup')\n                if not os.path.isfile(config_file):\n                    if os.path.isfile(backup_file):\n                        shutil.copy(backup_file, config_file)\n                    # Previously we didn't backup pg_ident.conf, if file is missing just create empty\n                    elif f == 'pg_ident.conf':\n                        open(config_file, 'w').close()\n        except IOError:\n            logger.exception('unable to restore configuration files from backup')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclone the master or replica", "response": "def clone(self, clone_member):\n        \"\"\"\n             - initialize the replica from an existing member (master or replica)\n             - initialize the replica using the replica creation method that\n               works without the replication connection (i.e. restore from on-disk\n               base backup)\n        \"\"\"\n\n        self._rewind_state = REWIND_STATUS.INITIAL\n        ret = self.create_replica(clone_member) == 0\n        if ret:\n            self._post_restore()\n            self._configure_server_parameters()\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bootstrap(self, config):\n        pg_hba = config.get('pg_hba', [])\n        method = config.get('method') or 'initdb'\n        self._running_custom_bootstrap = method != 'initdb' and method in config and 'command' in config[method]\n        if self._running_custom_bootstrap:\n            do_initialize = self._custom_bootstrap\n            config = config[method]\n        else:\n            do_initialize = self._initdb\n        return do_initialize(config) and self.append_pg_hba(pg_hba) and self.save_configuration_files() \\\n            and self._configure_server_parameters() and self.start()", "response": "Initialize a new node from scratch and start it."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pick_synchronous_standby(self, cluster):\n        current = cluster.sync.sync_standby\n        current = current.lower() if current else current\n        members = {m.name.lower(): m for m in cluster.members}\n        candidates = []\n        # Pick candidates based on who has flushed WAL farthest.\n        # TODO: for synchronous_commit = remote_write we actually want to order on write_location\n        for app_name, state, sync_state in self.query(\n                \"SELECT pg_catalog.lower(application_name), state, sync_state\"\n                \" FROM pg_catalog.pg_stat_replication\"\n                \" ORDER BY flush_{0} DESC\".format(self.lsn_name)):\n            member = members.get(app_name)\n            if state != 'streaming' or not member or member.tags.get('nosync', False):\n                continue\n            if sync_state == 'sync':\n                return app_name, True\n            if sync_state == 'potential' and app_name == current:\n                # Prefer current even if not the best one any more to avoid indecisivness and spurious swaps.\n                return current, False\n            if sync_state in ('async', 'potential'):\n                candidates.append(app_name)\n\n        if candidates:\n            return candidates[0], False\n        return None, False", "response": "Returns the best candidate to be the synchronous standby."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_synchronous_standby(self, name):\n        if name and name != '*':\n            name = quote_ident(name)\n        if name != self._synchronous_standby_names:\n            if name is None:\n                self._server_parameters.pop('synchronous_standby_names', None)\n            else:\n                self._server_parameters['synchronous_standby_names'] = name\n            self._synchronous_standby_names = name\n            if self.state == 'running':\n                self._write_postgresql_conf()\n                self.reload()", "response": "Sets a node to be synchronous standby and if changed does a reload for PostgreSQL."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting the server_version to integer", "response": "def postgres_version_to_int(pg_version):\n        \"\"\"Convert the server_version to integer\n\n        >>> Postgresql.postgres_version_to_int('9.5.3')\n        90503\n        >>> Postgresql.postgres_version_to_int('9.3.13')\n        90313\n        >>> Postgresql.postgres_version_to_int('10.1')\n        100001\n        >>> Postgresql.postgres_version_to_int('10')  # doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last):\n            ...\n        PostgresException: 'Invalid PostgreSQL version format: X.Y or X.Y.Z is accepted: 10'\n        >>> Postgresql.postgres_version_to_int('9.6')  # doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last):\n            ...\n        PostgresException: 'Invalid PostgreSQL version format: X.Y or X.Y.Z is accepted: 9.6'\n        >>> Postgresql.postgres_version_to_int('a.b.c')  # doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last):\n            ...\n        PostgresException: 'Invalid PostgreSQL version: a.b.c'\n        \"\"\"\n\n        try:\n            components = list(map(int, pg_version.split('.')))\n        except ValueError:\n            raise PostgresException('Invalid PostgreSQL version: {0}'.format(pg_version))\n\n        if len(components) < 2 or len(components) == 2 and components[0] < 10 or len(components) > 3:\n            raise PostgresException('Invalid PostgreSQL version format: X.Y or X.Y.Z is accepted: {0}'\n                                    .format(pg_version))\n\n        if len(components) == 2:\n            # new style verion numbers, i.e. 10.1 becomes 100001\n            components.insert(1, 0)\n\n        return int(''.join('{0:02d}'.format(c) for c in components))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the list of option names and values from postgres. opts Empty dict if read failed or no file", "response": "def read_postmaster_opts(self):\n        \"\"\"returns the list of option names/values from postgres.opts, Empty dict if read failed or no file\"\"\"\n        result = {}\n        try:\n            with open(os.path.join(self._data_dir, 'postmaster.opts')) as f:\n                data = f.read()\n                for opt in data.split('\" \"'):\n                    if '=' in opt and opt.startswith('--'):\n                        name, val = opt.split('=', 1)\n                        result[name.strip('-')] = val.rstrip('\"\\n')\n        except IOError:\n            logger.exception('Error when reading postmaster.opts')\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef single_user_mode(self, command=None, options=None):\n        cmd = [self._pgcommand('postgres'), '--single', '-D', self._data_dir]\n        for opt, val in sorted((options or {}).items()):\n            cmd.extend(['-c', '{0}={1}'.format(opt, val)])\n        # need a database name to connect\n        cmd.append(self._database)\n        return self.cancellable_subprocess_call(cmd, communicate_input=command)", "response": "run a given command in a single - user mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef deep_compare(obj1, obj2):\n\n    if set(list(obj1.keys())) != set(list(obj2.keys())):  # Objects have different sets of keys\n        return False\n\n    for key, value in obj1.items():\n        if isinstance(value, dict):\n            if not (isinstance(obj2[key], dict) and deep_compare(value, obj2[key])):\n                return False\n        elif str(value) != str(obj2[key]):\n            return False\n    return True", "response": "Deep compare two objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_bool(value):\n    value = str(value).lower()\n    if value in ('on', 'true', 'yes', '1'):\n        return True\n    if value in ('off', 'false', 'no', '0'):\n        return False", "response": "parse_bool - Parse boolean value into a tuple of True False and False."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse an integer into a base_unit.", "response": "def parse_int(value, base_unit=None):\n    \"\"\"\n    >>> parse_int('1') == 1\n    True\n    >>> parse_int(' 0x400 MB ', '16384kB') == 64\n    True\n    >>> parse_int('1MB', 'kB') == 1024\n    True\n    >>> parse_int('1000 ms', 's') == 1\n    True\n    >>> parse_int('1GB', 'MB') is None\n    True\n    >>> parse_int(0) == 0\n    True\n    \"\"\"\n\n    convert = {\n        'kB': {'kB': 1, 'MB': 1024, 'GB': 1024 * 1024, 'TB': 1024 * 1024 * 1024},\n        'ms': {'ms': 1, 's': 1000, 'min': 1000 * 60, 'h': 1000 * 60 * 60, 'd': 1000 * 60 * 60 * 24},\n        's': {'ms': -1000, 's': 1, 'min': 60, 'h': 60 * 60, 'd': 60 * 60 * 24},\n        'min': {'ms': -1000 * 60, 's': -60, 'min': 1, 'h': 60, 'd': 60 * 24}\n    }\n\n    value, unit = strtol(value)\n    if value is not None:\n        if not unit:\n            return value\n\n        if base_unit and base_unit not in convert:\n            base_value, base_unit = strtol(base_unit, False)\n        else:\n            base_value = 1\n        if base_unit in convert and unit in convert[base_unit]:\n            multiplier = convert[base_unit][unit]\n            if multiplier < 0:\n                value /= -multiplier\n            else:\n                value *= multiplier\n            return int(value/base_value)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compare_values(vartype, unit, old_value, new_value):\n\n    # if the integer or bool new_value is not correct this function will return False\n    if vartype == 'bool':\n        old_value = parse_bool(old_value)\n        new_value = parse_bool(new_value)\n    elif vartype == 'integer':\n        old_value = parse_int(old_value)\n        new_value = parse_int(new_value, unit)\n    elif vartype == 'enum':\n        return str(old_value).lower() == str(new_value).lower()\n    else:  # ('string', 'real')\n        return str(old_value) == str(new_value)\n    return old_value is not None and new_value is not None and old_value == new_value", "response": "Compare two values for a given vartype."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef polling_loop(timeout, interval=1):\n    start_time = time.time()\n    iteration = 0\n    end_time = start_time + timeout\n    while time.time() < end_time:\n        yield iteration\n        iteration += 1\n        time.sleep(interval)", "response": "Returns an iterator that returns values until timeout has passed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reset(self):\n        self._attempts = 0\n        self._cur_delay = self.delay\n        self._cur_stoptime = None", "response": "Reset the attempt counter and the current delay and the current stoptime"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a copy of this retry manager", "response": "def copy(self):\n        \"\"\"Return a clone of this retry manager\"\"\"\n        return Retry(max_tries=self.max_tries, delay=self.delay, backoff=self.backoff,\n                     max_jitter=self.max_jitter / 100.0, max_delay=self.max_delay, sleep_func=self.sleep_func,\n                     deadline=self.deadline, retry_exceptions=self.retry_exceptions)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndefault method for processing all GET requests which can not be routed to other methods", "response": "def do_GET(self, write_status_code_only=False):\n        \"\"\"Default method for processing all GET requests which can not be routed to other methods\"\"\"\n\n        path = '/master' if self.path == '/' else self.path\n        response = self.get_postgresql_status()\n\n        patroni = self.server.patroni\n        cluster = patroni.dcs.cluster\n\n        if not cluster and patroni.ha.is_paused():\n            primary_status_code = 200 if response['role'] == 'master' else 503\n        else:\n            primary_status_code = 200 if patroni.ha.is_leader() else 503\n\n        replica_status_code = 200 if not patroni.noloadbalance and response.get('role') == 'replica' else 503\n        status_code = 503\n\n        if patroni.ha.is_standby_cluster() and ('standby_leader' in path or 'standby-leader' in path):\n            status_code = 200 if patroni.ha.is_leader() else 503\n        elif 'master' in path or 'leader' in path or 'primary' in path or 'read-write' in path:\n            status_code = primary_status_code\n        elif 'replica' in path:\n            status_code = replica_status_code\n        elif 'read-only' in path:\n            status_code = 200 if primary_status_code == 200 else replica_status_code\n        elif cluster:  # dcs is available\n            is_synchronous = cluster.is_synchronous_mode() and cluster.sync \\\n                    and cluster.sync.sync_standby == patroni.postgresql.name\n            if path in ('/sync', '/synchronous') and is_synchronous:\n                status_code = replica_status_code\n            elif path in ('/async', '/asynchronous') and not is_synchronous:\n                status_code = replica_status_code\n\n        if write_status_code_only:  # when haproxy sends OPTIONS request it reads only status code and nothing more\n            message = self.responses[status_code][0]\n            self.wfile.write('{0} {1} {2}\\r\\n'.format(self.protocol_version, status_code, message).encode('utf-8'))\n        else:\n            self._write_status_response(status_code, response)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_schedule(schedule, action):\n        error = None\n        scheduled_at = None\n        try:\n            scheduled_at = dateutil.parser.parse(schedule)\n            if scheduled_at.tzinfo is None:\n                error = 'Timezone information is mandatory for the scheduled {0}'.format(action)\n                status_code = 400\n            elif scheduled_at < datetime.datetime.now(tzutc):\n                error = 'Cannot schedule {0} in the past'.format(action)\n                status_code = 422\n            else:\n                status_code = None\n        except (ValueError, TypeError):\n            logger.exception('Invalid scheduled %s time: %s', action, schedule)\n            error = 'Unable to parse scheduled timestamp. It should be in an unambiguous format, e.g. ISO 8601'\n            status_code = 422\n        return (status_code, error, scheduled_at)", "response": "Parses the given schedule and validates at"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_request(self):\n\n        ret = BaseHTTPRequestHandler.parse_request(self)\n        if ret:\n            mname = self.path.lstrip('/').split('/')[0]\n            mname = self.command + ('_' + mname if mname else '')\n            if hasattr(self, 'do_' + mname):\n                self.command = mname\n        return ret", "response": "Override parse_request method to enrich basic functionality of BaseHTTPRequestHandler class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading config. yaml from filesystem and applies some values which were set via ENV", "response": "def _load_config_file(self):\n        \"\"\"Loads config.yaml from filesystem and applies some values which were set via ENV\"\"\"\n        with open(self._config_file) as f:\n            config = yaml.safe_load(f)\n            patch_config(config, self.__environment_configuration)\n            return config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntranslate member name to valid PostgreSQL replication slot name.", "response": "def slot_name_from_member_name(member_name):\n    \"\"\"Translate member name to valid PostgreSQL slot name.\n\n    PostgreSQL replication slot names must be valid PostgreSQL names. This function maps the wider space of\n    member names to valid PostgreSQL names. Names are lowercased, dashes and periods common in hostnames\n    are replaced with underscores, other characters are encoded as their unicode codepoint. Name is truncated\n    to 64 characters. Multiple different member names may map to a single slot name.\"\"\"\n\n    def replace_char(match):\n        c = match.group(0)\n        return '_' if c in '-.' else \"u{:04d}\".format(ord(c))\n\n    slot_name = re.sub('[^a-z0-9_]', replace_char, member_name.lower())\n    return slot_name[0:63]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses connection string into two parts", "response": "def parse_connection_string(value):\n    \"\"\"Original Governor stores connection strings for each cluster members if a following format:\n        postgres://{username}:{password}@{connect_address}/postgres\n    Since each of our patroni instances provides own REST API endpoint it's good to store this information\n    in DCS among with postgresql connection string. In order to not introduce new keys and be compatible with\n    original Governor we decided to extend original connection string in a following way:\n        postgres://{username}:{password}@{connect_address}/postgres?application_name={api_url}\n    This way original Governor could use such connection string as it is, because of feature of `libpq` library.\n\n    This method is able to split connection string stored in DCS into two parts, `conn_url` and `api_url`\"\"\"\n\n    scheme, netloc, path, params, query, fragment = urlparse(value)\n    conn_url = urlunparse((scheme, netloc, path, params, '', fragment))\n    api_url = ([v for n, v in parse_qsl(query) if n == 'application_name'] or [None])[0]\n    return conn_url, api_url"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dcs_modules():\n\n    dcs_dirname = os.path.dirname(__file__)\n    module_prefix = __package__ + '.'\n\n    if getattr(sys, 'frozen', False):\n        importer = pkgutil.get_importer(dcs_dirname)\n        return [module for module in list(importer.toc) if module.startswith(module_prefix) and module.count('.') == 2]\n    else:\n        return [module_prefix + name for _, name, is_pkg in pkgutil.iter_modules([dcs_dirname]) if not is_pkg]", "response": "Get names of DCS modules depending on execution environment."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_node(index, name, session, data):\n        if data.startswith('postgres'):\n            conn_url, api_url = parse_connection_string(data)\n            data = {'conn_url': conn_url, 'api_url': api_url}\n        else:\n            try:\n                data = json.loads(data)\n            except (TypeError, ValueError):\n                data = {}\n        return Member(index, name, session, data)", "response": "Create a new Member instance from a node."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_node(index, data, modify_index=None):\n\n        try:\n            data = json.loads(data)\n        except (TypeError, ValueError):\n            data = None\n            modify_index = 0\n        if not isinstance(data, dict):\n            data = {}\n        return ClusterConfig(index, data, index if modify_index is None else modify_index)", "response": "Create a ClusterConfig object from a node."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new SyncState object from a node.", "response": "def from_node(index, value):\n        \"\"\"\n        >>> SyncState.from_node(1, None).leader is None\n        True\n        >>> SyncState.from_node(1, '{}').leader is None\n        True\n        >>> SyncState.from_node(1, '{').leader is None\n        True\n        >>> SyncState.from_node(1, '[]').leader is None\n        True\n        >>> SyncState.from_node(1, '{\"leader\": \"leader\"}').leader == \"leader\"\n        True\n        >>> SyncState.from_node(1, {\"leader\": \"leader\"}).leader == \"leader\"\n        True\n        \"\"\"\n        if isinstance(value, dict):\n            data = value\n        elif value:\n            try:\n                data = json.loads(value)\n                if not isinstance(data, dict):\n                    data = {}\n            except (TypeError, ValueError):\n                data = {}\n        else:\n            data = {}\n        return SyncState(index, data.get('leader'), data.get('sync_standby'))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new TimelineHistory object from a node.", "response": "def from_node(index, value):\n        \"\"\"\n        >>> h = TimelineHistory.from_node(1, 2)\n        >>> h.lines\n        []\n        \"\"\"\n        try:\n            lines = json.loads(value)\n        except (TypeError, ValueError):\n            lines = None\n        if not isinstance(lines, list):\n            lines = []\n        return TimelineHistory(index, value, lines)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef timeline(self):\n        if self.history:\n            if self.history.lines:\n                try:\n                    return int(self.history.lines[-1][0]) + 1\n                except Exception:\n                    logger.error('Failed to parse cluster history from DCS: %s', self.history.lines)\n            elif self.history.value == '[]':\n                return 1\n        return 0", "response": "Return the number of entries in the timeline."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall when the last observed subsets of a namespace has changed.", "response": "def subsets_changed(last_observed_subsets, subsets):\n        \"\"\"\n        >>> Kubernetes.subsets_changed([], [])\n        False\n        >>> Kubernetes.subsets_changed([], [k8s_client.V1EndpointSubset()])\n        True\n        >>> s1 = [k8s_client.V1EndpointSubset(addresses=[k8s_client.V1EndpointAddress(ip='1.2.3.4')])]\n        >>> s2 = [k8s_client.V1EndpointSubset(addresses=[k8s_client.V1EndpointAddress(ip='1.2.3.5')])]\n        >>> Kubernetes.subsets_changed(s1, s2)\n        True\n        >>> a = [k8s_client.V1EndpointAddress(ip='1.2.3.4')]\n        >>> s1 = [k8s_client.V1EndpointSubset(addresses=a, ports=[k8s_client.V1EndpointPort(protocol='TCP', port=1)])]\n        >>> s2 = [k8s_client.V1EndpointSubset(addresses=a, ports=[k8s_client.V1EndpointPort(port=5432)])]\n        >>> Kubernetes.subsets_changed(s1, s2)\n        True\n        >>> p1 = k8s_client.V1EndpointPort(name='port1', port=1)\n        >>> p2 = k8s_client.V1EndpointPort(name='port2', port=2)\n        >>> p3 = k8s_client.V1EndpointPort(name='port3', port=3)\n        >>> s1 = [k8s_client.V1EndpointSubset(addresses=a, ports=[p1, p2])]\n        >>> s2 = [k8s_client.V1EndpointSubset(addresses=a, ports=[p2, p3])]\n        >>> Kubernetes.subsets_changed(s1, s2)\n        True\n        >>> s2 = [k8s_client.V1EndpointSubset(addresses=a, ports=[p2, p1])]\n        >>> Kubernetes.subsets_changed(s1, s2)\n        False\n        \"\"\"\n        if len(last_observed_subsets) != len(subsets):\n            return True\n        if subsets == []:\n            return False\n        if len(last_observed_subsets[0].addresses or []) != 1 or \\\n                last_observed_subsets[0].addresses[0].ip != subsets[0].addresses[0].ip or \\\n                len(last_observed_subsets[0].ports) != len(subsets[0].ports):\n            return True\n        if len(subsets[0].ports) == 1:\n            return not Kubernetes.compare_ports(last_observed_subsets[0].ports[0], subsets[0].ports[0])\n        observed_ports = {p.name: p for p in last_observed_subsets[0].ports}\n        for p in subsets[0].ports:\n            if p.name not in observed_ports or not Kubernetes.compare_ports(p, observed_ports.pop(p.name)):\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef after_feature(context, feature):\n    context.pctl.stop_all()\n    shutil.rmtree(os.path.join(context.pctl.patroni_path, 'data'))\n    context.dcs_ctl.cleanup_service_tree()\n    if feature.status == 'failed':\n        shutil.copytree(context.pctl.output_dir, context.pctl.output_dir + '_failed')", "response": "Stop all Patronis and remove all data directory and cleanup the keys in etcd"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nterminate process and wipe out the temp work directory", "response": "def stop(self, kill=False, timeout=15):\n        \"\"\" terminate process and wipe out the temp work directory, but only if we actually started it\"\"\"\n        super(AbstractDcsController, self).stop(kill=kill, timeout=timeout)\n        if self._work_directory:\n            shutil.rmtree(self._work_directory)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ioctl(self, func, arg):\n        if self._fd is None:\n            raise WatchdogError(\"Watchdog device is closed\")\n        if os.name != 'nt':\n            import fcntl\n            fcntl.ioctl(self._fd, func, arg, True)", "response": "Runs the specified ioctl on the underlying fd."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn JSON - safe version of string.", "response": "def json_safe(string, content_type='application/octet-stream'):\n    \"\"\"Returns JSON-safe version of `string`.\n\n    If `string` is a Unicode string or a valid UTF-8, it is returned unmodified,\n    as it can safely be encoded to JSON string.\n\n    If `string` contains raw/binary data, it is Base64-encoded, formatted and\n    returned according to \"data\" URL scheme (RFC2397). Since JSON is not\n    suitable for binary data, some additional encoding was necessary; \"data\"\n    URL scheme was chosen for its simplicity.\n    \"\"\"\n    try:\n        string = string.decode('utf-8')\n        json.dumps(string)\n        return string\n    except (ValueError, TypeError):\n        return b''.join([\n            b'data:',\n            content_type.encode('utf-8'),\n            b';base64,',\n            base64.b64encode(string)\n        ]).decode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dict of all the files in the current context.", "response": "def get_files():\n    \"\"\"Returns files dict from request context.\"\"\"\n\n    files = dict()\n\n    for k, v in request.files.items():\n        content_type = request.files[k].content_type or 'application/octet-stream'\n        val = json_safe(v.read(), content_type)\n        if files.get(k):\n            if not isinstance(files[k], list):\n                files[k] = [files[k]]\n            files[k].append(val)\n        else:\n            files[k] = val\n\n    return files"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn headers dict from request context.", "response": "def get_headers(hide_env=True):\n    \"\"\"Returns headers dict from request context.\"\"\"\n\n    headers = dict(request.headers.items())\n\n    if hide_env and ('show_env' not in request.args):\n        for key in ENV_HEADERS:\n            try:\n                del headers[key]\n            except KeyError:\n                pass\n\n    return CaseInsensitiveDict(headers.items())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef semiflatten(multi):\n    if multi:\n        result = multi.to_dict(flat=False)\n        for k, v in result.items():\n            if len(v) == 1:\n                result[k] = v[0]\n        return result\n    else:\n        return multi", "response": "Convert a MutiDict into a regular dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_url(request):\n    protocol = request.headers.get('X-Forwarded-Proto') or request.headers.get('X-Forwarded-Protocol')\n    if protocol is None and request.headers.get('X-Forwarded-Ssl') == 'on':\n        protocol = 'https'\n    if protocol is None:\n        return request.url\n    url = list(urlparse(request.url))\n    url[0] = protocol\n    return urlunparse(url)", "response": "Get the URL of the request."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn request dict of given keys.", "response": "def get_dict(*keys, **extras):\n    \"\"\"Returns request dict of given keys.\"\"\"\n\n    _keys = ('url', 'args', 'form', 'data', 'origin', 'headers', 'files', 'json', 'method')\n\n    assert all(map(_keys.__contains__, keys))\n    data = request.data\n    form = semiflatten(request.form)\n\n    try:\n        _json = json.loads(data.decode('utf-8'))\n    except (ValueError, TypeError):\n        _json = None\n\n    d = dict(\n        url=get_url(request),\n        args=semiflatten(request.args),\n        form=form,\n        data=json_safe(data),\n        origin=request.headers.get('X-Forwarded-For', request.remote_addr),\n        headers=get_headers(),\n        files=get_files(),\n        json=_json,\n        method=request.method,\n    )\n\n    out_d = dict()\n\n    for key in keys:\n        out_d[key] = d.get(key)\n\n    out_d.update(extras)\n\n    return out_d"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef status_code(code):\n\n    redirect = dict(headers=dict(location=REDIRECT_LOCATION))\n\n    code_map = {\n        301: redirect,\n        302: redirect,\n        303: redirect,\n        304: dict(data=''),\n        305: redirect,\n        307: redirect,\n        401: dict(headers={'WWW-Authenticate': 'Basic realm=\"Fake Realm\"'}),\n        402: dict(\n            data='Fuck you, pay me!',\n            headers={\n                'x-more-info': 'http://vimeo.com/22053820'\n            }\n        ),\n        406: dict(data=json.dumps({\n                'message': 'Client did not request a supported media type.',\n                'accept': ACCEPTED_MEDIA_TYPES\n            }),\n            headers={\n                'Content-Type': 'application/json'\n            }),\n        407: dict(headers={'Proxy-Authenticate': 'Basic realm=\"Fake Realm\"'}),\n        418: dict(  # I'm a teapot!\n            data=ASCII_ART,\n            headers={\n                'x-more-info': 'http://tools.ietf.org/html/rfc2324'\n            }\n        ),\n\n    }\n\n    r = make_response()\n    r.status_code = code\n\n    if code in code_map:\n\n        m = code_map[code]\n\n        if 'data' in m:\n            r.data = m['data']\n        if 'headers' in m:\n            r.headers = m['headers']\n\n    return r", "response": "Returns response object of given status code."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_basic_auth(user, passwd):\n\n    auth = request.authorization\n    return auth and auth.username == user and auth.password == passwd", "response": "Checks user authentication using HTTP Basic Auth."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef HA1(realm, username, password, algorithm):\n    if not realm:\n        realm = u''\n    return H(b\":\".join([username.encode('utf-8'),\n                           realm.encode('utf-8'),\n                           password.encode('utf-8')]), algorithm)", "response": "Create HA1 hash by realm username password and algorithm"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating HA2 md5 hash of the entity body.", "response": "def HA2(credentials, request, algorithm):\n    \"\"\"Create HA2 md5 hash\n\n    If the qop directive's value is \"auth\" or is unspecified, then HA2:\n        HA2 = md5(A2) = MD5(method:digestURI)\n    If the qop directive's value is \"auth-int\" , then HA2 is\n        HA2 = md5(A2) = MD5(method:digestURI:MD5(entityBody))\n    \"\"\"\n    if credentials.get(\"qop\") == \"auth\" or credentials.get('qop') is None:\n        return H(b\":\".join([request['method'].encode('utf-8'), request['uri'].encode('utf-8')]), algorithm)\n    elif credentials.get(\"qop\") == \"auth-int\":\n        for k in 'method', 'uri', 'body':\n            if k not in request:\n                raise ValueError(\"%s required\" % k)\n        A2 = b\":\".join([request['method'].encode('utf-8'),\n                        request['uri'].encode('utf-8'),\n                        H(request['body'], algorithm).encode('utf-8')])\n        return H(A2, algorithm)\n    raise ValueError"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef response(credentials, password, request):\n    response = None\n    algorithm = credentials.get('algorithm')\n    HA1_value = HA1(\n        credentials.get('realm'),\n        credentials.get('username'),\n        password,\n        algorithm\n    )\n    HA2_value = HA2(credentials, request, algorithm)\n    if credentials.get('qop') is None:\n        response = H(b\":\".join([\n            HA1_value.encode('utf-8'),\n            credentials.get('nonce', '').encode('utf-8'),\n            HA2_value.encode('utf-8')\n        ]), algorithm)\n    elif credentials.get('qop') == 'auth' or credentials.get('qop') == 'auth-int':\n        for k in 'nonce', 'nc', 'cnonce', 'qop':\n            if k not in credentials:\n                raise ValueError(\"%s required for response H\" % k)\n        response = H(b\":\".join([HA1_value.encode('utf-8'),\n                               credentials.get('nonce').encode('utf-8'),\n                               credentials.get('nc').encode('utf-8'),\n                               credentials.get('cnonce').encode('utf-8'),\n                               credentials.get('qop').encode('utf-8'),\n                               HA2_value.encode('utf-8')]), algorithm)\n    else:\n        raise ValueError(\"qop value are wrong\")\n\n    return response", "response": "Compile digest auth response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck user authentication using HTTP Digest auth", "response": "def check_digest_auth(user, passwd):\n    \"\"\"Check user authentication using HTTP Digest auth\"\"\"\n\n    if request.headers.get('Authorization'):\n        credentials = parse_authorization_header(request.headers.get('Authorization'))\n        if not credentials:\n            return\n        request_uri = request.script_root + request.path\n        if request.query_string:\n            request_uri +=  '?' + request.query_string\n        response_hash = response(credentials, passwd, dict(uri=request_uri,\n                                                           body=request.data,\n                                                           method=request.method))\n        if credentials.get('response') == response_hash:\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the range header text from a GET request.", "response": "def __parse_request_range(range_header_text):\n    \"\"\" Return a tuple describing the byte range requested in a GET request\n    If the range is open ended on the left or right side, then a value of None\n    will be set.\n    RFC7233: http://svn.tools.ietf.org/svn/wg/httpbis/specs/rfc7233.html#header.range\n    Examples:\n      Range : bytes=1024-\n      Range : bytes=10-20\n      Range : bytes=-999\n    \"\"\"\n\n    left = None\n    right = None\n\n    if not range_header_text:\n        return left, right\n\n    range_header_text = range_header_text.strip()\n    if not range_header_text.startswith('bytes'):\n        return left, right\n\n    components = range_header_text.split(\"=\")\n    if len(components) != 2:\n        return left, right\n\n    components = components[1].split(\"-\")\n\n    try:\n        right = int(components[1])\n    except:\n        pass\n\n    try:\n        left = int(components[0])\n    except:\n        pass\n\n    return left, right"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_multi_value_header(header_str):\n    parsed_parts = []\n    if header_str:\n        parts = header_str.split(',')\n        for part in parts:\n            match = re.search('\\s*(W/)?\\\"?([^\"]*)\\\"?\\s*', part)\n            if match is not None:\n                parsed_parts.append(match.group(2))\n    return parsed_parts", "response": "Break apart an HTTP header string that is potentially a quoted comma separated list as used in entity headers in RFC2616."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef redirect_n_times(n):\n    assert n > 0\n\n    absolute = request.args.get(\"absolute\", \"false\").lower() == \"true\"\n\n    if n == 1:\n        return redirect(url_for(\"view_get\", _external=absolute))\n\n    if absolute:\n        return _redirect(\"absolute\", n, True)\n    else:\n        return _redirect(\"relative\", n, False)", "response": "A redirection for n times."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef relative_redirect_n_times(n):\n\n    assert n > 0\n\n    response = app.make_response(\"\")\n    response.status_code = 302\n\n    if n == 1:\n        response.headers[\"Location\"] = url_for(\"view_get\")\n        return response\n\n    response.headers[\"Location\"] = url_for(\"relative_redirect_n_times\", n=n - 1)\n    return response", "response": "Relatively 302 Redirects n times."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stream_n_messages(n):\n    response = get_dict(\"url\", \"args\", \"headers\", \"origin\")\n    n = min(n, 100)\n\n    def generate_stream():\n        for i in range(n):\n            response[\"id\"] = i\n            yield json.dumps(response) + \"\\n\"\n\n    return Response(generate_stream(), headers={\"Content-Type\": \"application/json\"})", "response": "Stream n JSON responses."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nviews status code of a list of codes.", "response": "def view_status_code(codes):\n    \"\"\"Return status code or random status code if more than one are given\n    ---\n    tags:\n      - Status codes\n    parameters:\n      - in: path\n        name: codes\n    produces:\n      - text/plain\n    responses:\n      100:\n        description: Informational responses\n      200:\n        description: Success\n      300:\n        description: Redirection\n      400:\n        description: Client Errors\n      500:\n        description: Server Errors\n    \"\"\"\n\n    if \",\" not in codes:\n        try:\n            code = int(codes)\n        except ValueError:\n            return Response(\"Invalid status code\", status=400)\n        return status_code(code)\n\n    choices = []\n    for choice in codes.split(\",\"):\n        if \":\" not in choice:\n            code = choice\n            weight = 1\n        else:\n            code, weight = choice.split(\":\")\n\n        try:\n            choices.append((int(code), float(weight)))\n        except ValueError:\n            return Response(\"Invalid status code\", status=400)\n\n    code = weighted_choice(choices)\n\n    return status_code(code)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef response_headers():\n    # Pending swaggerUI update\n    # https://github.com/swagger-api/swagger-ui/issues/3850\n    headers = MultiDict(request.args.items(multi=True))\n    response = jsonify(list(headers.lists()))\n\n    while True:\n        original_data = response.data\n        d = {}\n        for key in response.headers.keys():\n            value = response.headers.get_all(key)\n            if len(value) == 1:\n                value = value[0]\n            d[key] = value\n        response = jsonify(d)\n        for key, value in headers.items(multi=True):\n            response.headers.add(key, value)\n        response_has_changed = response.data != original_data\n        if not response_has_changed:\n            break\n    return response", "response": "Returns a set of response headers from the query string."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a JSON response with cookies.", "response": "def view_cookies(hide_env=True):\n    \"\"\"Returns cookie data.\n    ---\n    tags:\n      - Cookies\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Set cookies.\n    \"\"\"\n\n    cookies = dict(request.cookies.items())\n\n    if hide_env and (\"show_env\" not in request.args):\n        for key in ENV_COOKIES:\n            try:\n                del cookies[key]\n            except KeyError:\n                pass\n\n    return jsonify(cookies=cookies)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_cookie(name, value):\n\n    r = app.make_response(redirect(url_for(\"view_cookies\")))\n    r.set_cookie(key=name, value=value, secure=secure_cookie())\n\n    return r", "response": "Sets a cookie and redirects to cookie list."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_cookies():\n\n    cookies = dict(request.args.items())\n    r = app.make_response(redirect(url_for(\"view_cookies\")))\n    for key, value in cookies.items():\n        r.set_cookie(key=key, value=value, secure=secure_cookie())\n\n    return r", "response": "Sets the cookies in the freeform_cookie list to the values provided by the query string and redirects to cookie list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete cookies as provided by the query string and redirects to cookie list.", "response": "def delete_cookies():\n    \"\"\"Deletes cookie(s) as provided by the query string and redirects to cookie list.\n    ---\n    tags:\n      - Cookies\n    parameters:\n      - in: query\n        name: freeform\n        explode: true\n        allowEmptyValue: true\n        schema:\n          type: object\n          additionalProperties:\n            type: string\n        style: form\n    produces:\n      - text/plain\n    responses:\n      200:\n        description: Redirect to cookie list\n    \"\"\"\n\n    cookies = dict(request.args.items())\n    r = app.make_response(redirect(url_for(\"view_cookies\")))\n    for key, value in cookies.items():\n        r.delete_cookie(key=key)\n\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef basic_auth(user=\"user\", passwd=\"passwd\"):\n\n    if not check_basic_auth(user, passwd):\n        return status_code(401)\n\n    return jsonify(authenticated=True, user=user)", "response": "Prompts the user for authorization using HTTP Basic Auth."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprompting the user for authorization using HTTP Basic Auth.", "response": "def hidden_basic_auth(user=\"user\", passwd=\"passwd\"):\n    \"\"\"Prompts the user for authorization using HTTP Basic Auth.\n    ---\n    tags:\n      - Auth\n    parameters:\n      - in: path\n        name: user\n        type: string\n      - in: path\n        name: passwd\n        type: string\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Sucessful authentication.\n      404:\n        description: Unsuccessful authentication.\n    \"\"\"\n\n    if not check_basic_auth(user, passwd):\n        return status_code(404)\n    return jsonify(authenticated=True, user=user)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprompt the user for authorization using bearer authentication.", "response": "def bearer_auth():\n    \"\"\"Prompts the user for authorization using bearer authentication.\n    ---\n    tags:\n      - Auth\n    parameters:\n      - in: header\n        name: Authorization\n        schema:\n          type: string\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Sucessful authentication.\n      401:\n        description: Unsuccessful authentication.\n    \"\"\"\n    authorization = request.headers.get(\"Authorization\")\n    if not (authorization and authorization.startswith(\"Bearer \")):\n        response = app.make_response(\"\")\n        response.headers[\"WWW-Authenticate\"] = \"Bearer\"\n        response.status_code = 401\n        return response\n    slice_start = len(\"Bearer \")\n    token = authorization[slice_start:]\n\n    return jsonify(authenticated=True, token=token)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprompt the user for authorization using Digest Auth + Algorithm.", "response": "def digest_auth_nostale(qop=None, user=\"user\", passwd=\"passwd\", algorithm=\"MD5\"):\n    \"\"\"Prompts the user for authorization using Digest Auth + Algorithm.\n    ---\n    tags:\n      - Auth\n    parameters:\n      - in: path\n        name: qop\n        type: string\n        description: auth or auth-int\n      - in: path\n        name: user\n        type: string\n      - in: path\n        name: passwd\n        type: string\n      - in: path\n        name: algorithm\n        type: string\n        description: MD5, SHA-256, SHA-512\n        default: MD5\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Sucessful authentication.\n      401:\n        description: Unsuccessful authentication.\n    \"\"\"\n    return digest_auth(qop, user, passwd, algorithm, \"never\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef digest_auth(\n    qop=None, user=\"user\", passwd=\"passwd\", algorithm=\"MD5\", stale_after=\"never\"\n):\n    \"\"\"Prompts the user for authorization using Digest Auth + Algorithm.\n    allow settings the stale_after argument.\n    ---\n    tags:\n      - Auth\n    parameters:\n      - in: path\n        name: qop\n        type: string\n        description: auth or auth-int\n      - in: path\n        name: user\n        type: string\n      - in: path\n        name: passwd\n        type: string\n      - in: path\n        name: algorithm\n        type: string\n        description: MD5, SHA-256, SHA-512\n        default: MD5\n      - in: path\n        name: stale_after\n        type: string\n        default: never\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Sucessful authentication.\n      401:\n        description: Unsuccessful authentication.\n    \"\"\"\n    require_cookie_handling = request.args.get(\"require-cookie\", \"\").lower() in (\n        \"1\",\n        \"t\",\n        \"true\",\n    )\n    if algorithm not in (\"MD5\", \"SHA-256\", \"SHA-512\"):\n        algorithm = \"MD5\"\n\n    if qop not in (\"auth\", \"auth-int\"):\n        qop = None\n\n    authorization = request.headers.get(\"Authorization\")\n    credentials = None\n    if authorization:\n        credentials = parse_authorization_header(authorization)\n\n    if (\n        not authorization\n        or not credentials\n        or credentials.type.lower() != \"digest\"\n        or (require_cookie_handling and \"Cookie\" not in request.headers)\n    ):\n        response = digest_challenge_response(app, qop, algorithm)\n        response.set_cookie(\"stale_after\", value=stale_after)\n        response.set_cookie(\"fake\", value=\"fake_value\")\n        return response\n\n    if require_cookie_handling and request.cookies.get(\"fake\") != \"fake_value\":\n        response = jsonify({\"errors\": [\"missing cookie set on challenge\"]})\n        response.set_cookie(\"fake\", value=\"fake_value\")\n        response.status_code = 403\n        return response\n\n    current_nonce = credentials.get(\"nonce\")\n\n    stale_after_value = None\n    if \"stale_after\" in request.cookies:\n        stale_after_value = request.cookies.get(\"stale_after\")\n\n    if (\n        \"last_nonce\" in request.cookies\n        and current_nonce == request.cookies.get(\"last_nonce\")\n        or stale_after_value == \"0\"\n    ):\n        response = digest_challenge_response(app, qop, algorithm, True)\n        response.set_cookie(\"stale_after\", value=stale_after)\n        response.set_cookie(\"last_nonce\", value=current_nonce)\n        response.set_cookie(\"fake\", value=\"fake_value\")\n        return response\n\n    if not check_digest_auth(user, passwd):\n        response = digest_challenge_response(app, qop, algorithm, False)\n        response.set_cookie(\"stale_after\", value=stale_after)\n        response.set_cookie(\"last_nonce\", value=current_nonce)\n        response.set_cookie(\"fake\", value=\"fake_value\")\n        return response\n\n    response = jsonify(authenticated=True, user=user)\n    response.set_cookie(\"fake\", value=\"fake_value\")\n    if stale_after_value:\n        response.set_cookie(\n            \"stale_after\", value=next_stale_after_value(stale_after_value)\n        )\n\n    return response", "response": "Prompts the user for authorization using Digest Auth + Algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delay_response(delay):\n    delay = min(float(delay), 10)\n\n    time.sleep(delay)\n\n    return jsonify(\n        get_dict(\"url\", \"args\", \"form\", \"data\", \"origin\", \"headers\", \"files\")\n    )", "response": "Returns a delayed response."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndripping data over a duration after an optional initial delay.", "response": "def drip():\n    \"\"\"Drips data over a duration after an optional initial delay.\n    ---\n    tags:\n      - Dynamic data\n    parameters:\n      - in: query\n        name: duration\n        type: number\n        description: The amount of time (in seconds) over which to drip each byte\n        default: 2\n        required: false\n      - in: query\n        name: numbytes\n        type: integer\n        description: The number of bytes to respond with\n        default: 10\n        required: false\n      - in: query\n        name: code\n        type: integer\n        description: The response code that will be returned\n        default: 200\n        required: false\n      - in: query\n        name: delay\n        type: number\n        description: The amount of time (in seconds) to delay before responding\n        default: 2\n        required: false\n    produces:\n      - application/octet-stream\n    responses:\n      200:\n        description: A dripped response.\n    \"\"\"\n    args = CaseInsensitiveDict(request.args.items())\n    duration = float(args.get(\"duration\", 2))\n    numbytes = min(int(args.get(\"numbytes\", 10)), (10 * 1024 * 1024))  # set 10MB limit\n    code = int(args.get(\"code\", 200))\n\n    if numbytes <= 0:\n        response = Response(\"number of bytes must be positive\", status=400)\n        return response\n\n    delay = float(args.get(\"delay\", 0))\n    if delay > 0:\n        time.sleep(delay)\n\n    pause = duration / numbytes\n\n    def generate_bytes():\n        for i in xrange(numbytes):\n            yield b\"*\"\n            time.sleep(pause)\n\n    response = Response(\n        generate_bytes(),\n        headers={\n            \"Content-Type\": \"application/octet-stream\",\n            \"Content-Length\": str(numbytes),\n        },\n    )\n\n    response.status_code = code\n\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cache():\n    is_conditional = request.headers.get(\"If-Modified-Since\") or request.headers.get(\n        \"If-None-Match\"\n    )\n\n    if is_conditional is None:\n        response = view_get()\n        response.headers[\"Last-Modified\"] = http_date()\n        response.headers[\"ETag\"] = uuid.uuid4().hex\n        return response\n    else:\n        return status_code(304)", "response": "Returns a 304 if an If - Modified - Since header or If - None - Match is present. Returns a GET otherwise."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nassumes the resource has the given etag and responds to If - None - Match and If - Match headers appropriately.", "response": "def etag(etag):\n    \"\"\"Assumes the resource has the given etag and responds to If-None-Match and If-Match headers appropriately.\n    ---\n    tags:\n      - Response inspection\n    parameters:\n      - in: header\n        name: If-None-Match\n      - in: header\n        name: If-Match\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Normal response\n      412:\n        description: match\n\n    \"\"\"\n    if_none_match = parse_multi_value_header(request.headers.get(\"If-None-Match\"))\n    if_match = parse_multi_value_header(request.headers.get(\"If-Match\"))\n\n    if if_none_match:\n        if etag in if_none_match or \"*\" in if_none_match:\n            response = status_code(304)\n            response.headers[\"ETag\"] = etag\n            return response\n    elif if_match:\n        if etag not in if_match and \"*\" not in if_match:\n            return status_code(412)\n\n    # Special cases don't apply, return normal response\n    response = view_get()\n    response.headers[\"ETag\"] = etag\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn n random bytes generated with given seed", "response": "def random_bytes(n):\n    \"\"\"Returns n random bytes generated with given seed\n    ---\n    tags:\n      - Dynamic data\n    parameters:\n      - in: path\n        name: n\n        type: int\n    produces:\n      - application/octet-stream\n    responses:\n      200:\n        description: Bytes.\n    \"\"\"\n\n    n = min(n, 100 * 1024)  # set 100KB limit\n\n    params = CaseInsensitiveDict(request.args.items())\n    if \"seed\" in params:\n        random.seed(int(params[\"seed\"]))\n\n    response = make_response()\n\n    # Note: can't just use os.urandom here because it ignores the seed\n    response.data = bytearray(random.randint(0, 255) for i in range(n))\n    response.content_type = \"application/octet-stream\"\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stream_random_bytes(n):\n    n = min(n, 100 * 1024)  # set 100KB limit\n\n    params = CaseInsensitiveDict(request.args.items())\n    if \"seed\" in params:\n        random.seed(int(params[\"seed\"]))\n\n    if \"chunk_size\" in params:\n        chunk_size = max(1, int(params[\"chunk_size\"]))\n    else:\n        chunk_size = 10 * 1024\n\n    def generate_bytes():\n        chunks = bytearray()\n\n        for i in xrange(n):\n            chunks.append(random.randint(0, 255))\n            if len(chunks) == chunk_size:\n                yield (bytes(chunks))\n                chunks = bytearray()\n\n        if chunks:\n            yield (bytes(chunks))\n\n    headers = {\"Content-Type\": \"application/octet-stream\"}\n\n    return Response(generate_bytes(), headers=headers)", "response": "Streams n random bytes generated with given seed at given chunk size per packet."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstream n random bytes from given seed at given chunk size per packet.", "response": "def range_request(numbytes):\n    \"\"\"Streams n random bytes generated with given seed, at given chunk size per packet.\n    ---\n    tags:\n      - Dynamic data\n    parameters:\n      - in: path\n        name: numbytes\n        type: int\n    produces:\n      - application/octet-stream\n    responses:\n      200:\n        description: Bytes.\n    \"\"\"\n\n    if numbytes <= 0 or numbytes > (100 * 1024):\n        response = Response(\n            headers={\"ETag\": \"range%d\" % numbytes, \"Accept-Ranges\": \"bytes\"}\n        )\n        response.status_code = 404\n        response.data = \"number of bytes must be in the range (0, 102400]\"\n        return response\n\n    params = CaseInsensitiveDict(request.args.items())\n    if \"chunk_size\" in params:\n        chunk_size = max(1, int(params[\"chunk_size\"]))\n    else:\n        chunk_size = 10 * 1024\n\n    duration = float(params.get(\"duration\", 0))\n    pause_per_byte = duration / numbytes\n\n    request_headers = get_headers()\n    first_byte_pos, last_byte_pos = get_request_range(request_headers, numbytes)\n    range_length = (last_byte_pos + 1) - first_byte_pos\n\n    if (\n        first_byte_pos > last_byte_pos\n        or first_byte_pos not in xrange(0, numbytes)\n        or last_byte_pos not in xrange(0, numbytes)\n    ):\n        response = Response(\n            headers={\n                \"ETag\": \"range%d\" % numbytes,\n                \"Accept-Ranges\": \"bytes\",\n                \"Content-Range\": \"bytes */%d\" % numbytes,\n                \"Content-Length\": \"0\",\n            }\n        )\n        response.status_code = 416\n        return response\n\n    def generate_bytes():\n        chunks = bytearray()\n\n        for i in xrange(first_byte_pos, last_byte_pos + 1):\n\n            # We don't want the resource to change across requests, so we need\n            # to use a predictable data generation function\n            chunks.append(ord(\"a\") + (i % 26))\n            if len(chunks) == chunk_size:\n                yield (bytes(chunks))\n                time.sleep(pause_per_byte * chunk_size)\n                chunks = bytearray()\n\n        if chunks:\n            time.sleep(pause_per_byte * len(chunks))\n            yield (bytes(chunks))\n\n    content_range = \"bytes %d-%d/%d\" % (first_byte_pos, last_byte_pos, numbytes)\n    response_headers = {\n        \"Content-Type\": \"application/octet-stream\",\n        \"ETag\": \"range%d\" % numbytes,\n        \"Accept-Ranges\": \"bytes\",\n        \"Content-Length\": str(range_length),\n        \"Content-Range\": content_range,\n    }\n\n    response = Response(generate_bytes(), headers=response_headers)\n\n    if (first_byte_pos == 0) and (last_byte_pos == (numbytes - 1)):\n        response.status_code = 200\n    else:\n        response.status_code = 206\n\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a page containing n links to other pages which do the same.", "response": "def link_page(n, offset):\n    \"\"\"Generate a page containing n links to other pages which do the same.\n    ---\n    tags:\n      - Dynamic data\n    parameters:\n      - in: path\n        name: n\n        type: int\n      - in: path\n        name: offset\n        type: int\n    produces:\n      - text/html\n    responses:\n      200:\n        description: HTML links.\n    \"\"\"\n    n = min(max(1, n), 200)  # limit to between 1 and 200 links\n\n    link = \"<a href='{0}'>{1}</a> \"\n\n    html = [\"<html><head><title>Links</title></head><body>\"]\n    for i in xrange(n):\n        if i == offset:\n            html.append(\"{0} \".format(i))\n        else:\n            html.append(link.format(url_for(\"link_page\", n=n, offset=i), i))\n    html.append(\"</body></html>\")\n\n    return \"\".join(html)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef image():\n\n    headers = get_headers()\n    if \"accept\" not in headers:\n        return image_png()  # Default media type to png\n\n    accept = headers[\"accept\"].lower()\n\n    if \"image/webp\" in accept:\n        return image_webp()\n    elif \"image/svg+xml\" in accept:\n        return image_svg()\n    elif \"image/jpeg\" in accept:\n        return image_jpeg()\n    elif \"image/png\" in accept or \"image/*\" in accept:\n        return image_png()\n    else:\n        return status_code(406)", "response": "Returns a simple image of the type suggest by the Accept header."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a value from choices chosen by weighted random selection choices should be a list of tuples.", "response": "def weighted_choice(choices):\n    \"\"\"Returns a value from choices chosen by weighted random selection\n\n    choices should be a list of (value, weight) tuples.\n\n    eg. weighted_choice([('val1', 5), ('val2', 0.3), ('val3', 1)])\n\n    \"\"\"\n    values, weights = zip(*choices)\n    total = 0\n    cum_weights = []\n    for w in weights:\n        total += w\n        cum_weights.append(total)\n    x = random.uniform(0, total)\n    i = bisect.bisect(cum_weights, x)\n    return values[i]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef x_runtime(f, *args, **kwargs):\n\n    _t0 = now()\n    r = f(*args, **kwargs)\n    _t1 = now()\n    r.headers['X-Runtime'] = '{0}s'.format(Decimal(str(_t1 - _t0)))\n\n    return r", "response": "X - Runtime Flask Response Decorator."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gzip(f, *args, **kwargs):\n\n    data = f(*args, **kwargs)\n\n    if isinstance(data, Response):\n        content = data.data\n    else:\n        content = data\n\n    gzip_buffer = BytesIO()\n    gzip_file = gzip2.GzipFile(\n        mode='wb',\n        compresslevel=4,\n        fileobj=gzip_buffer\n    )\n    gzip_file.write(content)\n    gzip_file.close()\n\n    gzip_data = gzip_buffer.getvalue()\n\n    if isinstance(data, Response):\n        data.data = gzip_data\n        data.headers['Content-Encoding'] = 'gzip'\n        data.headers['Content-Length'] = str(len(data.data))\n\n        return data\n\n    return gzip_data", "response": "GZip Flask Response Decorator."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeflating Flask Response Decorator.", "response": "def deflate(f, *args, **kwargs):\n    \"\"\"Deflate Flask Response Decorator.\"\"\"\n\n    data = f(*args, **kwargs)\n\n    if isinstance(data, Response):\n        content = data.data\n    else:\n        content = data\n\n    deflater = zlib.compressobj()\n    deflated_data = deflater.compress(content)\n    deflated_data += deflater.flush()\n\n    if isinstance(data, Response):\n        data.data = deflated_data\n        data.headers['Content-Encoding'] = 'deflate'\n        data.headers['Content-Length'] = str(len(data.data))\n\n        return data\n\n    return deflated_data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the version out of libjsonnet. h", "response": "def get_version():\n    \"\"\"\n    Parses the version out of libjsonnet.h\n    \"\"\"\n    with open(os.path.join(DIR, 'include/libjsonnet.h')) as f:\n        for line in f:\n            if '#define' in line and 'LIB_JSONNET_VERSION' in line:\n                v_code = line.partition('LIB_JSONNET_VERSION')[2].strip('\\n \"')\n                if v_code[0] == \"v\":\n                    v_code = v_code[1:]\n                return v_code"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_fractal():\n\n    if check_etag():\n        return flask.make_response(), 304\n\n    level = int(flask.request.args.get(\"l\", \"0\"))\n    x = float(int(flask.request.args.get(\"x\", \"0\")))\n    y = float(int(flask.request.args.get(\"y\", \"0\")))\n\n    if level < 0:\n        level = 0\n\n    grid_size = math.pow(2, level)\n\n    x0 = \"%.30g\" % ((x - 0) / grid_size)\n    y0 = \"%.30g\" % ((y - 0) / grid_size)\n    x1 = \"%.30g\" % ((x + 1) / grid_size)\n    y1 = \"%.30g\" % ((y + 1) / grid_size)\n\n    print \"Tile: %s %s %s %s\" % (x0, y0, x1, y1)\n\n    width = str(CONF['width'])\n    height = str(CONF['height'])\n    iters = str(CONF['iters'])\n\n    cmd = ['./mandelbrot', width, height, iters, x0, y0, x1, y1]\n    image_data = subprocess.check_output(cmd)\n\n    response = flask.make_response(image_data)\n    response.headers[\"Content-Type\"] = \"image/png\"\n    response.headers[\"cache-control\"] = \"public, max-age=600\"\n    response.headers[\"ETag\"] = ETAG\n\n    return response", "response": "Get fractal coordinates from query string call mandelbrot to generate image."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef preprocess(config):\n\n    def aux(ctx, service_name, service):\n        compiler, _ = get_compiler(config, service)\n        r2 = compiler.preprocess(ctx, service_name, service)\n        ctx = ctx + [service_name]\n        for child_name, child in compiler.children(service):\n            r2[child_name] = aux(ctx, child_name, child)\n        return r2\n\n    r = {\n        'environments': config['environments'],\n    }\n\n    for service_name, service in services(config):\n        r[service_name] = aux([], service_name, service)\n\n    return r", "response": "Return a copy of the config with ${ - handled."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_build_artefacts(config):\n\n    def aux(ctx, service_name, service):\n        compiler, environment = get_compiler(config, service)\n        new_service, service_barts = compiler.getBuildArtefacts(environment, ctx, service)\n        ctx = ctx + [service_name]\n        for child_name, child in compiler.children(service):\n            new_child, child_barts = aux(ctx, child_name, child)\n            util.merge_into(service_barts, child_barts)\n            new_service[child_name] = new_child\n        return new_service, service_barts\n\n    barts = {}\n    new_config = {\n        'environments': config['environments'],\n    }\n    for service_name, service in services(config):\n        new_service, service_barts = aux([], service_name, service)\n        new_config[service_name] = new_service\n        util.merge_into(barts, service_barts)\n    return new_config, barts", "response": "Create all required build artefacts modify config to refer to them."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading the input - hidden weight matrix from the original word2vec - tool format.", "response": "def load_word2vec_format(\n            cls,\n            fname,\n            fvocab=None,\n            binary=False,\n            encoding='utf8',\n            unicode_errors='strict',\n            limit=None,\n            datatype=REAL):\n        \"\"\"\n        Load the input-hidden weight matrix from the original C word2vec-tool format.\n        Note that the information stored in the file is incomplete (the binary tree is missing),\n        so while you can query for word similarity etc., you cannot continue training\n        with a model loaded this way.\n        `binary` is a boolean indicating whether the data is in binary word2vec format.\n        `norm_only` is a boolean indicating whether to only store normalised word2vec vectors in memory.\n        Word counts are read from `fvocab` filename, if set (this is the file generated\n        by `-save-vocab` flag of the original C tool).\n        If you trained the C model using non-utf8 encoding for words, specify that\n        encoding in `encoding`.\n        `unicode_errors`, default 'strict', is a string suitable to be passed as the `errors`\n        argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n        file may include word tokens truncated in the middle of a multibyte unicode character\n        (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n        `limit` sets a maximum number of word-vectors to read from the file. The default,\n        None, means read all.\n        `datatype` (experimental) can coerce dimensions to a non-default float type (such\n        as np.float16) to save memory. (Such types may result in much slower bulk operations\n        or incompatibility with optimized routines.)\n        \"\"\"\n        counts = None\n        if fvocab is not None:\n            logging.debug(\"loading word counts from %s\" % fvocab)\n            counts = {}\n            with smart_open(fvocab) as fin:\n                for line in fin:\n                    word, count = to_unicode(line).strip().split()\n                    counts[word] = int(count)\n\n        logging.debug(\"loading projection weights from %s\" % fname)\n        with smart_open(fname) as fin:\n            header = to_unicode(fin.readline(), encoding=encoding)\n            # throws for invalid file format\n            vocab_size, vector_size = (int(x) for x in header.split())\n            if limit:\n                vocab_size = min(vocab_size, limit)\n            result = cls()\n            result.vector_size = vector_size\n            result.syn0 = zeros((vocab_size, vector_size), dtype=datatype)\n\n            def add_word(word, weights):\n                word_id = len(result.vocab)\n                # logging.debug(\"word id: %d, word: %s, weights: %s\" % (word_id, word, weights))\n                if word in result.vocab:\n                    logging.debug(\n                        \"duplicate word '%s' in %s, ignoring all but first\" %\n                        (word, fname))\n                    return\n                if counts is None:\n                    # most common scenario: no vocab file given. just make up\n                    # some bogus counts, in descending order\n                    result.vocab[word] = Vocab(\n                        index=word_id, count=vocab_size - word_id)\n                elif word in counts:\n                    # use count from the vocab file\n                    result.vocab[word] = Vocab(\n                        index=word_id, count=counts[word])\n                else:\n                    # vocab file given, but word is missing -- set count to\n                    # None (TODO: or raise?)\n                    logging.debug(\n                        \"vocabulary file is incomplete: '%s' is missing\" %\n                        word)\n                    result.vocab[word] = Vocab(index=word_id, count=None)\n                result.syn0[word_id] = weights\n                result.index2word.append(word)\n\n            if binary:\n                binary_len = dtype(REAL).itemsize * vector_size\n                for _ in xrange(vocab_size):\n                    # mixed text and binary: read text first, then binary\n                    word = []\n                    while True:\n                        ch = fin.read(1)\n                        if ch == b' ':\n                            break\n                        if ch == b'':\n                            raise EOFError(\n                                \"unexpected end of input; is count incorrect or file otherwise damaged?\")\n                        # ignore newlines in front of words (some binary files\n                        # have)\n                        if ch != b'\\n':\n                            word.append(ch)\n                    word = to_unicode(\n                        b''.join(word), encoding=encoding, errors=unicode_errors)\n                    weights = fromstring(fin.read(binary_len), dtype=REAL)\n                    add_word(word, weights)\n            else:\n                for line_no in xrange(vocab_size):\n                    line = fin.readline()\n                    if line == b'':\n                        raise EOFError(\n                            \"unexpected end of input; is count incorrect or file otherwise damaged?\")\n                    parts = to_unicode(\n                        line.rstrip(),\n                        encoding=encoding,\n                        errors=unicode_errors).split(\" \")\n                    if len(parts) != vector_size + 1:\n                        raise ValueError(\n                            \"invalid vector on line %s (is this really the text format?)\" %\n                            line_no)\n                    word, weights = parts[0], [REAL(x) for x in parts[1:]]\n                    add_word(word, weights)\n        if result.syn0.shape[0] != len(result.vocab):\n            logging.debug(\n                \"duplicate words detected, shrinking matrix size from %i to %i\" %\n                (result.syn0.shape[0], len(result.vocab)))\n            result.syn0 = ascontiguousarray(result.syn0[: len(result.vocab)])\n        assert (len(result.vocab), vector_size) == result.syn0.shape\n        '''\n        KDTree\n        Build KDTree with vectors.\n        http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree\n        '''\n        result.kdt = KDTree(result.syn0, leaf_size=10, metric = \"euclidean\")\n        logging.debug(\"loaded %s matrix from %s\" % (result.syn0.shape, fname))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef word_vec(self, word, use_norm=False):\n        if word in self.vocab:\n            if use_norm:\n                result = self.syn0norm[self.vocab[word].index]\n            else:\n                result = self.syn0[self.vocab[word].index]\n\n            result.setflags(write=False)\n            return result\n        else:\n            raise KeyError(\"word '%s' not in vocabulary\" % word)", "response": "Accept a single word as input. Returns a 1D numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets nearest words with KDTree ranking by cosine distance.", "response": "def neighbours(self, word, size = 10):\n        \"\"\"\n        Get nearest words with KDTree, ranking by cosine distance\n        \"\"\"\n        word = word.strip()\n        v = self.word_vec(word)\n        [distances], [points] = self.kdt.query(array([v]), k = size, return_distance = True)\n        assert len(distances) == len(points), \"distances and points should be in same shape.\"\n        words, scores = [], {}\n        for (x,y) in zip(points, distances):\n            w = self.index2word[x]\n            if w == word: s = 1.0\n            else: s = cosine(v, self.syn0[x])\n            if s < 0: s = abs(s)\n            words.append(w)\n            scores[w] = min(s, 1.0)\n        for x in sorted(words, key=scores.get, reverse=True):\n            yield x, scores[x]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_random_state(seed):\n    if seed is None or seed is np.random:\n        return np.random.mtrand._rand\n    if isinstance(seed, (numbers.Integral, np.integer)):\n        return np.random.RandomState(seed)\n    if isinstance(seed, np.random.RandomState):\n        return seed\n    raise ValueError(\n        '%r cannot be used to seed a np.random.RandomState instance' %\n        seed)", "response": "Turn seed into a np. random. RandomState instance."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef file_or_filename(input):\n    if isinstance(input, string_types):\n        # input was a filename: open as file\n        yield smart_open(input)\n    else:\n        # input already a file-like object; just reset to the beginning\n        input.seek(0)\n        yield input", "response": "Yields a file - like object ready to be read from the beginning of the file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef deaccent(text):\n    if not isinstance(text, unicode):\n        # assume utf8 for byte strings, use default (strict) error handling\n        text = text.decode('utf8')\n    norm = unicodedata.normalize(\"NFD\", text)\n    result = u('').join(ch for ch in norm if unicodedata.category(ch) != 'Mn')\n    return unicodedata.normalize(\"NFC\", result)", "response": "Remove accentuation from the given string. Input text is either a unicode string or utf8 encoded bytestring. Returns input string with accents removed as unicode."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a document into a list of tokens. This lowercases, tokenizes, de-accents (optional). -- the output are final tokens = unicode strings, that won't be processed any further.", "response": "def simple_preprocess(doc, deacc=False, min_len=2, max_len=15):\n    \"\"\"\n    Convert a document into a list of tokens.\n\n    This lowercases, tokenizes, de-accents (optional). -- the output are final\n    tokens = unicode strings, that won't be processed any further.\n\n    \"\"\"\n    tokens = [\n        token for token in tokenize(doc, lower=True, deacc=deacc, errors='ignore')\n        if min_len <= len(token) <= max_len and not token.startswith('_')\n    ]\n    return tokens"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef any2utf8(text, errors='strict', encoding='utf8'):\n    if isinstance(text, unicode):\n        return text.encode('utf8')\n    # do bytestring -> unicode -> utf8 full circle, to ensure valid utf8\n    return unicode(text, encoding, errors=errors).encode('utf8')", "response": "Convert a string to bytestring in utf8."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a string in encoding or unicode to unicode.", "response": "def any2unicode(text, encoding='utf8', errors='strict'):\n    \"\"\"Convert a string (bytestring in `encoding` or unicode), to unicode.\"\"\"\n    if isinstance(text, unicode):\n        return text\n    return unicode(text, encoding, errors=errors)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_digit(obj):\n    '''\n    Check if an object is Number\n    '''\n    return isinstance(obj, (numbers.Integral, numbers.Complex, numbers.Real))", "response": "Check if an object is a digit"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if ch is Chinese character.", "response": "def is_zh(ch):\n    \"\"\"return True if ch is Chinese character.\n    full-width puncts/latins are not counted in.\n    \"\"\"\n    x = ord(ch)\n    # CJK Radicals Supplement and Kangxi radicals\n    if 0x2e80 <= x <= 0x2fef:\n        return True\n    # CJK Unified Ideographs Extension A\n    elif 0x3400 <= x <= 0x4dbf:\n        return True\n    # CJK Unified Ideographs\n    elif 0x4e00 <= x <= 0x9fbb:\n        return True\n    # CJK Compatibility Ideographs\n    elif 0xf900 <= x <= 0xfad9:\n        return True\n    # CJK Unified Ideographs Extension B\n    elif 0x20000 <= x <= 0x2a6df:\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _segment_words(sen):\n    '''\n    segment words with jieba\n    '''\n    words, tags = [], []\n    m = _tokenizer.cut(sen, HMM=True)  # HMM\u66f4\u597d\u7684\u8bc6\u522b\u65b0\u8bcd\n    for x in m:\n        words.append(x.word)\n        tags.append(x.flag)\n    return words, tags", "response": "segment words with jieba\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _load_w2v(model_file=_f_model, binary=True):\n    '''\n    load word2vec model\n    '''\n    if not os.path.exists(model_file):\n        print(\"os.path : \", os.path)\n        raise Exception(\"Model file [%s] does not exist.\" % model_file)\n    return KeyedVectors.load_word2vec_format(\n        model_file, binary=binary, unicode_errors='ignore')", "response": "Load a word2vec model"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting word2vec data by sentence", "response": "def _get_wv(sentence, ignore=False):\n    '''\n    get word2vec data by sentence\n    sentence is segmented string.\n    '''\n    global _vectors\n    vectors = []\n    for y in sentence:\n        y_ = any2unicode(y).strip()\n        if y_ not in _stopwords:\n            syns = nearby(y_)[0]\n            # print(\"sentence %s word: %s\" %(sentence, y_))\n            # print(\"sentence %s word nearby: %s\" %(sentence, \" \".join(syns)))\n            c = []\n            try:\n                c.append(_vectors.word_vec(y_))\n            except KeyError as error:\n                if ignore:\n                    continue\n                else:\n                    logging.warning(\"not exist in w2v model: %s\" % y_)\n                    # c.append(np.zeros((100,), dtype=float))\n                    random_state = np.random.RandomState(seed=(hash(y_) % (2**32 - 1)))\n                    c.append(random_state.uniform(low=-10.0, high=10.0, size=(100,)))\n            for n in syns:\n                if n is None: continue\n                try:\n                    v = _vectors.word_vec(any2unicode(n))\n                except KeyError as error:\n                    # v = np.zeros((100,), dtype=float)\n                    random_state = np.random.RandomState(seed=(hash(n) % (2 ** 32 - 1)))\n                    v = random_state.uniform(low=10.0, high=10.0, size=(100,))\n                c.append(v)\n            r = np.average(c, axis=0)\n            vectors.append(r)\n    return vectors"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _levenshtein_distance(sentence1, sentence2):\n    '''\n    Return the Levenshtein distance between two strings.\n    Based on:\n        http://rosettacode.org/wiki/Levenshtein_distance#Python\n    '''\n    first = any2utf8(sentence1).decode('utf-8', 'ignore')\n    second = any2utf8(sentence2).decode('utf-8', 'ignore')\n    sentence1_len, sentence2_len = len(first), len(second)\n    maxlen = max(sentence1_len, sentence2_len)\n    if sentence1_len > sentence2_len:\n        first, second = second, first\n\n    distances = range(len(first) + 1)\n    for index2, char2 in enumerate(second):\n        new_distances = [index2 + 1]\n        for index1, char1 in enumerate(first):\n            if char1 == char2:\n                new_distances.append(distances[index1])\n            else:\n                new_distances.append(1 + min((distances[index1],\n                                             distances[index1 + 1],\n                                             new_distances[-1])))\n        distances = new_distances\n    levenshtein = distances[-1]\n    d = float((maxlen - levenshtein)/maxlen)\n    # smoothing\n    s = (sigmoid(d * 6) - 0.5) * 2\n    # print(\"smoothing[%s| %s]: %s -> %s\" % (sentence1, sentence2, d, s))\n    return s", "response": "Return the Levenshtein distance between two strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the Levenshtein distance between two words.", "response": "def _nearby_levenshtein_distance(s1, s2):\n    '''\n    \u4f7f\u7528\u7a7a\u95f4\u8ddd\u79bb\u8fd1\u7684\u8bcd\u6c47\u4f18\u5316\u7f16\u8f91\u8ddd\u79bb\u8ba1\u7b97\n    '''\n    s1_len, s2_len = len(s1), len(s2)\n    maxlen = s1_len\n    if s1_len == s2_len:\n        first, second = sorted([s1, s2])\n    elif s1_len < s2_len:\n        first = s1\n        second = s2\n        maxlen = s2_len\n    else:\n        first = s2\n        second = s1\n\n    ft = set() # all related words with first sentence \n    for x in first:\n        ft.add(x)\n        n, _ = nearby(x)\n        for o in n[:10]:\n            ft.add(o)\n    \n    scores = []\n    for x in second:\n        choices = [_levenshtein_distance(x, y) for y in ft]\n        if len(choices) > 0: scores.append(max(choices))\n\n    s = np.sum(scores) / maxlen if len(scores) > 0 else 0\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _similarity_distance(s1, s2, ignore):\n    '''\n    compute similarity with distance measurement\n    '''\n    g = 0.0\n    try:\n        g_ = cosine(_flat_sum_array(_get_wv(s1, ignore)), _flat_sum_array(_get_wv(s2, ignore)))\n        if is_digit(g_): g = g_\n    except: pass\n\n    u = _nearby_levenshtein_distance(s1, s2)\n    logging.debug(\"g: %s, u: %s\" % (g, u))\n    if u >= 0.99:\n        r = 1.0\n    elif u > 0.9:\n        r = _similarity_smooth(g, 0.05, u, 0.05)\n    elif u > 0.8:\n        r = _similarity_smooth(g, 0.1, u, 0.2)\n    elif u > 0.4:\n        r = _similarity_smooth(g, 0.2, u, 0.15)\n    elif u > 0.2:\n        r = _similarity_smooth(g, 0.3, u, 0.1)\n    else:\n        r = _similarity_smooth(g, 0.4, u, 0)\n\n    if r < 0: r = abs(r)\n    r = min(r, 1.0)\n    return float(\"%.3f\" % r)", "response": "compute similarity with distance measurement\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of words and scores for a given word.", "response": "def nearby(word):\n    '''\n    Nearby word\n    '''\n    w = any2unicode(word)\n    # read from cache\n    if w in _cache_nearby: return _cache_nearby[w]\n\n    words, scores = [], []\n    try:\n        for x in _vectors.neighbours(w):\n            words.append(x[0])\n            scores.append(x[1])\n    except: pass # ignore key error, OOV\n    # put into cache\n    _cache_nearby[w] = (words, scores)\n    return words, scores"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compare(s1, s2, seg=True, ignore=False, stopwords=False):\n    '''\n    compare similarity\n    s1 : sentence1\n    s2 : sentence2\n    seg : True : The original sentences need jieba.cut\n          Flase : The original sentences have been cut.\n    ignore: True: ignore OOV words\n            False: get vector randomly for OOV words\n    '''\n    if s1 == s2: return 1.0\n    \n    s1_words = []\n    s2_words = []\n\n    if seg:\n        s1 = [x for x in jieba.cut(s1, cut_all=False, HMM=False)]\n        s2 = [x for x in jieba.cut(s2, cut_all=False, HMM=False)]\n    else:\n        s1 = s1.split()\n        s2 = s2.split()\n\n    # check stopwords\n    if not stopwords:\n        global _stopwords\n        for x in s1: \n            if not x in _stopwords:\n                s1_words.append(x)\n        for x in s2:\n            if not x in _stopwords:\n                s2_words.append(x)\n    else:\n        s1_words = s1 \n        s2_words = s2\n\n    assert len(s1) > 0 and len(s2) > 0, \"The length of s1 and s2 should > 0.\"\n    return _similarity_distance(s1_words, s2_words, ignore)", "response": "compare similarity\n    s1 : sentence1\n    s2 : sentence2\n    seg : True : The original sentences need jieba.cut\n          Flase : The original sentences have been cut.\n    ignore: True: ignore OOV words\n            False: get vector randomly for OOV words"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a sample to the metric.", "response": "def add_sample(self, name, labels, value, timestamp=None, exemplar=None):\n        \"\"\"Add a sample to the metric.\n\n        Internal-only, do not use.\"\"\"\n        self.samples.append(Sample(name, labels, value, timestamp, exemplar))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a metric to the metric family.", "response": "def add_metric(self, labels, value, created=None, timestamp=None):\n        \"\"\"Add a metric to the metric family.\n\n        Args:\n          labels: A list of label values\n          value: The value of the metric\n          created: Optional unix timestamp the child was created at.\n        \"\"\"\n        self.samples.append(Sample(self.name + '_total', dict(zip(self._labelnames, labels)), value, timestamp))\n        if created is not None:\n            self.samples.append(Sample(self.name + '_created', dict(zip(self._labelnames, labels)), created, timestamp))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a metric to the metric family.", "response": "def add_metric(self, labels, count_value, sum_value, timestamp=None):\n        \"\"\"Add a metric to the metric family.\n\n        Args:\n          labels: A list of label values\n          count_value: The count value of the metric.\n          sum_value: The sum value of the metric.\n        \"\"\"\n        self.samples.append(Sample(self.name + '_count', dict(zip(self._labelnames, labels)), count_value, timestamp))\n        self.samples.append(Sample(self.name + '_sum', dict(zip(self._labelnames, labels)), sum_value, timestamp))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a metric to the metric family.", "response": "def add_metric(self, labels, buckets, sum_value, timestamp=None):\n        \"\"\"Add a metric to the metric family.\n\n        Args:\n          labels: A list of label values\n          buckets: A list of lists.\n              Each inner list can be a pair of bucket name and value,\n              or a triple of bucket name, value, and exemplar.\n              The buckets must be sorted, and +Inf present.\n          sum_value: The sum value of the metric.\n        \"\"\"\n        for b in buckets:\n            bucket, value = b[:2]\n            exemplar = None\n            if len(b) == 3:\n                exemplar = b[2]\n            self.samples.append(Sample(\n                self.name + '_bucket',\n                dict(list(zip(self._labelnames, labels)) + [('le', bucket)]),\n                value,\n                timestamp,\n                exemplar,\n            ))\n        # +Inf is last and provides the count value.\n        self.samples.extend([\n            Sample(self.name + '_count', dict(zip(self._labelnames, labels)), buckets[-1][1], timestamp),\n            Sample(self.name + '_sum', dict(zip(self._labelnames, labels)), sum_value, timestamp),\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a metric to the metric family.", "response": "def add_metric(self, labels, buckets, gsum_value, timestamp=None):\n        \"\"\"Add a metric to the metric family.\n\n        Args:\n          labels: A list of label values\n          buckets: A list of pairs of bucket names and values.\n              The buckets must be sorted, and +Inf present.\n          gsum_value: The sum value of the metric.\n        \"\"\"\n        for bucket, value in buckets:\n            self.samples.append(Sample(\n                self.name + '_bucket',\n                dict(list(zip(self._labelnames, labels)) + [('le', bucket)]),\n                value, timestamp))\n        # +Inf is last and provides the count value.\n        self.samples.extend([\n            Sample(self.name + '_gcount', dict(zip(self._labelnames, labels)), buckets[-1][1], timestamp),\n            Sample(self.name + '_gsum', dict(zip(self._labelnames, labels)), gsum_value, timestamp),\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a metric to the metric family.", "response": "def add_metric(self, labels, value, timestamp=None):\n        \"\"\"Add a metric to the metric family.\n\n        Args:\n          labels: A list of label values\n          value: A dict of labels\n        \"\"\"\n        self.samples.append(Sample(\n            self.name + '_info',\n            dict(dict(zip(self._labelnames, labels)), **value),\n            1,\n            timestamp,\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a metric to the metric family.", "response": "def add_metric(self, labels, value, timestamp=None):\n        \"\"\"Add a metric to the metric family.\n\n        Args:\n          labels: A list of label values\n          value: A dict of string state names to booleans\n        \"\"\"\n        labels = tuple(labels)\n        for state, enabled in sorted(value.items()):\n            v = (1 if enabled else 0)\n            self.samples.append(Sample(\n                self.name,\n                dict(zip(self._labelnames + (self.name,), labels + (state,))),\n                v,\n                timestamp,\n            ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a WSGI app which serves the metrics from a registry.", "response": "def make_wsgi_app(registry=REGISTRY):\n    \"\"\"Create a WSGI app which serves the metrics from a registry.\"\"\"\n\n    def prometheus_app(environ, start_response):\n        params = parse_qs(environ.get('QUERY_STRING', ''))\n        r = registry\n        encoder, content_type = choose_encoder(environ.get('HTTP_ACCEPT'))\n        if 'name[]' in params:\n            r = r.restricted_registry(params['name[]'])\n        output = encoder(r)\n\n        status = str('200 OK')\n        headers = [(str('Content-type'), content_type)]\n        start_response(status, headers)\n        return [output]\n\n    return prometheus_app"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef start_wsgi_server(port, addr='', registry=REGISTRY):\n    app = make_wsgi_app(registry)\n    httpd = make_server(addr, port, app, handler_class=_SilentHandler)\n    t = threading.Thread(target=httpd.serve_forever)\n    t.daemon = True\n    t.start()", "response": "Starts a WSGI server for prometheus metrics as a daemon thread."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef start_http_server(port, addr='', registry=REGISTRY):\n    CustomMetricsHandler = MetricsHandler.factory(registry)\n    httpd = _ThreadingSimpleServer((addr, port), CustomMetricsHandler)\n    t = threading.Thread(target=httpd.serve_forever)\n    t.daemon = True\n    t.start()", "response": "Starts an HTTP server for prometheus metrics as a daemon thread"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_to_textfile(path, registry):\n    tmppath = '%s.%s.%s' % (path, os.getpid(), threading.current_thread().ident)\n    with open(tmppath, 'wb') as f:\n        f.write(generate_latest(registry))\n    # rename(2) is atomic.\n    os.rename(tmppath, path)", "response": "Write metrics to the given path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndefault handler that implements HTTP and HTTPS connections.", "response": "def default_handler(url, method, timeout, headers, data):\n    \"\"\"Default handler that implements HTTP/HTTPS connections.\n\n    Used by the push_to_gateway functions. Can be re-used by other handlers.\"\"\"\n\n    def handle():\n        request = Request(url, data=data)\n        request.get_method = lambda: method\n        for k, v in headers:\n            request.add_header(k, v)\n        resp = build_opener(HTTPHandler).open(request, timeout=timeout)\n        if resp.code >= 400:\n            raise IOError(\"error talking to pushgateway: {0} {1}\".format(\n                resp.code, resp.msg))\n\n    return handle"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef push_to_gateway(\n        gateway, job, registry, grouping_key=None, timeout=30,\n        handler=default_handler):\n    \"\"\"Push metrics to the given pushgateway.\n\n    `gateway` the url for your push gateway. Either of the form\n              'http://pushgateway.local', or 'pushgateway.local'.\n              Scheme defaults to 'http' if none is provided\n    `job` is the job label to be attached to all pushed metrics\n    `registry` is an instance of CollectorRegistry\n    `grouping_key` please see the pushgateway documentation for details.\n                   Defaults to None\n    `timeout` is how long push will attempt to connect before giving up.\n              Defaults to 30s, can be set to None for no timeout.\n    `handler` is an optional function which can be provided to perform\n              requests to the 'gateway'.\n              Defaults to None, in which case an http or https request\n              will be carried out by a default handler.\n              If not None, the argument must be a function which accepts\n              the following arguments:\n              url, method, timeout, headers, and content\n              May be used to implement additional functionality not\n              supported by the built-in default handler (such as SSL\n              client certicates, and HTTP authentication mechanisms).\n              'url' is the URL for the request, the 'gateway' argument\n              described earlier will form the basis of this URL.\n              'method' is the HTTP method which should be used when\n              carrying out the request.\n              'timeout' requests not successfully completed after this\n              many seconds should be aborted.  If timeout is None, then\n              the handler should not set a timeout.\n              'headers' is a list of (\"header-name\",\"header-value\") tuples\n              which must be passed to the pushgateway in the form of HTTP\n              request headers.\n              The function should raise an exception (e.g. IOError) on\n              failure.\n              'content' is the data which should be used to form the HTTP\n              Message Body.\n\n    This overwrites all metrics with the same job and grouping_key.\n    This uses the PUT HTTP method.\"\"\"\n    _use_gateway('PUT', gateway, job, registry, grouping_key, timeout, handler)", "response": "Push metrics to the given pushgateway."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_from_gateway(\n        gateway, job, grouping_key=None, timeout=30, handler=default_handler):\n    \"\"\"Delete metrics from the given pushgateway.\n\n    `gateway` the url for your push gateway. Either of the form\n              'http://pushgateway.local', or 'pushgateway.local'.\n              Scheme defaults to 'http' if none is provided\n    `job` is the job label to be attached to all pushed metrics\n    `grouping_key` please see the pushgateway documentation for details.\n                   Defaults to None\n    `timeout` is how long delete will attempt to connect before giving up.\n              Defaults to 30s, can be set to None for no timeout.\n    `handler` is an optional function which can be provided to perform\n              requests to the 'gateway'.\n              Defaults to None, in which case an http or https request\n              will be carried out by a default handler.\n              See the 'prometheus_client.push_to_gateway' documentation\n              for implementation requirements.\n\n    This deletes metrics with the given job and grouping_key.\n    This uses the DELETE HTTP method.\"\"\"\n    _use_gateway('DELETE', gateway, job, None, grouping_key, timeout, handler)", "response": "Delete metrics from the given pushgateway."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a key that can be used to group the hosts by IP Address.", "response": "def instance_ip_grouping_key():\n    \"\"\"Grouping key with instance set to the IP Address of this host.\"\"\"\n    with closing(socket.socket(socket.AF_INET, socket.SOCK_DGRAM)) as s:\n        s.connect(('localhost', 0))\n        return {'instance': s.getsockname()[0]}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef factory(cls, registry):\n        # This implementation relies on MetricsHandler.registry\n        #  (defined above and defaulted to REGISTRY).\n\n        # As we have unicode_literals, we need to create a str()\n        #  object for type().\n        cls_name = str(cls.__name__)\n        MyMetricsHandler = type(cls_name, (cls, object),\n                                {\"registry\": registry})\n        return MyMetricsHandler", "response": "Returns a dynamic MetricsHandler class tied\n           to the passed registry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef text_fd_to_metric_families(fd):\n    name = None\n    allowed_names = []\n    eof = False\n\n    seen_metrics = set()\n\n    def build_metric(name, documentation, typ, unit, samples):\n        if name in seen_metrics:\n            raise ValueError(\"Duplicate metric: \" + name)\n        seen_metrics.add(name)\n        if typ is None:\n            typ = 'unknown'\n        if documentation is None:\n            documentation = ''\n        if unit is None:\n            unit = ''\n        if unit and not name.endswith(\"_\" + unit):\n            raise ValueError(\"Unit does not match metric name: \" + name)\n        if unit and typ in ['info', 'stateset']:\n            raise ValueError(\"Units not allowed for this metric type: \" + name)\n        if typ in ['histogram', 'gaugehistogram']:\n            _check_histogram(samples, name)\n        metric = Metric(name, documentation, typ, unit)\n        # TODO: check labelvalues are valid utf8\n        metric.samples = samples\n        return metric\n\n    for line in fd:\n        if line[-1] == '\\n':\n            line = line[:-1]\n\n        if eof:\n            raise ValueError(\"Received line after # EOF: \" + line)\n\n        if line == '# EOF':\n            eof = True\n        elif line.startswith('#'):\n            parts = line.split(' ', 3)\n            if len(parts) < 4:\n                raise ValueError(\"Invalid line: \" + line)\n            if parts[2] == name and samples:\n                raise ValueError(\"Received metadata after samples: \" + line)\n            if parts[2] != name:\n                if name is not None:\n                    yield build_metric(name, documentation, typ, unit, samples)\n                # New metric\n                name = parts[2]\n                unit = None\n                typ = None\n                documentation = None\n                group = None\n                seen_groups = set()\n                group_timestamp = None\n                group_timestamp_samples = set()\n                samples = []\n                allowed_names = [parts[2]]\n\n            if parts[1] == 'HELP':\n                if documentation is not None:\n                    raise ValueError(\"More than one HELP for metric: \" + line)\n                if len(parts) == 4:\n                    documentation = _unescape_help(parts[3])\n                elif len(parts) == 3:\n                    raise ValueError(\"Invalid line: \" + line)\n            elif parts[1] == 'TYPE':\n                if typ is not None:\n                    raise ValueError(\"More than one TYPE for metric: \" + line)\n                typ = parts[3]\n                if typ == 'untyped':\n                    raise ValueError(\"Invalid TYPE for metric: \" + line)\n                allowed_names = {\n                    'counter': ['_total', '_created'],\n                    'summary': ['_count', '_sum', '', '_created'],\n                    'histogram': ['_count', '_sum', '_bucket', '_created'],\n                    'gaugehistogram': ['_gcount', '_gsum', '_bucket'],\n                    'info': ['_info'],\n                }.get(typ, [''])\n                allowed_names = [name + n for n in allowed_names]\n            elif parts[1] == 'UNIT':\n                if unit is not None:\n                    raise ValueError(\"More than one UNIT for metric: \" + line)\n                unit = parts[3]\n            else:\n                raise ValueError(\"Invalid line: \" + line)\n        else:\n            sample = _parse_sample(line)\n            if sample.name not in allowed_names:\n                if name is not None:\n                    yield build_metric(name, documentation, typ, unit, samples)\n                # Start an unknown metric.\n                name = sample.name\n                documentation = None\n                unit = None\n                typ = 'unknown'\n                samples = []\n                group = None\n                group_timestamp = None\n                group_timestamp_samples = set()\n                seen_groups = set()\n                allowed_names = [sample.name]\n\n            if typ == 'stateset' and name not in sample.labels:\n                raise ValueError(\"Stateset missing label: \" + line)\n            if (typ in ['histogram', 'gaugehistogram'] and name + '_bucket' == sample.name\n                    and (float(sample.labels.get('le', -1)) < 0\n                         or sample.labels['le'] != floatToGoString(sample.labels['le']))):\n                raise ValueError(\"Invalid le label: \" + line)\n            if (typ == 'summary' and name == sample.name\n                    and (not (0 <= float(sample.labels.get('quantile', -1)) <= 1)\n                         or sample.labels['quantile'] != floatToGoString(sample.labels['quantile']))):\n                raise ValueError(\"Invalid quantile label: \" + line)\n\n            g = tuple(sorted(_group_for_sample(sample, name, typ).items()))\n            if group is not None and g != group and g in seen_groups:\n                raise ValueError(\"Invalid metric grouping: \" + line)\n            if group is not None and g == group:\n                if (sample.timestamp is None) != (group_timestamp is None):\n                    raise ValueError(\"Mix of timestamp presence within a group: \" + line)\n                if group_timestamp is not None and group_timestamp > sample.timestamp and typ != 'info':\n                    raise ValueError(\"Timestamps went backwards within a group: \" + line)\n            else:\n                group_timestamp_samples = set()\n\n            series_id = (sample.name, tuple(sorted(sample.labels.items())))\n            if sample.timestamp != group_timestamp or series_id not in group_timestamp_samples:\n                # Not a duplicate due to timestamp truncation.\n                samples.append(sample)\n            group_timestamp_samples.add(series_id)\n\n            group = g\n            group_timestamp = sample.timestamp\n            seen_groups.add(g)\n\n            if typ == 'stateset' and sample.value not in [0, 1]:\n                raise ValueError(\"Stateset samples can only have values zero and one: \" + line)\n            if typ == 'info' and sample.value != 1:\n                raise ValueError(\"Info samples can only have value one: \" + line)\n            if typ == 'summary' and name == sample.name and sample.value < 0:\n                raise ValueError(\"Quantile values cannot be negative: \" + line)\n            if sample.name[len(name):] in ['_total', '_sum', '_count', '_bucket', '_gcount', '_gsum'] and math.isnan(\n                    sample.value):\n                raise ValueError(\"Counter-like samples cannot be NaN: \" + line)\n            if sample.name[len(name):] in ['_total', '_sum', '_count', '_bucket', '_gcount',\n                                           '_gsum'] and sample.value < 0:\n                raise ValueError(\"Counter-like samples cannot be negative: \" + line)\n            if sample.exemplar and not (\n                    typ in ['histogram', 'gaugehistogram']\n                    and sample.name.endswith('_bucket')):\n                raise ValueError(\"Invalid line only histogram/gaugehistogram buckets can have exemplars: \" + line)\n\n    if name is not None:\n        yield build_metric(name, documentation, typ, unit, samples)\n\n    if not eof:\n        raise ValueError(\"Missing # EOF at end\")", "response": "Parse Prometheus text format from a file descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a collector to the registry.", "response": "def register(self, collector):\n        \"\"\"Add a collector to the registry.\"\"\"\n        with self._lock:\n            names = self._get_names(collector)\n            duplicates = set(self._names_to_collectors).intersection(names)\n            if duplicates:\n                raise ValueError(\n                    'Duplicated timeseries in CollectorRegistry: {0}'.format(\n                        duplicates))\n            for name in names:\n                self._names_to_collectors[name] = collector\n            self._collector_to_names[collector] = names"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove a collector from the registry.", "response": "def unregister(self, collector):\n        \"\"\"Remove a collector from the registry.\"\"\"\n        with self._lock:\n            for name in self._collector_to_names[collector]:\n                del self._names_to_collectors[name]\n            del self._collector_to_names[collector]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets names of timeseries the collector produces.", "response": "def _get_names(self, collector):\n        \"\"\"Get names of timeseries the collector produces.\"\"\"\n        desc_func = None\n        # If there's a describe function, use it.\n        try:\n            desc_func = collector.describe\n        except AttributeError:\n            pass\n        # Otherwise, if auto describe is enabled use the collect function.\n        if not desc_func and self._auto_describe:\n            desc_func = collector.collect\n\n        if not desc_func:\n            return []\n\n        result = []\n        type_suffixes = {\n            'counter': ['_total', '_created'],\n            'summary': ['', '_sum', '_count', '_created'],\n            'histogram': ['_bucket', '_sum', '_count', '_created'],\n            'gaugehistogram': ['_bucket', '_gsum', '_gcount'],\n            'info': ['_info'],\n        }\n        for metric in desc_func():\n            for suffix in type_suffixes.get(metric.type, ['']):\n                result.append(metric.name + suffix)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nyielding metrics from the collectors in the registry.", "response": "def collect(self):\n        \"\"\"Yields metrics from the collectors in the registry.\"\"\"\n        collectors = None\n        with self._lock:\n            collectors = copy.copy(self._collector_to_names)\n        for collector in collectors:\n            for metric in collector.collect():\n                yield metric"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef restricted_registry(self, names):\n        names = set(names)\n        collectors = set()\n        with self._lock:\n            for name in names:\n                if name in self._names_to_collectors:\n                    collectors.add(self._names_to_collectors[name])\n        metrics = []\n        for collector in collectors:\n            for metric in collector.collect():\n                samples = [s for s in metric.samples if s[0] in names]\n                if samples:\n                    m = Metric(metric.name, metric.documentation, metric.type)\n                    m.samples = samples\n                    metrics.append(m)\n\n        class RestrictedRegistry(object):\n            def collect(self):\n                return metrics\n\n        return RestrictedRegistry()", "response": "Returns an object that only collects some metrics with the given names."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the sample value for the given name and labels.", "response": "def get_sample_value(self, name, labels=None):\n        \"\"\"Returns the sample value, or None if not found.\n\n        This is inefficient, and intended only for use in unittests.\n        \"\"\"\n        if labels is None:\n            labels = {}\n        for metric in self.collect():\n            for s in metric.samples:\n                if s.name == name and s.labels == labels:\n                    return s.value\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmarking a process as dead.", "response": "def mark_process_dead(pid, path=None):\n    \"\"\"Do bookkeeping for when one process dies in a multi-process setup.\"\"\"\n    if path is None:\n        path = os.environ.get('prometheus_multiproc_dir')\n    for f in glob.glob(os.path.join(path, 'gauge_livesum_{0}.db'.format(pid))):\n        os.remove(f)\n    for f in glob.glob(os.path.join(path, 'gauge_liveall_{0}.db'.format(pid))):\n        os.remove(f)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef merge(files, accumulate=True):\n        metrics = {}\n        for f in files:\n            parts = os.path.basename(f).split('_')\n            typ = parts[0]\n            d = MmapedDict(f, read_mode=True)\n            for key, value in d.read_all_values():\n                metric_name, name, labels = json.loads(key)\n                labels_key = tuple(sorted(labels.items()))\n\n                metric = metrics.get(metric_name)\n                if metric is None:\n                    metric = Metric(metric_name, 'Multiprocess metric', typ)\n                    metrics[metric_name] = metric\n\n                if typ == 'gauge':\n                    pid = parts[2][:-3]\n                    metric._multiprocess_mode = parts[1]\n                    metric.add_sample(name, labels_key + (('pid', pid),), value)\n                else:\n                    # The duplicates and labels are fixed in the next for.\n                    metric.add_sample(name, labels_key, value)\n            d.close()\n\n        for metric in metrics.values():\n            samples = defaultdict(float)\n            buckets = {}\n            for s in metric.samples:\n                name, labels, value = s.name, s.labels, s.value\n                if metric.type == 'gauge':\n                    without_pid = tuple(l for l in labels if l[0] != 'pid')\n                    if metric._multiprocess_mode == 'min':\n                        current = samples.setdefault((name, without_pid), value)\n                        if value < current:\n                            samples[(s.name, without_pid)] = value\n                    elif metric._multiprocess_mode == 'max':\n                        current = samples.setdefault((name, without_pid), value)\n                        if value > current:\n                            samples[(s.name, without_pid)] = value\n                    elif metric._multiprocess_mode == 'livesum':\n                        samples[(name, without_pid)] += value\n                    else:  # all/liveall\n                        samples[(name, labels)] = value\n\n                elif metric.type == 'histogram':\n                    bucket = tuple(float(l[1]) for l in labels if l[0] == 'le')\n                    if bucket:\n                        # _bucket\n                        without_le = tuple(l for l in labels if l[0] != 'le')\n                        buckets.setdefault(without_le, {})\n                        buckets[without_le].setdefault(bucket[0], 0.0)\n                        buckets[without_le][bucket[0]] += value\n                    else:\n                        # _sum/_count\n                        samples[(s.name, labels)] += value\n\n                else:\n                    # Counter and Summary.\n                    samples[(s.name, labels)] += value\n\n            # Accumulate bucket values.\n            if metric.type == 'histogram':\n                for labels, values in buckets.items():\n                    acc = 0.0\n                    for bucket, value in sorted(values.items()):\n                        sample_key = (\n                            metric.name + '_bucket',\n                            labels + (('le', floatToGoString(bucket)),),\n                        )\n                        if accumulate:\n                            acc += value\n                            samples[sample_key] = acc\n                        else:\n                            samples[sample_key] = value\n                    if accumulate:\n                        samples[(metric.name + '_count', labels)] = acc\n\n            # Convert to correct sample format.\n            metric.samples = [Sample(name_, dict(labels), value) for (name_, labels), value in samples.items()]\n        return metrics.values()", "response": "Merge metrics from given mmap files."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getargspec(f):\n    spec = getfullargspec(f)\n    return ArgSpec(spec.args, spec.varargs, spec.varkw, spec.defaults)", "response": "A replacement for inspect. getargspec that returns ArgSpec"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef decorate(func, caller):\n    evaldict = dict(_call_=caller, _func_=func)\n    fun = FunctionMaker.create(\n        func, \"return _call_(_func_, %(shortsignature)s)\",\n        evaldict, __wrapped__=func)\n    if hasattr(func, '__qualname__'):\n        fun.__qualname__ = func.__qualname__\n    return fun", "response": "Decorates a function using a caller.\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef append(a, vancestors):\n    add = True\n    for j, va in enumerate(vancestors):\n        if issubclass(va, a):\n            add = False\n            break\n        if issubclass(a, va):\n            vancestors[j] = a\n            add = False\n    if add:\n        vancestors.append(a)", "response": "Append a to the list of virtual ancestors unless it is already included."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing Prometheus text format from a file descriptor. Yields Metric s.", "response": "def text_fd_to_metric_families(fd):\n    \"\"\"Parse Prometheus text format from a file descriptor.\n\n    This is a laxer parser than the main Go parser,\n    so successful parsing does not imply that the parsed\n    text meets the specification.\n\n    Yields Metric's.\n    \"\"\"\n    name = ''\n    documentation = ''\n    typ = 'untyped'\n    samples = []\n    allowed_names = []\n\n    def build_metric(name, documentation, typ, samples):\n        # Munge counters into OpenMetrics representation\n        # used internally.\n        if typ == 'counter':\n            if name.endswith('_total'):\n                name = name[:-6]\n            else:\n                new_samples = []\n                for s in samples:\n                    new_samples.append(Sample(s[0] + '_total', *s[1:]))\n                    samples = new_samples\n        metric = Metric(name, documentation, typ)\n        metric.samples = samples\n        return metric\n\n    for line in fd:\n        line = line.strip()\n\n        if line.startswith('#'):\n            parts = line.split(None, 3)\n            if len(parts) < 2:\n                continue\n            if parts[1] == 'HELP':\n                if parts[2] != name:\n                    if name != '':\n                        yield build_metric(name, documentation, typ, samples)\n                    # New metric\n                    name = parts[2]\n                    typ = 'untyped'\n                    samples = []\n                    allowed_names = [parts[2]]\n                if len(parts) == 4:\n                    documentation = _replace_help_escaping(parts[3])\n                else:\n                    documentation = ''\n            elif parts[1] == 'TYPE':\n                if parts[2] != name:\n                    if name != '':\n                        yield build_metric(name, documentation, typ, samples)\n                    # New metric\n                    name = parts[2]\n                    documentation = ''\n                    samples = []\n                typ = parts[3]\n                allowed_names = {\n                    'counter': [''],\n                    'gauge': [''],\n                    'summary': ['_count', '_sum', ''],\n                    'histogram': ['_count', '_sum', '_bucket'],\n                }.get(typ, [''])\n                allowed_names = [name + n for n in allowed_names]\n            else:\n                # Ignore other comment tokens\n                pass\n        elif line == '':\n            # Ignore blank lines\n            pass\n        else:\n            sample = _parse_sample(line)\n            if sample.name not in allowed_names:\n                if name != '':\n                    yield build_metric(name, documentation, typ, samples)\n                # New metric, yield immediately as untyped singleton\n                name = ''\n                documentation = ''\n                typ = 'untyped'\n                samples = []\n                allowed_names = []\n                yield build_metric(sample[0], documentation, typ, [sample])\n            else:\n                samples.append(sample)\n\n    if name != '':\n        yield build_metric(name, documentation, typ, samples)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mmap_key(metric_name, name, labelnames, labelvalues):\n    # ensure labels are in consistent order for identity\n    labels = dict(zip(labelnames, labelvalues))\n    return json.dumps([metric_name, name, labels], sort_keys=True)", "response": "Format a key for use in the mmap file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _init_value(self, key):\n        encoded = key.encode('utf-8')\n        # Pad to be 8-byte aligned.\n        padded = encoded + (b' ' * (8 - (len(encoded) + 4) % 8))\n        value = struct.pack('i{0}sd'.format(len(padded)).encode(), len(encoded), padded, 0.0)\n        while self._used + len(value) > self._capacity:\n            self._capacity *= 2\n            self._f.truncate(self._capacity)\n            self._m = mmap.mmap(self._f.fileno(), self._capacity)\n        self._m[self._used:self._used + len(value)] = value\n\n        # Update how much space we've used.\n        self._used += len(value)\n        _pack_integer(self._m, 0, self._used)\n        self._positions[key] = self._used - 8", "response": "Initialize a value. Lock must be held by caller."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read_all_values(self):\n\n        pos = 8\n\n        # cache variables to local ones and prevent attributes lookup\n        # on every loop iteration\n        used = self._used\n        data = self._m\n        unpack_from = struct.unpack_from\n\n        while pos < used:\n            encoded_len = _unpack_integer(data, pos)[0]\n            # check we are not reading beyond bounds\n            if encoded_len + pos > used:\n                msg = 'Read beyond file size detected, %s is corrupted.'\n                raise RuntimeError(msg % self._fname)\n            pos += 4\n            encoded = unpack_from(('%ss' % encoded_len).encode(), data, pos)[0]\n            padded_len = encoded_len + (8 - (encoded_len + 4) % 8)\n            pos += padded_len\n            value = _unpack_double(data, pos)[0]\n            yield encoded.decode('utf-8'), value, pos\n            pos += 8", "response": "Yields all the values in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the child for the given labelset.", "response": "def labels(self, *labelvalues, **labelkwargs):\n        \"\"\"Return the child for the given labelset.\n\n        All metrics can have labels, allowing grouping of related time series.\n        Taking a counter as an example:\n\n            from prometheus_client import Counter\n\n            c = Counter('my_requests_total', 'HTTP Failures', ['method', 'endpoint'])\n            c.labels('get', '/').inc()\n            c.labels('post', '/submit').inc()\n\n        Labels can also be provided as keyword arguments:\n\n            from prometheus_client import Counter\n\n            c = Counter('my_requests_total', 'HTTP Failures', ['method', 'endpoint'])\n            c.labels(method='get', endpoint='/').inc()\n            c.labels(method='post', endpoint='/submit').inc()\n\n        See the best practices on [naming](http://prometheus.io/docs/practices/naming/)\n        and [labels](http://prometheus.io/docs/practices/instrumentation/#use-labels).\n        \"\"\"\n        if not self._labelnames:\n            raise ValueError('No label names were set when constructing %s' % self)\n\n        if self._labelvalues:\n            raise ValueError('%s already has labels set (%s); can not chain calls to .labels()' % (\n                self,\n                dict(zip(self._labelnames, self._labelvalues))\n            ))\n\n        if labelvalues and labelkwargs:\n            raise ValueError(\"Can't pass both *args and **kwargs\")\n\n        if labelkwargs:\n            if sorted(labelkwargs) != sorted(self._labelnames):\n                raise ValueError('Incorrect label names')\n            labelvalues = tuple(unicode(labelkwargs[l]) for l in self._labelnames)\n        else:\n            if len(labelvalues) != len(self._labelnames):\n                raise ValueError('Incorrect label count')\n            labelvalues = tuple(unicode(l) for l in labelvalues)\n        with self._lock:\n            if labelvalues not in self._metrics:\n                self._metrics[labelvalues] = self.__class__(\n                    self._name,\n                    documentation=self._documentation,\n                    labelnames=self._labelnames,\n                    unit=self._unit,\n                    labelvalues=labelvalues,\n                    **self._kwargs\n                )\n            return self._metrics[labelvalues]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove(self, *labelvalues):\n        if not self._labelnames:\n            raise ValueError('No label names were set when constructing %s' % self)\n\n        \"\"\"Remove the given labelset from the metric.\"\"\"\n        if len(labelvalues) != len(self._labelnames):\n            raise ValueError('Incorrect label count (expected %d, got %s)' % (len(self._labelnames), labelvalues))\n        labelvalues = tuple(unicode(l) for l in labelvalues)\n        with self._lock:\n            del self._metrics[labelvalues]", "response": "Removes the given labelset from the metric."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalling the provided function to return the Gauge value.", "response": "def set_function(self, f):\n        \"\"\"Call the provided function to return the Gauge value.\n\n        The function must return a float, and may be called from\n        multiple threads. All other methods of the Gauge become NOOPs.\n        \"\"\"\n\n        def samples(self):\n            return (('', {}, float(f())),)\n\n        self._child_samples = types.MethodType(samples, self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nobserves the given amount.", "response": "def observe(self, amount):\n        \"\"\"Observe the given amount.\"\"\"\n        self._count.inc(1)\n        self._sum.inc(amount)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef observe(self, amount):\n        self._sum.inc(amount)\n        for i, bound in enumerate(self._upper_bounds):\n            if amount <= bound:\n                self._buckets[i].inc(1)\n                break", "response": "Observe the given amount."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef info(self, val):\n        if self._labelname_set.intersection(val.keys()):\n            raise ValueError('Overlapping labels for Info metric, metric: %s child: %s' % (\n                self._labelnames, val))\n        with self._lock:\n            self._value = dict(val)", "response": "Set the value of the Info metric."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef state(self, state):\n        with self._lock:\n            self._value = self._states.index(state)", "response": "Set enum metric state."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an instance of the class from given settings.", "response": "def from_settings(cls, settings):\n        \"\"\"Returns an instance from given settings.\n\n        This uses by default the key ``dupefilter:<timestamp>``. When using the\n        ``scrapy_redis.scheduler.Scheduler`` class, this method is not used as\n        it needs to pass the spider name in the key.\n\n        Parameters\n        ----------\n        settings : scrapy.settings.Settings\n\n        Returns\n        -------\n        RFPDupeFilter\n            A RFPDupeFilter instance.\n\n\n        \"\"\"\n        server = get_redis_from_settings(settings)\n        # XXX: This creates one-time key. needed to support to use this\n        # class as standalone dupefilter with scrapy's default scheduler\n        # if scrapy passes spider on open() method this wouldn't be needed\n        # TODO: Use SCRAPY_JOB env as default and fallback to timestamp.\n        key = defaults.DUPEFILTER_KEY % {'timestamp': int(time.time())}\n        debug = settings.getbool('DUPEFILTER_DEBUG')\n        return cls(server, key=key, debug=debug)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef request_seen(self, request):\n        fp = self.request_fingerprint(request)\n        # This returns the number of values added, zero if already exists.\n        added = self.server.sadd(self.key, fp)\n        return added == 0", "response": "Returns True if request was already seen."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlogging given request. Parameters ---------- request : scrapy.http.Request spider : scrapy.spiders.Spider", "response": "def log(self, request, spider):\n        \"\"\"Logs given request.\n\n        Parameters\n        ----------\n        request : scrapy.http.Request\n        spider : scrapy.spiders.Spider\n\n        \"\"\"\n        if self.debug:\n            msg = \"Filtered duplicate request: %(request)s\"\n            self.logger.debug(msg, {'request': request}, extra={'spider': spider})\n        elif self.logdupes:\n            msg = (\"Filtered duplicate request %(request)s\"\n                   \" - no more duplicates will be shown\"\n                   \" (see DUPEFILTER_DEBUG to show all duplicates)\")\n            self.logger.debug(msg, {'request': request}, extra={'spider': spider})\n            self.logdupes = False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprocess items from a redis queue.", "response": "def process_items(r, keys, timeout, limit=0, log_every=1000, wait=.1):\n    \"\"\"Process items from a redis queue.\n\n    Parameters\n    ----------\n    r : Redis\n        Redis connection instance.\n    keys : list\n        List of keys to read the items from.\n    timeout: int\n        Read timeout.\n\n    \"\"\"\n    limit = limit or float('inf')\n    processed = 0\n    while processed < limit:\n        # Change ``blpop`` to ``brpop`` to process as LIFO.\n        ret = r.blpop(keys, timeout)\n        # If data is found before the timeout then we consider we are done.\n        if ret is None:\n            time.sleep(wait)\n            continue\n\n        source, data = ret\n        try:\n            item = json.loads(data)\n        except Exception:\n            logger.exception(\"Failed to load item:\\n%r\", pprint.pformat(data))\n            continue\n\n        try:\n            name = item.get('name') or item.get('title')\n            url = item.get('url') or item.get('link')\n            logger.debug(\"[%s] Processing item: %s <%s>\", source, name, url)\n        except KeyError:\n            logger.exception(\"[%s] Failed to process item:\\n%r\",\n                             source, pprint.pformat(item))\n            continue\n\n        processed += 1\n        if processed % log_every == 0:\n            logger.info(\"Processed %s items\", processed)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_redis_from_settings(settings):\n    params = defaults.REDIS_PARAMS.copy()\n    params.update(settings.getdict('REDIS_PARAMS'))\n    # XXX: Deprecate REDIS_* settings.\n    for source, dest in SETTINGS_PARAMS_MAP.items():\n        val = settings.get(source)\n        if val:\n            params[dest] = val\n\n    # Allow ``redis_cls`` to be a path to a class.\n    if isinstance(params.get('redis_cls'), six.string_types):\n        params['redis_cls'] = load_object(params['redis_cls'])\n\n    return get_redis(**params)", "response": "Returns a Redis client instance from a Scrapy settings object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a Redis client instance.", "response": "def get_redis(**kwargs):\n    \"\"\"Returns a redis client instance.\n\n    Parameters\n    ----------\n    redis_cls : class, optional\n        Defaults to ``redis.StrictRedis``.\n    url : str, optional\n        If given, ``redis_cls.from_url`` is used to instantiate the class.\n    **kwargs\n        Extra parameters to be passed to the ``redis_cls`` class.\n\n    Returns\n    -------\n    server\n        Redis client instance.\n\n    \"\"\"\n    redis_cls = kwargs.pop('redis_cls', defaults.REDIS_CLS)\n    url = kwargs.pop('url', None)\n    if url:\n        return redis_cls.from_url(url, **kwargs)\n    else:\n        return redis_cls(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bytes_to_str(s, encoding='utf-8'):\n    if six.PY3 and isinstance(s, bytes):\n        return s.decode(encoding)\n    return s", "response": "Returns a str if a bytes object is given."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setup_redis(self, crawler=None):\n        if self.server is not None:\n            return\n\n        if crawler is None:\n            # We allow optional crawler argument to keep backwards\n            # compatibility.\n            # XXX: Raise a deprecation warning.\n            crawler = getattr(self, 'crawler', None)\n\n        if crawler is None:\n            raise ValueError(\"crawler is required\")\n\n        settings = crawler.settings\n\n        if self.redis_key is None:\n            self.redis_key = settings.get(\n                'REDIS_START_URLS_KEY', defaults.START_URLS_KEY,\n            )\n\n        self.redis_key = self.redis_key % {'name': self.name}\n\n        if not self.redis_key.strip():\n            raise ValueError(\"redis_key must not be empty\")\n\n        if self.redis_batch_size is None:\n            # TODO: Deprecate this setting (REDIS_START_URLS_BATCH_SIZE).\n            self.redis_batch_size = settings.getint(\n                'REDIS_START_URLS_BATCH_SIZE',\n                settings.getint('CONCURRENT_REQUESTS'),\n            )\n\n        try:\n            self.redis_batch_size = int(self.redis_batch_size)\n        except (TypeError, ValueError):\n            raise ValueError(\"redis_batch_size must be an integer\")\n\n        if self.redis_encoding is None:\n            self.redis_encoding = settings.get('REDIS_ENCODING', defaults.REDIS_ENCODING)\n\n        self.logger.info(\"Reading start URLs from redis key '%(redis_key)s' \"\n                         \"(batch size: %(redis_batch_size)s, encoding: %(redis_encoding)s\",\n                         self.__dict__)\n\n        self.server = connection.from_settings(crawler.settings)\n        # The idle signal is called when the spider has no requests left,\n        # that's when we will schedule new requests from redis queue\n        crawler.signals.connect(self.spider_idle, signal=signals.spider_idle)", "response": "Setup redis connection and idle signal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a generator that yields the next set of requests.", "response": "def next_requests(self):\n        \"\"\"Returns a request to be scheduled or none.\"\"\"\n        use_set = self.settings.getbool('REDIS_START_URLS_AS_SET', defaults.START_URLS_AS_SET)\n        fetch_one = self.server.spop if use_set else self.server.lpop\n        # XXX: Do we need to use a timeout here?\n        found = 0\n        # TODO: Use redis pipeline execution.\n        while found < self.redis_batch_size:\n            data = fetch_one(self.redis_key)\n            if not data:\n                # Queue empty.\n                break\n            req = self.make_request_from_data(data)\n            if req:\n                yield req\n                found += 1\n            else:\n                self.logger.debug(\"Request not made from data: %r\", data)\n\n        if found:\n            self.logger.debug(\"Read %s requests from '%s'\", found, self.redis_key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_request_from_data(self, data):\n        url = bytes_to_str(data, self.redis_encoding)\n        return self.make_requests_from_url(url)", "response": "Returns a Request instance from data coming from Redis."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nschedules a request if available.", "response": "def schedule_next_requests(self):\n        \"\"\"Schedules a request if available\"\"\"\n        # TODO: While there is capacity, schedule a batch of redis requests.\n        for req in self.next_requests():\n            self.crawler.engine.crawl(req, spider=self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _encode_request(self, request):\n        obj = request_to_dict(request, self.spider)\n        return self.serializer.dumps(obj)", "response": "Encode a request object"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndecodes an encoded request from the serialized version", "response": "def _decode_request(self, encoded_request):\n        \"\"\"Decode an request previously encoded\"\"\"\n        obj = self.serializer.loads(encoded_request)\n        return request_from_dict(obj, self.spider)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npush a request onto the end of the list.", "response": "def push(self, request):\n        \"\"\"Push a request\"\"\"\n        self.server.lpush(self.key, self._encode_request(request))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef push(self, request):\n        data = self._encode_request(request)\n        score = -request.priority\n        # We don't use zadd method as the order of arguments change depending on\n        # whether the class is Redis or StrictRedis, and the option of using\n        # kwargs only accepts strings, not bytes.\n        self.server.execute_command('ZADD', self.key, score, data)", "response": "Push a request to the set with the highest priority."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npop a request from the queue.", "response": "def pop(self, timeout=0):\n        \"\"\"\n        Pop a request\n        timeout not support in this queue class\n        \"\"\"\n        # use atomic range/remove using multi/exec\n        pipe = self.server.pipeline()\n        pipe.multi()\n        pipe.zrange(self.key, 0, 0).zremrangebyrank(self.key, 0, 0)\n        results, count = pipe.execute()\n        if results:\n            return self._decode_request(results[0])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rgb2short(r, g, b):\n    incs = (0x00, 0x5f, 0x87, 0xaf, 0xd7, 0xff)\n    # Break 6-char RGB code into 3 integer vals.\n    parts = [ r, g, b] \n    res = []\n    for part in parts:\n        i = 0\n        while i < len(incs)-1:\n            s, b = incs[i], incs[i+1]  # smaller, bigger\n            if s <= part <= b:\n                s1 = abs(s - part)\n                b1 = abs(b - part)\n                if s1 < b1: closest = s\n                else: closest = b\n                res.append(closest)\n                break\n            i += 1\n    #print '***', res\n    return RGB2SHORT_DICT[tuple(res)]", "response": "Find the closest xterm - 256 approximation to the given RGB value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef n_or_empty(self, _key):\n        return unicode_(self[_key]) if int(self[_key]) > 1 else u''", "response": "Returns a string that represents the number of the entry in the class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef maybe_shorten_name(powerline, name):\n    max_size = powerline.segment_conf(\"cwd\", \"max_dir_size\")\n    if max_size:\n        return name[:max_size]\n    return name", "response": "If the user has asked for each directory name to be shortened return the full\n    name. Otherwise returns the full\n    name."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the foreground and background color to use for the given name.", "response": "def get_fg_bg(powerline, name, is_last_dir):\n    \"\"\"Returns the foreground and background color to use for the given name.\n    \"\"\"\n    if requires_special_home_display(powerline, name):\n        return (powerline.theme.HOME_FG, powerline.theme.HOME_BG,)\n\n    if is_last_dir:\n        return (powerline.theme.CWD_FG, powerline.theme.PATH_BG,)\n    else:\n        return (powerline.theme.PATH_FG, powerline.theme.PATH_BG,)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_valid_cwd():\n    try:\n        cwd = _current_dir()\n    except:\n        warn(\"Your current directory is invalid. If you open a ticket at \" +\n            \"https://github.com/milkbikis/powerline-shell/issues/new \" +\n            \"we would love to help fix the issue.\")\n        sys.stdout.write(\"> \")\n        sys.exit(1)\n\n    parts = cwd.split(os.sep)\n    up = cwd\n    while parts and not os.path.exists(up):\n        parts.pop()\n        up = os.sep.join(parts)\n    if cwd != up:\n        warn(\"Your current directory is invalid. Lowest valid directory: \"\n             + up)\n    return cwd", "response": "Determine and check the current working directory for validity."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_email(value: str) -> Tuple[str, str]:\n    if email_validator is None:\n        raise ImportError('email-validator is not installed, run `pip install pydantic[email]`')\n\n    m = PRETTY_REGEX.fullmatch(value)\n    name: Optional[str] = None\n    if m:\n        name, value = m.groups()\n\n    email = value.strip()\n\n    try:\n        email_validator.validate_email(email, check_deliverability=False)\n    except email_validator.EmailNotValidError as e:\n        raise errors.EmailError() from e\n\n    return name or email[: email.index('@')], email.lower()", "response": "Validate an email address."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a DSN from connection settings.", "response": "def make_dsn(\n    *,\n    driver: str,\n    user: str = None,\n    password: str = None,\n    host: str = None,\n    port: str = None,\n    name: str = None,\n    query: Dict[str, Any] = None,\n) -> str:\n    \"\"\"\n    Create a DSN from from connection settings.\n\n    Stolen approximately from sqlalchemy/engine/url.py:URL.\n    \"\"\"\n    s = driver + '://'\n    if user is not None:\n        s += _rfc_1738_quote(user)\n        if password is not None:\n            s += ':' + _rfc_1738_quote(password)\n        s += '@'\n    if host is not None:\n        if ':' in host:\n            s += '[{}]'.format(host)\n        else:\n            s += host\n    if port is not None:\n        s += ':{}'.format(int(port))\n    if name is not None:\n        s += '/' + name\n    query = query or {}\n    if query:\n        keys = list(query)\n        keys.sort()\n        s += '?' + '&'.join('{}={}'.format(k, query[k]) for k in keys)\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nimporting a dotted module path and return the attribute or class designated by the last name in the path. Raise ImportError if the import fails.", "response": "def import_string(dotted_path: str) -> Any:\n    \"\"\"\n    Stolen approximately from django. Import a dotted module path and return the attribute/class designated by the\n    last name in the path. Raise ImportError if the import fails.\n    \"\"\"\n    try:\n        module_path, class_name = dotted_path.strip(' ').rsplit('.', 1)\n    except ValueError as e:\n        raise ImportError(f'\"{dotted_path}\" doesn\\'t look like a module path') from e\n\n    module = import_module(module_path)\n    try:\n        return getattr(module, class_name)\n    except AttributeError as e:\n        raise ImportError(f'Module \"{module_path}\" does not define a \"{class_name}\" attribute') from e"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef truncate(v: str, *, max_len: int = 80) -> str:\n    if isinstance(v, str) and len(v) > (max_len - 2):\n        # -3 so quote + string + \u2026 + quote has correct length\n        return repr(v[: (max_len - 3)] + '\u2026')\n    v = repr(v)\n    if len(v) > max_len:\n        v = v[: max_len - 1] + '\u2026'\n    return v", "response": "Truncates a value and adds an ellipsis to the end if it was too long."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nensuring that the field s name shadows an existing attribute of the BaseModel.", "response": "def validate_field_name(bases: List[Type['BaseModel']], field_name: str) -> None:\n    \"\"\"\n    Ensure that the field's name does not shadow an existing attribute of the model.\n    \"\"\"\n    for base in bases:\n        if getattr(base, field_name, None):\n            raise NameError(\n                f'Field name \"{field_name}\" shadows a BaseModel attribute; '\n                f'use a different field name with \"alias=\\'{field_name}\\'\".'\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a regex pattern that matches the given URL.", "response": "def url_regex_generator(*, relative: bool, require_tld: bool) -> Pattern[str]:\n    \"\"\"\n    Url regex generator taken from Marshmallow library,\n    for details please follow library source code:\n        https://github.com/marshmallow-code/marshmallow/blob/298870ef6c089fb4d91efae9ca4168453ffe00d2/marshmallow/validate.py#L37\n    \"\"\"\n    return re.compile(\n        r''.join(\n            (\n                r'^',\n                r'(' if relative else r'',\n                r'(?:[a-z0-9\\.\\-\\+]*)://',  # scheme is validated separately\n                r'(?:[^:@]+?:[^:@]*?@|)',  # basic auth\n                r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+',\n                r'(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|',  # domain...\n                r'localhost|',  # localhost...\n                (\n                    r'(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.?)|' if not require_tld else r''\n                ),  # allow dotless hostnames\n                r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}|',  # ...or ipv4\n                r'\\[[A-F0-9]*:[A-F0-9:]+\\])',  # ...or ipv6\n                r'(?::\\d+)?',  # optional port\n                r')?' if relative else r'',  # host is optional, allow for relative URLs\n                r'(?:/?|[/?]\\S+)$',\n            )\n        ),\n        re.IGNORECASE,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resolve_annotations(raw_annotations: Dict[str, AnyType], module_name: Optional[str]) -> Dict[str, AnyType]:\n    if module_name:\n        base_globals: Optional[Dict[str, Any]] = sys.modules[module_name].__dict__\n    else:\n        base_globals = None\n    annotations = {}\n    for name, value in raw_annotations.items():\n        if isinstance(value, str):\n            value = ForwardRef(value, is_argument=False)\n        try:\n            value = _eval_type(value, base_globals, None)\n        except NameError:\n            # this is ok, it can be fixed with update_forward_refs\n            pass\n        annotations[name] = value\n    return annotations", "response": "Resolve string or ForwardRef annotations into type objects if possible."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_field_forward_refs(field: 'Field', globalns: Any, localns: Any) -> None:\n    if type(field.type_) == ForwardRef:\n        field.type_ = field.type_._evaluate(globalns, localns or None)  # type: ignore\n        field.prepare()\n    if field.sub_fields:\n        for sub_f in field.sub_fields:\n            update_field_forward_refs(sub_f, globalns=globalns, localns=localns)", "response": "Update ForwardRefs on a Field based on globalns and localns."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nassuming IPv4Network initialised with a default ``strict`` argument See more: https://docs.python.org/library/ipaddress.html#ipaddress.IPv4Network", "response": "def ip_v4_network_validator(v: Any) -> IPv4Network:\n    \"\"\"\n    Assume IPv4Network initialised with a default ``strict`` argument\n\n    See more:\n    https://docs.python.org/library/ipaddress.html#ipaddress.IPv4Network\n    \"\"\"\n    if isinstance(v, IPv4Network):\n        return v\n\n    with change_exception(errors.IPv4NetworkError, ValueError):\n        return IPv4Network(v)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nassumes IPv6Network initialised with a default ``strict`` argument See more: https://docs.python.org/library/ipaddress.html#ipaddress.IPv6Network", "response": "def ip_v6_network_validator(v: Any) -> IPv6Network:\n    \"\"\"\n    Assume IPv6Network initialised with a default ``strict`` argument\n\n    See more:\n    https://docs.python.org/library/ipaddress.html#ipaddress.IPv6Network\n    \"\"\"\n    if isinstance(v, IPv6Network):\n        return v\n\n    with change_exception(errors.IPv6NetworkError, ValueError):\n        return IPv6Network(v)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates that the value is callable.", "response": "def callable_validator(v: Any) -> AnyCallable:\n    \"\"\"\n    Perform a simple check if the value is callable.\n\n    Note: complete matching of argument type hints and return types is not performed\n    \"\"\"\n    if callable(v):\n        return v\n\n    raise errors.CallableError(value=v)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build_environ(self) -> Dict[str, Optional[str]]:\n        d: Dict[str, Optional[str]] = {}\n\n        if self.__config__.case_insensitive:\n            env_vars = {k.lower(): v for k, v in os.environ.items()}\n        else:\n            env_vars = cast(Dict[str, str], os.environ)\n\n        for field in self.__fields__.values():\n            if field.has_alias:\n                env_name = field.alias\n            else:\n                env_name = self.__config__.env_prefix + field.name.upper()\n\n            env_name_ = env_name.lower() if self.__config__.case_insensitive else env_name\n            env_val = env_vars.get(env_name_, None)\n\n            if env_val:\n                if field.is_complex():\n                    try:\n                        env_val = json.loads(env_val)\n                    except ValueError as e:\n                        raise SettingsError(f'error parsing JSON for \"{env_name}\"') from e\n                d[field.alias] = env_val\n        return d", "response": "Build environment variables suitable for passing to the Model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndecorate methods on the class indicating that they should be used to validate fields :param fields: which field(s) the method should be called on :param pre: whether or not this validator should be called before the standard validators (else after) :param whole: for complex objects (sets, lists etc.) whether to validate individual elements or the whole object :param always: whether this method and other validators should be called even if the value is missing :param check_fields: whether to check that the fields actually exist on the model", "response": "def validator(\n    *fields: str, pre: bool = False, whole: bool = False, always: bool = False, check_fields: bool = True\n) -> Callable[[AnyCallable], classmethod]:\n    \"\"\"\n    Decorate methods on the class indicating that they should be used to validate fields\n    :param fields: which field(s) the method should be called on\n    :param pre: whether or not this validator should be called before the standard validators (else after)\n    :param whole: for complex objects (sets, lists etc.) whether to validate individual elements or the whole object\n    :param always: whether this method and other validators should be called even if the value is missing\n    :param check_fields: whether to check that the fields actually exist on the model\n    \"\"\"\n    if not fields:\n        raise ConfigError('validator with no fields specified')\n    elif isinstance(fields[0], FunctionType):\n        raise ConfigError(\n            \"validators should be used with fields and keyword arguments, not bare. \"\n            \"E.g. usage should be `@validator('<field_name>', ...)`\"\n        )\n\n    def dec(f: AnyCallable) -> classmethod:\n        # avoid validators with duplicated names since without this validators can be overwritten silently\n        # which generally isn't the intended behaviour, don't run in ipython - see #312\n        if not in_ipython():  # pragma: no branch\n            ref = f.__module__ + '.' + f.__qualname__\n            if ref in _FUNCS:\n                raise ConfigError(f'duplicate validator function \"{ref}\"')\n            _FUNCS.add(ref)\n        f_cls = classmethod(f)\n        f_cls.__validator_config = fields, Validator(f, pre, whole, always, check_fields)  # type: ignore\n        return f_cls\n\n    return dec"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_generic_validator(validator: AnyCallable) -> 'ValidatorCallable':\n    sig = signature(validator)\n    args = list(sig.parameters.keys())\n    first_arg = args.pop(0)\n    if first_arg == 'self':\n        raise ConfigError(\n            f'Invalid signature for validator {validator}: {sig}, \"self\" not permitted as first argument, '\n            f'should be: (cls, value, values, config, field), \"values\", \"config\" and \"field\" are all optional.'\n        )\n    elif first_arg == 'cls':\n        # assume the second argument is value\n        return wraps(validator)(_generic_validator_cls(validator, sig, set(args[1:])))\n    else:\n        # assume the first argument was value which has already been removed\n        return wraps(validator)(_generic_validator_basic(validator, sig, set(args)))", "response": "Make a generic function which calls a validator with the right arguments."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _validate_sequence_like(\n        self, v: Any, values: Dict[str, Any], loc: 'LocType', cls: Optional['ModelOrDc']\n    ) -> 'ValidateReturn':\n        \"\"\"\n        Validate sequence-like containers: lists, tuples, sets and generators\n        \"\"\"\n\n        if not sequence_like(v):\n            e: errors_.PydanticTypeError\n            if self.shape is Shape.LIST:\n                e = errors_.ListError()\n            elif self.shape is Shape.SET:\n                e = errors_.SetError()\n            else:\n                e = errors_.SequenceError()\n            return v, ErrorWrapper(e, loc=loc, config=self.model_config)\n\n        result = []\n        errors: List[ErrorList] = []\n        for i, v_ in enumerate(v):\n            v_loc = *loc, i\n            r, ee = self._validate_singleton(v_, values, v_loc, cls)\n            if ee:\n                errors.append(ee)\n            else:\n                result.append(r)\n\n        if errors:\n            return v, errors\n\n        converted: Union[List[Any], Set[Any], Tuple[Any, ...], Iterator[Any]] = result\n\n        if self.shape is Shape.SET:\n            converted = set(result)\n        elif self.shape is Shape.SEQUENCE:\n            if isinstance(v, tuple):\n                converted = tuple(result)\n            elif isinstance(v, set):\n                converted = set(result)\n            elif isinstance(v, Generator):\n                converted = iter(result)\n        return converted, None", "response": "Validate a sequence - like container."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef schema(\n    models: Sequence[Type['main.BaseModel']],\n    *,\n    by_alias: bool = True,\n    title: Optional[str] = None,\n    description: Optional[str] = None,\n    ref_prefix: Optional[str] = None,\n) -> Dict[str, Any]:\n    \"\"\"\n    Process a list of models and generate a single JSON Schema with all of them defined in the ``definitions``\n    top-level JSON key, including their sub-models.\n\n    :param models: a list of models to include in the generated JSON Schema\n    :param by_alias: generate the schemas using the aliases defined, if any\n    :param title: title for the generated schema that includes the definitions\n    :param description: description for the generated schema\n    :param ref_prefix: the JSON Pointer prefix for schema references with ``$ref``, if None, will be set to the\n      default of ``#/definitions/``. Update it if you want the schemas to reference the definitions somewhere\n      else, e.g. for OpenAPI use ``#/components/schemas/``. The resulting generated schemas will still be at the\n      top-level key ``definitions``, so you can extract them from there. But all the references will have the set\n      prefix.\n    :return: dict with the JSON Schema with a ``definitions`` top-level key including the schema definitions for\n      the models and sub-models passed in ``models``.\n    \"\"\"\n    ref_prefix = ref_prefix or default_prefix\n    flat_models = get_flat_models_from_models(models)\n    model_name_map = get_model_name_map(flat_models)\n    definitions = {}\n    output_schema: Dict[str, Any] = {}\n    if title:\n        output_schema['title'] = title\n    if description:\n        output_schema['description'] = description\n    for model in models:\n        m_schema, m_definitions = model_process_schema(\n            model, by_alias=by_alias, model_name_map=model_name_map, ref_prefix=ref_prefix\n        )\n        definitions.update(m_definitions)\n        model_name = model_name_map[model]\n        definitions[model_name] = m_schema\n    if definitions:\n        output_schema['definitions'] = definitions\n    return output_schema", "response": "Generates a JSON Schema for a list of models and sub - models."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef model_schema(\n    model: Type['main.BaseModel'], by_alias: bool = True, ref_prefix: Optional[str] = None\n) -> Dict[str, Any]:\n    \"\"\"\n    Generate a JSON Schema for one model. With all the sub-models defined in the ``definitions`` top-level\n    JSON key.\n\n    :param model: a Pydantic model (a class that inherits from BaseModel)\n    :param by_alias: generate the schemas using the aliases defined, if any\n    :param ref_prefix: the JSON Pointer prefix for schema references with ``$ref``, if None, will be set to the\n      default of ``#/definitions/``. Update it if you want the schemas to reference the definitions somewhere\n      else, e.g. for OpenAPI use ``#/components/schemas/``. The resulting generated schemas will still be at the\n      top-level key ``definitions``, so you can extract them from there. But all the references will have the set\n      prefix.\n    :return: dict with the JSON Schema for the passed ``model``\n    \"\"\"\n    ref_prefix = ref_prefix or default_prefix\n    flat_models = get_flat_models_from_model(model)\n    model_name_map = get_model_name_map(flat_models)\n    m_schema, m_definitions = model_process_schema(\n        model, by_alias=by_alias, model_name_map=model_name_map, ref_prefix=ref_prefix\n    )\n    if m_definitions:\n        m_schema.update({'definitions': m_definitions})\n    return m_schema", "response": "Generate a JSON Schema for a Pydantic model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a tuple with a JSON Schema for this field and additional definitions.", "response": "def field_schema(\n    field: Field,\n    *,\n    by_alias: bool = True,\n    model_name_map: Dict[Type['main.BaseModel'], str],\n    ref_prefix: Optional[str] = None,\n) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n    \"\"\"\n    Process a Pydantic field and return a tuple with a JSON Schema for it as the first item.\n    Also return a dictionary of definitions with models as keys and their schemas as values. If the passed field\n    is a model and has sub-models, and those sub-models don't have overrides (as ``title``, ``default``, etc), they\n    will be included in the definitions and referenced in the schema instead of included recursively.\n\n    :param field: a Pydantic ``Field``\n    :param by_alias: use the defined alias (if any) in the returned schema\n    :param model_name_map: used to generate the JSON Schema references to other models included in the definitions\n    :param ref_prefix: the JSON Pointer prefix to use for references to other schemas, if None, the default of\n      #/definitions/ will be used\n    :return: tuple of the schema for this field and additional definitions\n    \"\"\"\n    ref_prefix = ref_prefix or default_prefix\n    schema_overrides = False\n    schema = cast('Schema', field.schema)\n    s = dict(title=schema.title or field.alias.title())\n    if schema.title:\n        schema_overrides = True\n\n    if schema.description:\n        s['description'] = schema.description\n        schema_overrides = True\n\n    if not field.required and field.default is not None:\n        s['default'] = encode_default(field.default)\n        schema_overrides = True\n\n    validation_schema = get_field_schema_validations(field)\n    if validation_schema:\n        s.update(validation_schema)\n        schema_overrides = True\n\n    f_schema, f_definitions = field_type_schema(\n        field,\n        by_alias=by_alias,\n        model_name_map=model_name_map,\n        schema_overrides=schema_overrides,\n        ref_prefix=ref_prefix,\n    )\n    # $ref will only be returned when there are no schema_overrides\n    if '$ref' in f_schema:\n        return f_schema, f_definitions\n    else:\n        s.update(f_schema)\n        return s, f_definitions"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the JSON Schema validation keywords for a field with an annotation of a Pydantic Schema with validation arguments.", "response": "def get_field_schema_validations(field: Field) -> Dict[str, Any]:\n    \"\"\"\n    Get the JSON Schema validation keywords for a ``field`` with an annotation of\n    a Pydantic ``Schema`` with validation arguments.\n    \"\"\"\n    f_schema: Dict[str, Any] = {}\n    if lenient_issubclass(field.type_, (str, bytes)):\n        for attr_name, t, keyword in _str_types_attrs:\n            attr = getattr(field.schema, attr_name, None)\n            if isinstance(attr, t):\n                f_schema[keyword] = attr\n    if lenient_issubclass(field.type_, numeric_types) and not issubclass(field.type_, bool):\n        for attr_name, t, keyword in _numeric_types_attrs:\n            attr = getattr(field.schema, attr_name, None)\n            if isinstance(attr, t):\n                f_schema[keyword] = attr\n    schema = cast('Schema', field.schema)\n    if schema.extra:\n        f_schema.update(schema.extra)\n    return f_schema"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_model_name_map(unique_models: Set[Type['main.BaseModel']]) -> Dict[Type['main.BaseModel'], str]:\n    name_model_map = {}\n    conflicting_names: Set[str] = set()\n    for model in unique_models:\n        model_name = model.__name__\n        if model_name in conflicting_names:\n            model_name = get_long_model_name(model)\n            name_model_map[model_name] = model\n        elif model_name in name_model_map:\n            conflicting_names.add(model_name)\n            conflicting_model = name_model_map.pop(model_name)\n            name_model_map[get_long_model_name(conflicting_model)] = conflicting_model\n            name_model_map[get_long_model_name(model)] = model\n        else:\n            name_model_map[model_name] = model\n    return {v: k for k, v in name_model_map.items()}", "response": "Generates a dictionary mapping models to their names."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntake a single model and generates a set with itself and all the sub - models in the tree.", "response": "def get_flat_models_from_model(model: Type['main.BaseModel']) -> Set[Type['main.BaseModel']]:\n    \"\"\"\n    Take a single ``model`` and generate a set with itself and all the sub-models in the tree. I.e. if you pass\n    model ``Foo`` (subclass of Pydantic ``BaseModel``) as ``model``, and it has a field of type ``Bar`` (also\n    subclass of ``BaseModel``) and that model ``Bar`` has a field of type ``Baz`` (also subclass of ``BaseModel``),\n    the return value will be ``set([Foo, Bar, Baz])``.\n\n    :param model: a Pydantic ``BaseModel`` subclass\n    :return: a set with the initial model and all its sub-models\n    \"\"\"\n    flat_models: Set[Type['main.BaseModel']] = set()\n    flat_models.add(model)\n    fields = cast(Sequence[Field], model.__fields__.values())\n    flat_models |= get_flat_models_from_fields(fields)\n    return flat_models"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a set of all the models that are used in the field.", "response": "def get_flat_models_from_field(field: Field) -> Set[Type['main.BaseModel']]:\n    \"\"\"\n    Take a single Pydantic ``Field`` (from a model) that could have been declared as a sublcass of BaseModel\n    (so, it could be a submodel), and generate a set with its model and all the sub-models in the tree.\n    I.e. if you pass a field that was declared to be of type ``Foo`` (subclass of BaseModel) as ``field``, and that\n    model ``Foo`` has a field of type ``Bar`` (also subclass of ``BaseModel``) and that model ``Bar`` has a field of\n    type ``Baz`` (also subclass of ``BaseModel``), the return value will be ``set([Foo, Bar, Baz])``.\n\n    :param field: a Pydantic ``Field``\n    :return: a set with the model used in the declaration for this field, if any, and all its sub-models\n    \"\"\"\n    flat_models: Set[Type['main.BaseModel']] = set()\n    if field.sub_fields:\n        flat_models |= get_flat_models_from_fields(field.sub_fields)\n    elif lenient_issubclass(field.type_, main.BaseModel):\n        flat_models |= get_flat_models_from_model(field.type_)\n    elif lenient_issubclass(getattr(field.type_, '__pydantic_model__', None), main.BaseModel):\n        field.type_ = cast(Type['dataclasses.DataclassType'], field.type_)\n        flat_models |= get_flat_models_from_model(field.type_.__pydantic_model__)\n    return flat_models"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a list of Pydantic Field s generate a set of all the models declared in the fields and return a set of all the sub - models that are also in the tree.", "response": "def get_flat_models_from_fields(fields: Sequence[Field]) -> Set[Type['main.BaseModel']]:\n    \"\"\"\n    Take a list of Pydantic  ``Field``s (from a model) that could have been declared as sublcasses of ``BaseModel``\n    (so, any of them could be a submodel), and generate a set with their models and all the sub-models in the tree.\n    I.e. if you pass a the fields of a model ``Foo`` (subclass of ``BaseModel``) as ``fields``, and on of them has a\n    field of type ``Bar`` (also subclass of ``BaseModel``) and that model ``Bar`` has a field of type ``Baz`` (also\n    subclass of ``BaseModel``), the return value will be ``set([Foo, Bar, Baz])``.\n\n    :param fields: a list of Pydantic ``Field``s\n    :return: a set with any model declared in the fields, and all their sub-models\n    \"\"\"\n    flat_models: Set[Type['main.BaseModel']] = set()\n    for field in fields:\n        flat_models |= get_flat_models_from_field(field)\n    return flat_models"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_flat_models_from_models(models: Sequence[Type['main.BaseModel']]) -> Set[Type['main.BaseModel']]:\n    flat_models: Set[Type['main.BaseModel']] = set()\n    for model in models:\n        flat_models |= get_flat_models_from_model(model)\n    return flat_models", "response": "Given a list of models and a set of models generate a set with them and all their sub - models."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a schema for a single object or list of objects.", "response": "def field_type_schema(\n    field: Field,\n    *,\n    by_alias: bool,\n    model_name_map: Dict[Type['main.BaseModel'], str],\n    schema_overrides: bool = False,\n    ref_prefix: Optional[str] = None,\n) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n    \"\"\"\n    Used by ``field_schema()``, you probably should be using that function.\n\n    Take a single ``field`` and generate the schema for its type only, not including additional\n    information as title, etc. Also return additional schema definitions, from sub-models.\n    \"\"\"\n    definitions = {}\n    ref_prefix = ref_prefix or default_prefix\n    if field.shape is Shape.LIST:\n        f_schema, f_definitions = field_singleton_schema(\n            field, by_alias=by_alias, model_name_map=model_name_map, ref_prefix=ref_prefix\n        )\n        definitions.update(f_definitions)\n        return {'type': 'array', 'items': f_schema}, definitions\n    elif field.shape is Shape.SET:\n        f_schema, f_definitions = field_singleton_schema(\n            field, by_alias=by_alias, model_name_map=model_name_map, ref_prefix=ref_prefix\n        )\n        definitions.update(f_definitions)\n        return {'type': 'array', 'uniqueItems': True, 'items': f_schema}, definitions\n    elif field.shape is Shape.MAPPING:\n        dict_schema: Dict[str, Any] = {'type': 'object'}\n        key_field = cast(Field, field.key_field)\n        regex = getattr(key_field.type_, 'regex', None)\n        f_schema, f_definitions = field_singleton_schema(\n            field, by_alias=by_alias, model_name_map=model_name_map, ref_prefix=ref_prefix\n        )\n        definitions.update(f_definitions)\n        if regex:\n            # Dict keys have a regex pattern\n            # f_schema might be a schema or empty dict, add it either way\n            dict_schema['patternProperties'] = {regex.pattern: f_schema}\n        elif f_schema:\n            # The dict values are not simply Any, so they need a schema\n            dict_schema['additionalProperties'] = f_schema\n        return dict_schema, definitions\n    elif field.shape is Shape.TUPLE:\n        sub_schema = []\n        sub_fields = cast(List[Field], field.sub_fields)\n        for sf in sub_fields:\n            sf_schema, sf_definitions = field_type_schema(\n                sf, by_alias=by_alias, model_name_map=model_name_map, ref_prefix=ref_prefix\n            )\n            definitions.update(sf_definitions)\n            sub_schema.append(sf_schema)\n        if len(sub_schema) == 1:\n            sub_schema = sub_schema[0]  # type: ignore\n        return {'type': 'array', 'items': sub_schema}, definitions\n    else:\n        assert field.shape is Shape.SINGLETON, field.shape\n        f_schema, f_definitions = field_singleton_schema(\n            field,\n            by_alias=by_alias,\n            model_name_map=model_name_map,\n            schema_overrides=schema_overrides,\n            ref_prefix=ref_prefix,\n        )\n        definitions.update(f_definitions)\n        return f_schema, definitions"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef model_process_schema(\n    model: Type['main.BaseModel'],\n    *,\n    by_alias: bool = True,\n    model_name_map: Dict[Type['main.BaseModel'], str],\n    ref_prefix: Optional[str] = None,\n) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n    \"\"\"\n    Used by ``model_schema()``, you probably should be using that function.\n\n    Take a single ``model`` and generate its schema. Also return additional schema definitions, from sub-models. The\n    sub-models of the returned schema will be referenced, but their definitions will not be included in the schema. All\n    the definitions are returned as the second value.\n    \"\"\"\n    ref_prefix = ref_prefix or default_prefix\n    s = {'title': model.__config__.title or model.__name__}\n    if model.__doc__:\n        s['description'] = clean_docstring(model.__doc__)\n    m_schema, m_definitions = model_type_schema(\n        model, by_alias=by_alias, model_name_map=model_name_map, ref_prefix=ref_prefix\n    )\n    s.update(m_schema)\n    return s, m_definitions", "response": "Generates a schema from a single model."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a schema for a single object of type object.", "response": "def model_type_schema(\n    model: Type['main.BaseModel'],\n    *,\n    by_alias: bool,\n    model_name_map: Dict[Type['main.BaseModel'], str],\n    ref_prefix: Optional[str] = None,\n) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n    \"\"\"\n    You probably should be using ``model_schema()``, this function is indirectly used by that function.\n\n    Take a single ``model`` and generate the schema for its type only, not including additional\n    information as title, etc. Also return additional schema definitions, from sub-models.\n    \"\"\"\n    ref_prefix = ref_prefix or default_prefix\n    properties = {}\n    required = []\n    definitions: Dict[str, Any] = {}\n    for k, f in model.__fields__.items():\n        try:\n            f_schema, f_definitions = field_schema(\n                f, by_alias=by_alias, model_name_map=model_name_map, ref_prefix=ref_prefix\n            )\n        except SkipField as skip:\n            warnings.warn(skip.message, UserWarning)\n            continue\n        definitions.update(f_definitions)\n        if by_alias:\n            properties[f.alias] = f_schema\n            if f.required:\n                required.append(f.alias)\n        else:\n            properties[k] = f_schema\n            if f.required:\n                required.append(k)\n    out_schema = {'type': 'object', 'properties': properties}\n    if required:\n        out_schema['required'] = required\n    return out_schema, definitions"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a schema for a list of sub - fields.", "response": "def field_singleton_sub_fields_schema(\n    sub_fields: Sequence[Field],\n    *,\n    by_alias: bool,\n    model_name_map: Dict[Type['main.BaseModel'], str],\n    schema_overrides: bool = False,\n    ref_prefix: Optional[str] = None,\n) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n    \"\"\"\n    This function is indirectly used by ``field_schema()``, you probably should be using that function.\n\n    Take a list of Pydantic ``Field`` from the declaration of a type with parameters, and generate their\n    schema. I.e., fields used as \"type parameters\", like ``str`` and ``int`` in ``Tuple[str, int]``.\n    \"\"\"\n    ref_prefix = ref_prefix or default_prefix\n    definitions = {}\n    sub_fields = [sf for sf in sub_fields if sf.include_in_schema()]\n    if len(sub_fields) == 1:\n        return field_type_schema(\n            sub_fields[0],\n            by_alias=by_alias,\n            model_name_map=model_name_map,\n            schema_overrides=schema_overrides,\n            ref_prefix=ref_prefix,\n        )\n    else:\n        sub_field_schemas = []\n        for sf in sub_fields:\n            sub_schema, sub_definitions = field_type_schema(\n                sf,\n                by_alias=by_alias,\n                model_name_map=model_name_map,\n                schema_overrides=schema_overrides,\n                ref_prefix=ref_prefix,\n            )\n            definitions.update(sub_definitions)\n            sub_field_schemas.append(sub_schema)\n        return {'anyOf': sub_field_schemas}, definitions"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef field_singleton_schema(  # noqa: C901 (ignore complexity)\n    field: Field,\n    *,\n    by_alias: bool,\n    model_name_map: Dict[Type['main.BaseModel'], str],\n    schema_overrides: bool = False,\n    ref_prefix: Optional[str] = None,\n) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n    \"\"\"\n    This function is indirectly used by ``field_schema()``, you should probably be using that function.\n\n    Take a single Pydantic ``Field``, and return its schema and any additional definitions from sub-models.\n    \"\"\"\n\n    ref_prefix = ref_prefix or default_prefix\n    definitions: Dict[str, Any] = {}\n    if field.sub_fields:\n        return field_singleton_sub_fields_schema(\n            field.sub_fields,\n            by_alias=by_alias,\n            model_name_map=model_name_map,\n            schema_overrides=schema_overrides,\n            ref_prefix=ref_prefix,\n        )\n    if field.type_ is Any:\n        return {}, definitions  # no restrictions\n    if is_callable_type(field.type_):\n        raise SkipField(f'Callable {field.name} was excluded from schema since JSON schema has no equivalent type.')\n    f_schema: Dict[str, Any] = {}\n    if issubclass(field.type_, Enum):\n        f_schema.update({'enum': [item.value for item in field.type_]})  # type: ignore\n        # Don't return immediately, to allow adding specific types\n    for field_name, schema_name in validation_attribute_to_schema_keyword.items():\n        field_value = getattr(field.type_, field_name, None)\n        if field_value is not None:\n            if field_name == 'regex':\n                field_value = field_value.pattern\n            f_schema[schema_name] = field_value\n    for type_, t_schema in field_class_to_schema_enum_enabled:\n        if issubclass(field.type_, type_):\n            f_schema.update(t_schema)\n            break\n    # Return schema, with or without enum definitions\n    if f_schema:\n        return f_schema, definitions\n    for type_, t_schema in field_class_to_schema_enum_disabled:\n        if issubclass(field.type_, type_):\n            return t_schema, definitions\n    # Handle dataclass-based models\n    field_type = field.type_\n    if lenient_issubclass(getattr(field_type, '__pydantic_model__', None), main.BaseModel):\n        field_type = cast(Type['dataclasses.DataclassType'], field_type)\n        field_type = field_type.__pydantic_model__\n    if issubclass(field_type, main.BaseModel):\n        sub_schema, sub_definitions = model_process_schema(\n            field_type, by_alias=by_alias, model_name_map=model_name_map, ref_prefix=ref_prefix\n        )\n        definitions.update(sub_definitions)\n        if not schema_overrides:\n            model_name = model_name_map[field_type]\n            definitions[model_name] = sub_schema\n            return {'$ref': f'{ref_prefix}{model_name}'}, definitions\n        else:\n            return sub_schema, definitions\n    raise ValueError(f'Value not declarable with JSON Schema, field: {field}')", "response": "Returns a schema for a single Pydantic Field and any additional definitions from sub - models."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_annotation_from_schema(annotation: Any, schema: Schema) -> Type[Any]:\n    if isinstance(annotation, type):\n        attrs: Optional[Tuple[str, ...]] = None\n        constraint_func: Optional[Callable[..., type]] = None\n        if issubclass(annotation, str) and not issubclass(annotation, (EmailStr, DSN, UrlStr, ConstrainedStr)):\n            attrs = ('max_length', 'min_length', 'regex')\n            constraint_func = constr\n        elif lenient_issubclass(annotation, numeric_types) and not issubclass(\n            annotation, (ConstrainedInt, ConstrainedFloat, ConstrainedDecimal, bool)\n        ):\n            # Is numeric type\n            attrs = ('gt', 'lt', 'ge', 'le', 'multiple_of')\n            numeric_type = next(t for t in numeric_types if issubclass(annotation, t))  # pragma: no branch\n            constraint_func = _map_types_constraint[numeric_type]\n\n        if attrs:\n            kwargs = {\n                attr_name: attr\n                for attr_name, attr in ((attr_name, getattr(schema, attr_name)) for attr_name in attrs)\n                if attr is not None\n            }\n            if kwargs:\n                constraint_func = cast(Callable[..., type], constraint_func)\n                return constraint_func(**kwargs)\n    return annotation", "response": "Get an annotation from a field specification as str ConstrainedStr or ConstrainedInt."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_model(  # noqa: C901 (ignore complexity)\n    model_name: str,\n    *,\n    __config__: Type[BaseConfig] = None,\n    __base__: Type[BaseModel] = None,\n    __module__: Optional[str] = None,\n    __validators__: Dict[str, classmethod] = None,\n    **field_definitions: Any,\n) -> BaseModel:\n    \"\"\"\n    Dynamically create a model.\n    :param model_name: name of the created model\n    :param __config__: config class to use for the new model\n    :param __base__: base class for the new model to inherit from\n    :param __validators__: a dict of method names and @validator class methods\n    :param **field_definitions: fields of the model (or extra fields if a base is supplied) in the format\n        `<name>=(<type>, <default default>)` or `<name>=<default value> eg. `foobar=(str, ...)` or `foobar=123`\n    \"\"\"\n    if __base__:\n        if __config__ is not None:\n            raise ConfigError('to avoid confusion __config__ and __base__ cannot be used together')\n    else:\n        __base__ = BaseModel\n\n    fields = {}\n    annotations = {}\n\n    for f_name, f_def in field_definitions.items():\n        if f_name.startswith('_'):\n            warnings.warn(f'fields may not start with an underscore, ignoring \"{f_name}\"', RuntimeWarning)\n        if isinstance(f_def, tuple):\n            try:\n                f_annotation, f_value = f_def\n            except ValueError as e:\n                raise ConfigError(\n                    f'field definitions should either be a tuple of (<type>, <default>) or just a '\n                    f'default value, unfortunately this means tuples as '\n                    f'default values are not allowed'\n                ) from e\n        else:\n            f_annotation, f_value = None, f_def\n\n        if f_annotation:\n            annotations[f_name] = f_annotation\n        fields[f_name] = f_value\n\n    namespace: 'DictStrAny' = {'__annotations__': annotations, '__module__': __module__}\n    if __validators__:\n        namespace.update(__validators__)\n    namespace.update(fields)\n    if __config__:\n        namespace['Config'] = inherit_config(__config__, BaseConfig)\n\n    return type(model_name, (__base__,), namespace)", "response": "Dynamically creates a new model."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates a single object against a single model.", "response": "def validate_model(  # noqa: C901 (ignore complexity)\n    model: Union[BaseModel, Type[BaseModel]], input_data: 'DictStrAny', raise_exc: bool = True, cls: 'ModelOrDc' = None\n) -> Union['DictStrAny', Tuple['DictStrAny', Optional[ValidationError]]]:\n    \"\"\"\n    validate data against a model.\n    \"\"\"\n    values = {}\n    errors = []\n    names_used = set()\n    config = model.__config__\n    check_extra = config.extra is not Extra.ignore\n\n    for name, field in model.__fields__.items():\n        if type(field.type_) == ForwardRef:\n            raise ConfigError(\n                f'field \"{field.name}\" not yet prepared so type is still a ForwardRef, '\n                f'you might need to call {model.__class__.__name__}.update_forward_refs().'\n            )\n\n        value = input_data.get(field.alias, _missing)\n        using_name = False\n        if value is _missing and config.allow_population_by_alias and field.alt_alias:\n            value = input_data.get(field.name, _missing)\n            using_name = True\n\n        if value is _missing:\n            if field.required:\n                errors.append(ErrorWrapper(MissingError(), loc=field.alias, config=model.__config__))\n                continue\n            value = deepcopy(field.default)\n            if not model.__config__.validate_all and not field.validate_always:\n                values[name] = value\n                continue\n        elif check_extra:\n            names_used.add(field.name if using_name else field.alias)\n\n        v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls or model.__class__)  # type: ignore\n        if isinstance(errors_, ErrorWrapper):\n            errors.append(errors_)\n        elif isinstance(errors_, list):\n            errors.extend(errors_)\n        else:\n            values[name] = v_\n\n    if check_extra:\n        extra = input_data.keys() - names_used\n        if extra:\n            if config.extra is Extra.allow:\n                for f in extra:\n                    values[f] = input_data[f]\n            else:\n                for f in sorted(extra):\n                    errors.append(ErrorWrapper(ExtraError(), loc=f, config=config))\n\n    if not raise_exc:\n        return values, ValidationError(errors) if errors else None\n\n    if errors:\n        raise ValidationError(errors)\n    return values"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dict(\n        self, *, include: 'SetStr' = None, exclude: 'SetStr' = None, by_alias: bool = False, skip_defaults: bool = False\n    ) -> 'DictStrAny':\n        \"\"\"\n        Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n        \"\"\"\n        get_key = self._get_key_factory(by_alias)\n        get_key = partial(get_key, self.fields)\n\n        return_keys = self._calculate_keys(include=include, exclude=exclude, skip_defaults=skip_defaults)\n        if return_keys is None:\n            return {get_key(k): v for k, v in self._iter(by_alias=by_alias, skip_defaults=skip_defaults)}\n        else:\n            return {\n                get_key(k): v for k, v in self._iter(by_alias=by_alias, skip_defaults=skip_defaults) if k in return_keys\n            }", "response": "Generate a dictionary representation of the object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef json(\n        self,\n        *,\n        include: 'SetStr' = None,\n        exclude: 'SetStr' = None,\n        by_alias: bool = False,\n        skip_defaults: bool = False,\n        encoder: Optional[Callable[[Any], Any]] = None,\n        **dumps_kwargs: Any,\n    ) -> str:\n        \"\"\"\n        Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n\n        `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n        \"\"\"\n        encoder = cast(Callable[[Any], Any], encoder or self._json_encoder)\n        return json.dumps(\n            self.dict(include=include, exclude=exclude, by_alias=by_alias, skip_defaults=skip_defaults),\n            default=encoder,\n            **dumps_kwargs,\n        )", "response": "Generate a JSON representation of the object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing a new object and sets values and fields_set.", "response": "def construct(cls: Type['Model'], values: 'DictAny', fields_set: 'SetStr') -> 'Model':\n        \"\"\"\n        Creates a new model and set __values__ without any validation, thus values should already be trusted.\n        Chances are you don't want to use this method directly.\n        \"\"\"\n        m = cls.__new__(cls)\n        object.__setattr__(m, '__values__', values)\n        object.__setattr__(m, '__fields_set__', fields_set)\n        return m"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a copy of the current model with the same fields as the current one.", "response": "def copy(\n        self: 'Model',\n        *,\n        include: 'SetStr' = None,\n        exclude: 'SetStr' = None,\n        update: 'DictStrAny' = None,\n        deep: bool = False,\n    ) -> 'Model':\n        \"\"\"\n        Duplicate a model, optionally choose which fields to include, exclude and change.\n\n        :param include: fields to include in new model\n        :param exclude: fields to exclude from new model, as with values this takes precedence over include\n        :param update: values to change/add in the new model. Note: the data is not validated before creating\n            the new model: you should trust this data\n        :param deep: set to `True` to make a deep copy of the model\n        :return: new model instance\n        \"\"\"\n        if include is None and exclude is None and update is None:\n            # skip constructing values if no arguments are passed\n            v = self.__values__\n        else:\n            return_keys = self._calculate_keys(include=include, exclude=exclude, skip_defaults=False)\n            if return_keys:\n                v = {**{k: v for k, v in self.__values__.items() if k in return_keys}, **(update or {})}\n            else:\n                v = {**self.__values__, **(update or {})}\n\n        if deep:\n            v = deepcopy(v)\n        m = self.__class__.construct(v, self.__fields_set__.copy())\n        return m"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating ForwardRefs on all the related objects in this class based on the given localns.", "response": "def update_forward_refs(cls, **localns: Any) -> None:\n        \"\"\"\n        Try to update ForwardRefs on fields based on this Model, globalns and localns.\n        \"\"\"\n        globalns = sys.modules[cls.__module__].__dict__\n        globalns.setdefault(cls.__name__, cls)\n        for f in cls.__fields__.values():\n            update_field_forward_refs(f, globalns=globalns, localns=localns)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert timedelta to ISO 8601 string.", "response": "def timedelta_isoformat(td: datetime.timedelta) -> str:\n    \"\"\"\n    ISO 8601 encoding for timedeltas.\n    \"\"\"\n    minutes, seconds = divmod(td.seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    return f'P{td.days}DT{hours:d}H{minutes:d}M{seconds:d}.{td.microseconds:06d}S'"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a date string and return a datetime. date.", "response": "def parse_date(value: Union[date, StrIntFloat]) -> date:\n    \"\"\"\n    Parse a date/int/float/string and return a datetime.date.\n\n    Raise ValueError if the input is well formatted but not a valid date.\n    Raise ValueError if the input isn't well formatted.\n    \"\"\"\n    if isinstance(value, date):\n        if isinstance(value, datetime):\n            return value.date()\n        else:\n            return value\n\n    number = get_numeric(value)\n    if number is not None:\n        return from_unix_seconds(number).date()\n\n    match = date_re.match(cast(str, value))\n    if not match:\n        raise errors.DateError()\n\n    kw = {k: int(v) for k, v in match.groupdict().items()}\n\n    with change_exception(errors.DateError, ValueError):\n        return date(**kw)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a time string and return a datetime. time. time object.", "response": "def parse_time(value: Union[time, str]) -> time:\n    \"\"\"\n    Parse a time/string and return a datetime.time.\n\n    This function doesn't support time zone offsets.\n\n    Raise ValueError if the input is well formatted but not a valid time.\n    Raise ValueError if the input isn't well formatted, in particular if it contains an offset.\n    \"\"\"\n    if isinstance(value, time):\n        return value\n\n    match = time_re.match(value)\n    if not match:\n        raise errors.TimeError()\n\n    kw = match.groupdict()\n    if kw['microsecond']:\n        kw['microsecond'] = kw['microsecond'].ljust(6, '0')\n\n    kw_ = {k: int(v) for k, v in kw.items() if v is not None}\n\n    with change_exception(errors.TimeError, ValueError):\n        return time(**kw_)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_datetime(value: Union[datetime, StrIntFloat]) -> datetime:\n    if isinstance(value, datetime):\n        return value\n\n    number = get_numeric(value)\n    if number is not None:\n        return from_unix_seconds(number)\n\n    match = datetime_re.match(cast(str, value))\n    if not match:\n        raise errors.DateTimeError()\n\n    kw = match.groupdict()\n    if kw['microsecond']:\n        kw['microsecond'] = kw['microsecond'].ljust(6, '0')\n\n    tzinfo_str = kw.pop('tzinfo')\n    if tzinfo_str == 'Z':\n        tzinfo = timezone.utc\n    elif tzinfo_str is not None:\n        offset_mins = int(tzinfo_str[-2:]) if len(tzinfo_str) > 3 else 0\n        offset = 60 * int(tzinfo_str[1:3]) + offset_mins\n        if tzinfo_str[0] == '-':\n            offset = -offset\n        tzinfo = timezone(timedelta(minutes=offset))\n    else:\n        tzinfo = None\n\n    kw_: Dict[str, Union[int, timezone]] = {k: int(v) for k, v in kw.items() if v is not None}\n    kw_['tzinfo'] = tzinfo\n\n    with change_exception(errors.DateTimeError, ValueError):\n        return datetime(**kw_)", "response": "Parse a datetime string and return a datetime. datetime object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_duration(value: StrIntFloat) -> timedelta:\n    if isinstance(value, timedelta):\n        return value\n\n    if isinstance(value, (int, float)):\n        # bellow code requires a string\n        value = str(value)\n\n    match = standard_duration_re.match(value) or iso8601_duration_re.match(value)\n    if not match:\n        raise errors.DurationError()\n\n    kw = match.groupdict()\n    sign = -1 if kw.pop('sign', '+') == '-' else 1\n    if kw.get('microseconds'):\n        kw['microseconds'] = kw['microseconds'].ljust(6, '0')\n\n    if kw.get('seconds') and kw.get('microseconds') and kw['seconds'].startswith('-'):\n        kw['microseconds'] = '-' + kw['microseconds']\n\n    kw_ = {k: float(v) for k, v in kw.items() if v is not None}\n\n    return sign * timedelta(**kw_)", "response": "Parse a duration int float or string and return a datetime. timedelta."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef translate_resource_args(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        \"\"\"\n        :type args: *Any\n        :type kwargs: **Any\n        :return: Any\n        \"\"\"\n        arg_list = []\n        for arg in args:\n            if isinstance(arg, (Issue, Project)):\n                arg_list.append(arg.key)\n            else:\n                arg_list.append(arg)\n        result = func(*arg_list, **kwargs)\n        return result\n\n    return wrapper", "response": "Decorator that converts Issue and Project resources to their keys when used as arguments."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the current version of the library is outdated.", "response": "def _check_update_(self):\n        \"\"\"Check if the current version of the library is outdated.\"\"\"\n        try:\n            data = requests.get(\"https://pypi.python.org/pypi/jira/json\", timeout=2.001).json()\n\n            released_version = data['info']['version']\n            if parse_version(released_version) > parse_version(__version__):\n                warnings.warn(\n                    \"You are running an outdated version of JIRA Python %s. Current version is %s. Do not file any bugs against older versions.\" % (\n                        __version__, released_version))\n        except requests.RequestException:\n            pass\n        except Exception as e:\n            logging.warning(e)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _fetch_pages(self,\n                     item_type,\n                     items_key,\n                     request_path,\n                     startAt=0,\n                     maxResults=50,\n                     params=None,\n                     base=JIRA_BASE_URL,\n                     ):\n        \"\"\"Fetch pages.\n\n        :param item_type: Type of single item. ResultList of such items will be returned.\n        :type item_type: type\n        :param items_key: Path to the items in JSON returned from server.\n                Set it to None, if response is an array, and not a JSON object.\n        :type items_key: Optional[str]\n        :param request_path: path in request URL\n        :type request_path: str\n        :param startAt: index of the first record to be fetched. (Default: 0)\n        :type startAt: int\n        :param maxResults: Maximum number of items to return.\n                If maxResults evaluates as False, it will try to get all items in batches. (Default:50)\n        :type maxResults: int\n        :param params: Params to be used in all requests. Should not contain startAt and maxResults,\n                        as they will be added for each request created from this function.\n        :type params: Dict[str, Any]\n        :param base: base URL\n        :type base: str\n        :rtype: ResultList\n        \"\"\"\n        async_class = None\n        if self._options['async']:\n            try:\n                from requests_futures.sessions import FuturesSession\n                async_class = FuturesSession\n            except ImportError:\n                pass\n            async_workers = self._options['async_workers']\n        page_params = params.copy() if params else {}\n        if startAt:\n            page_params['startAt'] = startAt\n        if maxResults:\n            page_params['maxResults'] = maxResults\n\n        resource = self._get_json(request_path, params=page_params, base=base)\n        next_items_page = self._get_items_from_page(item_type, items_key,\n                                                    resource)\n        items = next_items_page\n\n        if True:  # isinstance(resource, dict):\n\n            if isinstance(resource, dict):\n                total = resource.get('total')\n                # 'isLast' is the optional key added to responses in JIRA Agile 6.7.6. So far not used in basic JIRA API.\n                is_last = resource.get('isLast', False)\n                start_at_from_response = resource.get('startAt', 0)\n                max_results_from_response = resource.get('maxResults', 1)\n            else:\n                # if is a list\n                total = 1\n                is_last = True\n                start_at_from_response = 0\n                max_results_from_response = 1\n\n            # If maxResults evaluates as False, get all items in batches\n            if not maxResults:\n                page_size = max_results_from_response or len(items)\n                page_start = (startAt or start_at_from_response or 0) + page_size\n                if async_class is not None and not is_last and (\n                        total is not None and len(items) < total):\n                    async_fetches = []\n                    future_session = async_class(session=self._session, max_workers=async_workers)\n                    for start_index in range(page_start, total, page_size):\n                        page_params = params.copy()\n                        page_params['startAt'] = start_index\n                        page_params['maxResults'] = page_size\n                        url = self._get_url(request_path)\n                        r = future_session.get(url, params=page_params)\n                        async_fetches.append(r)\n                    for future in async_fetches:\n                        response = future.result()\n                        resource = json_loads(response)\n                        if resource:\n                            next_items_page = self._get_items_from_page(\n                                item_type, items_key, resource)\n                            items.extend(next_items_page)\n                while async_class is None and not is_last and (\n                    total is None or page_start < total) and len(\n                        next_items_page) == page_size:\n                    page_params['startAt'] = page_start\n                    page_params['maxResults'] = page_size\n                    resource = self._get_json(request_path, params=page_params, base=base)\n                    if resource:\n                        next_items_page = self._get_items_from_page(\n                            item_type, items_key, resource)\n                        items.extend(next_items_page)\n                        page_start += page_size\n                    else:\n                        # if resource is an empty dictionary we assume no-results\n                        break\n\n            return ResultList(items, start_at_from_response, max_results_from_response, total, is_last)\n        else:\n            # it seams that search_users can return a list() containing a single user!\n            return ResultList([item_type(self._options, self._session, resource)], 0, 1, 1, True)", "response": "Fetch all pages from the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the items from the page.", "response": "def _get_items_from_page(self,\n                             item_type,\n                             items_key,\n                             resource,\n                             ):\n        \"\"\"\n        :type item_type: type\n        :type items_key: str\n        :type resource: Dict[str, Any]\n        :rtype: Union[List[Dashboard], List[Issue]]\n        \"\"\"\n        try:\n            return [item_type(self._options, self._session, raw_issue_json) for raw_issue_json in\n                    (resource[items_key] if items_key else resource)]\n        except KeyError as e:\n            # improving the error text so we know why it happened\n            raise KeyError(str(e) + \" : \" + json.dumps(resource))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find(self, resource_format, ids=None):\n        resource = Resource(resource_format, self._options, self._session)\n        resource.find(ids)\n        return resource", "response": "Find any resource in the server."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef async_do(self, size=10):\n        if hasattr(self._session, '_async_jobs'):\n            logging.info(\"Executing asynchronous %s jobs found in queue by using %s threads...\" % (\n                len(self._session._async_jobs), size))\n            threaded_requests.map(self._session._async_jobs, size=size)", "response": "Execute all asynchronous jobs and wait for them to finish."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef application_properties(self, key=None):\n        params = {}\n        if key is not None:\n            params['key'] = key\n        return self._get_json('application-properties', params=params)", "response": "Return the mutable server application properties."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the application property.", "response": "def set_application_property(self, key, value):\n        \"\"\"Set the application property.\n\n        :param key: key of the property to set\n        :type key: str\n        :param value: value to assign to the property\n        :type value: str\n        \"\"\"\n        url = self._options['server'] + \\\n            '/rest/api/latest/application-properties/' + key\n        payload = {\n            'id': key,\n            'value': value}\n        return self._session.put(\n            url, data=json.dumps(payload))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef applicationlinks(self, cached=True):\n        # if cached, return the last result\n        if cached and hasattr(self, '_applicationlinks'):\n            return self._applicationlinks\n\n        # url = self._options['server'] + '/rest/applinks/latest/applicationlink'\n        url = self._options['server'] + \\\n            '/rest/applinks/latest/listApplicationlinks'\n\n        r = self._session.get(url)\n\n        o = json_loads(r)\n        if 'list' in o:\n            self._applicationlinks = o['list']\n        else:\n            self._applicationlinks = []\n        return self._applicationlinks", "response": "List of application links."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_attachment(self, issue, attachment, filename=None):\n        if isinstance(attachment, string_types):\n            attachment = open(attachment, \"rb\")\n        if hasattr(attachment, 'read') and hasattr(attachment, 'mode') and attachment.mode != 'rb':\n            logging.warning(\n                \"%s was not opened in 'rb' mode, attaching file may fail.\" % attachment.name)\n\n        url = self._get_url('issue/' + str(issue) + '/attachments')\n\n        fname = filename\n        if not fname:\n            fname = os.path.basename(attachment.name)\n\n        if 'MultipartEncoder' not in globals():\n            method = 'old'\n            r = self._session.post(\n                url,\n                files={\n                    'file': (fname, attachment, 'application/octet-stream')},\n                headers=CaseInsensitiveDict({'content-type': None, 'X-Atlassian-Token': 'nocheck'}))\n        else:\n            method = 'MultipartEncoder'\n\n            def file_stream():\n                \"\"\"Returns files stream of attachment.\n\n                :rtype: MultipartEncoder\n                \"\"\"\n                return MultipartEncoder(\n                    fields={\n                        'file': (fname, attachment, 'application/octet-stream')})\n            m = file_stream()\n            r = self._session.post(\n                url, data=m, headers=CaseInsensitiveDict({'content-type': m.content_type, 'X-Atlassian-Token': 'nocheck'}), retry_data=file_stream)\n\n        js = json_loads(r)\n        if not js or not isinstance(js, collections.Iterable):\n            raise JIRAError(\"Unable to parse JSON: %s\" % js)\n        attachment = Attachment(self._options, self._session, js[0])\n        if attachment.size == 0:\n            raise JIRAError(\"Added empty attachment via %s method?!: r: %s\\nattachment: %s\" % (method, r, attachment))\n        return attachment", "response": "Attach an attachment to an issue and returns a Resource object for it."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete an attachment by id.", "response": "def delete_attachment(self, id):\n        \"\"\"Delete attachment by id.\n\n        :param id: ID of the attachment to delete\n        :type id: str\n        \"\"\"\n        url = self._get_url('attachment/' + str(id))\n        return self._session.delete(url)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new component inside a project and returns a Resource for it.", "response": "def create_component(self,\n                         name,\n                         project,\n                         description=None,\n                         leadUserName=None,\n                         assigneeType=None,\n                         isAssigneeTypeValid=False,\n                         ):\n        \"\"\"Create a component inside a project and return a Resource for it.\n\n        :param name: name of the component\n        :type name: str\n        :param project: key of the project to create the component in\n        :type project: str\n        :param description: a description of the component\n        :type description: str\n        :param leadUserName: the username of the user responsible for this component\n        :type leadUserName: Optional[str]\n        :param assigneeType: see the ComponentBean.AssigneeType class for valid values\n        :type assigneeType: Optional[str]\n        :param isAssigneeTypeValid: boolean specifying whether the assignee type is acceptable (Default: False)\n        :type isAssigneeTypeValid: bool\n        :rtype: Component\n        \"\"\"\n        data = {\n            'name': name,\n            'project': project,\n            'isAssigneeTypeValid': isAssigneeTypeValid}\n        if description is not None:\n            data['description'] = description\n        if leadUserName is not None:\n            data['leadUserName'] = leadUserName\n        if assigneeType is not None:\n            data['assigneeType'] = assigneeType\n\n        url = self._get_url('component')\n        r = self._session.post(\n            url, data=json.dumps(data))\n\n        component = Component(self._options, self._session, raw=json_loads(r))\n        return component"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting a component by id.", "response": "def delete_component(self, id):\n        \"\"\"Delete component by id.\n\n        :param id: ID of the component to use\n        :type id: str\n        :rtype: Response\n        \"\"\"\n        url = self._get_url('component/' + str(id))\n        return self._session.delete(url)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a ResultList of Dashboard resources and a total count.", "response": "def dashboards(self, filter=None, startAt=0, maxResults=20):\n        \"\"\"Return a ResultList of Dashboard resources and a ``total`` count.\n\n        :param filter: either \"favourite\" or \"my\", the type of dashboards to return\n        :type filter: Optional[str]\n        :param startAt: index of the first dashboard to return (Default: 0)\n        :type startAt: int\n        :param maxResults: maximum number of dashboards to return.\n            If maxResults evaluates as False, it will try to get all items in batches. (Default: 20)\n        :type maxResults: int\n\n        :rtype: ResultList\n        \"\"\"\n        params = {}\n        if filter is not None:\n            params['filter'] = filter\n        return self._fetch_pages(Dashboard, 'dashboards', 'dashboard', startAt, maxResults, params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef favourite_filters(self):\n        r_json = self._get_json('filter/favourite')\n        filters = [Filter(self._options, self._session, raw_filter_json)\n                   for raw_filter_json in r_json]\n        return filters", "response": "Get a list of filter Resources which are the favourites of the currently authenticated user."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_filter(self, name=None, description=None,\n                      jql=None, favourite=None):\n        \"\"\"Create a new filter and return a filter Resource for it.\n\n        :param name: name of the new filter\n        :type name: str\n        :param description: useful human readable description of the new filter\n        :type description: str\n        :param jql: query string that defines the filter\n        :type jql: str\n        :param favourite: whether to add this filter to the current user's favorites\n        :type favourite: bool\n        :rtype: Filter\n\n        \"\"\"\n        data = {}\n        if name is not None:\n            data['name'] = name\n        if description is not None:\n            data['description'] = description\n        if jql is not None:\n            data['jql'] = jql\n        if favourite is not None:\n            data['favourite'] = favourite\n        url = self._get_url('filter')\n        r = self._session.post(\n            url, data=json.dumps(data))\n\n        raw_filter_json = json_loads(r)\n        return Filter(self._options, self._session, raw=raw_filter_json)", "response": "Create a new filter and return a filter Resource for it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_filter(self, filter_id,\n                      name=None, description=None,\n                      jql=None, favourite=None):\n        \"\"\"Update a filter and return a filter Resource for it.\n\n        :param name: name of the new filter\n        :type name: Optional[str]\n        :param description: useful human readable description of the new filter\n        :type description: Optional[str]\n        :param jql: query string that defines the filter\n        :type jql: Optional[str]\n        :param favourite: whether to add this filter to the current user's favorites\n        :type favourite: Optional[bool]\n\n        \"\"\"\n        filter = self.filter(filter_id)\n        data = {}\n        data['name'] = name or filter.name\n        data['description'] = description or filter.description\n        data['jql'] = jql or filter.jql\n        data['favourite'] = favourite or filter.favourite\n\n        url = self._get_url('filter/%s' % filter_id)\n        r = self._session.put(url, headers={'content-type': 'application/json'},\n                              data=json.dumps(data))\n\n        raw_filter_json = json.loads(r.text)\n        return Filter(self._options, self._session, raw=raw_filter_json)", "response": "Update a filter and return a filter Resource for it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef group(self, id, expand=None):\n        group = Group(self._options, self._session)\n        params = {}\n        if expand is not None:\n            params['expand'] = expand\n        group.find(id, params=params)\n        return group", "response": "Get a group Resource from the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of groups matching the specified criteria.", "response": "def groups(self, query=None, exclude=None, maxResults=9999):\n        \"\"\"Return a list of groups matching the specified criteria.\n\n        :param query: filter groups by name with this string\n        :type query: Optional[str]\n        :param exclude: filter out groups by name with this string\n        :type exclude: Optional[Any]\n        :param maxResults: maximum results to return. (Default: 9999)\n        :type maxResults: int\n        :rtype: List[str]\n\n        \"\"\"\n        params = {}\n        groups = []\n        if query is not None:\n            params['query'] = query\n        if exclude is not None:\n            params['exclude'] = exclude\n        if maxResults is not None:\n            params['maxResults'] = maxResults\n        for group in self._get_json('groups/picker', params=params)['groups']:\n            groups.append(group['name'])\n        return sorted(groups)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef group_members(self, group):\n        if self._version < (6, 0, 0):\n            raise NotImplementedError(\n                \"Group members is not implemented in JIRA before version 6.0, upgrade the instance, if possible.\")\n\n        params = {'groupname': group, 'expand': \"users\"}\n        r = self._get_json('group', params=params)\n        size = r['users']['size']\n        end_index = r['users']['end-index']\n\n        while end_index < size - 1:\n            params = {'groupname': group, 'expand': \"users[%s:%s]\" % (\n                end_index + 1, end_index + 50)}\n            r2 = self._get_json('group', params=params)\n            for user in r2['users']['items']:\n                r['users']['items'].append(user)\n            end_index = r2['users']['end-index']\n            size = r['users']['size']\n\n        result = {}\n        for user in r['users']['items']:\n            result[user['key']] = {'name': user['name'],\n                                   'fullname': user['displayName'],\n                                   'email': user.get('emailAddress', 'hidden'),\n                                   'active': user['active']}\n        return OrderedDict(sorted(result.items(), key=lambda t: t[0]))", "response": "Returns a hash or users with their information. Requires JIRA 6. 0. 0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_group(self, groupname):\n        url = self._options['server'] + '/rest/api/latest/group'\n\n        # implementation based on\n        # https://docs.atlassian.com/jira/REST/ondemand/#d2e5173\n\n        x = OrderedDict()\n\n        x['name'] = groupname\n\n        payload = json.dumps(x)\n\n        self._session.post(url, data=payload)\n\n        return True", "response": "Create a new group in JIRA."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting a group from the JIRA instance.", "response": "def remove_group(self, groupname):\n        \"\"\"Delete a group from the JIRA instance.\n\n        :param groupname: The group to be deleted from the JIRA instance.\n        :type groupname: str\n        :return: Boolean. Returns True on success.\n        :rtype: bool\n        \"\"\"\n        # implementation based on\n        # https://docs.atlassian.com/jira/REST/ondemand/#d2e5173\n        url = self._options['server'] + '/rest/api/latest/group'\n        x = {'groupname': groupname}\n        self._session.delete(url, params=x)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef issue(self, id, fields=None, expand=None):\n        # this allows us to pass Issue objects to issue()\n        if isinstance(id, Issue):\n            return id\n\n        issue = Issue(self._options, self._session)\n\n        params = {}\n        if fields is not None:\n            params['fields'] = fields\n        if expand is not None:\n            params['expand'] = expand\n        issue.find(id, params=params)\n        return issue", "response": "Get an issue resource from the server."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_issue(self, fields=None, prefetch=True, **fieldargs):\n        data = _field_worker(fields, **fieldargs)\n\n        p = data['fields']['project']\n\n        if isinstance(p, string_types) or isinstance(p, integer_types):\n            data['fields']['project'] = {'id': self.project(p).id}\n\n        p = data['fields']['issuetype']\n        if isinstance(p, integer_types):\n            data['fields']['issuetype'] = {'id': p}\n        if isinstance(p, string_types) or isinstance(p, integer_types):\n            data['fields']['issuetype'] = {'id': self.issue_type_by_name(p).id}\n\n        url = self._get_url('issue')\n        r = self._session.post(url, data=json.dumps(data))\n\n        raw_issue_json = json_loads(r)\n        if 'key' not in raw_issue_json:\n            raise JIRAError(r.status_code, response=r, url=url, text=json.dumps(data))\n        if prefetch:\n            return self.issue(raw_issue_json['key'])\n        else:\n            return Issue(self._options, self._session, raw=raw_issue_json)", "response": "Create a new issue and return a new issue Resource for it."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbulk create new issues and return an issue Resource for each successfully created issue.", "response": "def create_issues(self, field_list, prefetch=True):\n        \"\"\"Bulk create new issues and return an issue Resource for each successfully created issue.\n\n        See `create_issue` documentation for field information.\n\n        :param field_list: a list of dicts each containing field names and the values to use. Each dict\n            is an individual issue to create and is subject to its minimum requirements.\n        :type field_list: List[Dict[str, Any]]\n        :param prefetch: whether to reload the created issue Resource for each created issue so that all\n            of its data is present in the value returned from this method.\n        :type prefetch: bool\n        :rtype: List[Dict[str, Any]]\n\n        \"\"\"\n        data = {'issueUpdates': []}\n        for field_dict in field_list:\n            issue_data = _field_worker(field_dict)\n            p = issue_data['fields']['project']\n\n            if isinstance(p, string_types) or isinstance(p, integer_types):\n                issue_data['fields']['project'] = {'id': self.project(p).id}\n\n            p = issue_data['fields']['issuetype']\n            if isinstance(p, integer_types):\n                issue_data['fields']['issuetype'] = {'id': p}\n            if isinstance(p, string_types) or isinstance(p, integer_types):\n                issue_data['fields']['issuetype'] = {'id': self.issue_type_by_name(p).id}\n\n            data['issueUpdates'].append(issue_data)\n\n        url = self._get_url('issue/bulk')\n        try:\n            r = self._session.post(url, data=json.dumps(data))\n            raw_issue_json = json_loads(r)\n        # Catching case where none of the issues has been created. See https://github.com/pycontribs/jira/issues/350\n        except JIRAError as je:\n            if je.status_code == 400:\n                raw_issue_json = json.loads(je.response.text)\n            else:\n                raise\n        issue_list = []\n        errors = {}\n        for error in raw_issue_json['errors']:\n            errors[error['failedElementNumber']] = error['elementErrors']['errors']\n        for index, fields in enumerate(field_list):\n            if index in errors:\n                issue_list.append({'status': 'Error', 'error': errors[index],\n                                   'issue': None, 'input_fields': fields})\n            else:\n                issue = raw_issue_json['issues'].pop(0)\n                if prefetch:\n                    issue = self.issue(issue['key'])\n                else:\n                    issue = Issue(self._options, self._session, raw=issue)\n                issue_list.append({'status': 'Success', 'issue': issue,\n                                   'error': None, 'input_fields': fields})\n        return issue_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning whether or not the JIRA instance supports service desk.", "response": "def supports_service_desk(self):\n        \"\"\"Returns whether or not the JIRA instance supports service desk.\n\n        :rtype: bool\n        \"\"\"\n        url = self._options['server'] + '/rest/servicedeskapi/info'\n        headers = {'X-ExperimentalApi': 'opt-in'}\n        try:\n            r = self._session.get(url, headers=headers)\n            return r.status_code == 200\n        except JIRAError:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new customer and return an issue Resource for it.", "response": "def create_customer(self, email, displayName):\n        \"\"\"Create a new customer and return an issue Resource for it.\n\n        :param email: Customer Email\n        :type email: str\n        :param displayName: Customer display name\n        :type displayName: str\n        :rtype: Customer\n\n        \"\"\"\n        url = self._options['server'] + '/rest/servicedeskapi/customer'\n        headers = {'X-ExperimentalApi': 'opt-in'}\n        r = self._session.post(url, headers=headers, data=json.dumps({\n            'email': email,\n            'displayName': displayName\n        }))\n\n        raw_customer_json = json_loads(r)\n\n        if r.status_code != 201:\n            raise JIRAError(r.status_code, request=r)\n        return Customer(self._options, self._session, raw=raw_customer_json)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a list of ServiceDesk Resources from the server visible to the current authenticated user.", "response": "def service_desks(self):\n        \"\"\"Get a list of ServiceDesk Resources from the server visible to the current authenticated user.\n\n        :rtype: List[ServiceDesk]\n\n        \"\"\"\n        url = self._options['server'] + '/rest/servicedeskapi/servicedesk'\n        headers = {'X-ExperimentalApi': 'opt-in'}\n        r_json = json_loads(self._session.get(url, headers=headers))\n        projects = [ServiceDesk(self._options, self._session, raw_project_json)\n                    for raw_project_json in r_json['values']]\n        return projects"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new customer request and return an issue Resource for it.", "response": "def create_customer_request(self, fields=None, prefetch=True, **fieldargs):\n        \"\"\"Create a new customer request and return an issue Resource for it.\n\n        Each keyword argument (other than the predefined ones) is treated as a field name and the argument's value\n        is treated as the intended value for that field -- if the fields argument is used, all other keyword arguments\n        will be ignored.\n\n        By default, the client will immediately reload the issue Resource created by this method in order to return\n        a complete Issue object to the caller; this behavior can be controlled through the 'prefetch' argument.\n\n        JIRA projects may contain many different issue types. Some issue screens have different requirements for\n        fields in a new issue. This information is available through the 'createmeta' method. Further examples are\n        available here: https://developer.atlassian.com/display/JIRADEV/JIRA+REST+API+Example+-+Create+Issue\n\n        :param fields: a dict containing field names and the values to use. If present, all other keyword arguments\n            will be ignored\n        :type fields: Dict[str, Any]\n        :param prefetch: whether to reload the created issue Resource so that all of its data is present in the value\n            returned from this method\n        :type prefetch: bool\n        :rtype: Issue\n        \"\"\"\n        data = fields\n\n        p = data['serviceDeskId']\n        service_desk = None\n\n        if isinstance(p, string_types) or isinstance(p, integer_types):\n            service_desk = self.service_desk(p)\n        elif isinstance(p, ServiceDesk):\n            service_desk = p\n\n        data['serviceDeskId'] = service_desk.id\n\n        p = data['requestTypeId']\n        if isinstance(p, integer_types):\n            data['requestTypeId'] = p\n        elif isinstance(p, string_types):\n            data['requestTypeId'] = self.request_type_by_name(\n                service_desk, p).id\n\n        url = self._options['server'] + '/rest/servicedeskapi/request'\n        headers = {'X-ExperimentalApi': 'opt-in'}\n        r = self._session.post(url, headers=headers, data=json.dumps(data))\n\n        raw_issue_json = json_loads(r)\n        if 'issueKey' not in raw_issue_json:\n            raise JIRAError(r.status_code, request=r)\n        if prefetch:\n            return self.issue(raw_issue_json['issueKey'])\n        else:\n            return Issue(self._options, self._session, raw=raw_issue_json)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the metadata required to create issues.", "response": "def createmeta(self,\n                   projectKeys=None,\n                   projectIds=[],\n                   issuetypeIds=None,\n                   issuetypeNames=None,\n                   expand=None,\n                   ):\n        \"\"\"Get the metadata required to create issues, optionally filtered by projects and issue types.\n\n        :param projectKeys: keys of the projects to filter the results with.\n            Can be a single value or a comma-delimited string. May be combined\n            with projectIds.\n        :type projectKeys: Union[None, Tuple[str, str], str]\n        :param projectIds: IDs of the projects to filter the results with. Can\n            be a single value or a comma-delimited string. May be combined with\n            projectKeys.\n        :type projectIds: Union[List, Tuple[str, str]]\n        :param issuetypeIds: IDs of the issue types to filter the results with.\n            Can be a single value or a comma-delimited string. May be combined\n            with issuetypeNames.\n        :type issuetypeIds: Optional[List[str]]\n        :param issuetypeNames: Names of the issue types to filter the results\n            with. Can be a single value or a comma-delimited string. May be\n            combined with issuetypeIds.\n        :type issuetypeNames: Optional[str]\n        :param expand: extra information to fetch inside each resource.\n        :type expand: Optional[str]\n        :rtype: Dict[str, Any]\n\n        \"\"\"\n        params = {}\n        if projectKeys is not None:\n            params['projectKeys'] = projectKeys\n        if projectIds is not None:\n            if isinstance(projectIds, string_types):\n                projectIds = projectIds.split(',')\n            params['projectIds'] = projectIds\n        if issuetypeIds is not None:\n            params['issuetypeIds'] = issuetypeIds\n        if issuetypeNames is not None:\n            params['issuetypeNames'] = issuetypeNames\n        if expand is not None:\n            params['expand'] = expand\n        return self._get_json('issue/createmeta', params)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef assign_issue(self, issue, assignee):\n        url = self._options['server'] + \\\n            '/rest/api/latest/issue/' + str(issue) + '/assignee'\n        payload = {'name': assignee}\n        r = self._session.put(\n            url, data=json.dumps(payload))\n        raise_on_error(r)\n        return True", "response": "Assign an issue to a user."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a list of comment Resources.", "response": "def comments(self, issue):\n        \"\"\"Get a list of comment Resources.\n\n        :param issue: the issue to get comments from\n        :type issue: str\n        :rtype: List[Comment]\n        \"\"\"\n        r_json = self._get_json('issue/' + str(issue) + '/comment')\n\n        comments = [Comment(self._options, self._session, raw_comment_json)\n                    for raw_comment_json in r_json['comments']]\n        return comments"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_comment(self, issue, body, visibility=None, is_internal=False):\n        data = {\n            'body': body,\n        }\n\n        if is_internal:\n            data.update({\n                'properties': [\n                    {'key': 'sd.public.comment',\n                     'value': {'internal': is_internal}}\n                ]\n            })\n\n        if visibility is not None:\n            data['visibility'] = visibility\n\n        url = self._get_url('issue/' + str(issue) + '/comment')\n        r = self._session.post(\n            url, data=json.dumps(data)\n        )\n\n        comment = Comment(self._options, self._session, raw=json_loads(r))\n        return comment", "response": "Adds a comment to the current authenticated user on the specified issue and returns a Resource for it."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remote_links(self, issue):\n        r_json = self._get_json('issue/' + str(issue) + '/remotelink')\n        remote_links = [RemoteLink(\n            self._options, self._session, raw_remotelink_json) for raw_remotelink_json in r_json]\n        return remote_links", "response": "Get a list of remote link Resources from an issue."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_remote_link(self, issue, destination, globalId=None, application=None, relationship=None):\n        try:\n            applicationlinks = self.applicationlinks()\n        except JIRAError as e:\n            applicationlinks = []\n            # In many (if not most) configurations, non-admin users are\n            # not allowed to list applicationlinks; if we aren't allowed,\n            # let's let people try to add remote links anyway, we just\n            # won't be able to be quite as helpful.\n            warnings.warn(\n                \"Unable to gather applicationlinks; you will not be able \"\n                \"to add links to remote issues: (%s) %s\" % (\n                    e.status_code,\n                    e.text),\n                Warning)\n\n        data = {}\n        if isinstance(destination, Issue):\n\n            data['object'] = {\n                'title': str(destination),\n                'url': destination.permalink()}\n\n            for x in applicationlinks:\n                if x['application']['displayUrl'] == destination._options['server']:\n                    data['globalId'] = \"appId=%s&issueId=%s\" % (\n                        x['application']['id'], destination.raw['id'])\n                    data['application'] = {\n                        'name': x['application']['name'], 'type': \"com.atlassian.jira\"}\n                    break\n            if 'globalId' not in data:\n                raise NotImplementedError(\n                    \"Unable to identify the issue to link to.\")\n        else:\n\n            if globalId is not None:\n                data['globalId'] = globalId\n            if application is not None:\n                data['application'] = application\n            data['object'] = destination\n\n        if relationship is not None:\n            data['relationship'] = relationship\n\n        # check if the link comes from one of the configured application links\n        for x in applicationlinks:\n            if x['application']['displayUrl'] == self._options['server']:\n                data['globalId'] = \"appId=%s&issueId=%s\" % (\n                    x['application']['id'], destination.raw['id'])\n                data['application'] = {\n                    'name': x['application']['name'], 'type': \"com.atlassian.jira\"}\n                break\n\n        url = self._get_url('issue/' + str(issue) + '/remotelink')\n        r = self._session.post(\n            url, data=json.dumps(data))\n\n        remote_link = RemoteLink(\n            self._options, self._session, raw=json_loads(r))\n        return remote_link", "response": "Add a remote link from an issue to an external application and returns a Resource object for it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a simple remote link from an issue to the web resource.", "response": "def add_simple_link(self, issue, object):\n        \"\"\"Add a simple remote link from an issue to web resource.\n\n        This avoids the admin access problems from add_remote_link by just\n            using a simple object and presuming all fields are correct and not\n            requiring more complex ``application`` data.\n\n        ``object`` should be a dict containing at least ``url`` to the\n            linked external URL and ``title`` to display for the link inside JIRA.\n\n        For definitions of the allowable fields for ``object`` , see https://developer.atlassian.com/display/JIRADEV/JIRA+REST+API+for+Remote+Issue+Links.\n\n        :param issue: the issue to add the remote link to\n        :param object: the dictionary used to create remotelink data\n        \"\"\"\n        data = {\"object\": object}\n        url = self._get_url('issue/' + str(issue) + '/remotelink')\n        r = self._session.post(\n            url, data=json.dumps(data))\n\n        simple_link = RemoteLink(\n            self._options, self._session, raw=json_loads(r))\n        return simple_link"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a list of the transitions available on the current user.", "response": "def transitions(self, issue, id=None, expand=None):\n        \"\"\"Get a list of the transitions available on the specified issue to the current user.\n\n        :param issue: ID or key of the issue to get the transitions from\n        :param id: if present, get only the transition matching this ID\n        :param expand: extra information to fetch inside each transition\n        \"\"\"\n        params = {}\n        if id is not None:\n            params['transitionId'] = id\n        if expand is not None:\n            params['expand'] = expand\n        return self._get_json('issue/' + str(issue) + '/transitions', params=params)['transitions']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_transitionid_by_name(self, issue, transition_name):\n        transitions_json = self.transitions(issue)\n        id = None\n\n        for transition in transitions_json:\n            if transition[\"name\"].lower() == transition_name.lower():\n                id = transition[\"id\"]\n                break\n        return id", "response": "Find a transitionid available on the specified issue to the current user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nperform a transition on an issue.", "response": "def transition_issue(self, issue, transition, fields=None, comment=None, worklog=None, **fieldargs):\n        \"\"\"Perform a transition on an issue.\n\n        Each keyword argument (other than the predefined ones) is treated as a field name and the argument's value\n        is treated as the intended value for that field -- if the fields argument is used, all other keyword arguments\n        will be ignored. Field values will be set on the issue as part of the transition process.\n\n        :param issue: ID or key of the issue to perform the transition on\n        :param transition: ID or name of the transition to perform\n        :param comment: *Optional* String to add as comment to the issue when\n            performing the transition.\n        :param fields: a dict containing field names and the values to use.\n            If present, all other keyword arguments will be ignored\n        \"\"\"\n        transitionId = None\n\n        try:\n            transitionId = int(transition)\n        except Exception:\n            # cannot cast to int, so try to find transitionId by name\n            transitionId = self.find_transitionid_by_name(issue, transition)\n            if transitionId is None:\n                raise JIRAError(\"Invalid transition name. %s\" % transition)\n\n        data = {\n            'transition': {\n                'id': transitionId}}\n        if comment:\n            data['update'] = {'comment': [{'add': {'body': comment}}]}\n        if worklog:\n            data['update'] = {'worklog': [{'add': {'timeSpent': worklog}}]}\n        if fields is not None:\n            data['fields'] = fields\n        else:\n            fields_dict = {}\n            for field in fieldargs:\n                fields_dict[field] = fieldargs[field]\n            data['fields'] = fields_dict\n\n        url = self._get_url('issue/' + str(issue) + '/transitions')\n        r = self._session.post(\n            url, data=json.dumps(data))\n        try:\n            r_json = json_loads(r)\n        except ValueError as e:\n            logging.error(\"%s\\n%s\" % (e, r.text))\n            raise e\n        return r_json"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nregistering a vote for the current authenticated user on an issue.", "response": "def add_vote(self, issue):\n        \"\"\"Register a vote for the current authenticated user on an issue.\n\n        :param issue: ID or key of the issue to vote on\n        :rtype: Response\n        \"\"\"\n        url = self._get_url('issue/' + str(issue) + '/votes')\n        return self._session.post(url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_vote(self, issue):\n        url = self._get_url('issue/' + str(issue) + '/votes')\n        self._session.delete(url)", "response": "Remove the current authenticated user s vote from an issue."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_watcher(self, issue, watcher):\n        url = self._get_url('issue/' + str(issue) + '/watchers')\n        self._session.post(\n            url, data=json.dumps(watcher))", "response": "Add a user to an issue s watchers list."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove a user from an issue s watch list.", "response": "def remove_watcher(self, issue, watcher):\n        \"\"\"Remove a user from an issue's watch list.\n\n        :param issue: ID or key of the issue affected\n        :param watcher: username of the user to remove from the watchers list\n        :rtype: Response\n        \"\"\"\n        url = self._get_url('issue/' + str(issue) + '/watchers')\n        params = {'username': watcher}\n        result = self._session.delete(url, params=params)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a list of worklog Resources from the server for an issue.", "response": "def worklogs(self, issue):\n        \"\"\"Get a list of worklog Resources from the server for an issue.\n\n        :param issue: ID or key of the issue to get worklogs from\n        :rtype: List[Worklog]\n        \"\"\"\n        r_json = self._get_json('issue/' + str(issue) + '/worklog')\n        worklogs = [Worklog(self._options, self._session, raw_worklog_json)\n                    for raw_worklog_json in r_json['worklogs']]\n        return worklogs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_worklog(self,\n                    issue,\n                    timeSpent=None,\n                    timeSpentSeconds=None,\n                    adjustEstimate=None,\n                    newEstimate=None,\n                    reduceBy=None,\n                    comment=None,\n                    started=None,\n                    user=None,\n                    ):\n        \"\"\"Add a new worklog entry on an issue and return a Resource for it.\n\n        :param issue: the issue to add the worklog to\n        :param timeSpent: a worklog entry with this amount of time spent, e.g. \"2d\"\n        :param adjustEstimate: (optional) allows the user to provide specific instructions to update the remaining\n            time estimate of the issue. The value can either be ``new``, ``leave``, ``manual`` or ``auto`` (default).\n        :param newEstimate: the new value for the remaining estimate field. e.g. \"2d\"\n        :param reduceBy: the amount to reduce the remaining estimate by e.g. \"2d\"\n        :param started: Moment when the work is logged, if not specified will default to now\n        :param comment: optional worklog comment\n        :rtype: Worklog\n        \"\"\"\n        params = {}\n        if adjustEstimate is not None:\n            params['adjustEstimate'] = adjustEstimate\n        if newEstimate is not None:\n            params['newEstimate'] = newEstimate\n        if reduceBy is not None:\n            params['reduceBy'] = reduceBy\n\n        data = {}\n        if timeSpent is not None:\n            data['timeSpent'] = timeSpent\n        if timeSpentSeconds is not None:\n            data['timeSpentSeconds'] = timeSpentSeconds\n        if comment is not None:\n            data['comment'] = comment\n        elif user:\n            # we log user inside comment as it doesn't always work\n            data['comment'] = user\n\n        if started is not None:\n            # based on REST Browser it needs: \"2014-06-03T08:21:01.273+0000\"\n            if started.tzinfo is None:\n                data['started'] = started.strftime(\"%Y-%m-%dT%H:%M:%S.000+0000\")\n            else:\n                data['started'] = started.strftime(\"%Y-%m-%dT%H:%M:%S.000%z\")\n        if user is not None:\n            data['author'] = {\"name\": user,\n                              'self': self.JIRA_BASE_URL + '/rest/api/latest/user?username=' + user,\n                              'displayName': user,\n                              'active': False\n                              }\n            data['updateAuthor'] = data['author']\n        # report bug to Atlassian: author and updateAuthor parameters are\n        # ignored.\n        url = self._get_url('issue/{0}/worklog'.format(issue))\n        r = self._session.post(url, params=params, data=json.dumps(data))\n\n        return Worklog(self._options, self._session, json_loads(r))", "response": "Adds a new worklog entry to an issue and returns a Resource object for it."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_issue_link(self, type, inwardIssue, outwardIssue, comment=None):\n        # let's see if we have the right issue link 'type' and fix it if needed\n        if not hasattr(self, '_cached_issuetypes'):\n            self._cached_issue_link_types = self.issue_link_types()\n\n        if type not in self._cached_issue_link_types:\n            for lt in self._cached_issue_link_types:\n                if lt.outward == type:\n                    # we are smart to figure it out what he meant\n                    type = lt.name\n                    break\n                elif lt.inward == type:\n                    # so that's the reverse, so we fix the request\n                    type = lt.name\n                    inwardIssue, outwardIssue = outwardIssue, inwardIssue\n                    break\n\n        data = {\n            'type': {\n                'name': type},\n            'inwardIssue': {\n                'key': inwardIssue},\n            'outwardIssue': {\n                'key': outwardIssue},\n            'comment': comment}\n        url = self._get_url('issueLink')\n        return self._session.post(\n            url, data=json.dumps(data))", "response": "Creates a link between two issues."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes a link between two issues.", "response": "def delete_issue_link(self, id):\n        \"\"\"Delete a link between two issues.\n\n        :param id: ID of the issue link to delete\n        \"\"\"\n        url = self._get_url('issueLink') + \"/\" + id\n        return self._session.delete(url)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef issue_link_types(self):\n        r_json = self._get_json('issueLinkType')\n        link_types = [IssueLinkType(self._options, self._session, raw_link_json) for raw_link_json in\n                      r_json['issueLinkTypes']]\n        return link_types", "response": "Get a list of issue link type Resources from the server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef issue_types(self):\n        r_json = self._get_json('issuetype')\n        issue_types = [IssueType(\n            self._options, self._session, raw_type_json) for raw_type_json in r_json]\n        return issue_types", "response": "Get a list of issue type Resources from the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef issue_type_by_name(self, name):\n        issue_types = self.issue_types()\n        try:\n            issue_type = [it for it in issue_types if it.name == name][0]\n        except IndexError:\n            raise KeyError(\"Issue type '%s' is unknown.\" % name)\n        return issue_type", "response": "Returns the issue type with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of request types supported by a service desk instance.", "response": "def request_types(self, service_desk):\n        \"\"\" Returns request types supported by a service desk instance.\n        :param service_desk: The service desk instance.\n        :type service_desk: ServiceDesk\n        :rtype: List[RequestType]\n        \"\"\"\n        if hasattr(service_desk, 'id'):\n            service_desk = service_desk.id\n        url = (self._options['server'] +\n               '/rest/servicedeskapi/servicedesk/%s/requesttype'\n               % service_desk)\n        headers = {'X-ExperimentalApi': 'opt-in'}\n        r_json = json_loads(self._session.get(url, headers=headers))\n        request_types = [\n            RequestType(self._options, self._session, raw_type_json)\n            for raw_type_json in r_json['values']]\n        return request_types"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef my_permissions(self,\n                       projectKey=None,\n                       projectId=None,\n                       issueKey=None,\n                       issueId=None,\n                       ):\n        \"\"\"Get a dict of all available permissions on the server.\n\n        :param projectKey: limit returned permissions to the specified project\n        :type projectKey: Optional[str]\n        :param projectId: limit returned permissions to the specified project\n        :type projectId: Optional[str]\n        :param issueKey: limit returned permissions to the specified issue\n        :type issueKey: Optional[str]\n        :param issueId: limit returned permissions to the specified issue\n        :type issueId: Optional[str]\n        :rtype: Dict[str, Dict[str, Dict[str, str]]]\n        \"\"\"\n        params = {}\n        if projectKey is not None:\n            params['projectKey'] = projectKey\n        if projectId is not None:\n            params['projectId'] = projectId\n        if issueKey is not None:\n            params['issueKey'] = issueKey\n        if issueId is not None:\n            params['issueId'] = issueId\n        return self._get_json('mypermissions', params=params)", "response": "Get a dict of all available permissions on the server."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a list of priority Resources from the server.", "response": "def priorities(self):\n        \"\"\"Get a list of priority Resources from the server.\n\n        :rtype: List[Priority]\n\n        \"\"\"\n        r_json = self._get_json('priority')\n        priorities = [Priority(\n            self._options, self._session, raw_priority_json) for raw_priority_json in r_json]\n        return priorities"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a list of project Resources from the server visible to the current authenticated user.", "response": "def projects(self):\n        \"\"\"Get a list of project Resources from the server visible to the current authenticated user.\n\n        :rtype: List[Project]\n\n        \"\"\"\n        r_json = self._get_json('project')\n        projects = [Project(\n            self._options, self._session, raw_project_json) for raw_project_json in r_json]\n        return projects"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_temp_project_avatar(self, project, filename, size, avatar_img, contentType=None, auto_confirm=False):\n        size_from_file = os.path.getsize(filename)\n        if size != size_from_file:\n            size = size_from_file\n\n        params = {\n            'filename': filename,\n            'size': size}\n\n        headers = {'X-Atlassian-Token': 'no-check'}\n        if contentType is not None:\n            headers['content-type'] = contentType\n        else:\n            # try to detect content-type, this may return None\n            headers['content-type'] = self._get_mime_type(avatar_img)\n\n        url = self._get_url('project/' + project + '/avatar/temporary')\n        r = self._session.post(\n            url, params=params, headers=headers, data=avatar_img)\n\n        cropping_properties = json_loads(r)\n        if auto_confirm:\n            return self.confirm_project_avatar(project, cropping_properties)\n        else:\n            return cropping_properties", "response": "Register an image file as a project avatar."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconfirm the temporary avatar image previously uploaded with the specified cropping properties.", "response": "def confirm_project_avatar(self, project, cropping_properties):\n        \"\"\"Confirm the temporary avatar image previously uploaded with the specified cropping.\n\n        After a successful registry with :py:meth:`create_temp_project_avatar`, use this method to confirm the avatar\n        for use. The final avatar can be a subarea of the uploaded image, which is customized with the\n        ``cropping_properties``: the return value of :py:meth:`create_temp_project_avatar` should be used for this\n        argument.\n\n        :param project: ID or key of the project to confirm the avatar in\n        :param cropping_properties: a dict of cropping properties from :py:meth:`create_temp_project_avatar`\n        \"\"\"\n        data = cropping_properties\n        url = self._get_url('project/' + project + '/avatar')\n        r = self._session.post(\n            url, data=json.dumps(data))\n\n        return json_loads(r)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_project_avatar(self, project, avatar):\n        self._set_avatar(\n            None, self._get_url('project/' + project + '/avatar'), avatar)", "response": "Set a project s avatar."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_project_avatar(self, project, avatar):\n        url = self._get_url('project/' + project + '/avatar/' + avatar)\n        return self._session.delete(url)", "response": "Delete a project s avatar."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef project_components(self, project):\n        r_json = self._get_json('project/' + project + '/components')\n        components = [Component(\n            self._options, self._session, raw_comp_json) for raw_comp_json in r_json]\n        return components", "response": "Get a list of component Resources present on a project."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef project_versions(self, project):\n        r_json = self._get_json('project/' + project + '/versions')\n        versions = [\n            Version(self._options, self._session, raw_ver_json) for raw_ver_json in r_json]\n        return versions", "response": "Get a list of version Resources present on a project."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_project_version_by_name(self, project, version_name):\n        versions = self.project_versions(project)\n        for version in versions:\n            if version.name == version_name:\n                return version", "response": "Get a version Resource by its name present on a project."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rename_version(self, project, old_name, new_name):\n        version = self.get_project_version_by_name(project, old_name)\n        if version:\n            version.update(name=new_name)", "response": "Rename a version Resource on a project."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a dict of role names to resource locations for a project.", "response": "def project_roles(self, project):\n        \"\"\"Get a dict of role names to resource locations for a project.\n\n        :param project: ID or key of the project to get roles from\n        \"\"\"\n        path = 'project/' + project + '/role'\n        _rolesdict = self._get_json(path)\n        rolesdict = {}\n\n        for k, v in _rolesdict.items():\n            tmp = {}\n            tmp['id'] = v.split(\"/\")[-1]\n            tmp['url'] = v\n            rolesdict[k] = tmp\n        return rolesdict"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef project_role(self, project, id):\n        if isinstance(id, Number):\n            id = \"%s\" % id\n        return self._find_for_resource(Role, (project, id))", "response": "Get a role from a project."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resolutions(self):\n        r_json = self._get_json('resolution')\n        resolutions = [Resolution(\n            self._options, self._session, raw_res_json) for raw_res_json in r_json]\n        return resolutions", "response": "Get a list of resolution Resources from the server."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsearch for issues in a JQL string.", "response": "def search_issues(self,\n                      jql_str,\n                      startAt=0,\n                      maxResults=50,\n                      validate_query=True,\n                      fields=None,\n                      expand=None,\n                      json_result=None,\n                      ):\n        \"\"\"Get a :class:`~jira.client.ResultList` of issue Resources matching a JQL search string.\n\n        :param jql_str: The JQL search string.\n        :type jql_str: str\n        :param startAt: Index of the first issue to return. (Default: 0)\n        :type startAt: int\n        :param maxResults: Maximum number of issues to return. Total number of results\n            is available in the ``total`` attribute of the returned :class:`~jira.client.ResultList`.\n            If maxResults evaluates as False, it will try to get all issues in batches. (Default: 50)\n        :type maxResults: int\n        :param validate_query: Whether or not the query should be validated. (Default: True)\n        :type validate_query: bool\n        :param fields: comma-separated string or list of issue fields to include in the results.\n            Default is to include all fields.\n        :type fields: Optional[str or list]\n        :param expand: extra information to fetch inside each resource\n        :type expand: Optional[str]\n        :param json_result: JSON response will be returned when this parameter is set to True.\n                Otherwise, :class:`~jira.client.ResultList` will be returned.\n        :type json_result: bool\n\n        :rtype: dict or :class:`~jira.client.ResultList`\n\n        \"\"\"\n        if isinstance(fields, string_types):\n            fields = fields.split(\",\")\n        else:\n            fields = list(fields or [])\n\n        # this will translate JQL field names to REST API Name\n        # most people do know the JQL names so this will help them use the API easier\n        untranslate = {}  # use to add friendly aliases when we get the results back\n        if self._fields:\n            for i, field in enumerate(fields):\n                if field in self._fields:\n                    untranslate[self._fields[field]] = fields[i]\n                    fields[i] = self._fields[field]\n\n        search_params = {\n            \"jql\": jql_str,\n            \"startAt\": startAt,\n            \"validateQuery\": validate_query,\n            \"fields\": fields,\n            \"expand\": expand}\n        if json_result:\n            search_params[\"maxResults\"] = maxResults\n            if not maxResults:\n                warnings.warn('All issues cannot be fetched at once, when json_result parameter is set', Warning)\n            return self._get_json('search', params=search_params)\n\n        issues = self._fetch_pages(Issue, 'issues', 'search', startAt, maxResults, search_params)\n\n        if untranslate:\n            for i in issues:\n                for k, v in iteritems(untranslate):\n                    if k in i.raw.get('fields', {}):\n                        i.raw['fields'][v] = i.raw['fields'][k]\n\n        return issues"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef server_info(self):\n        retry = 0\n        j = self._get_json('serverInfo')\n        while not j and retry < 3:\n            logging.warning(\"Bug https://jira.atlassian.com/browse/JRA-59676 trying again...\")\n            retry += 1\n            j = self._get_json('serverInfo')\n        return j", "response": "Get a dict of server information for this JIRA instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef statuses(self):\n        r_json = self._get_json('status')\n        statuses = [Status(self._options, self._session, raw_stat_json)\n                    for raw_stat_json in r_json]\n        return statuses", "response": "Get a list of status Resources from the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef statuscategories(self):\n        r_json = self._get_json('statuscategory')\n        statuscategories = [StatusCategory(self._options, self._session, raw_stat_json)\n                            for raw_stat_json in r_json]\n        return statuscategories", "response": "Get a list of status category Resources from the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a user Resource from the server.", "response": "def user(self, id, expand=None):\n        \"\"\"Get a user Resource from the server.\n\n        :param id: ID of the user to get\n        :param id: str\n        :param expand: Extra information to fetch inside each resource\n        :type expand: Optional[Any]\n\n        :rtype: User\n        \"\"\"\n        user = User(self._options, self._session)\n        params = {}\n        if expand is not None:\n            params['expand'] = expand\n        user.find(id, params=params)\n        return user"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsearches for user Resources that match the search string and can be assigned issues for projects.", "response": "def search_assignable_users_for_projects(self, username, projectKeys, startAt=0, maxResults=50):\n        \"\"\"Get a list of user Resources that match the search string and can be assigned issues for projects.\n\n        :param username: A string to match usernames against\n        :type username: str\n        :param projectKeys: Comma-separated list of project keys to check for issue assignment permissions\n        :type projectKeys: str\n        :param startAt: Index of the first user to return (Default: 0)\n        :type startAt: int\n        :param maxResults: Maximum number of users to return.\n                If maxResults evaluates as False, it will try to get all users in batches. (Default: 50)\n        :type maxResults: int\n\n        :rtype: ResultList\n\n        \"\"\"\n        params = {\n            'username': username,\n            'projectKeys': projectKeys}\n        return self._fetch_pages(User, None, 'user/assignable/multiProjectSearch', startAt, maxResults, params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef search_assignable_users_for_issues(self,\n                                           username,\n                                           project=None,\n                                           issueKey=None,\n                                           expand=None,\n                                           startAt=0,\n                                           maxResults=50,\n                                           ):\n        \"\"\"Get a list of user Resources that match the search string for assigning or creating issues.\n\n        This method is intended to find users that are eligible to create issues in a project or be assigned\n        to an existing issue. When searching for eligible creators, specify a project. When searching for eligible\n        assignees, specify an issue key.\n\n        :param username: A string to match usernames against\n        :type username: str\n        :param project: Filter returned users by permission in this project (expected if a result will be used to\n            create an issue)\n        :type project: Optional[str]\n        :param issueKey: Filter returned users by this issue (expected if a result will be used to edit this issue)\n        :type issueKey: Optional[str]\n        :param expand: Extra information to fetch inside each resource\n        :type expand: Optional[Any]\n        :param startAt: Index of the first user to return (Default: 0)\n        :type startAt: int\n        :param maxResults: maximum number of users to return.\n                If maxResults evaluates as False, it will try to get all items in batches. (Default: 50)\n\n        :rtype: ResultList\n        \"\"\"\n        params = {\n            'username': username}\n        if project is not None:\n            params['project'] = project\n        if issueKey is not None:\n            params['issueKey'] = issueKey\n        if expand is not None:\n            params['expand'] = expand\n        return self._fetch_pages(User, None, 'user/assignable/search', startAt, maxResults, params)", "response": "Search for users that are eligible to create issues."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_temp_user_avatar(self,\n                                user,\n                                filename,\n                                size,\n                                avatar_img,\n                                contentType=None,\n                                auto_confirm=False,\n                                ):\n        \"\"\"Register an image file as a user avatar.\n\n        The avatar created is temporary and must be confirmed before it can\n        be used.\n\n        Avatar images are specified by a filename, size, and file object. By default, the client will attempt to\n        autodetect the picture's content type: this mechanism relies on ``libmagic`` and will not work out of the box\n        on Windows systems (see http://filemagic.readthedocs.org/en/latest/guide.html for details on how to install\n        support). The ``contentType`` argument can be used to explicitly set the value (note that JIRA will reject any\n        type other than the well-known ones for images, e.g. ``image/jpg``, ``image/png``, etc.)\n\n        This method returns a dict of properties that can be used to crop a subarea of a larger image for use. This\n        dict should be saved and passed to :py:meth:`confirm_user_avatar` to finish the avatar creation process. If you\n        want to cut out the middleman and confirm the avatar with JIRA's default cropping, pass the ``auto_confirm``\n        argument with a truthy value and :py:meth:`confirm_user_avatar` will be called for you before this method\n        returns.\n\n        :param user: User to register the avatar for\n        :type user: str\n        :param filename: name of the avatar file\n        :type filename: str\n        :param size: size of the avatar file\n        :type size: int\n        :param avatar_img: file-like object containing the avatar\n        :type avatar_img: bytes\n        :param contentType: explicit specification for the avatar image's content-type\n        :type contentType: Optional[Any]\n        :param auto_confirm: whether to automatically confirm the temporary avatar by calling\n            :py:meth:`confirm_user_avatar` with the return value of this method. (Default: False)\n        :type auto_confirm: bool\n\n        :rtype: NoReturn\n        \"\"\"\n        size_from_file = os.path.getsize(filename)\n        if size != size_from_file:\n            size = size_from_file\n\n        # remove path from filename\n        filename = os.path.split(filename)[1]\n\n        params = {\n            'username': user,\n            'filename': filename,\n            'size': size}\n\n        headers = {'X-Atlassian-Token': 'no-check'}\n        if contentType is not None:\n            headers['content-type'] = contentType\n        else:\n            # try to detect content-type, this may return None\n            headers['content-type'] = self._get_mime_type(avatar_img)\n\n        url = self._get_url('user/avatar/temporary')\n        r = self._session.post(\n            url, params=params, headers=headers, data=avatar_img)\n\n        cropping_properties = json_loads(r)\n        if auto_confirm:\n            return self.confirm_user_avatar(user, cropping_properties)\n        else:\n            return cropping_properties", "response": "This method creates a temporary user avatar."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef confirm_user_avatar(self, user, cropping_properties):\n        data = cropping_properties\n        url = self._get_url('user/avatar')\n        r = self._session.post(url, params={'username': user},\n                               data=json.dumps(data))\n\n        return json_loads(r)", "response": "Confirm the temporary avatar image previously uploaded with the specified cropping properties."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_user_avatar(self, username, avatar):\n        self._set_avatar(\n            {'username': username}, self._get_url('user/avatar'), avatar)", "response": "Set a user s avatar for the specified user"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes a user s avatar from the cache.", "response": "def delete_user_avatar(self, username, avatar):\n        \"\"\"Delete a user's avatar.\n\n        :param username: the user to delete the avatar from\n        :param avatar: ID of the avatar to remove\n        \"\"\"\n        params = {'username': username}\n        url = self._get_url('user/avatar/' + avatar)\n        return self._session.delete(url, params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef search_users(self, user, startAt=0, maxResults=50, includeActive=True, includeInactive=False):\n        params = {\n            'username': user,\n            'includeActive': includeActive,\n            'includeInactive': includeInactive}\n        return self._fetch_pages(User, None, 'user/search', startAt, maxResults, params)", "response": "Search for users in the database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef search_allowed_users_for_issue(self, user, issueKey=None, projectKey=None, startAt=0, maxResults=50):\n        params = {\n            'username': user}\n        if issueKey is not None:\n            params['issueKey'] = issueKey\n        if projectKey is not None:\n            params['projectKey'] = projectKey\n        return self._fetch_pages(User, None, 'user/viewissue/search', startAt, maxResults, params)", "response": "Search allowed users for a user in a given issue or project."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a version in a project and return a Resource object for it.", "response": "def create_version(self,\n                       name,\n                       project,\n                       description=None,\n                       releaseDate=None,\n                       startDate=None,\n                       archived=False,\n                       released=False,\n                       ):\n        \"\"\"Create a version in a project and return a Resource for it.\n\n        :param name: name of the version to create\n        :type name: str\n        :param project: key of the project to create the version in\n        :type project: str\n        :param description: a description of the version\n        :type description: str\n        :param releaseDate: the release date assigned to the version\n        :type releaseDate: Optional[Any]\n        :param startDate: The start date for the version\n        :type startDate: Optional[Any]\n        :param archived: Denotes whether a version should be archived. (Default: False)\n        :type archived: bool\n        :param released: Denotes whether a version is released. (Default: False)\n        :type released: bool\n\n        :rtype: Version\n        \"\"\"\n        data = {\n            'name': name,\n            'project': project,\n            'archived': archived,\n            'released': released}\n        if description is not None:\n            data['description'] = description\n        if releaseDate is not None:\n            data['releaseDate'] = releaseDate\n        if startDate is not None:\n            data['startDate'] = startDate\n\n        url = self._get_url('version')\n        r = self._session.post(\n            url, data=json.dumps(data))\n\n        time.sleep(1)\n        version = Version(self._options, self._session, raw=json_loads(r))\n        return version"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef move_version(self, id, after=None, position=None):\n        data = {}\n        if after is not None:\n            data['after'] = after\n        elif position is not None:\n            data['position'] = position\n\n        url = self._get_url('version/' + id + '/move')\n        r = self._session.post(\n            url, data=json.dumps(data))\n\n        version = Version(self._options, self._session, raw=json_loads(r))\n        return version", "response": "Move a version within a project s ordered version list and return a new version Resource for it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a version of a resource.", "response": "def version(self, id, expand=None):\n        \"\"\"Get a version Resource.\n\n        :param id: ID of the version to get\n        :type id: str\n        :param expand: extra information to fetch inside each resource\n        :type expand: Optional[Any]\n\n        :rtype: Version\n        \"\"\"\n        version = Version(self._options, self._session)\n        params = {}\n        if expand is not None:\n            params['expand'] = expand\n        version.find(id, params=params)\n        return version"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a dict of the current authenticated user s session information.", "response": "def session(self, auth=None):\n        \"\"\"Get a dict of the current authenticated user's session information.\n\n        :param auth: Tuple of username and password.\n        :type auth: Optional[Tuple[str,str]]\n\n        :rtype: User\n\n        \"\"\"\n        url = '{server}{auth_url}'.format(**self._options)\n\n        if isinstance(self._session.auth, tuple) or auth:\n            if not auth:\n                auth = self._session.auth\n            username, password = auth\n            authentication_data = {'username': username, 'password': password}\n            r = self._session.post(url, data=json.dumps(authentication_data))\n        else:\n            r = self._session.get(url)\n\n        user = User(self._options, self._session, json_loads(r))\n        return user"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nkill the user s current WebSudo session.", "response": "def kill_websudo(self):\n        \"\"\"Destroy the user's current WebSudo session.\n\n        Works only for non-cloud deployments, for others does nothing.\n\n        :rtype: Optional[Any]\n        \"\"\"\n        if self.deploymentType != 'Cloud':\n            url = self._options['server'] + '/rest/auth/1/websudo'\n            return self._session.delete(url)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a basic http session.", "response": "def _create_http_basic_session(self, username, password, timeout=None):\n        \"\"\" Creates a basic http session.\n\n        :param username: Username for the session\n        :type username: str\n        :param password: Password for the username\n        :type password: str\n        :param timeout: If set determines the timeout period for the Session.\n        :type timeout: Optional[int]\n\n        :rtype: NoReturn\n        \"\"\"\n        verify = self._options['verify']\n        self._session = ResilientSession(timeout=timeout)\n        self._session.verify = verify\n        self._session.auth = (username, password)\n        self._session.cert = self._options['client_cert']"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_url(self, path, base=JIRA_BASE_URL):\n        options = self._options.copy()\n        options.update({'path': path})\n        return base.format(**options)", "response": "Returns the full url based on the JIRA base url and the path provided."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_json(self,\n                  path,\n                  params=None,\n                  base=JIRA_BASE_URL,\n                  ):\n        \"\"\"Get the json for a given path and params.\n\n        :param path: The subpath required\n        :type path: str\n        :param params: Parameters to filter the json query.\n        :type params: Optional[Dict[str, Any]]\n        :param base: The Base JIRA URL, defaults to the instance base.\n        :type base: Optional[str]\n\n        :rtype: Union[Dict[str, Any], List[Dict[str, str]]]\n\n        \"\"\"\n        url = self._get_url(path, base)\n        r = self._session.get(url, params=params)\n        try:\n            r_json = json_loads(r)\n        except ValueError as e:\n            logging.error(\"%s\\n%s\" % (e, r.text))\n            raise e\n        return r_json", "response": "Get the json for a given path and params."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the MIME type for a given stream of bytes .", "response": "def _get_mime_type(self, buff):\n        \"\"\"Get the MIME type for a given stream of bytes\n\n        :param buff: Stream of bytes\n        :type buff: bytes\n\n        :rtype: str\n\n        \"\"\"\n        if self._magic is not None:\n            return self._magic.id_buffer(buff)\n        else:\n            try:\n                return mimetypes.guess_type(\"f.\" + imghdr.what(0, buff))[0]\n            except (IOError, TypeError):\n                logging.warning(\"Couldn't detect content type of avatar image\"\n                                \". Specify the 'contentType' parameter explicitly.\")\n                return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rename_user(self, old_user, new_user):\n        if self._version > (6, 0, 0):\n            url = self._options['server'] + '/rest/api/latest/user'\n            payload = {\n                \"name\": new_user}\n            params = {\n                'username': old_user}\n\n            # raw displayName\n            logging.debug(\"renaming %s\" % self.user(old_user).emailAddress)\n\n            r = self._session.put(url, params=params,\n                                  data=json.dumps(payload))\n            raise_on_error(r)\n        else:\n            raise NotImplementedError(\"Support for renaming users in Jira \"\n                                      \"< 6.0.0 has been removed.\")", "response": "Rename a JIRA user."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_user(self, username):\n\n        url = self._options['server'] + '/rest/api/latest/user/?username=%s' % username\n\n        r = self._session.delete(url)\n        if 200 <= r.status_code <= 299:\n            return True\n        else:\n            logging.error(r.status_code)\n            return False", "response": "Deletes a JIRA User."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef deactivate_user(self, username):\n        if self.deploymentType == 'Cloud':\n            # Disabling users now needs cookie auth in the Cloud - see https://jira.atlassian.com/browse/ID-6230\n            if 'authCookie' not in vars(self):\n                user = self.session()\n                if user.raw is None:\n                    raise JIRAError(\"Can not log in!\")\n                self.authCookie = '%s=%s' % (user.raw['session']['name'], user.raw['session']['value'])\n            url = self._options['server'] + '/admin/rest/um/1/user/deactivate?username=%s' % (username)\n            # We can't use our existing session here - this endpoint is fragile and objects to extra headers\n            try:\n                r = requests.post(url, headers={'Cookie': self.authCookie, 'Content-Type': 'application/json'},\n                                  proxies=self._session.proxies, data={})\n                if r.status_code == 200:\n                    return True\n                else:\n                    logging.warning(\n                        'Got response from deactivating %s: %s' % (username, r.status_code))\n                    return r.status_code\n            except Exception as e:\n                logging.error(\n                    \"Error Deactivating %s: %s\" % (username, e))\n                raise JIRAError(\"Error Deactivating %s: %s\" % (username, e))\n        else:\n            url = self._options['server'] + '/secure/admin/user/EditUser.jspa'\n            self._options['headers']['Content-Type'] = 'application/x-www-form-urlencoded; charset=UTF-8'\n            user = self.user(username)\n            userInfo = {\n                'inline': 'true',\n                'decorator': 'dialog',\n                'username': user.name,\n                'fullName': user.displayName,\n                'email': user.emailAddress,\n                'editName': user.name\n            }\n            try:\n                r = self._session.post(url, headers=self._options['headers'], data=userInfo)\n                if r.status_code == 200:\n                    return True\n                else:\n                    logging.warning(\n                        'Got response from deactivating %s: %s' % (username, r.status_code))\n                    return r.status_code\n            except Exception as e:\n                logging.error(\n                    \"Error Deactivating %s: %s\" % (username, e))\n                raise JIRAError(\"Error Deactivating %s: %s\" % (username, e))", "response": "Disable or deactivate a user."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef backup(self, filename='backup.zip', attachments=False):\n        if self.deploymentType == 'Cloud':\n            url = self._options['server'] + '/rest/backup/1/export/runbackup'\n            payload = json.dumps({\"cbAttachments\": attachments})\n            self._options['headers']['X-Requested-With'] = 'XMLHttpRequest'\n        else:\n            url = self._options['server'] + '/secure/admin/XmlBackup.jspa'\n            payload = {'filename': filename}\n        try:\n            r = self._session.post(url, headers=self._options['headers'], data=payload)\n            if r.status_code == 200:\n                return True\n            else:\n                logging.warning(\n                    'Got %s response from calling backup.' % r.status_code)\n                return r.status_code\n        except Exception as e:\n            logging.error(\"I see %s\", e)", "response": "Will call jira export to backup as zipped xml. Returning with success does not mean that the backup process finished."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef backup_progress(self):\n        epoch_time = int(time.time() * 1000)\n        if self.deploymentType == 'Cloud':\n            url = self._options['server'] + '/rest/obm/1.0/getprogress?_=%i' % epoch_time\n        else:\n            logging.warning(\n                'This functionality is not available in Server version')\n            return None\n        r = self._session.get(\n            url, headers=self._options['headers'])\n        # This is weird.  I used to get xml, but now I'm getting json\n        try:\n            return json.loads(r.text)\n        except Exception:\n            import defusedxml.ElementTree as etree\n\n            progress = {}\n            try:\n                root = etree.fromstring(r.text)\n            except etree.ParseError as pe:\n                logging.warning('Unable to find backup info.  You probably need to initiate a new backup. %s' % pe)\n                return None\n            for k in root.keys():\n                progress[k] = root.get(k)\n            return progress", "response": "Return status of cloud backup as a dict."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef backup_complete(self):\n        if self.deploymentType != 'Cloud':\n            logging.warning(\n                'This functionality is not available in Server version')\n            return None\n        status = self.backup_progress()\n        perc_complete = int(re.search(r\"\\s([0-9]*)\\s\",\n                                      status['alternativePercentage']).group(1))\n        file_size = int(status['size'])\n        return perc_complete >= 100 and file_size > 0", "response": "Return boolean based on alternativePercentage and size returned from backup_progress ( cloud only."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef backup_download(self, filename=None):\n        if self.deploymentType != 'Cloud':\n            logging.warning(\n                'This functionality is not available in Server version')\n            return None\n        remote_file = self.backup_progress()['fileName']\n        local_file = filename or remote_file\n        url = self._options['server'] + '/webdav/backupmanager/' + remote_file\n        try:\n            logging.debug('Writing file to %s' % local_file)\n            with open(local_file, 'wb') as file:\n                try:\n                    resp = self._session.get(url, headers=self._options['headers'], stream=True)\n                except Exception:\n                    raise JIRAError()\n                if not resp.ok:\n                    logging.error(\"Something went wrong with download: %s\" % resp.text)\n                    raise JIRAError(resp.text)\n                for block in resp.iter_content(1024):\n                    file.write(block)\n        except JIRAError as je:\n            logging.error('Unable to access remote backup file: %s' % je)\n        except IOError as ioe:\n            logging.error(ioe)\n        return None", "response": "Download backup file from WebDAV."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef current_user(self):\n        if not hasattr(self, '_serverInfo') or 'username' not in self._serverInfo:\n\n            url = self._get_url('serverInfo')\n            r = self._session.get(url, headers=self._options['headers'])\n\n            r_json = json_loads(r)\n            if 'x-ausername' in r.headers:\n                r_json['username'] = r.headers['x-ausername']\n            else:\n                r_json['username'] = None\n            self._serverInfo = r_json\n            # del r_json['self']  # this isn't really an addressable resource\n        return self._serverInfo['username']", "response": "Returns the username of the current user."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_project(self, pid):\n        # allows us to call it with Project objects\n        if hasattr(pid, 'id'):\n            pid = pid.id\n\n        # Check if pid is a number - then we assume that it is\n        # projectID\n        try:\n            str(int(pid)) == pid\n        except Exception as e:\n            # pid looks like a slug, lets verify that\n            r_json = self._get_json('project')\n            for e in r_json:\n                if e['key'] == pid or e['name'] == pid:\n                    pid = e['id']\n                    break\n            else:\n                # pid is not a Project\n                # not a projectID and not a slug - we raise error here\n                raise ValueError('Parameter pid=\"%s\" is not a Project, '\n                                 'projectID or slug' % pid)\n\n        uri = '/rest/api/2/project/%s' % pid\n        url = self._options['server'] + uri\n        try:\n            r = self._session.delete(\n                url, headers={'Content-Type': 'application/json'}\n            )\n        except JIRAError as je:\n            if '403' in str(je):\n                raise JIRAError('Not enough permissions to delete project')\n            if '404' in str(je):\n                raise JIRAError('Project not found in Jira')\n            raise je\n\n        if r.status_code == 204:\n            return True", "response": "Delete a project from JIRA."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_project(self, key, name=None, assignee=None, type=\"Software\", template_name=None):\n        if assignee is None:\n            assignee = self.current_user()\n        if name is None:\n            name = key\n\n        possible_templates = ['Basic', 'JIRA Classic', 'JIRA Default Schemes', 'Basic software development']\n\n        if template_name is not None:\n            possible_templates = [template_name]\n\n        # https://confluence.atlassian.com/jirakb/creating-a-project-via-rest-based-on-jira-default-schemes-744325852.html\n        templates = self.templates()\n        # TODO(ssbarnea): find a better logic to pick a default fallback template\n        template_key = list(templates.values())[0]['projectTemplateModuleCompleteKey']\n        for template_name, template_dic in templates.items():\n            if template_name in possible_templates:\n                template_key = template_dic['projectTemplateModuleCompleteKey']\n                break\n\n        payload = {'name': name,\n                   'key': key,\n                   'keyEdited': 'false',\n                   # 'projectTemplate': 'com.atlassian.jira-core-project-templates:jira-issuetracking',\n                   # 'permissionScheme': '',\n                   'projectTemplateWebItemKey': template_key,\n                   'projectTemplateModuleKey': template_key,\n                   'lead': assignee,\n                   # 'assigneeType': '2',\n                   }\n\n        if self._version[0] > 6:\n            # JIRA versions before 7 will throw an error if we specify type parameter\n            payload['type'] = type\n\n        headers = CaseInsensitiveDict(\n            {'Content-Type': 'application/x-www-form-urlencoded'})\n        url = self._options['server'] + \\\n            '/rest/project-templates/latest/templates'\n\n        r = self._session.post(url, data=payload, headers=headers)\n\n        if r.status_code == 200:\n            r_json = json_loads(r)\n            return r_json\n\n        f = tempfile.NamedTemporaryFile(\n            suffix='.html', prefix='python-jira-error-create-project-', delete=False)\n        f.write(r.text)\n\n        if self.logging:\n            logging.error(\n                \"Unexpected result while running create project. Server response saved in %s for further investigation [HTTP response=%s].\" % (\n                    f.name, r.status_code))\n        return False", "response": "Create a new project based on the specified parameters."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new user in JIRA.", "response": "def add_user(self,\n                 username,\n                 email,\n                 directoryId=1,\n                 password=None,\n                 fullname=None,\n                 notify=False,\n                 active=True,\n                 ignore_existing=False,\n                 application_keys=None,\n                 ):\n        \"\"\"Create a new JIRA user.\n\n        :param username: the username of the new user\n        :type username: str\n        :param email: email address of the new user\n        :type email: str\n        :param directoryId: The directory ID the new user should be a part of (Default: 1)\n        :type directoryId: int\n        :param password: Optional, the password for the new user\n        :type password: Optional[str]\n        :param fullname: Optional, the full name of the new user\n        :type fullname: Optional[str]\n        :param notify: Whether or not to send a notification to the new user. (Default: False)\n        :type notify: bool\n        :param active: Whether or not to make the new user active upon creation. (Default: True)\n        :type active: bool\n        :param ignore_existing: Whether or not to ignore and existing user. (Default: False)\n        :type ignore_existing: bool\n        :param applicationKeys: Keys of products user should have access to\n        :type applicationKeys: Optional[list]\n\n        :return: Whether or not the user creation was successful.\n        :rtype: bool\n\n        :raises JIRAError:  If username already exists and `ignore_existing` has not been set to `True`.\n\n        \"\"\"\n        if not fullname:\n            fullname = username\n        # TODO(ssbarnea): default the directoryID to the first directory in jira instead\n        # of 1 which is the internal one.\n        url = self._options['server'] + '/rest/api/latest/user'\n\n        # implementation based on\n        # https://docs.atlassian.com/jira/REST/ondemand/#d2e5173\n        x = OrderedDict()\n\n        x['displayName'] = fullname\n        x['emailAddress'] = email\n        x['name'] = username\n        if password:\n            x['password'] = password\n        if notify:\n            x['notification'] = 'True'\n        if application_keys is not None:\n            x['applicationKeys'] = application_keys\n\n        payload = json.dumps(x)\n        try:\n            self._session.post(url, data=payload)\n        except JIRAError as e:\n            err = e.response.json()['errors']\n            if 'username' in err and err['username'] == 'A user with that username already exists.' and ignore_existing:\n                return True\n            raise e\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_user_to_group(self, username, group):\n        url = self._options['server'] + '/rest/api/latest/group/user'\n        x = {'groupname': group}\n        y = {'name': username}\n\n        payload = json.dumps(y)\n\n        r = json_loads(self._session.post(url, params=x, data=payload))\n        if 'name' not in r or r['name'] != group:\n            return False\n        else:\n            return r", "response": "Add a user to an existing group."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove a user from a group.", "response": "def remove_user_from_group(self, username, groupname):\n        \"\"\"Remove a user from a group.\n\n        :param username: The user to remove from the group.\n        :param groupname: The group that the user will be removed from.\n        \"\"\"\n        url = self._options['server'] + '/rest/api/latest/group/user'\n        x = {'groupname': groupname,\n             'username': username}\n\n        self._session.delete(url, params=x)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef boards(self, startAt=0, maxResults=50, type=None, name=None, projectKeyOrID=None):\n        params = {}\n        if type:\n            params['type'] = type\n        if name:\n            params['name'] = name\n        if projectKeyOrID:\n            params['projectKeyOrId'] = projectKeyOrID\n\n        if self._options['agile_rest_path'] == GreenHopperResource.GREENHOPPER_REST_PATH:\n            # Old, private API did not support pagination, all records were present in response,\n            #   and no parameters were supported.\n            if startAt or maxResults or params:\n                warnings.warn('Old private GreenHopper API is used, all parameters will be ignored.', Warning)\n\n            r_json = self._get_json('rapidviews/list', base=self.AGILE_BASE_URL)\n            boards = [Board(self._options, self._session, raw_boards_json) for raw_boards_json in r_json['views']]\n            return ResultList(boards, 0, len(boards), len(boards), True)\n        else:\n            return self._fetch_pages(Board, 'values', 'board', startAt, maxResults, params, base=self.AGILE_BASE_URL)", "response": "Get a list of boards."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a list of sprints from a given board.", "response": "def sprints(self, board_id, extended=False, startAt=0, maxResults=50, state=None):\n        \"\"\"Get a list of sprint GreenHopperResources.\n\n        :param board_id: the board to get sprints from\n        :param extended: Used only by old GreenHopper API to fetch additional information like\n            startDate, endDate, completeDate, much slower because it requires an additional requests for each sprint.\n            New JIRA Agile API always returns this information without a need for additional requests.\n        :param startAt: the index of the first sprint to return (0 based)\n        :param maxResults: the maximum number of sprints to return\n        :param state: Filters results to sprints in specified states. Valid values: `future`, `active`, `closed`.\n            You can define multiple states separated by commas\n\n        :type board_id: int\n        :type extended: bool\n        :type startAt: int\n        :type maxResults: int\n        :type state: str\n\n        :rtype: list of :class:`~jira.resources.Sprint`\n        :return: (content depends on API version, but always contains id, name, state, startDate and endDate)\n            When old GreenHopper private API is used, paging is not enabled,\n            and `startAt`, `maxResults` and `state` parameters are ignored.\n        \"\"\"\n        params = {}\n        if state:\n            params['state'] = state\n\n        if self._options['agile_rest_path'] == GreenHopperResource.GREENHOPPER_REST_PATH:\n            r_json = self._get_json('sprintquery/%s?includeHistoricSprints=true&includeFutureSprints=true' % board_id,\n                                    base=self.AGILE_BASE_URL)\n\n            if params:\n                warnings.warn('Old private GreenHopper API is used, parameters %s will be ignored.' % params, Warning)\n\n            if extended:\n                sprints = [Sprint(self._options, self._session, self.sprint_info(None, raw_sprints_json['id']))\n                           for raw_sprints_json in r_json['sprints']]\n            else:\n                sprints = [Sprint(self._options, self._session, raw_sprints_json)\n                           for raw_sprints_json in r_json['sprints']]\n\n            return ResultList(sprints, 0, len(sprints), len(sprints), True)\n        else:\n            return self._fetch_pages(Sprint, 'values', 'board/%s/sprint' % board_id, startAt, maxResults, params,\n                                     self.AGILE_BASE_URL)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef incompletedIssuesEstimateSum(self, board_id, sprint_id):\n        return self._get_json('rapid/charts/sprintreport?rapidViewId=%s&sprintId=%s' % (board_id, sprint_id),\n                              base=self.AGILE_BASE_URL)['contents']['incompletedIssuesEstimateSum']['value']", "response": "Return the total incompleted points this sprint."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef removed_issues(self, board_id, sprint_id):\n        r_json = self._get_json('rapid/charts/sprintreport?rapidViewId=%s&sprintId=%s' % (board_id, sprint_id),\n                                base=self.AGILE_BASE_URL)\n        issues = [Issue(self._options, self._session, raw_issues_json) for raw_issues_json in\n                  r_json['contents']['puntedIssues']]\n\n        return issues", "response": "Return the completed issues for the sprint."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the total incompleted points this sprint.", "response": "def removedIssuesEstimateSum(self, board_id, sprint_id):\n        \"\"\"Return the total incompleted points this sprint.\"\"\"\n        return self._get_json('rapid/charts/sprintreport?rapidViewId=%s&sprintId=%s' % (board_id, sprint_id),\n                              base=self.AGILE_BASE_URL)['contents']['puntedIssuesEstimateSum']['value']"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the information about a sprint.", "response": "def sprint_info(self, board_id, sprint_id):\n        \"\"\"Return the information about a sprint.\n\n        :param board_id: the board retrieving issues from. Deprecated and ignored.\n        :param sprint_id: the sprint retrieving issues from\n        \"\"\"\n        sprint = Sprint(self._options, self._session)\n        sprint.find(sprint_id)\n        return sprint.raw"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the information about a sprint.", "response": "def sprint(self, id):\n        \"\"\"Return the information about a sprint.\n\n        :param sprint_id: the sprint retrieving issues from\n\n        :type sprint_id: int\n\n        :rtype: :class:`~jira.resources.Sprint`\n        \"\"\"\n        sprint = Sprint(self._options, self._session)\n        sprint.find(id)\n        return sprint"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_board(self, id):\n        board = Board(self._options, self._session, raw={'id': id})\n        board.delete()", "response": "Delete an agile board."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_board(self, name, project_ids, preset=\"scrum\",\n                     location_type='user', location_id=None):\n        \"\"\"Create a new board for the ``project_ids``.\n\n        :param name: name of the board\n        :type name: str\n        :param project_ids: the projects to create the board in\n        :type project_ids: str\n        :param preset: What preset to use for this board. (Default: \"scrum\")\n        :type preset: 'kanban', 'scrum', 'diy'\n        :param location_type: the location type. Available in cloud. (Default: \"user\")\n        :type location_type: 'user', 'project'\n        :param location_id: the id of project that the board should be\n            located under. Omit this for a 'user' location_type. Available in cloud.\n        :type location_id: Optional[str]\n\n        :return: The newly created board\n        :rtype: Board\n        \"\"\"\n        if self._options['agile_rest_path'] != GreenHopperResource.GREENHOPPER_REST_PATH:\n            raise NotImplementedError('JIRA Agile Public API does not support this request')\n\n        payload = {}\n        if isinstance(project_ids, string_types):\n            ids = []\n            for p in project_ids.split(','):\n                ids.append(self.project(p).id)\n            project_ids = ','.join(ids)\n        if location_id is not None:\n            location_id = self.project(location_id).id\n        payload['name'] = name\n        if isinstance(project_ids, string_types):\n            project_ids = project_ids.split(',')\n        payload['projectIds'] = project_ids\n        payload['preset'] = preset\n        if self.deploymentType == 'Cloud':\n            payload['locationType'] = location_type\n            payload['locationId'] = location_id\n        url = self._get_url(\n            'rapidview/create/presets', base=self.AGILE_BASE_URL)\n        r = self._session.post(\n            url, data=json.dumps(payload))\n\n        raw_issue_json = json_loads(r)\n        return Board(self._options, self._session, raw=raw_issue_json)", "response": "Creates a new board for the given project_ids."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_sprint(self, name, board_id, startDate=None, endDate=None):\n        payload = {'name': name}\n        if startDate:\n            payload[\"startDate\"] = startDate\n        if endDate:\n            payload[\"endDate\"] = endDate\n\n        if self._options['agile_rest_path'] == GreenHopperResource.GREENHOPPER_REST_PATH:\n            url = self._get_url('sprint/%s' % board_id, base=self.AGILE_BASE_URL)\n            r = self._session.post(url)\n            raw_issue_json = json_loads(r)\n            \"\"\" now r contains something like:\n            {\n                  \"id\": 742,\n                  \"name\": \"Sprint 89\",\n                  \"state\": \"FUTURE\",\n                  \"linkedPagesCount\": 0,\n                  \"startDate\": \"None\",\n                  \"endDate\": \"None\",\n                  \"completeDate\": \"None\",\n                  \"remoteLinks\": []\n            }\"\"\"\n\n            url = self._get_url(\n                'sprint/%s' % raw_issue_json['id'], base=self.AGILE_BASE_URL)\n            r = self._session.put(\n                url, data=json.dumps(payload))\n            raw_issue_json = json_loads(r)\n        else:\n            url = self._get_url('sprint', base=self.AGILE_BASE_URL)\n            payload['originBoardId'] = board_id\n            r = self._session.post(url, data=json.dumps(payload))\n            raw_issue_json = json_loads(r)\n\n        return Sprint(self._options, self._session, raw=raw_issue_json)", "response": "Creates a new sprint for the given board."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_issues_to_sprint(self, sprint_id, issue_keys):\n        if self._options['agile_rest_path'] == GreenHopperResource.AGILE_BASE_REST_PATH:\n            url = self._get_url('sprint/%s/issue' % sprint_id, base=self.AGILE_BASE_URL)\n            payload = {'issues': issue_keys}\n            try:\n                self._session.post(url, data=json.dumps(payload))\n            except JIRAError as e:\n                if e.status_code == 404:\n                    warnings.warn('Status code 404 may mean, that too old JIRA Agile version is installed.'\n                                  ' At least version 6.7.10 is required.')\n                raise\n        elif self._options['agile_rest_path'] == GreenHopperResource.GREENHOPPER_REST_PATH:\n            # In old, private API the function does not exist anymore and we need to use\n            # issue.update() to perform this operation\n            # Workaround based on https://answers.atlassian.com/questions/277651/jira-agile-rest-api-example\n\n            sprint_field_id = self._get_sprint_field_id()\n\n            data = {'idOrKeys': issue_keys, 'customFieldId': sprint_field_id,\n                    'sprintId': sprint_id, 'addToBacklog': False}\n            url = self._get_url('sprint/rank', base=self.AGILE_BASE_URL)\n            return self._session.put(url, data=json.dumps(data))\n        else:\n            raise NotImplementedError('No API for adding issues to sprint for agile_rest_path=\"%s\"' %\n                                      self._options['agile_rest_path'])", "response": "Adds the issues in issue_keys to the sprint."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd the issues in issue_keys to the epic_id.", "response": "def add_issues_to_epic(self, epic_id, issue_keys, ignore_epics=True):\n        \"\"\"Add the issues in ``issue_keys`` to the ``epic_id``.\n\n        :param epic_id: The ID for the epic where issues should be added.\n        :type epic_id: int\n        :param issue_keys: The issues to add to the epic\n        :type issue_keys: str\n        :param ignore_epics: ignore any issues listed in ``issue_keys`` that are epics. (Default: True)\n        :type ignore_epics: bool\n\n        \"\"\"\n        if self._options['agile_rest_path'] != GreenHopperResource.GREENHOPPER_REST_PATH:\n            # TODO(ssbarnea): simulate functionality using issue.update()?\n            raise NotImplementedError('JIRA Agile Public API does not support this request')\n\n        data = {}\n        data['issueKeys'] = issue_keys\n        data['ignoreEpics'] = ignore_epics\n        url = self._get_url('epics/%s/add' %\n                            epic_id, base=self.AGILE_BASE_URL)\n        return self._session.put(\n            url, data=json.dumps(data))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rank(self, issue, next_issue):\n        if not self._rank:\n            for field in self.fields():\n                if field['name'] == 'Rank':\n                    if field['schema']['custom'] == \"com.pyxis.greenhopper.jira:gh-lexo-rank\":\n                        self._rank = field['schema']['customId']\n                        break\n                    elif field['schema']['custom'] == \"com.pyxis.greenhopper.jira:gh-global-rank\":\n                        # Obsolete since JIRA v6.3.13.1\n                        self._rank = field['schema']['customId']\n\n        if self._options['agile_rest_path'] == GreenHopperResource.AGILE_BASE_REST_PATH:\n            url = self._get_url('issue/rank', base=self.AGILE_BASE_URL)\n            payload = {'issues': [issue], 'rankBeforeIssue': next_issue, 'rankCustomFieldId': self._rank}\n            try:\n                return self._session.put(url, data=json.dumps(payload))\n            except JIRAError as e:\n                if e.status_code == 404:\n                    warnings.warn('Status code 404 may mean, that too old JIRA Agile version is installed.'\n                                  ' At least version 6.7.10 is required.')\n                raise\n        elif self._options['agile_rest_path'] == GreenHopperResource.GREENHOPPER_REST_PATH:\n            data = {\n                \"issueKeys\": [issue], \"rankBeforeKey\": next_issue, \"customFieldId\": self._rank}\n            url = self._get_url('rank', base=self.AGILE_BASE_URL)\n            return self._session.put(url, data=json.dumps(data))\n        else:\n            raise NotImplementedError('No API for ranking issues for agile_rest_path=\"%s\"' %\n                                      self._options['agile_rest_path'])", "response": "Rank an issue before another using the default Ranking field."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dict2resource(raw, top=None, options=None, session=None):\n    if top is None:\n        top = PropertyHolder(raw)\n\n    seqs = tuple, list, set, frozenset\n    for i, j in iteritems(raw):\n        if isinstance(j, dict):\n            if 'self' in j:\n                resource = cls_for_resource(j['self'])(options, session, j)\n                setattr(top, i, resource)\n            elif i == 'timetracking':\n                setattr(top, 'timetracking', TimeTracking(options, session, j))\n            else:\n                setattr(\n                    top, i, dict2resource(j, options=options, session=session))\n        elif isinstance(j, seqs):\n            seq_list = []\n            for seq_elem in j:\n                if isinstance(seq_elem, dict):\n                    if 'self' in seq_elem:\n                        resource = cls_for_resource(seq_elem['self'])(\n                            options, session, seq_elem)\n                        seq_list.append(resource)\n                    else:\n                        seq_list.append(\n                            dict2resource(seq_elem, options=options, session=session))\n                else:\n                    seq_list.append(seq_elem)\n            setattr(top, i, seq_list)\n        else:\n            setattr(top, i, j)\n    return top", "response": "Convert a dictionary into a Jira Resource object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find(self,\n             id,\n             params=None,\n             ):\n        \"\"\"Finds a resource based on the input parameters.\n\n        :type id: Union[Tuple[str, str], int, str]\n        :type params: Optional[Dict[str, str]]\n\n        \"\"\"\n\n        if params is None:\n            params = {}\n\n        if isinstance(id, tuple):\n            path = self._resource.format(*id)\n        else:\n            path = self._resource.format(id)\n        url = self._get_url(path)\n        self._load(url, params=params)", "response": "Finds a resource based on the input parameters."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_url(self, path):\n        options = self._options.copy()\n        options.update({'path': path})\n        return self._base_url.format(**options)", "response": "Gets the url for the specified path."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the resource s attributes.", "response": "def update(self, fields=None, async_=None, jira=None, notify=True, **kwargs):\n        \"\"\"Update this resource on the server.\n\n        Keyword arguments are marshalled into a dict before being sent. If this\n        resource doesn't support ``PUT``, a :py:exc:`.JIRAError` will be raised; subclasses that specialize this method\n        will only raise errors in case of user error.\n\n        :param fields: Fields which should be updated for the object.\n        :type fields: Optional[Dict[str, Any]]\n        :param async_: If true the request will be added to the queue so it can be executed later using async_run()\n        :type async_: bool\n        :param jira: Instance of JIRA Client\n        :type jira: jira.JIRA\n        :param notify: Whether or not to notify users about the update. (Default: True)\n        :type notify: bool\n        :type kwargs: **Any\n        \"\"\"\n        if async_ is None:\n            async_ = self._options['async']\n\n        data = {}\n        if fields is not None:\n            data.update(fields)\n        data.update(kwargs)\n\n        data = json.dumps(data)\n\n        if not notify:\n            querystring = \"?notifyUsers=false\"\n        else:\n            querystring = \"\"\n\n        r = self._session.put(\n            self.self + querystring, data=data)\n        if 'autofix' in self._options and \\\n                r.status_code == 400:\n            user = None\n            error_list = get_error_list(r)\n            logging.error(error_list)\n            if \"The reporter specified is not a user.\" in error_list:\n                if 'reporter' not in data['fields']:\n                    logging.warning(\n                        \"autofix: setting reporter to '%s' and retrying the update.\" % self._options['autofix'])\n                    data['fields']['reporter'] = {\n                        'name': self._options['autofix']}\n\n            if \"Issues must be assigned.\" in error_list:\n                if 'assignee' not in data['fields']:\n                    logging.warning(\"autofix: setting assignee to '%s' for %s and retrying the update.\" % (\n                        self._options['autofix'], self.key))\n                    data['fields']['assignee'] = {\n                        'name': self._options['autofix']}\n                    # for some reason the above approach fails on Jira 5.2.11\n                    # so we need to change the assignee before\n\n            if \"Issue type is a sub-task but parent issue key or id not specified.\" in error_list:\n                logging.warning(\n                    \"autofix: trying to fix sub-task without parent by converting to it to bug\")\n                data['fields']['issuetype'] = {\"name\": \"Bug\"}\n            if \"The summary is invalid because it contains newline characters.\" in error_list:\n                logging.warning(\"autofix: trying to fix newline in summary\")\n                data['fields'][\n                    'summary'] = self.fields.summary.replace(\"/n\", \"\")\n            for error in error_list:\n                if re.search(r\"^User '(.*)' was not found in the system\\.\", error, re.U):\n                    m = re.search(\n                        r\"^User '(.*)' was not found in the system\\.\", error, re.U)\n                    if m:\n                        user = m.groups()[0]\n                    else:\n                        raise NotImplementedError()\n                if re.search(r\"^User '(.*)' does not exist\\.\", error):\n                    m = re.search(r\"^User '(.*)' does not exist\\.\", error)\n                    if m:\n                        user = m.groups()[0]\n                    else:\n                        raise NotImplementedError()\n\n            if user:\n                logging.warning(\n                    \"Trying to add missing orphan user '%s' in order to complete the previous failed operation.\" % user)\n                jira.add_user(user, 'noreply@example.com', 10100, active=False)\n                # if 'assignee' not in data['fields']:\n                #    logging.warning(\"autofix: setting assignee to '%s' and retrying the update.\" % self._options['autofix'])\n                #    data['fields']['assignee'] = {'name': self._options['autofix']}\n            # EXPERIMENTAL --->\n            if async_:\n                if not hasattr(self._session, '_async_jobs'):\n                    self._session._async_jobs = set()\n                self._session._async_jobs.add(threaded_requests.put(\n                    self.self, data=json.dumps(data)))\n            else:\n                r = self._session.put(\n                    self.self, data=json.dumps(data))\n\n        time.sleep(self._options['delay_reload'])\n        self._load(self.self)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete this resource from the server passing the specified query parameters.", "response": "def delete(self, params=None):\n        \"\"\"Delete this resource from the server, passing the specified query parameters.\n\n        If this resource doesn't support ``DELETE``, a :py:exc:`.JIRAError`\n        will be raised; subclasses that specialize this method will only raise errors\n        in case of user error.\n\n        :param params: Parameters for the delete request.\n        :type params: Optional[Dict[str, Any]]\n\n        :rtype: Response\n        \"\"\"\n        if self._options['async']:\n            if not hasattr(self._session, '_async_jobs'):\n                self._session._async_jobs = set()\n            self._session._async_jobs.add(\n                threaded_requests.delete(url=self.self, params=params))\n        else:\n            return self._session.delete(url=self.self, params=params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _load(self,\n              url,\n              headers=CaseInsensitiveDict(),\n              params=None,\n              path=None,\n              ):\n        \"\"\" Load a resource.\n\n        :type url: str\n        :type headers: CaseInsensitiveDict\n        :type params: Optional[Dict[str,str]]\n        :type path: Optional[str]\n\n        \"\"\"\n        r = self._session.get(url, headers=headers, params=params)\n        try:\n            j = json_loads(r)\n        except ValueError as e:\n            logging.error(\"%s:\\n%s\" % (e, r.text))\n            raise e\n        if path:\n            j = j[path]\n        self._parse_raw(j)", "response": "Load a resource from the server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_raw(self, raw):\n        self.raw = raw\n        if not raw:\n            raise NotImplementedError(\"We cannot instantiate empty resources: %s\" % raw)\n        dict2resource(raw, self, self._options, self._session)", "response": "Parse a raw dictionary to create a resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self):\n        r = self._session.get(self.content, headers={'Accept': '*/*'})\n        return r.content", "response": "Return the file content as a string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef iter_content(self, chunk_size=1024):\n        r = self._session.get(self.content, stream=True)\n        return r.iter_content(chunk_size)", "response": "Return the file content as an iterable stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes this component from the server.", "response": "def delete(self, moveIssuesTo=None):\n        \"\"\"Delete this component from the server.\n\n        :param moveIssuesTo: the name of the component to which to move any issues this component is applied\n        \"\"\"\n        params = {}\n        if moveIssuesTo is not None:\n            params['moveIssuesTo'] = moveIssuesTo\n\n        super(Component, self).delete(params)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, fields=None, update=None, async_=None, jira=None, notify=True, **fieldargs):\n        data = {}\n        if fields is not None:\n            fields_dict = fields\n        else:\n            fields_dict = {}\n        data['fields'] = fields_dict\n        if update is not None:\n            update_dict = update\n        else:\n            update_dict = {}\n        data['update'] = update_dict\n        for field in sorted(fieldargs.keys()):\n            value = fieldargs[field]\n            # apply some heuristics to make certain changes easier\n            if isinstance(value, string_types):\n                if field == 'assignee' or field == 'reporter':\n                    fields_dict['assignee'] = {'name': value}\n                elif field == 'comment':\n                    if 'comment' not in update_dict:\n                        update_dict['comment'] = []\n                    update_dict['comment'].append({\n                        'add': {'body': value}})\n                else:\n                    fields_dict[field] = value\n            elif isinstance(value, list):\n                if field not in update_dict:\n                    update_dict[field] = []\n                update_dict[field].extend(value)\n            else:\n                fields_dict[field] = value\n\n        super(Issue, self).update(async_=async_, jira=jira, notify=notify, fields=data)", "response": "Update this issue on the server."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_field_value(self, field, value):\n        super(Issue, self).update(fields={\"update\": {field: [{\"add\": value}]}})", "response": "Add a value to a field that supports multiple values without resetting the existing values."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes this issue from the server.", "response": "def delete(self, deleteSubtasks=False):\n        \"\"\"Delete this issue from the server.\n\n        :param deleteSubtasks: if the issue has subtasks, this argument must be set to true for the call to succeed.\n\n        :type deleteSubtasks: bool\n        \"\"\"\n        super(Issue, self).delete(params={'deleteSubtasks': deleteSubtasks})"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(self, object, globalId=None, application=None, relationship=None):\n        data = {\n            'object': object}\n        if globalId is not None:\n            data['globalId'] = globalId\n        if application is not None:\n            data['application'] = application\n        if relationship is not None:\n            data['relationship'] = relationship\n\n        super(RemoteLink, self).update(**data)", "response": "Update a RemoteLink. object is required."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete(self, adjustEstimate=None, newEstimate=None, increaseBy=None):\n        params = {}\n        if adjustEstimate is not None:\n            params['adjustEstimate'] = adjustEstimate\n        if newEstimate is not None:\n            params['newEstimate'] = newEstimate\n        if increaseBy is not None:\n            params['increaseBy'] = increaseBy\n\n        super(Worklog, self).delete(params)", "response": "Delete this worklog entry from its associated issue."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(self, users=None, groups=None):\n\n        if users is not None and isinstance(users, string_types):\n            users = (users,)\n        if groups is not None and isinstance(groups, string_types):\n            groups = (groups,)\n\n        data = {\n            'id': self.id,\n            'categorisedActors': {\n                'atlassian-user-role-actor': users,\n                'atlassian-group-role-actor': groups}}\n\n        super(Role, self).update(**data)", "response": "Update the role with the specified users or groups."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_user(self, users=None, groups=None):\n\n        if users is not None and isinstance(users, string_types):\n            users = (users,)\n        if groups is not None and isinstance(groups, string_types):\n            groups = (groups,)\n\n        data = {\n            'user': users}\n        self._session.post(self.self, data=json.dumps(data))", "response": "Add the specified users or groups to this project role."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes this project version from the server.", "response": "def delete(self, moveFixIssuesTo=None, moveAffectedIssuesTo=None):\n        \"\"\"Delete this project version from the server.\n\n        If neither of the arguments are specified, the version is\n            removed from all issues it is attached to.\n\n        :param moveFixIssuesTo: in issues for which this version is a fix\n            version, add this argument version to the fix version list\n        :param moveAffectedIssuesTo: in issues for which this version is an\n            affected version, add this argument version to the affected version list\n        \"\"\"\n\n        params = {}\n        if moveFixIssuesTo is not None:\n            params['moveFixIssuesTo'] = moveFixIssuesTo\n        if moveAffectedIssuesTo is not None:\n            params['moveAffectedIssuesTo'] = moveAffectedIssuesTo\n\n        return super(Version, self).delete(params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update(self, **args):\n        data = {}\n        for field in args:\n            data[field] = args[field]\n\n        super(Version, self).update(**data)", "response": "Update this project version from the server. It is prior used to archive versions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a JIRA object by loading the connection details from the config. ini file.", "response": "def get_jira(profile=None, url=\"http://localhost:2990\", username=\"admin\", password=\"admin\", appid=None, autofix=False, verify=True):\n    \"\"\"Return a JIRA object by loading the connection details from the `config.ini` file.\n\n    :param profile: The name of the section from config.ini file that stores server config url/username/password\n    :param url: URL of the Jira server\n    :param username: username to use for authentication\n    :param password: password to use for authentication\n    :param verify: boolean indicating whether SSL certificates should be verified\n    :return: JIRA -- an instance to a JIRA object.\n    :raises: EnvironmentError\n\n    Usage:\n\n        >>> from jira.config import get_jira\n        >>>\n        >>> jira = get_jira(profile='jira')\n\n    Also create a `config.ini` like this and put it in current directory, user home directory or PYTHONPATH.\n\n    .. code-block:: none\n\n        [jira]\n        url=https://jira.atlassian.com\n        # only the `url` is mandatory\n        user=...\n        pass=...\n        appid=...\n        verify=...\n\n    \"\"\"\n    def findfile(path):\n        \"\"\"Find the file named path in the sys.path.\n\n        Returns the full path name if found, None if not found\n        \"\"\"\n        paths = ['.', os.path.expanduser('~')]\n        paths.extend(sys.path)\n        for dirname in paths:\n            possible = os.path.abspath(os.path.join(dirname, path))\n            if os.path.isfile(possible):\n                return possible\n        return None\n    config = configparser.ConfigParser(defaults={'user': None, 'pass': None, 'appid': appid, 'autofix': autofix,\n                                                 'verify': 'yes' if verify else 'no'}, allow_no_value=True)\n\n    config_file = findfile('config.ini')\n    if config_file:\n        logging.debug(\"Found %s config file\" % config_file)\n\n    if not profile:\n        if config_file:\n            config.read(config_file)\n            try:\n                profile = config.get('general', 'default-jira-profile')\n            except configparser.NoOptionError:\n                pass\n\n    if profile:\n        if config_file:\n            config.read(config_file)\n            url = config.get(profile, 'url')\n            username = config.get(profile, 'user')\n            password = config.get(profile, 'pass')\n            appid = config.get(profile, 'appid')\n            autofix = config.get(profile, 'autofix')\n            verify = config.getboolean(profile, 'verify')\n\n        else:\n            raise EnvironmentError(\n                \"%s was not able to locate the config.ini file in current directory, user home directory or PYTHONPATH.\" % __name__)\n\n    options = JIRA.DEFAULT_OPTIONS\n    options['server'] = url\n    options['autofix'] = autofix\n    options['appid'] = appid\n    options['verify'] = verify\n\n    return JIRA(options=options, basic_auth=(username, password))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload a set of attributes from a Python file.", "response": "def from_pyfile(self, filename):\n        \"\"\"\n        \u5728\u4e00\u4e2a Python \u6587\u4ef6\u4e2d\u8bfb\u53d6\u914d\u7f6e\u3002\n\n        :param filename: \u914d\u7f6e\u6587\u4ef6\u7684\u6587\u4ef6\u540d\n        :return: \u5982\u679c\u8bfb\u53d6\u6210\u529f\uff0c\u8fd4\u56de ``True``\uff0c\u5982\u679c\u5931\u8d25\uff0c\u4f1a\u629b\u51fa\u9519\u8bef\u5f02\u5e38\n        \"\"\"\n        d = types.ModuleType('config')\n        d.__file__ = filename\n        with open(filename) as config_file:\n            exec(compile(config_file.read(), filename, 'exec'), d.__dict__)\n        self.from_object(d)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a handler that can be used to handle a single message.", "response": "def make_handler(robot):\n    \"\"\"\n    \u4e3a\u4e00\u4e2a BaseRoBot \u751f\u6210 Tornado Handler\u3002\n\n    Usage ::\n\n        import tornado.ioloop\n        import tornado.web\n        from werobot import WeRoBot\n        from tornado_werobot import make_handler\n\n        robot = WeRoBot(token='token')\n\n\n        @robot.handler\n        def hello(message):\n            return 'Hello World!'\n\n        application = tornado.web.Application([\n            (r\"/\", make_handler(robot)),\n        ])\n\n    :param robot: \u4e00\u4e2a BaseRoBot \u5b9e\u4f8b\u3002\n    :return: \u4e00\u4e2a\u6807\u51c6\u7684 Tornado Handler\n    \"\"\"\n\n    class WeRoBotHandler(RequestHandler):\n        def prepare(self):\n            timestamp = self.get_argument('timestamp', '')\n            nonce = self.get_argument('nonce', '')\n            signature = self.get_argument('signature', '')\n\n            if not robot.check_signature(\n                timestamp=timestamp, nonce=nonce, signature=signature\n            ):\n                self.set_status(403)\n                self.write(\n                    robot.make_error_page(\n                        html.escape(\n                            self.request.protocol + \"://\" + self.request.host +\n                            self.request.uri\n                        )\n                    )\n                )\n                return\n\n        def get(self):\n            echostr = self.get_argument('echostr', '')\n            self.write(echostr)\n\n        def post(self):\n            timestamp = self.get_argument('timestamp', '')\n            nonce = self.get_argument('nonce', '')\n            msg_signature = self.get_argument('msg_signature', '')\n            message = robot.parse_message(\n                self.request.body,\n                timestamp=timestamp,\n                nonce=nonce,\n                msg_signature=msg_signature\n            )\n            self.set_header(\"Content-Type\", \"application/xml;charset=utf-8\")\n            self.write(robot.get_encrypted_reply(message))\n\n    return WeRoBotHandler"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, id):\n        document = self._get_document(id)\n        if document:\n            session_json = document[\"session\"]\n            return json_loads(session_json)\n        return {}", "response": "get a session by id"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set(self, id, value):\n        session = json_dumps(value)\n        self.collection.replace_one(\n            {\n                \"wechat_id\": id\n            }, {\n                \"wechat_id\": id,\n                \"session\": session\n            },\n            upsert=True\n        )", "response": "set a new value for the wechat_id"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngrant a token to the user.", "response": "def grant_token(self):\n        \"\"\"\n        \u83b7\u53d6 Access Token\u3002\n\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        return self.get(\n            url=\"https://api.weixin.qq.com/cgi-bin/token\",\n            params={\n                \"grant_type\": \"client_credential\",\n                \"appid\": self.appid,\n                \"secret\": self.appsecret\n            }\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_access_token(self):\n        if self._token:\n            now = time.time()\n            if self.token_expires_at - now > 60:\n                return self._token\n        json = self.grant_token()\n        self._token = json[\"access_token\"]\n        self.token_expires_at = int(time.time()) + json[\"expires_in\"]\n        return self._token", "response": "Get the access token"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_custom_menu(self, menu_data, matchrule):\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/menu/addconditional\",\n            data={\n                \"button\": menu_data,\n                \"matchrule\": matchrule\n            }\n        )", "response": "\u521b\u5efa\u4e2a\u6027\u5316\u83dc\u5355::\n\n            button = [\n                {\n                    \"type\":\"click\",\n                    \"name\":\"\u4eca\u65e5\u6b4c\u66f2\",\n                    \"key\":\"V1001_TODAY_MUSIC\"\n                },\n                {\n                    \"name\":\"\u83dc\u5355\",\n                    \"sub_button\":[\n                    {\n                        \"type\":\"view\",\n                        \"name\":\"\u641c\u7d22\",\n                        \"url\":\"http://www.soso.com/\"\n                    },\n                    {\n                        \"type\":\"view\",\n                        \"name\":\"\u89c6\u9891\",\n                        \"url\":\"http://v.qq.com/\"\n                    },\n                    {\n                        \"type\":\"click\",\n                        \"name\":\"\u8d5e\u4e00\u4e0b\u6211\u4eec\",\n                        \"key\":\"V1001_GOOD\"\n                    }]\n             }]\n             matchrule = {\n                \"group_id\":\"2\",\n                \"sex\":\"1\",\n                \"country\":\"\u4e2d\u56fd\",\n                \"province\":\"\u5e7f\u4e1c\",\n                \"city\":\"\u5e7f\u5dde\",\n                \"client_platform_type\":\"2\",\n                \"language\":\"zh_CN\"\n            }\n            client.create_custom_menu(button, matchrule)\n\n        :param menu_data: \u5982\u4e0a\u6240\u793a\u7684 Python \u5b57\u5178\n        :param matchrule: \u5982\u4e0a\u6240\u793a\u7684\u5339\u914d\u89c4\u5219\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a custom service account.", "response": "def add_custom_service_account(self, account, nickname, password):\n        \"\"\"\n        \u6dfb\u52a0\u5ba2\u670d\u5e10\u53f7\u3002\n\n        :param account: \u5ba2\u670d\u8d26\u53f7\u7684\u7528\u6237\u540d\n        :param nickname: \u5ba2\u670d\u8d26\u53f7\u7684\u6635\u79f0\n        :param password: \u5ba2\u670d\u8d26\u53f7\u7684\u5bc6\u7801\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        return self.post(\n            url=\"https://api.weixin.qq.com/customservice/kfaccount/add\",\n            data={\n                \"kf_account\": account,\n                \"nickname\": nickname,\n                \"password\": password\n            }\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_custom_service_account(self, account, nickname, password):\n        return self.post(\n            url=\"https://api.weixin.qq.com/customservice/kfaccount/update\",\n            data={\n                \"kf_account\": account,\n                \"nickname\": nickname,\n                \"password\": password\n            }\n        )", "response": "Update the custom service account."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_custom_service_account(self, account, nickname, password):\n        return self.post(\n            url=\"https://api.weixin.qq.com/customservice/kfaccount/del\",\n            data={\n                \"kf_account\": account,\n                \"nickname\": nickname,\n                \"password\": password\n            }\n        )", "response": "Delete a custom service account."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef upload_custom_service_account_avatar(self, account, avatar):\n        return self.post(\n            url=\n            \"http://api.weixin.qq.com/customservice/kfaccount/uploadheadimg\",\n            params={\n                \"access_token\": self.token,\n                \"kf_account\": account\n            },\n            files={\"media\": avatar}\n        )", "response": "Upload a custom service account avatar"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef download_media(self, media_id):\n        return requests.get(\n            url=\"https://api.weixin.qq.com/cgi-bin/media/get\",\n            params={\n                \"access_token\": self.token,\n                \"media_id\": media_id\n            }\n        )", "response": "Download a media from the API."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef upload_news_picture(self, file):\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/media/uploadimg\",\n            params={\"access_token\": self.token},\n            files={\"media\": file}\n        )", "response": "Upload a news picture to the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuploads permanent media to the KE - MEAN.", "response": "def upload_permanent_media(self, media_type, media_file):\n        \"\"\"\n        \u4e0a\u4f20\u5176\u4ed6\u7c7b\u578b\u6c38\u4e45\u7d20\u6750\u3002\n\n        :param media_type: \u5a92\u4f53\u6587\u4ef6\u7c7b\u578b\uff0c\u5206\u522b\u6709\u56fe\u7247\uff08image\uff09\u3001\u8bed\u97f3\uff08voice\uff09\u548c\u7f29\u7565\u56fe\uff08thumb\uff09\n        :param media_file: \u8981\u4e0a\u4f20\u7684\u6587\u4ef6\uff0c\u4e00\u4e2a File-object\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/material/add_material\",\n            params={\n                \"access_token\": self.token,\n                \"type\": media_type\n            },\n            files={\"media\": media_file}\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuploads a permanent video to the Keen - Music API.", "response": "def upload_permanent_video(self, title, introduction, video):\n        \"\"\"\n        \u4e0a\u4f20\u6c38\u4e45\u89c6\u9891\u3002\n\n        :param title: \u89c6\u9891\u7d20\u6750\u7684\u6807\u9898\n        :param introduction: \u89c6\u9891\u7d20\u6750\u7684\u63cf\u8ff0\n        :param video: \u8981\u4e0a\u4f20\u7684\u89c6\u9891\uff0c\u4e00\u4e2a File-object\n        :return: requests \u7684 Response \u5b9e\u4f8b\n        \"\"\"\n        return requests.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/material/add_material\",\n            params={\n                \"access_token\": self.token,\n                \"type\": \"video\"\n            },\n            data={\n                \"description\": _json.dumps(\n                    {\n                        \"title\": title,\n                        \"introduction\": introduction\n                    },\n                    ensure_ascii=False\n                ).encode(\"utf-8\")\n            },\n            files={\"media\": video}\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef download_permanent_media(self, media_id):\n        return requests.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/material/get_material\",\n            params={\"access_token\": self.token},\n            data=_json.dumps({\n                \"media_id\": media_id\n            }, ensure_ascii=False).encode(\"utf-8\")\n        )", "response": "Downloads permanent media from the API."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a list of media from the API.", "response": "def get_media_list(self, media_type, offset, count):\n        \"\"\"\n        \u83b7\u53d6\u7d20\u6750\u5217\u8868\u3002\n\n        :param media_type: \u7d20\u6750\u7684\u7c7b\u578b\uff0c\u56fe\u7247\uff08image\uff09\u3001\u89c6\u9891\uff08video\uff09\u3001\u8bed\u97f3 \uff08voice\uff09\u3001\u56fe\u6587\uff08news\uff09\n        :param offset: \u4ece\u5168\u90e8\u7d20\u6750\u7684\u8be5\u504f\u79fb\u4f4d\u7f6e\u5f00\u59cb\u8fd4\u56de\uff0c0\u8868\u793a\u4ece\u7b2c\u4e00\u4e2a\u7d20\u6750\u8fd4\u56de\n        :param count: \u8fd4\u56de\u7d20\u6750\u7684\u6570\u91cf\uff0c\u53d6\u503c\u57281\u523020\u4e4b\u95f4\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/material/batchget_material\",\n            data={\n                \"type\": media_type,\n                \"offset\": offset,\n                \"count\": count\n            }\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the group name and ID", "response": "def update_group(self, group_id, name):\n        \"\"\"\n        \u4fee\u6539\u5206\u7ec4\u540d\u3002\n\n        :param group_id: \u5206\u7ec4 ID\uff0c\u7531\u5fae\u4fe1\u5206\u914d\n        :param name: \u5206\u7ec4\u540d\u5b57\uff0830\u4e2a\u5b57\u7b26\u4ee5\u5185\uff09\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/groups/update\",\n            data={\"group\": {\n                \"id\": int(group_id),\n                \"name\": to_text(name)\n            }}\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef move_users(self, user_id_list, group_id):\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/groups/members/batchupdate\",\n            data={\n                \"openid_list\": user_id_list,\n                \"to_groupid\": group_id\n            }\n        )", "response": "Move users to a group"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate user s remark", "response": "def remark_user(self, user_id, remark):\n        \"\"\"\n        \u8bbe\u7f6e\u5907\u6ce8\u540d\u3002\n\n        :param user_id: \u8bbe\u7f6e\u5907\u6ce8\u540d\u7684\u7528\u6237 ID\n        :param remark: \u65b0\u7684\u5907\u6ce8\u540d\uff0c\u957f\u5ea6\u5fc5\u987b\u5c0f\u4e8e30\u5b57\u7b26\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/user/info/updateremark\",\n            data={\n                \"openid\": user_id,\n                \"remark\": remark\n            }\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_user_info(self, user_id, lang=\"zh_CN\"):\n        return self.get(\n            url=\"https://api.weixin.qq.com/cgi-bin/user/info\",\n            params={\n                \"access_token\": self.token,\n                \"openid\": user_id,\n                \"lang\": lang\n            }\n        )", "response": "\u83b7\u53d6\u7528\u6237\u57fa\u672c\u4fe1\u606f\u3002\n\n        :param user_id: \u7528\u6237 ID \u3002 \u5c31\u662f\u4f60\u6536\u5230\u7684 `Message` \u7684 source\n        :param lang: \u8fd4\u56de\u56fd\u5bb6\u5730\u533a\u8bed\u8a00\u7248\u672c\uff0czh_CN \u7b80\u4f53\uff0czh_TW \u7e41\u4f53\uff0cen \u82f1\u8bed\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_users_info(self, user_id_list, lang=\"zh_CN\"):\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/user/info/batchget\",\n            data={\n                \"user_list\": [\n                    {\n                        \"openid\": user_id,\n                        \"lang\": lang\n                    } for user_id in user_id_list\n                ]\n            }\n        )", "response": "\u6279\u91cf\u83b7\u53d6\u7528\u6237\u57fa\u672c\u4fe1\u606f\u3002\n\n        :param user_id_list: \u7528\u6237 ID \u7684\u5217\u8868\n        :param lang: \u8fd4\u56de\u56fd\u5bb6\u5730\u533a\u8bed\u8a00\u7248\u672c\uff0czh_CN \u7b80\u4f53\uff0czh_TW \u7e41\u4f53\uff0cen \u82f1\u8bed\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the list of followers.", "response": "def get_followers(self, first_user_id=None):\n        \"\"\"\n        \u83b7\u53d6\u5173\u6ce8\u8005\u5217\u8868\n        \u8be6\u60c5\u8bf7\u53c2\u8003 http://mp.weixin.qq.com/wiki/index.php?title=\u83b7\u53d6\u5173\u6ce8\u8005\u5217\u8868\n\n        :param first_user_id: \u53ef\u9009\u3002\u7b2c\u4e00\u4e2a\u62c9\u53d6\u7684OPENID\uff0c\u4e0d\u586b\u9ed8\u8ba4\u4ece\u5934\u5f00\u59cb\u62c9\u53d6\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        params = {\"access_token\": self.token}\n        if first_user_id:\n            params[\"next_openid\"] = first_user_id\n        return self.get(\n            \"https://api.weixin.qq.com/cgi-bin/user/get\", params=params\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a text message to a user", "response": "def send_text_message(self, user_id, content, kf_account=None):\n        \"\"\"\n        \u53d1\u9001\u6587\u672c\u6d88\u606f\u3002\n\n        :param user_id: \u7528\u6237 ID \u3002 \u5c31\u662f\u4f60\u6536\u5230\u7684 `Message` \u7684 source\n        :param content: \u6d88\u606f\u6b63\u6587\n        :param kf_account: \u53d1\u9001\u6d88\u606f\u7684\u5ba2\u670d\u8d26\u6237\uff0c\u9ed8\u8ba4\u503c\u4e3a None\uff0cNone \u4e3a\u4e0d\u6307\u5b9a\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        data = {\n            \"touser\": user_id,\n            \"msgtype\": \"text\",\n            \"text\": {\n                \"content\": content\n            }\n        }\n        if kf_account is not None:\n            data['customservice'] = {'kf_account': kf_account}\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/message/custom/send\",\n            data=data\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send_image_message(self, user_id, media_id, kf_account=None):\n        data = {\n            \"touser\": user_id,\n            \"msgtype\": \"image\",\n            \"image\": {\n                \"media_id\": media_id\n            }\n        }\n        if kf_account is not None:\n            data['customservice'] = {'kf_account': kf_account}\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/message/custom/send\",\n            data=data\n        )", "response": "Send an image message to the user."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends voice message to user", "response": "def send_voice_message(self, user_id, media_id, kf_account=None):\n        \"\"\"\n        \u53d1\u9001\u8bed\u97f3\u6d88\u606f\u3002\n\n        :param user_id: \u7528\u6237 ID \u3002 \u5c31\u662f\u4f60\u6536\u5230\u7684 `Message` \u7684 source\n        :param media_id: \u53d1\u9001\u7684\u8bed\u97f3\u7684\u5a92\u4f53ID\u3002 \u53ef\u4ee5\u901a\u8fc7 :func:`upload_media` \u4e0a\u4f20\u3002\n        :param kf_account: \u53d1\u9001\u6d88\u606f\u7684\u5ba2\u670d\u8d26\u6237\uff0c\u9ed8\u8ba4\u503c\u4e3a None\uff0cNone \u4e3a\u4e0d\u6307\u5b9a\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        data = {\n            \"touser\": user_id,\n            \"msgtype\": \"voice\",\n            \"voice\": {\n                \"media_id\": media_id\n            }\n        }\n        if kf_account is not None:\n            data['customservice'] = {'kf_account': kf_account}\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/message/custom/send\",\n            data=data\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send_music_message(\n        self,\n        user_id,\n        url,\n        hq_url,\n        thumb_media_id,\n        title=None,\n        description=None,\n        kf_account=None\n    ):\n        \"\"\"\n        \u53d1\u9001\u97f3\u4e50\u6d88\u606f\u3002\n        \u6ce8\u610f\u5982\u679c\u4f60\u9047\u5230\u4e86\u7f29\u7565\u56fe\u4e0d\u80fd\u6b63\u5e38\u663e\u793a\u7684\u95ee\u9898\uff0c \u4e0d\u8981\u614c\u5f20\uff1b \u76ee\u524d\u6765\u770b\u662f\u5fae\u4fe1\u670d\u52a1\u5668\u7aef\u7684\u95ee\u9898\u3002\n        \u5bf9\u6b64\u6211\u4eec\u4e5f\u65e0\u80fd\u4e3a\u529b ( `#197 <https://github.com/whtsky/WeRoBot/issues/197>`_ )\n\n        :param user_id: \u7528\u6237 ID \u3002 \u5c31\u662f\u4f60\u6536\u5230\u7684 `Message` \u7684 source\n        :param url: \u97f3\u4e50\u94fe\u63a5\n        :param hq_url: \u9ad8\u54c1\u8d28\u97f3\u4e50\u94fe\u63a5\uff0cwifi\u73af\u5883\u4f18\u5148\u4f7f\u7528\u8be5\u94fe\u63a5\u64ad\u653e\u97f3\u4e50\n        :param thumb_media_id: \u7f29\u7565\u56fe\u7684\u5a92\u4f53ID\u3002 \u53ef\u4ee5\u901a\u8fc7 :func:`upload_media` \u4e0a\u4f20\u3002\n        :param title: \u97f3\u4e50\u6807\u9898\n        :param description: \u97f3\u4e50\u63cf\u8ff0\n        :param kf_account: \u53d1\u9001\u6d88\u606f\u7684\u5ba2\u670d\u8d26\u6237\uff0c\u9ed8\u8ba4\u503c\u4e3a None\uff0cNone \u4e3a\u4e0d\u6307\u5b9a\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        music_data = {\n            \"musicurl\": url,\n            \"hqmusicurl\": hq_url,\n            \"thumb_media_id\": thumb_media_id\n        }\n        if title:\n            music_data[\"title\"] = title\n        if description:\n            music_data[\"description\"] = description\n        data = {\"touser\": user_id, \"msgtype\": \"music\", \"music\": music_data}\n        if kf_account is not None:\n            data['customservice'] = {'kf_account': kf_account}\n\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/message/custom/send\",\n            data=data\n        )", "response": "Send a music message to the WeRoBot."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_article_message(self, user_id, articles, kf_account=None):\n        if isinstance(articles[0], Article):\n            formatted_articles = []\n            for article in articles:\n                result = article.args\n                result[\"picurl\"] = result.pop(\"img\")\n                formatted_articles.append(result)\n        else:\n            formatted_articles = articles\n        data = {\n            \"touser\": user_id,\n            \"msgtype\": \"news\",\n            \"news\": {\n                \"articles\": formatted_articles\n            }\n        }\n        if kf_account is not None:\n            data['customservice'] = {'kf_account': kf_account}\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/message/custom/send\",\n            data=data\n        )", "response": "Send an article message to a user."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a news message to a user", "response": "def send_news_message(self, user_id, media_id, kf_account=None):\n        \"\"\"\n        \u53d1\u9001\u6c38\u4e45\u7d20\u6750\u4e2d\u7684\u56fe\u6587\u6d88\u606f\u3002\n\n        :param user_id: \u7528\u6237 ID \u3002 \u5c31\u662f\u4f60\u6536\u5230\u7684 `Message` \u7684 source\n        :param media_id: \u5a92\u4f53\u6587\u4ef6 ID\n        :param kf_account: \u53d1\u9001\u6d88\u606f\u7684\u5ba2\u670d\u8d26\u6237\uff0c\u9ed8\u8ba4\u503c\u4e3a None\uff0cNone \u4e3a\u4e0d\u6307\u5b9a\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        data = {\n            \"touser\": user_id,\n            \"msgtype\": \"mpnews\",\n            \"mpnews\": {\n                \"media_id\": media_id\n            }\n        }\n        if kf_account is not None:\n            data['customservice'] = {'kf_account': kf_account}\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/message/custom/send\",\n            data=data\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend a message to a user.", "response": "def send_miniprogrampage_message(\n        self, user_id, title, appid, pagepath, thumb_media_id, kf_account=None\n    ):\n        \"\"\"\n        \u53d1\u9001\u5c0f\u7a0b\u5e8f\u5361\u7247\uff08\u8981\u6c42\u5c0f\u7a0b\u5e8f\u4e0e\u516c\u4f17\u53f7\u5df2\u5173\u8054\uff09\n\n        :param user_id: \u7528\u6237 ID \u3002 \u5c31\u662f\u4f60\u6536\u5230\u7684 `Message` \u7684 source\n        :param title: \u5c0f\u7a0b\u5e8f\u5361\u7247\u7684\u6807\u9898\n        :param appid: \u5c0f\u7a0b\u5e8f\u7684 appid\uff0c\u8981\u6c42\u5c0f\u7a0b\u5e8f\u7684 appid \u9700\u8981\u4e0e\u516c\u4f17\u53f7\u6709\u5173\u8054\u5173\u7cfb\n        :param pagepath: \u5c0f\u7a0b\u5e8f\u7684\u9875\u9762\u8def\u5f84\uff0c\u8ddf app.json \u5bf9\u9f50\uff0c\u652f\u6301\u53c2\u6570\uff0c\u6bd4\u5982 pages/index/index?foo=bar\n        :param thumb_media_id: \u5c0f\u7a0b\u5e8f\u5361\u7247\u56fe\u7247\u7684\u5a92\u4f53 ID\uff0c\u5c0f\u7a0b\u5e8f\u5361\u7247\u56fe\u7247\u5efa\u8bae\u5927\u5c0f\u4e3a 520*416\n        :param kf_account: \u9700\u8981\u4ee5\u67d0\u4e2a\u5ba2\u670d\u5e10\u53f7\u6765\u53d1\u6d88\u606f\u65f6\u6307\u5b9a\u7684\u5ba2\u670d\u8d26\u6237\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        data = {\n            \"touser\": user_id,\n            \"msgtype\": \"miniprogrampage\",\n            \"miniprogrampage\": {\n                \"title\": title,\n                \"appid\": appid,\n                \"pagepath\": pagepath,\n                \"thumb_media_id\": thumb_media_id\n            }\n        }\n        if kf_account is not None:\n            data[\"customservice\"] = {\"kf_account\": kf_account}\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/message/custom/send\",\n            data=data\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a template message to a user", "response": "def send_template_message(self, user_id, template_id, data, url=''):\n        \"\"\"\n        \u53d1\u9001\u6a21\u677f\u6d88\u606f\n        \u8be6\u60c5\u8bf7\u53c2\u8003 http://mp.weixin.qq.com/wiki/17/304c1885ea66dbedf7dc170d84999a9d.html\n\n        :param user_id: \u7528\u6237 ID \u3002 \u5c31\u662f\u4f60\u6536\u5230\u7684 `Message` \u7684 source\n        :param template_id: \u6a21\u677f ID\u3002\n        :param data: \u7528\u4e8e\u6e32\u67d3\u6a21\u677f\u7684\u6570\u636e\u3002\n        :param url: \u6a21\u677f\u6d88\u606f\u7684\u53ef\u9009\u94fe\u63a5\u3002\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/message/template/send\",\n            data={\n                \"touser\": user_id,\n                \"template_id\": template_id,\n                \"url\": url,\n                \"data\": data\n            }\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the tag with the given ID and name.", "response": "def update_tag(self, tag_id, tag_name):\n        \"\"\"\n        \u4fee\u6539\u6807\u7b7e\n\n        :param tag_id: \u6807\u7b7e ID\n        :param tag_name: \u65b0\u7684\u6807\u7b7e\u540d\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/tags/update\",\n            data={\"tag\": {\n                \"id\": tag_id,\n                \"name\": tag_name\n            }}\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets users by tag", "response": "def get_users_by_tag(self, tag_id, next_open_id=\"\"):\n        \"\"\"\n        \u83b7\u53d6\u6807\u7b7e\u4e0b\u7c89\u4e1d\u5217\u8868\n\n        :param tag_id: \u6807\u7b7e ID\n        :param next_open_id: \u7b2c\u4e00\u4e2a\u62c9\u53d6\u7528\u6237\u7684 OPENID\uff0c\u9ed8\u8ba4\u4ece\u5934\u5f00\u59cb\u62c9\u53d6\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/user/tag/get\",\n            data={\n                \"tagid\": tag_id,\n                \"next_openid\": next_open_id\n            }\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tag_users(self, tag_id, open_id_list):\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/tags/members/batchtagging\",\n            data={\n                \"openid_list\": open_id_list,\n                \"tagid\": tag_id\n            }\n        )", "response": "Tag users with a given tag."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef untag_users(self, tag_id, open_id_list):\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/tags/members/batchuntagging\",\n            data={\n                \"openid_list\": open_id_list,\n                \"tagid\": tag_id\n            }\n        )", "response": "Untag users with a given tag."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a single object from the cache", "response": "def get(self, id):\n        \"\"\"\n        \u6839\u636e id \u83b7\u53d6\u6570\u636e\u3002\n\n        :param id: \u8981\u83b7\u53d6\u7684\u6570\u636e\u7684 id\n        :return: \u8fd4\u56de\u53d6\u5230\u7684\u6570\u636e\uff0c\u5982\u679c\u662f\u7a7a\u5219\u8fd4\u56de\u4e00\u4e2a\u7a7a\u7684 ``dict`` \u5bf9\u8c61\n        \"\"\"\n        try:\n            session_json = self.db[id]\n        except KeyError:\n            session_json = \"{}\"\n        return json_loads(session_json)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef encrypt(self, text, app_id):\n        text = b\"\".join(\n            [\n                to_binary(self.get_random_string()),\n                struct.pack(b\"I\", socket.htonl(len(to_binary(text)))),\n                to_binary(text),\n                to_binary(app_id)\n            ]\n        )\n        text = pkcs7.encode(text)\n        encryptor = self.cipher.encryptor()\n        ciphertext = to_binary(encryptor.update(text) + encryptor.finalize())\n        return base64.b64encode(ciphertext)", "response": "encrypt text with the current cipher"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decrypt(self, text, app_id):\n        text = to_binary(text)\n        decryptor = self.cipher.decryptor()\n        plain_text = decryptor.update(base64.b64decode(text)\n                                      ) + decryptor.finalize()\n\n        padding = byte2int(plain_text, -1)\n        content = plain_text[16:-padding]\n\n        xml_len = socket.ntohl(struct.unpack(\"I\", content[:4])[0])\n        xml_content = content[4:xml_len + 4]\n        from_appid = content[xml_len + 4:]\n\n        if to_text(from_appid) != app_id:\n            raise AppIdValidationError(text, app_id)\n\n        return xml_content", "response": "Decrypt a text and return the xml content"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encrypt_message(self, reply, timestamp=None, nonce=None):\n        if hasattr(reply, \"render\"):\n            reply = reply.render()\n\n        timestamp = timestamp or to_text(int(time.time()))\n        nonce = nonce or generate_token(5)\n        encrypt = to_text(self.prp_crypto.encrypt(reply, self.app_id))\n        signature = get_signature(self.token, timestamp, nonce, encrypt)\n        return to_text(\n            self.ENCRYPTED_MESSAGE_XML.format(\n                encrypt=encrypt,\n                signature=signature,\n                timestamp=timestamp,\n                nonce=nonce\n            )\n        )", "response": "\u52a0\u5bc6\u5fae\u4fe1\u56de\u590d\n        :param reply: \u52a0\u5bc6\u524d\u7684\u56de\u590d\n        :type reply: WeChatReply \u6216 XML \u6587\u672c\n        :return: \u52a0\u5bc6\u540e\u7684\u56de\u590d\u6587\u672c"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set(self, id, value):\n        return self.kv.set(self.key_name(id), value)", "response": "set value in the cache"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a Django view that can be used to view a single object.", "response": "def make_view(robot):\n    \"\"\"\n    \u4e3a\u4e00\u4e2a BaseRoBot \u751f\u6210 Django view\u3002\n\n    :param robot: \u4e00\u4e2a BaseRoBot \u5b9e\u4f8b\u3002\n    :return: \u4e00\u4e2a\u6807\u51c6\u7684 Django view\n    \"\"\"\n\n    @csrf_exempt\n    def werobot_view(request):\n        timestamp = request.GET.get(\"timestamp\", \"\")\n        nonce = request.GET.get(\"nonce\", \"\")\n        signature = request.GET.get(\"signature\", \"\")\n\n        if not robot.check_signature(\n            timestamp=timestamp, nonce=nonce, signature=signature\n        ):\n            return HttpResponseForbidden(\n                robot.make_error_page(\n                    html.escape(request.build_absolute_uri())\n                )\n            )\n        if request.method == \"GET\":\n            return HttpResponse(request.GET.get(\"echostr\", \"\"))\n        elif request.method == \"POST\":\n            message = robot.parse_message(\n                request.body,\n                timestamp=timestamp,\n                nonce=nonce,\n                msg_signature=request.GET.get(\"msg_signature\", \"\")\n            )\n            return HttpResponse(\n                robot.get_encrypted_reply(message),\n                content_type=\"application/xml;charset=utf-8\"\n            )\n        return HttpResponseNotAllowed(['GET', 'POST'])\n\n    return werobot_view"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_js_pay_package(self, **package):\n        assert self.pay_partner_id, \"PAY_PARTNER_ID IS EMPTY\"\n        assert self.pay_partner_key, \"PAY_PARTNER_KEY IS EMPTY\"\n\n        package.update({\n            'partner': self.pay_partner_id,\n        })\n\n        package.setdefault('bank_type', 'WX')\n        package.setdefault('fee_type', '1')\n        package.setdefault('input_charset', 'UTF-8')\n\n        params = package.items()\n        params.sort()\n\n        sign = md5(\n            '&'.join(\n                [\n                    \"%s=%s\" % (str(p[0]), str(p[1]))\n                    for p in params + [('key', self.pay_partner_key)]\n                ]\n            )\n        ).hexdigest().upper()\n\n        return urlencode(params + [('sign', sign)])", "response": "\u7b7e\u540d pay package \u9700\u8981\u7684\u53c2\u6570\n        \u8be6\u60c5\u8bf7\u53c2\u8003 \u652f\u4ed8\u5f00\u53d1\u6587\u6863\n\n        :param package: \u9700\u8981\u7b7e\u540d\u7684\u7684\u53c2\u6570\n        :return: \u53ef\u4ee5\u4f7f\u7528\u7684packagestr"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_js_pay_params(self, **package):\n        pay_param, sign, sign_type = self._pay_sign_dict(\n            package=self.create_js_pay_package(**package)\n        )\n        pay_param['paySign'] = sign\n        pay_param['signType'] = sign_type\n\n        # \u817e\u8baf\u8fd9\u4e2a\u8fd8\u5f97\u8f6c\u6210\u5927\u5199 JS \u624d\u8ba4\n        for key in ['appId', 'timeStamp', 'nonceStr']:\n            pay_param[key] = str(pay_param.pop(key.lower()))\n\n        return pay_param", "response": "create js \u9700\u8981\u7b7e\u540d js \u9700\u8981\u7684 js \u9700\u8981\u7b7e\u540d js \u9700\u8981\u7684 js \u9700\u8981\u7684 js \u9700\u8981\u7b7e\u540d js \u9700\u8981\u7684 js \u9700\u8981\u7b7e"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate js_edit_address_param - create js_edit_address_param", "response": "def create_js_edit_address_param(self, accesstoken, **params):\n        \"\"\"\n        alpha\n        \u6682\u65f6\u4e0d\u5efa\u8bae\u4f7f\u7528\n        \u8fd9\u4e2a\u63a5\u53e3\u4f7f\u7528\u8d77\u6765\u5341\u5206\u4e0d\u53cb\u597d\n        \u800c\u4e14\u4f1a\u5f15\u8d77\u5de8\u5927\u7684\u8bef\u89e3\n\n        url \u9700\u8981\u5e26\u4e0a code \u548c state (url?code=xxx&state=1)\n        code \u548cstate \u662f oauth \u65f6\u5019\u56de\u6765\u7684\n\n        token \u8981\u4f20\u7528\u6237\u7684 token\n\n        \u8fd9\u5c3c\u739b \u4f60\u80fd\u76f8\u4fe1\u8fd9\u4e9b\u652f\u4ed8\u63a5\u53e3\u90fd\u662f\u817e\u8baf\u51fa\u7684\uff1f\n        \"\"\"\n        params.update(\n            {\n                'appId': self.appid,\n                'nonceStr': generate_token(8),\n                'timeStamp': int(time.time())\n            }\n        )\n\n        _params = [(k.lower(), str(v)) for k, v in params.items()]\n        _params += [('accesstoken', accesstoken)]\n        _params.sort()\n\n        string1 = '&'.join([\"%s=%s\" % (p[0], p[1]) for p in _params])\n        sign = sha1(string1).hexdigest()\n\n        params = dict([(k, str(v)) for k, v in params.items()])\n\n        params['addrSign'] = sign\n        params['signType'] = 'sha1'\n        params['scope'] = params.get('scope', 'jsapi_address')\n\n        return params"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pay_deliver_notify(self, **deliver_info):\n        params, sign, _ = self._pay_sign_dict(\n            add_noncestr=False, add_timestamp=False, **deliver_info\n        )\n\n        params['app_signature'] = sign\n        params['sign_method'] = 'sha1'\n\n        return self.post(\n            url=\"https://api.weixin.qq.com/pay/delivernotify\", data=params\n        )", "response": "\u901a\u77e5 \u817e\u8baf\u53d1\u8d27\n\n        \u4e00\u822c\u5f62\u5f0f ::\n            wxclient.pay_delivernotify(\n                openid=openid,\n                transid=transaction_id,\n                out_trade_no=\u672c\u5730\u8ba2\u5355\u53f7,\n                deliver_timestamp=int(time.time()),\n                deliver_status=\"1\",\n                deliver_msg=\"ok\"\n            )\n\n        :param \u9700\u8981\u7b7e\u540d\u7684\u7684\u53c2\u6570\n        :return: \u652f\u4ed8\u9700\u8981\u7684\u5bf9\u8c61"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pay_order_query(self, out_trade_no):\n\n        package = {\n            'partner': self.pay_partner_id,\n            'out_trade_no': out_trade_no,\n        }\n\n        _package = package.items()\n        _package.sort()\n\n        s = '&'.join(\n            [\n                \"%s=%s\" % (p[0], str(p[1]))\n                for p in (_package + [('key', self.pay_partner_key)])\n            ]\n        )\n        package['sign'] = md5(s).hexdigest().upper()\n\n        package = '&'.join([\"%s=%s\" % (p[0], p[1]) for p in package.items()])\n\n        params, sign, _ = self._pay_sign_dict(\n            add_noncestr=False, package=package\n        )\n\n        params['app_signature'] = sign\n        params['sign_method'] = 'sha1'\n\n        return self.post(\n            url=\"https://api.weixin.qq.com/pay/orderquery\", data=params\n        )", "response": "\u67e5\u8be2\u8ba2\u5355\u72b6\u6001\n        \u4e00\u822c\u7528\u4e8e\u65e0\u6cd5\u786e\u5b9a \u8ba2\u5355\u72b6\u6001\u65f6\u5019\u8865\u507f\n\n        :param out_trade_no: \u672c\u5730\u8ba2\u5355\u53f7\n        :return: \u8ba2\u5355\u4fe1\u606fdict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the ASCII int value of a character in a string.", "response": "def byte2int(s, index=0):\n    \"\"\"Get the ASCII int value of a character in a string.\n\n    :param s: a string\n    :param index: the position of desired character\n\n    :return: ASCII int value\n    \"\"\"\n    if six.PY2:\n        return ord(s[index])\n    return s[index]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pay_sign_dict(\n    appid,\n    pay_sign_key,\n    add_noncestr=True,\n    add_timestamp=True,\n    add_appid=True,\n    **kwargs\n):\n    \"\"\"\n    \u652f\u4ed8\u53c2\u6570\u7b7e\u540d\n    \"\"\"\n    assert pay_sign_key, \"PAY SIGN KEY IS EMPTY\"\n\n    if add_appid:\n        kwargs.update({'appid': appid})\n\n    if add_noncestr:\n        kwargs.update({'noncestr': generate_token()})\n\n    if add_timestamp:\n        kwargs.update({'timestamp': int(time.time())})\n\n    params = kwargs.items()\n\n    _params = [\n        (k.lower(), v) for k, v in kwargs.items() if k.lower() != \"appid\"\n    ]\n    _params += [('appid', appid), ('appkey', pay_sign_key)]\n    _params.sort()\n\n    sign = '&'.join([\"%s=%s\" % (str(p[0]), str(p[1]))\n                     for p in _params]).encode(\"utf-8\")\n    sign = sha1(sign).hexdigest()\n    sign_type = 'SHA1'\n\n    return dict(params), sign, sign_type", "response": "Returns a dict with the parameters and the sign and sign_type"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef key_click(self, key):\n\n        def wraps(f):\n            argc = len(signature(f).parameters.keys())\n\n            @self.click\n            def onclick(message, session=None):\n                if message.key == key:\n                    return f(*[message, session][:argc])\n\n            return f\n\n        return wraps", "response": "Decorator for key click event handler"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef filter(self, *args):\n\n        def wraps(f):\n            self.add_filter(func=f, rules=list(args))\n            return f\n\n        return wraps", "response": "Decorator to add a filter to the current instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_handler(self, func, type='all'):\n        if not callable(func):\n            raise ValueError(\"{} is not callable\".format(func))\n\n        self._handlers[type].append(\n            (func, len(signature(func).parameters.keys()))\n        )", "response": "Add a handler to the list of handlers."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding filter to BaseRoBot.", "response": "def add_filter(self, func, rules):\n        \"\"\"\n        \u4e3a BaseRoBot \u6dfb\u52a0\u4e00\u4e2a ``filter handler``\u3002\n\n        :param func: \u5982\u679c rules \u901a\u8fc7\uff0c\u5219\u5904\u7406\u8be5\u6d88\u606f\u7684 handler\u3002\n        :param rules: \u4e00\u4e2a list\uff0c\u5305\u542b\u8981\u5339\u914d\u7684\u5b57\u7b26\u4e32\u6216\u8005\u6b63\u5219\u8868\u8fbe\u5f0f\u3002\n        :return: None\n        \"\"\"\n        if not callable(func):\n            raise ValueError(\"{} is not callable\".format(func))\n        if not isinstance(rules, list):\n            raise ValueError(\"{} is not list\".format(rules))\n        if len(rules) > 1:\n            for x in rules:\n                self.add_filter(func, [x])\n        else:\n            target_content = rules[0]\n            if isinstance(target_content, six.string_types):\n                target_content = to_text(target_content)\n\n                def _check_content(message):\n                    return message.content == target_content\n            elif is_regex(target_content):\n\n                def _check_content(message):\n                    return target_content.match(message.content)\n            else:\n                raise TypeError(\"%s is not a valid rule\" % target_content)\n            argc = len(signature(func).parameters.keys())\n\n            @self.text\n            def _f(message, session=None):\n                _check_result = _check_content(message)\n                if _check_result:\n                    if isinstance(_check_result, bool):\n                        _check_result = None\n                    return func(*[message, session, _check_result][:argc])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a message from a raw XML body.", "response": "def parse_message(\n        self, body, timestamp=None, nonce=None, msg_signature=None\n    ):\n        \"\"\"\n        \u89e3\u6790\u83b7\u53d6\u5230\u7684 Raw XML \uff0c\u5982\u679c\u9700\u8981\u7684\u8bdd\u8fdb\u884c\u89e3\u5bc6\uff0c\u8fd4\u56de WeRoBot Message\u3002\n        :param body: \u5fae\u4fe1\u670d\u52a1\u5668\u53d1\u6765\u7684\u8bf7\u6c42\u4e2d\u7684 Body\u3002\n        :return: WeRoBot Message\n        \"\"\"\n        message_dict = parse_xml(body)\n        if \"Encrypt\" in message_dict:\n            xml = self.crypto.decrypt_message(\n                timestamp=timestamp,\n                nonce=nonce,\n                msg_signature=msg_signature,\n                encrypt_msg=message_dict[\"Encrypt\"]\n            )\n            message_dict = parse_xml(xml)\n        return process_message(message_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets reply from the message.", "response": "def get_reply(self, message):\n        \"\"\"\n        \u6839\u636e message \u7684\u5185\u5bb9\u83b7\u53d6 Reply \u5bf9\u8c61\u3002\n\n        :param message: \u8981\u5904\u7406\u7684 message\n        :return: \u83b7\u53d6\u7684 Reply \u5bf9\u8c61\n        \"\"\"\n        session_storage = self.session_storage\n\n        id = None\n        session = None\n        if session_storage and hasattr(message, \"source\"):\n            id = to_binary(message.source)\n            session = session_storage[id]\n\n        handlers = self.get_handlers(message.type)\n        try:\n            for handler, args_count in handlers:\n                args = [message, session][:args_count]\n                reply = handler(*args)\n                if session_storage and id:\n                    session_storage[id] = session\n                if reply:\n                    return process_function_reply(reply, message=message)\n        except:\n            self.logger.exception(\"Catch an exception\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the signature is correct.", "response": "def check_signature(self, timestamp, nonce, signature):\n        \"\"\"\n        \u6839\u636e\u65f6\u95f4\u6233\u548c\u751f\u6210\u7b7e\u540d\u7684\u5b57\u7b26\u4e32 (nonce) \u68c0\u67e5\u7b7e\u540d\u3002\n\n        :param timestamp: \u65f6\u95f4\u6233\n        :param nonce: \u751f\u6210\u7b7e\u540d\u7684\u968f\u673a\u5b57\u7b26\u4e32\n        :param signature: \u8981\u68c0\u67e5\u7684\u7b7e\u540d\n        :return: \u5982\u679c\u7b7e\u540d\u5408\u6cd5\u5c06\u8fd4\u56de ``True``\uff0c\u4e0d\u5408\u6cd5\u5c06\u8fd4\u56de ``False``\n        \"\"\"\n        return check_signature(\n            self.config[\"TOKEN\"], timestamp, nonce, signature\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(\n        self, server=None, host=None, port=None, enable_pretty_logging=True\n    ):\n        \"\"\"\n        \u8fd0\u884c WeRoBot\u3002\n\n        :param server: \u4f20\u9012\u7ed9 Bottle \u6846\u67b6 run \u65b9\u6cd5\u7684\u53c2\u6570\uff0c\u8be6\u60c5\u89c1\\\n        `bottle \u6587\u6863 <https://bottlepy.org/docs/dev/deployment.html#switching-the-server-backend>`_\n        :param host: \u8fd0\u884c\u65f6\u7ed1\u5b9a\u7684\u4e3b\u673a\u5730\u5740\n        :param port: \u8fd0\u884c\u65f6\u7ed1\u5b9a\u7684\u4e3b\u673a\u7aef\u53e3\n        :param enable_pretty_logging: \u662f\u5426\u5f00\u542f log \u7684\u8f93\u51fa\u683c\u5f0f\u4f18\u5316\n        \"\"\"\n        if enable_pretty_logging:\n            from werobot.logger import enable_pretty_logging\n            enable_pretty_logging(self.logger)\n        if server is None:\n            server = self.config[\"SERVER\"]\n        if host is None:\n            host = self.config[\"HOST\"]\n        if port is None:\n            port = self.config[\"PORT\"]\n        try:\n            self.wsgi.run(server=server, host=host, port=port)\n        except KeyboardInterrupt:\n            exit(0)", "response": "\u8fd0\u884c WeRoBot\u3002\n\n        :param server: \u4f20\u9012\u7ed9 Bottle \u6846\u67b6 run \u65b9\u6cd5\u7684\u53c2\u6570\uff0c\u8be6\u60c5\u89c1\\\n        `bottle \u6587\u6863 <https://bottlepy.org/docs/dev/deployment.html#switching-the-server-backend>`_\n        :param host: \u8fd0\u884c\u65f6\u7ed1\u5b9a\u7684\u4e3b\u673a\u5730\u5740\n        :param port: \u8fd0\u884c\u65f6\u7ed1\u5b9a\u7684\u4e3b\u673a\u7aef\u53e3\n        :param enable_pretty_logging: \u662f\u5426\u5f00\u542f log \u7684\u8f93\u51fa\u683c\u5f0f\u4f18\u5316"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a specific id from the database", "response": "def get(self, id):\n        \"\"\"\n        \u6839\u636e id \u83b7\u53d6\u6570\u636e\u3002\n\n        :param id: \u8981\u83b7\u53d6\u7684\u6570\u636e\u7684 id\n        :return: \u8fd4\u56de\u53d6\u5230\u7684\u6570\u636e\uff0c\u5982\u679c\u662f\u7a7a\u5219\u8fd4\u56de\u4e00\u4e2a\u7a7a\u7684 ``dict`` \u5bf9\u8c61\n        \"\"\"\n        cur = self.conn.cursor()\n        cur.execute(\"SELECT value FROM WeRoBot WHERE id=%s LIMIT 1;\", (id, ))\n        session_json = cur.fetchone()\n        if session_json is None:\n            return {}\n        return json_loads(session_json[0])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting a werobot from the database.", "response": "def delete(self, id):\n        \"\"\"\n        \u6839\u636e id \u5220\u9664\u6570\u636e\u3002\n\n        :param id: \u8981\u5220\u9664\u7684\u6570\u636e\u7684 id\n        \"\"\"\n        self.conn.cursor().execute(\"DELETE FROM WeRoBot WHERE id=%s\", (id, ))\n        self.conn.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprocesses a message dict and return a Message object", "response": "def process_message(message):\n    \"\"\"\n    Process a message dict and return a Message Object\n    :param message: Message dict returned by `parse_xml` function\n    :return: Message Object\n    \"\"\"\n    message[\"type\"] = message.pop(\"MsgType\").lower()\n    if message[\"type\"] == 'event':\n        message[\"type\"] = str(message.pop(\"Event\")).lower() + '_event'\n        message_type = EventMetaClass.TYPES.get(message[\"type\"], UnknownEvent)\n    else:\n        message_type = MessageMetaClass.TYPES.get(\n            message[\"type\"], UnknownMessage\n        )\n    return message_type(message)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting value for id", "response": "def set(self, id, value):\n        \"\"\"\n        \u6839\u636e id \u5199\u5165\u6570\u636e\u3002\n\n        :param id: \u8981\u5199\u5165\u7684 id\n        :param value: \u8981\u5199\u5165\u7684\u6570\u636e\uff0c\u53ef\u4ee5\u662f\u4e00\u4e2a ``dict`` \u5bf9\u8c61\n        \"\"\"\n        value = json_dumps(value)\n        self.conn.cursor().execute(\n            \"INSERT INTO WeRoBot (id, value) VALUES (%s,%s) \\\n                ON DUPLICATE KEY UPDATE value=%s\", (\n                id,\n                value,\n                value,\n            )\n        )\n        self.conn.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(self, id):\n        id = self.key_name(id)\n        session_json = self.redis.get(id) or '{}'\n        return json_loads(session_json)", "response": "get \u83b7\u53d6 id \u8981\u83b7\u53d6\u6570\u636e"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set(self, id, value):\n        id = self.key_name(id)\n        self.redis.set(id, json_dumps(value))", "response": "set value in the cache"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete(self, id):\n        id = self.key_name(id)\n        self.redis.delete(id)", "response": "delete a key from the cache"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_vectors(self, vectors, **kwargs):\n        if not isinstance(vectors, list):\n            vectors = [vectors]\n        for idx, vector in enumerate(vectors):\n            if six.PY2 and isinstance(vector, str):\n                vector = six.text_type(vector)\n            if isinstance(vector, six.string_types):\n                # Convert the string pretrained vector identifier\n                # to a Vectors object\n                if vector not in pretrained_aliases:\n                    raise ValueError(\n                        \"Got string input vector {}, but allowed pretrained \"\n                        \"vectors are {}\".format(\n                            vector, list(pretrained_aliases.keys())))\n                vectors[idx] = pretrained_aliases[vector](**kwargs)\n            elif not isinstance(vector, Vectors):\n                raise ValueError(\n                    \"Got input vectors of type {}, expected str or \"\n                    \"Vectors object\".format(type(vector)))\n\n        tot_dim = sum(v.dim for v in vectors)\n        self.vectors = torch.Tensor(len(self), tot_dim)\n        for i, token in enumerate(self.itos):\n            start_dim = 0\n            for v in vectors:\n                end_dim = start_dim + v.dim\n                self.vectors[i][start_dim:end_dim] = v[token.strip()]\n                start_dim = end_dim\n            assert(start_dim == tot_dim)", "response": "Loads vectors from the input vectors."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the vectors for the Vocab instance from a collection of Tensors.", "response": "def set_vectors(self, stoi, vectors, dim, unk_init=torch.Tensor.zero_):\n        \"\"\"\n        Set the vectors for the Vocab instance from a collection of Tensors.\n\n        Arguments:\n            stoi: A dictionary of string to the index of the associated vector\n                in the `vectors` input argument.\n            vectors: An indexed iterable (or other structure supporting __getitem__) that\n                given an input index, returns a FloatTensor representing the vector\n                for the token associated with the index. For example,\n                vector[stoi[\"string\"]] should return the vector for \"string\".\n            dim: The dimensionality of the vectors.\n            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n                to zero vectors; can be any function that takes in a Tensor and\n                returns a Tensor of the same size. Default: torch.Tensor.zero_\n        \"\"\"\n        self.vectors = torch.Tensor(len(self), dim)\n        for i, token in enumerate(self.itos):\n            wv_index = stoi.get(token, None)\n            if wv_index is not None:\n                self.vectors[i] = vectors[wv_index]\n            else:\n                self.vectors[i] = unk_init(self.vectors[i])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef splits(cls, text_field, label_field, root='.data',\n               train='train.txt', validation='dev.txt', test='test.txt',\n               train_subtrees=False, **kwargs):\n        \"\"\"Create dataset objects for splits of the SST dataset.\n\n        Arguments:\n            text_field: The field that will be used for the sentence.\n            label_field: The field that will be used for label data.\n            root: The root directory that the dataset's zip archive will be\n                expanded into; therefore the directory in whose trees\n                subdirectory the data files will be stored.\n            train: The filename of the train data. Default: 'train.txt'.\n            validation: The filename of the validation data, or None to not\n                load the validation set. Default: 'dev.txt'.\n            test: The filename of the test data, or None to not load the test\n                set. Default: 'test.txt'.\n            train_subtrees: Whether to use all subtrees in the training set.\n                Default: False.\n            Remaining keyword arguments: Passed to the splits method of\n                Dataset.\n        \"\"\"\n        path = cls.download(root)\n\n        train_data = None if train is None else cls(\n            os.path.join(path, train), text_field, label_field, subtrees=train_subtrees,\n            **kwargs)\n        val_data = None if validation is None else cls(\n            os.path.join(path, validation), text_field, label_field, **kwargs)\n        test_data = None if test is None else cls(\n            os.path.join(path, test), text_field, label_field, **kwargs)\n        return tuple(d for d in (train_data, val_data, test_data)\n                     if d is not None)", "response": "Creates a tuple of Dataset objects for splits of the SST dataset."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate iterator objects for splits of the SST dataset.", "response": "def iters(cls, batch_size=32, device=0, root='.data', vectors=None, **kwargs):\n        \"\"\"Create iterator objects for splits of the SST dataset.\n\n        Arguments:\n            batch_size: Batch_size\n            device: Device to create batches on. Use - 1 for CPU and None for\n                the currently active GPU device.\n            root: The root directory that the dataset's zip archive will be\n                expanded into; therefore the directory in whose trees\n                subdirectory the data files will be stored.\n            vectors: one of the available pretrained vectors or a list with each\n                element one of the available pretrained vectors (see Vocab.load_vectors)\n            Remaining keyword arguments: Passed to the splits method.\n        \"\"\"\n        TEXT = data.Field()\n        LABEL = data.Field(sequential=False)\n\n        train, val, test = cls.splits(TEXT, LABEL, root=root, **kwargs)\n\n        TEXT.build_vocab(train, vectors=vectors)\n        LABEL.build_vocab(train)\n\n        return data.BucketIterator.splits(\n            (train, val, test), batch_size=batch_size, device=device)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninterleave bits from two sort keys to form a joint sort key.", "response": "def interleave_keys(a, b):\n    \"\"\"Interleave bits from two sort keys to form a joint sort key.\n\n    Examples that are similar in both of the provided keys will have similar\n    values for the key defined by this function. Useful for tasks with two\n    text fields like machine translation or natural language inference.\n    \"\"\"\n    def interleave(args):\n        return ''.join([x for t in zip(*args) for x in t])\n    return int(''.join(interleave(format(x, '016b') for x in (a, b))), base=2)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing a specific RNG state.", "response": "def use_internal_state(self):\n        \"\"\"Use a specific RNG state.\"\"\"\n        old_state = random.getstate()\n        random.setstate(self._random_state)\n        yield\n        self._random_state = random.getstate()\n        random.setstate(old_state)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndownload file from url and save it to path", "response": "def download_from_url(url, path):\n    \"\"\"Download file, with logic (from tensor2tensor) for Google Drive\"\"\"\n    def process_response(r):\n        chunk_size = 16 * 1024\n        total_size = int(r.headers.get('Content-length', 0))\n        with open(path, \"wb\") as file:\n            with tqdm(total=total_size, unit='B',\n                      unit_scale=1, desc=path.split('/')[-1]) as t:\n                for chunk in r.iter_content(chunk_size):\n                    if chunk:\n                        file.write(chunk)\n                        t.update(len(chunk))\n\n    if 'drive.google.com' not in url:\n        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, stream=True)\n        process_response(response)\n        return\n\n    print('downloading from Google Drive; may take a few minutes')\n    confirm_token = None\n    session = requests.Session()\n    response = session.get(url, stream=True)\n    for k, v in response.cookies.items():\n        if k.startswith(\"download_warning\"):\n            confirm_token = v\n\n    if confirm_token:\n        url = url + \"&confirm=\" + confirm_token\n        response = session.get(url, stream=True)\n\n    process_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a tuple of Dataset objects for splits of a TranslationDataset.", "response": "def splits(cls, exts, fields, path=None, root='.data',\n               train='train', validation='val', test='test', **kwargs):\n        \"\"\"Create dataset objects for splits of a TranslationDataset.\n\n        Arguments:\n            exts: A tuple containing the extension to path for each language.\n            fields: A tuple containing the fields that will be used for data\n                in each language.\n            path (str): Common prefix of the splits' file paths, or None to use\n                the result of cls.download(root).\n            root: Root dataset storage directory. Default is '.data'.\n            train: The prefix of the train data. Default: 'train'.\n            validation: The prefix of the validation data. Default: 'val'.\n            test: The prefix of the test data. Default: 'test'.\n            Remaining keyword arguments: Passed to the splits method of\n                Dataset.\n        \"\"\"\n        if path is None:\n            path = cls.download(root)\n\n        train_data = None if train is None else cls(\n            os.path.join(path, train), exts, fields, **kwargs)\n        val_data = None if validation is None else cls(\n            os.path.join(path, validation), exts, fields, **kwargs)\n        test_data = None if test is None else cls(\n            os.path.join(path, test), exts, fields, **kwargs)\n        return tuple(d for d in (train_data, val_data, test_data)\n                     if d is not None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef splits(cls, exts, fields, root='.data',\n               train='train', validation='val', test='test2016', **kwargs):\n        \"\"\"Create dataset objects for splits of the Multi30k dataset.\n\n        Arguments:\n            exts: A tuple containing the extension to path for each language.\n            fields: A tuple containing the fields that will be used for data\n                in each language.\n            root: Root dataset storage directory. Default is '.data'.\n            train: The prefix of the train data. Default: 'train'.\n            validation: The prefix of the validation data. Default: 'val'.\n            test: The prefix of the test data. Default: 'test'.\n            Remaining keyword arguments: Passed to the splits method of\n                Dataset.\n        \"\"\"\n\n        # TODO: This is a _HORRIBLE_ patch related to #208\n        # 'path' can be passed as a kwarg to the translation dataset constructor\n        # or has to be set (so the download wouldn't be duplicated). A good idea\n        # seems to rename the existence check variable from path to something else\n        if 'path' not in kwargs:\n            expected_folder = os.path.join(root, cls.name)\n            path = expected_folder if os.path.exists(expected_folder) else None\n        else:\n            path = kwargs['path']\n            del kwargs['path']\n\n        return super(Multi30k, cls).splits(\n            exts, fields, path, root, train, validation, test, **kwargs)", "response": "Create dataset objects for splits of the Multi30k dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef splits(cls, exts, fields, root='.data',\n               train='train', validation='IWSLT16.TED.tst2013',\n               test='IWSLT16.TED.tst2014', **kwargs):\n        \"\"\"Create dataset objects for splits of the IWSLT dataset.\n\n        Arguments:\n            exts: A tuple containing the extension to path for each language.\n            fields: A tuple containing the fields that will be used for data\n                in each language.\n            root: Root dataset storage directory. Default is '.data'.\n            train: The prefix of the train data. Default: 'train'.\n            validation: The prefix of the validation data. Default: 'val'.\n            test: The prefix of the test data. Default: 'test'.\n            Remaining keyword arguments: Passed to the splits method of\n                Dataset.\n        \"\"\"\n        cls.dirname = cls.base_dirname.format(exts[0][1:], exts[1][1:])\n        cls.urls = [cls.base_url.format(exts[0][1:], exts[1][1:], cls.dirname)]\n        check = os.path.join(root, cls.name, cls.dirname)\n        path = cls.download(root, check=check)\n\n        train = '.'.join([train, cls.dirname])\n        validation = '.'.join([validation, cls.dirname])\n        if test is not None:\n            test = '.'.join([test, cls.dirname])\n\n        if not os.path.exists(os.path.join(path, train) + exts[0]):\n            cls.clean(path)\n\n        train_data = None if train is None else cls(\n            os.path.join(path, train), exts, fields, **kwargs)\n        val_data = None if validation is None else cls(\n            os.path.join(path, validation), exts, fields, **kwargs)\n        test_data = None if test is None else cls(\n            os.path.join(path, test), exts, fields, **kwargs)\n        return tuple(d for d in (train_data, val_data, test_data)\n                     if d is not None)", "response": "Creates a tuple of dataset objects for splits of the IWSLT dataset."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process(self, batch, *args, **kwargs):\n        if self.postprocessing is not None:\n            batch = self.postprocessing(batch)\n        return batch", "response": "Processes a list of examples to create a batch."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef preprocess(self, x):\n        if (six.PY2 and isinstance(x, six.string_types)\n                and not isinstance(x, six.text_type)):\n            x = Pipeline(lambda s: six.text_type(s, encoding='utf-8'))(x)\n        if self.sequential and isinstance(x, six.text_type):\n            x = self.tokenize(x.rstrip('\\n'))\n        if self.lower:\n            x = Pipeline(six.text_type.lower)(x)\n        if self.sequential and self.use_vocab and self.stop_words is not None:\n            x = [w for w in x if w not in self.stop_words]\n        if self.preprocessing is not None:\n            return self.preprocessing(x)\n        else:\n            return x", "response": "Load a single example using this field tokenizing if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process(self, batch, device=None):\n        padded = self.pad(batch)\n        tensor = self.numericalize(padded, device=device)\n        return tensor", "response": "Process a list of examples to create a torch. Tensor.\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pad(self, minibatch):\n        minibatch = list(minibatch)\n        if not self.sequential:\n            return minibatch\n        if self.fix_length is None:\n            max_len = max(len(x) for x in minibatch)\n        else:\n            max_len = self.fix_length + (\n                self.init_token, self.eos_token).count(None) - 2\n        padded, lengths = [], []\n        for x in minibatch:\n            if self.pad_first:\n                padded.append(\n                    [self.pad_token] * max(0, max_len - len(x))\n                    + ([] if self.init_token is None else [self.init_token])\n                    + list(x[-max_len:] if self.truncate_first else x[:max_len])\n                    + ([] if self.eos_token is None else [self.eos_token]))\n            else:\n                padded.append(\n                    ([] if self.init_token is None else [self.init_token])\n                    + list(x[-max_len:] if self.truncate_first else x[:max_len])\n                    + ([] if self.eos_token is None else [self.eos_token])\n                    + [self.pad_token] * max(0, max_len - len(x)))\n            lengths.append(len(padded[-1]) - max(0, max_len - len(x)))\n        if self.include_lengths:\n            return (padded, lengths)\n        return padded", "response": "Pads a batch of examples using this field."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconstructs the Vocab object for this field from one or more datasets.", "response": "def build_vocab(self, *args, **kwargs):\n        \"\"\"Construct the Vocab object for this field from one or more datasets.\n\n        Arguments:\n            Positional arguments: Dataset objects or other iterable data\n                sources from which to construct the Vocab object that\n                represents the set of possible values for this field. If\n                a Dataset object is provided, all columns corresponding\n                to this field are used; individual columns can also be\n                provided directly.\n            Remaining keyword arguments: Passed to the constructor of Vocab.\n        \"\"\"\n        counter = Counter()\n        sources = []\n        for arg in args:\n            if isinstance(arg, Dataset):\n                sources += [getattr(arg, name) for name, field in\n                            arg.fields.items() if field is self]\n            else:\n                sources.append(arg)\n        for data in sources:\n            for x in data:\n                if not self.sequential:\n                    x = [x]\n                try:\n                    counter.update(x)\n                except TypeError:\n                    counter.update(chain.from_iterable(x))\n        specials = list(OrderedDict.fromkeys(\n            tok for tok in [self.unk_token, self.pad_token, self.init_token,\n                            self.eos_token] + kwargs.pop('specials', [])\n            if tok is not None))\n        self.vocab = self.vocab_cls(counter, specials=specials, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef numericalize(self, arr, device=None):\n        if self.include_lengths and not isinstance(arr, tuple):\n            raise ValueError(\"Field has include_lengths set to True, but \"\n                             \"input data is not a tuple of \"\n                             \"(data batch, batch lengths).\")\n        if isinstance(arr, tuple):\n            arr, lengths = arr\n            lengths = torch.tensor(lengths, dtype=self.dtype, device=device)\n\n        if self.use_vocab:\n            if self.sequential:\n                arr = [[self.vocab.stoi[x] for x in ex] for ex in arr]\n            else:\n                arr = [self.vocab.stoi[x] for x in arr]\n\n            if self.postprocessing is not None:\n                arr = self.postprocessing(arr, self.vocab)\n        else:\n            if self.dtype not in self.dtypes:\n                raise ValueError(\n                    \"Specified Field dtype {} can not be used with \"\n                    \"use_vocab=False because we do not know how to numericalize it. \"\n                    \"Please raise an issue at \"\n                    \"https://github.com/pytorch/text/issues\".format(self.dtype))\n            numericalization_func = self.dtypes[self.dtype]\n            # It doesn't make sense to explicitly coerce to a numeric type if\n            # the data is sequential, since it's unclear how to coerce padding tokens\n            # to a numeric type.\n            if not self.sequential:\n                arr = [numericalization_func(x) if isinstance(x, six.string_types)\n                       else x for x in arr]\n            if self.postprocessing is not None:\n                arr = self.postprocessing(arr, None)\n\n        var = torch.tensor(arr, dtype=self.dtype, device=device)\n\n        if self.sequential and not self.batch_first:\n            var.t_()\n        if self.sequential:\n            var = var.contiguous()\n\n        if self.include_lengths:\n            return var, lengths\n        return var", "response": "Turn a batch of examples that use this field into a Variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef segment(self, *args):\n        sources = []\n        for arg in args:\n            if isinstance(arg, Dataset):\n                sources += [getattr(arg, name) for name, field in\n                            arg.fields.items() if field is self]\n            else:\n                sources.append(arg)\n        for data in sources:\n            for x in tqdm(data, 'segmenting'):\n                x[:] = self.vocab.segment(x)", "response": "Segment one or more Dataset objects or other mutable sequences to the current subword field."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef preprocess(self, xs):\n        return [self.nesting_field.preprocess(x)\n                for x in super(NestedField, self).preprocess(xs)]", "response": "Preprocess a single example."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pad(self, minibatch):\n        minibatch = list(minibatch)\n        if not self.nesting_field.sequential:\n            return super(NestedField, self).pad(minibatch)\n\n        # Save values of attributes to be monkeypatched\n        old_pad_token = self.pad_token\n        old_init_token = self.init_token\n        old_eos_token = self.eos_token\n        old_fix_len = self.nesting_field.fix_length\n        # Monkeypatch the attributes\n        if self.nesting_field.fix_length is None:\n            max_len = max(len(xs) for ex in minibatch for xs in ex)\n            fix_len = max_len + 2 - (self.nesting_field.init_token,\n                                     self.nesting_field.eos_token).count(None)\n            self.nesting_field.fix_length = fix_len\n        self.pad_token = [self.pad_token] * self.nesting_field.fix_length\n        if self.init_token is not None:\n            # self.init_token = self.nesting_field.pad([[self.init_token]])[0]\n            self.init_token = [self.init_token]\n        if self.eos_token is not None:\n            # self.eos_token = self.nesting_field.pad([[self.eos_token]])[0]\n            self.eos_token = [self.eos_token]\n        # Do padding\n        old_include_lengths = self.include_lengths\n        self.include_lengths = True\n        self.nesting_field.include_lengths = True\n        padded, sentence_lengths = super(NestedField, self).pad(minibatch)\n        padded_with_lengths = [self.nesting_field.pad(ex) for ex in padded]\n        word_lengths = []\n        final_padded = []\n        max_sen_len = len(padded[0])\n        for (pad, lens), sentence_len in zip(padded_with_lengths, sentence_lengths):\n            if sentence_len == max_sen_len:\n                lens = lens\n                pad = pad\n            elif self.pad_first:\n                lens[:(max_sen_len - sentence_len)] = (\n                    [0] * (max_sen_len - sentence_len))\n                pad[:(max_sen_len - sentence_len)] = (\n                    [self.pad_token] * (max_sen_len - sentence_len))\n            else:\n                lens[-(max_sen_len - sentence_len):] = (\n                    [0] * (max_sen_len - sentence_len))\n                pad[-(max_sen_len - sentence_len):] = (\n                    [self.pad_token] * (max_sen_len - sentence_len))\n            word_lengths.append(lens)\n            final_padded.append(pad)\n        padded = final_padded\n\n        # Restore monkeypatched attributes\n        self.nesting_field.fix_length = old_fix_len\n        self.pad_token = old_pad_token\n        self.init_token = old_init_token\n        self.eos_token = old_eos_token\n        self.include_lengths = old_include_lengths\n        if self.include_lengths:\n            return padded, sentence_lengths, word_lengths\n        return padded", "response": "Pads a batch of examples using this field."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconstructs the Vocab object for nesting field and combine it with this field's vocab. Arguments: Positional arguments: Dataset objects or other iterable data sources from which to construct the Vocab object that represents the set of possible values for the nesting field. If a Dataset object is provided, all columns corresponding to this field are used; individual columns can also be provided directly. Remaining keyword arguments: Passed to the constructor of Vocab.", "response": "def build_vocab(self, *args, **kwargs):\n        \"\"\"Construct the Vocab object for nesting field and combine it with this field's vocab.\n\n        Arguments:\n            Positional arguments: Dataset objects or other iterable data\n                sources from which to construct the Vocab object that\n                represents the set of possible values for the nesting field. If\n                a Dataset object is provided, all columns corresponding\n                to this field are used; individual columns can also be\n                provided directly.\n            Remaining keyword arguments: Passed to the constructor of Vocab.\n        \"\"\"\n        sources = []\n        for arg in args:\n            if isinstance(arg, Dataset):\n                sources.extend(\n                    [getattr(arg, name) for name, field in arg.fields.items()\n                     if field is self]\n                )\n            else:\n                sources.append(arg)\n\n        flattened = []\n        for source in sources:\n            flattened.extend(source)\n        old_vectors = None\n        old_unk_init = None\n        old_vectors_cache = None\n        if \"vectors\" in kwargs.keys():\n            old_vectors = kwargs[\"vectors\"]\n            kwargs[\"vectors\"] = None\n        if \"unk_init\" in kwargs.keys():\n            old_unk_init = kwargs[\"unk_init\"]\n            kwargs[\"unk_init\"] = None\n        if \"vectors_cache\" in kwargs.keys():\n            old_vectors_cache = kwargs[\"vectors_cache\"]\n            kwargs[\"vectors_cache\"] = None\n        # just build vocab and does not load vector\n        self.nesting_field.build_vocab(*flattened, **kwargs)\n        super(NestedField, self).build_vocab()\n        self.vocab.extend(self.nesting_field.vocab)\n        self.vocab.freqs = self.nesting_field.vocab.freqs.copy()\n        if old_vectors is not None:\n            self.vocab.load_vectors(old_vectors,\n                                    unk_init=old_unk_init, cache=old_vectors_cache)\n\n        self.nesting_field.vocab = self.vocab"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a padded minibatch into a variable tensor.", "response": "def numericalize(self, arrs, device=None):\n        \"\"\"Convert a padded minibatch into a variable tensor.\n\n        Each item in the minibatch will be numericalized independently and the resulting\n        tensors will be stacked at the first dimension.\n\n        Arguments:\n            arr (List[List[str]]): List of tokenized and padded examples.\n            device (str or torch.device): A string or instance of `torch.device`\n                specifying which device the Variables are going to be created on.\n                If left as default, the tensors will be created on cpu. Default: None.\n        \"\"\"\n        numericalized = []\n        self.nesting_field.include_lengths = False\n        if self.include_lengths:\n            arrs, sentence_lengths, word_lengths = arrs\n\n        for arr in arrs:\n            numericalized_ex = self.nesting_field.numericalize(\n                arr, device=device)\n            numericalized.append(numericalized_ex)\n        padded_batch = torch.stack(numericalized)\n\n        self.nesting_field.include_lengths = True\n        if self.include_lengths:\n            sentence_lengths = \\\n                torch.tensor(sentence_lengths, dtype=self.dtype, device=device)\n            word_lengths = torch.tensor(word_lengths, dtype=self.dtype, device=device)\n            return (padded_batch, sentence_lengths, word_lengths)\n        return padded_batch"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nyielding elements from data in chunks of batch_size.", "response": "def batch(data, batch_size, batch_size_fn=None):\n    \"\"\"Yield elements from data in chunks of batch_size.\"\"\"\n    if batch_size_fn is None:\n        def batch_size_fn(new, count, sofar):\n            return count\n    minibatch, size_so_far = [], 0\n    for ex in data:\n        minibatch.append(ex)\n        size_so_far = batch_size_fn(ex, len(minibatch), size_so_far)\n        if size_so_far == batch_size:\n            yield minibatch\n            minibatch, size_so_far = [], 0\n        elif size_so_far > batch_size:\n            yield minibatch[:-1]\n            minibatch, size_so_far = minibatch[-1:], batch_size_fn(ex, 1, 0)\n    if minibatch:\n        yield minibatch"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pool(data, batch_size, key, batch_size_fn=lambda new, count, sofar: count,\n         random_shuffler=None, shuffle=False, sort_within_batch=False):\n    \"\"\"Sort within buckets, then batch, then shuffle batches.\n\n    Partitions data into chunks of size 100*batch_size, sorts examples within\n    each chunk using sort_key, then batch these examples and shuffle the\n    batches.\n    \"\"\"\n    if random_shuffler is None:\n        random_shuffler = random.shuffle\n    for p in batch(data, batch_size * 100, batch_size_fn):\n        p_batch = batch(sorted(p, key=key), batch_size, batch_size_fn) \\\n            if sort_within_batch \\\n            else batch(p, batch_size, batch_size_fn)\n        if shuffle:\n            for b in random_shuffler(list(p_batch)):\n                yield b\n        else:\n            for b in list(p_batch):\n                yield b", "response": "Yields the examples within a random set of buckets."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef splits(cls, datasets, batch_sizes=None, **kwargs):\n        if batch_sizes is None:\n            batch_sizes = [kwargs.pop('batch_size')] * len(datasets)\n        ret = []\n        for i in range(len(datasets)):\n            train = i == 0\n            ret.append(cls(\n                datasets[i], batch_size=batch_sizes[i], train=train, **kwargs))\n        return tuple(ret)", "response": "Create an iterator object for multiple splits of a dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the examples in the dataset in order sorted or shuffled.", "response": "def data(self):\n        \"\"\"Return the examples in the dataset in order, sorted, or shuffled.\"\"\"\n        if self.sort:\n            xs = sorted(self.dataset, key=self.sort_key)\n        elif self.shuffle:\n            xs = [self.dataset[i] for i in self.random_shuffler(range(len(self.dataset)))]\n        else:\n            xs = self.dataset\n        return xs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef init_epoch(self):\n\n        if self._restored_from_state:\n            self.random_shuffler.random_state = self._random_state_this_epoch\n        else:\n            self._random_state_this_epoch = self.random_shuffler.random_state\n\n        self.create_batches()\n\n        if self._restored_from_state:\n            self._restored_from_state = False\n        else:\n            self._iterations_this_epoch = 0\n\n        if not self.repeat:\n            self.iterations = 0", "response": "Initialize the batch generator for a new epoch."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fromvars(cls, dataset, batch_size, train=None, **kwargs):\n        batch = cls()\n        batch.batch_size = batch_size\n        batch.dataset = dataset\n        batch.fields = dataset.fields.keys()\n        for k, v in kwargs.items():\n            setattr(batch, k, v)\n        return batch", "response": "Create a Batch directly from a number of Variables."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef splits(cls, fields, root=\".data\", train=\"en-ud-tag.v2.train.txt\",\n               validation=\"en-ud-tag.v2.dev.txt\",\n               test=\"en-ud-tag.v2.test.txt\", **kwargs):\n        \"\"\"Downloads and loads the Universal Dependencies Version 2 POS Tagged\n        data.\n        \"\"\"\n\n        return super(UDPOS, cls).splits(\n            fields=fields, root=root, train=train, validation=validation,\n            test=test, **kwargs)", "response": "Downloads and loads the Universal Dependencies Version 2 POS Tagged\n        data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef splits(cls, fields, root=\".data\", train=\"train.txt\",\n               test=\"test.txt\", validation_frac=0.1, **kwargs):\n        \"\"\"Downloads and loads the CoNLL 2000 Chunking dataset.\n        NOTE: There is only a train and test dataset so we use\n              10% of the train set as validation\n        \"\"\"\n\n        train, test = super(CoNLL2000Chunking, cls).splits(\n            fields=fields, root=root, train=train,\n            test=test, separator=' ', **kwargs)\n\n        # HACK: Saving the sort key function as the split() call removes it\n        sort_key = train.sort_key\n\n        # Now split the train set\n        # Force a random seed to make the split deterministic\n        random.seed(0)\n        train, val = train.split(1 - validation_frac, random_state=random.getstate())\n        # Reset the seed\n        random.seed()\n\n        # HACK: Set the sort key\n        train.sort_key = sort_key\n        val.sort_key = sort_key\n\n        return train, val, test", "response": "Downloads and loads the CoNLL 2000 Chunking dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef splits(cls, text_field, label_field, root='.data',\n               train='train', test='test', **kwargs):\n        \"\"\"Create dataset objects for splits of the IMDB dataset.\n\n        Arguments:\n            text_field: The field that will be used for the sentence.\n            label_field: The field that will be used for label data.\n            root: Root dataset storage directory. Default is '.data'.\n            train: The directory that contains the training examples\n            test: The directory that contains the test examples\n            Remaining keyword arguments: Passed to the splits method of\n                Dataset.\n        \"\"\"\n        return super(IMDB, cls).splits(\n            root=root, text_field=text_field, label_field=label_field,\n            train=train, validation=None, test=test, **kwargs)", "response": "Create dataset objects for splits of the IMDB dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates dataset objects for splits of the WikiText - 2 dataset.", "response": "def splits(cls, text_field, root='.data', train='wiki.train.tokens',\n               validation='wiki.valid.tokens', test='wiki.test.tokens',\n               **kwargs):\n        \"\"\"Create dataset objects for splits of the WikiText-2 dataset.\n\n        This is the most flexible way to use the dataset.\n\n        Arguments:\n            text_field: The field that will be used for text data.\n            root: The root directory that the dataset's zip archive will be\n                expanded into; therefore the directory in whose wikitext-2\n                subdirectory the data files will be stored.\n            train: The filename of the train data. Default: 'wiki.train.tokens'.\n            validation: The filename of the validation data, or None to not\n                load the validation set. Default: 'wiki.valid.tokens'.\n            test: The filename of the test data, or None to not load the test\n                set. Default: 'wiki.test.tokens'.\n        \"\"\"\n        return super(WikiText2, cls).splits(\n            root=root, train=train, validation=validation, test=test,\n            text_field=text_field, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an iterator object for splits of the WikiText - 2 dataset.", "response": "def iters(cls, batch_size=32, bptt_len=35, device=0, root='.data',\n              vectors=None, **kwargs):\n        \"\"\"Create iterator objects for splits of the WikiText-2 dataset.\n\n        This is the simplest way to use the dataset, and assumes common\n        defaults for field, vocabulary, and iterator parameters.\n\n        Arguments:\n            batch_size: Batch size.\n            bptt_len: Length of sequences for backpropagation through time.\n            device: Device to create batches on. Use -1 for CPU and None for\n                the currently active GPU device.\n            root: The root directory that the dataset's zip archive will be\n                expanded into; therefore the directory in whose wikitext-2\n                subdirectory the data files will be stored.\n            wv_dir, wv_type, wv_dim: Passed to the Vocab constructor for the\n                text field. The word vectors are accessible as\n                train.dataset.fields['text'].vocab.vectors.\n            Remaining keyword arguments: Passed to the splits method.\n        \"\"\"\n        TEXT = data.Field()\n\n        train, val, test = cls.splits(TEXT, root=root, **kwargs)\n\n        TEXT.build_vocab(train, vectors=vectors)\n\n        return data.BPTTIterator.splits(\n            (train, val, test), batch_size=batch_size, bptt_len=bptt_len,\n            device=device)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef call(self, x, *args):\n        if isinstance(x, list):\n            return [self.convert_token(tok, *args) for tok in x]\n        return self.convert_token(x, *args)", "response": "Applies the convert_token function of the current pipeline\nalid to the input."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a Pipeline to be applied before this processing pipeline.", "response": "def add_before(self, pipeline):\n        \"\"\"Add a Pipeline to be applied before this processing pipeline.\n\n        Arguments:\n            pipeline: The Pipeline or callable to apply before this\n                Pipeline.\n        \"\"\"\n        if not isinstance(pipeline, Pipeline):\n            pipeline = Pipeline(pipeline)\n        self.pipes = pipeline.pipes[:] + self.pipes[:]\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a Pipeline to be applied after this processing pipeline.", "response": "def add_after(self, pipeline):\n        \"\"\"Add a Pipeline to be applied after this processing pipeline.\n\n        Arguments:\n            pipeline: The Pipeline or callable to apply after this\n                Pipeline.\n        \"\"\"\n        if not isinstance(pipeline, Pipeline):\n            pipeline = Pipeline(pipeline)\n        self.pipes = self.pipes[:] + pipeline.pipes[:]\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking that the split ratio argument is not malformed", "response": "def check_split_ratio(split_ratio):\n    \"\"\"Check that the split ratio argument is not malformed\"\"\"\n    valid_ratio = 0.\n    if isinstance(split_ratio, float):\n        # Only the train set relative ratio is provided\n        # Assert in bounds, validation size is zero\n        assert 0. < split_ratio < 1., (\n            \"Split ratio {} not between 0 and 1\".format(split_ratio))\n\n        test_ratio = 1. - split_ratio\n        return (split_ratio, test_ratio, valid_ratio)\n    elif isinstance(split_ratio, list):\n        # A list of relative ratios is provided\n        length = len(split_ratio)\n        assert length == 2 or length == 3, (\n            \"Length of split ratio list should be 2 or 3, got {}\".format(split_ratio))\n\n        # Normalize if necessary\n        ratio_sum = sum(split_ratio)\n        if not ratio_sum == 1.:\n            split_ratio = [float(ratio) / ratio_sum for ratio in split_ratio]\n\n        if length == 2:\n            return tuple(split_ratio + [valid_ratio])\n        return tuple(split_ratio)\n    else:\n        raise ValueError('Split ratio must be float or a list, got {}'\n                         .format(type(split_ratio)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate train - test - valid splits from the instance s examples.", "response": "def split(self, split_ratio=0.7, stratified=False, strata_field='label',\n              random_state=None):\n        \"\"\"Create train-test(-valid?) splits from the instance's examples.\n\n        Arguments:\n            split_ratio (float or List of floats): a number [0, 1] denoting the amount\n                of data to be used for the training split (rest is used for validation),\n                or a list of numbers denoting the relative sizes of train, test and valid\n                splits respectively. If the relative size for valid is missing, only the\n                train-test split is returned. Default is 0.7 (for the train set).\n            stratified (bool): whether the sampling should be stratified.\n                Default is False.\n            strata_field (str): name of the examples Field stratified over.\n                Default is 'label' for the conventional label field.\n            random_state (tuple): the random seed used for shuffling.\n                A return value of `random.getstate()`.\n\n        Returns:\n            Tuple[Dataset]: Datasets for train, validation, and\n            test splits in that order, if the splits are provided.\n        \"\"\"\n        train_ratio, test_ratio, val_ratio = check_split_ratio(split_ratio)\n\n        # For the permutations\n        rnd = RandomShuffler(random_state)\n        if not stratified:\n            train_data, test_data, val_data = rationed_split(self.examples, train_ratio,\n                                                             test_ratio, val_ratio, rnd)\n        else:\n            if strata_field not in self.fields:\n                raise ValueError(\"Invalid field name for strata_field {}\"\n                                 .format(strata_field))\n            strata = stratify(self.examples, strata_field)\n            train_data, test_data, val_data = [], [], []\n            for group in strata:\n                # Stratify each group and add together the indices.\n                group_train, group_test, group_val = rationed_split(group, train_ratio,\n                                                                    test_ratio, val_ratio,\n                                                                    rnd)\n                train_data += group_train\n                test_data += group_test\n                val_data += group_val\n\n        splits = tuple(Dataset(d, self.fields)\n                       for d in (train_data, val_data, test_data) if d)\n\n        # In case the parent sort key isn't none\n        if self.sort_key:\n            for subset in splits:\n                subset.sort_key = self.sort_key\n        return splits"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndownloading and unzip an online archive.", "response": "def download(cls, root, check=None):\n        \"\"\"Download and unzip an online archive (.zip, .gz, or .tgz).\n\n        Arguments:\n            root (str): Folder to download data to.\n            check (str or None): Folder whose existence indicates\n                that the dataset has already been downloaded, or\n                None to check the existence of root/{cls.name}.\n\n        Returns:\n            str: Path to extracted dataset.\n        \"\"\"\n        path = os.path.join(root, cls.name)\n        check = path if check is None else check\n        if not os.path.isdir(check):\n            for url in cls.urls:\n                if isinstance(url, tuple):\n                    url, filename = url\n                else:\n                    filename = os.path.basename(url)\n                zpath = os.path.join(path, filename)\n                if not os.path.isfile(zpath):\n                    if not os.path.exists(os.path.dirname(zpath)):\n                        os.makedirs(os.path.dirname(zpath))\n                    print('downloading {}'.format(filename))\n                    download_from_url(url, zpath)\n                zroot, ext = os.path.splitext(zpath)\n                _, ext_inner = os.path.splitext(zroot)\n                if ext == '.zip':\n                    with zipfile.ZipFile(zpath, 'r') as zfile:\n                        print('extracting')\n                        zfile.extractall(path)\n                # tarfile cannot handle bare .gz files\n                elif ext == '.tgz' or ext == '.gz' and ext_inner == '.tar':\n                    with tarfile.open(zpath, 'r:gz') as tar:\n                        dirs = [member for member in tar.getmembers()]\n                        tar.extractall(path=path, members=dirs)\n                elif ext == '.gz':\n                    with gzip.open(zpath, 'rb') as gz:\n                        with open(zroot, 'wb') as uncompressed:\n                            shutil.copyfileobj(gz, uncompressed)\n\n        return os.path.join(path, cls.dirname)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filter_examples(self, field_names):\n        for i, example in enumerate(self.examples):\n            for field_name in field_names:\n                vocab = set(self.fields[field_name].vocab.stoi)\n                text = getattr(example, field_name)\n                example_part = [word for word in text if word in vocab]\n                setattr(example, field_name, example_part)\n            self.examples[i] = example", "response": "Remove unknown words from dataset examples with respect to given field."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef splits(cls, text_field, label_field, parse_field=None,\n               extra_fields={}, root='.data', train='train.jsonl',\n               validation='val.jsonl', test='test.jsonl'):\n        \"\"\"Create dataset objects for splits of the SNLI dataset.\n\n        This is the most flexible way to use the dataset.\n\n        Arguments:\n            text_field: The field that will be used for premise and hypothesis\n                data.\n            label_field: The field that will be used for label data.\n            parse_field: The field that will be used for shift-reduce parser\n                transitions, or None to not include them.\n            extra_fields: A dict[json_key: Tuple(field_name, Field)]\n            root: The root directory that the dataset's zip archive will be\n                expanded into.\n            train: The filename of the train data. Default: 'train.jsonl'.\n            validation: The filename of the validation data, or None to not\n                load the validation set. Default: 'dev.jsonl'.\n            test: The filename of the test data, or None to not load the test\n                set. Default: 'test.jsonl'.\n        \"\"\"\n        path = cls.download(root)\n\n        if parse_field is None:\n            fields = {'sentence1': ('premise', text_field),\n                      'sentence2': ('hypothesis', text_field),\n                      'gold_label': ('label', label_field)}\n        else:\n            fields = {'sentence1_binary_parse': [('premise', text_field),\n                                                 ('premise_transitions', parse_field)],\n                      'sentence2_binary_parse': [('hypothesis', text_field),\n                                                 ('hypothesis_transitions', parse_field)],\n                      'gold_label': ('label', label_field)}\n\n        for key in extra_fields:\n            if key not in fields.keys():\n                fields[key] = extra_fields[key]\n\n        return super(NLIDataset, cls).splits(\n            path, root, train, validation, test,\n            format='json', fields=fields,\n            filter_pred=lambda ex: ex.label != '-')", "response": "Create a new dataset object for splits of the SNLI dataset."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef iters(cls, batch_size=32, device=0, root='.data',\n              vectors=None, trees=False, **kwargs):\n        \"\"\"Create iterator objects for splits of the SNLI dataset.\n\n        This is the simplest way to use the dataset, and assumes common\n        defaults for field, vocabulary, and iterator parameters.\n\n        Arguments:\n            batch_size: Batch size.\n            device: Device to create batches on. Use -1 for CPU and None for\n                the currently active GPU device.\n            root: The root directory that the dataset's zip archive will be\n                expanded into; therefore the directory in whose wikitext-2\n                subdirectory the data files will be stored.\n            vectors: one of the available pretrained vectors or a list with each\n                element one of the available pretrained vectors (see Vocab.load_vectors)\n            trees: Whether to include shift-reduce parser transitions.\n                Default: False.\n            Remaining keyword arguments: Passed to the splits method.\n        \"\"\"\n        if trees:\n            TEXT = ParsedTextField()\n            TRANSITIONS = ShiftReduceField()\n        else:\n            TEXT = data.Field(tokenize='spacy')\n            TRANSITIONS = None\n        LABEL = data.Field(sequential=False)\n\n        train, val, test = cls.splits(\n            TEXT, LABEL, TRANSITIONS, root=root, **kwargs)\n\n        TEXT.build_vocab(train, vectors=vectors)\n        LABEL.build_vocab(train)\n\n        return data.BucketIterator.splits(\n            (train, val, test), batch_size=batch_size, device=device)", "response": "Create iterator objects for splits of the SNLI dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bytes_to_unicode(byte_data, escape, skip_printable=False):\n    if isnumber(byte_data):\n        if skip_printable:\n            return num2unistr(byte_data)\n        else:\n            byte_data = num2bytes(byte_data)\n    else:\n        assert (isinstance(byte_data, type(b'')))\n        if skip_printable and all(0x20 <= bval(ch) <= 0x7E for ch in byte_data):\n            escape = STRING_ESCAPE_RAW\n\n    if escape == STRING_ESCAPE_RAW:\n        return byte_data.decode('latin-1')\n    elif escape == STRING_ESCAPE_PRINT:\n        return escape_ascii(byte_data)\n    elif escape == STRING_ESCAPE_UTF8:\n        return escape_utf8(byte_data)\n    elif escape == STRING_ESCAPE_BASE64:\n        return codecs.decode(base64.b64encode(byte_data), 'latin-1')\n    else:\n        raise UnicodeEncodeError(\"Unknown escape option\")", "response": "Decode given bytes using specified escaping method."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef apply_escape_bytes(byte_data, escape, skip_printable=False):\n\n    if isnumber(byte_data):\n        if skip_printable:\n            return num2bytes(byte_data)\n        else:\n            byte_data = num2bytes(byte_data)\n    else:\n        assert (isinstance(byte_data, type(b'')))\n        if skip_printable and all(0x20 <= bval(ch) <= 0x7E for ch in byte_data):\n            escape = STRING_ESCAPE_RAW\n\n    if escape == STRING_ESCAPE_RAW:\n        return byte_data\n    elif escape == STRING_ESCAPE_PRINT:\n        return escape_ascii_bytes(byte_data)\n    elif escape == STRING_ESCAPE_UTF8:\n        return codecs.encode(escape_utf8(byte_data), 'utf-8')\n    elif escape == STRING_ESCAPE_BASE64:\n        return base64.b64encode(byte_data)\n    else:\n        raise UnicodeEncodeError(\"Unknown escape option\")", "response": "Apply the specified escape method on the given byte - like object with bytes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecode module id to string based on @antirez moduleTypeNameByID function from redis / src / module. c", "response": "def _decode_module_id(self, module_id):\n        \"\"\"\n        decode module id to string\n        based on @antirez moduleTypeNameByID function from redis/src/module.c\n        :param module_id: 64bit integer\n        :return: string\n        \"\"\"\n        name = [''] * 9\n        module_id >>= 10\n        for i in reversed(range(9)):\n            name[i] = self.charset[module_id & 63]\n            module_id >>= 6\n        return ''.join(name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the step value in format suitable for display.", "response": "def format_step(step, zero_prefix=False):\n    \"\"\"Return the step value in format suitable for display.\"\"\"\n    if isinstance(step, int):\n        return \"{:06}\".format(step) if zero_prefix else \"{}\".format(step)\n    elif isinstance(step, tuple):\n        return \"{:04}:{:06}\".format(*step) if zero_prefix else \"{}:{}\".format(*step)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef log(self, step, **kwargs):\n        assert isinstance(step, (int, tuple)), \"Step must be an int or a tuple of two ints\"\n        self.step = step\n        # Any new metrics we haven't seen before?\n        self.metrics |= set(kwargs.keys())\n        # Insert (or update) record of the step\n        if step not in self.history:\n            self.history[step] = {}\n        self.history[step].update({k:utils.to_data(v) for k, v in kwargs.items()})\n        # Update step timestamp\n        self.history[step][\"__timestamp__\"] = time.time()", "response": "Record the metrics at a specific step."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the total time between when the first and last steps where logged. This usually correspondspnods to the total training time where the training time is logged.", "response": "def get_total_time(self):\n        \"\"\"Returns the total period between when the first and last steps\n        where logged. This usually correspnods to the total training time\n        if there were no gaps in the training.\n        \"\"\"\n        first_step = self.steps[0]\n        last_step = self.steps[-1]\n        seconds = self.history[last_step][\"__timestamp__\"] \\\n                  - self.history[first_step][\"__timestamp__\"]\n        return datetime.timedelta(seconds=seconds)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef outgoing(self, node):\n        nodes = node if isinstance(node, list) else [node]\n        node_ids = [self.id(n) for n in nodes]\n        # Find edges outgoing from this group but not incoming to it\n        outgoing = [self[e[1]] for e in self.edges\n                    if e[0] in node_ids and e[1] not in node_ids]\n        return outgoing", "response": "Returns nodes connecting out of the given node or list of nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef siblings(self, node):\n        incoming = self.incoming(node)\n        # TODO: Not handling the case of multiple incoming nodes yet\n        if len(incoming) == 1:\n            incoming = incoming[0]\n            siblings = self.outgoing(incoming)\n            return siblings\n        else:\n            return [node]", "response": "Returns all nodes that share the same parent with\n        the given node including the itself."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove(self, nodes):\n        nodes = nodes if isinstance(nodes, list) else [nodes]\n        for node in nodes:\n            k = self.id(node)\n            self.edges = list(filter(lambda e: e[0] != k and e[1] != k, self.edges))\n            del self.nodes[k]", "response": "Remove a node and its edges."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef search(self, pattern):\n        for node in self.nodes.values():\n            match, following = pattern.match(self, node)\n            if match:\n                return match, following\n        return [], None", "response": "Searches the graph for a sub - graph that matches the given pattern\n       . Returns a list of the first match it finds."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake up an ID for a sequence of nodes.", "response": "def sequence_id(self, sequence):\n        \"\"\"Make up an ID for a sequence (list) of nodes.\n        Note: `getrandbits()` is very uninformative as a \"readable\" ID. Here, we build a name\n        such that when the mouse hovers over the drawn node in Jupyter, one can figure out\n        which original nodes make up the sequence. This is actually quite useful.\n        \"\"\"\n        if self.meaningful_ids:\n            # TODO: This might fail if the ID becomes too long\n            return \"><\".join([node.id for node in sequence])\n        else:\n            return getrandbits(64)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_dot(self):\n        from graphviz import Digraph\n\n        # Build GraphViz Digraph\n        dot = Digraph()\n        dot.attr(\"graph\", \n                 bgcolor=self.theme[\"background_color\"],\n                 color=self.theme[\"outline_color\"],\n                 fontsize=self.theme[\"font_size\"],\n                 fontcolor=self.theme[\"font_color\"],\n                 fontname=self.theme[\"font_name\"],\n                 margin=self.theme[\"margin\"],\n                 rankdir=\"LR\",\n                 pad=self.theme[\"padding\"])\n        dot.attr(\"node\", shape=\"box\", \n                 style=\"filled\", margin=\"0,0\",\n                 fillcolor=self.theme[\"fill_color\"],\n                 color=self.theme[\"outline_color\"],\n                 fontsize=self.theme[\"font_size\"],\n                 fontcolor=self.theme[\"font_color\"],\n                 fontname=self.theme[\"font_name\"])\n        dot.attr(\"edge\", style=\"solid\", \n                 color=self.theme[\"outline_color\"],\n                 fontsize=self.theme[\"font_size\"],\n                 fontcolor=self.theme[\"font_color\"],\n                 fontname=self.theme[\"font_name\"])\n\n        for k, n in self.nodes.items():\n            label = \"<tr><td cellpadding='6'>{}</td></tr>\".format(n.title)\n            if n.caption:\n                label += \"<tr><td>{}</td></tr>\".format(n.caption)\n            if n.repeat > 1:\n                label += \"<tr><td align='right' cellpadding='2'>x{}</td></tr>\".format(n.repeat)\n            label = \"<<table border='0' cellborder='0' cellpadding='0'>\" + label + \"</table>>\"\n            dot.node(str(k), label)\n        for a, b, label in self.edges:\n            if isinstance(label, (list, tuple)):\n                label = \"x\".join([str(l or \"?\") for l in label])\n\n            dot.edge(str(a), str(b), label)\n        return dot", "response": "Generate a GraphViz Dot object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_data(value):\n    # TODO: Use get_framework() for better detection.\n    if value.__class__.__module__.startswith(\"torch\"):\n        import torch\n        if isinstance(value, torch.nn.parameter.Parameter):\n            value = value.data\n        if isinstance(value, torch.Tensor):\n            if value.requires_grad:\n                value = value.detach()\n            value = value.cpu().numpy().copy()\n        # If 0-dim array, convert to scalar\n        if not value.shape:\n            value = value.item()\n    # Convert Numpy scalar types to Python types\n    if value.__class__.__module__ == \"numpy\" and value.__class__.__name__ != \"ndarray\":\n        value = value.item()\n    return value", "response": "Standardize data types. Converts PyTorch tensors to Numpy arrays and Numpy scalars to Python scalars."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nliking print but recognizes tensors and arrays and show more details about them.", "response": "def write(*args):\n    \"\"\"Like print(), but recognizes tensors and arrays and show\n    more details about them.\n\n    Example:\n        hl.write(\"My Tensor\", my_tensor)\n    \n        Prints:\n            My Tensor  float32 (10, 3, 224, 224)  min: 0.0  max: 1.0\n    \"\"\"\n    s = \"\"\n    for a in args:\n        # Convert tensors to Numpy arrays\n        a = to_data(a)\n\n        if isinstance(a, np.ndarray):\n            # Numpy Array\n            s += (\"\\t\" if s else \"\") + \"Tensor  {} {}  min: {:.3f}  max: {:.3f}\".format(\n                a.dtype, a.shape, a.min(), a.max())\n            print(s)\n            s = \"\"\n        elif isinstance(a, list):\n            s += (\"\\t\" if s else \"\") + \"list    len: {}  {}\".format(len(a), a[:10])\n        else:\n            s += (\" \" if s else \"\") + str(a)\n    if s:\n        print(s)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nnormalizes an image to [ 0 1 ) range.", "response": "def norm(image):\n    \"\"\"Normalize an image to [0, 1] range.\"\"\"\n    min_value = image.min()\n    max_value = image.max()\n    if min_value == max_value:\n        return image - min_value\n    return (image - min_value) / (max_value - min_value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef show_images(images, titles=None, cols=5, **kwargs):\n    # The images param can be a list or an array\n\n    titles = titles or [\"\"] * len(images)\n    rows = math.ceil(len(images) / cols)\n    height_ratio = 1.2 * (rows/cols) * (0.5 if type(images[0]) is not np.ndarray else 1)\n    plt.figure(figsize=(11, 11 * height_ratio))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.axis(\"off\")\n        # Is image a list? If so, merge them into one image.\n        if type(image) is not np.ndarray:\n            image = [norm(g) for g in image]\n            image = np.concatenate(image, axis=1)\n        else:\n            image = norm(image)\n        plt.title(title, fontsize=9)\n        plt.imshow(image, cmap=\"Greys_r\", **kwargs)\n        i += 1\n    plt.tight_layout(h_pad=0, w_pad=0)", "response": "Show the images in a single order tree structure."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef draw_summary(self, history, title=\"\"):\n        # Generate summary string\n        time_str = str(history.get_total_time()).split(\".\")[0]  # remove microseconds\n        summary = \"Step: {}      Time: {}\".format(history.step, time_str)\n        if title:\n            summary = title + \"\\n\\n\" + summary\n        self.figure.suptitle(summary)", "response": "Inserts a text summary at the top that lists the number of steps and total\n        training time."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef draw_plot(self, metrics, labels=None, ylabel=\"\"):\n        metrics = metrics if isinstance(metrics, list) else [metrics]\n        # Loop through metrics\n        title = \"\"\n        for i, m in enumerate(metrics):\n            label = labels[i] if labels else m.name\n            # TODO: use a standard formating function for values\n            title += (\"   \" if title else \"\") + \"{}: {}\".format(label, m.data[-1])\n            self.ax.plot(m.formatted_steps, m.data, label=label)\n        self.ax.set_title(title)\n        self.ax.set_ylabel(ylabel)\n        self.ax.legend()\n        self.ax.set_xlabel(\"Steps\")\n        self.ax.xaxis.set_major_locator(plt.AutoLocator())", "response": "Draws the history of the entries in the given metrics."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndisplay a series of images at different time steps.", "response": "def draw_image(self, metric, limit=5):\n        \"\"\"Display a series of images at different time steps.\"\"\"\n        rows = 1\n        cols = limit\n        self.ax.axis(\"off\")\n        # Take the Axes gridspec and divide it into a grid\n        gs = matplotlib.gridspec.GridSpecFromSubplotSpec(\n            rows, cols, subplot_spec=self.gs)\n        # Loop through images in last few steps\n        for i, image in enumerate(metric.data[-cols:]):\n            ax = self.figure.add_subplot(gs[0, i])\n            ax.axis('off')\n            ax.set_title(metric.formatted_steps[-cols:][i])\n            ax.imshow(norm(image))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef draw_hist(self, metric, title=\"\"):\n        # TODO: assert isinstance(list(values.values())[0], np.ndarray)\n\n        rows = 1\n        cols = 1\n        limit = 10  # max steps to show\n\n        # We need a 3D projection Subplot, so ignore the one provided to\n        # as an create a new one.\n        ax = self.figure.add_subplot(self.gs, projection=\"3d\")\n        ax.view_init(30, -80)\n\n        # Compute histograms\n        verts = []\n        area_colors = []\n        edge_colors = []\n        for i, s in enumerate(metric.steps[-limit:]):\n            hist, edges = np.histogram(metric.data[-i-1:])\n            # X is bin centers\n            x = np.diff(edges)/2 + edges[:-1]\n            # Y is hist values\n            y = hist\n            x = np.concatenate([x[0:1], x, x[-1:]])\n            y = np.concatenate([[0], y, [0]])\n\n            # Ranges\n            if i == 0:\n                x_min = x.min()\n                x_max = x.max()\n                y_min = y.min()\n                y_max = y.max()\n            x_min = np.minimum(x_min, x.min())\n            x_max = np.maximum(x_max, x.max())\n            y_min = np.minimum(y_min, y.min())\n            y_max = np.maximum(y_max, y.max())\n\n            alpha = 0.8 * (i+1) / min(limit, len(metric.steps))\n            verts.append(list(zip(x, y)))\n            area_colors.append(np.array(self.theme[\"hist_color\"] + [alpha]))\n            edge_colors.append(np.array(self.theme[\"hist_outline_color\"] + [alpha]))\n\n        poly = PolyCollection(verts, facecolors=area_colors, edgecolors=edge_colors)\n        ax.add_collection3d(poly, zs=list(range(min(limit, len(metric.steps)))), zdir='y')\n\n        ax.set_xlim(x_min, x_max)\n        ax.set_ylim(0, limit)\n        ax.set_yticklabels(metric.formatted_steps[-limit:])\n        ax.set_zlim(y_min, y_max)\n        ax.set_title(metric.name)", "response": "Draw a series of histograms over the selected keys over different training steps."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _load(self, dataset='train'):\n        data, labels = None, None\n        if dataset is 'train':\n            files = [os.path.join(self.cifar10_dir, 'data_batch_%d' % i) for i in range(1, 6)]\n        else:\n            files = [os.path.join(self.cifar10_dir, 'test_batch')]\n\n        for file in files:\n            if not os.path.exists(file):\n                raise FileNotFoundError('Failed to find file: ' + file)\n\n        # Load the data from the batch files\n        for file in files:\n            with open(file, 'rb') as f:\n                cifar10 = pickle.load(f, encoding='latin1')\n\n            if labels is None:\n                labels = np.array(cifar10['labels'])\n            else:\n                labels = np.concatenate((labels, cifar10['labels']), axis=0)\n\n            if data is None:\n                data = cifar10['data']\n            else:\n                data = np.concatenate((data, cifar10['data']), axis=0)\n\n        # Adapt the format of the data to our convnet\n        data = np.array(data, dtype=float) / 255.0\n        data = data.reshape([-1, self.num_channels, self.img_size, self.img_size])\n        data = data.transpose([0, 2, 3, 1])\n\n        # One-hot encode labels (see https://stackoverflow.com/a/42874726)\n        labels = np.eye(self.num_classes)[np.array(labels).reshape(-1)]\n\n        if dataset is 'train':\n            self._train_data, self._train_labels = data, labels\n        else:\n            self._test_data, self._test_labels = data, labels", "response": "Loads the data in memory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild a simple convnet with the last op containing the predictions.", "response": "def model(self, inputs, mode='train'):\n        \"\"\"Build a simple convnet (BN before ReLU).\n        Args:\n            inputs: a tensor of size [batch_size, height, width, channels]\n            mode: string in ['train', 'test']\n        Returns:\n            the last op containing the predictions\n        Note:\n            Best score\n            Step:  7015 - Epoch: 18/20 - best batch acc: 0.8984 - loss: 1.5656\n            Worst score\n            Step:  7523 - Epoch: 20/20 - best batch acc: 0.7734 - loss: 1.6874\n        \"\"\"\n        # Extract features\n        training = (mode == 'train')\n        with tf.variable_scope('conv1') as scope:\n            conv = tf.layers.conv2d(inputs=inputs, filters=16, kernel_size=[3, 3], padding='SAME')\n            bn = tf.layers.batch_normalization(inputs=conv, training=training)\n            bn = tf.nn.relu(bn)\n            conv = tf.layers.conv2d(inputs=bn, filters=16, kernel_size=[3, 3], padding='SAME')\n            bn = tf.layers.batch_normalization(inputs=conv, training=training)\n            bn = tf.nn.relu(bn)\n            pool = tf.layers.max_pooling2d(bn, pool_size=[2, 2], strides=2, padding='SAME', name=scope.name)\n\n        with tf.variable_scope('conv2') as scope:\n            conv = tf.layers.conv2d(inputs=pool, filters=32, kernel_size=[3, 3], padding='SAME')\n            bn = tf.layers.batch_normalization(inputs=conv, training=training)\n            bn = tf.nn.relu(bn)\n            conv = tf.layers.conv2d(inputs=bn, filters=32, kernel_size=[3, 3], padding='SAME')\n            bn = tf.layers.batch_normalization(inputs=conv, training=training)\n            bn = tf.nn.relu(bn)\n            pool = tf.layers.max_pooling2d(bn, pool_size=[2, 2], strides=2, padding='SAME', name=scope.name)\n\n        with tf.variable_scope('conv3') as scope:\n            conv = tf.layers.conv2d(inputs=pool, filters=32, kernel_size=[3, 3], padding='SAME')\n            bn = tf.layers.batch_normalization(inputs=conv, training=training)\n            bn = tf.nn.relu(bn)\n            conv = tf.layers.conv2d(inputs=bn, filters=32, kernel_size=[3, 3], padding='SAME')\n            bn = tf.layers.batch_normalization(inputs=conv, training=training)\n            bn = tf.nn.relu(bn)\n            pool = tf.layers.max_pooling2d(bn, pool_size=[2, 2], strides=2, padding='SAME', name=scope.name)\n\n        # Classify\n        with tf.variable_scope('fc') as scope:\n            flat = tf.layers.flatten(pool)\n            fc = tf.layers.dense(inputs=flat, units=32, activation=tf.nn.relu)\n            softmax = tf.layers.dense(inputs=fc, units=self.num_classes, activation=tf.nn.softmax)\n\n        return softmax"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef maybe_download_and_extract(self):\n        if not os.path.exists(self.cifar10_dir):\n\n            if not os.path.exists(self.data_dir):\n                os.makedirs(self.data_dir)\n\n            def _progress(count, block_size, total_size):\n                status_msg = '\\r>> Downloading {} {:>3}%   '\n                sys.stdout.write(status_msg.format(self.cifar10_tarball, float(count * block_size) / total_size * 100.0))\n                sys.stdout.flush()\n\n            file_path, _ = urlretrieve(CIFAR10_URL, self.cifar10_tarball, _progress)\n\n            stat_info = os.stat(file_path)\n            print('\\nSuccessfully downloaded', file_path, stat_info.st_size, 'bytes.\\n')\n\n            tarfile.open(file_path, 'r:gz').extractall(self.data_dir)", "response": "Download and extract the tarball from Alex Krizhevsky s website."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dump_pytorch_graph(graph):\n    f = \"{:25} {:40}   {} -> {}\"\n    print(f.format(\"kind\", \"scopeName\", \"inputs\", \"outputs\"))\n    for node in graph.nodes():\n        print(f.format(node.kind(), node.scopeName(),\n                       [i.unique() for i in node.inputs()],\n                       [i.unique() for i in node.outputs()]\n                       ))", "response": "List all the nodes in a PyTorch graph."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pytorch_id(node):\n    # After ONNX simplification, the scopeName is not unique anymore\n    # so append node outputs to guarantee uniqueness\n    return node.scopeName() + \"/outputs/\" + \"/\".join([o.uniqueName() for o in node.outputs()])", "response": "Returns a unique ID for a node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the output shape of the given Pytorch node.", "response": "def get_shape(torch_node):\n    \"\"\"Return the output shape of the given Pytorch node.\"\"\"\n    # Extract node output shape from the node string representation\n    # This is a hack because there doesn't seem to be an official way to do it.\n    # See my quesiton in the PyTorch forum:\n    # https://discuss.pytorch.org/t/node-output-shape-from-trace-graph/24351/2\n    # TODO: find a better way to extract output shape\n    # TODO: Assuming the node has one output. Update if we encounter a multi-output node.\n    m = re.match(r\".*Float\\(([\\d\\s\\,]+)\\).*\", str(next(torch_node.outputs())))\n    if m:\n        shape = m.group(1)\n        shape = shape.split(\",\")\n        shape = tuple(map(int, shape))\n    else:\n        shape = None\n    return shape"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dump_tf_graph(tfgraph, tfgraphdef):\n    print(\"Nodes ({})\".format(len(tfgraphdef.node)))\n    f = \"{:15} {:59} {:20} {}\"\n    print(f.format(\"kind\", \"scopeName\", \"shape\", \"inputs\"))\n    for node in tfgraphdef.node:\n        scopename = node.name\n        kind = node.op\n        inputs = node.input\n        shape = tf.graph_util.tensor_shape_from_node_def_name(tfgraph, scopename)\n        print(f.format(kind, scopename, str(shape), inputs))", "response": "Dump a TF graph to stdout."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a TF graph to a directed graph.", "response": "def import_graph(hl_graph, tf_graph, output=None, verbose=False):\n    \"\"\"Convert TF graph to directed graph\n    tfgraph: A TF Graph object.\n    output: Name of the output node (string).\n    verbose: Set to True for debug print output\n    \"\"\"\n    # Get clean(er) list of nodes\n    graph_def = tf_graph.as_graph_def(add_shapes=True)\n    graph_def = tf.graph_util.remove_training_nodes(graph_def)\n\n    # Dump list of TF nodes (DEBUG only)\n    if verbose:\n        dump_tf_graph(tf_graph, graph_def)\n\n    # Loop through nodes and build the matching directed graph\n    for tf_node in graph_def.node:\n        # Read node details\n        try:\n            op,  uid, name, shape, params = import_node(tf_node, tf_graph, verbose)\n        except:\n            if verbose:\n                logging.exception(\"Failed to read node {}\".format(tf_node))\n            continue\n\n        # Add node\n        hl_node = Node(uid=uid, name=name, op=op, output_shape=shape, params=params)\n        hl_graph.add_node(hl_node)\n\n        # Add edges\n        for target_node in graph_def.node:\n            target_inputs = target_node.input\n            if uid in target_node.input:\n                hl_graph.add_edge_by_id(uid, target_node.name, shape)\n    return hl_graph"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending FetchRequests for all assigned partitions that do not already have an in - flight fetch or pending fetch data. Returns a list of Futures that resolves to a FetchResponse", "response": "def send_fetches(self):\n        \"\"\"Send FetchRequests for all assigned partitions that do not already have\n        an in-flight fetch or pending fetch data.\n\n        Returns:\n            List of Futures: each future resolves to a FetchResponse\n        \"\"\"\n        futures = []\n        for node_id, request in six.iteritems(self._create_fetch_requests()):\n            if self._client.ready(node_id):\n                log.debug(\"Sending FetchRequest to node %s\", node_id)\n                future = self._client.send(node_id, request)\n                future.add_callback(self._handle_fetch_response, request, time.time())\n                future.add_errback(log.error, 'Fetch to node %s failed: %s', node_id)\n                futures.append(future)\n        self._fetch_futures.extend(futures)\n        self._clean_done_fetch_futures()\n        return futures"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresets offsets for any partitions which are awaiting an explicit reset.", "response": "def reset_offsets_if_needed(self, partitions):\n        \"\"\"Lookup and set offsets for any partitions which are awaiting an\n        explicit reset.\n\n        Arguments:\n            partitions (set of TopicPartitions): the partitions to reset\n        \"\"\"\n        for tp in partitions:\n            # TODO: If there are several offsets to reset, we could submit offset requests in parallel\n            if self._subscriptions.is_assigned(tp) and self._subscriptions.is_offset_reset_needed(tp):\n                self._reset_offset(tp)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the fetch positions for the provided partitions.", "response": "def update_fetch_positions(self, partitions):\n        \"\"\"Update the fetch positions for the provided partitions.\n\n        Arguments:\n            partitions (list of TopicPartitions): partitions to update\n\n        Raises:\n            NoOffsetForPartitionError: if no offset is stored for a given\n                partition and no reset policy is available\n        \"\"\"\n        # reset the fetch position to the committed position\n        for tp in partitions:\n            if not self._subscriptions.is_assigned(tp):\n                log.warning(\"partition %s is not assigned - skipping offset\"\n                            \" update\", tp)\n                continue\n            elif self._subscriptions.is_fetchable(tp):\n                log.warning(\"partition %s is still fetchable -- skipping offset\"\n                            \" update\", tp)\n                continue\n\n            if self._subscriptions.is_offset_reset_needed(tp):\n                self._reset_offset(tp)\n            elif self._subscriptions.assignment[tp].committed is None:\n                # there's no committed position, so we need to reset with the\n                # default strategy\n                self._subscriptions.need_offset_reset(tp)\n                self._reset_offset(tp)\n            else:\n                committed = self._subscriptions.assignment[tp].committed\n                log.debug(\"Resetting offset for partition %s to the committed\"\n                          \" offset %s\", tp, committed)\n                self._subscriptions.seek(tp, committed)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreset the offset for the given topic partition.", "response": "def _reset_offset(self, partition):\n        \"\"\"Reset offsets for the given partition using the offset reset strategy.\n\n        Arguments:\n            partition (TopicPartition): the partition that needs reset offset\n\n        Raises:\n            NoOffsetForPartitionError: if no offset reset strategy is defined\n        \"\"\"\n        timestamp = self._subscriptions.assignment[partition].reset_strategy\n        if timestamp is OffsetResetStrategy.EARLIEST:\n            strategy = 'earliest'\n        elif timestamp is OffsetResetStrategy.LATEST:\n            strategy = 'latest'\n        else:\n            raise NoOffsetForPartitionError(partition)\n\n        log.debug(\"Resetting offset for partition %s to %s offset.\",\n                  partition, strategy)\n        offsets = self._retrieve_offsets({partition: timestamp})\n        if partition not in offsets:\n            raise NoOffsetForPartitionError(partition)\n        offset = offsets[partition][0]\n\n        # we might lose the assignment while fetching the offset,\n        # so check it is still active\n        if self._subscriptions.is_assigned(partition):\n            self._subscriptions.seek(partition, offset)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching offsets for each topic partition passed in timestamps.", "response": "def _retrieve_offsets(self, timestamps, timeout_ms=float(\"inf\")):\n        \"\"\"Fetch offset for each partition passed in ``timestamps`` map.\n\n        Blocks until offsets are obtained, a non-retriable exception is raised\n        or ``timeout_ms`` passed.\n\n        Arguments:\n            timestamps: {TopicPartition: int} dict with timestamps to fetch\n                offsets by. -1 for the latest available, -2 for the earliest\n                available. Otherwise timestamp is treated as epoch miliseconds.\n\n        Returns:\n            {TopicPartition: (int, int)}: Mapping of partition to\n                retrieved offset and timestamp. If offset does not exist for\n                the provided timestamp, that partition will be missing from\n                this mapping.\n        \"\"\"\n        if not timestamps:\n            return {}\n\n        start_time = time.time()\n        remaining_ms = timeout_ms\n        while remaining_ms > 0:\n            future = self._send_offset_requests(timestamps)\n            self._client.poll(future=future, timeout_ms=remaining_ms)\n\n            if future.succeeded():\n                return future.value\n            if not future.retriable():\n                raise future.exception  # pylint: disable-msg=raising-bad-type\n\n            elapsed_ms = (time.time() - start_time) * 1000\n            remaining_ms = timeout_ms - elapsed_ms\n            if remaining_ms < 0:\n                break\n\n            if future.exception.invalid_metadata:\n                refresh_future = self._client.cluster.request_update()\n                self._client.poll(future=refresh_future, timeout_ms=remaining_ms)\n            else:\n                time.sleep(self.config['retry_backoff_ms'] / 1000.0)\n\n            elapsed_ms = (time.time() - start_time) * 1000\n            remaining_ms = timeout_ms - elapsed_ms\n\n        raise Errors.KafkaTimeoutError(\n            \"Failed to get offsets by timestamps in %s ms\" % (timeout_ms,))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fetched_records(self, max_records=None):\n        if max_records is None:\n            max_records = self.config['max_poll_records']\n        assert max_records > 0\n\n        drained = collections.defaultdict(list)\n        records_remaining = max_records\n\n        while records_remaining > 0:\n            if not self._next_partition_records:\n                if not self._completed_fetches:\n                    break\n                completion = self._completed_fetches.popleft()\n                self._next_partition_records = self._parse_fetched_data(completion)\n            else:\n                records_remaining -= self._append(drained,\n                                                  self._next_partition_records,\n                                                  records_remaining)\n        return dict(drained), bool(self._completed_fetches)", "response": "Returns previously fetched records and updates consumed offsets."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _message_generator(self):\n        while self._next_partition_records or self._completed_fetches:\n\n            if not self._next_partition_records:\n                completion = self._completed_fetches.popleft()\n                self._next_partition_records = self._parse_fetched_data(completion)\n                continue\n\n            # Send additional FetchRequests when the internal queue is low\n            # this should enable moderate pipelining\n            if len(self._completed_fetches) <= self.config['iterator_refetch_records']:\n                self.send_fetches()\n\n            tp = self._next_partition_records.topic_partition\n\n            # We can ignore any prior signal to drop pending message sets\n            # because we are starting from a fresh one where fetch_offset == position\n            # i.e., the user seek()'d to this position\n            self._subscriptions.assignment[tp].drop_pending_message_set = False\n\n            for msg in self._next_partition_records.take():\n\n                # Because we are in a generator, it is possible for\n                # subscription state to change between yield calls\n                # so we need to re-check on each loop\n                # this should catch assignment changes, pauses\n                # and resets via seek_to_beginning / seek_to_end\n                if not self._subscriptions.is_fetchable(tp):\n                    log.debug(\"Not returning fetched records for partition %s\"\n                              \" since it is no longer fetchable\", tp)\n                    self._next_partition_records = None\n                    break\n\n                # If there is a seek during message iteration,\n                # we should stop unpacking this message set and\n                # wait for a new fetch response that aligns with the\n                # new seek position\n                elif self._subscriptions.assignment[tp].drop_pending_message_set:\n                    log.debug(\"Skipping remainder of message set for partition %s\", tp)\n                    self._subscriptions.assignment[tp].drop_pending_message_set = False\n                    self._next_partition_records = None\n                    break\n\n                # Compressed messagesets may include earlier messages\n                elif msg.offset < self._subscriptions.assignment[tp].position:\n                    log.debug(\"Skipping message offset: %s (expecting %s)\",\n                              msg.offset,\n                              self._subscriptions.assignment[tp].position)\n                    continue\n\n                self._subscriptions.assignment[tp].position = msg.offset + 1\n                yield msg\n\n            self._next_partition_records = None", "response": "Iterate over fetched_records and yield messages from the broker."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _send_offset_requests(self, timestamps):\n        timestamps_by_node = collections.defaultdict(dict)\n        for partition, timestamp in six.iteritems(timestamps):\n            node_id = self._client.cluster.leader_for_partition(partition)\n            if node_id is None:\n                self._client.add_topic(partition.topic)\n                log.debug(\"Partition %s is unknown for fetching offset,\"\n                          \" wait for metadata refresh\", partition)\n                return Future().failure(Errors.StaleMetadata(partition))\n            elif node_id == -1:\n                log.debug(\"Leader for partition %s unavailable for fetching \"\n                          \"offset, wait for metadata refresh\", partition)\n                return Future().failure(\n                    Errors.LeaderNotAvailableError(partition))\n            else:\n                timestamps_by_node[node_id][partition] = timestamp\n\n        # Aggregate results until we have all\n        list_offsets_future = Future()\n        responses = []\n        node_count = len(timestamps_by_node)\n\n        def on_success(value):\n            responses.append(value)\n            if len(responses) == node_count:\n                offsets = {}\n                for r in responses:\n                    offsets.update(r)\n                list_offsets_future.success(offsets)\n\n        def on_fail(err):\n            if not list_offsets_future.is_done:\n                list_offsets_future.failure(err)\n\n        for node_id, timestamps in six.iteritems(timestamps_by_node):\n            _f = self._send_offset_request(node_id, timestamps)\n            _f.add_callback(on_success)\n            _f.add_errback(on_fail)\n        return list_offsets_future", "response": "Send offsets for each partition in timestamps dict to each node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates fetch requests for all assigned partitions grouped by node.", "response": "def _create_fetch_requests(self):\n        \"\"\"Create fetch requests for all assigned partitions, grouped by node.\n\n        FetchRequests skipped if no leader, or node has requests in flight\n\n        Returns:\n            dict: {node_id: FetchRequest, ...} (version depends on api_version)\n        \"\"\"\n        # create the fetch info as a dict of lists of partition info tuples\n        # which can be passed to FetchRequest() via .items()\n        fetchable = collections.defaultdict(lambda: collections.defaultdict(list))\n\n        for partition in self._fetchable_partitions():\n            node_id = self._client.cluster.leader_for_partition(partition)\n\n            # advance position for any deleted compacted messages if required\n            if self._subscriptions.assignment[partition].last_offset_from_message_batch:\n                next_offset_from_batch_header = self._subscriptions.assignment[partition].last_offset_from_message_batch + 1\n                if next_offset_from_batch_header > self._subscriptions.assignment[partition].position:\n                    log.debug(\n                        \"Advance position for partition %s from %s to %s (last message batch location plus one)\"\n                        \" to correct for deleted compacted messages\",\n                        partition, self._subscriptions.assignment[partition].position, next_offset_from_batch_header)\n                    self._subscriptions.assignment[partition].position = next_offset_from_batch_header\n\n            position = self._subscriptions.assignment[partition].position\n\n            # fetch if there is a leader and no in-flight requests\n            if node_id is None or node_id == -1:\n                log.debug(\"No leader found for partition %s.\"\n                          \" Requesting metadata update\", partition)\n                self._client.cluster.request_update()\n\n            elif self._client.in_flight_request_count(node_id) == 0:\n                partition_info = (\n                    partition.partition,\n                    position,\n                    self.config['max_partition_fetch_bytes']\n                )\n                fetchable[node_id][partition.topic].append(partition_info)\n                log.debug(\"Adding fetch request for partition %s at offset %d\",\n                          partition, position)\n            else:\n                log.log(0, \"Skipping fetch for partition %s because there is an inflight request to node %s\",\n                        partition, node_id)\n\n        if self.config['api_version'] >= (0, 11, 0):\n            version = 4\n        elif self.config['api_version'] >= (0, 10, 1):\n            version = 3\n        elif self.config['api_version'] >= (0, 10):\n            version = 2\n        elif self.config['api_version'] == (0, 9):\n            version = 1\n        else:\n            version = 0\n        requests = {}\n        for node_id, partition_data in six.iteritems(fetchable):\n            if version < 3:\n                requests[node_id] = FetchRequest[version](\n                    -1,  # replica_id\n                    self.config['fetch_max_wait_ms'],\n                    self.config['fetch_min_bytes'],\n                    partition_data.items())\n            else:\n                # As of version == 3 partitions will be returned in order as\n                # they are requested, so to avoid starvation with\n                # `fetch_max_bytes` option we need this shuffle\n                # NOTE: we do have partition_data in random order due to usage\n                #       of unordered structures like dicts, but that does not\n                #       guarantee equal distribution, and starting in Python3.6\n                #       dicts retain insert order.\n                partition_data = list(partition_data.items())\n                random.shuffle(partition_data)\n                if version == 3:\n                    requests[node_id] = FetchRequest[version](\n                        -1,  # replica_id\n                        self.config['fetch_max_wait_ms'],\n                        self.config['fetch_min_bytes'],\n                        self.config['fetch_max_bytes'],\n                        partition_data)\n                else:\n                    requests[node_id] = FetchRequest[version](\n                        -1,  # replica_id\n                        self.config['fetch_max_wait_ms'],\n                        self.config['fetch_min_bytes'],\n                        self.config['fetch_max_bytes'],\n                        self._isolation_level,\n                        partition_data)\n        return requests"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _handle_fetch_response(self, request, send_time, response):\n        fetch_offsets = {}\n        for topic, partitions in request.topics:\n            for partition_data in partitions:\n                partition, offset = partition_data[:2]\n                fetch_offsets[TopicPartition(topic, partition)] = offset\n\n        partitions = set([TopicPartition(topic, partition_data[0])\n                          for topic, partitions in response.topics\n                          for partition_data in partitions])\n        metric_aggregator = FetchResponseMetricAggregator(self._sensors, partitions)\n\n        # randomized ordering should improve balance for short-lived consumers\n        random.shuffle(response.topics)\n        for topic, partitions in response.topics:\n            random.shuffle(partitions)\n            for partition_data in partitions:\n                tp = TopicPartition(topic, partition_data[0])\n                completed_fetch = CompletedFetch(\n                    tp, fetch_offsets[tp],\n                    response.API_VERSION,\n                    partition_data[1:],\n                    metric_aggregator\n                )\n                self._completed_fetches.append(completed_fetch)\n\n        if response.API_VERSION >= 1:\n            self._sensors.fetch_throttle_time_sensor.record(response.throttle_time_ms)\n        self._sensors.fetch_latency.record((time.time() - send_time) * 1000)", "response": "The callback for fetching a new topic from the server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef record(self, partition, num_bytes, num_records):\n        self.unrecorded_partitions.remove(partition)\n        self.total_bytes += num_bytes\n        self.total_records += num_records\n\n        # once all expected partitions from the fetch have reported in, record the metrics\n        if not self.unrecorded_partitions:\n            self.sensors.bytes_fetched.record(self.total_bytes)\n            self.sensors.records_fetched.record(self.total_records)", "response": "Record the current metrics for the given partition."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _auto_commit(self):\n\n        # Check if we are supposed to do an auto-commit\n        if not self.auto_commit or self.auto_commit_every_n is None:\n            return\n\n        if self.count_since_commit >= self.auto_commit_every_n:\n            self.commit()", "response": "Check if we have to commit based on number of messages and commit\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pending(self, partitions=None):\n        if partitions is None:\n            partitions = self.offsets.keys()\n\n        total = 0\n        reqs = []\n\n        for partition in partitions:\n            reqs.append(OffsetRequestPayload(self.topic, partition, -1, 1))\n\n        resps = self.client.send_offset_request(reqs)\n        for resp in resps:\n            partition = resp.partition\n            pending = resp.offsets[0]\n            offset = self.offsets[partition]\n            total += pending - offset\n\n        return total", "response": "Gets the number of pending messages for the specified partitions"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef murmur2(data):\n    # Python2 bytes is really a str, causing the bitwise operations below to fail\n    # so convert to bytearray.\n    if six.PY2:\n        data = bytearray(bytes(data))\n\n    length = len(data)\n    seed = 0x9747b28c\n    # 'm' and 'r' are mixing constants generated offline.\n    # They're not really 'magic', they just happen to work well.\n    m = 0x5bd1e995\n    r = 24\n\n    # Initialize the hash to a random value\n    h = seed ^ length\n    length4 = length // 4\n\n    for i in range(length4):\n        i4 = i * 4\n        k = ((data[i4 + 0] & 0xff) +\n            ((data[i4 + 1] & 0xff) << 8) +\n            ((data[i4 + 2] & 0xff) << 16) +\n            ((data[i4 + 3] & 0xff) << 24))\n        k &= 0xffffffff\n        k *= m\n        k &= 0xffffffff\n        k ^= (k % 0x100000000) >> r # k ^= k >>> r\n        k &= 0xffffffff\n        k *= m\n        k &= 0xffffffff\n\n        h *= m\n        h &= 0xffffffff\n        h ^= k\n        h &= 0xffffffff\n\n    # Handle the last few bytes of the input array\n    extra_bytes = length % 4\n    if extra_bytes >= 3:\n        h ^= (data[(length & ~3) + 2] & 0xff) << 16\n        h &= 0xffffffff\n    if extra_bytes >= 2:\n        h ^= (data[(length & ~3) + 1] & 0xff) << 8\n        h &= 0xffffffff\n    if extra_bytes >= 1:\n        h ^= (data[length & ~3] & 0xff)\n        h &= 0xffffffff\n        h *= m\n        h &= 0xffffffff\n\n    h ^= (h % 0x100000000) >> 13 # h >>> 13;\n    h &= 0xffffffff\n    h *= m\n    h &= 0xffffffff\n    h ^= (h % 0x100000000) >> 15 # h >>> 15;\n    h &= 0xffffffff\n\n    return h", "response": "Pure - python Murmur2 implementation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfetching the specified number of messages from the queue and return them as a list of Message objects.", "response": "def get_messages(self, count=1, block=True, timeout=10):\n        \"\"\"\n        Fetch the specified number of messages\n\n        Keyword Arguments:\n            count: Indicates the maximum number of messages to be fetched\n            block: If True, the API will block till all messages are fetched.\n                If block is a positive integer the API will block until that\n                many messages are fetched.\n            timeout: When blocking is requested the function will block for\n                the specified time (in seconds) until count messages is\n                fetched. If None, it will block forever.\n        \"\"\"\n        messages = []\n\n        # Give a size hint to the consumers. Each consumer process will fetch\n        # a maximum of \"count\" messages. This will fetch more messages than\n        # necessary, but these will not be committed to kafka. Also, the extra\n        # messages can be provided in subsequent runs\n        self.size.value = count\n        self.events.pause.clear()\n\n        if timeout is not None:\n            max_time = time.time() + timeout\n\n        new_offsets = {}\n        while count > 0 and (timeout is None or timeout > 0):\n            # Trigger consumption only if the queue is empty\n            # By doing this, we will ensure that consumers do not\n            # go into overdrive and keep consuming thousands of\n            # messages when the user might need only a few\n            if self.queue.empty():\n                self.events.start.set()\n\n            block_next_call = block is True or block > len(messages)\n            try:\n                partition, message = self.queue.get(block_next_call,\n                                                    timeout)\n            except queue.Empty:\n                break\n\n            _msg = (partition, message) if self.partition_info else message\n            messages.append(_msg)\n            new_offsets[partition] = message.offset + 1\n            count -= 1\n            if timeout is not None:\n                timeout = max_time - time.time()\n\n        self.size.value = 0\n        self.events.start.clear()\n        self.events.pause.set()\n\n        # Update and commit offsets if necessary\n        self.offsets.update(new_offsets)\n        self.count_since_commit += len(messages)\n        self._auto_commit()\n\n        return messages"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mark(self, partition, offset):\n        max_offset = max(offset + 1, self.high_water_mark.get(partition, 0))\n\n        self.logger.debug(\"Setting high-water mark to: %s\",\n                          {partition: max_offset})\n\n        self.high_water_mark[partition] = max_offset", "response": "Set the high - water mark for the given partition."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef commit(self):\n        if self.high_water_mark:\n            self.logger.info(\"Committing offsets: %s\", self.high_water_mark)\n            self.commit_partition_offsets(self.high_water_mark)\n            self.update_consumer_offsets(self.high_water_mark)\n        else:\n            self.update_consumer_offsets(self.initial_offsets)", "response": "Commit the offsets of this context."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rollback(self):\n        self.logger.info(\"Rolling back context: %s\", self.initial_offsets)\n        self.update_consumer_offsets(self.initial_offsets)", "response": "Roll back the consumer context."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncommit explicit partition and offset pairs.", "response": "def commit_partition_offsets(self, partition_offsets):\n        \"\"\"\n        Commit explicit partition/offset pairs.\n        \"\"\"\n        self.logger.debug(\"Committing partition offsets: %s\", partition_offsets)\n\n        commit_requests = [\n            OffsetCommitRequestPayload(self.consumer.topic, partition, offset, None)\n            for partition, offset in partition_offsets.items()\n        ]\n        commit_responses = self.consumer.client.send_offset_commit_request(\n            self.consumer.group,\n            commit_requests,\n        )\n        for commit_response in commit_responses:\n            check_error(commit_response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_consumer_offsets(self, partition_offsets):\n        self.logger.debug(\"Updating consumer offsets to: %s\", partition_offsets)\n\n        for partition, offset in partition_offsets.items():\n            self.consumer.offsets[partition] = offset\n\n        # consumer keeps other offset states beyond its `offsets` dictionary,\n        # a relative seek with zero delta forces the consumer to reset to the\n        # current value of the `offsets` dictionary\n        self.consumer.seek(0, 1)", "response": "Update consumer offsets to explicit positions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef coordinator(self):\n        if self.coordinator_id is None:\n            return None\n        elif self._client.is_disconnected(self.coordinator_id):\n            self.coordinator_dead('Node Disconnected')\n            return None\n        else:\n            return self.coordinator_id", "response": "Get the current coordinator id"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nblocking until the coordinator is ready for this group.", "response": "def ensure_coordinator_ready(self):\n        \"\"\"Block until the coordinator for this group is known\n        (and we have an active connection -- java client uses unsent queue).\n        \"\"\"\n        with self._client._lock, self._lock:\n            while self.coordinator_unknown():\n\n                # Prior to 0.8.2 there was no group coordinator\n                # so we will just pick a node at random and treat\n                # it as the \"coordinator\"\n                if self.config['api_version'] < (0, 8, 2):\n                    self.coordinator_id = self._client.least_loaded_node()\n                    if self.coordinator_id is not None:\n                        self._client.maybe_connect(self.coordinator_id)\n                    continue\n\n                future = self.lookup_coordinator()\n                self._client.poll(future=future)\n\n                if future.failed():\n                    if future.retriable():\n                        if getattr(future.exception, 'invalid_metadata', False):\n                            log.debug('Requesting metadata for group coordinator request: %s', future.exception)\n                            metadata_update = self._client.cluster.request_update()\n                            self._client.poll(future=metadata_update)\n                        else:\n                            time.sleep(self.config['retry_backoff_ms'] / 1000)\n                    else:\n                        raise future.exception"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npolls the heartbeat thread for the given entry.", "response": "def poll_heartbeat(self):\n        \"\"\"\n        Check the status of the heartbeat thread (if it is active) and indicate\n        the liveness of the client. This must be called periodically after\n        joining with :meth:`.ensure_active_group` to ensure that the member stays\n        in the group. If an interval of time longer than the provided rebalance\n        timeout (max_poll_interval_ms) expires without calling this method, then\n        the client will proactively leave the group.\n\n        Raises: RuntimeError for unexpected errors raised from the heartbeat thread\n        \"\"\"\n        with self._lock:\n            if self._heartbeat_thread is not None:\n                if self._heartbeat_thread.failed:\n                    # set the heartbeat thread to None and raise an exception.\n                    # If the user catches it, the next call to ensure_active_group()\n                    # will spawn a new heartbeat thread.\n                    cause = self._heartbeat_thread.failed\n                    self._heartbeat_thread = None\n                    raise cause  # pylint: disable-msg=raising-bad-type\n\n                # Awake the heartbeat thread if needed\n                if self.heartbeat.should_heartbeat():\n                    self._lock.notify()\n                self.heartbeat.poll()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ensure_active_group(self):\n        with self._client._lock, self._lock:\n            if self._heartbeat_thread is None:\n                self._start_heartbeat_thread()\n\n            while self.need_rejoin() or self._rejoin_incomplete():\n                self.ensure_coordinator_ready()\n\n                # call on_join_prepare if needed. We set a flag\n                # to make sure that we do not call it a second\n                # time if the client is woken up before a pending\n                # rebalance completes. This must be called on each\n                # iteration of the loop because an event requiring\n                # a rebalance (such as a metadata refresh which\n                # changes the matched subscription set) can occur\n                # while another rebalance is still in progress.\n                if not self.rejoining:\n                    self._on_join_prepare(self._generation.generation_id,\n                                          self._generation.member_id)\n                    self.rejoining = True\n\n                # ensure that there are no pending requests to the coordinator.\n                # This is important in particular to avoid resending a pending\n                # JoinGroup request.\n                while not self.coordinator_unknown():\n                    if not self._client.in_flight_request_count(self.coordinator_id):\n                        break\n                    self._client.poll()\n                else:\n                    continue\n\n                # we store the join future in case we are woken up by the user\n                # after beginning the rebalance in the call to poll below.\n                # This ensures that we do not mistakenly attempt to rejoin\n                # before the pending rebalance has completed.\n                if self.join_future is None:\n                    # Fence off the heartbeat thread explicitly so that it cannot\n                    # interfere with the join group. Note that this must come after\n                    # the call to _on_join_prepare since we must be able to continue\n                    # sending heartbeats if that callback takes some time.\n                    self._heartbeat_thread.disable()\n\n                    self.state = MemberState.REBALANCING\n                    future = self._send_join_group_request()\n\n                    self.join_future = future  # this should happen before adding callbacks\n\n                    # handle join completion in the callback so that the\n                    # callback will be invoked even if the consumer is woken up\n                    # before finishing the rebalance\n                    future.add_callback(self._handle_join_success)\n\n                    # we handle failures below after the request finishes.\n                    # If the join completes after having been woken up, the\n                    # exception is ignored and we will rejoin\n                    future.add_errback(self._handle_join_failure)\n\n                else:\n                    future = self.join_future\n\n                self._client.poll(future=future)\n\n                if future.succeeded():\n                    self._on_join_complete(self._generation.generation_id,\n                                           self._generation.member_id,\n                                           self._generation.protocol,\n                                           future.value)\n                    self.join_future = None\n                    self.rejoining = False\n\n                else:\n                    self.join_future = None\n                    exception = future.exception\n                    if isinstance(exception, (Errors.UnknownMemberIdError,\n                                              Errors.RebalanceInProgressError,\n                                              Errors.IllegalGenerationError)):\n                        continue\n                    elif not future.retriable():\n                        raise exception  # pylint: disable-msg=raising-bad-type\n                    time.sleep(self.config['retry_backoff_ms'] / 1000)", "response": "Ensure that the group is active."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a JoinGroupRequest request to the coordinator.", "response": "def _send_join_group_request(self):\n        \"\"\"Join the group and return the assignment for the next generation.\n\n        This function handles both JoinGroup and SyncGroup, delegating to\n        :meth:`._perform_assignment` if elected leader by the coordinator.\n\n        Returns:\n            Future: resolves to the encoded-bytes assignment returned from the\n                group leader\n        \"\"\"\n        if self.coordinator_unknown():\n            e = Errors.GroupCoordinatorNotAvailableError(self.coordinator_id)\n            return Future().failure(e)\n\n        elif not self._client.ready(self.coordinator_id, metadata_priority=False):\n            e = Errors.NodeNotReadyError(self.coordinator_id)\n            return Future().failure(e)\n\n        # send a join group request to the coordinator\n        log.info(\"(Re-)joining group %s\", self.group_id)\n        member_metadata = [\n            (protocol, metadata if isinstance(metadata, bytes) else metadata.encode())\n            for protocol, metadata in self.group_protocols()\n        ]\n        if self.config['api_version'] < (0, 9):\n            raise Errors.KafkaError('JoinGroupRequest api requires 0.9+ brokers')\n        elif (0, 9) <= self.config['api_version'] < (0, 10, 1):\n            request = JoinGroupRequest[0](\n                self.group_id,\n                self.config['session_timeout_ms'],\n                self._generation.member_id,\n                self.protocol_type(),\n                member_metadata)\n        elif (0, 10, 1) <= self.config['api_version'] < (0, 11, 0):\n            request = JoinGroupRequest[1](\n                self.group_id,\n                self.config['session_timeout_ms'],\n                self.config['max_poll_interval_ms'],\n                self._generation.member_id,\n                self.protocol_type(),\n                member_metadata)\n        else:\n            request = JoinGroupRequest[2](\n                self.group_id,\n                self.config['session_timeout_ms'],\n                self.config['max_poll_interval_ms'],\n                self._generation.member_id,\n                self.protocol_type(),\n                member_metadata)\n\n        # create the request for the coordinator\n        log.debug(\"Sending JoinGroup (%s) to coordinator %s\", request, self.coordinator_id)\n        future = Future()\n        _f = self._client.send(self.coordinator_id, request)\n        _f.add_callback(self._handle_join_group_response, future, time.time())\n        _f.add_errback(self._failed_request, self.coordinator_id,\n                       request, future)\n        return future"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _on_join_leader(self, response):\n        try:\n            group_assignment = self._perform_assignment(response.leader_id,\n                                                        response.group_protocol,\n                                                        response.members)\n        except Exception as e:\n            return Future().failure(e)\n\n        version = 0 if self.config['api_version'] < (0, 11, 0) else 1\n        request = SyncGroupRequest[version](\n            self.group_id,\n            self._generation.generation_id,\n            self._generation.member_id,\n            [(member_id,\n              assignment if isinstance(assignment, bytes) else assignment.encode())\n             for member_id, assignment in six.iteritems(group_assignment)])\n\n        log.debug(\"Sending leader SyncGroup for group %s to coordinator %s: %s\",\n                  self.group_id, self.coordinator_id, request)\n        return self._send_sync_group_request(request)", "response": "Perform leader synchronization and send back the assignment\n        for the group via SyncGroupRequest\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend a request to the broker to discover the current coordinator for the group.", "response": "def _send_group_coordinator_request(self):\n        \"\"\"Discover the current coordinator for the group.\n\n        Returns:\n            Future: resolves to the node id of the coordinator\n        \"\"\"\n        node_id = self._client.least_loaded_node()\n        if node_id is None:\n            return Future().failure(Errors.NoBrokersAvailable())\n\n        elif not self._client.ready(node_id, metadata_priority=False):\n            e = Errors.NodeNotReadyError(node_id)\n            return Future().failure(e)\n\n        log.debug(\"Sending group coordinator request for group %s to broker %s\",\n                  self.group_id, node_id)\n        request = GroupCoordinatorRequest[0](self.group_id)\n        future = Future()\n        _f = self._client.send(node_id, request)\n        _f.add_callback(self._handle_group_coordinator_response, future)\n        _f.add_errback(self._failed_request, node_id, request, future)\n        return future"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef coordinator_dead(self, error):\n        if self.coordinator_id is not None:\n            log.warning(\"Marking the coordinator dead (node %s) for group %s: %s.\",\n                        self.coordinator_id, self.group_id, error)\n            self.coordinator_id = None", "response": "Mark the current coordinator as dead."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generation(self):\n        with self._lock:\n            if self.state is not MemberState.STABLE:\n                return None\n            return self._generation", "response": "Get the current generation state if the group is stable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nresetting the generation and memberId because we have fallen out of the group.", "response": "def reset_generation(self):\n        \"\"\"Reset the generation and memberId because we have fallen out of the group.\"\"\"\n        with self._lock:\n            self._generation = Generation.NO_GENERATION\n            self.rejoin_needed = True\n            self.state = MemberState.UNJOINED"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef maybe_leave_group(self):\n        with self._client._lock, self._lock:\n            if (not self.coordinator_unknown()\n                and self.state is not MemberState.UNJOINED\n                and self._generation is not Generation.NO_GENERATION):\n\n                # this is a minimal effort attempt to leave the group. we do not\n                # attempt any resending if the request fails or times out.\n                log.info('Leaving consumer group (%s).', self.group_id)\n                version = 0 if self.config['api_version'] < (0, 11, 0) else 1\n                request = LeaveGroupRequest[version](self.group_id, self._generation.member_id)\n                future = self._client.send(self.coordinator_id, request)\n                future.add_callback(self._handle_leave_group_response)\n                future.add_errback(log.error, \"LeaveGroup request failed: %s\")\n                self._client.poll(future=future)\n\n            self.reset_generation()", "response": "Leave the current group and reset local generation and memberId."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a heartbeat request to the group", "response": "def _send_heartbeat_request(self):\n        \"\"\"Send a heartbeat request\"\"\"\n        if self.coordinator_unknown():\n            e = Errors.GroupCoordinatorNotAvailableError(self.coordinator_id)\n            return Future().failure(e)\n\n        elif not self._client.ready(self.coordinator_id, metadata_priority=False):\n            e = Errors.NodeNotReadyError(self.coordinator_id)\n            return Future().failure(e)\n\n        version = 0 if self.config['api_version'] < (0, 11, 0) else 1\n        request = HeartbeatRequest[version](self.group_id,\n                                            self._generation.generation_id,\n                                            self._generation.member_id)\n        log.debug(\"Heartbeat: %s[%s] %s\", request.group, request.generation_id, request.member_id)  # pylint: disable-msg=no-member\n        future = Future()\n        _f = self._client.send(self.coordinator_id, request)\n        _f.add_callback(self._handle_heartbeat_response, future, time.time())\n        _f.add_errback(self._failed_request, self.coordinator_id,\n                       request, future)\n        return future"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef metric_name(self, name, group, description='', tags=None):\n        combined_tags = dict(self.config.tags)\n        combined_tags.update(tags or {})\n        return MetricName(name, group, description, combined_tags)", "response": "Creates a MetricName object with the given name group description and tags."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sensor(self, name, config=None,\n               inactive_sensor_expiration_time_seconds=sys.maxsize,\n               parents=None):\n        \"\"\"\n        Get or create a sensor with the given unique name and zero or\n        more parent sensors. All parent sensors will receive every value\n        recorded with this sensor.\n\n        Arguments:\n            name (str): The name of the sensor\n            config (MetricConfig, optional): A default configuration to use\n                for this sensor for metrics that don't have their own config\n            inactive_sensor_expiration_time_seconds (int, optional):\n                If no value if recorded on the Sensor for this duration of\n                time, it is eligible for removal\n            parents (list of Sensor): The parent sensors\n\n        Returns:\n            Sensor: The sensor that is created\n        \"\"\"\n        sensor = self.get_sensor(name)\n        if sensor:\n            return sensor\n\n        with self._lock:\n            sensor = self.get_sensor(name)\n            if not sensor:\n                sensor = Sensor(self, name, parents, config or self.config,\n                                inactive_sensor_expiration_time_seconds)\n                self._sensors[name] = sensor\n                if parents:\n                    for parent in parents:\n                        children = self._children_sensors.get(parent)\n                        if not children:\n                            children = []\n                            self._children_sensors[parent] = children\n                        children.append(sensor)\n                logger.debug('Added sensor with name %s', name)\n            return sensor", "response": "Get or create a Sensor with the given unique name and configuration."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves a sensor from the cache.", "response": "def remove_sensor(self, name):\n        \"\"\"\n        Remove a sensor (if it exists), associated metrics and its children.\n\n        Arguments:\n            name (str): The name of the sensor to be removed\n        \"\"\"\n        sensor = self._sensors.get(name)\n        if sensor:\n            child_sensors = None\n            with sensor._lock:\n                with self._lock:\n                    val = self._sensors.pop(name, None)\n                    if val and val == sensor:\n                        for metric in sensor.metrics:\n                            self.remove_metric(metric.metric_name)\n                        logger.debug('Removed sensor with name %s', name)\n                        child_sensors = self._children_sensors.pop(sensor, None)\n            if child_sensors:\n                for child_sensor in child_sensors:\n                    self.remove_sensor(child_sensor.name)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a metric to monitor an object that implements measurable. This metric will be added to any existing sensors.", "response": "def add_metric(self, metric_name, measurable, config=None):\n        \"\"\"\n        Add a metric to monitor an object that implements measurable.\n        This metric won't be associated with any sensor.\n        This is a way to expose existing values as metrics.\n\n        Arguments:\n            metricName (MetricName): The name of the metric\n            measurable (AbstractMeasurable): The measurable that will be\n                measured by this metric\n            config (MetricConfig, optional): The configuration to use when\n                measuring this measurable\n        \"\"\"\n        # NOTE there was a lock here, but i don't think it's needed\n        metric = KafkaMetric(metric_name, measurable, config or self.config)\n        self.register_metric(metric)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove a metric from the internal cache if it exists and return it.", "response": "def remove_metric(self, metric_name):\n        \"\"\"\n        Remove a metric if it exists and return it. Return None otherwise.\n        If a metric is removed, `metric_removal` will be invoked\n        for each reporter.\n\n        Arguments:\n            metric_name (MetricName): The name of the metric\n\n        Returns:\n            KafkaMetric: the removed `KafkaMetric` or None if no such\n                metric exists\n        \"\"\"\n        with self._lock:\n            metric = self._metrics.pop(metric_name, None)\n            if metric:\n                for reporter in self._reporters:\n                    reporter.metric_removal(metric)\n            return metric"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a MetricReporter to the list of available metrics.", "response": "def add_reporter(self, reporter):\n        \"\"\"Add a MetricReporter\"\"\"\n        with self._lock:\n            reporter.init(list(self.metrics.values()))\n            self._reporters.append(reporter)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nclose this metrics repository.", "response": "def close(self):\n        \"\"\"Close this metrics repository.\"\"\"\n        for reporter in self._reporters:\n            reporter.close()\n\n        self._metrics.clear()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconstruct a Snappy Message containing multiple Messages", "response": "def create_snappy_message(payloads, key=None):\n    \"\"\"\n    Construct a Snappy Message containing multiple Messages\n\n    The given payloads will be encoded, compressed, and sent as a single atomic\n    message to Kafka.\n\n    Arguments:\n        payloads: list(bytes), a list of payload to send be sent to Kafka\n        key: bytes, a key used for partition routing (optional)\n\n    \"\"\"\n    message_set = KafkaProtocol._encode_message_set(\n        [create_message(payload, pl_key) for payload, pl_key in payloads])\n\n    snapped = snappy_encode(message_set)\n    codec = ATTRIBUTE_CODEC_MASK & CODEC_SNAPPY\n\n    return kafka.structs.Message(0, 0x00 | codec, key, snapped)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a message set using the given codec.", "response": "def create_message_set(messages, codec=CODEC_NONE, key=None, compresslevel=None):\n    \"\"\"Create a message set using the given codec.\n\n    If codec is CODEC_NONE, return a list of raw Kafka messages. Otherwise,\n    return a list containing a single codec-encoded message.\n    \"\"\"\n    if codec == CODEC_NONE:\n        return [create_message(m, k) for m, k in messages]\n    elif codec == CODEC_GZIP:\n        return [create_gzip_message(messages, key, compresslevel)]\n    elif codec == CODEC_SNAPPY:\n        return [create_snappy_message(messages, key)]\n    else:\n        raise UnsupportedCodecError(\"Codec 0x%02x unsupported\" % (codec,))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _encode_message_header(cls, client_id, correlation_id, request_key,\n                               version=0):\n        \"\"\"\n        Encode the common request envelope\n        \"\"\"\n        return struct.pack('>hhih%ds' % len(client_id),\n                           request_key,          # ApiKey\n                           version,              # ApiVersion\n                           correlation_id,       # CorrelationId\n                           len(client_id),       # ClientId size\n                           client_id)", "response": "Encode the message header."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _encode_message_set(cls, messages):\n        message_set = []\n        for message in messages:\n            encoded_message = KafkaProtocol._encode_message(message)\n            message_set.append(struct.pack('>qi%ds' % len(encoded_message), 0,\n                                           len(encoded_message),\n                                           encoded_message))\n        return b''.join(message_set)", "response": "Encode a list of messages into a message set."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef encode_produce_request(cls, payloads=(), acks=1, timeout=1000):\n        if acks not in (1, 0, -1):\n            raise ValueError('ProduceRequest acks (%s) must be 1, 0, -1' % acks)\n\n        topics = []\n        for topic, topic_payloads in group_by_topic_and_partition(payloads).items():\n            topic_msgs = []\n            for partition, payload in topic_payloads.items():\n                partition_msgs = []\n                for msg in payload.messages:\n                    m = kafka.protocol.message.Message(\n                          msg.value, key=msg.key,\n                          magic=msg.magic, attributes=msg.attributes\n                    )\n                    partition_msgs.append((0, m.encode()))\n                topic_msgs.append((partition, MessageSet.encode(partition_msgs, prepend_size=False)))\n            topics.append((topic, topic_msgs))\n\n\n        return kafka.protocol.produce.ProduceRequest[0](\n            required_acks=acks,\n            timeout=timeout,\n            topics=topics\n        )", "response": "Encodes ProduceRequest struct into a ProduceRequest structure."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndecoding ProduceResponse to ProduceResponsePayloads", "response": "def decode_produce_response(cls, response):\n        \"\"\"\n        Decode ProduceResponse to ProduceResponsePayload\n\n        Arguments:\n            response: ProduceResponse\n\n        Return: list of ProduceResponsePayload\n        \"\"\"\n        return [\n            kafka.structs.ProduceResponsePayload(topic, partition, error, offset)\n            for topic, partitions in response.topics\n            for partition, error, offset in partitions\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef encode_fetch_request(cls, payloads=(), max_wait_time=100, min_bytes=4096):\n        return kafka.protocol.fetch.FetchRequest[0](\n            replica_id=-1,\n            max_wait_time=max_wait_time,\n            min_bytes=min_bytes,\n            topics=[(\n                topic,\n                [(\n                    partition,\n                    payload.offset,\n                    payload.max_bytes)\n                for partition, payload in topic_payloads.items()])\n            for topic, topic_payloads in group_by_topic_and_partition(payloads).items()])", "response": "Encodes a FetchRequest struct into a FetchRequest struct."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef decode_fetch_response(cls, response):\n        return [\n            kafka.structs.FetchResponsePayload(\n                topic, partition, error, highwater_offset, [\n                    offset_and_msg\n                    for offset_and_msg in cls.decode_message_set(messages)])\n            for topic, partitions in response.topics\n                for partition, error, highwater_offset, messages in partitions\n        ]", "response": "Decodes FetchResponse struct to FetchResponsePayloads\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef decode_offset_response(cls, response):\n        return [\n            kafka.structs.OffsetResponsePayload(topic, partition, error, tuple(offsets))\n            for topic, partitions in response.topics\n            for partition, error, offsets in partitions\n        ]", "response": "Decodes OffsetResponse into OffsetResponsePayloads\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decode_list_offset_response(cls, response):\n        return [\n            kafka.structs.ListOffsetResponsePayload(topic, partition, error, timestamp, offset)\n            for topic, partitions in response.topics\n            for partition, error, timestamp, offset in partitions\n        ]", "response": "Decode OffsetResponse_v2 into ListOffsetResponsePayloads\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nencodes a MetadataRequest object", "response": "def encode_metadata_request(cls, topics=(), payloads=None):\n        \"\"\"\n        Encode a MetadataRequest\n\n        Arguments:\n            topics: list of strings\n        \"\"\"\n        if payloads is not None:\n            topics = payloads\n\n        return kafka.protocol.metadata.MetadataRequest[0](topics)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef encode_consumer_metadata_request(cls, client_id, correlation_id, payloads):\n        message = []\n        message.append(cls._encode_message_header(client_id, correlation_id,\n                                                  KafkaProtocol.CONSUMER_METADATA_KEY))\n        message.append(struct.pack('>h%ds' % len(payloads), len(payloads), payloads))\n\n        msg = b''.join(message)\n        return write_int_string(msg)", "response": "Encode a ConsumerMetadataRequest object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef decode_consumer_metadata_response(cls, data):\n        ((correlation_id, error, nodeId), cur) = relative_unpack('>ihi', data, 0)\n        (host, cur) = read_short_string(data, cur)\n        ((port,), cur) = relative_unpack('>i', data, cur)\n\n        return kafka.structs.ConsumerMetadataResponse(error, nodeId, host, port)", "response": "Decodes bytes to a kafka. structs. ConsumerMetadataResponse object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef encode_offset_commit_request(cls, group, payloads):\n        return kafka.protocol.commit.OffsetCommitRequest[0](\n            consumer_group=group,\n            topics=[(\n                topic,\n                [(\n                    partition,\n                    payload.offset,\n                    payload.metadata)\n                for partition, payload in six.iteritems(topic_payloads)])\n            for topic, topic_payloads in six.iteritems(group_by_topic_and_partition(payloads))])", "response": "Encode an OffsetCommitRequest struct that is used to commit the offsets for the specified topic and partition."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef decode_offset_commit_response(cls, response):\n        return [\n            kafka.structs.OffsetCommitResponsePayload(topic, partition, error)\n            for topic, partitions in response.topics\n            for partition, error in partitions\n        ]", "response": "Decodes an OffsetCommitResponse to an OffsetCommitResponsePayload"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef encode_offset_fetch_request(cls, group, payloads, from_kafka=False):\n        version = 1 if from_kafka else 0\n        return kafka.protocol.commit.OffsetFetchRequest[version](\n            consumer_group=group,\n            topics=[(\n                topic,\n                list(topic_payloads.keys()))\n            for topic, topic_payloads in six.iteritems(group_by_topic_and_partition(payloads))])", "response": "Encode an OffsetFetchRequest struct."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decode_offset_fetch_response(cls, response):\n        return [\n            kafka.structs.OffsetFetchResponsePayload(\n                topic, partition, offset, metadata, error\n            )\n            for topic, partitions in response.topics\n            for partition, offset, metadata, error in partitions\n        ]", "response": "Decode OffsetFetchResponse to OffsetFetchResponsePayloads\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a file descriptor from a file object.", "response": "def _fileobj_lookup(self, fileobj):\n        \"\"\"Return a file descriptor from a file object.\n\n        This wraps _fileobj_to_fd() to do an exhaustive search in case\n        the object is invalid but we still have it in our map.  This\n        is used by unregister() so we can unregister an object that\n        was previously registered even if it is closed.  It is also\n        used by _SelectorMapping.\n        \"\"\"\n        try:\n            return _fileobj_to_fd(fileobj)\n        except ValueError:\n            # Do an exhaustive search.\n            for key in self._fd_to_key.values():\n                if key.fileobj is fileobj:\n                    return key.fd\n            # Raise ValueError after all.\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_forest(self, sensors):\n        if self in sensors:\n            raise ValueError('Circular dependency in sensors: %s is its own'\n                             'parent.' % (self.name,))\n        sensors.add(self)\n        for parent in self._parents:\n            parent._check_forest(sensors)", "response": "Validate that this sensor doesn t end up referencing itself."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrecording a value at a known time.", "response": "def record(self, value=1.0, time_ms=None):\n        \"\"\"\n        Record a value at a known time.\n        Arguments:\n            value (double): The value we are recording\n            time_ms (int): A POSIX timestamp in milliseconds.\n                Default: The time when record() is evaluated (now)\n\n        Raises:\n            QuotaViolationException: if recording this value moves a\n                metric beyond its configured maximum or minimum bound\n        \"\"\"\n        if time_ms is None:\n            time_ms = time.time() * 1000\n        self._last_record_time = time_ms\n        with self._lock:  # XXX high volume, might be performance issue\n            # increment all the stats\n            for stat in self._stats:\n                stat.record(self._config, value, time_ms)\n            self._check_quotas(time_ms)\n        for parent in self._parents:\n            parent.record(value, time_ms)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if we have violated our quota for any of the metrics that have a configured quota.", "response": "def _check_quotas(self, time_ms):\n        \"\"\"\n        Check if we have violated our quota for any metric that\n        has a configured quota\n        \"\"\"\n        for metric in self._metrics:\n            if metric.config and metric.config.quota:\n                value = metric.value(time_ms)\n                if not metric.config.quota.is_acceptable(value):\n                    raise QuotaViolationError(\"'%s' violated quota. Actual: \"\n                                              \"%d, Threshold: %d\" %\n                                              (metric.metric_name,\n                                               value,\n                                               metric.config.quota.bound))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_compound(self, compound_stat, config=None):\n        if not compound_stat:\n            raise ValueError('compound stat must be non-empty')\n        self._stats.append(compound_stat)\n        for named_measurable in compound_stat.stats():\n            metric = KafkaMetric(named_measurable.name, named_measurable.stat,\n                                 config or self._config)\n            self._registry.register_metric(metric)\n            self._metrics.append(metric)", "response": "Adds a compound stat to this sensor which is a list of measurable quantities."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a metric to the registry.", "response": "def add(self, metric_name, stat, config=None):\n        \"\"\"\n        Register a metric with this sensor\n\n        Arguments:\n            metric_name (MetricName): The name of the metric\n            stat (AbstractMeasurableStat): The statistic to keep\n            config (MetricConfig): A special configuration for this metric.\n                If None use the sensor default configuration.\n        \"\"\"\n        with self._lock:\n            metric = KafkaMetric(metric_name, stat, config or self._config)\n            self._registry.register_metric(metric)\n            self._metrics.append(metric)\n            self._stats.append(stat)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn list of preferred ( protocols metadata ) for all available assignors.", "response": "def group_protocols(self):\n        \"\"\"Returns list of preferred (protocols, metadata)\"\"\"\n        if self._subscription.subscription is None:\n            raise Errors.IllegalStateError('Consumer has not subscribed to topics')\n        # dpkp note: I really dislike this.\n        # why? because we are using this strange method group_protocols,\n        # which is seemingly innocuous, to set internal state (_joined_subscription)\n        # that is later used to check whether metadata has changed since we joined a group\n        # but there is no guarantee that this method, group_protocols, will get called\n        # in the correct sequence or that it will only be called when we want it to be.\n        # So this really should be moved elsewhere, but I don't have the energy to\n        # work that out right now. If you read this at some later date after the mutable\n        # state has bitten you... I'm sorry! It mimics the java client, and that's the\n        # best I've got for now.\n        self._joined_subscription = set(self._subscription.subscription)\n        metadata_list = []\n        for assignor in self.config['assignors']:\n            metadata = assignor.metadata(self._joined_subscription)\n            group_protocol = (assignor.name, metadata)\n            metadata_list.append(group_protocol)\n        return metadata_list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef poll(self):\n        if self.group_id is None or self.config['api_version'] < (0, 8, 2):\n            return\n\n        self._invoke_completed_offset_commit_callbacks()\n        self.ensure_coordinator_ready()\n\n        if self.config['api_version'] >= (0, 9) and self._subscription.partitions_auto_assigned():\n            if self.need_rejoin():\n                # due to a race condition between the initial metadata fetch and the\n                # initial rebalance, we need to ensure that the metadata is fresh\n                # before joining initially, and then request the metadata update. If\n                # metadata update arrives while the rebalance is still pending (for\n                # example, when the join group is still inflight), then we will lose\n                # track of the fact that we need to rebalance again to reflect the\n                # change to the topic subscription. Without ensuring that the\n                # metadata is fresh, any metadata update that changes the topic\n                # subscriptions and arrives while a rebalance is in progress will\n                # essentially be ignored. See KAFKA-3949 for the complete\n                # description of the problem.\n                if self._subscription.subscribed_pattern:\n                    metadata_update = self._client.cluster.request_update()\n                    self._client.poll(future=metadata_update)\n\n                self.ensure_active_group()\n\n            self.poll_heartbeat()\n\n        self._maybe_auto_commit_offsets_async()", "response": "Poll for coordinator events. Only applicable if group_id is set and broker version supports GroupCoordinators."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns seconds remaining until the next poll should be called again", "response": "def time_to_next_poll(self):\n        \"\"\"Return seconds (float) remaining until :meth:`.poll` should be called again\"\"\"\n        if not self.config['enable_auto_commit']:\n            return self.time_to_next_heartbeat()\n\n        if time.time() > self.next_auto_commit_deadline:\n            return 0\n\n        return min(self.next_auto_commit_deadline - time.time(),\n                   self.time_to_next_heartbeat())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking whether the consumer should rejoin group", "response": "def need_rejoin(self):\n        \"\"\"Check whether the group should be rejoined\n\n        Returns:\n            bool: True if consumer should rejoin group, False otherwise\n        \"\"\"\n        if not self._subscription.partitions_auto_assigned():\n            return False\n\n        if self._auto_assign_all_partitions():\n            return False\n\n        # we need to rejoin if we performed the assignment and metadata has changed\n        if (self._assignment_snapshot is not None\n            and self._assignment_snapshot != self._metadata_snapshot):\n            return True\n\n        # we need to join if our subscription has changed since the last join\n        if (self._joined_subscription is not None\n            and self._joined_subscription != self._subscription.subscription):\n            return True\n\n        return super(ConsumerCoordinator, self).need_rejoin()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef refresh_committed_offsets_if_needed(self):\n        if self._subscription.needs_fetch_committed_offsets:\n            offsets = self.fetch_committed_offsets(self._subscription.assigned_partitions())\n            for partition, offset in six.iteritems(offsets):\n                # verify assignment is still active\n                if self._subscription.is_assigned(partition):\n                    self._subscription.assignment[partition].committed = offset.offset\n            self._subscription.needs_fetch_committed_offsets = False", "response": "Fetch committed offsets for assigned partitions."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fetch_committed_offsets(self, partitions):\n        if not partitions:\n            return {}\n\n        while True:\n            self.ensure_coordinator_ready()\n\n            # contact coordinator to fetch committed offsets\n            future = self._send_offset_fetch_request(partitions)\n            self._client.poll(future=future)\n\n            if future.succeeded():\n                return future.value\n\n            if not future.retriable():\n                raise future.exception # pylint: disable-msg=raising-bad-type\n\n            time.sleep(self.config['retry_backoff_ms'] / 1000)", "response": "Fetch the current committed offsets for the specified partitions"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclose the coordinator and resets local generation and member_id.", "response": "def close(self, autocommit=True):\n        \"\"\"Close the coordinator, leave the current group,\n        and reset local generation / member_id.\n\n        Keyword Arguments:\n            autocommit (bool): If auto-commit is configured for this consumer,\n                this optional flag causes the consumer to attempt to commit any\n                pending consumed offsets prior to close. Default: True\n        \"\"\"\n        try:\n            if autocommit:\n                self._maybe_auto_commit_offsets_sync()\n        finally:\n            super(ConsumerCoordinator, self).close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncommits specific offsets asynchronously.", "response": "def commit_offsets_async(self, offsets, callback=None):\n        \"\"\"Commit specific offsets asynchronously.\n\n        Arguments:\n            offsets (dict {TopicPartition: OffsetAndMetadata}): what to commit\n            callback (callable, optional): called as callback(offsets, response)\n                response will be either an Exception or a OffsetCommitResponse\n                struct. This callback can be used to trigger custom actions when\n                a commit request completes.\n\n        Returns:\n            kafka.future.Future\n        \"\"\"\n        self._invoke_completed_offset_commit_callbacks()\n        if not self.coordinator_unknown():\n            future = self._do_commit_offsets_async(offsets, callback)\n        else:\n            # we don't know the current coordinator, so try to find it and then\n            # send the commit or fail (we don't want recursive retries which can\n            # cause offset commits to arrive out of order). Note that there may\n            # be multiple offset commits chained to the same coordinator lookup\n            # request. This is fine because the listeners will be invoked in the\n            # same order that they were added. Note also that BaseCoordinator\n            # prevents multiple concurrent coordinator lookup requests.\n            future = self.lookup_coordinator()\n            future.add_callback(lambda r: functools.partial(self._do_commit_offsets_async, offsets, callback)())\n            if callback:\n                future.add_errback(lambda e: self.completed_offset_commits.appendleft((callback, offsets, e)))\n\n        # ensure the commit has a chance to be transmitted (without blocking on\n        # its completion). Note that commits are treated as heartbeats by the\n        # coordinator, so there is no need to explicitly allow heartbeats\n        # through delayed task execution.\n        self._client.poll(timeout_ms=0) # no wakeup if we add that feature\n\n        return future"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef commit_offsets_sync(self, offsets):\n        assert self.config['api_version'] >= (0, 8, 1), 'Unsupported Broker API'\n        assert all(map(lambda k: isinstance(k, TopicPartition), offsets))\n        assert all(map(lambda v: isinstance(v, OffsetAndMetadata),\n                       offsets.values()))\n        self._invoke_completed_offset_commit_callbacks()\n        if not offsets:\n            return\n\n        while True:\n            self.ensure_coordinator_ready()\n\n            future = self._send_offset_commit_request(offsets)\n            self._client.poll(future=future)\n\n            if future.succeeded():\n                return future.value\n\n            if not future.retriable():\n                raise future.exception # pylint: disable-msg=raising-bad-type\n\n            time.sleep(self.config['retry_backoff_ms'] / 1000)", "response": "Commit specific offsets synchronously."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _send_offset_commit_request(self, offsets):\n        assert self.config['api_version'] >= (0, 8, 1), 'Unsupported Broker API'\n        assert all(map(lambda k: isinstance(k, TopicPartition), offsets))\n        assert all(map(lambda v: isinstance(v, OffsetAndMetadata),\n                       offsets.values()))\n        if not offsets:\n            log.debug('No offsets to commit')\n            return Future().success(None)\n\n        node_id = self.coordinator()\n        if node_id is None:\n            return Future().failure(Errors.GroupCoordinatorNotAvailableError)\n\n\n        # create the offset commit request\n        offset_data = collections.defaultdict(dict)\n        for tp, offset in six.iteritems(offsets):\n            offset_data[tp.topic][tp.partition] = offset\n\n        if self._subscription.partitions_auto_assigned():\n            generation = self.generation()\n        else:\n            generation = Generation.NO_GENERATION\n\n        # if the generation is None, we are not part of an active group\n        # (and we expect to be). The only thing we can do is fail the commit\n        # and let the user rejoin the group in poll()\n        if self.config['api_version'] >= (0, 9) and generation is None:\n            return Future().failure(Errors.CommitFailedError())\n\n        if self.config['api_version'] >= (0, 9):\n            request = OffsetCommitRequest[2](\n                self.group_id,\n                generation.generation_id,\n                generation.member_id,\n                OffsetCommitRequest[2].DEFAULT_RETENTION_TIME,\n                [(\n                    topic, [(\n                        partition,\n                        offset.offset,\n                        offset.metadata\n                    ) for partition, offset in six.iteritems(partitions)]\n                ) for topic, partitions in six.iteritems(offset_data)]\n            )\n        elif self.config['api_version'] >= (0, 8, 2):\n            request = OffsetCommitRequest[1](\n                self.group_id, -1, '',\n                [(\n                    topic, [(\n                        partition,\n                        offset.offset,\n                        -1,\n                        offset.metadata\n                    ) for partition, offset in six.iteritems(partitions)]\n                ) for topic, partitions in six.iteritems(offset_data)]\n            )\n        elif self.config['api_version'] >= (0, 8, 1):\n            request = OffsetCommitRequest[0](\n                self.group_id,\n                [(\n                    topic, [(\n                        partition,\n                        offset.offset,\n                        offset.metadata\n                    ) for partition, offset in six.iteritems(partitions)]\n                ) for topic, partitions in six.iteritems(offset_data)]\n            )\n\n        log.debug(\"Sending offset-commit request with %s for group %s to %s\",\n                  offsets, self.group_id, node_id)\n\n        future = Future()\n        _f = self._client.send(node_id, request)\n        _f.add_callback(self._handle_offset_commit_response, offsets, future, time.time())\n        _f.add_errback(self._failed_request, node_id, request, future)\n        return future", "response": "This method is used to send the offset commit request to the broker."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending an OffsetFetchRequest to the broker.", "response": "def _send_offset_fetch_request(self, partitions):\n        \"\"\"Fetch the committed offsets for a set of partitions.\n\n        This is a non-blocking call. The returned future can be polled to get\n        the actual offsets returned from the broker.\n\n        Arguments:\n            partitions (list of TopicPartition): the partitions to fetch\n\n        Returns:\n            Future: resolves to dict of offsets: {TopicPartition: int}\n        \"\"\"\n        assert self.config['api_version'] >= (0, 8, 1), 'Unsupported Broker API'\n        assert all(map(lambda k: isinstance(k, TopicPartition), partitions))\n        if not partitions:\n            return Future().success({})\n\n        node_id = self.coordinator()\n        if node_id is None:\n            return Future().failure(Errors.GroupCoordinatorNotAvailableError)\n\n        # Verify node is ready\n        if not self._client.ready(node_id):\n            log.debug(\"Node %s not ready -- failing offset fetch request\",\n                      node_id)\n            return Future().failure(Errors.NodeNotReadyError)\n\n        log.debug(\"Group %s fetching committed offsets for partitions: %s\",\n                  self.group_id, partitions)\n        # construct the request\n        topic_partitions = collections.defaultdict(set)\n        for tp in partitions:\n            topic_partitions[tp.topic].add(tp.partition)\n\n        if self.config['api_version'] >= (0, 8, 2):\n            request = OffsetFetchRequest[1](\n                self.group_id,\n                list(topic_partitions.items())\n            )\n        else:\n            request = OffsetFetchRequest[0](\n                self.group_id,\n                list(topic_partitions.items())\n            )\n\n        # send the request with a callback\n        future = Future()\n        _f = self._client.send(node_id, request)\n        _f.add_callback(self._handle_offset_fetch_response, future)\n        _f.add_errback(self._failed_request, node_id, request, future)\n        return future"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsubscribe to a list of topics or a pattern.", "response": "def subscribe(self, topics=(), pattern=None, listener=None):\n        \"\"\"Subscribe to a list of topics, or a topic regex pattern.\n\n        Partitions will be dynamically assigned via a group coordinator.\n        Topic subscriptions are not incremental: this list will replace the\n        current assignment (if there is one).\n\n        This method is incompatible with assign_from_user()\n\n        Arguments:\n            topics (list): List of topics for subscription.\n            pattern (str): Pattern to match available topics. You must provide\n                either topics or pattern, but not both.\n            listener (ConsumerRebalanceListener): Optionally include listener\n                callback, which will be called before and after each rebalance\n                operation.\n\n                As part of group management, the consumer will keep track of the\n                list of consumers that belong to a particular group and will\n                trigger a rebalance operation if one of the following events\n                trigger:\n\n                * Number of partitions change for any of the subscribed topics\n                * Topic is created or deleted\n                * An existing member of the consumer group dies\n                * A new member is added to the consumer group\n\n                When any of these events are triggered, the provided listener\n                will be invoked first to indicate that the consumer's assignment\n                has been revoked, and then again when the new assignment has\n                been received. Note that this listener will immediately override\n                any listener set in a previous call to subscribe. It is\n                guaranteed, however, that the partitions revoked/assigned\n                through this interface are from topics subscribed in this call.\n        \"\"\"\n        if self._user_assignment or (topics and pattern):\n            raise IllegalStateError(self._SUBSCRIPTION_EXCEPTION_MESSAGE)\n        assert topics or pattern, 'Must provide topics or pattern'\n\n        if pattern:\n            log.info('Subscribing to pattern: /%s/', pattern)\n            self.subscription = set()\n            self.subscribed_pattern = re.compile(pattern)\n        else:\n            self.change_subscription(topics)\n\n        if listener and not isinstance(listener, ConsumerRebalanceListener):\n            raise TypeError('listener must be a ConsumerRebalanceListener')\n        self.listener = listener"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nensures that the topic name is valid according to the kafka source.", "response": "def _ensure_valid_topic_name(self, topic):\n        \"\"\" Ensures that the topic name is valid according to the kafka source. \"\"\"\n\n        # See Kafka Source:\n        # https://github.com/apache/kafka/blob/39eb31feaeebfb184d98cc5d94da9148c2319d81/clients/src/main/java/org/apache/kafka/common/internals/Topic.java\n        if topic is None:\n            raise TypeError('All topics must not be None')\n        if not isinstance(topic, six.string_types):\n            raise TypeError('All topics must be strings')\n        if len(topic) == 0:\n            raise ValueError('All topics must be non-empty strings')\n        if topic == '.' or topic == '..':\n            raise ValueError('Topic name cannot be \".\" or \"..\"')\n        if len(topic) > self._MAX_NAME_LENGTH:\n            raise ValueError('Topic name is illegal, it can\\'t be longer than {0} characters, topic: \"{1}\"'.format(self._MAX_NAME_LENGTH, topic))\n        if not self._TOPIC_LEGAL_CHARS.match(topic):\n            raise ValueError('Topic name \"{0}\" is illegal, it contains a character other than ASCII alphanumerics, \".\", \"_\" and \"-\"'.format(topic))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef change_subscription(self, topics):\n        if self._user_assignment:\n            raise IllegalStateError(self._SUBSCRIPTION_EXCEPTION_MESSAGE)\n\n        if isinstance(topics, six.string_types):\n            topics = [topics]\n\n        if self.subscription == set(topics):\n            log.warning(\"subscription unchanged by change_subscription(%s)\",\n                        topics)\n            return\n\n        for t in topics:\n            self._ensure_valid_topic_name(t)\n\n        log.info('Updating subscribed topics to: %s', topics)\n        self.subscription = set(topics)\n        self._group_subscription.update(topics)\n\n        # Remove any assigned partitions which are no longer subscribed to\n        for tp in set(self.assignment.keys()):\n            if tp.topic not in self.subscription:\n                del self.assignment[tp]", "response": "Change the topic subscription."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds topics to the current group subscription.", "response": "def group_subscribe(self, topics):\n        \"\"\"Add topics to the current group subscription.\n\n        This is used by the group leader to ensure that it receives metadata\n        updates for all topics that any member of the group is subscribed to.\n\n        Arguments:\n            topics (list of str): topics to add to the group subscription\n        \"\"\"\n        if self._user_assignment:\n            raise IllegalStateError(self._SUBSCRIPTION_EXCEPTION_MESSAGE)\n        self._group_subscription.update(topics)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nresetting the group s subscription to only contain topics subscribed by this consumer.", "response": "def reset_group_subscription(self):\n        \"\"\"Reset the group's subscription to only contain topics subscribed by this consumer.\"\"\"\n        if self._user_assignment:\n            raise IllegalStateError(self._SUBSCRIPTION_EXCEPTION_MESSAGE)\n        assert self.subscription is not None, 'Subscription required'\n        self._group_subscription.intersection_update(self.subscription)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nassigning a list of TopicPartitions to this consumer.", "response": "def assign_from_user(self, partitions):\n        \"\"\"Manually assign a list of TopicPartitions to this consumer.\n\n        This interface does not allow for incremental assignment and will\n        replace the previous assignment (if there was one).\n\n        Manual topic assignment through this method does not use the consumer's\n        group management functionality. As such, there will be no rebalance\n        operation triggered when group membership or cluster and topic metadata\n        change. Note that it is not possible to use both manual partition\n        assignment with assign() and group assignment with subscribe().\n\n        Arguments:\n            partitions (list of TopicPartition): assignment for this instance.\n\n        Raises:\n            IllegalStateError: if consumer has already called subscribe()\n        \"\"\"\n        if self.subscription is not None:\n            raise IllegalStateError(self._SUBSCRIPTION_EXCEPTION_MESSAGE)\n\n        if self._user_assignment != set(partitions):\n            self._user_assignment = set(partitions)\n\n            for partition in partitions:\n                if partition not in self.assignment:\n                    self._add_assigned_partition(partition)\n\n            for tp in set(self.assignment.keys()) - self._user_assignment:\n                del self.assignment[tp]\n\n            self.needs_fetch_committed_offsets = True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the assignment to the specified partitions based on the topic subscription.", "response": "def assign_from_subscribed(self, assignments):\n        \"\"\"Update the assignment to the specified partitions\n\n        This method is called by the coordinator to dynamically assign\n        partitions based on the consumer's topic subscription. This is different\n        from assign_from_user() which directly sets the assignment from a\n        user-supplied TopicPartition list.\n\n        Arguments:\n            assignments (list of TopicPartition): partitions to assign to this\n                consumer instance.\n        \"\"\"\n        if not self.partitions_auto_assigned():\n            raise IllegalStateError(self._SUBSCRIPTION_EXCEPTION_MESSAGE)\n\n        for tp in assignments:\n            if tp.topic not in self.subscription:\n                raise ValueError(\"Assigned partition %s for non-subscribed topic.\" % (tp,))\n\n        # after rebalancing, we always reinitialize the assignment state\n        self.assignment.clear()\n        for tp in assignments:\n            self._add_assigned_partition(tp)\n        self.needs_fetch_committed_offsets = True\n        log.info(\"Updated partition assignment: %s\", assignments)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unsubscribe(self):\n        self.subscription = None\n        self._user_assignment.clear()\n        self.assignment.clear()\n        self.subscribed_pattern = None", "response": "Clear all topic subscriptions and partition assignments"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn current set of paused TopicPartitions.", "response": "def paused_partitions(self):\n        \"\"\"Return current set of paused TopicPartitions.\"\"\"\n        return set(partition for partition in self.assignment\n                   if self.is_paused(partition))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fetchable_partitions(self):\n        fetchable = set()\n        for partition, state in six.iteritems(self.assignment):\n            if state.is_fetchable():\n                fetchable.add(partition)\n        return fetchable", "response": "Return set of TopicPartitions that should be Fetched."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dict of all consumed offsets as OffsetAndMetadata", "response": "def all_consumed_offsets(self):\n        \"\"\"Returns consumed offsets as {TopicPartition: OffsetAndMetadata}\"\"\"\n        all_consumed = {}\n        for partition, state in six.iteritems(self.assignment):\n            if state.has_valid_position:\n                all_consumed[partition] = OffsetAndMetadata(state.position, '')\n        return all_consumed"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef need_offset_reset(self, partition, offset_reset_strategy=None):\n        if offset_reset_strategy is None:\n            offset_reset_strategy = self._default_offset_reset_strategy\n        self.assignment[partition].await_reset(offset_reset_strategy)", "response": "Mark the specified topic partition for offset reset."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a cleanup clojure that doesn t increase our ref count", "response": "def _cleanup_factory(self):\n        \"\"\"Build a cleanup clojure that doesn't increase our ref count\"\"\"\n        _self = weakref.proxy(self)\n        def wrapper():\n            try:\n                _self.close(timeout=0)\n            except (ReferenceError, AttributeError):\n                pass\n        return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclose the Kafka producer.", "response": "def close(self, timeout=None):\n        \"\"\"Close this producer.\n\n        Arguments:\n            timeout (float, optional): timeout in seconds to wait for completion.\n        \"\"\"\n\n        # drop our atexit handler now to avoid leaks\n        self._unregister_cleanup()\n\n        if not hasattr(self, '_closed') or self._closed:\n            log.info('Kafka producer closed')\n            return\n        if timeout is None:\n            # threading.TIMEOUT_MAX is available in Python3.3+\n            timeout = getattr(threading, 'TIMEOUT_MAX', float('inf'))\n        if getattr(threading, 'TIMEOUT_MAX', False):\n            assert 0 <= timeout <= getattr(threading, 'TIMEOUT_MAX')\n        else:\n            assert timeout >= 0\n\n        log.info(\"Closing the Kafka producer with %s secs timeout.\", timeout)\n        #first_exception = AtomicReference() # this will keep track of the first encountered exception\n        invoked_from_callback = bool(threading.current_thread() is self._sender)\n        if timeout > 0:\n            if invoked_from_callback:\n                log.warning(\"Overriding close timeout %s secs to 0 in order to\"\n                            \" prevent useless blocking due to self-join. This\"\n                            \" means you have incorrectly invoked close with a\"\n                            \" non-zero timeout from the producer call-back.\",\n                            timeout)\n            else:\n                # Try to close gracefully.\n                if self._sender is not None:\n                    self._sender.initiate_close()\n                    self._sender.join(timeout)\n\n        if self._sender is not None and self._sender.is_alive():\n\n            log.info(\"Proceeding to force close the producer since pending\"\n                     \" requests could not be completed within timeout %s.\",\n                     timeout)\n            self._sender.force_close()\n            # Only join the sender thread when not calling from callback.\n            if not invoked_from_callback:\n                self._sender.join()\n\n        self._metrics.close()\n        try:\n            self.config['key_serializer'].close()\n        except AttributeError:\n            pass\n        try:\n            self.config['value_serializer'].close()\n        except AttributeError:\n            pass\n        self._closed = True\n        log.debug(\"The Kafka producer has closed.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns set of all known partitions for the topic.", "response": "def partitions_for(self, topic):\n        \"\"\"Returns set of all known partitions for the topic.\"\"\"\n        max_wait = self.config['max_block_ms'] / 1000.0\n        return self._wait_on_metadata(topic, max_wait)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npublishes a message to a topic.", "response": "def send(self, topic, value=None, key=None, headers=None, partition=None, timestamp_ms=None):\n        \"\"\"Publish a message to a topic.\n\n        Arguments:\n            topic (str): topic where the message will be published\n            value (optional): message value. Must be type bytes, or be\n                serializable to bytes via configured value_serializer. If value\n                is None, key is required and message acts as a 'delete'.\n                See kafka compaction documentation for more details:\n                https://kafka.apache.org/documentation.html#compaction\n                (compaction requires kafka >= 0.8.1)\n            partition (int, optional): optionally specify a partition. If not\n                set, the partition will be selected using the configured\n                'partitioner'.\n            key (optional): a key to associate with the message. Can be used to\n                determine which partition to send the message to. If partition\n                is None (and producer's partitioner config is left as default),\n                then messages with the same key will be delivered to the same\n                partition (but if key is None, partition is chosen randomly).\n                Must be type bytes, or be serializable to bytes via configured\n                key_serializer.\n            headers (optional): a list of header key value pairs. List items\n                are tuples of str key and bytes value.\n            timestamp_ms (int, optional): epoch milliseconds (from Jan 1 1970 UTC)\n                to use as the message timestamp. Defaults to current time.\n\n        Returns:\n            FutureRecordMetadata: resolves to RecordMetadata\n\n        Raises:\n            KafkaTimeoutError: if unable to fetch topic metadata, or unable\n                to obtain memory buffer prior to configured max_block_ms\n        \"\"\"\n        assert value is not None or self.config['api_version'] >= (0, 8, 1), (\n            'Null messages require kafka >= 0.8.1')\n        assert not (value is None and key is None), 'Need at least one: key or value'\n        key_bytes = value_bytes = None\n        try:\n            self._wait_on_metadata(topic, self.config['max_block_ms'] / 1000.0)\n\n            key_bytes = self._serialize(\n                self.config['key_serializer'],\n                topic, key)\n            value_bytes = self._serialize(\n                self.config['value_serializer'],\n                topic, value)\n            assert type(key_bytes) in (bytes, bytearray, memoryview, type(None))\n            assert type(value_bytes) in (bytes, bytearray, memoryview, type(None))\n\n            partition = self._partition(topic, partition, key, value,\n                                        key_bytes, value_bytes)\n\n            if headers is None:\n                headers = []\n            assert type(headers) == list\n            assert all(type(item) == tuple and len(item) == 2 and type(item[0]) == str and type(item[1]) == bytes for item in headers)\n\n            message_size = self._estimate_size_in_bytes(key_bytes, value_bytes, headers)\n            self._ensure_valid_record_size(message_size)\n\n            tp = TopicPartition(topic, partition)\n            log.debug(\"Sending (key=%r value=%r headers=%r) to %s\", key, value, headers, tp)\n            result = self._accumulator.append(tp, timestamp_ms,\n                                              key_bytes, value_bytes, headers,\n                                              self.config['max_block_ms'],\n                                              estimated_size=message_size)\n            future, batch_is_full, new_batch_created = result\n            if batch_is_full or new_batch_created:\n                log.debug(\"Waking up the sender since %s is either full or\"\n                          \" getting a new batch\", tp)\n                self._sender.wakeup()\n\n            return future\n            # handling exceptions and record the errors;\n            # for API exceptions return them in the future,\n            # for other exceptions raise directly\n        except Errors.BrokerResponseError as e:\n            log.debug(\"Exception occurred during message send: %s\", e)\n            return FutureRecordMetadata(\n                FutureProduceResult(TopicPartition(topic, partition)),\n                -1, None, None,\n                len(key_bytes) if key_bytes is not None else -1,\n                len(value_bytes) if value_bytes is not None else -1,\n                sum(len(h_key.encode(\"utf-8\")) + len(h_value) for h_key, h_value in headers) if headers else -1,\n            ).failure(e)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninvokes this method makes all buffered records immediately available to send (even if linger_ms is greater than 0) and blocks on the completion of the requests associated with these records. The post-condition of :meth:`~kafka.KafkaProducer.flush` is that any previously sent record will have completed (e.g. Future.is_done() == True). A request is considered completed when either it is successfully acknowledged according to the 'acks' configuration for the producer, or it results in an error. Other threads can continue sending messages while one thread is blocked waiting for a flush call to complete; however, no guarantee is made about the completion of messages sent after the flush call begins. Arguments: timeout (float, optional): timeout in seconds to wait for completion. Raises: KafkaTimeoutError: failure to flush buffered records within the provided timeout", "response": "def flush(self, timeout=None):\n        \"\"\"\n        Invoking this method makes all buffered records immediately available\n        to send (even if linger_ms is greater than 0) and blocks on the\n        completion of the requests associated with these records. The\n        post-condition of :meth:`~kafka.KafkaProducer.flush` is that any\n        previously sent record will have completed\n        (e.g. Future.is_done() == True). A request is considered completed when\n        either it is successfully acknowledged according to the 'acks'\n        configuration for the producer, or it results in an error.\n\n        Other threads can continue sending messages while one thread is blocked\n        waiting for a flush call to complete; however, no guarantee is made\n        about the completion of messages sent after the flush call begins.\n\n        Arguments:\n            timeout (float, optional): timeout in seconds to wait for completion.\n\n        Raises:\n            KafkaTimeoutError: failure to flush buffered records within the\n                provided timeout\n        \"\"\"\n        log.debug(\"Flushing accumulated records in producer.\")  # trace\n        self._accumulator.begin_flush()\n        self._sender.wakeup()\n        self._accumulator.await_flush_completion(timeout=timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvalidates that the record size isn t too large.", "response": "def _ensure_valid_record_size(self, size):\n        \"\"\"Validate that the record size isn't too large.\"\"\"\n        if size > self.config['max_request_size']:\n            raise Errors.MessageSizeTooLargeError(\n                \"The message is %d bytes when serialized which is larger than\"\n                \" the maximum request size you have configured with the\"\n                \" max_request_size configuration\" % (size,))\n        if size > self.config['buffer_memory']:\n            raise Errors.MessageSizeTooLargeError(\n                \"The message is %d bytes when serialized which is larger than\"\n                \" the total memory buffer you have configured with the\"\n                \" buffer_memory configuration.\" % (size,))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwait for the cluster metadata including partitions for the given topic to be available.", "response": "def _wait_on_metadata(self, topic, max_wait):\n        \"\"\"\n        Wait for cluster metadata including partitions for the given topic to\n        be available.\n\n        Arguments:\n            topic (str): topic we want metadata for\n            max_wait (float): maximum time in secs for waiting on the metadata\n\n        Returns:\n            set: partition ids for the topic\n\n        Raises:\n            KafkaTimeoutError: if partitions for topic were not obtained before\n                specified max_wait timeout\n        \"\"\"\n        # add topic to metadata topic list if it is not there already.\n        self._sender.add_topic(topic)\n        begin = time.time()\n        elapsed = 0.0\n        metadata_event = None\n        while True:\n            partitions = self._metadata.partitions_for_topic(topic)\n            if partitions is not None:\n                return partitions\n\n            if not metadata_event:\n                metadata_event = threading.Event()\n\n            log.debug(\"Requesting metadata update for topic %s\", topic)\n\n            metadata_event.clear()\n            future = self._metadata.request_update()\n            future.add_both(lambda e, *args: e.set(), metadata_event)\n            self._sender.wakeup()\n            metadata_event.wait(max_wait - elapsed)\n            elapsed = time.time() - begin\n            if not metadata_event.is_set():\n                raise Errors.KafkaTimeoutError(\n                    \"Failed to update metadata after %.1f secs.\" % (max_wait,))\n            elif topic in self._metadata.unauthorized_topics:\n                raise Errors.TopicAuthorizationFailedError(topic)\n            else:\n                log.debug(\"_wait_on_metadata woke after %s secs.\", elapsed)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef metrics(self, raw=False):\n        if raw:\n            return self._metrics.metrics.copy()\n\n        metrics = {}\n        for k, v in six.iteritems(self._metrics.metrics.copy()):\n            if k.group not in metrics:\n                metrics[k.group] = {}\n            if k.name not in metrics[k.group]:\n                metrics[k.group][k.name] = {}\n            metrics[k.group][k.name] = v.value()\n        return metrics", "response": "Get metrics on producer performance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reset_partition_offset(self, partition):\n        LATEST = -1\n        EARLIEST = -2\n        if self.auto_offset_reset == 'largest':\n            reqs = [OffsetRequestPayload(self.topic, partition, LATEST, 1)]\n        elif self.auto_offset_reset == 'smallest':\n            reqs = [OffsetRequestPayload(self.topic, partition, EARLIEST, 1)]\n        else:\n            # Let's raise an reasonable exception type if user calls\n            # outside of an exception context\n            if sys.exc_info() == (None, None, None):\n                raise OffsetOutOfRangeError('Cannot reset partition offsets without a '\n                                            'valid auto_offset_reset setting '\n                                            '(largest|smallest)')\n            # Otherwise we should re-raise the upstream exception\n            # b/c it typically includes additional data about\n            # the request that triggered it, and we do not want to drop that\n            raise # pylint: disable=E0704\n\n        # send_offset_request\n        log.info('Resetting topic-partition offset to %s for %s:%d',\n                 self.auto_offset_reset, self.topic, partition)\n        try:\n            (resp, ) = self.client.send_offset_request(reqs)\n        except KafkaError as e:\n            log.error('%s sending offset request for %s:%d',\n                      e.__class__.__name__, self.topic, partition)\n        else:\n            self.offsets[partition] = resp.offsets[0]\n            self.fetch_offsets[partition] = resp.offsets[0]\n            return resp.offsets[0]", "response": "Reset the offset for the specified topic - partition."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef seek(self, offset, whence=None, partition=None):\n\n        if whence is None: # set an absolute offset\n            if partition is None:\n                for tmp_partition in self.offsets:\n                    self.offsets[tmp_partition] = offset\n            else:\n                self.offsets[partition] = offset\n        elif whence == 1:  # relative to current position\n            if partition is None:\n                for tmp_partition, _offset in self.offsets.items():\n                    self.offsets[tmp_partition] = _offset + offset\n            else:\n                self.offsets[partition] += offset\n        elif whence in (0, 2):  # relative to beginning or end\n            reqs = []\n            deltas = {}\n            if partition is None:\n                # divide the request offset by number of partitions,\n                # distribute the remained evenly\n                (delta, rem) = divmod(offset, len(self.offsets))\n                for tmp_partition, r in izip_longest(self.offsets.keys(),\n                                                     repeat(1, rem),\n                                                     fillvalue=0):\n                    deltas[tmp_partition] = delta + r\n\n                for tmp_partition in self.offsets.keys():\n                    if whence == 0:\n                        reqs.append(OffsetRequestPayload(self.topic, tmp_partition, -2, 1))\n                    elif whence == 2:\n                        reqs.append(OffsetRequestPayload(self.topic, tmp_partition, -1, 1))\n                    else:\n                        pass\n            else:\n                deltas[partition] = offset\n                if whence == 0:\n                    reqs.append(OffsetRequestPayload(self.topic, partition, -2, 1))\n                elif whence == 2:\n                    reqs.append(OffsetRequestPayload(self.topic, partition, -1, 1))\n                else:\n                    pass\n\n            resps = self.client.send_offset_request(reqs)\n            for resp in resps:\n                self.offsets[resp.partition] = \\\n                    resp.offsets[0] + deltas[resp.partition]\n        else:\n            raise ValueError('Unexpected value for `whence`, %d' % (whence,))\n\n        # Reset queue and fetch offsets since they are invalid\n        self.fetch_offsets = self.offsets.copy()\n        self.count_since_commit += 1\n        if self.auto_commit:\n            self.commit()\n\n        self.queue = queue.Queue()", "response": "Seeks the consumer to the specified offset in the consumer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_messages(self, count=1, block=True, timeout=0.1):\n        messages = []\n        if timeout is not None:\n            timeout += time.time()\n\n        new_offsets = {}\n        log.debug('getting %d messages', count)\n        while len(messages) < count:\n            block_time = timeout - time.time()\n            log.debug('calling _get_message block=%s timeout=%s', block, block_time)\n            block_next_call = block is True or block > len(messages)\n            result = self._get_message(block_next_call, block_time,\n                                       get_partition_info=True,\n                                       update_offset=False)\n            log.debug('got %s from _get_messages', result)\n            if not result:\n                if block_next_call and (timeout is None or time.time() <= timeout):\n                    continue\n                break\n\n            partition, message = result\n            _msg = (partition, message) if self.partition_info else message\n            messages.append(_msg)\n            new_offsets[partition] = message.offset + 1\n\n        # Update and commit offsets if necessary\n        self.offsets.update(new_offsets)\n        self.count_since_commit += len(messages)\n        self._auto_commit()\n        log.debug('got %d messages: %s', len(messages), messages)\n        return messages", "response": "Fetch the specified number of messages from the API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a message from the queue.", "response": "def _get_message(self, block=True, timeout=0.1, get_partition_info=None,\n                     update_offset=True):\n        \"\"\"\n        If no messages can be fetched, returns None.\n        If get_partition_info is None, it defaults to self.partition_info\n        If get_partition_info is True, returns (partition, message)\n        If get_partition_info is False, returns message\n        \"\"\"\n        start_at = time.time()\n        while self.queue.empty():\n            # We're out of messages, go grab some more.\n            log.debug('internal queue empty, fetching more messages')\n            with FetchContext(self, block, timeout):\n                self._fetch()\n\n            if not block or time.time() > (start_at + timeout):\n                break\n\n        try:\n            partition, message = self.queue.get_nowait()\n\n            if update_offset:\n                # Update partition offset\n                self.offsets[partition] = message.offset + 1\n\n                # Count, check and commit messages if necessary\n                self.count_since_commit += 1\n                self._auto_commit()\n\n            if get_partition_info is None:\n                get_partition_info = self.partition_info\n            if get_partition_info:\n                return partition, message\n            else:\n                return message\n        except queue.Empty:\n            log.debug('internal queue empty after fetch - returning None')\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef encode_varint_1(num):\n    # Shift sign to the end of number\n    num = (num << 1) ^ (num >> 63)\n    # Max 10 bytes. We assert those are allocated\n    buf = bytearray(10)\n\n    for i in range(10):\n        # 7 lowest bits from the number and set 8th if we still have pending\n        # bits left to encode\n        buf[i] = num & 0x7f | (0x80 if num > 0x7f else 0)\n        num = num >> 7\n        if num == 0:\n            break\n    else:\n        # Max size of endcoded double is 10 bytes for unsigned values\n        raise ValueError(\"Out of double range\")\n    return buf[:i + 1]", "response": "Encode an integer to a varint presentation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef size_of_varint_1(value):\n    value = (value << 1) ^ (value >> 63)\n    res = 0\n    while True:\n        res += 1\n        value = value >> 7\n        if value == 0:\n            break\n    return res", "response": "Returns the number of bytes needed to encode an integer in variable - length format."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the number of bytes needed to encode an integer in variable - length format.", "response": "def size_of_varint_2(value):\n    \"\"\" Number of bytes needed to encode an integer in variable-length format.\n    \"\"\"\n    value = (value << 1) ^ (value >> 63)\n    if value <= 0x7f:\n        return 1\n    if value <= 0x3fff:\n        return 2\n    if value <= 0x1fffff:\n        return 3\n    if value <= 0xfffffff:\n        return 4\n    if value <= 0x7ffffffff:\n        return 5\n    if value <= 0x3ffffffffff:\n        return 6\n    if value <= 0x1ffffffffffff:\n        return 7\n    if value <= 0xffffffffffffff:\n        return 8\n    if value <= 0x7fffffffffffffff:\n        return 9\n    return 10"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndecoding an integer from a varint presentation.", "response": "def decode_varint_1(buffer, pos=0):\n    \"\"\" Decode an integer from a varint presentation. See\n    https://developers.google.com/protocol-buffers/docs/encoding?csw=1#varints\n    on how those can be produced.\n\n        Arguments:\n            buffer (bytes-like): any object acceptable by ``memoryview``\n            pos (int): optional position to read from\n\n        Returns:\n            (int, int): Decoded int value and next read position\n    \"\"\"\n    value = 0\n    shift = 0\n    memview = memoryview(buffer)\n    for i in range(pos, pos + 10):\n        try:\n            byte = _read_byte(memview, i)\n        except IndexError:\n            raise ValueError(\"End of byte stream\")\n        if byte & 0x80 != 0:\n            value |= (byte & 0x7f) << shift\n            shift += 7\n        else:\n            value |= byte << shift\n            break\n    else:\n        # Max size of endcoded double is 10 bytes for unsigned values\n        raise ValueError(\"Out of double range\")\n    # Normalize sign\n    return (value >> 1) ^ -(value & 1), i + 1"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef purge_obsolete_samples(self, config, now):\n        expire_age = config.samples * config.time_window_ms\n        for sample in self._samples:\n            if now - sample.last_window_ms >= expire_age:\n                sample.reset(now)", "response": "Purge obsolete samples that have expired in the absence of any events."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncloses the KafkaAdminClient connection to the Kafka broker.", "response": "def close(self):\n        \"\"\"Close the KafkaAdminClient connection to the Kafka broker.\"\"\"\n        if not hasattr(self, '_closed') or self._closed:\n            log.info(\"KafkaAdminClient already closed.\")\n            return\n\n        self._metrics.close()\n        self._client.close()\n        self._closed = True\n        log.debug(\"KafkaAdminClient is now closed.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _matching_api_version(self, operation):\n        version = min(len(operation) - 1,\n                      self._client.get_api_versions()[operation[0].API_KEY][1])\n        if version < self._client.get_api_versions()[operation[0].API_KEY][0]:\n            # max library version is less than min broker version. Currently,\n            # no Kafka versions specify a min msg version. Maybe in the future?\n            raise IncompatibleBrokerVersion(\n                \"No version of the '{}' Kafka protocol is supported by both the client and broker.\"\n                .format(operation.__name__))\n        return version", "response": "Find the latest version of the protocol operation supported by both this library and the broker."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetermines the Kafka cluster controller.", "response": "def _refresh_controller_id(self):\n        \"\"\"Determine the Kafka cluster controller.\"\"\"\n        version = self._matching_api_version(MetadataRequest)\n        if 1 <= version <= 6:\n            request = MetadataRequest[version]()\n            response = self._send_request_to_node(self._client.least_loaded_node(), request)\n            controller_id = response.controller_id\n            # verify the controller is new enough to support our requests\n            controller_version = self._client.check_version(controller_id)\n            if controller_version < (0, 10, 0):\n                raise IncompatibleBrokerVersion(\n                    \"The controller appears to be running Kafka {}. KafkaAdminClient requires brokers >= 0.10.0.0.\"\n                    .format(controller_version))\n            self._controller_id = controller_id\n        else:\n            raise UnrecognizedBrokerVersion(\n                \"Kafka Admin interface cannot determine the controller using MetadataRequest_v{}.\"\n                .format(version))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the broker node_id of the coordinator of the given group. Sends a FindCoordinatorRequest message to the cluster. Will block until the FindCoordinatorResponse is received. Any errors are immediately raised. :param group_id: The consumer group ID. This is typically the group name as a string. :return: The node_id of the broker that is the coordinator.", "response": "def _find_group_coordinator_id(self, group_id):\n        \"\"\"Find the broker node_id of the coordinator of the given group.\n\n        Sends a FindCoordinatorRequest message to the cluster. Will block until\n        the FindCoordinatorResponse is received. Any errors are immediately\n        raised.\n\n        :param group_id: The consumer group ID. This is typically the group\n            name as a string.\n        :return: The node_id of the broker that is the coordinator.\n        \"\"\"\n        # Note: Java may change how this is implemented in KAFKA-6791.\n        #\n        # TODO add support for dynamically picking version of\n        # GroupCoordinatorRequest which was renamed to FindCoordinatorRequest.\n        # When I experimented with this, GroupCoordinatorResponse_v1 didn't\n        # match GroupCoordinatorResponse_v0 and I couldn't figure out why.\n        gc_request = GroupCoordinatorRequest[0](group_id)\n        gc_response = self._send_request_to_node(self._client.least_loaded_node(), gc_request)\n        # use the extra error checking in add_group_coordinator() rather than\n        # immediately returning the group coordinator.\n        success = self._client.cluster.add_group_coordinator(group_id, gc_response)\n        if not success:\n            error_type = Errors.for_code(gc_response.error_code)\n            assert error_type is not Errors.NoError\n            # Note: When error_type.retriable, Java will retry... see\n            # KafkaAdminClient's handleFindCoordinatorError method\n            raise error_type(\n                \"Could not identify group coordinator for group_id '{}' from response '{}'.\"\n                .format(group_id, gc_response))\n        group_coordinator = self._client.cluster.coordinator_for_group(group_id)\n        # will be None if the coordinator was never populated, which should never happen here\n        assert group_coordinator is not None\n        # will be -1 if add_group_coordinator() failed... but by this point the\n        # error should have been raised.\n        assert group_coordinator != -1\n        return group_coordinator"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a request to a specific broker.", "response": "def _send_request_to_node(self, node_id, request):\n        \"\"\"Send a Kafka protocol message to a specific broker.\n\n        Will block until the message result is received.\n\n        :param node_id: The broker id to which to send the message.\n        :param request: The message to send.\n        :return: The Kafka protocol response for the message.\n        :exception: The exception if the message could not be sent.\n        \"\"\"\n        while not self._client.ready(node_id):\n            # poll until the connection to broker is ready, otherwise send()\n            # will fail with NodeNotReadyError\n            self._client.poll()\n        future = self._client.send(node_id, request)\n        self._client.poll(future=future)\n        if future.succeeded():\n            return future.value\n        else:\n            raise future.exception"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _send_request_to_controller(self, request):\n        tries = 2  # in case our cached self._controller_id is outdated\n        while tries:\n            tries -= 1\n            response = self._send_request_to_node(self._controller_id, request)\n            # In Java, the error fieldname is inconsistent:\n            #  - CreateTopicsResponse / CreatePartitionsResponse uses topic_errors\n            #  - DeleteTopicsResponse uses topic_error_codes\n            # So this is a little brittle in that it assumes all responses have\n            # one of these attributes and that they always unpack into\n            # (topic, error_code) tuples.\n            topic_error_tuples = (response.topic_errors if hasattr(response, 'topic_errors')\n                else response.topic_error_codes)\n            # Also small py2/py3 compatibility -- py3 can ignore extra values\n            # during unpack via: for x, y, *rest in list_of_values. py2 cannot.\n            # So for now we have to map across the list and explicitly drop any\n            # extra values (usually the error_message)\n            for topic, error_code in map(lambda e: e[:2], topic_error_tuples):\n                error_type = Errors.for_code(error_code)\n                if tries and error_type is NotControllerError:\n                    # No need to inspect the rest of the errors for\n                    # non-retriable errors because NotControllerError should\n                    # either be thrown for all errors or no errors.\n                    self._refresh_controller_id()\n                    break\n                elif error_type is not Errors.NoError:\n                    raise error_type(\n                        \"Request '{}' failed with response '{}'.\"\n                        .format(request, response))\n            else:\n                return response\n        raise RuntimeError(\"This should never happen, please file a bug with full stacktrace if encountered\")", "response": "Send a request to the Kafka cluster controller."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating new topics in the cluster.", "response": "def create_topics(self, new_topics, timeout_ms=None, validate_only=False):\n        \"\"\"Create new topics in the cluster.\n\n        :param new_topics: A list of NewTopic objects.\n        :param timeout_ms: Milliseconds to wait for new topics to be created\n            before the broker returns.\n        :param validate_only: If True, don't actually create new topics.\n            Not supported by all versions. Default: False\n        :return: Appropriate version of CreateTopicResponse class.\n        \"\"\"\n        version = self._matching_api_version(CreateTopicsRequest)\n        timeout_ms = self._validate_timeout(timeout_ms)\n        if version == 0:\n            if validate_only:\n                raise IncompatibleBrokerVersion(\n                    \"validate_only requires CreateTopicsRequest >= v1, which is not supported by Kafka {}.\"\n                    .format(self.config['api_version']))\n            request = CreateTopicsRequest[version](\n                create_topic_requests=[self._convert_new_topic_request(new_topic) for new_topic in new_topics],\n                timeout=timeout_ms\n            )\n        elif version <= 2:\n            request = CreateTopicsRequest[version](\n                create_topic_requests=[self._convert_new_topic_request(new_topic) for new_topic in new_topics],\n                timeout=timeout_ms,\n                validate_only=validate_only\n            )\n        else:\n            raise NotImplementedError(\n                \"Support for CreateTopics v{} has not yet been added to KafkaAdminClient.\"\n                .format(version))\n        # TODO convert structs to a more pythonic interface\n        # TODO raise exceptions if errors\n        return self._send_request_to_controller(request)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_topics(self, topics, timeout_ms=None):\n        version = self._matching_api_version(DeleteTopicsRequest)\n        timeout_ms = self._validate_timeout(timeout_ms)\n        if version <= 1:\n            request = DeleteTopicsRequest[version](\n                topics=topics,\n                timeout=timeout_ms\n            )\n            response = self._send_request_to_controller(request)\n        else:\n            raise NotImplementedError(\n                \"Support for DeleteTopics v{} has not yet been added to KafkaAdminClient.\"\n                .format(version))\n        return response", "response": "Delete topics from the broker."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfetches configuration parameters for one or more Kafka resources.", "response": "def describe_configs(self, config_resources, include_synonyms=False):\n        \"\"\"Fetch configuration parameters for one or more Kafka resources.\n\n        :param config_resources: An list of ConfigResource objects.\n            Any keys in ConfigResource.configs dict will be used to filter the\n            result. Setting the configs dict to None will get all values. An\n            empty dict will get zero values (as per Kafka protocol).\n        :param include_synonyms: If True, return synonyms in response. Not\n            supported by all versions. Default: False.\n        :return: Appropriate version of DescribeConfigsResponse class.\n        \"\"\"\n        version = self._matching_api_version(DescribeConfigsRequest)\n        if version == 0:\n            if include_synonyms:\n                raise IncompatibleBrokerVersion(\n                    \"include_synonyms requires DescribeConfigsRequest >= v1, which is not supported by Kafka {}.\"\n                    .format(self.config['api_version']))\n            request = DescribeConfigsRequest[version](\n                resources=[self._convert_describe_config_resource_request(config_resource) for config_resource in config_resources]\n            )\n        elif version == 1:\n            request = DescribeConfigsRequest[version](\n                resources=[self._convert_describe_config_resource_request(config_resource) for config_resource in config_resources],\n                include_synonyms=include_synonyms\n            )\n        else:\n            raise NotImplementedError(\n                \"Support for DescribeConfigs v{} has not yet been added to KafkaAdminClient.\"\n                .format(version))\n        return self._send_request_to_node(self._client.least_loaded_node(), request)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef alter_configs(self, config_resources):\n        version = self._matching_api_version(AlterConfigsRequest)\n        if version == 0:\n            request = AlterConfigsRequest[version](\n                resources=[self._convert_alter_config_resource_request(config_resource) for config_resource in config_resources]\n            )\n        else:\n            raise NotImplementedError(\n                \"Support for AlterConfigs v{} has not yet been added to KafkaAdminClient.\"\n                .format(version))\n        # TODO the Java client has the note:\n        # // We must make a separate AlterConfigs request for every BROKER resource we want to alter\n        # // and send the request to that specific broker. Other resources are grouped together into\n        # // a single request that may be sent to any broker.\n        #\n        # So this is currently broken as it always sends to the least_loaded_node()\n        return self._send_request_to_node(self._client.least_loaded_node(), request)", "response": "Send an AlterConfigs request to the broker that the broker has already set up."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_partitions(self, topic_partitions, timeout_ms=None, validate_only=False):\n        version = self._matching_api_version(CreatePartitionsRequest)\n        timeout_ms = self._validate_timeout(timeout_ms)\n        if version == 0:\n            request = CreatePartitionsRequest[version](\n                topic_partitions=[self._convert_create_partitions_request(topic_name, new_partitions) for topic_name, new_partitions in topic_partitions.items()],\n                timeout=timeout_ms,\n                validate_only=validate_only\n            )\n        else:\n            raise NotImplementedError(\n                \"Support for CreatePartitions v{} has not yet been added to KafkaAdminClient.\"\n                .format(version))\n        return self._send_request_to_controller(request)", "response": "Create additional partitions for an existing topic."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef describe_consumer_groups(self, group_ids, group_coordinator_id=None):\n        group_descriptions = []\n        version = self._matching_api_version(DescribeGroupsRequest)\n        for group_id in group_ids:\n            if group_coordinator_id is not None:\n                this_groups_coordinator_id = group_coordinator_id\n            else:\n                this_groups_coordinator_id = self._find_group_coordinator_id(group_id)\n            if version <= 1:\n                # Note: KAFKA-6788 A potential optimization is to group the\n                # request per coordinator and send one request with a list of\n                # all consumer groups. Java still hasn't implemented this\n                # because the error checking is hard to get right when some\n                # groups error and others don't.\n                request = DescribeGroupsRequest[version](groups=(group_id,))\n                response = self._send_request_to_node(this_groups_coordinator_id, request)\n                assert len(response.groups) == 1\n                # TODO need to implement converting the response tuple into\n                # a more accessible interface like a namedtuple and then stop\n                # hardcoding tuple indices here. Several Java examples,\n                # including KafkaAdminClient.java\n                group_description = response.groups[0]\n                error_code = group_description[0]\n                error_type = Errors.for_code(error_code)\n                # Java has the note: KAFKA-6789, we can retry based on the error code\n                if error_type is not Errors.NoError:\n                    raise error_type(\n                        \"Request '{}' failed with response '{}'.\"\n                        .format(request, response))\n                # TODO Java checks the group protocol type, and if consumer\n                # (ConsumerProtocol.PROTOCOL_TYPE) or empty string, it decodes\n                # the members' partition assignments... that hasn't yet been\n                # implemented here so just return the raw struct results\n                group_descriptions.append(group_description)\n            else:\n                raise NotImplementedError(\n                    \"Support for DescribeGroups v{} has not yet been added to KafkaAdminClient.\"\n                    .format(version))\n        return group_descriptions", "response": "This method returns a list of consumer groups and their associated metadata."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_consumer_groups(self, broker_ids=None):\n        # While we return a list, internally use a set to prevent duplicates\n        # because if a group coordinator fails after being queried, and its\n        # consumer groups move to new brokers that haven't yet been queried,\n        # then the same group could be returned by multiple brokers.\n        consumer_groups = set()\n        if broker_ids is None:\n            broker_ids = [broker.nodeId for broker in self._client.cluster.brokers()]\n        version = self._matching_api_version(ListGroupsRequest)\n        if version <= 2:\n            request = ListGroupsRequest[version]()\n            for broker_id in broker_ids:\n                response = self._send_request_to_node(broker_id, request)\n                error_type = Errors.for_code(response.error_code)\n                if error_type is not Errors.NoError:\n                    raise error_type(\n                        \"Request '{}' failed with response '{}'.\"\n                        .format(request, response))\n                consumer_groups.update(response.groups)\n        else:\n            raise NotImplementedError(\n                \"Support for ListGroups v{} has not yet been added to KafkaAdminClient.\"\n                .format(version))\n        return list(consumer_groups)", "response": "This method returns a list of consumer groups known to the broker."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfetching the offsets for a specific consumer group.", "response": "def list_consumer_group_offsets(self, group_id, group_coordinator_id=None,\n                                    partitions=None):\n        \"\"\"Fetch Consumer Group Offsets.\n\n        Note:\n        This does not verify that the group_id or partitions actually exist\n        in the cluster.\n\n        As soon as any error is encountered, it is immediately raised.\n\n        :param group_id: The consumer group id name for which to fetch offsets.\n        :param group_coordinator_id: The node_id of the group's coordinator\n            broker. If set to None, will query the cluster to find the group\n            coordinator. Explicitly specifying this can be useful to prevent\n            that extra network round trip if you already know the group\n            coordinator. Default: None.\n        :param partitions: A list of TopicPartitions for which to fetch\n            offsets. On brokers >= 0.10.2, this can be set to None to fetch all\n            known offsets for the consumer group. Default: None.\n        :return dictionary: A dictionary with TopicPartition keys and\n            OffsetAndMetada values. Partitions that are not specified and for\n            which the group_id does not have a recorded offset are omitted. An\n            offset value of `-1` indicates the group_id has no offset for that\n            TopicPartition. A `-1` can only happen for partitions that are\n            explicitly specified.\n        \"\"\"\n        group_offsets_listing = {}\n        if group_coordinator_id is None:\n            group_coordinator_id = self._find_group_coordinator_id(group_id)\n        version = self._matching_api_version(OffsetFetchRequest)\n        if version <= 3:\n            if partitions is None:\n                if version <= 1:\n                    raise ValueError(\n                        \"\"\"OffsetFetchRequest_v{} requires specifying the\n                        partitions for which to fetch offsets. Omitting the\n                        partitions is only supported on brokers >= 0.10.2.\n                        For details, see KIP-88.\"\"\".format(version))\n                topics_partitions = None\n            else:\n                # transform from [TopicPartition(\"t1\", 1), TopicPartition(\"t1\", 2)] to [(\"t1\", [1, 2])]\n                topics_partitions_dict = defaultdict(set)\n                for topic, partition in partitions:\n                    topics_partitions_dict[topic].add(partition)\n                topics_partitions = list(six.iteritems(topics_partitions_dict))\n            request = OffsetFetchRequest[version](group_id, topics_partitions)\n            response = self._send_request_to_node(group_coordinator_id, request)\n            if version > 1:  # OffsetFetchResponse_v1 lacks a top-level error_code\n                error_type = Errors.for_code(response.error_code)\n                if error_type is not Errors.NoError:\n                    # optionally we could retry if error_type.retriable\n                    raise error_type(\n                        \"Request '{}' failed with response '{}'.\"\n                        .format(request, response))\n            # transform response into a dictionary with TopicPartition keys and\n            # OffsetAndMetada values--this is what the Java AdminClient returns\n            for topic, partitions in response.topics:\n                for partition, offset, metadata, error_code in partitions:\n                    error_type = Errors.for_code(error_code)\n                    if error_type is not Errors.NoError:\n                        raise error_type(\n                            \"Unable to fetch offsets for group_id {}, topic {}, partition {}\"\n                            .format(group_id, topic, partition))\n                    group_offsets_listing[TopicPartition(topic, partition)] = OffsetAndMetadata(offset, metadata)\n        else:\n            raise NotImplementedError(\n                \"Support for OffsetFetch v{} has not yet been added to KafkaAdminClient.\"\n                .format(version))\n        return group_offsets_listing"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef crc_update(crc, data):\n    if type(data) != array.array or data.itemsize != 1:\n        buf = array.array(\"B\", data)\n    else:\n        buf = data\n    crc = crc ^ _MASK\n    for b in buf:\n        table_index = (crc ^ b) & 0xff\n        crc = (CRC_TABLE[table_index] ^ (crc >> 8)) & _MASK\n    return crc ^ _MASK", "response": "Update CRC - 32C checksum with data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexpiring batches if metadata is not available.", "response": "def maybe_expire(self, request_timeout_ms, retry_backoff_ms, linger_ms, is_full):\n        \"\"\"Expire batches if metadata is not available\n\n        A batch whose metadata is not available should be expired if one\n        of the following is true:\n\n          * the batch is not in retry AND request timeout has elapsed after\n            it is ready (full or linger.ms has reached).\n\n          * the batch is in retry AND request timeout has elapsed after the\n            backoff period ended.\n        \"\"\"\n        now = time.time()\n        since_append = now - self.last_append\n        since_ready = now - (self.created + linger_ms / 1000.0)\n        since_backoff = now - (self.last_attempt + retry_backoff_ms / 1000.0)\n        timeout = request_timeout_ms / 1000.0\n\n        error = None\n        if not self.in_retry() and is_full and timeout < since_append:\n            error = \"%d seconds have passed since last append\" % (since_append,)\n        elif not self.in_retry() and timeout < since_ready:\n            error = \"%d seconds have passed since batch creation plus linger time\" % (since_ready,)\n        elif self.in_retry() and timeout < since_backoff:\n            error = \"%d seconds have passed since last attempt plus backoff time\" % (since_backoff,)\n\n        if error:\n            self.records.close()\n            self.done(-1, None, Errors.KafkaTimeoutError(\n                \"Batch for %s containing %s record(s) expired: %s\" % (\n                self.topic_partition, self.records.next_offset(), error)))\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef append(self, tp, timestamp_ms, key, value, headers, max_time_to_block_ms,\n               estimated_size=0):\n        \"\"\"Add a record to the accumulator, return the append result.\n\n        The append result will contain the future metadata, and flag for\n        whether the appended batch is full or a new batch is created\n\n        Arguments:\n            tp (TopicPartition): The topic/partition to which this record is\n                being sent\n            timestamp_ms (int): The timestamp of the record (epoch ms)\n            key (bytes): The key for the record\n            value (bytes): The value for the record\n            headers (List[Tuple[str, bytes]]): The header fields for the record\n            max_time_to_block_ms (int): The maximum time in milliseconds to\n                block for buffer memory to be available\n\n        Returns:\n            tuple: (future, batch_is_full, new_batch_created)\n        \"\"\"\n        assert isinstance(tp, TopicPartition), 'not TopicPartition'\n        assert not self._closed, 'RecordAccumulator is closed'\n        # We keep track of the number of appending thread to make sure we do\n        # not miss batches in abortIncompleteBatches().\n        self._appends_in_progress.increment()\n        try:\n            if tp not in self._tp_locks:\n                with self._tp_locks[None]:\n                    if tp not in self._tp_locks:\n                        self._tp_locks[tp] = threading.Lock()\n\n            with self._tp_locks[tp]:\n                # check if we have an in-progress batch\n                dq = self._batches[tp]\n                if dq:\n                    last = dq[-1]\n                    future = last.try_append(timestamp_ms, key, value, headers)\n                    if future is not None:\n                        batch_is_full = len(dq) > 1 or last.records.is_full()\n                        return future, batch_is_full, False\n\n            size = max(self.config['batch_size'], estimated_size)\n            log.debug(\"Allocating a new %d byte message buffer for %s\", size, tp) # trace\n            buf = self._free.allocate(size, max_time_to_block_ms)\n            with self._tp_locks[tp]:\n                # Need to check if producer is closed again after grabbing the\n                # dequeue lock.\n                assert not self._closed, 'RecordAccumulator is closed'\n\n                if dq:\n                    last = dq[-1]\n                    future = last.try_append(timestamp_ms, key, value, headers)\n                    if future is not None:\n                        # Somebody else found us a batch, return the one we\n                        # waited for! Hopefully this doesn't happen often...\n                        self._free.deallocate(buf)\n                        batch_is_full = len(dq) > 1 or last.records.is_full()\n                        return future, batch_is_full, False\n\n                records = MemoryRecordsBuilder(\n                    self.config['message_version'],\n                    self.config['compression_attrs'],\n                    self.config['batch_size']\n                )\n\n                batch = ProducerBatch(tp, records, buf)\n                future = batch.try_append(timestamp_ms, key, value, headers)\n                if not future:\n                    raise Exception()\n\n                dq.append(batch)\n                self._incomplete.add(batch)\n                batch_is_full = len(dq) > 1 or batch.records.is_full()\n                return future, batch_is_full, True\n        finally:\n            self._appends_in_progress.decrement()", "response": "Append a record to the accumulator."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef abort_expired_batches(self, request_timeout_ms, cluster):\n        expired_batches = []\n        to_remove = []\n        count = 0\n        for tp in list(self._batches.keys()):\n            assert tp in self._tp_locks, 'TopicPartition not in locks dict'\n\n            # We only check if the batch should be expired if the partition\n            # does not have a batch in flight. This is to avoid the later\n            # batches get expired when an earlier batch is still in progress.\n            # This protection only takes effect when user sets\n            # max.in.flight.request.per.connection=1. Otherwise the expiration\n            # order is not guranteed.\n            if tp in self.muted:\n                continue\n\n            with self._tp_locks[tp]:\n                # iterate over the batches and expire them if they have stayed\n                # in accumulator for more than request_timeout_ms\n                dq = self._batches[tp]\n                for batch in dq:\n                    is_full = bool(bool(batch != dq[-1]) or batch.records.is_full())\n                    # check if the batch is expired\n                    if batch.maybe_expire(request_timeout_ms,\n                                          self.config['retry_backoff_ms'],\n                                          self.config['linger_ms'],\n                                          is_full):\n                        expired_batches.append(batch)\n                        to_remove.append(batch)\n                        count += 1\n                        self.deallocate(batch)\n                    else:\n                        # Stop at the first batch that has not expired.\n                        break\n\n                # Python does not allow us to mutate the dq during iteration\n                # Assuming expired batches are infrequent, this is better than\n                # creating a new copy of the deque for iteration on every loop\n                if to_remove:\n                    for batch in to_remove:\n                        dq.remove(batch)\n                    to_remove = []\n\n        if expired_batches:\n            log.warning(\"Expired %d batches in accumulator\", count) # trace\n\n        return expired_batches", "response": "Abort the batches that have been sitting in RecordAccumulator for more than the configured request_timeout_ms."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reenqueue(self, batch):\n        now = time.time()\n        batch.attempts += 1\n        batch.last_attempt = now\n        batch.last_append = now\n        batch.set_retry()\n        assert batch.topic_partition in self._tp_locks, 'TopicPartition not in locks dict'\n        assert batch.topic_partition in self._batches, 'TopicPartition not in batches'\n        dq = self._batches[batch.topic_partition]\n        with self._tp_locks[batch.topic_partition]:\n            dq.appendleft(batch)", "response": "Re - enqueue the given record batch in the accumulator to retry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ready(self, cluster):\n        ready_nodes = set()\n        next_ready_check = 9999999.99\n        unknown_leaders_exist = False\n        now = time.time()\n\n        exhausted = bool(self._free.queued() > 0)\n        # several threads are accessing self._batches -- to simplify\n        # concurrent access, we iterate over a snapshot of partitions\n        # and lock each partition separately as needed\n        partitions = list(self._batches.keys())\n        for tp in partitions:\n            leader = cluster.leader_for_partition(tp)\n            if leader is None or leader == -1:\n                unknown_leaders_exist = True\n                continue\n            elif leader in ready_nodes:\n                continue\n            elif tp in self.muted:\n                continue\n\n            with self._tp_locks[tp]:\n                dq = self._batches[tp]\n                if not dq:\n                    continue\n                batch = dq[0]\n                retry_backoff = self.config['retry_backoff_ms'] / 1000.0\n                linger = self.config['linger_ms'] / 1000.0\n                backing_off = bool(batch.attempts > 0 and\n                                   batch.last_attempt + retry_backoff > now)\n                waited_time = now - batch.last_attempt\n                time_to_wait = retry_backoff if backing_off else linger\n                time_left = max(time_to_wait - waited_time, 0)\n                full = bool(len(dq) > 1 or batch.records.is_full())\n                expired = bool(waited_time >= time_to_wait)\n\n                sendable = (full or expired or exhausted or self._closed or\n                            self._flush_in_progress())\n\n                if sendable and not backing_off:\n                    ready_nodes.add(leader)\n                else:\n                    # Note that this results in a conservative estimate since\n                    # an un-sendable partition may have a leader that will\n                    # later be found to have sendable data. However, this is\n                    # good enough since we'll just wake up and then sleep again\n                    # for the remaining time.\n                    next_ready_check = min(time_left, next_ready_check)\n\n        return ready_nodes, next_ready_check, unknown_leaders_exist", "response": "Returns a list of nodes whose partitions are ready to be sent and the earliest time at which any non - sendable partition will be sent."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef has_unsent(self):\n        for tp in list(self._batches.keys()):\n            with self._tp_locks[tp]:\n                dq = self._batches[tp]\n                if len(dq):\n                    return True\n        return False", "response": "Return whether there is any unsent record in the accumulator."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef drain(self, cluster, nodes, max_size):\n        if not nodes:\n            return {}\n\n        now = time.time()\n        batches = {}\n        for node_id in nodes:\n            size = 0\n            partitions = list(cluster.partitions_for_broker(node_id))\n            ready = []\n            # to make starvation less likely this loop doesn't start at 0\n            self._drain_index %= len(partitions)\n            start = self._drain_index\n            while True:\n                tp = partitions[self._drain_index]\n                if tp in self._batches and tp not in self.muted:\n                    with self._tp_locks[tp]:\n                        dq = self._batches[tp]\n                        if dq:\n                            first = dq[0]\n                            backoff = (\n                                bool(first.attempts > 0) and\n                                bool(first.last_attempt +\n                                     self.config['retry_backoff_ms'] / 1000.0\n                                     > now)\n                            )\n                            # Only drain the batch if it is not during backoff\n                            if not backoff:\n                                if (size + first.records.size_in_bytes() > max_size\n                                    and len(ready) > 0):\n                                    # there is a rare case that a single batch\n                                    # size is larger than the request size due\n                                    # to compression; in this case we will\n                                    # still eventually send this batch in a\n                                    # single request\n                                    break\n                                else:\n                                    batch = dq.popleft()\n                                    batch.records.close()\n                                    size += batch.records.size_in_bytes()\n                                    ready.append(batch)\n                                    batch.drained = now\n\n                self._drain_index += 1\n                self._drain_index %= len(partitions)\n                if start == self._drain_index:\n                    break\n\n            batches[node_id] = ready\n        return batches", "response": "Drain all the data for the given nodes and return a list of ProducerBatches that will fit within the specified size."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwaits until all partitions are ready to send and block until the send is complete.", "response": "def await_flush_completion(self, timeout=None):\n        \"\"\"\n        Mark all partitions as ready to send and block until the send is complete\n        \"\"\"\n        try:\n            for batch in self._incomplete.all():\n                log.debug('Waiting on produce to %s',\n                          batch.produce_future.topic_partition)\n                if not batch.produce_future.wait(timeout=timeout):\n                    raise Errors.KafkaTimeoutError('Timeout waiting for future')\n                if not batch.produce_future.is_done:\n                    raise Errors.UnknownError('Future not done')\n\n                if batch.produce_future.failed():\n                    log.warning(batch.produce_future.exception)\n        finally:\n            self._flushes_in_progress.decrement()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nabort all the incomplete batches and returns.", "response": "def abort_incomplete_batches(self):\n        \"\"\"\n        This function is only called when sender is closed forcefully. It will fail all the\n        incomplete batches and return.\n        \"\"\"\n        # We need to keep aborting the incomplete batch until no thread is trying to append to\n        # 1. Avoid losing batches.\n        # 2. Free up memory in case appending threads are blocked on buffer full.\n        # This is a tight loop but should be able to get through very quickly.\n        while True:\n            self._abort_batches()\n            if not self._appends_in_progress.get():\n                break\n        # After this point, no thread will append any messages because they will see the close\n        # flag set. We need to do the last abort after no thread was appending in case the there was a new\n        # batch appended by the last appending thread.\n        self._abort_batches()\n        self._batches.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _abort_batches(self):\n        error = Errors.IllegalStateError(\"Producer is closed forcefully.\")\n        for batch in self._incomplete.all():\n            tp = batch.topic_partition\n            # Close the batch before aborting\n            with self._tp_locks[tp]:\n                batch.records.close()\n            batch.done(exception=error)\n            self.deallocate(batch)", "response": "Abort all incomplete batches and deallocate them."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nappending a new record to the buffer.", "response": "def append(self, timestamp, key, value, headers=[]):\n        \"\"\" Append a message to the buffer.\n\n        Returns: RecordMetadata or None if unable to append\n        \"\"\"\n        if self._closed:\n            return None\n\n        offset = self._next_offset\n        metadata = self._builder.append(offset, timestamp, key, value, headers)\n        # Return of None means there's no space to add a new message\n        if metadata is None:\n            return None\n\n        self._next_offset += 1\n        return metadata"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nappending a new entry to the messageset.", "response": "def append(self, offset, timestamp, key, value, headers,\n               # Cache for LOAD_FAST opcodes\n               encode_varint=encode_varint, size_of_varint=size_of_varint,\n               get_type=type, type_int=int, time_time=time.time,\n               byte_like=(bytes, bytearray, memoryview),\n               bytearray_type=bytearray, len_func=len, zero_len_varint=1\n               ):\n        \"\"\" Write message to messageset buffer with MsgVersion 2\n        \"\"\"\n        # Check types\n        if get_type(offset) != type_int:\n            raise TypeError(offset)\n        if timestamp is None:\n            timestamp = type_int(time_time() * 1000)\n        elif get_type(timestamp) != type_int:\n            raise TypeError(timestamp)\n        if not (key is None or get_type(key) in byte_like):\n            raise TypeError(\n                \"Not supported type for key: {}\".format(type(key)))\n        if not (value is None or get_type(value) in byte_like):\n            raise TypeError(\n                \"Not supported type for value: {}\".format(type(value)))\n\n        # We will always add the first message, so those will be set\n        if self._first_timestamp is None:\n            self._first_timestamp = timestamp\n            self._max_timestamp = timestamp\n            timestamp_delta = 0\n            first_message = 1\n        else:\n            timestamp_delta = timestamp - self._first_timestamp\n            first_message = 0\n\n        # We can't write record right away to out buffer, we need to\n        # precompute the length as first value...\n        message_buffer = bytearray_type(b\"\\x00\")  # Attributes\n        write_byte = message_buffer.append\n        write = message_buffer.extend\n\n        encode_varint(timestamp_delta, write_byte)\n        # Base offset is always 0 on Produce\n        encode_varint(offset, write_byte)\n\n        if key is not None:\n            encode_varint(len_func(key), write_byte)\n            write(key)\n        else:\n            write_byte(zero_len_varint)\n\n        if value is not None:\n            encode_varint(len_func(value), write_byte)\n            write(value)\n        else:\n            write_byte(zero_len_varint)\n\n        encode_varint(len_func(headers), write_byte)\n\n        for h_key, h_value in headers:\n            h_key = h_key.encode(\"utf-8\")\n            encode_varint(len_func(h_key), write_byte)\n            write(h_key)\n            if h_value is not None:\n                encode_varint(len_func(h_value), write_byte)\n                write(h_value)\n            else:\n                write_byte(zero_len_varint)\n\n        message_len = len_func(message_buffer)\n        main_buffer = self._buffer\n\n        required_size = message_len + size_of_varint(message_len)\n        # Check if we can write this message\n        if (required_size + len_func(main_buffer) > self._batch_size and\n                not first_message):\n            return None\n\n        # Those should be updated after the length check\n        if self._max_timestamp < timestamp:\n            self._max_timestamp = timestamp\n        self._num_records += 1\n        self._last_offset = offset\n\n        encode_varint(message_len, main_buffer.append)\n        main_buffer.extend(message_buffer)\n\n        return DefaultRecordMetadata(offset, required_size, timestamp)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef estimate_size_in_bytes(cls, key, value, headers):\n        return (\n            cls.HEADER_STRUCT.size + cls.MAX_RECORD_OVERHEAD +\n            cls.size_of(key, value, headers)\n        )", "response": "Estimate the size of the record in bytes based on the size of the headers."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns seconds remaining before next heartbeat should be sent", "response": "def time_to_next_heartbeat(self):\n        \"\"\"Returns seconds (float) remaining before next heartbeat should be sent\"\"\"\n        time_since_last_heartbeat = time.time() - max(self.last_send, self.last_reset)\n        if self.heartbeat_failed:\n            delay_to_next_heartbeat = self.config['retry_backoff_ms'] / 1000\n        else:\n            delay_to_next_heartbeat = self.config['heartbeat_interval_ms'] / 1000\n        return max(0, delay_to_next_heartbeat - time_since_last_heartbeat)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining the family of an address or hostname", "response": "def _address_family(address):\n    \"\"\"\n        Attempt to determine the family of an address (or hostname)\n\n        :return: either socket.AF_INET or socket.AF_INET6 or socket.AF_UNSPEC if the address family\n                 could not be determined\n    \"\"\"\n    if address.startswith('[') and address.endswith(']'):\n        return socket.AF_INET6\n    for af in (socket.AF_INET, socket.AF_INET6):\n        try:\n            socket.inet_pton(af, address)\n            return af\n        except (ValueError, AttributeError, socket.error):\n            continue\n    return socket.AF_UNSPEC"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_ip_port_afi(host_and_port_str):\n    host_and_port_str = host_and_port_str.strip()\n    if host_and_port_str.startswith('['):\n        af = socket.AF_INET6\n        host, rest = host_and_port_str[1:].split(']')\n        if rest:\n            port = int(rest[1:])\n        else:\n            port = DEFAULT_KAFKA_PORT\n        return host, port, af\n    else:\n        if ':' not in host_and_port_str:\n            af = _address_family(host_and_port_str)\n            return host_and_port_str, DEFAULT_KAFKA_PORT, af\n        else:\n            # now we have something with a colon in it and no square brackets. It could be\n            # either an IPv6 address literal (e.g., \"::1\") or an IP:port pair or a host:port pair\n            try:\n                # if it decodes as an IPv6 address, use that\n                socket.inet_pton(socket.AF_INET6, host_and_port_str)\n                return host_and_port_str, DEFAULT_KAFKA_PORT, socket.AF_INET6\n            except AttributeError:\n                log.warning('socket.inet_pton not available on this platform.'\n                            ' consider `pip install win_inet_pton`')\n                pass\n            except (ValueError, socket.error):\n                # it's a host:port pair\n                pass\n            host, port = host_and_port_str.rsplit(':', 1)\n            port = int(port)\n\n            af = _address_family(host)\n            return host, port, af", "response": "Parse the IP and port from a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncollect a comma - separated set of hosts and optionally randomize the returned list.", "response": "def collect_hosts(hosts, randomize=True):\n    \"\"\"\n    Collects a comma-separated set of hosts (host:port) and optionally\n    randomize the returned list.\n    \"\"\"\n\n    if isinstance(hosts, six.string_types):\n        hosts = hosts.strip().split(',')\n\n    result = []\n    afi = socket.AF_INET\n    for host_port in hosts:\n\n        host, port, afi = get_ip_port_afi(host_port)\n\n        if port < 0:\n            port = DEFAULT_KAFKA_PORT\n\n        result.append((host, port, afi))\n\n    if randomize:\n        shuffle(result)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dns_lookup(host, port, afi=socket.AF_UNSPEC):\n    # XXX: all DNS functions in Python are blocking. If we really\n    # want to be non-blocking here, we need to use a 3rd-party\n    # library like python-adns, or move resolution onto its\n    # own thread. This will be subject to the default libc\n    # name resolution timeout (5s on most Linux boxes)\n    try:\n        return list(filter(is_inet_4_or_6,\n                           socket.getaddrinfo(host, port, afi,\n                                              socket.SOCK_STREAM)))\n    except socket.gaierror as ex:\n        log.warning('DNS lookup failed for %s:%d,'\n                    ' exception was %s. Is your'\n                    ' advertised.listeners (called'\n                    ' advertised.host.name before Kafka 9)'\n                    ' correct and resolvable?',\n                    host, port, ex)\n        return []", "response": "Returns a list of getaddrinfo structs optionally filtered to an AFI."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nattempting to connect and returns ConnectionState", "response": "def connect(self):\n        \"\"\"Attempt to connect and return ConnectionState\"\"\"\n        if self.state is ConnectionStates.DISCONNECTED and not self.blacked_out():\n            self.last_attempt = time.time()\n            next_lookup = self._next_afi_sockaddr()\n            if not next_lookup:\n                self.close(Errors.KafkaConnectionError('DNS failure'))\n                return self.state\n            else:\n                log.debug('%s: creating new socket', self)\n                assert self._sock is None\n                self._sock_afi, self._sock_addr = next_lookup\n                self._sock = socket.socket(self._sock_afi, socket.SOCK_STREAM)\n\n            for option in self.config['socket_options']:\n                log.debug('%s: setting socket option %s', self, option)\n                self._sock.setsockopt(*option)\n\n            self._sock.setblocking(False)\n            self.state = ConnectionStates.CONNECTING\n            self.config['state_change_callback'](self.node_id, self._sock, self)\n            log.info('%s: connecting to %s:%d [%s %s]', self, self.host,\n                     self.port, self._sock_addr, AFI_NAMES[self._sock_afi])\n\n        if self.state is ConnectionStates.CONNECTING:\n            # in non-blocking mode, use repeated calls to socket.connect_ex\n            # to check connection status\n            ret = None\n            try:\n                ret = self._sock.connect_ex(self._sock_addr)\n            except socket.error as err:\n                ret = err.errno\n\n            # Connection succeeded\n            if not ret or ret == errno.EISCONN:\n                log.debug('%s: established TCP connection', self)\n\n                if self.config['security_protocol'] in ('SSL', 'SASL_SSL'):\n                    log.debug('%s: initiating SSL handshake', self)\n                    self.state = ConnectionStates.HANDSHAKE\n                    self.config['state_change_callback'](self.node_id, self._sock, self)\n                    # _wrap_ssl can alter the connection state -- disconnects on failure\n                    self._wrap_ssl()\n\n                elif self.config['security_protocol'] == 'SASL_PLAINTEXT':\n                    log.debug('%s: initiating SASL authentication', self)\n                    self.state = ConnectionStates.AUTHENTICATING\n                    self.config['state_change_callback'](self.node_id, self._sock, self)\n\n                else:\n                    # security_protocol PLAINTEXT\n                    log.info('%s: Connection complete.', self)\n                    self.state = ConnectionStates.CONNECTED\n                    self._reset_reconnect_backoff()\n                    self.config['state_change_callback'](self.node_id, self._sock, self)\n\n            # Connection failed\n            # WSAEINVAL == 10022, but errno.WSAEINVAL is not available on non-win systems\n            elif ret not in (errno.EINPROGRESS, errno.EALREADY, errno.EWOULDBLOCK, 10022):\n                log.error('Connect attempt to %s returned error %s.'\n                          ' Disconnecting.', self, ret)\n                errstr = errno.errorcode.get(ret, 'UNKNOWN')\n                self.close(Errors.KafkaConnectionError('{} {}'.format(ret, errstr)))\n                return self.state\n\n            # Needs retry\n            else:\n                pass\n\n        if self.state is ConnectionStates.HANDSHAKE:\n            if self._try_handshake():\n                log.debug('%s: completed SSL handshake.', self)\n                if self.config['security_protocol'] == 'SASL_SSL':\n                    log.debug('%s: initiating SASL authentication', self)\n                    self.state = ConnectionStates.AUTHENTICATING\n                else:\n                    log.info('%s: Connection complete.', self)\n                    self.state = ConnectionStates.CONNECTED\n                    self._reset_reconnect_backoff()\n                self.config['state_change_callback'](self.node_id, self._sock, self)\n\n        if self.state is ConnectionStates.AUTHENTICATING:\n            assert self.config['security_protocol'] in ('SASL_PLAINTEXT', 'SASL_SSL')\n            if self._try_authenticate():\n                # _try_authenticate has side-effects: possibly disconnected on socket errors\n                if self.state is ConnectionStates.AUTHENTICATING:\n                    log.info('%s: Connection complete.', self)\n                    self.state = ConnectionStates.CONNECTED\n                    self._reset_reconnect_backoff()\n                    self.config['state_change_callback'](self.node_id, self._sock, self)\n\n        if self.state not in (ConnectionStates.CONNECTED,\n                              ConnectionStates.DISCONNECTED):\n            # Connection timed out\n            request_timeout = self.config['request_timeout_ms'] / 1000.0\n            if time.time() > request_timeout + self.last_attempt:\n                log.error('Connection attempt to %s timed out', self)\n                self.close(Errors.KafkaConnectionError('timeout'))\n                return self.state\n\n        return self.state"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a string representation of the OPTIONAL key - value pairs that can be sent with an OAUTHBEARER initial request.", "response": "def _token_extensions(self):\n        \"\"\"\n        Return a string representation of the OPTIONAL key-value pairs that can be sent with an OAUTHBEARER\n        initial request.\n        \"\"\"\n        token_provider = self.config['sasl_oauth_token_provider']\n\n        # Only run if the #extensions() method is implemented by the clients Token Provider class\n        # Builds up a string separated by \\x01 via a dict of key value pairs\n        if callable(getattr(token_provider, \"extensions\", None)) and len(token_provider.extensions()) > 0:\n            msg = \"\\x01\".join([\"{}={}\".format(k, v) for k, v in token_provider.extensions().items()])\n            return \"\\x01\" + msg\n        else:\n            return \"\""}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef blacked_out(self):\n        if self.state is ConnectionStates.DISCONNECTED:\n            if time.time() < self.last_attempt + self._reconnect_backoff:\n                return True\n        return False", "response": "Return True if we are disconnected from the given node and can t re - establish a connection yet."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connection_delay(self):\n        time_waited = time.time() - (self.last_attempt or 0)\n        if self.state is ConnectionStates.DISCONNECTED:\n            return max(self._reconnect_backoff - time_waited, 0) * 1000\n        elif self.connecting():\n            return 0\n        else:\n            return float('inf')", "response": "Returns the number of milliseconds to wait for the connection to finish."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if the connection is still connecting.", "response": "def connecting(self):\n        \"\"\"Returns True if still connecting (this may encompass several\n        different states, such as SSL handshake, authorization, etc).\"\"\"\n        return self.state in (ConnectionStates.CONNECTING,\n                              ConnectionStates.HANDSHAKE,\n                              ConnectionStates.AUTHENTICATING)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef close(self, error=None):\n        if self.state is ConnectionStates.DISCONNECTED:\n            return\n        with self._lock:\n            if self.state is ConnectionStates.DISCONNECTED:\n                return\n            log.info('%s: Closing connection. %s', self, error or '')\n            self._update_reconnect_backoff()\n            self._sasl_auth_future = None\n            self._protocol = KafkaProtocol(\n                client_id=self.config['client_id'],\n                api_version=self.config['api_version'])\n            if error is None:\n                error = Errors.Cancelled(str(self))\n            ifrs = list(self.in_flight_requests.items())\n            self.in_flight_requests.clear()\n            self.state = ConnectionStates.DISCONNECTED\n            # To avoid race conditions and/or deadlocks\n            # keep a reference to the socket but leave it\n            # open until after the state_change_callback\n            # This should give clients a change to deregister\n            # the socket fd from selectors cleanly.\n            sock = self._sock\n            self._sock = None\n\n        # drop lock before state change callback and processing futures\n        self.config['state_change_callback'](self.node_id, sock, self)\n        sock.close()\n        for (_correlation_id, (future, _timestamp)) in ifrs:\n            future.failure(error)", "response": "Closes the connection and fails all pending in - flight requests."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nqueues a request for async network send", "response": "def send(self, request, blocking=True):\n        \"\"\"Queue request for async network send, return Future()\"\"\"\n        future = Future()\n        if self.connecting():\n            return future.failure(Errors.NodeNotReadyError(str(self)))\n        elif not self.connected():\n            return future.failure(Errors.KafkaConnectionError(str(self)))\n        elif not self.can_send_more():\n            return future.failure(Errors.TooManyInFlightRequests(str(self)))\n        return self._send(request, blocking=blocking)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send_pending_requests(self):\n        try:\n            with self._lock:\n                if not self._can_send_recv():\n                    return Errors.NodeNotReadyError(str(self))\n                # In the future we might manage an internal write buffer\n                # and send bytes asynchronously. For now, just block\n                # sending each request payload\n                data = self._protocol.send_bytes()\n                total_bytes = self._send_bytes_blocking(data)\n            if self._sensors:\n                self._sensors.bytes_sent.record(total_bytes)\n            return total_bytes\n        except ConnectionError as e:\n            log.exception(\"Error sending request data to %s\", self)\n            error = Errors.KafkaConnectionError(\"%s: %s\" % (self, e))\n            self.close(error=error)\n            return error", "response": "Send pending requests to the broker."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _recv(self):\n        recvd = []\n        self._lock.acquire()\n        if not self._can_send_recv():\n            log.warning('%s cannot recv: socket not connected', self)\n            self._lock.release()\n            return ()\n\n        while len(recvd) < self.config['sock_chunk_buffer_count']:\n            try:\n                data = self._sock.recv(self.config['sock_chunk_bytes'])\n                # We expect socket.recv to raise an exception if there are no\n                # bytes available to read from the socket in non-blocking mode.\n                # but if the socket is disconnected, we will get empty data\n                # without an exception raised\n                if not data:\n                    log.error('%s: socket disconnected', self)\n                    self._lock.release()\n                    self.close(error=Errors.KafkaConnectionError('socket disconnected'))\n                    return []\n                else:\n                    recvd.append(data)\n\n            except SSLWantReadError:\n                break\n            except ConnectionError as e:\n                if six.PY2 and e.errno == errno.EWOULDBLOCK:\n                    break\n                log.exception('%s: Error receiving network data'\n                              ' closing socket', self)\n                self._lock.release()\n                self.close(error=Errors.KafkaConnectionError(e))\n                return []\n            except BlockingIOError:\n                if six.PY3:\n                    break\n                self._lock.release()\n                raise\n\n        recvd_data = b''.join(recvd)\n        if self._sensors:\n            self._sensors.bytes_received.record(len(recvd_data))\n\n        try:\n            responses = self._protocol.receive_bytes(recvd_data)\n        except Errors.KafkaProtocolError as e:\n            self._lock.release()\n            self.close(e)\n            return []\n        else:\n            self._lock.release()\n            return responses", "response": "Take all available bytes from socket return list of any responses from parser"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nattempts to guess the broker version.", "response": "def check_version(self, timeout=2, strict=False, topics=[]):\n        \"\"\"Attempt to guess the broker version.\n\n        Note: This is a blocking call.\n\n        Returns: version tuple, i.e. (0, 10), (0, 9), (0, 8, 2), ...\n        \"\"\"\n        timeout_at = time.time() + timeout\n        log.info('Probing node %s broker version', self.node_id)\n        # Monkeypatch some connection configurations to avoid timeouts\n        override_config = {\n            'request_timeout_ms': timeout * 1000,\n            'max_in_flight_requests_per_connection': 5\n        }\n        stashed = {}\n        for key in override_config:\n            stashed[key] = self.config[key]\n            self.config[key] = override_config[key]\n\n        # kafka kills the connection when it doesn't recognize an API request\n        # so we can send a test request and then follow immediately with a\n        # vanilla MetadataRequest. If the server did not recognize the first\n        # request, both will be failed with a ConnectionError that wraps\n        # socket.error (32, 54, or 104)\n        from kafka.protocol.admin import ApiVersionRequest, ListGroupsRequest\n        from kafka.protocol.commit import OffsetFetchRequest, GroupCoordinatorRequest\n\n        test_cases = [\n            # All cases starting from 0.10 will be based on ApiVersionResponse\n            ((0, 10), ApiVersionRequest[0]()),\n            ((0, 9), ListGroupsRequest[0]()),\n            ((0, 8, 2), GroupCoordinatorRequest[0]('kafka-python-default-group')),\n            ((0, 8, 1), OffsetFetchRequest[0]('kafka-python-default-group', [])),\n            ((0, 8, 0), MetadataRequest[0](topics)),\n        ]\n\n        for version, request in test_cases:\n            if not self.connect_blocking(timeout_at - time.time()):\n                raise Errors.NodeNotReadyError()\n            f = self.send(request)\n            # HACK: sleeping to wait for socket to send bytes\n            time.sleep(0.1)\n            # when broker receives an unrecognized request API\n            # it abruptly closes our socket.\n            # so we attempt to send a second request immediately\n            # that we believe it will definitely recognize (metadata)\n            # the attempt to write to a disconnected socket should\n            # immediately fail and allow us to infer that the prior\n            # request was unrecognized\n            mr = self.send(MetadataRequest[0](topics))\n\n            selector = self.config['selector']()\n            selector.register(self._sock, selectors.EVENT_READ)\n            while not (f.is_done and mr.is_done):\n                selector.select(1)\n                for response, future in self.recv():\n                    future.success(response)\n            selector.close()\n\n            if f.succeeded():\n                if isinstance(request, ApiVersionRequest[0]):\n                    # Starting from 0.10 kafka broker we determine version\n                    # by looking at ApiVersionResponse\n                    api_versions = self._handle_api_version_response(f.value)\n                    version = self._infer_broker_version_from_api_versions(api_versions)\n                log.info('Broker version identifed as %s', '.'.join(map(str, version)))\n                log.info('Set configuration api_version=%s to skip auto'\n                         ' check_version requests on startup', version)\n                break\n\n            # Only enable strict checking to verify that we understand failure\n            # modes. For most users, the fact that the request failed should be\n            # enough to rule out a particular broker version.\n            if strict:\n                # If the socket flush hack did not work (which should force the\n                # connection to close and fail all pending requests), then we\n                # get a basic Request Timeout. This is not ideal, but we'll deal\n                if isinstance(f.exception, Errors.RequestTimedOutError):\n                    pass\n\n                # 0.9 brokers do not close the socket on unrecognized api\n                # requests (bug...). In this case we expect to see a correlation\n                # id mismatch\n                elif (isinstance(f.exception, Errors.CorrelationIdError) and\n                      version == (0, 10)):\n                    pass\n                elif six.PY2:\n                    assert isinstance(f.exception.args[0], socket.error)\n                    assert f.exception.args[0].errno in (32, 54, 104)\n                else:\n                    assert isinstance(f.exception.args[0], ConnectionError)\n            log.info(\"Broker is not v%s -- it did not recognize %s\",\n                     version, request.__class__.__name__)\n        else:\n            raise Errors.UnrecognizedBrokerVersion()\n\n        for key in stashed:\n            self.config[key] = stashed[key]\n        return version"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(self):\n        log.debug(\"Starting Kafka producer I/O thread.\")\n\n        # main loop, runs until close is called\n        while self._running:\n            try:\n                self.run_once()\n            except Exception:\n                log.exception(\"Uncaught error in kafka producer I/O thread\")\n\n        log.debug(\"Beginning shutdown of Kafka producer I/O thread, sending\"\n                  \" remaining records.\")\n\n        # okay we stopped accepting requests but there may still be\n        # requests in the accumulator or waiting for acknowledgment,\n        # wait until these are completed.\n        while (not self._force_close\n               and (self._accumulator.has_unsent()\n                    or self._client.in_flight_request_count() > 0)):\n            try:\n                self.run_once()\n            except Exception:\n                log.exception(\"Uncaught error in kafka producer I/O thread\")\n\n        if self._force_close:\n            # We need to fail all the incomplete batches and wake up the\n            # threads waiting on the futures.\n            self._accumulator.abort_incomplete_batches()\n\n        try:\n            self._client.close()\n        except Exception:\n            log.exception(\"Failed to close network client\")\n\n        log.debug(\"Shutdown of Kafka producer I/O thread has completed.\")", "response": "The main loop for the sender thread."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_once(self):\n        while self._topics_to_add:\n            self._client.add_topic(self._topics_to_add.pop())\n\n        # get the list of partitions with data ready to send\n        result = self._accumulator.ready(self._metadata)\n        ready_nodes, next_ready_check_delay, unknown_leaders_exist = result\n\n        # if there are any partitions whose leaders are not known yet, force\n        # metadata update\n        if unknown_leaders_exist:\n            log.debug('Unknown leaders exist, requesting metadata update')\n            self._metadata.request_update()\n\n        # remove any nodes we aren't ready to send to\n        not_ready_timeout = float('inf')\n        for node in list(ready_nodes):\n            if not self._client.is_ready(node):\n                log.debug('Node %s not ready; delaying produce of accumulated batch', node)\n                self._client.maybe_connect(node, wakeup=False)\n                ready_nodes.remove(node)\n                not_ready_timeout = min(not_ready_timeout,\n                                        self._client.connection_delay(node))\n\n        # create produce requests\n        batches_by_node = self._accumulator.drain(\n            self._metadata, ready_nodes, self.config['max_request_size'])\n\n        if self.config['guarantee_message_order']:\n            # Mute all the partitions drained\n            for batch_list in six.itervalues(batches_by_node):\n                for batch in batch_list:\n                    self._accumulator.muted.add(batch.topic_partition)\n\n        expired_batches = self._accumulator.abort_expired_batches(\n            self.config['request_timeout_ms'], self._metadata)\n        for expired_batch in expired_batches:\n            self._sensors.record_errors(expired_batch.topic_partition.topic, expired_batch.record_count)\n\n        self._sensors.update_produce_request_metrics(batches_by_node)\n        requests = self._create_produce_requests(batches_by_node)\n        # If we have any nodes that are ready to send + have sendable data,\n        # poll with 0 timeout so this can immediately loop and try sending more\n        # data. Otherwise, the timeout is determined by nodes that have\n        # partitions with data that isn't yet sendable (e.g. lingering, backing\n        # off). Note that this specifically does not include nodes with\n        # sendable data that aren't ready to send since they would cause busy\n        # looping.\n        poll_timeout_ms = min(next_ready_check_delay * 1000, not_ready_timeout)\n        if ready_nodes:\n            log.debug(\"Nodes with data ready to send: %s\", ready_nodes) # trace\n            log.debug(\"Created %d produce requests: %s\", len(requests), requests) # trace\n            poll_timeout_ms = 0\n\n        for node_id, request in six.iteritems(requests):\n            batches = batches_by_node[node_id]\n            log.debug('Sending Produce Request: %r', request)\n            (self._client.send(node_id, request, wakeup=False)\n                 .add_callback(\n                     self._handle_produce_response, node_id, time.time(), batches)\n                 .add_errback(\n                     self._failed_produce, batches, node_id))\n\n        # if some partitions are already ready to be sent, the select time\n        # would be 0; otherwise if some partition already has some data\n        # accumulated but not ready yet, the select time will be the time\n        # difference between now and its linger expiry time; otherwise the\n        # select time will be the time difference between now and the\n        # metadata expiry time\n        self._client.poll(poll_timeout_ms)", "response": "Run a single iteration of sending."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstart closing the sender.", "response": "def initiate_close(self):\n        \"\"\"Start closing the sender (won't complete until all data is sent).\"\"\"\n        self._running = False\n        self._accumulator.close()\n        self.wakeup()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _handle_produce_response(self, node_id, send_time, batches, response):\n        # if we have a response, parse it\n        log.debug('Parsing produce response: %r', response)\n        if response:\n            batches_by_partition = dict([(batch.topic_partition, batch)\n                                         for batch in batches])\n\n            for topic, partitions in response.topics:\n                for partition_info in partitions:\n                    if response.API_VERSION < 2:\n                        partition, error_code, offset = partition_info\n                        ts = None\n                    else:\n                        partition, error_code, offset, ts = partition_info\n                    tp = TopicPartition(topic, partition)\n                    error = Errors.for_code(error_code)\n                    batch = batches_by_partition[tp]\n                    self._complete_batch(batch, error, offset, ts)\n\n            if response.API_VERSION > 0:\n                self._sensors.record_throttle_time(response.throttle_time_ms, node=node_id)\n\n        else:\n            # this is the acks = 0 case, just complete all requests\n            for batch in batches:\n                self._complete_batch(batch, None, -1, None)", "response": "Handle a produce response."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _complete_batch(self, batch, error, base_offset, timestamp_ms=None):\n        # Standardize no-error to None\n        if error is Errors.NoError:\n            error = None\n\n        if error is not None and self._can_retry(batch, error):\n            # retry\n            log.warning(\"Got error produce response on topic-partition %s,\"\n                        \" retrying (%d attempts left). Error: %s\",\n                        batch.topic_partition,\n                        self.config['retries'] - batch.attempts - 1,\n                        error)\n            self._accumulator.reenqueue(batch)\n            self._sensors.record_retries(batch.topic_partition.topic, batch.record_count)\n        else:\n            if error is Errors.TopicAuthorizationFailedError:\n                error = error(batch.topic_partition.topic)\n\n            # tell the user the result of their request\n            batch.done(base_offset, timestamp_ms, error)\n            self._accumulator.deallocate(batch)\n            if error is not None:\n                self._sensors.record_errors(batch.topic_partition.topic, batch.record_count)\n\n        if getattr(error, 'invalid_metadata', False):\n            self._metadata.request_update()\n\n        # Unmute the completed partition.\n        if self.config['guarantee_message_order']:\n            self._accumulator.muted.remove(batch.topic_partition)", "response": "Complete the given batch of records."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _can_retry(self, batch, error):\n        return (batch.attempts < self.config['retries']\n                and getattr(error, 'retriable', False))", "response": "Return whether we can retry a send if the error is transient or not."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntransfer the record batches into a list of produce requests on a node basis.", "response": "def _create_produce_requests(self, collated):\n        \"\"\"\n        Transfer the record batches into a list of produce requests on a\n        per-node basis.\n\n        Arguments:\n            collated: {node_id: [RecordBatch]}\n\n        Returns:\n            dict: {node_id: ProduceRequest} (version depends on api_version)\n        \"\"\"\n        requests = {}\n        for node_id, batches in six.iteritems(collated):\n            requests[node_id] = self._produce_request(\n                node_id, self.config['acks'],\n                self.config['request_timeout_ms'], batches)\n        return requests"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a ProduceRequest object from the given record batches.", "response": "def _produce_request(self, node_id, acks, timeout, batches):\n        \"\"\"Create a produce request from the given record batches.\n\n        Returns:\n            ProduceRequest (version depends on api_version)\n        \"\"\"\n        produce_records_by_partition = collections.defaultdict(dict)\n        for batch in batches:\n            topic = batch.topic_partition.topic\n            partition = batch.topic_partition.partition\n\n            buf = batch.records.buffer()\n            produce_records_by_partition[topic][partition] = buf\n\n        kwargs = {}\n        if self.config['api_version'] >= (0, 11):\n            version = 3\n            kwargs = dict(transactional_id=None)\n        elif self.config['api_version'] >= (0, 10):\n            version = 2\n        elif self.config['api_version'] == (0, 9):\n            version = 1\n        else:\n            version = 0\n        return ProduceRequest[version](\n            required_acks=acks,\n            timeout=timeout,\n            topics=[(topic, list(partition_info.items()))\n                    for topic, partition_info\n                    in six.iteritems(produce_records_by_partition)],\n            **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nencoding the given data with snappy compression.", "response": "def snappy_encode(payload, xerial_compatible=True, xerial_blocksize=32*1024):\n    \"\"\"Encodes the given data with snappy compression.\n\n    If xerial_compatible is set then the stream is encoded in a fashion\n    compatible with the xerial snappy library.\n\n    The block size (xerial_blocksize) controls how frequent the blocking occurs\n    32k is the default in the xerial library.\n\n    The format winds up being:\n\n\n        +-------------+------------+--------------+------------+--------------+\n        |   Header    | Block1 len | Block1 data  | Blockn len | Blockn data  |\n        +-------------+------------+--------------+------------+--------------+\n        |  16 bytes   |  BE int32  | snappy bytes |  BE int32  | snappy bytes |\n        +-------------+------------+--------------+------------+--------------+\n\n\n    It is important to note that the blocksize is the amount of uncompressed\n    data presented to snappy at each block, whereas the blocklen is the number\n    of bytes that will be present in the stream; so the length will always be\n    <= blocksize.\n\n    \"\"\"\n\n    if not has_snappy():\n        raise NotImplementedError(\"Snappy codec is not available\")\n\n    if not xerial_compatible:\n        return snappy.compress(payload)\n\n    out = io.BytesIO()\n    for fmt, dat in zip(_XERIAL_V1_FORMAT, _XERIAL_V1_HEADER):\n        out.write(struct.pack('!' + fmt, dat))\n\n    # Chunk through buffers to avoid creating intermediate slice copies\n    if PYPY:\n        # on pypy, snappy.compress() on a sliced buffer consumes the entire\n        # buffer... likely a python-snappy bug, so just use a slice copy\n        chunker = lambda payload, i, size: payload[i:size+i]\n\n    elif six.PY2:\n        # Sliced buffer avoids additional copies\n        # pylint: disable-msg=undefined-variable\n        chunker = lambda payload, i, size: buffer(payload, i, size)\n    else:\n        # snappy.compress does not like raw memoryviews, so we have to convert\n        # tobytes, which is a copy... oh well. it's the thought that counts.\n        # pylint: disable-msg=undefined-variable\n        chunker = lambda payload, i, size: memoryview(payload)[i:size+i].tobytes()\n\n    for chunk in (chunker(payload, i, xerial_blocksize)\n                  for i in range(0, len(payload), xerial_blocksize)):\n\n        block = snappy.compress(chunk)\n        block_size = len(block)\n        out.write(struct.pack('!i', block_size))\n        out.write(block)\n\n    return out.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndecodes payload using interoperable LZ4 framing. Requires Kafka >= 0. 10", "response": "def lz4f_decode(payload):\n    \"\"\"Decode payload using interoperable LZ4 framing. Requires Kafka >= 0.10\"\"\"\n    # pylint: disable-msg=no-member\n    ctx = lz4f.createDecompContext()\n    data = lz4f.decompressFrame(payload, ctx)\n    lz4f.freeDecompContext(ctx)\n\n    # lz4f python module does not expose how much of the payload was\n    # actually read if the decompression was only partial.\n    if data['next'] != 0:\n        raise RuntimeError('lz4f unable to decompress full payload')\n    return data['decomp']"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nencodes payload for 0. 8 / 0. 9 brokers -- requires an incorrect header checksum.", "response": "def lz4_encode_old_kafka(payload):\n    \"\"\"Encode payload for 0.8/0.9 brokers -- requires an incorrect header checksum.\"\"\"\n    assert xxhash is not None\n    data = lz4_encode(payload)\n    header_size = 7\n    flg = data[4]\n    if not isinstance(flg, int):\n        flg = ord(flg)\n\n    content_size_bit = ((flg >> 3) & 1)\n    if content_size_bit:\n        # Old kafka does not accept the content-size field\n        # so we need to discard it and reset the header flag\n        flg -= 8\n        data = bytearray(data)\n        data[4] = flg\n        data = bytes(data)\n        payload = data[header_size+8:]\n    else:\n        payload = data[header_size:]\n\n    # This is the incorrect hc\n    hc = xxhash.xxh32(data[0:header_size-1]).digest()[-2:-1]  # pylint: disable-msg=no-member\n\n    return b''.join([\n        data[0:header_size-1],\n        hc,\n        payload\n    ])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget all BrokerMetadata objects in this cluster", "response": "def brokers(self):\n        \"\"\"Get all BrokerMetadata\n\n        Returns:\n            set: {BrokerMetadata, ...}\n        \"\"\"\n        return set(self._brokers.values()) or set(self._bootstrap_brokers.values())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef broker_metadata(self, broker_id):\n        return self._brokers.get(broker_id) or self._bootstrap_brokers.get(broker_id)", "response": "Get the broker metadata for a specific broker"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef partitions_for_topic(self, topic):\n        if topic not in self._partitions:\n            return None\n        return set(self._partitions[topic].keys())", "response": "Returns set of all partitions for a topic"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef available_partitions_for_topic(self, topic):\n        if topic not in self._partitions:\n            return None\n        return set([partition for partition, metadata\n                              in six.iteritems(self._partitions[topic])\n                              if metadata.leader != -1])", "response": "Returns set of partitions with known leaders for topic."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn node_id of leader for given partition.", "response": "def leader_for_partition(self, partition):\n        \"\"\"Return node_id of leader, -1 unavailable, None if unknown.\"\"\"\n        if partition.topic not in self._partitions:\n            return None\n        elif partition.partition not in self._partitions[partition.topic]:\n            return None\n        return self._partitions[partition.topic][partition.partition].leader"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ttl(self):\n        now = time.time() * 1000\n        if self._need_update:\n            ttl = 0\n        else:\n            metadata_age = now - self._last_successful_refresh_ms\n            ttl = self.config['metadata_max_age_ms'] - metadata_age\n\n        retry_age = now - self._last_refresh_ms\n        next_retry = self.config['retry_backoff_ms'] - retry_age\n\n        return max(ttl, next_retry, 0)", "response": "Milliseconds until metadata should be refreshed"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrequest metadata for update", "response": "def request_update(self):\n        \"\"\"Flags metadata for update, return Future()\n\n        Actual update must be handled separately. This method will only\n        change the reported ttl()\n\n        Returns:\n            kafka.future.Future (value will be the cluster object after update)\n        \"\"\"\n        with self._lock:\n            self._need_update = True\n            if not self._future or self._future.is_done:\n              self._future = Future()\n            return self._future"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets set of known topics.", "response": "def topics(self, exclude_internal_topics=True):\n        \"\"\"Get set of known topics.\n\n        Arguments:\n            exclude_internal_topics (bool): Whether records from internal topics\n                (such as offsets) should be exposed to the consumer. If set to\n                True the only way to receive records from an internal topic is\n                subscribing to it. Default True\n\n        Returns:\n            set: {topic (str), ...}\n        \"\"\"\n        topics = set(self._partitions.keys())\n        if exclude_internal_topics:\n            return topics - self.internal_topics\n        else:\n            return topics"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the state of the cluster with a failed MetadataRequest.", "response": "def failed_update(self, exception):\n        \"\"\"Update cluster state given a failed MetadataRequest.\"\"\"\n        f = None\n        with self._lock:\n            if self._future:\n                f = self._future\n                self._future = None\n        if f:\n            f.failure(exception)\n        self._last_refresh_ms = time.time() * 1000"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the state of the cluster given a MetadataResponse.", "response": "def update_metadata(self, metadata):\n        \"\"\"Update cluster state given a MetadataResponse.\n\n        Arguments:\n            metadata (MetadataResponse): broker response to a metadata request\n\n        Returns: None\n        \"\"\"\n        # In the common case where we ask for a single topic and get back an\n        # error, we should fail the future\n        if len(metadata.topics) == 1 and metadata.topics[0][0] != 0:\n            error_code, topic = metadata.topics[0][:2]\n            error = Errors.for_code(error_code)(topic)\n            return self.failed_update(error)\n\n        if not metadata.brokers:\n            log.warning(\"No broker metadata found in MetadataResponse -- ignoring.\")\n            return self.failed_update(Errors.MetadataEmptyBrokerList(metadata))\n\n        _new_brokers = {}\n        for broker in metadata.brokers:\n            if metadata.API_VERSION == 0:\n                node_id, host, port = broker\n                rack = None\n            else:\n                node_id, host, port, rack = broker\n            _new_brokers.update({\n                node_id: BrokerMetadata(node_id, host, port, rack)\n            })\n\n        if metadata.API_VERSION == 0:\n            _new_controller = None\n        else:\n            _new_controller = _new_brokers.get(metadata.controller_id)\n\n        _new_partitions = {}\n        _new_broker_partitions = collections.defaultdict(set)\n        _new_unauthorized_topics = set()\n        _new_internal_topics = set()\n\n        for topic_data in metadata.topics:\n            if metadata.API_VERSION == 0:\n                error_code, topic, partitions = topic_data\n                is_internal = False\n            else:\n                error_code, topic, is_internal, partitions = topic_data\n            if is_internal:\n                _new_internal_topics.add(topic)\n            error_type = Errors.for_code(error_code)\n            if error_type is Errors.NoError:\n                _new_partitions[topic] = {}\n                for p_error, partition, leader, replicas, isr in partitions:\n                    _new_partitions[topic][partition] = PartitionMetadata(\n                        topic=topic, partition=partition, leader=leader,\n                        replicas=replicas, isr=isr, error=p_error)\n                    if leader != -1:\n                        _new_broker_partitions[leader].add(\n                            TopicPartition(topic, partition))\n\n            elif error_type is Errors.LeaderNotAvailableError:\n                log.warning(\"Topic %s is not available during auto-create\"\n                            \" initialization\", topic)\n            elif error_type is Errors.UnknownTopicOrPartitionError:\n                log.error(\"Topic %s not found in cluster metadata\", topic)\n            elif error_type is Errors.TopicAuthorizationFailedError:\n                log.error(\"Topic %s is not authorized for this client\", topic)\n                _new_unauthorized_topics.add(topic)\n            elif error_type is Errors.InvalidTopicError:\n                log.error(\"'%s' is not a valid topic name\", topic)\n            else:\n                log.error(\"Error fetching metadata for topic %s: %s\",\n                          topic, error_type)\n\n        with self._lock:\n            self._brokers = _new_brokers\n            self.controller = _new_controller\n            self._partitions = _new_partitions\n            self._broker_partitions = _new_broker_partitions\n            self.unauthorized_topics = _new_unauthorized_topics\n            self.internal_topics = _new_internal_topics\n            f = None\n            if self._future:\n                f = self._future\n            self._future = None\n            self._need_update = False\n\n        now = time.time() * 1000\n        self._last_refresh_ms = now\n        self._last_successful_refresh_ms = now\n\n        if f:\n            f.success(self)\n        log.debug(\"Updated cluster metadata to %s\", self)\n\n        for listener in self._listeners:\n            listener(self)\n\n        if self.need_all_topic_metadata:\n            # the listener may change the interested topics,\n            # which could cause another metadata refresh.\n            # If we have already fetched all topics, however,\n            # another fetch should be unnecessary.\n            self._need_update = False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_group_coordinator(self, group, response):\n        log.debug(\"Updating coordinator for %s: %s\", group, response)\n        error_type = Errors.for_code(response.error_code)\n        if error_type is not Errors.NoError:\n            log.error(\"GroupCoordinatorResponse error: %s\", error_type)\n            self._groups[group] = -1\n            return False\n\n        node_id = response.coordinator_id\n        coordinator = BrokerMetadata(\n            response.coordinator_id,\n            response.host,\n            response.port,\n            None)\n\n        # Assume that group coordinators are just brokers\n        # (this is true now, but could diverge in future)\n        if node_id not in self._brokers:\n            self._brokers[node_id] = coordinator\n\n        # If this happens, either brokers have moved without\n        # changing IDs, or our assumption above is wrong\n        else:\n            node = self._brokers[node_id]\n            if coordinator.host != node.host or coordinator.port != node.port:\n                log.error(\"GroupCoordinator metadata conflicts with existing\"\n                          \" broker metadata. Coordinator: %s, Broker: %s\",\n                          coordinator, node)\n                self._groups[group] = node_id\n                return False\n\n        log.info(\"Group coordinator for %s is %s\", group, coordinator)\n        self._groups[group] = node_id\n        return True", "response": "Update with metadata for a group coordinator"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a copy of this cluster metadata with partitions added", "response": "def with_partitions(self, partitions_to_add):\n        \"\"\"Returns a copy of cluster metadata with partitions added\"\"\"\n        new_metadata = ClusterMetadata(**self.config)\n        new_metadata._brokers = copy.deepcopy(self._brokers)\n        new_metadata._partitions = copy.deepcopy(self._partitions)\n        new_metadata._broker_partitions = copy.deepcopy(self._broker_partitions)\n        new_metadata._groups = copy.deepcopy(self._groups)\n        new_metadata.internal_topics = copy.deepcopy(self.internal_topics)\n        new_metadata.unauthorized_topics = copy.deepcopy(self.unauthorized_topics)\n\n        for partition in partitions_to_add:\n            new_metadata._partitions[partition.topic][partition.partition] = partition\n\n            if partition.leader is not None and partition.leader != -1:\n                new_metadata._broker_partitions[partition.leader].add(\n                    TopicPartition(partition.topic, partition.partition))\n\n        return new_metadata"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef assign(self, partitions):\n        self._subscription.assign_from_user(partitions)\n        self._client.set_topics([tp.topic for tp in partitions])", "response": "Manually assign a list of TopicPartitions to this consumer."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nclosing the consumer and all associated resources.", "response": "def close(self, autocommit=True):\n        \"\"\"Close the consumer, waiting indefinitely for any needed cleanup.\n\n        Keyword Arguments:\n            autocommit (bool): If auto-commit is configured for this consumer,\n                this optional flag causes the consumer to attempt to commit any\n                pending consumed offsets prior to close. Default: True\n        \"\"\"\n        if self._closed:\n            return\n        log.debug(\"Closing the KafkaConsumer.\")\n        self._closed = True\n        self._coordinator.close(autocommit=autocommit)\n        self._metrics.close()\n        self._client.close()\n        try:\n            self.config['key_deserializer'].close()\n        except AttributeError:\n            pass\n        try:\n            self.config['value_deserializer'].close()\n        except AttributeError:\n            pass\n        log.debug(\"The KafkaConsumer has closed.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef commit_async(self, offsets=None, callback=None):\n        assert self.config['api_version'] >= (0, 8, 1), 'Requires >= Kafka 0.8.1'\n        assert self.config['group_id'] is not None, 'Requires group_id'\n        if offsets is None:\n            offsets = self._subscription.all_consumed_offsets()\n        log.debug(\"Committing offsets: %s\", offsets)\n        future = self._coordinator.commit_offsets_async(\n            offsets, callback=callback)\n        return future", "response": "Commits the current consumed offsets to Kafka asynchronously."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef commit(self, offsets=None):\n        assert self.config['api_version'] >= (0, 8, 1), 'Requires >= Kafka 0.8.1'\n        assert self.config['group_id'] is not None, 'Requires group_id'\n        if offsets is None:\n            offsets = self._subscription.all_consumed_offsets()\n        self._coordinator.commit_offsets_sync(offsets)", "response": "Commits the current consumed offsets to Kafka."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef committed(self, partition):\n        assert self.config['api_version'] >= (0, 8, 1), 'Requires >= Kafka 0.8.1'\n        assert self.config['group_id'] is not None, 'Requires group_id'\n        if not isinstance(partition, TopicPartition):\n            raise TypeError('partition must be a TopicPartition namedtuple')\n        if self._subscription.is_assigned(partition):\n            committed = self._subscription.assignment[partition].committed\n            if committed is None:\n                self._coordinator.refresh_committed_offsets_if_needed()\n                committed = self._subscription.assignment[partition].committed\n        else:\n            commit_map = self._coordinator.fetch_committed_offsets([partition])\n            if partition in commit_map:\n                committed = commit_map[partition].offset\n            else:\n                committed = None\n        return committed", "response": "Get the last committed offset for the given partition."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget all topics the user is authorized to view.", "response": "def topics(self):\n        \"\"\"Get all topics the user is authorized to view.\n\n        Returns:\n            set: topics\n        \"\"\"\n        cluster = self._client.cluster\n        if self._client._metadata_refresh_in_progress and self._client._topics:\n            future = cluster.request_update()\n            self._client.poll(future=future)\n        stash = cluster.need_all_topic_metadata\n        cluster.need_all_topic_metadata = True\n        future = cluster.request_update()\n        self._client.poll(future=future)\n        cluster.need_all_topic_metadata = stash\n        return cluster.topics()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef poll(self, timeout_ms=0, max_records=None):\n        assert timeout_ms >= 0, 'Timeout must not be negative'\n        if max_records is None:\n            max_records = self.config['max_poll_records']\n        assert isinstance(max_records, int), 'max_records must be an integer'\n        assert max_records > 0, 'max_records must be positive'\n\n        # Poll for new data until the timeout expires\n        start = time.time()\n        remaining = timeout_ms\n        while True:\n            records = self._poll_once(remaining, max_records)\n            if records:\n                return records\n\n            elapsed_ms = (time.time() - start) * 1000\n            remaining = timeout_ms - elapsed_ms\n\n            if remaining <= 0:\n                return {}", "response": "Poll for new data from assigned topics and partitions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndo one round of polling.", "response": "def _poll_once(self, timeout_ms, max_records):\n        \"\"\"Do one round of polling. In addition to checking for new data, this does\n        any needed heart-beating, auto-commits, and offset updates.\n\n        Arguments:\n            timeout_ms (int): The maximum time in milliseconds to block.\n\n        Returns:\n            dict: Map of topic to list of records (may be empty).\n        \"\"\"\n        self._coordinator.poll()\n\n        # Fetch positions if we have partitions we're subscribed to that we\n        # don't know the offset for\n        if not self._subscription.has_all_fetch_positions():\n            self._update_fetch_positions(self._subscription.missing_fetch_positions())\n\n        # If data is available already, e.g. from a previous network client\n        # poll() call to commit, then just return it immediately\n        records, partial = self._fetcher.fetched_records(max_records)\n        if records:\n            # Before returning the fetched records, we can send off the\n            # next round of fetches and avoid block waiting for their\n            # responses to enable pipelining while the user is handling the\n            # fetched records.\n            if not partial:\n                self._fetcher.send_fetches()\n            return records\n\n        # Send any new fetches (won't resend pending fetches)\n        self._fetcher.send_fetches()\n\n        timeout_ms = min(timeout_ms, self._coordinator.time_to_next_poll() * 1000)\n        self._client.poll(timeout_ms=timeout_ms)\n        # after the long poll, we should check whether the group needs to rebalance\n        # prior to returning data so that the group can stabilize faster\n        if self._coordinator.need_rejoin():\n            return {}\n\n        records, _ = self._fetcher.fetched_records(max_records)\n        return records"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the offset of the next record that will be fetched by the given partition", "response": "def position(self, partition):\n        \"\"\"Get the offset of the next record that will be fetched\n\n        Arguments:\n            partition (TopicPartition): Partition to check\n\n        Returns:\n            int: Offset\n        \"\"\"\n        if not isinstance(partition, TopicPartition):\n            raise TypeError('partition must be a TopicPartition namedtuple')\n        assert self._subscription.is_assigned(partition), 'Partition is not assigned'\n        offset = self._subscription.assignment[partition].position\n        if offset is None:\n            self._update_fetch_positions([partition])\n            offset = self._subscription.assignment[partition].position\n        return offset"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef highwater(self, partition):\n        if not isinstance(partition, TopicPartition):\n            raise TypeError('partition must be a TopicPartition namedtuple')\n        assert self._subscription.is_assigned(partition), 'Partition is not assigned'\n        return self._subscription.assignment[partition].highwater", "response": "Returns the last known highwater offset for a topic partition."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pause(self, *partitions):\n        if not all([isinstance(p, TopicPartition) for p in partitions]):\n            raise TypeError('partitions must be TopicPartition namedtuples')\n        for partition in partitions:\n            log.debug(\"Pausing partition %s\", partition)\n            self._subscription.pause(partition)", "response": "Suspend fetching records from the requested partitions."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nseek to a specific offset for a topic partition.", "response": "def seek(self, partition, offset):\n        \"\"\"Manually specify the fetch offset for a TopicPartition.\n\n        Overrides the fetch offsets that the consumer will use on the next\n        :meth:`~kafka.KafkaConsumer.poll`. If this API is invoked for the same\n        partition more than once, the latest offset will be used on the next\n        :meth:`~kafka.KafkaConsumer.poll`.\n\n        Note: You may lose data if this API is arbitrarily used in the middle of\n        consumption to reset the fetch offsets.\n\n        Arguments:\n            partition (TopicPartition): Partition for seek operation\n            offset (int): Message offset in partition\n\n        Raises:\n            AssertionError: If offset is not an int >= 0; or if partition is not\n                currently assigned.\n        \"\"\"\n        if not isinstance(partition, TopicPartition):\n            raise TypeError('partition must be a TopicPartition namedtuple')\n        assert isinstance(offset, int) and offset >= 0, 'Offset must be >= 0'\n        assert partition in self._subscription.assigned_partitions(), 'Unassigned partition'\n        log.debug(\"Seeking to offset %s for partition %s\", offset, partition)\n        self._subscription.assignment[partition].seek(offset)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nseeking to the oldest available offset for partitions.", "response": "def seek_to_beginning(self, *partitions):\n        \"\"\"Seek to the oldest available offset for partitions.\n\n        Arguments:\n            *partitions: Optionally provide specific TopicPartitions, otherwise\n                default to all assigned partitions.\n\n        Raises:\n            AssertionError: If any partition is not currently assigned, or if\n                no partitions are assigned.\n        \"\"\"\n        if not all([isinstance(p, TopicPartition) for p in partitions]):\n            raise TypeError('partitions must be TopicPartition namedtuples')\n        if not partitions:\n            partitions = self._subscription.assigned_partitions()\n            assert partitions, 'No partitions are currently assigned'\n        else:\n            for p in partitions:\n                assert p in self._subscription.assigned_partitions(), 'Unassigned partition'\n\n        for tp in partitions:\n            log.debug(\"Seeking to beginning of partition %s\", tp)\n            self._subscription.need_offset_reset(tp, OffsetResetStrategy.EARLIEST)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nseeks to the most recent available offset for partitions.", "response": "def seek_to_end(self, *partitions):\n        \"\"\"Seek to the most recent available offset for partitions.\n\n        Arguments:\n            *partitions: Optionally provide specific TopicPartitions, otherwise\n                default to all assigned partitions.\n\n        Raises:\n            AssertionError: If any partition is not currently assigned, or if\n                no partitions are assigned.\n        \"\"\"\n        if not all([isinstance(p, TopicPartition) for p in partitions]):\n            raise TypeError('partitions must be TopicPartition namedtuples')\n        if not partitions:\n            partitions = self._subscription.assigned_partitions()\n            assert partitions, 'No partitions are currently assigned'\n        else:\n            for p in partitions:\n                assert p in self._subscription.assigned_partitions(), 'Unassigned partition'\n\n        for tp in partitions:\n            log.debug(\"Seeking to end of partition %s\", tp)\n            self._subscription.need_offset_reset(tp, OffsetResetStrategy.LATEST)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsubscribe to a list of topics or a regex pattern.", "response": "def subscribe(self, topics=(), pattern=None, listener=None):\n        \"\"\"Subscribe to a list of topics, or a topic regex pattern.\n\n        Partitions will be dynamically assigned via a group coordinator.\n        Topic subscriptions are not incremental: this list will replace the\n        current assignment (if there is one).\n\n        This method is incompatible with :meth:`~kafka.KafkaConsumer.assign`.\n\n        Arguments:\n            topics (list): List of topics for subscription.\n            pattern (str): Pattern to match available topics. You must provide\n                either topics or pattern, but not both.\n            listener (ConsumerRebalanceListener): Optionally include listener\n                callback, which will be called before and after each rebalance\n                operation.\n\n                As part of group management, the consumer will keep track of the\n                list of consumers that belong to a particular group and will\n                trigger a rebalance operation if one of the following events\n                trigger:\n\n                * Number of partitions change for any of the subscribed topics\n                * Topic is created or deleted\n                * An existing member of the consumer group dies\n                * A new member is added to the consumer group\n\n                When any of these events are triggered, the provided listener\n                will be invoked first to indicate that the consumer's assignment\n                has been revoked, and then again when the new assignment has\n                been received. Note that this listener will immediately override\n                any listener set in a previous call to subscribe. It is\n                guaranteed, however, that the partitions revoked/assigned\n                through this interface are from topics subscribed in this call.\n\n        Raises:\n            IllegalStateError: If called after previously calling\n                :meth:`~kafka.KafkaConsumer.assign`.\n            AssertionError: If neither topics or pattern is provided.\n            TypeError: If listener is not a ConsumerRebalanceListener.\n        \"\"\"\n        # SubscriptionState handles error checking\n        self._subscription.subscribe(topics=topics,\n                                     pattern=pattern,\n                                     listener=listener)\n\n        # Regex will need all topic metadata\n        if pattern is not None:\n            self._client.cluster.need_all_topic_metadata = True\n            self._client.set_topics([])\n            self._client.cluster.request_update()\n            log.debug(\"Subscribed to topic pattern: %s\", pattern)\n        else:\n            self._client.cluster.need_all_topic_metadata = False\n            self._client.set_topics(self._subscription.group_subscription())\n            log.debug(\"Subscribed to topic(s): %s\", topics)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unsubscribe(self):\n        self._subscription.unsubscribe()\n        self._coordinator.close()\n        self._client.cluster.need_all_topic_metadata = False\n        self._client.set_topics([])\n        log.debug(\"Unsubscribed all topics or patterns and assigned partitions\")", "response": "Unsubscribe from all topics and clear all assigned partitions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef offsets_for_times(self, timestamps):\n        if self.config['api_version'] <= (0, 10, 0):\n            raise UnsupportedVersionError(\n                \"offsets_for_times API not supported for cluster version {}\"\n                .format(self.config['api_version']))\n        for tp, ts in six.iteritems(timestamps):\n            timestamps[tp] = int(ts)\n            if ts < 0:\n                raise ValueError(\n                    \"The target time for partition {} is {}. The target time \"\n                    \"cannot be negative.\".format(tp, ts))\n        return self._fetcher.get_offsets_by_times(\n            timestamps, self.config['request_timeout_ms'])", "response": "Look up the offsets for the given timestamps."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef beginning_offsets(self, partitions):\n        offsets = self._fetcher.beginning_offsets(\n            partitions, self.config['request_timeout_ms'])\n        return offsets", "response": "Get the earliest available offsets for the given partitions."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the last offset for the given partitions.", "response": "def end_offsets(self, partitions):\n        \"\"\"Get the last offset for the given partitions. The last offset of a\n        partition is the offset of the upcoming message, i.e. the offset of the\n        last available message + 1.\n\n        This method does not change the current consumer position of the\n        partitions.\n\n        Note:\n            This method may block indefinitely if the partition does not exist.\n\n        Arguments:\n            partitions (list): List of TopicPartition instances to fetch\n                offsets for.\n\n        Returns:\n            ``{TopicPartition: int}``: The end offsets for the given partitions.\n\n        Raises:\n            UnsupportedVersionError: If the broker does not support looking\n                up the offsets by timestamp.\n            KafkaTimeoutError: If fetch failed in request_timeout_ms\n        \"\"\"\n        offsets = self._fetcher.end_offsets(\n            partitions, self.config['request_timeout_ms'])\n        return offsets"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True iff this consumer can join a broker - coordinated group.", "response": "def _use_consumer_group(self):\n        \"\"\"Return True iff this consumer can/should join a broker-coordinated group.\"\"\"\n        if self.config['api_version'] < (0, 9):\n            return False\n        elif self.config['group_id'] is None:\n            return False\n        elif not self._subscription.partitions_auto_assigned():\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the fetch positions for the given partitions.", "response": "def _update_fetch_positions(self, partitions):\n        \"\"\"Set the fetch position to the committed position (if there is one)\n        or reset it using the offset reset policy the user has configured.\n\n        Arguments:\n            partitions (List[TopicPartition]): The partitions that need\n                updating fetch positions.\n\n        Raises:\n            NoOffsetForPartitionError: If no offset is stored for a given\n                partition and no offset reset policy is defined.\n        \"\"\"\n        # Lookup any positions for partitions which are awaiting reset (which may be the\n        # case if the user called :meth:`seek_to_beginning` or :meth:`seek_to_end`. We do\n        # this check first to avoid an unnecessary lookup of committed offsets (which\n        # typically occurs when the user is manually assigning partitions and managing\n        # their own offsets).\n        self._fetcher.reset_offsets_if_needed(partitions)\n\n        if not self._subscription.has_all_fetch_positions():\n            # if we still don't have offsets for all partitions, then we should either seek\n            # to the last committed position or reset using the auto reset policy\n            if (self.config['api_version'] >= (0, 8, 1) and\n                self.config['group_id'] is not None):\n                # first refresh commits for all assigned partitions\n                self._coordinator.refresh_committed_offsets_if_needed()\n\n            # Then, do any offset lookups in case some positions are not known\n            self._fetcher.update_fetch_positions(partitions)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a nested dictionary snapshot of all metrics and their values at this time.", "response": "def snapshot(self):\n        \"\"\"\n        Return a nested dictionary snapshot of all metrics and their\n        values at this time. Example:\n        {\n            'category': {\n                'metric1_name': 42.0,\n                'metric2_name': 'foo'\n            }\n        }\n        \"\"\"\n        return dict((category, dict((name, metric.value())\n                                    for name, metric in list(metrics.items())))\n                    for category, metrics in\n                    list(self._store.items()))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_category(self, metric):\n        tags = ','.join('%s=%s' % (k, v) for k, v in\n                        sorted(metric.metric_name.tags.items()))\n        return '.'.join(x for x in\n                        [self._prefix, metric.metric_name.group, tags] if x)", "response": "Returns a string category for the metric."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if a node is ready to be connected to the broker.", "response": "def maybe_connect(self, node_id, wakeup=True):\n        \"\"\"Queues a node for asynchronous connection during the next .poll()\"\"\"\n        if self._can_connect(node_id):\n            self._connecting.add(node_id)\n            # Wakeup signal is useful in case another thread is\n            # blocked waiting for incoming network traffic while holding\n            # the client lock in poll().\n            if wakeup:\n                self.wakeup()\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _maybe_connect(self, node_id):\n        with self._lock:\n            conn = self._conns.get(node_id)\n\n            if conn is None:\n                broker = self.cluster.broker_metadata(node_id)\n                assert broker, 'Broker id %s not in current metadata' % (node_id,)\n\n                log.debug(\"Initiating connection to node %s at %s:%s\",\n                          node_id, broker.host, broker.port)\n                host, port, afi = get_ip_port_afi(broker.host)\n                cb = WeakMethod(self._conn_state_change)\n                conn = BrokerConnection(host, broker.port, afi,\n                                        state_change_callback=cb,\n                                        node_id=node_id,\n                                        **self.config)\n                self._conns[node_id] = conn\n\n            # Check if existing connection should be recreated because host/port changed\n            elif self._should_recycle_connection(conn):\n                self._conns.pop(node_id)\n                return False\n\n            elif conn.connected():\n                return True\n\n            conn.connect()\n            return conn.connected()", "response": "Idempotent non - blocking connection attempt to the given node id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ready(self, node_id, metadata_priority=True):\n        self.maybe_connect(node_id)\n        return self.is_ready(node_id, metadata_priority=metadata_priority)", "response": "Check whether a node is ready to send more requests."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True iff the node_id is connected.", "response": "def connected(self, node_id):\n        \"\"\"Return True iff the node_id is connected.\"\"\"\n        conn = self._conns.get(node_id)\n        if conn is None:\n            return False\n        return conn.connected()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef close(self, node_id=None):\n        with self._lock:\n            if node_id is None:\n                self._close()\n                conns = list(self._conns.values())\n                self._conns.clear()\n                for conn in conns:\n                    conn.close()\n            elif node_id in self._conns:\n                self._conns.pop(node_id).close()\n            else:\n                log.warning(\"Node %s not found in current connection list; skipping\", node_id)\n                return", "response": "Closes one or all broker connections."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_disconnected(self, node_id):\n        conn = self._conns.get(node_id)\n        if conn is None:\n            return False\n        return conn.disconnected()", "response": "Checks whether the node connection has been disconnected or failed."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the number of milliseconds to wait for the connection to finish.", "response": "def connection_delay(self, node_id):\n        \"\"\"\n        Return the number of milliseconds to wait, based on the connection\n        state, before attempting to send data. When disconnected, this respects\n        the reconnect backoff time. When connecting, returns 0 to allow\n        non-blocking connect to finish. When connected, returns a very large\n        number to handle slow/stalled connections.\n\n        Arguments:\n            node_id (int): The id of the node to check\n\n        Returns:\n            int: The number of milliseconds to wait.\n        \"\"\"\n        conn = self._conns.get(node_id)\n        if conn is None:\n            return 0\n        return conn.connection_delay()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_ready(self, node_id, metadata_priority=True):\n        if not self._can_send_request(node_id):\n            return False\n\n        # if we need to update our metadata now declare all requests unready to\n        # make metadata requests first priority\n        if metadata_priority:\n            if self._metadata_refresh_in_progress:\n                return False\n            if self.cluster.ttl() == 0:\n                return False\n        return True", "response": "Check whether a node is ready to send more requests."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a request to a specific node.", "response": "def send(self, node_id, request, wakeup=True):\n        \"\"\"Send a request to a specific node. Bytes are placed on an\n        internal per-connection send-queue. Actual network I/O will be\n        triggered in a subsequent call to .poll()\n\n        Arguments:\n            node_id (int): destination node\n            request (Struct): request object (not-encoded)\n            wakeup (bool): optional flag to disable thread-wakeup\n\n        Raises:\n            AssertionError: if node_id is not in current cluster metadata\n\n        Returns:\n            Future: resolves to Response struct or Error\n        \"\"\"\n        conn = self._conns.get(node_id)\n        if not conn or not self._can_send_request(node_id):\n            self.maybe_connect(node_id, wakeup=wakeup)\n            return Future().failure(Errors.NodeNotReadyError(node_id))\n\n        # conn.send will queue the request internally\n        # we will need to call send_pending_requests()\n        # to trigger network I/O\n        future = conn.send(request, blocking=False)\n\n        # Wakeup signal is useful in case another thread is\n        # blocked waiting for incoming network traffic while holding\n        # the client lock in poll().\n        if wakeup:\n            self.wakeup()\n\n        return future"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef poll(self, timeout_ms=None, future=None):\n        if future is not None:\n            timeout_ms = 100\n        elif timeout_ms is None:\n            timeout_ms = self.config['request_timeout_ms']\n        elif not isinstance(timeout_ms, (int, float)):\n            raise TypeError('Invalid type for timeout: %s' % type(timeout_ms))\n\n        # Loop for futures, break after first loop if None\n        responses = []\n        while True:\n            with self._lock:\n                if self._closed:\n                    break\n\n                # Attempt to complete pending connections\n                for node_id in list(self._connecting):\n                    self._maybe_connect(node_id)\n\n                # Send a metadata request if needed\n                metadata_timeout_ms = self._maybe_refresh_metadata()\n\n                # If we got a future that is already done, don't block in _poll\n                if future is not None and future.is_done:\n                    timeout = 0\n                else:\n                    idle_connection_timeout_ms = self._idle_expiry_manager.next_check_ms()\n                    timeout = min(\n                        timeout_ms,\n                        metadata_timeout_ms,\n                        idle_connection_timeout_ms,\n                        self.config['request_timeout_ms'])\n                    timeout = max(0, timeout / 1000)  # avoid negative timeouts\n\n                self._poll(timeout)\n\n                responses.extend(self._fire_pending_completed_requests())\n\n            # If all we had was a timeout (future is None) - only do one poll\n            # If we do have a future, we keep looping until it is done\n            if future is None or future.is_done:\n                break\n\n        return responses", "response": "Try to read and write to sockets."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef in_flight_request_count(self, node_id=None):\n        if node_id is not None:\n            conn = self._conns.get(node_id)\n            if conn is None:\n                return 0\n            return len(conn.in_flight_requests)\n        else:\n            return sum([len(conn.in_flight_requests)\n                        for conn in list(self._conns.values())])", "response": "Get the number of in - flight requests for a specific node or all nodes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nchoose the least loaded node with few outstanding requests with fallbacks.", "response": "def least_loaded_node(self):\n        \"\"\"Choose the node with fewest outstanding requests, with fallbacks.\n\n        This method will prefer a node with an existing connection and no\n        in-flight-requests. If no such node is found, a node will be chosen\n        randomly from disconnected nodes that are not \"blacked out\" (i.e.,\n        are not subject to a reconnect backoff). If no node metadata has been\n        obtained, will return a bootstrap node (subject to exponential backoff).\n\n        Returns:\n            node_id or None if no suitable node was found\n        \"\"\"\n        nodes = [broker.nodeId for broker in self.cluster.brokers()]\n        random.shuffle(nodes)\n\n        inflight = float('inf')\n        found = None\n        for node_id in nodes:\n            conn = self._conns.get(node_id)\n            connected = conn is not None and conn.connected()\n            blacked_out = conn is not None and conn.blacked_out()\n            curr_inflight = len(conn.in_flight_requests) if conn is not None else 0\n            if connected and curr_inflight == 0:\n                # if we find an established connection\n                # with no in-flight requests, we can stop right away\n                return node_id\n            elif not blacked_out and curr_inflight < inflight:\n                # otherwise if this is the best we have found so far, record that\n                inflight = curr_inflight\n                found = node_id\n\n        if found is not None:\n            return found\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets specific topics to track for metadata.", "response": "def set_topics(self, topics):\n        \"\"\"Set specific topics to track for metadata.\n\n        Arguments:\n            topics (list of str): topics to check for metadata\n\n        Returns:\n            Future: resolves after metadata request/response\n        \"\"\"\n        if set(topics).difference(self._topics):\n            future = self.cluster.request_update()\n        else:\n            future = Future().success(set(topics))\n        self._topics = set(topics)\n        return future"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a topic to the list of topics tracked via metadata.", "response": "def add_topic(self, topic):\n        \"\"\"Add a topic to the list of topics tracked via metadata.\n\n        Arguments:\n            topic (str): topic to track\n\n        Returns:\n            Future: resolves after metadata request/response\n        \"\"\"\n        if topic in self._topics:\n            return Future().success(set(self._topics))\n\n        self._topics.add(topic)\n        return self.cluster.request_update()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends a metadata request if needed.", "response": "def _maybe_refresh_metadata(self, wakeup=False):\n        \"\"\"Send a metadata request if needed.\n\n        Returns:\n            int: milliseconds until next refresh\n        \"\"\"\n        ttl = self.cluster.ttl()\n        wait_for_in_progress_ms = self.config['request_timeout_ms'] if self._metadata_refresh_in_progress else 0\n        metadata_timeout = max(ttl, wait_for_in_progress_ms)\n\n        if metadata_timeout > 0:\n            return metadata_timeout\n\n        # Beware that the behavior of this method and the computation of\n        # timeouts for poll() are highly dependent on the behavior of\n        # least_loaded_node()\n        node_id = self.least_loaded_node()\n        if node_id is None:\n            log.debug(\"Give up sending metadata request since no node is available\");\n            return self.config['reconnect_backoff_ms']\n\n        if self._can_send_request(node_id):\n            topics = list(self._topics)\n            if not topics and self.cluster.is_bootstrap(node_id):\n                topics = list(self.config['bootstrap_topics_filter'])\n\n            if self.cluster.need_all_topic_metadata or not topics:\n                topics = [] if self.config['api_version'] < (0, 10) else None\n            api_version = 0 if self.config['api_version'] < (0, 10) else 1\n            request = MetadataRequest[api_version](topics)\n            log.debug(\"Sending metadata request %s to node %s\", request, node_id)\n            future = self.send(node_id, request, wakeup=wakeup)\n            future.add_callback(self.cluster.update_metadata)\n            future.add_errback(self.cluster.failed_update)\n\n            self._metadata_refresh_in_progress = True\n            def refresh_done(val_or_error):\n                self._metadata_refresh_in_progress = False\n            future.add_callback(refresh_done)\n            future.add_errback(refresh_done)\n            return self.config['request_timeout_ms']\n\n        # If there's any connection establishment underway, wait until it completes. This prevents\n        # the client from unnecessarily connecting to additional nodes while a previous connection\n        # attempt has not been completed.\n        if self._connecting:\n            return self.config['reconnect_backoff_ms']\n\n        if self.maybe_connect(node_id, wakeup=wakeup):\n            log.debug(\"Initializing connection to node %s for metadata request\", node_id)\n            return self.config['reconnect_backoff_ms']\n\n        # connected but can't send more, OR connecting\n        # In either case we just need to wait for a network event\n        # to let us know the selected connection might be usable again.\n        return float('inf')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_version(self, node_id=None, timeout=2, strict=False):\n        self._lock.acquire()\n        end = time.time() + timeout\n        while time.time() < end:\n\n            # It is possible that least_loaded_node falls back to bootstrap,\n            # which can block for an increasing backoff period\n            try_node = node_id or self.least_loaded_node()\n            if try_node is None:\n                self._lock.release()\n                raise Errors.NoBrokersAvailable()\n            self._maybe_connect(try_node)\n            conn = self._conns[try_node]\n\n            # We will intentionally cause socket failures\n            # These should not trigger metadata refresh\n            self._refresh_on_disconnects = False\n            try:\n                remaining = end - time.time()\n                version = conn.check_version(timeout=remaining, strict=strict, topics=list(self.config['bootstrap_topics_filter']))\n                if version >= (0, 10, 0):\n                    # cache the api versions map if it's available (starting\n                    # in 0.10 cluster version)\n                    self._api_versions = conn.get_api_versions()\n                self._lock.release()\n                return version\n            except Errors.NodeNotReadyError:\n                # Only raise to user if this is a node-specific request\n                if node_id is not None:\n                    self._lock.release()\n                    raise\n            finally:\n                self._refresh_on_disconnects = True\n\n        # Timeout\n        else:\n            self._lock.release()\n            raise Errors.NoBrokersAvailable()", "response": "Attempt to guess the version of a Kafka broker."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef window_size(self, config, now):\n        # purge old samples before we compute the window size\n        self._stat.purge_obsolete_samples(config, now)\n\n        \"\"\"\n        Here we check the total amount of time elapsed since the oldest\n        non-obsolete window. This give the total window_size of the batch\n        which is the time used for Rate computation. However, there is\n        an issue if we do not have sufficient data for e.g. if only\n        1 second has elapsed in a 30 second window, the measured rate\n        will be very high. Hence we assume that the elapsed time is\n        always N-1 complete windows plus whatever fraction of the final\n        window is complete.\n\n        Note that we could simply count the amount of time elapsed in\n        the current window and add n-1 windows to get the total time,\n        but this approach does not account for sleeps. AbstractSampledStat\n        only creates samples whenever record is called, if no record is\n        called for a period of time that time is not accounted for in\n        window_size and produces incorrect results.\n        \"\"\"\n        total_elapsed_time_ms = now - self._stat.oldest(now).last_window_ms\n        # Check how many full windows of data we have currently retained\n        num_full_windows = int(total_elapsed_time_ms / config.time_window_ms)\n        min_full_windows = config.samples - 1\n\n        # If the available windows are less than the minimum required,\n        # add the difference to the totalElapsedTime\n        if num_full_windows < min_full_windows:\n            total_elapsed_time_ms += ((min_full_windows - num_full_windows) *\n                                      config.time_window_ms)\n\n        return total_elapsed_time_ms", "response": "Compute the total size of the batch record for this time window."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send_messages(self, topic, partition, *msg):\n        return self._send_messages(topic, partition, *msg)", "response": "This method sends a message to the broker."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstops the producer (async mode). Blocks until async thread completes.", "response": "def stop(self, timeout=None):\n        \"\"\"\n        Stop the producer (async mode). Blocks until async thread completes.\n        \"\"\"\n        if timeout is not None:\n            log.warning('timeout argument to stop() is deprecated - '\n                        'it will be removed in future release')\n\n        if not self.async_send:\n            log.warning('producer.stop() called, but producer is not async')\n            return\n\n        if self.stopped:\n            log.warning('producer.stop() called, but producer is already stopped')\n            return\n\n        if self.async_send:\n            self.queue.put((STOP_ASYNC_PRODUCER, None, None))\n            self.thread_stop_event.set()\n            self.thread.join()\n\n        if hasattr(self, '_cleanup_func'):\n            # Remove cleanup handler now that we've stopped\n\n            # py3 supports unregistering\n            if hasattr(atexit, 'unregister'):\n                atexit.unregister(self._cleanup_func)  # pylint: disable=no-member\n\n            # py2 requires removing from private attribute...\n            else:\n\n                # ValueError on list.remove() if the exithandler no longer exists\n                # but that is fine here\n                try:\n                    atexit._exithandlers.remove(  # pylint: disable=no-member\n                        (self._cleanup_func, (self,), {}))\n                except ValueError:\n                    pass\n\n            del self._cleanup_func\n\n        self.stopped = True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef append(self, offset, timestamp, key, value, headers=None):\n        assert not headers, \"Headers not supported in v0/v1\"\n        # Check types\n        if type(offset) != int:\n            raise TypeError(offset)\n        if self._magic == 0:\n            timestamp = self.NO_TIMESTAMP\n        elif timestamp is None:\n            timestamp = int(time.time() * 1000)\n        elif type(timestamp) != int:\n            raise TypeError(\n                \"`timestamp` should be int, but {} provided\".format(\n                    type(timestamp)))\n        if not (key is None or\n                isinstance(key, (bytes, bytearray, memoryview))):\n            raise TypeError(\n                \"Not supported type for key: {}\".format(type(key)))\n        if not (value is None or\n                isinstance(value, (bytes, bytearray, memoryview))):\n            raise TypeError(\n                \"Not supported type for value: {}\".format(type(value)))\n\n        # Check if we have room for another message\n        pos = len(self._buffer)\n        size = self.size_in_bytes(offset, timestamp, key, value)\n        # We always allow at least one record to be appended\n        if offset != 0 and pos + size >= self._batch_size:\n            return None\n\n        # Allocate proper buffer length\n        self._buffer.extend(bytearray(size))\n\n        # Encode message\n        crc = self._encode_msg(pos, offset, timestamp, key, value)\n\n        return LegacyRecordMetadata(offset, crc, size, timestamp)", "response": "Append a new record to the batch."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _encode_msg(self, start_pos, offset, timestamp, key, value,\n                    attributes=0):\n        \"\"\" Encode msg data into the `msg_buffer`, which should be allocated\n            to at least the size of this message.\n        \"\"\"\n        magic = self._magic\n        buf = self._buffer\n        pos = start_pos\n\n        # Write key and value\n        pos += self.KEY_OFFSET_V0 if magic == 0 else self.KEY_OFFSET_V1\n\n        if key is None:\n            struct.pack_into(\">i\", buf, pos, -1)\n            pos += self.KEY_LENGTH\n        else:\n            key_size = len(key)\n            struct.pack_into(\">i\", buf, pos, key_size)\n            pos += self.KEY_LENGTH\n            buf[pos: pos + key_size] = key\n            pos += key_size\n\n        if value is None:\n            struct.pack_into(\">i\", buf, pos, -1)\n            pos += self.VALUE_LENGTH\n        else:\n            value_size = len(value)\n            struct.pack_into(\">i\", buf, pos, value_size)\n            pos += self.VALUE_LENGTH\n            buf[pos: pos + value_size] = value\n            pos += value_size\n        length = (pos - start_pos) - self.LOG_OVERHEAD\n\n        # Write msg header. Note, that Crc will be updated later\n        if magic == 0:\n            self.HEADER_STRUCT_V0.pack_into(\n                buf, start_pos,\n                offset, length, 0, magic, attributes)\n        else:\n            self.HEADER_STRUCT_V1.pack_into(\n                buf, start_pos,\n                offset, length, 0, magic, attributes, timestamp)\n\n        # Calculate CRC for msg\n        crc_data = memoryview(buf)[start_pos + self.MAGIC_OFFSET:]\n        crc = calc_crc32(crc_data)\n        struct.pack_into(\">I\", buf, start_pos + self.CRC_OFFSET, crc)\n        return crc", "response": "Encode the message into the msg_buffer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the size of the record in bytes.", "response": "def size_in_bytes(self, offset, timestamp, key, value, headers=None):\n        \"\"\" Actual size of message to add\n        \"\"\"\n        assert not headers, \"Headers not supported in v0/v1\"\n        magic = self._magic\n        return self.LOG_OVERHEAD + self.record_size(magic, key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef estimate_size_in_bytes(cls, magic, compression_type, key, value):\n        assert magic in [0, 1], \"Not supported magic\"\n        # In case of compression we may need another overhead for inner msg\n        if compression_type:\n            return (\n                cls.LOG_OVERHEAD + cls.record_overhead(magic) +\n                cls.record_size(magic, key, value)\n            )\n        return cls.LOG_OVERHEAD + cls.record_size(magic, key, value)", "response": "Estimate the size of a record in bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef allocate(self, size, max_time_to_block_ms):\n        with self._lock:\n            # check if we have a free buffer of the right size pooled\n            if self._free:\n                return self._free.popleft()\n\n            elif self._poolable_size == 0:\n                return io.BytesIO()\n\n            else:\n                # we are out of buffers and will have to block\n                buf = None\n                more_memory = threading.Condition(self._lock)\n                self._waiters.append(more_memory)\n                # loop over and over until we have a buffer or have reserved\n                # enough memory to allocate one\n                while buf is None:\n                    start_wait = time.time()\n                    more_memory.wait(max_time_to_block_ms / 1000.0)\n                    end_wait = time.time()\n                    if self.wait_time:\n                        self.wait_time.record(end_wait - start_wait)\n\n                    if self._free:\n                        buf = self._free.popleft()\n                    else:\n                        self._waiters.remove(more_memory)\n                        raise Errors.KafkaTimeoutError(\n                            \"Failed to allocate memory within the configured\"\n                            \" max blocking time\")\n\n                # remove the condition for this thread to let the next thread\n                # in line start getting memory\n                removed = self._waiters.popleft()\n                assert removed is more_memory, 'Wrong condition'\n\n                # signal any additional waiters if there is more memory left\n                # over for them\n                if self._free and self._waiters:\n                    self._waiters[0].notify()\n\n                # unlock and return the buffer\n                return buf", "response": "Allocate a buffer of the given size. This method blocks if there is not enough memory and the buffer pool is configured with blocking mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef deallocate(self, buf):\n        with self._lock:\n            # BytesIO.truncate here makes the pool somewhat pointless\n            # but we stick with the BufferPool API until migrating to\n            # bytesarray / memoryview. The buffer we return must not\n            # expose any prior data on read().\n            buf.truncate(0)\n            self._free.append(buf)\n            if self._waiters:\n                self._waiters[0].notify()", "response": "Deallocate a buffer from the pool."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndecode a message set into a list of items.", "response": "def decode(cls, data, bytes_to_read=None):\n        \"\"\"Compressed messages should pass in bytes_to_read (via message size)\n        otherwise, we decode from data as Int32\n        \"\"\"\n        if isinstance(data, bytes):\n            data = io.BytesIO(data)\n        if bytes_to_read is None:\n            bytes_to_read = Int32.decode(data)\n\n        # if FetchRequest max_bytes is smaller than the available message set\n        # the server returns partial data for the final message\n        # So create an internal buffer to avoid over-reading\n        raw = io.BytesIO(data.read(bytes_to_read))\n\n        items = []\n        while bytes_to_read:\n            try:\n                offset = Int64.decode(raw)\n                msg_bytes = Bytes.decode(raw)\n                bytes_to_read -= 8 + 4 + len(msg_bytes)\n                items.append((offset, len(msg_bytes), Message.decode(msg_bytes)))\n            except ValueError:\n                # PartialMessage to signal that max_bytes may be too small\n                items.append((None, None, PartialMessage()))\n                break\n        return items"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nencoding an integer to a varint presentation.", "response": "def encode_varint(value, write):\n    \"\"\" Encode an integer to a varint presentation. See\n    https://developers.google.com/protocol-buffers/docs/encoding?csw=1#varints\n    on how those can be produced.\n\n        Arguments:\n            value (int): Value to encode\n            write (function): Called per byte that needs to be writen\n\n        Returns:\n            int: Number of bytes written\n    \"\"\"\n    value = (value << 1) ^ (value >> 63)\n\n    if value <= 0x7f:  # 1 byte\n        write(value)\n        return 1\n    if value <= 0x3fff:  # 2 bytes\n        write(0x80 | (value & 0x7f))\n        write(value >> 7)\n        return 2\n    if value <= 0x1fffff:  # 3 bytes\n        write(0x80 | (value & 0x7f))\n        write(0x80 | ((value >> 7) & 0x7f))\n        write(value >> 14)\n        return 3\n    if value <= 0xfffffff:  # 4 bytes\n        write(0x80 | (value & 0x7f))\n        write(0x80 | ((value >> 7) & 0x7f))\n        write(0x80 | ((value >> 14) & 0x7f))\n        write(value >> 21)\n        return 4\n    if value <= 0x7ffffffff:  # 5 bytes\n        write(0x80 | (value & 0x7f))\n        write(0x80 | ((value >> 7) & 0x7f))\n        write(0x80 | ((value >> 14) & 0x7f))\n        write(0x80 | ((value >> 21) & 0x7f))\n        write(value >> 28)\n        return 5\n    else:\n        # Return to general algorithm\n        bits = value & 0x7f\n        value >>= 7\n        i = 0\n        while value:\n            write(0x80 | bits)\n            bits = value & 0x7f\n            value >>= 7\n            i += 1\n    write(bits)\n    return i"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the number of bytes needed to encode an integer in variable - length format.", "response": "def size_of_varint(value):\n    \"\"\" Number of bytes needed to encode an integer in variable-length format.\n    \"\"\"\n    value = (value << 1) ^ (value >> 63)\n    if value <= 0x7f:\n        return 1\n    if value <= 0x3fff:\n        return 2\n    if value <= 0x1fffff:\n        return 3\n    if value <= 0xfffffff:\n        return 4\n    if value <= 0x7ffffffff:\n        return 5\n    if value <= 0x3ffffffffff:\n        return 6\n    if value <= 0x1ffffffffffff:\n        return 7\n    if value <= 0xffffffffffffff:\n        return 8\n    if value <= 0x7fffffffffffffff:\n        return 9\n    return 10"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef decode_varint(buffer, pos=0):\n    result = buffer[pos]\n    if not (result & 0x81):\n        return (result >> 1), pos + 1\n    if not (result & 0x80):\n        return (result >> 1) ^ (~0), pos + 1\n\n    result &= 0x7f\n    pos += 1\n    shift = 7\n    while 1:\n        b = buffer[pos]\n        result |= ((b & 0x7f) << shift)\n        pos += 1\n        if not (b & 0x80):\n            return ((result >> 1) ^ -(result & 1), pos)\n        shift += 7\n        if shift >= 64:\n            raise ValueError(\"Out of int64 range\")", "response": "Decodes an integer from a varint presentation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_request(self, request, correlation_id=None):\n        log.debug('Sending request %s', request)\n        if correlation_id is None:\n            correlation_id = self._next_correlation_id()\n        header = RequestHeader(request,\n                               correlation_id=correlation_id,\n                               client_id=self._client_id)\n        message = b''.join([header.encode(), request.encode()])\n        size = Int32.encode(len(message))\n        data = size + message\n        self.bytes_to_send.append(data)\n        if request.expect_response():\n            ifr = (correlation_id, request)\n            self.in_flight_requests.append(ifr)\n        return correlation_id", "response": "Encode and queue a kafka api request for sending."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving all pending bytes to send on the network", "response": "def send_bytes(self):\n        \"\"\"Retrieve all pending bytes to send on the network\"\"\"\n        data = b''.join(self.bytes_to_send)\n        self.bytes_to_send = []\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef receive_bytes(self, data):\n        i = 0\n        n = len(data)\n        responses = []\n        while i < n:\n\n            # Not receiving is the state of reading the payload header\n            if not self._receiving:\n                bytes_to_read = min(4 - self._header.tell(), n - i)\n                self._header.write(data[i:i+bytes_to_read])\n                i += bytes_to_read\n\n                if self._header.tell() == 4:\n                    self._header.seek(0)\n                    nbytes = Int32.decode(self._header)\n                    # reset buffer and switch state to receiving payload bytes\n                    self._rbuffer = KafkaBytes(nbytes)\n                    self._receiving = True\n                elif self._header.tell() > 4:\n                    raise Errors.KafkaError('this should not happen - are you threading?')\n\n            if self._receiving:\n                total_bytes = len(self._rbuffer)\n                staged_bytes = self._rbuffer.tell()\n                bytes_to_read = min(total_bytes - staged_bytes, n - i)\n                self._rbuffer.write(data[i:i+bytes_to_read])\n                i += bytes_to_read\n\n                staged_bytes = self._rbuffer.tell()\n                if staged_bytes > total_bytes:\n                    raise Errors.KafkaError('Receive buffer has more bytes than expected?')\n\n                if staged_bytes != total_bytes:\n                    break\n\n                self._receiving = False\n                self._rbuffer.seek(0)\n                resp = self._process_response(self._rbuffer)\n                responses.append(resp)\n                self._reset_buffer()\n        return responses", "response": "Process bytes received from a Kafka broker to a Kafka broker."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_conn(self, host, port, afi):\n        host_key = (host, port)\n        if host_key not in self._conns:\n            self._conns[host_key] = BrokerConnection(\n                host, port, afi,\n                request_timeout_ms=self.timeout * 1000,\n                client_id=self.client_id\n            )\n\n        conn = self._conns[host_key]\n        if not conn.connect_blocking(self.timeout):\n            conn.close()\n            raise KafkaConnectionError(\"%s:%s (%s)\" % (host, port, afi))\n        return conn", "response": "Get or create a connection to a broker using host and port"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the leader for a topic and partition.", "response": "def _get_leader_for_partition(self, topic, partition):\n        \"\"\"\n        Returns the leader for a partition or None if the partition exists\n        but has no leader.\n\n        Raises:\n            UnknownTopicOrPartitionError: If the topic or partition is not part\n                of the metadata.\n            LeaderNotAvailableError: If the server has metadata, but there is no\n        current leader.\n        \"\"\"\n\n        key = TopicPartition(topic, partition)\n\n        # Use cached metadata if it is there\n        if self.topics_to_brokers.get(key) is not None:\n            return self.topics_to_brokers[key]\n\n        # Otherwise refresh metadata\n\n        # If topic does not already exist, this will raise\n        # UnknownTopicOrPartitionError if not auto-creating\n        # LeaderNotAvailableError otherwise until partitions are created\n        self.load_metadata_for_topics(topic)\n\n        # If the partition doesn't actually exist, raise\n        if partition not in self.topic_partitions.get(topic, []):\n            raise UnknownTopicOrPartitionError(key)\n\n        # If there's no leader for the partition, raise\n        leader = self.topic_partitions[topic][partition]\n        if leader == -1:\n            raise LeaderNotAvailableError((topic, partition))\n\n        # Otherwise return the BrokerMetadata\n        return self.brokers[leader]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_coordinator_for_group(self, group):\n\n        resp = self.send_consumer_metadata_request(group)\n\n        # If there's a problem with finding the coordinator, raise the\n        # provided error\n        kafka.errors.check_error(resp)\n\n        # Otherwise return the BrokerMetadata\n        return BrokerMetadata(resp.nodeId, resp.host, resp.port, None)", "response": "Returns the coordinator broker for a given consumer group."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a broker - aware request to all available brokers.", "response": "def _send_broker_unaware_request(self, payloads, encoder_fn, decoder_fn):\n        \"\"\"\n        Attempt to send a broker-agnostic request to one of the available\n        brokers. Keep trying until you succeed.\n        \"\"\"\n        hosts = set()\n        for broker in self.brokers.values():\n            host, port, afi = get_ip_port_afi(broker.host)\n            hosts.add((host, broker.port, afi))\n\n        hosts.update(self.hosts)\n        hosts = list(hosts)\n        random.shuffle(hosts)\n\n        for (host, port, afi) in hosts:\n            try:\n                conn = self._get_conn(host, port, afi)\n            except KafkaConnectionError:\n                log.warning(\"Skipping unconnected connection: %s:%s (AFI %s)\",\n                            host, port, afi)\n                continue\n            request = encoder_fn(payloads=payloads)\n            future = conn.send(request)\n\n            # Block\n            while not future.is_done:\n                for r, f in conn.recv():\n                    f.success(r)\n\n            if future.failed():\n                log.error(\"Request failed: %s\", future.exception)\n                continue\n\n            return decoder_fn(future.value)\n\n        raise KafkaUnavailableError('All servers failed to process request: %s' % (hosts,))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _send_broker_aware_request(self, payloads, encoder_fn, decoder_fn):\n        # encoders / decoders do not maintain ordering currently\n        # so we need to keep this so we can rebuild order before returning\n        original_ordering = [(p.topic, p.partition) for p in payloads]\n\n        # Connection errors generally mean stale metadata\n        # although sometimes it means incorrect api request\n        # Unfortunately there is no good way to tell the difference\n        # so we'll just reset metadata on all errors to be safe\n        refresh_metadata = False\n\n        # For each broker, send the list of request payloads\n        # and collect the responses and errors\n        payloads_by_broker = self._payloads_by_broker(payloads)\n        responses = {}\n\n        def failed_payloads(payloads):\n            for payload in payloads:\n                topic_partition = (str(payload.topic), payload.partition)\n                responses[(topic_partition)] = FailedPayloadsError(payload)\n\n        # For each BrokerConnection keep the real socket so that we can use\n        # a select to perform unblocking I/O\n        connections_by_future = {}\n        for broker, broker_payloads in six.iteritems(payloads_by_broker):\n            if broker is None:\n                failed_payloads(broker_payloads)\n                continue\n\n            host, port, afi = get_ip_port_afi(broker.host)\n            try:\n                conn = self._get_conn(host, broker.port, afi)\n            except KafkaConnectionError:\n                refresh_metadata = True\n                failed_payloads(broker_payloads)\n                continue\n\n            request = encoder_fn(payloads=broker_payloads)\n            future = conn.send(request)\n\n            if future.failed():\n                refresh_metadata = True\n                failed_payloads(broker_payloads)\n                continue\n\n            if not request.expect_response():\n                for payload in broker_payloads:\n                    topic_partition = (str(payload.topic), payload.partition)\n                    responses[topic_partition] = None\n                continue\n\n            connections_by_future[future] = (conn, broker)\n\n        conn = None\n        while connections_by_future:\n            futures = list(connections_by_future.keys())\n\n            # block until a socket is ready to be read\n            sockets = [\n                conn._sock\n                for future, (conn, _) in six.iteritems(connections_by_future)\n                if not future.is_done and conn._sock is not None]\n            if sockets:\n                read_socks, _, _ = select.select(sockets, [], [])\n\n            for future in futures:\n\n                if not future.is_done:\n                    conn, _ = connections_by_future[future]\n                    for r, f in conn.recv():\n                        f.success(r)\n                    continue\n\n                _, broker = connections_by_future.pop(future)\n                if future.failed():\n                    refresh_metadata = True\n                    failed_payloads(payloads_by_broker[broker])\n\n                else:\n                    for payload_response in decoder_fn(future.value):\n                        topic_partition = (str(payload_response.topic),\n                                           payload_response.partition)\n                        responses[topic_partition] = payload_response\n\n        if refresh_metadata:\n            self.reset_all_metadata()\n\n        # Return responses in the same order as provided\n        return [responses[tp] for tp in original_ordering]", "response": "Send a list of payloads to the broker and return a list of response objects."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _send_consumer_aware_request(self, group, payloads, encoder_fn, decoder_fn):\n        # encoders / decoders do not maintain ordering currently\n        # so we need to keep this so we can rebuild order before returning\n        original_ordering = [(p.topic, p.partition) for p in payloads]\n\n        broker = self._get_coordinator_for_group(group)\n\n        # Send the list of request payloads and collect the responses and\n        # errors\n        responses = {}\n        request_id = self._next_id()\n        log.debug('Request %s to %s: %s', request_id, broker, payloads)\n        request = encoder_fn(client_id=self.client_id,\n                             correlation_id=request_id, payloads=payloads)\n\n        # Send the request, recv the response\n        try:\n            host, port, afi = get_ip_port_afi(broker.host)\n            conn = self._get_conn(host, broker.port, afi)\n        except KafkaConnectionError as e:\n            log.warning('KafkaConnectionError attempting to send request %s '\n                        'to server %s: %s', request_id, broker, e)\n\n            for payload in payloads:\n                topic_partition = (payload.topic, payload.partition)\n                responses[topic_partition] = FailedPayloadsError(payload)\n\n        # No exception, try to get response\n        else:\n\n            future = conn.send(request_id, request)\n            while not future.is_done:\n                for r, f in conn.recv():\n                    f.success(r)\n\n            # decoder_fn=None signal that the server is expected to not\n            # send a response.  This probably only applies to\n            # ProduceRequest w/ acks = 0\n            if decoder_fn is None:\n                log.debug('Request %s does not expect a response '\n                          '(skipping conn.recv)', request_id)\n                for payload in payloads:\n                    topic_partition = (payload.topic, payload.partition)\n                    responses[topic_partition] = None\n                return []\n\n            if future.failed():\n                log.warning('Error attempting to receive a '\n                            'response to request %s from server %s: %s',\n                            request_id, broker, future.exception)\n\n                for payload in payloads:\n                    topic_partition = (payload.topic, payload.partition)\n                    responses[topic_partition] = FailedPayloadsError(payload)\n\n            else:\n                response = future.value\n                _resps = []\n                for payload_response in decoder_fn(response):\n                    topic_partition = (payload_response.topic,\n                                       payload_response.partition)\n                    responses[topic_partition] = payload_response\n                    _resps.append(payload_response)\n                log.debug('Response %s: %s', request_id, _resps)\n\n        # Return responses in the same order as provided\n        return [responses[tp] for tp in original_ordering]", "response": "Send a list of requests to the broker for the specified group and return the list of responses."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef copy(self):\n        _conns = self._conns\n        self._conns = {}\n        c = copy.deepcopy(self)\n        self._conns = _conns\n        return c", "response": "Create an inactive copy of the client object suitable for passing\n        to a separate thread."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_metadata_for_topics(self, *topics, **kwargs):\n        if 'ignore_leadernotavailable' in kwargs:\n            ignore_leadernotavailable = kwargs['ignore_leadernotavailable']\n        else:\n            ignore_leadernotavailable = False\n\n        if topics:\n            self.reset_topic_metadata(*topics)\n        else:\n            self.reset_all_metadata()\n\n        resp = self.send_metadata_request(topics)\n\n        log.debug('Updating broker metadata: %s', resp.brokers)\n        log.debug('Updating topic metadata: %s', [topic for _, topic, _ in resp.topics])\n\n        self.brokers = dict([(nodeId, BrokerMetadata(nodeId, host, port, None))\n                             for nodeId, host, port in resp.brokers])\n\n        for error, topic, partitions in resp.topics:\n            # Errors expected for new topics\n            if error:\n                error_type = kafka.errors.kafka_errors.get(error, UnknownError)\n                if error_type in (UnknownTopicOrPartitionError, LeaderNotAvailableError):\n                    log.error('Error loading topic metadata for %s: %s (%s)',\n                              topic, error_type, error)\n                    if topic not in topics:\n                        continue\n                    elif (error_type is LeaderNotAvailableError and\n                          ignore_leadernotavailable):\n                        continue\n                raise error_type(topic)\n\n            self.topic_partitions[topic] = {}\n            for error, partition, leader, _, _ in partitions:\n\n                self.topic_partitions[topic][partition] = leader\n\n                # Populate topics_to_brokers dict\n                topic_part = TopicPartition(topic, partition)\n\n                # Check for partition errors\n                if error:\n                    error_type = kafka.errors.kafka_errors.get(error, UnknownError)\n\n                    # If No Leader, topics_to_brokers topic_partition -> None\n                    if error_type is LeaderNotAvailableError:\n                        log.error('No leader for topic %s partition %d', topic, partition)\n                        self.topics_to_brokers[topic_part] = None\n                        continue\n\n                    # If one of the replicas is unavailable -- ignore\n                    # this error code is provided for admin purposes only\n                    # we never talk to replicas, only the leader\n                    elif error_type is ReplicaNotAvailableError:\n                        log.debug('Some (non-leader) replicas not available for topic %s partition %d', topic, partition)\n\n                    else:\n                        raise error_type(topic_part)\n\n                # If Known Broker, topic_partition -> BrokerMetadata\n                if leader in self.brokers:\n                    self.topics_to_brokers[topic_part] = self.brokers[leader]\n\n                # If Unknown Broker, fake BrokerMetadata so we don't lose the id\n                # (not sure how this could happen. server could be in bad state)\n                else:\n                    self.topics_to_brokers[topic_part] = BrokerMetadata(\n                        leader, None, None, None\n                    )", "response": "Fetch broker and topic - partition metadata for the specified topics."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending a ProduceRequest to the broker.", "response": "def send_produce_request(self, payloads=(), acks=1, timeout=1000,\n                             fail_on_error=True, callback=None):\n        \"\"\"\n        Encode and send some ProduceRequests\n\n        ProduceRequests will be grouped by (topic, partition) and then\n        sent to a specific broker. Output is a list of responses in the\n        same order as the list of payloads specified\n\n        Arguments:\n            payloads (list of ProduceRequest): produce requests to send to kafka\n                ProduceRequest payloads must not contain duplicates for any\n                topic-partition.\n            acks (int, optional): how many acks the servers should receive from replica\n                brokers before responding to the request. If it is 0, the server\n                will not send any response. If it is 1, the server will wait\n                until the data is written to the local log before sending a\n                response.  If it is -1, the server will wait until the message\n                is committed by all in-sync replicas before sending a response.\n                For any value > 1, the server will wait for this number of acks to\n                occur (but the server will never wait for more acknowledgements than\n                there are in-sync replicas). defaults to 1.\n            timeout (int, optional): maximum time in milliseconds the server can\n                await the receipt of the number of acks, defaults to 1000.\n            fail_on_error (bool, optional): raise exceptions on connection and\n                server response errors, defaults to True.\n            callback (function, optional): instead of returning the ProduceResponse,\n                first pass it through this function, defaults to None.\n\n        Returns:\n            list of ProduceResponses, or callback results if supplied, in the\n            order of input payloads\n        \"\"\"\n\n        encoder = functools.partial(\n            KafkaProtocol.encode_produce_request,\n            acks=acks,\n            timeout=timeout)\n\n        if acks == 0:\n            decoder = None\n        else:\n            decoder = KafkaProtocol.decode_produce_response\n\n        resps = self._send_broker_aware_request(payloads, encoder, decoder)\n\n        return [resp if not callback else callback(resp) for resp in resps\n                if resp is not None and\n                (not fail_on_error or not self._raise_on_response_error(resp))]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send_fetch_request(self, payloads=(), fail_on_error=True,\n                           callback=None, max_wait_time=100, min_bytes=4096):\n        \"\"\"\n        Encode and send a FetchRequest\n\n        Payloads are grouped by topic and partition so they can be pipelined\n        to the same brokers.\n        \"\"\"\n\n        encoder = functools.partial(KafkaProtocol.encode_fetch_request,\n                          max_wait_time=max_wait_time,\n                          min_bytes=min_bytes)\n\n        resps = self._send_broker_aware_request(\n            payloads, encoder,\n            KafkaProtocol.decode_fetch_response)\n\n        return [resp if not callback else callback(resp) for resp in resps\n                if not fail_on_error or not self._raise_on_response_error(resp)]", "response": "Send a FetchRequest to all brokers."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nverifying that the token is valid and that the token type and exp claims are present.", "response": "def verify(self):\n        \"\"\"\n        Performs additional validation steps which were not performed when this\n        token was decoded.  This method is part of the \"public\" API to indicate\n        the intention that it may be overridden in subclasses.\n        \"\"\"\n        # According to RFC 7519, the \"exp\" claim is OPTIONAL\n        # (https://tools.ietf.org/html/rfc7519#section-4.1.4).  As a more\n        # correct behavior for authorization tokens, we require an \"exp\"\n        # claim.  We don't want any zombie tokens walking around.\n        self.check_exp()\n\n        # Ensure token id is present\n        if api_settings.JTI_CLAIM not in self.payload:\n            raise TokenError(_('Token has no id'))\n\n        self.verify_token_type()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef verify_token_type(self):\n        try:\n            token_type = self.payload[api_settings.TOKEN_TYPE_CLAIM]\n        except KeyError:\n            raise TokenError(_('Token has no type'))\n\n        if self.token_type != token_type:\n            raise TokenError(_('Token has wrong type'))", "response": "Verifies that the token type claim is present and has the correct value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the expiration time of a token.", "response": "def set_exp(self, claim='exp', from_time=None, lifetime=None):\n        \"\"\"\n        Updates the expiration time of a token.\n        \"\"\"\n        if from_time is None:\n            from_time = self.current_time\n\n        if lifetime is None:\n            lifetime = self.lifetime\n\n        self.payload[claim] = datetime_to_epoch(from_time + lifetime)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks whether a timestamp value in the given claim has passed since the given datetime value in current_time. Raises a TokenError if so.", "response": "def check_exp(self, claim='exp', current_time=None):\n        \"\"\"\n        Checks whether a timestamp value in the given claim has passed (since\n        the given datetime value in `current_time`).  Raises a TokenError with\n        a user-facing error message if so.\n        \"\"\"\n        if current_time is None:\n            current_time = self.current_time\n\n        try:\n            claim_value = self.payload[claim]\n        except KeyError:\n            raise TokenError(format_lazy(_(\"Token has no '{}' claim\"), claim))\n\n        claim_time = datetime_from_epoch(claim_value)\n        if claim_time <= current_time:\n            raise TokenError(format_lazy(_(\"Token '{}' claim has expired\"), claim))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an authorization token for the given user.", "response": "def for_user(cls, user):\n        \"\"\"\n        Returns an authorization token for the given user that will be provided\n        after authenticating the user's credentials.\n        \"\"\"\n        user_id = getattr(user, api_settings.USER_ID_FIELD)\n        if not isinstance(user_id, int):\n            user_id = str(user_id)\n\n        token = cls()\n        token[api_settings.USER_ID_CLAIM] = user_id\n\n        return token"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef access_token(self):\n        access = AccessToken()\n\n        # Use instantiation time of refresh token as relative timestamp for\n        # access token \"exp\" claim.  This ensures that both a refresh and\n        # access token expire relative to the same time if they are created as\n        # a pair.\n        access.set_exp(from_time=self.current_time)\n\n        no_copy = self.no_copy_claims\n        for claim, value in self.payload.items():\n            if claim in no_copy:\n                continue\n            access[claim] = value\n\n        return access", "response": "Returns an access token created from this refresh token."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an encoded token for the given payload dictionary.", "response": "def encode(self, payload):\n        \"\"\"\n        Returns an encoded token for the given payload dictionary.\n        \"\"\"\n        token = jwt.encode(payload, self.signing_key, algorithm=self.algorithm)\n        return token.decode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndecoding a given token and returns its payload dictionary.", "response": "def decode(self, token, verify=True):\n        \"\"\"\n        Performs a validation of the given token and returns its payload\n        dictionary.\n\n        Raises a `TokenBackendError` if the token is malformed, if its\n        signature check fails, or if its 'exp' claim indicates it has expired.\n        \"\"\"\n        try:\n            return jwt.decode(token, self.verifying_key, algorithms=[self.algorithm], verify=verify)\n        except InvalidTokenError:\n            raise TokenBackendError(_('Token is invalid or expired'))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_header(self, request):\n        header = request.META.get('HTTP_AUTHORIZATION')\n\n        if isinstance(header, str):\n            # Work around django test client oddness\n            header = header.encode(HTTP_HEADER_ENCODING)\n\n        return header", "response": "Extracts the header containing the JSON web token from the given request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_raw_token(self, header):\n        parts = header.split()\n\n        if len(parts) == 0:\n            # Empty AUTHORIZATION header sent\n            return None\n\n        if parts[0] not in AUTH_HEADER_TYPE_BYTES:\n            # Assume the header does not contain a JSON web token\n            return None\n\n        if len(parts) != 2:\n            raise AuthenticationFailed(\n                _('Authorization header must contain two space-delimited values'),\n                code='bad_authorization_header',\n            )\n\n        return parts[1]", "response": "Extracts an unvalidated JSON web token from the given Authorization header value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating an encoded JSON web token and returns a validated token wrapper object.", "response": "def get_validated_token(self, raw_token):\n        \"\"\"\n        Validates an encoded JSON web token and returns a validated token\n        wrapper object.\n        \"\"\"\n        messages = []\n        for AuthToken in api_settings.AUTH_TOKEN_CLASSES:\n            try:\n                return AuthToken(raw_token)\n            except TokenError as e:\n                messages.append({'token_class': AuthToken.__name__,\n                                 'token_type': AuthToken.token_type,\n                                 'message': e.args[0]})\n\n        raise InvalidToken({\n            'detail': _('Given token not valid for any token type'),\n            'messages': messages,\n        })"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nattempts to find and return a user using the given validated token.", "response": "def get_user(self, validated_token):\n        \"\"\"\n        Attempts to find and return a user using the given validated token.\n        \"\"\"\n        try:\n            user_id = validated_token[api_settings.USER_ID_CLAIM]\n        except KeyError:\n            raise InvalidToken(_('Token contained no recognizable user identification'))\n\n        try:\n            user = User.objects.get(**{api_settings.USER_ID_FIELD: user_id})\n        except User.DoesNotExist:\n            raise AuthenticationFailed(_('User not found'), code='user_not_found')\n\n        if not user.is_active:\n            raise AuthenticationFailed(_('User is inactive'), code='user_inactive')\n\n        return user"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a stateless user object which is backed by the given validated_token.", "response": "def get_user(self, validated_token):\n        \"\"\"\n        Returns a stateless user object which is backed by the given validated\n        token.\n        \"\"\"\n        if api_settings.USER_ID_CLAIM not in validated_token:\n            # The TokenUser class assumes tokens will have a recognizable user\n            # identifier claim.\n            raise InvalidToken(_('Token contained no recognizable user identification'))\n\n        return TokenUser(validated_token)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef default_cx(self):\n        px_width = self.image.px_width\n        horz_dpi = self.image.horz_dpi\n        width_in_inches = px_width / horz_dpi\n        return Inches(width_in_inches)", "response": "Default width of this image in inches."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndefaulting height of this image in pixels and the vertical dots per inch.", "response": "def default_cy(self):\n        \"\"\"\n        Native height of this image, calculated from its height in pixels and\n        vertical dots per inch (dpi).\n        \"\"\"\n        px_height = self.image.px_height\n        horz_dpi = self.image.horz_dpi\n        height_in_emu = 914400 * px_height / horz_dpi\n        return Emu(height_in_emu)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filename(self):\n        if self._image is not None:\n            return self._image.filename\n        return 'image.%s' % self.partname.ext", "response": "Returns the name of the image part from which this image part was originally created."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_image(cls, image, partname):\n        return ImagePart(partname, image.content_type, image.blob, image)", "response": "Return an |ImagePart| instance created from image and partname."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads an image part from a file.", "response": "def load(cls, partname, content_type, blob, package):\n        \"\"\"\n        Called by ``docx.opc.package.PartFactory`` to load an image part from\n        a package being opened by ``Document(...)`` call.\n        \"\"\"\n        return cls(partname, content_type, blob)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_tab_stop(self, position, alignment=WD_TAB_ALIGNMENT.LEFT,\n                     leader=WD_TAB_LEADER.SPACES):\n        \"\"\"\n        Add a new tab stop at *position*, a |Length| object specifying the\n        location of the tab stop relative to the paragraph edge. A negative\n        *position* value is valid and appears in hanging indentation. Tab\n        alignment defaults to left, but may be specified by passing a member\n        of the :ref:`WdTabAlignment` enumeration as *alignment*. An optional\n        leader character can be specified by passing a member of the\n        :ref:`WdTabLeader` enumeration as *leader*.\n        \"\"\"\n        tabs = self._pPr.get_or_add_tabs()\n        tab = tabs.insert_tab_in_order(position, alignment, leader)\n        return TabStop(tab)", "response": "Adds a new tab stop at position."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef style(self, style):\n        if style is None:\n            self._remove_rStyle()\n        elif self.rStyle is None:\n            self._add_rStyle(val=style)\n        else:\n            self.rStyle.val = style", "response": "Set val attribute of a\n            element to style."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the value of the boolean child element having name name. e. g. c1 c2 c3 c4 c5 c6 c7 c9 c9", "response": "def _get_bool_val(self, name):\n        \"\"\"\n        Return the value of the boolean child element having *name*, e.g.\n        'b', 'i', and 'smallCaps'.\n        \"\"\"\n        element = getattr(self, name)\n        if element is None:\n            return None\n        return element.val"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef val(self):\n        val = self.get(qn('w:val'))\n        underline = WD_UNDERLINE.from_xml(val)\n        if underline == WD_UNDERLINE.SINGLE:\n            return True\n        if underline == WD_UNDERLINE.NONE:\n            return False\n        return underline", "response": "Return the value of the w : val attribute."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef new(cls, package):\n        partname = package.next_partname(\"/word/footer%d.xml\")\n        content_type = CT.WML_FOOTER\n        element = parse_xml(cls._default_footer_xml())\n        return cls(partname, content_type, element, package)", "response": "Return newly created footer part."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new(cls, package):\n        partname = package.next_partname(\"/word/header%d.xml\")\n        content_type = CT.WML_HEADER\n        element = parse_xml(cls._default_header_xml())\n        return cls(partname, content_type, element, package)", "response": "Return newly created header part."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef default(cls, package):\n        partname = PackURI('/word/styles.xml')\n        content_type = CT.WML_STYLES\n        element = parse_xml(cls._default_styles_xml())\n        return cls(partname, content_type, element, package)", "response": "Return a newly created styles part containing a default set of ArcGIS elements."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _default_styles_xml(cls):\n        path = os.path.join(\n            os.path.split(__file__)[0], '..', 'templates',\n            'default-styles.xml'\n        )\n        with open(path, 'rb') as f:\n            xml_bytes = f.read()\n        return xml_bytes", "response": "Return a bytestream containing XML for a default styles part."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a CT_HdrFtrRef element of type_ with rId.", "response": "def add_footerReference(self, type_, rId):\n        \"\"\"Return newly added CT_HdrFtrRef element of *type_* with *rId*.\n\n        The element tag is `w:footerReference`.\n        \"\"\"\n        footerReference = self._add_footerReference()\n        footerReference.type_ = type_\n        footerReference.rId = rId\n        return footerReference"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_headerReference(self, type_, rId):\n        headerReference = self._add_headerReference()\n        headerReference.type_ = type_\n        headerReference.rId = rId\n        return headerReference", "response": "Add a CT_HdrFtrRef element of type_ with rId."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_footerReference(self, type_):\n        path = \"./w:footerReference[@w:type='%s']\" % WD_HEADER_FOOTER.to_xml(type_)\n        footerReferences = self.xpath(path)\n        if not footerReferences:\n            return None\n        return footerReferences[0]", "response": "Return footerReference element of type_ * or None."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_headerReference(self, type_):\n        matching_headerReferences = self.xpath(\n            \"./w:headerReference[@w:type='%s']\" % WD_HEADER_FOOTER.to_xml(type_)\n        )\n        if len(matching_headerReferences) == 0:\n            return None\n        return matching_headerReferences[0]", "response": "Return headerReference element of type_ or None."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef orientation(self):\n        pgSz = self.pgSz\n        if pgSz is None:\n            return WD_ORIENTATION.PORTRAIT\n        return pgSz.orient", "response": "Returns the value of the orientation attribute of the WD_ORIENTATION enumeration corresponding to the passed in object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_footerReference(self, type_):\n        footerReference = self.get_footerReference(type_)\n        rId = footerReference.rId\n        self.remove(footerReference)\n        return rId", "response": "Return rId of w : footerReference child of type_ * after removing it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn rId of w : headerReference child of type_ * after removing it.", "response": "def remove_headerReference(self, type_):\n        \"\"\"Return rId of w:headerReference child of *type_* after removing it.\"\"\"\n        headerReference = self.get_headerReference(type_)\n        rId = headerReference.rId\n        self.remove(headerReference)\n        return rId"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the value of the start type attribute of the WD_SECTION_START element.", "response": "def start_type(self):\n        \"\"\"\n        The member of the ``WD_SECTION_START`` enumeration corresponding to\n        the value of the ``val`` attribute of the ``<w:type>`` child element,\n        or ``WD_SECTION_START.NEW_PAGE`` if not present.\n        \"\"\"\n        type = self.type\n        if type is None or type.val is None:\n            return WD_SECTION_START.NEW_PAGE\n        return type.val"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bool_prop(self, attr_name):\n        value = getattr(self, attr_name)\n        if value is None:\n            return False\n        return value", "response": "Return the boolean value of the attribute having attr_name * or\n        |False| if not present."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef base_style(self):\n        basedOn = self.basedOn\n        if basedOn is None:\n            return None\n        styles = self.getparent()\n        base_style = styles.get_by_id(basedOn.val)\n        if base_style is None:\n            return None\n        return base_style", "response": "Returns the base style of this CT_Style element."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the next Sibling CT_Style element.", "response": "def next_style(self):\n        \"\"\"\n        Sibling CT_Style element identified by the value of `w:name/@w:val`\n        or |None| if no value is present or no style with that style id\n        is found.\n        \"\"\"\n        next = self.next\n        if next is None:\n            return None\n        styles = self.getparent()\n        return styles.get_by_id(next.val)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_style_of_type(self, name, style_type, builtin):\n        style = self.add_style()\n        style.type = style_type\n        style.customStyle = None if builtin else True\n        style.styleId = styleId_from_name(name)\n        style.name_val = name\n        return style", "response": "Return a newly added w : style element having name and style_type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef default_for(self, style_type):\n        default_styles_for_type = [\n            s for s in self._iter_styles()\n            if s.type == style_type and s.default\n        ]\n        if not default_styles_for_type:\n            return None\n        # spec calls for last default in document order\n        return default_styles_for_type[-1]", "response": "Return the default for a given style type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_break(self, break_type=WD_BREAK.LINE):\n        type_, clear = {\n            WD_BREAK.LINE:             (None,           None),\n            WD_BREAK.PAGE:             ('page',         None),\n            WD_BREAK.COLUMN:           ('column',       None),\n            WD_BREAK.LINE_CLEAR_LEFT:  ('textWrapping', 'left'),\n            WD_BREAK.LINE_CLEAR_RIGHT: ('textWrapping', 'right'),\n            WD_BREAK.LINE_CLEAR_ALL:   ('textWrapping', 'all'),\n        }[break_type]\n        br = self._r.add_br()\n        if type_ is not None:\n            br.type = type_\n        if clear is not None:\n            br.clear = clear", "response": "Add a break element of type WD_BREAK."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a picture to the end of this run.", "response": "def add_picture(self, image_path_or_stream, width=None, height=None):\n        \"\"\"\n        Return an |InlineShape| instance containing the image identified by\n        *image_path_or_stream*, added to the end of this run.\n        *image_path_or_stream* can be a path (a string) or a file-like object\n        containing a binary image. If neither width nor height is specified,\n        the picture appears at its native size. If only one is specified, it\n        is used to compute a scaling factor that is then applied to the\n        unspecified dimension, preserving the aspect ratio of the image. The\n        native size of the picture is calculated using the dots-per-inch\n        (dpi) value specified in the image file, defaulting to 72 dpi if no\n        value is specified, as is often the case.\n        \"\"\"\n        inline = self.part.new_pic_inline(image_path_or_stream, width, height)\n        self._r.add_drawing(inline)\n        return InlineShape(inline)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_text(self, text):\n        t = self._r.add_t(text)\n        return _Text(t)", "response": "Adds a text to the run containing the new\n       ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef style(self):\n        style_id = self._r.style\n        return self.part.get_style(style_id, WD_STYLE_TYPE.CHARACTER)", "response": "A |_CharacterStyle| object representing the character style applied to this run."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the value of the WdLineSpacing enumeration.", "response": "def spacing_lineRule(self):\n        \"\"\"\n        The value of `w:spacing/@w:lineRule` as a member of the\n        :ref:`WdLineSpacing` enumeration. Only the `MULTIPLE`, `EXACTLY`, and\n        `AT_LEAST` members are used. It is the responsibility of the client\n        to calculate the use of `SINGLE`, `DOUBLE`, and `MULTIPLE` based on\n        the value of `w:spacing/@w:line` if that behavior is desired.\n        \"\"\"\n        spacing = self.spacing\n        if spacing is None:\n            return None\n        lineRule = spacing.lineRule\n        if lineRule is None and spacing.line is not None:\n            return WD_LINE_SPACING.MULTIPLE\n        return lineRule"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef style(self, style):\n        if style is None:\n            self._remove_pStyle()\n            return\n        pStyle = self.get_or_add_pStyle()\n        pStyle.val = style", "response": "Set val attribute of a\n        child element to style."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef insert_tab_in_order(self, pos, align, leader):\n        new_tab = self._new_tab()\n        new_tab.pos, new_tab.val, new_tab.leader = pos, align, leader\n        for tab in self.tab_lst:\n            if new_tab.pos < tab.pos:\n                tab.addprevious(new_tab)\n                return new_tab\n        self.append(new_tab)\n        return new_tab", "response": "Insert a newly created w : tab element in * pos * order."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a new inline element populated with the values passed as parameters.", "response": "def new(cls, cx, cy, shape_id, pic):\n        \"\"\"\n        Return a new ``<wp:inline>`` element populated with the values passed\n        as parameters.\n        \"\"\"\n        inline = parse_xml(cls._inline_xml())\n        inline.extent.cx = cx\n        inline.extent.cy = cy\n        inline.docPr.id = shape_id\n        inline.docPr.name = 'Picture %d' % shape_id\n        inline.graphic.graphicData.uri = (\n            'http://schemas.openxmlformats.org/drawingml/2006/picture'\n        )\n        inline.graphic.graphicData._insert_pic(pic)\n        return inline"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new pic element containing the pic : pic element containing the pic : pic specified by the argument values.", "response": "def new_pic_inline(cls, shape_id, rId, filename, cx, cy):\n        \"\"\"\n        Return a new `wp:inline` element containing the `pic:pic` element\n        specified by the argument values.\n        \"\"\"\n        pic_id = 0  # Word doesn't seem to use this, but does not omit it\n        pic = CT_Picture.new(pic_id, filename, rId, cx, cy)\n        inline = cls.new(cx, cy, shape_id, pic)\n        inline.graphic.graphicData._insert_pic(pic)\n        return inline"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a new Pic element populated with the minimal contents required to define a viable picture element.", "response": "def new(cls, pic_id, filename, rId, cx, cy):\n        \"\"\"\n        Return a new ``<pic:pic>`` element populated with the minimal\n        contents required to define a viable picture element, based on the\n        values passed as parameters.\n        \"\"\"\n        pic = parse_xml(cls._pic_xml())\n        pic.nvPicPr.cNvPr.id = pic_id\n        pic.nvPicPr.cNvPr.name = filename\n        pic.blipFill.blip.embed = rId\n        pic.spPr.cx = cx\n        pic.spPr.cy = cy\n        return pic"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a new style object of type style_type to the document.", "response": "def add_style(self, name, style_type, builtin=False):\n        \"\"\"\n        Return a newly added style object of *style_type* and identified\n        by *name*. A builtin style can be defined by passing True for the\n        optional *builtin* argument.\n        \"\"\"\n        style_name = BabelFish.ui2internal(name)\n        if style_name in self:\n            raise ValueError(\"document already contains style '%s'\" % name)\n        style = self._element.add_style_of_type(\n            style_name, style_type, builtin\n        )\n        return StyleFactory(style)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the default style for the given style type.", "response": "def default(self, style_type):\n        \"\"\"\n        Return the default style for *style_type* or |None| if no default is\n        defined for that type (not common).\n        \"\"\"\n        style = self._element.default_for(style_type)\n        if style is None:\n            return None\n        return StyleFactory(style)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the style with the given style_id matching style_type.", "response": "def get_by_id(self, style_id, style_type):\n        \"\"\"Return the style of *style_type* matching *style_id*.\n\n        Returns the default for *style_type* if *style_id* is not found or is |None|, or\n        if the style having *style_id* is not of *style_type*.\n        \"\"\"\n        if style_id is None:\n            return self.default(style_type)\n        return self._get_by_id(style_id, style_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_style_id(self, style_or_name, style_type):\n        if style_or_name is None:\n            return None\n        elif isinstance(style_or_name, BaseStyle):\n            return self._get_style_id_from_style(style_or_name, style_type)\n        else:\n            return self._get_style_id_from_name(style_or_name, style_type)", "response": "Returns the id of the style corresponding to style_or_name or None if the style is not found."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the style of style_type matching style_id. Returns the default style for style_type.", "response": "def _get_by_id(self, style_id, style_type):\n        \"\"\"\n        Return the style of *style_type* matching *style_id*. Returns the\n        default for *style_type* if *style_id* is not found or if the style\n        having *style_id* is not of *style_type*.\n        \"\"\"\n        style = self._element.get_by_id(style_id)\n        if style is None or style.type != style_type:\n            return self.default(style_type)\n        return StyleFactory(style)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new instance from an RGB color hex string like 3C2F80.", "response": "def from_string(cls, rgb_hex_str):\n        \"\"\"\n        Return a new instance from an RGB color hex string like ``'3C2F80'``.\n        \"\"\"\n        r = int(rgb_hex_str[:2], 16)\n        g = int(rgb_hex_str[2:4], 16)\n        b = int(rgb_hex_str[4:], 16)\n        return cls(r, g, b)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iter_rels(self):\n        def walk_rels(source, visited=None):\n            visited = [] if visited is None else visited\n            for rel in source.rels.values():\n                yield rel\n                if rel.is_external:\n                    continue\n                part = rel.target_part\n                if part in visited:\n                    continue\n                visited.append(part)\n                new_source = part\n                for rel in walk_rels(new_source, visited):\n                    yield rel\n\n        for rel in walk_rels(self):\n            yield rel", "response": "Generate exactly one reference to each relationship in the package by iterating over the rels in the package."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\niterating over the parts of the package by iterating over the parts in the package.", "response": "def iter_parts(self):\n        \"\"\"\n        Generate exactly one reference to each of the parts in the package by\n        performing a depth-first traversal of the rels graph.\n        \"\"\"\n        def walk_parts(source, visited=list()):\n            for rel in source.rels.values():\n                if rel.is_external:\n                    continue\n                part = rel.target_part\n                if part in visited:\n                    continue\n                visited.append(part)\n                yield part\n                new_source = part\n                for part in walk_parts(new_source, visited):\n                    yield part\n\n        for part in walk_parts(self):\n            yield part"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_rel(self, reltype, target, rId, is_external=False):\n        return self.rels.add_relationship(reltype, target, rId, is_external)", "response": "Load a relationship between thisCOOKIE and a target with key rId."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef next_partname(self, template):\n        partnames = {part.partname for part in self.iter_parts()}\n        for n in range(1, len(partnames) + 2):\n            candidate_partname = template % n\n            if candidate_partname not in partnames:\n                return PackURI(candidate_partname)", "response": "Return a |PackURI| instance representing the next available partname matching template."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef open(cls, pkg_file):\n        pkg_reader = PackageReader.from_file(pkg_file)\n        package = cls()\n        Unmarshaller.unmarshal(pkg_reader, package, PartFactory)\n        return package", "response": "Open an OpcPackage instance from the contents of pkg_file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef relate_to(self, part, reltype):\n        rel = self.rels.get_or_add(reltype, part)\n        return rel.rId", "response": "Return rId key of relationship to part"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsave this package to a file - like object.", "response": "def save(self, pkg_file):\n        \"\"\"\n        Save this package to *pkg_file*, where *file* can be either a path to\n        a file (a string) or a file-like object.\n        \"\"\"\n        for part in self.parts:\n            part.before_marshal()\n        PackageWriter.write(pkg_file, self.rels, self.parts)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _core_properties_part(self):\n        try:\n            return self.part_related_by(RT.CORE_PROPERTIES)\n        except KeyError:\n            core_properties_part = CorePropertiesPart.default(self)\n            self.relate_to(core_properties_part, RT.CORE_PROPERTIES)\n            return core_properties_part", "response": "Creates a core properties part related to this package."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unmarshal(pkg_reader, package, part_factory):\n        parts = Unmarshaller._unmarshal_parts(\n            pkg_reader, package, part_factory\n        )\n        Unmarshaller._unmarshal_relationships(pkg_reader, package, parts)\n        for part in parts.values():\n            part.after_unmarshal()\n        package.after_unmarshal()", "response": "Unmarshalls the contents of pkg_reader into the package."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dictionary of |Part| instances unmarshalled from *pkg_reader*, keyed by partname. Side-effect is that each part in *pkg_reader* is constructed using *part_factory*.", "response": "def _unmarshal_parts(pkg_reader, package, part_factory):\n        \"\"\"\n        Return a dictionary of |Part| instances unmarshalled from\n        *pkg_reader*, keyed by partname. Side-effect is that each part in\n        *pkg_reader* is constructed using *part_factory*.\n        \"\"\"\n        parts = {}\n        for partname, content_type, reltype, blob in pkg_reader.iter_sparts():\n            parts[partname] = part_factory(\n                partname, content_type, reltype, blob, package\n            )\n        return parts"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _unmarshal_relationships(pkg_reader, package, parts):\n        for source_uri, srel in pkg_reader.iter_srels():\n            source = package if source_uri == '/' else parts[source_uri]\n            target = (srel.target_ref if srel.is_external\n                      else parts[srel.target_partname])\n            source.load_rel(srel.reltype, target, srel.rId, srel.is_external)", "response": "Unmarshalls the relationships in pkg_reader into the source object corresponding to each of the actual\n        relationships in parts."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef default(cls, package):\n        partname = PackURI('/word/settings.xml')\n        content_type = CT.WML_SETTINGS\n        element = parse_xml(cls._default_settings_xml())\n        return cls(partname, content_type, element, package)", "response": "Return a newly created settings part containing a default\n        xml."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_latent_style(self, name):\n        lsdException = self._element.add_lsdException()\n        lsdException.name = BabelFish.ui2internal(name)\n        return _LatentStyle(lsdException)", "response": "Add a latent style to the current object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef relate_to(self, target, reltype, is_external=False):\n        if is_external:\n            return self.rels.get_or_add_ext_rel(reltype, target)\n        else:\n            rel = self.rels.get_or_add(reltype, target)\n            return rel.rId", "response": "Return rId key of relationship of reltype to target."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the count of references in this part s XML to the relationship identified by rId.", "response": "def _rel_ref_count(self, rId):\n        \"\"\"\n        Return the count of references in this part's XML to the relationship\n        identified by *rId*.\n        \"\"\"\n        rIds = self._element.xpath('//@r:id')\n        return len([_rId for _rId in rIds if _rId == rId])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the custom part class registered for the given content type.", "response": "def _part_cls_for(cls, content_type):\n        \"\"\"\n        Return the custom part class registered for *content_type*, or the\n        default part class if no custom class is registered for\n        *content_type*.\n        \"\"\"\n        if content_type in cls.part_type_for:\n            return cls.part_type_for[content_type]\n        return cls.default_part_type"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a newly added <w : t > element containing text.", "response": "def add_t(self, text):\n        \"\"\"\n        Return a newly added ``<w:t>`` element containing *text*.\n        \"\"\"\n        t = self._add_t(text=text)\n        if len(text.strip()) < len(text):\n            t.set(qn('xml:space'), 'preserve')\n        return t"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a drawing element to the current CT_Drawing element.", "response": "def add_drawing(self, inline_or_anchor):\n        \"\"\"\n        Return a newly appended ``CT_Drawing`` (``<w:drawing>``) child\n        element having *inline_or_anchor* as its child.\n        \"\"\"\n        drawing = self._add_drawing()\n        drawing.append(inline_or_anchor)\n        return drawing"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clear_content(self):\n        content_child_elms = self[1:] if self.rPr is not None else self[:]\n        for child in content_child_elms:\n            self.remove(child)", "response": "Removes all child elements except the rPr element."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef append_to_run_from_text(cls, r, text):\n        appender = cls(r)\n        appender.add_text(text)", "response": "Create a one - shot appender and append the text to the run content elements corresponding to r."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_text(self, text):\n        for char in text:\n            self.add_char(char)\n        self.flush()", "response": "Append the run content elements corresponding to text to the base element of this instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_char(self, char):\n        if char == '\\t':\n            self.flush()\n            self._r.add_tab()\n        elif char in '\\r\\n':\n            self.flush()\n            self._r.add_br()\n        else:\n            self._bfr.append(char)", "response": "Add a character to the internal list of available items."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a |CorePropertiesPart| object initialized with default values for its base properties.", "response": "def default(cls, package):\n        \"\"\"\n        Return a new |CorePropertiesPart| object initialized with default\n        values for its base properties.\n        \"\"\"\n        core_properties_part = cls._new(package)\n        core_properties = core_properties_part.core_properties\n        core_properties.title = 'Word Document'\n        core_properties.last_modified_by = 'python-docx'\n        core_properties.revision = 1\n        core_properties.modified = datetime.utcnow()\n        return core_properties_part"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the value of boolean child having name.", "response": "def _get_bool_prop(self, name):\n        \"\"\"\n        Return the value of boolean child of `w:rPr` having *name*.\n        \"\"\"\n        rPr = self._element.rPr\n        if rPr is None:\n            return None\n        return rPr._get_bool_val(name)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nassigning value to the boolean child named name of the rPr.", "response": "def _set_bool_prop(self, name, value):\n        \"\"\"\n        Assign *value* to the boolean child *name* of `w:rPr`.\n        \"\"\"\n        rPr = self._element.get_or_add_rPr()\n        rPr._set_bool_val(name, value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_column(self, width):\n        tblGrid = self._tbl.tblGrid\n        gridCol = tblGrid.add_gridCol()\n        gridCol.w = width\n        for tr in self._tbl.tr_lst:\n            tc = tr.add_tc()\n            tc.width = width\n        return _Column(gridCol, self)", "response": "Return a |_Column| object of * width* newly added rightmost to the\n        table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a |_Row| instance newly added bottom - most to the table.", "response": "def add_row(self):\n        \"\"\"\n        Return a |_Row| instance, newly added bottom-most to the table.\n        \"\"\"\n        tbl = self._tbl\n        tr = tbl.add_tr()\n        for gridCol in tbl.tblGrid.gridCol_lst:\n            tc = tr.add_tc()\n            tc.width = gridCol.w\n        return _Row(tr, self)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning |_Cell| instance correponding to table cell at row_idx col_idx intersection.", "response": "def cell(self, row_idx, col_idx):\n        \"\"\"\n        Return |_Cell| instance correponding to table cell at *row_idx*,\n        *col_idx* intersection, where (0, 0) is the top, left-most cell.\n        \"\"\"\n        cell_idx = col_idx + (row_idx * self._column_count)\n        return self._cells[cell_idx]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef column_cells(self, column_idx):\n        cells = self._cells\n        idxs = range(column_idx, len(cells), self._column_count)\n        return [cells[idx] for idx in idxs]", "response": "Sequence of cells in the column at column_idx."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a sequence of cells in the row at the given index.", "response": "def row_cells(self, row_idx):\n        \"\"\"\n        Sequence of cells in the row at *row_idx* in this table.\n        \"\"\"\n        column_count = self._column_count\n        start = row_idx * column_count\n        end = start + column_count\n        return self._cells[start:end]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _cells(self):\n        col_count = self._column_count\n        cells = []\n        for tc in self._tbl.iter_tcs():\n            for grid_span_idx in range(tc.grid_span):\n                if tc.vMerge == ST_Merge.CONTINUE:\n                    cells.append(cells[-col_count])\n                elif grid_span_idx > 0:\n                    cells.append(cells[-1])\n                else:\n                    cells.append(_Cell(tc, self))\n        return cells", "response": "A sequence of |_Cell| objects one for each cell of the layout grid."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_paragraph(self, text='', style=None):\n        return super(_Cell, self).add_paragraph(text, style)", "response": "Add a paragraph to the end of the content in this cell."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_table(self, rows, cols):\n        width = self.width if self.width is not None else Inches(1)\n        table = super(_Cell, self).add_table(rows, cols, width)\n        self.add_paragraph()\n        return table", "response": "Add a table to this cell after any existing cell with rows and cols columns."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a merged cell with this and other_cell as diagonal corners. Raises |InvalidSpanError| if the rectangular region is not defined.", "response": "def merge(self, other_cell):\n        \"\"\"\n        Return a merged cell created by spanning the rectangular region\n        having this cell and *other_cell* as diagonal corners. Raises\n        |InvalidSpanError| if the cells do not define a rectangular region.\n        \"\"\"\n        tc, tc_2 = self._tc, other_cell._tc\n        merged_tc = tc.merge(tc_2)\n        return _Cell(merged_tc, self._parent)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning rId image pair for image identified by image_descriptor.", "response": "def get_or_add_image(self, image_descriptor):\n        \"\"\"Return (rId, image) pair for image identified by *image_descriptor*.\n\n        *rId* is the str key (often like \"rId7\") for the relationship between this story\n        part and the image part, reused if already present, newly created if not.\n        *image* is an |Image| instance providing access to the properties of the image,\n        such as dimensions and image type.\n        \"\"\"\n        image_part = self._package.get_or_add_image_part(image_descriptor)\n        rId = self.relate_to(image_part, RT.IMAGE)\n        return rId, image_part.image"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a newly - created w : inline element.", "response": "def new_pic_inline(self, image_descriptor, width, height):\n        \"\"\"Return a newly-created `w:inline` element.\n\n        The element contains the image specified by *image_descriptor* and is scaled\n        based on the values of *width* and *height*.\n        \"\"\"\n        rId, image = self.get_or_add_image(image_descriptor)\n        cx, cy = image.scaled_dimensions(width, height)\n        shape_id, filename = self.next_id, image.filename\n        return CT_Inline.new_pic_inline(shape_id, rId, filename, cx, cy)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the next available positive integer id value in this story XML document.", "response": "def next_id(self):\n        \"\"\"Next available positive integer id value in this story XML document.\n\n        The value is determined by incrementing the maximum existing id value. Gaps in\n        the existing id sequence are not filled. The id attribute value is unique in the\n        document, without regard to the element type it appears on.\n        \"\"\"\n        id_str_lst = self._element.xpath('//@id')\n        used_ids = [int(id_str) for id_str in id_str_lst if id_str.isdigit()]\n        if not used_ids:\n            return 1\n        return max(used_ids) + 1"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn |_Marker| or subclass instance appropriate for marker at offset.", "response": "def _MarkerFactory(marker_code, stream, offset):\n    \"\"\"\n    Return |_Marker| or subclass instance appropriate for marker at *offset*\n    in *stream* having *marker_code*.\n    \"\"\"\n    if marker_code == JPEG_MARKER_CODE.APP0:\n        marker_cls = _App0Marker\n    elif marker_code == JPEG_MARKER_CODE.APP1:\n        marker_cls = _App1Marker\n    elif marker_code in JPEG_MARKER_CODE.SOF_MARKER_CODES:\n        marker_cls = _SofMarker\n    else:\n        marker_cls = _Marker\n    return marker_cls.from_stream(stream, marker_code, offset)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an exif instance from an image stream.", "response": "def from_stream(cls, stream):\n        \"\"\"\n        Return |Exif| instance having header properties parsed from Exif\n        image in *stream*.\n        \"\"\"\n        markers = _JfifMarkers.from_stream(stream)\n        # print('\\n%s' % markers)\n\n        px_width = markers.sof.px_width\n        px_height = markers.sof.px_height\n        horz_dpi = markers.app1.horz_dpi\n        vert_dpi = markers.app1.vert_dpi\n\n        return cls(px_width, px_height, horz_dpi, vert_dpi)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new |_JfifMarkers| instance from a stream.", "response": "def from_stream(cls, stream):\n        \"\"\"\n        Return a |_JfifMarkers| instance containing a |_JfifMarker| subclass\n        instance for each marker in *stream*.\n        \"\"\"\n        marker_parser = _MarkerParser.from_stream(stream)\n        markers = []\n        for marker in marker_parser.iter_markers():\n            markers.append(marker)\n            if marker.marker_code == JPEG_MARKER_CODE.SOS:\n                break\n        return cls(markers)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef app0(self):\n        for m in self._markers:\n            if m.marker_code == JPEG_MARKER_CODE.APP0:\n                return m\n        raise KeyError('no APP0 marker in image')", "response": "Return the first APP0 marker in image markers."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef app1(self):\n        for m in self._markers:\n            if m.marker_code == JPEG_MARKER_CODE.APP1:\n                return m\n        raise KeyError('no APP1 marker in image')", "response": "Return the first APP1 marker in image markers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the first start of frame in this sequence.", "response": "def sof(self):\n        \"\"\"\n        First start of frame (SOFn) marker in this sequence.\n        \"\"\"\n        for m in self._markers:\n            if m.marker_code in JPEG_MARKER_CODE.SOF_MARKER_CODES:\n                return m\n        raise KeyError('no start of frame (SOFn) marker in image')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\niterate over the ISO - 1440 ISO - 1440 ISO - 1440 ISO - 1440 ISO - 1440 ISO - 1440 ISO - 1440 ISO - 1440 ISO - 1440 ISO - 1440 ISO - 1440 ISO - 1440 ISO - 1440 ISO - 1440 ISO - 1440 ISO - 1440 ISO - 1440 ISO - 1440 ISO - 1440 ISO - 1440 ISO - 1440 ISO - 1440 ISO - 1440 ISO - 1440.", "response": "def iter_markers(self):\n        \"\"\"\n        Generate a (marker_code, segment_offset) 2-tuple for each marker in\n        the JPEG *stream*, in the order they occur in the stream.\n        \"\"\"\n        marker_finder = _MarkerFinder.from_stream(self._stream)\n        start = 0\n        marker_code = None\n        while marker_code != JPEG_MARKER_CODE.EOI:\n            marker_code, segment_offset = marker_finder.next(start)\n            marker = _MarkerFactory(\n                marker_code, self._stream, segment_offset\n            )\n            yield marker\n            start = segment_offset + marker.segment_length"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef next(self, start):\n        position = start\n        while True:\n            # skip over any non-\\xFF bytes\n            position = self._offset_of_next_ff_byte(start=position)\n            # skip over any \\xFF padding bytes\n            position, byte_ = self._next_non_ff_byte(start=position+1)\n            # 'FF 00' sequence is not a marker, start over if found\n            if byte_ == b'\\x00':\n                continue\n            # this is a marker, gather return values and break out of scan\n            marker_code, segment_offset = byte_, position+1\n            break\n        return marker_code, segment_offset", "response": "Return the next available and available ISO - 8601 marker code and segment offset."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an offset byte 2 - tuple for the next non - FF byte in the stream starting with the offset start.", "response": "def _next_non_ff_byte(self, start):\n        \"\"\"\n        Return an offset, byte 2-tuple for the next byte in *stream* that is\n        not '\\xFF', starting with the byte at offset *start*. If the byte at\n        offset *start* is not '\\xFF', *start* and the returned *offset* will\n        be the same.\n        \"\"\"\n        self._stream.seek(start)\n        byte_ = self._read_byte()\n        while byte_ == b'\\xFF':\n            byte_ = self._read_byte()\n        offset_of_non_ff_byte = self._stream.tell() - 1\n        return offset_of_non_ff_byte, byte_"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _offset_of_next_ff_byte(self, start):\n        self._stream.seek(start)\n        byte_ = self._read_byte()\n        while byte_ != b'\\xFF':\n            byte_ = self._read_byte()\n        offset_of_ff_byte = self._stream.tell() - 1\n        return offset_of_ff_byte", "response": "Return the offset of the next '\\xFF' byte in the stream starting with start."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a |_Marker| instance from a stream.", "response": "def from_stream(cls, stream, marker_code, offset):\n        \"\"\"\n        Return a generic |_Marker| instance for the marker at *offset* in\n        *stream* having *marker_code*.\n        \"\"\"\n        if JPEG_MARKER_CODE.is_standalone(marker_code):\n            segment_length = 0\n        else:\n            segment_length = stream.read_short(offset)\n        return cls(marker_code, offset, segment_length)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _dpi(self, density):\n        if self._density_units == 1:\n            dpi = density\n        elif self._density_units == 2:\n            dpi = int(round(density * 2.54))\n        else:\n            dpi = 72\n        return dpi", "response": "Return dots per inch corresponding to density value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_stream(cls, stream, marker_code, offset):\n        # field               off  type   notes\n        # ------------------  ---  -----  -------------------\n        # segment length       0   short\n        # JFIF identifier      2   5 chr  'JFIF\\x00'\n        # major JPEG version   7   byte   typically 1\n        # minor JPEG version   8   byte   typically 1 or 2\n        # density units        9   byte   1=inches, 2=cm\n        # horz dots per unit  10   short\n        # vert dots per unit  12   short\n        # ------------------  ---  -----  -------------------\n        segment_length = stream.read_short(offset)\n        density_units = stream.read_byte(offset, 9)\n        x_density = stream.read_short(offset, 10)\n        y_density = stream.read_short(offset, 12)\n        return cls(\n            marker_code, offset, segment_length, density_units, x_density,\n            y_density\n        )", "response": "Read an APP0 marker from the given stream and return a |_App0Marker| instance."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_stream(cls, stream, marker_code, offset):\n        # field                 off  len  type   notes\n        # --------------------  ---  ---  -----  ----------------------------\n        # segment length         0    2   short\n        # Exif identifier        2    6   6 chr  'Exif\\x00\\x00'\n        # TIFF byte order        8    2   2 chr  'II'=little 'MM'=big endian\n        # meaning of universe   10    2   2 chr  '*\\x00' or '\\x00*' depending\n        # IFD0 off fr/II or MM  10   16   long   relative to ...?\n        # --------------------  ---  ---  -----  ----------------------------\n        segment_length = stream.read_short(offset)\n        if cls._is_non_Exif_APP1_segment(stream, offset):\n            return cls(marker_code, offset, segment_length, 72, 72)\n        tiff = cls._tiff_from_exif_segment(stream, offset, segment_length)\n        return cls(\n            marker_code, offset, segment_length, tiff.horz_dpi, tiff.vert_dpi\n        )", "response": "Extract the horizontal and vertical dots - per - inch values from the APP1 header at the given offset."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _is_non_Exif_APP1_segment(cls, stream, offset):\n        stream.seek(offset+2)\n        exif_signature = stream.read(6)\n        return exif_signature != b'Exif\\x00\\x00'", "response": "Return True if the APP1 segment at offset is NOT an EXIF segment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse an EXIF segment into a |Tiff| instance.", "response": "def _tiff_from_exif_segment(cls, stream, offset, segment_length):\n        \"\"\"\n        Return a |Tiff| instance parsed from the Exif APP1 segment of\n        *segment_length* at *offset* in *stream*.\n        \"\"\"\n        # wrap full segment in its own stream and feed to Tiff()\n        stream.seek(offset+8)\n        segment_bytes = stream.read(segment_length-8)\n        substream = BytesIO(segment_bytes)\n        return Tiff.from_stream(substream)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread a SOFn marker from the given stream and return a |_SofMarker| instance.", "response": "def from_stream(cls, stream, marker_code, offset):\n        \"\"\"\n        Return an |_SofMarker| instance for the SOFn marker at *offset* in\n        stream.\n        \"\"\"\n        # field                 off  type   notes\n        # ------------------  ---  -----  ----------------------------\n        # segment length       0   short\n        # Data precision       2   byte\n        # Vertical lines       3   short  px_height\n        # Horizontal lines     5   short  px_width\n        # ------------------  ---  -----  ----------------------------\n        segment_length = stream.read_short(offset)\n        px_height = stream.read_short(offset, 3)\n        px_width = stream.read_short(offset, 5)\n        return cls(marker_code, offset, segment_length, px_width, px_height)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the tc element appearing at a given index. Raises |ValueError| if no tc element appearing at that grid column. Raises |ValueError| if no tc element appearing at that grid column.", "response": "def tc_at_grid_col(self, idx):\n        \"\"\"\n        The ``<w:tc>`` element appearing at grid column *idx*. Raises\n        |ValueError| if no ``w:tc`` element begins at that grid column.\n        \"\"\"\n        grid_col = 0\n        for tc in self.tc_lst:\n            if grid_col == idx:\n                return tc\n            grid_col += tc.grid_span\n            if grid_col > idx:\n                raise ValueError('no cell on grid column %d' % idx)\n        raise ValueError('index out of bounds')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new w : tbl element having rows cols and width distributed evenly between the columns.", "response": "def new_tbl(cls, rows, cols, width):\n        \"\"\"\n        Return a new `w:tbl` element having *rows* rows and *cols* columns\n        with *width* distributed evenly between the columns.\n        \"\"\"\n        return parse_xml(cls._tbl_xml(rows, cols, width))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the value of w : tblPr. tblStyle. val to * styleId*.", "response": "def tblStyle_val(self, styleId):\n        \"\"\"\n        Set the value of `w:tblPr/w:tblStyle/@w:val` (a table style id) to\n        *styleId*. If *styleId* is None, remove the `w:tblStyle` element.\n        \"\"\"\n        tblPr = self.tblPr\n        tblPr._remove_tblStyle()\n        if styleId is None:\n            return\n        tblPr._add_tblStyle().val = styleId"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef autofit(self):\n        tblLayout = self.tblLayout\n        if tblLayout is None:\n            return True\n        return False if tblLayout.type == 'fixed' else True", "response": "Return |True| if there is a child with the type set to fixed. Otherwise return |False|."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bottom(self):\n        if self.vMerge is not None:\n            tc_below = self._tc_below\n            if tc_below is not None and tc_below.vMerge == ST_Merge.CONTINUE:\n                return tc_below.bottom\n        return self._tr_idx + 1", "response": "Returns the index of the bottom extent of the cell."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clear_content(self):\n        new_children = []\n        tcPr = self.tcPr\n        if tcPr is not None:\n            new_children.append(tcPr)\n        self[:] = new_children", "response": "Removes all content child elements preserving the block - level\n        element."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef iter_block_items(self):\n        block_item_tags = (qn('w:p'), qn('w:tbl'), qn('w:sdt'))\n        for child in self:\n            if child.tag in block_item_tags:\n                yield child", "response": "Iterate over the block - level content elements in this cell."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef merge(self, other_tc):\n        top, left, height, width = self._span_dimensions(other_tc)\n        top_tc = self._tbl.tr_lst[top].tc_at_grid_col(left)\n        top_tc._grow_to(width, height)\n        return top_tc", "response": "Return the top - left element of a new span formed by this tc element and other_tc as diagonal corners."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the top - most row index in the vertical span of this cell.", "response": "def top(self):\n        \"\"\"\n        The top-most row index in the vertical span of this cell.\n        \"\"\"\n        if self.vMerge is None or self.vMerge == ST_Merge.RESTART:\n            return self._tr_idx\n        return self._tc_above.top"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd the width of other_tc to this cell. Does nothing if self. width is not equal to other_tc.", "response": "def _add_width_of(self, other_tc):\n        \"\"\"\n        Add the width of *other_tc* to this cell. Does nothing if either this\n        tc or *other_tc* does not have a specified width.\n        \"\"\"\n        if self.width and other_tc.width:\n            self.width += other_tc.width"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _grow_to(self, width, height, top_tc=None):\n        def vMerge_val(top_tc):\n            if top_tc is not self:\n                return ST_Merge.CONTINUE\n            if height == 1:\n                return None\n            return ST_Merge.RESTART\n\n        top_tc = self if top_tc is None else top_tc\n        self._span_to_width(width, top_tc, vMerge_val(top_tc))\n        if height > 1:\n            self._tc_below._grow_to(width, height-1, top_tc)", "response": "Grow this cell to width height by expanding horizontal spans and creating continuation cells for vertical ones."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _is_empty(self):\n        block_items = list(self.iter_block_items())\n        if len(block_items) > 1:\n            return False\n        p = block_items[0]  # cell must include at least one <w:p> element\n        if len(p.r_lst) == 0:\n            return True\n        return False", "response": "Returns True if this cell contains only a single empty element."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _move_content_to(self, other_tc):\n        if other_tc is self:\n            return\n        if self._is_empty:\n            return\n        other_tc._remove_trailing_empty_p()\n        # appending moves each element from self to other_tc\n        for block_element in self.iter_block_items():\n            other_tc.append(block_element)\n        # add back the required minimum single empty <w:p> element\n        self.append(self._new_p())", "response": "Moves the content of this cell to other_tc."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _remove_trailing_empty_p(self):\n        block_items = list(self.iter_block_items())\n        last_content_elm = block_items[-1]\n        if last_content_elm.tag != qn('w:p'):\n            return\n        p = last_content_elm\n        if len(p.r_lst) > 0:\n            return\n        self.remove(p)", "response": "Remove the last empty content element from this cell if it is an empty\n        element."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _span_dimensions(self, other_tc):\n        def raise_on_inverted_L(a, b):\n            if a.top == b.top and a.bottom != b.bottom:\n                raise InvalidSpanError('requested span not rectangular')\n            if a.left == b.left and a.right != b.right:\n                raise InvalidSpanError('requested span not rectangular')\n\n        def raise_on_tee_shaped(a, b):\n            top_most, other = (a, b) if a.top < b.top else (b, a)\n            if top_most.top < other.top and top_most.bottom > other.bottom:\n                raise InvalidSpanError('requested span not rectangular')\n\n            left_most, other = (a, b) if a.left < b.left else (b, a)\n            if left_most.left < other.left and left_most.right > other.right:\n                raise InvalidSpanError('requested span not rectangular')\n\n        raise_on_inverted_L(self, other_tc)\n        raise_on_tee_shaped(self, other_tc)\n\n        top = min(self.top, other_tc.top)\n        left = min(self.left, other_tc.left)\n        bottom = max(self.bottom, other_tc.bottom)\n        right = max(self.right, other_tc.right)\n\n        return top, left, bottom - top, right - left", "response": "Return the top left height and width of the merged cell formed by using this tc and other_tc as opposite ArcGIS corner extents."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _span_to_width(self, grid_width, top_tc, vMerge):\n        self._move_content_to(top_tc)\n        while self.grid_span < grid_width:\n            self._swallow_next_tc(grid_width, top_tc)\n        self.vMerge = vMerge", "response": "Incorporate and then remove w : tc elements from the right of this oneCOOKIE."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _swallow_next_tc(self, grid_width, top_tc):\n        def raise_on_invalid_swallow(next_tc):\n            if next_tc is None:\n                raise InvalidSpanError('not enough grid columns')\n            if self.grid_span + next_tc.grid_span > grid_width:\n                raise InvalidSpanError('span is not rectangular')\n\n        next_tc = self._next_tc\n        raise_on_invalid_swallow(next_tc)\n        next_tc._move_content_to(top_tc)\n        self._add_width_of(next_tc)\n        self.grid_span += next_tc.grid_span\n        next_tc._remove()", "response": "Swallows the next <w : tc element in the row."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _tc_below(self):\n        tr_below = self._tr_below\n        if tr_below is None:\n            return None\n        return tr_below.tc_at_grid_col(self._grid_col)", "response": "The tc element immediately below this one in its grid column."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef alias(*aliases):\n    def decorator(cls):\n        # alias must be set in globals from caller's frame\n        caller = sys._getframe(1)\n        globals_dict = caller.f_globals\n        for alias in aliases:\n            globals_dict[alias] = cls\n        return cls\n    return decorator", "response": "A class decorator that adds an alias to the class to be referenced by each of the names provided as arguments."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a string representation of the enumeration page.", "response": "def page_str(self):\n        \"\"\"\n        The RestructuredText documentation page for the enumeration. This is\n        the only API member for the class.\n        \"\"\"\n        tmpl = '.. _%s:\\n\\n%s\\n\\n%s\\n\\n----\\n\\n%s'\n        components = (\n            self._ms_name, self._page_title, self._intro_text,\n            self._member_defs\n        )\n        return tmpl % components"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _intro_text(self):\n        try:\n            cls_docstring = self._clsdict['__doc__']\n        except KeyError:\n            cls_docstring = ''\n\n        if cls_docstring is None:\n            return ''\n\n        return textwrap.dedent(cls_docstring).strip()", "response": "Return the enumeration docstring formatted for documentation page."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an individual member definition formatted as an RST glossary entry.", "response": "def _member_def(self, member):\n        \"\"\"\n        Return an individual member definition formatted as an RST glossary\n        entry, wrapped to fit within 78 columns.\n        \"\"\"\n        member_docstring = textwrap.dedent(member.docstring).strip()\n        member_docstring = textwrap.fill(\n            member_docstring, width=78, initial_indent=' '*4,\n            subsequent_indent=' '*4\n        )\n        return '%s\\n%s\\n' % (member.name, member_docstring)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _member_defs(self):\n        members = self._clsdict['__members__']\n        member_defs = [\n            self._member_def(member) for member in members\n            if member.name is not None\n        ]\n        return '\\n'.join(member_defs)", "response": "A single string containing the aggregated member definitions section\n            of the documentation page\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndispatching each member of the enumeration class to each member of the enumeration class.", "response": "def _add_enum_members(meta, clsdict):\n        \"\"\"\n        Dispatch ``.add_to_enum()`` call to each member so it can do its\n        thing to properly add itself to the enumeration class. This\n        delegation allows member sub-classes to add specialized behaviors.\n        \"\"\"\n        enum_members = clsdict['__members__']\n        for member in enum_members:\n            member.add_to_enum(clsdict)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _collect_valid_settings(meta, clsdict):\n        enum_members = clsdict['__members__']\n        valid_settings = []\n        for member in enum_members:\n            valid_settings.extend(member.valid_settings)\n        clsdict['_valid_settings'] = valid_settings", "response": "Collect the valid_settings from the meta class and add them to the classdict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate(cls, value):\n        if value not in cls._valid_settings:\n            raise ValueError(\n                \"%s not a member of %s enumeration\" % (value, cls.__name__)\n            )", "response": "Validate the value of the resource."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the enumeration member corresponding to the XML value xml_val.", "response": "def from_xml(cls, xml_val):\n        \"\"\"\n        Return the enumeration member corresponding to the XML value\n        *xml_val*.\n        \"\"\"\n        if xml_val not in cls._xml_to_member:\n            raise InvalidXmlError(\n                \"attribute value '%s' not valid for this type\" % xml_val\n            )\n        return cls._xml_to_member[xml_val]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_xml(cls, enum_val):\n        if enum_val not in cls._member_to_xml:\n            raise ValueError(\n                \"value '%s' not in enumeration %s\" % (enum_val, cls.__name__)\n            )\n        return cls._member_to_xml[enum_val]", "response": "Return the XML value of the enumeration value enum_val."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a member name to the class dict clsdict containing the value of this member object.", "response": "def register_name(self, clsdict):\n        \"\"\"\n        Add a member name to the class dict *clsdict* containing the value of\n        this member object. Where the name of this object is None, do\n        nothing; this allows out-of-band values to be defined without adding\n        a name to the class dict.\n        \"\"\"\n        if self.name is None:\n            return\n        clsdict[self.name] = self.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds XML mappings to the enumeration.", "response": "def add_to_enum(self, clsdict):\n        \"\"\"\n        Compile XML mappings in addition to base add behavior.\n        \"\"\"\n        super(XmlMappedEnumMember, self).add_to_enum(clsdict)\n        self.register_xml_mapping(clsdict)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef register_xml_mapping(self, clsdict):\n        member_to_xml = self._get_or_add_member_to_xml(clsdict)\n        member_to_xml[self.value] = self.xml_value\n        xml_to_member = self._get_or_add_xml_to_member(clsdict)\n        xml_to_member[self.xml_value] = self.value", "response": "Add XML mappings to the enumeration class state for this member."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a |Document| object loaded from a. docx file or a file - like object.", "response": "def Document(docx=None):\n    \"\"\"\n    Return a |Document| object loaded from *docx*, where *docx* can be\n    either a path to a ``.docx`` file (a string) or a file-like object. If\n    *docx* is missing or ``None``, the built-in default document \"template\"\n    is loaded.\n    \"\"\"\n    docx = _default_docx_path() if docx is None else docx\n    document_part = Package.open(docx).main_document_part\n    if document_part.content_type != CT.WML_DOCUMENT_MAIN:\n        tmpl = \"file '%s' is not a Word file, content type is '%s'\"\n        raise ValueError(tmpl % (docx, document_part.content_type))\n    return document_part.document"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the path to the built - in default. docx package.", "response": "def _default_docx_path():\n    \"\"\"\n    Return the path to the built-in default .docx package.\n    \"\"\"\n    _thisdir = os.path.split(__file__)[0]\n    return os.path.join(_thisdir, 'templates', 'default-docx-template')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning |Pt| value representing the line spacing between baselines in successive lines of the paragraph.", "response": "def line_spacing(self):\n        \"\"\"\n        |float| or |Length| value specifying the space between baselines in\n        successive lines of the paragraph. A value of |None| indicates line\n        spacing is inherited from the style hierarchy. A float value, e.g.\n        ``2.0`` or ``1.75``, indicates spacing is applied in multiples of\n        line heights. A |Length| value such as ``Pt(12)`` indicates spacing\n        is a fixed height. The |Pt| value class is a convenient way to apply\n        line spacing in units of points. Assigning |None| resets line spacing\n        to inherit from the style hierarchy.\n        \"\"\"\n        pPr = self._element.pPr\n        if pPr is None:\n            return None\n        return self._line_spacing(pPr.spacing_line, pPr.spacing_lineRule)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the line spacing rule for the current entry.", "response": "def line_spacing_rule(self):\n        \"\"\"\n        A member of the :ref:`WdLineSpacing` enumeration indicating how the\n        value of :attr:`line_spacing` should be interpreted. Assigning any of\n        the :ref:`WdLineSpacing` members :attr:`SINGLE`, :attr:`DOUBLE`, or\n        :attr:`ONE_POINT_FIVE` will cause the value of :attr:`line_spacing`\n        to be updated to produce the corresponding line spacing.\n        \"\"\"\n        pPr = self._element.pPr\n        if pPr is None:\n            return None\n        return self._line_spacing_rule(\n            pPr.spacing_line, pPr.spacing_lineRule\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _line_spacing(spacing_line, spacing_lineRule):\n        if spacing_line is None:\n            return None\n        if spacing_lineRule == WD_LINE_SPACING.MULTIPLE:\n            return spacing_line / Pt(12)\n        return spacing_line", "response": "Return the line spacing value calculated from the combination of\n        spacing_line and spacing_lineRule."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the line spacing rule value calculated from the combination of the line and lineRule.", "response": "def _line_spacing_rule(line, lineRule):\n        \"\"\"\n        Return the line spacing rule value calculated from the combination of\n        *line* and *lineRule*. Returns special members of the\n        :ref:`WdLineSpacing` enumeration when line spacing is single, double,\n        or 1.5 lines.\n        \"\"\"\n        if lineRule == WD_LINE_SPACING.MULTIPLE:\n            if line == Twips(240):\n                return WD_LINE_SPACING.SINGLE\n            if line == Twips(360):\n                return WD_LINE_SPACING.ONE_POINT_FIVE\n            if line == Twips(480):\n                return WD_LINE_SPACING.DOUBLE\n        return lineRule"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef new(cls, num_id, abstractNum_id):\n        num = OxmlElement('w:num')\n        num.numId = num_id\n        abstractNumId = CT_DecimalNumber.new(\n            'w:abstractNumId', abstractNum_id\n        )\n        num.append(abstractNumId)\n        return num", "response": "Return a new num element with numId of num_id and abstractNumId of abstractNum_id."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_num(self, abstractNum_id):\n        next_num_id = self._next_numId\n        num = CT_Num.new(next_num_id, abstractNum_id)\n        return self._insert_num(num)", "response": "Return a newly added CT_Num element referencing the abstract numbering definition identified by abstractNum_id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the first child element having numId attribute matching numId*.", "response": "def num_having_numId(self, numId):\n        \"\"\"\n        Return the ``<w:num>`` child element having ``numId`` attribute\n        matching *numId*.\n        \"\"\"\n        xpath = './w:num[@w:numId=\"%d\"]' % numId\n        try:\n            return self.xpath(xpath)[0]\n        except IndexError:\n            raise KeyError('no <w:num> element with numId %d' % numId)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _next_numId(self):\n        numId_strs = self.xpath('./w:num/@w:numId')\n        num_ids = [int(numId_str) for numId_str in numId_strs]\n        for num in range(1, len(num_ids)+2):\n            if num not in num_ids:\n                break\n        return num", "response": "Returns the next unused numId in the list of available numId elements."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a relationship to the set.", "response": "def add_relationship(self, reltype, target, rId, is_external=False):\n        \"\"\"\n        Return a newly added |_Relationship| instance.\n        \"\"\"\n        rel = _Relationship(rId, reltype, target, self._baseURI, is_external)\n        self[rId] = rel\n        if not is_external:\n            self._target_parts_by_rId[rId] = target\n        return rel"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_or_add(self, reltype, target_part):\n        rel = self._get_matching(reltype, target_part)\n        if rel is None:\n            rId = self._next_rId\n            rel = self.add_relationship(reltype, target_part, rId)\n        return rel", "response": "Get or add a new not\n            relationship."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets or add an external relationship of the specified type to the collection.", "response": "def get_or_add_ext_rel(self, reltype, target_ref):\n        \"\"\"\n        Return rId of external relationship of *reltype* to *target_ref*,\n        newly added if not already present in collection.\n        \"\"\"\n        rel = self._get_matching(reltype, target_ref, is_external=True)\n        if rel is None:\n            rId = self._next_rId\n            rel = self.add_relationship(\n                reltype, target_ref, rId, is_external=True\n            )\n        return rel.rId"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nserializing this relationship collection into XML suitable for storage as a. rels file in an OPC package.", "response": "def xml(self):\n        \"\"\"\n        Serialize this relationship collection into XML suitable for storage\n        as a .rels file in an OPC package.\n        \"\"\"\n        rels_elm = CT_Relationships.new()\n        for rel in self.values():\n            rels_elm.add_rel(\n                rel.rId, rel.reltype, rel.target_ref, rel.is_external\n            )\n        return rels_elm.xml"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the relationship of matching reltype target and is_external", "response": "def _get_matching(self, reltype, target, is_external=False):\n        \"\"\"\n        Return relationship of matching *reltype*, *target*, and\n        *is_external* from collection, or None if not found.\n        \"\"\"\n        def matches(rel, reltype, target, is_external):\n            if rel.reltype != reltype:\n                return False\n            if rel.is_external != is_external:\n                return False\n            rel_target = rel.target_ref if rel.is_external else rel.target_part\n            if rel_target != target:\n                return False\n            return True\n\n        for rel in self.values():\n            if matches(rel, reltype, target, is_external):\n                return rel\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the next available rId in the collection.", "response": "def _next_rId(self):\n        \"\"\"\n        Next available rId in collection, starting from 'rId1' and making use\n        of any gaps in numbering, e.g. 'rId2' for rIds ['rId1', 'rId3'].\n        \"\"\"\n        for n in range(1, len(self)+2):\n            rId_candidate = 'rId%d' % n  # like 'rId19'\n            if rId_candidate not in self:\n                return rId_candidate"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a new OxmlElement having tagname nsptagname and val.", "response": "def new(cls, nsptagname, val):\n        \"\"\"\n        Return a new ``CT_DecimalNumber`` element having tagname *nsptagname*\n        and ``val`` attribute set to *val*.\n        \"\"\"\n        return OxmlElement(nsptagname, attrs={qn('w:val'): str(val)})"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a new OxmlElement with tagname nsptagname and val.", "response": "def new(cls, nsptagname, val):\n        \"\"\"\n        Return a new ``CT_String`` element with tagname *nsptagname* and\n        ``val`` attribute set to *val*.\n        \"\"\"\n        elm = OxmlElement(nsptagname)\n        elm.val = val\n        return elm"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a |BaseStyle| subclass according to the type of style_elm.", "response": "def StyleFactory(style_elm):\n    \"\"\"\n    Return a style object of the appropriate |BaseStyle| subclass, according\n    to the type of *style_elm*.\n    \"\"\"\n    style_cls = {\n        WD_STYLE_TYPE.PARAGRAPH: _ParagraphStyle,\n        WD_STYLE_TYPE.CHARACTER: _CharacterStyle,\n        WD_STYLE_TYPE.TABLE:     _TableStyle,\n        WD_STYLE_TYPE.LIST:      _NumberingStyle\n    }[style_elm.type]\n\n    return style_cls(style_elm)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef name(self):\n        name = self._element.name_val\n        if name is None:\n            return None\n        return BabelFish.internal2ui(name)", "response": "The name of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef type(self):\n        type = self._element.type\n        if type is None:\n            return WD_STYLE_TYPE.PARAGRAPH\n        return type", "response": "Returns the type of this style."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn |_ParagraphStyle| object representing the next paragraph style.", "response": "def next_paragraph_style(self):\n        \"\"\"\n        |_ParagraphStyle| object representing the style to be applied\n        automatically to a new paragraph inserted after a paragraph of this\n        style. Returns self if no next paragraph style is defined. Assigning\n        |None| or *self* removes the setting such that new paragraphs are\n        created using this same style.\n        \"\"\"\n        next_style_elm = self._element.next_style\n        if next_style_elm is None:\n            return self\n        if next_style_elm.type != WD_STYLE_TYPE.PARAGRAPH:\n            return self\n        return StyleFactory(next_style_elm)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef revision_number(self):\n        revision = self.revision\n        if revision is None:\n            return 0\n        revision_str = revision.text\n        try:\n            revision = int(revision_str)\n        except ValueError:\n            # non-integer revision strings also resolve to 0\n            revision = 0\n        # as do negative integers\n        if revision < 0:\n            revision = 0\n        return revision", "response": "Return the revision number of the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset revision property to string value of integer value.", "response": "def revision_number(self, value):\n        \"\"\"\n        Set revision property to string value of integer *value*.\n        \"\"\"\n        if not isinstance(value, int) or value < 1:\n            tmpl = \"revision property requires positive int, got '%s'\"\n            raise ValueError(tmpl % value)\n        revision = self.get_or_add_revision()\n        revision.text = str(value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_or_add(self, prop_name):\n        get_or_add_method_name = 'get_or_add_%s' % prop_name\n        get_or_add_method = getattr(self, get_or_add_method_name)\n        element = get_or_add_method()\n        return element", "response": "Get or add an entry from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _offset_dt(cls, dt, offset_str):\n        match = cls._offset_pattern.match(offset_str)\n        if match is None:\n            raise ValueError(\n                \"'%s' is not a valid offset string\" % offset_str\n            )\n        sign, hours_str, minutes_str = match.groups()\n        sign_factor = -1 if sign == '+' else 1\n        hours = int(hours_str) * sign_factor\n        minutes = int(minutes_str) * sign_factor\n        td = timedelta(hours=hours, minutes=minutes)\n        return dt + td", "response": "Return a |datetime| instance that is offset from datetime dt by the timezone offset specified in offset_str."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _set_element_datetime(self, prop_name, value):\n        if not isinstance(value, datetime):\n            tmpl = (\n                \"property requires <type 'datetime.datetime'> object, got %s\"\n            )\n            raise ValueError(tmpl % type(value))\n        element = self._get_or_add(prop_name)\n        dt_str = value.strftime('%Y-%m-%dT%H:%M:%SZ')\n        element.text = dt_str\n        if prop_name in ('created', 'modified'):\n            # These two require an explicit 'xsi:type=\"dcterms:W3CDTF\"'\n            # attribute. The first and last line are a hack required to add\n            # the xsi namespace to the root element rather than each child\n            # element in which it is referenced\n            self.set(qn('xsi:foo'), 'bar')\n            element.set(qn('xsi:type'), 'dcterms:W3CDTF')\n            del self.attrib[qn('xsi:foo')]", "response": "Set the date or time value of the child element having prop_name to value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _set_element_text(self, prop_name, value):\n        if not is_string(value):\n            value = str(value)\n\n        if len(value) > 255:\n            tmpl = (\n                \"exceeded 255 char limit for property, got:\\n\\n'%s'\"\n            )\n            raise ValueError(tmpl % value)\n        element = self._get_or_add(prop_name)\n        element.text = value", "response": "Set string value of name * property to value *."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the text of the element matching the given property name.", "response": "def _text_of_element(self, property_name):\n        \"\"\"\n        Return the text in the element matching *property_name*, or an empty\n        string if the element is not present or contains no text.\n        \"\"\"\n        element = getattr(self, property_name)\n        if element is None:\n            return ''\n        if element.text is None:\n            return ''\n        return element.text"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new Gif instance from a file - like object.", "response": "def from_stream(cls, stream):\n        \"\"\"\n        Return |Gif| instance having header properties parsed from GIF image\n        in *stream*.\n        \"\"\"\n        px_width, px_height = cls._dimensions_from_stream(stream)\n        return cls(px_width, px_height, 72, 72)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef blob_for(self, pack_uri):\n        path = os.path.join(self._path, pack_uri.membername)\n        with open(path, 'rb') as f:\n            blob = f.read()\n        return blob", "response": "Return contents of file corresponding to pack_uri in package\n        directory."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rels_xml_for(self, source_uri):\n        try:\n            rels_xml = self.blob_for(source_uri.rels_uri)\n        except IOError:\n            rels_xml = None\n        return rels_xml", "response": "Return rels item XML for source with source_uri."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rels_xml_for(self, source_uri):\n        try:\n            rels_xml = self.blob_for(source_uri.rels_uri)\n        except KeyError:\n            rels_xml = None\n        return rels_xml", "response": "Return rels item XML for source with source_uri."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write(self, pack_uri, blob):\n        self._zipf.writestr(pack_uri.membername, blob)", "response": "Writes the given blob to the named zip file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_stream(cls, stream):\n        stream_rdr = StreamReader(stream, LITTLE_ENDIAN)\n\n        px_width = stream_rdr.read_long(0x12)\n        px_height = stream_rdr.read_long(0x16)\n\n        horz_px_per_meter = stream_rdr.read_long(0x26)\n        vert_px_per_meter = stream_rdr.read_long(0x2A)\n\n        horz_dpi = cls._dpi(horz_px_per_meter)\n        vert_dpi = cls._dpi(vert_px_per_meter)\n\n        return cls(px_width, px_height, horz_dpi, vert_dpi)", "response": "Create a new Bmp object from a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a Clark - notation qualified tag name for a namespace prefixed tag name.", "response": "def qn(tag):\n    \"\"\"\n    Stands for \"qualified name\", a utility function to turn a namespace\n    prefixed tag name into a Clark-notation qualified tag name for lxml. For\n    example, ``qn('p:cSld')`` returns ``'{http://schemas.../main}cSld'``.\n    \"\"\"\n    prefix, tagroot = tag.split(':')\n    uri = nsmap[prefix]\n    return '{%s}%s' % (uri, tagroot)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clear_content(self):\n        for child in self[:]:\n            if child.tag == qn('w:pPr'):\n                continue\n            self.remove(child)", "response": "Removes all child elements except the <w : pPr element."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a |PackageReader| instance loaded with contents of pkg_file.", "response": "def from_file(pkg_file):\n        \"\"\"\n        Return a |PackageReader| instance loaded with contents of *pkg_file*.\n        \"\"\"\n        phys_reader = PhysPkgReader(pkg_file)\n        content_types = _ContentTypeMap.from_xml(phys_reader.content_types_xml)\n        pkg_srels = PackageReader._srels_for(phys_reader, PACKAGE_URI)\n        sparts = PackageReader._load_serialized_parts(\n            phys_reader, pkg_srels, content_types\n        )\n        phys_reader.close()\n        return PackageReader(content_types, pkg_srels, sparts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef iter_sparts(self):\n        for s in self._sparts:\n            yield (s.partname, s.content_type, s.reltype, s.blob)", "response": "Iterate over the serialized parts in the package."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iter_srels(self):\n        for srel in self._pkg_srels:\n            yield (PACKAGE_URI, srel)\n        for spart in self._sparts:\n            for srel in spart.srels:\n                yield (spart.partname, srel)", "response": "Iterate over the relationships in the package."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _srels_for(phys_reader, source_uri):\n        rels_xml = phys_reader.rels_xml_for(source_uri)\n        return _SerializedRelationships.load_from_xml(\n            source_uri.baseURI, rels_xml)", "response": "Return |_SerializedRelationships| instance populated with\n            relationships for source identified by source_uri."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a |_ContentTypeMap| instance populated with the contents of content_types_xml.", "response": "def from_xml(content_types_xml):\n        \"\"\"\n        Return a new |_ContentTypeMap| instance populated with the contents\n        of *content_types_xml*.\n        \"\"\"\n        types_elm = parse_xml(content_types_xml)\n        ct_map = _ContentTypeMap()\n        for o in types_elm.overrides:\n            ct_map._add_override(o.partname, o.content_type)\n        for d in types_elm.defaults:\n            ct_map._add_default(d.extension, d.content_type)\n        return ct_map"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef target_partname(self):\n        if self.is_external:\n            msg = ('target_partname attribute on Relationship is undefined w'\n                   'here TargetMode == \"External\"')\n            raise ValueError(msg)\n        # lazy-load _target_partname attribute\n        if not hasattr(self, '_target_partname'):\n            self._target_partname = PackURI.from_rel_ref(self._baseURI,\n                                                         self.target_ref)\n        return self._target_partname", "response": "Return |PackURI| instance containing partname targeted by this relationship."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading the _SerializedRelationships| instance from the XML representation of the item.", "response": "def load_from_xml(baseURI, rels_item_xml):\n        \"\"\"\n        Return |_SerializedRelationships| instance loaded with the\n        relationships contained in *rels_item_xml*. Returns an empty\n        collection if *rels_item_xml* is |None|.\n        \"\"\"\n        srels = _SerializedRelationships()\n        if rels_item_xml is not None:\n            rels_elm = parse_xml(rels_item_xml)\n            for rel_elm in rels_elm.Relationship_lst:\n                srels._srels.append(_SerializedRelationship(baseURI, rel_elm))\n        return srels"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_rel_ref(baseURI, relative_ref):\n        joined_uri = posixpath.join(baseURI, relative_ref)\n        abs_uri = posixpath.abspath(joined_uri)\n        return PackURI(abs_uri)", "response": "Return a |PackURI| instance containing the absolute pack URI formed by translating relative_ref onto baseURI."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ext(self):\n        # raw_ext is either empty string or starts with period, e.g. '.xml'\n        raw_ext = posixpath.splitext(self)[1]\n        return raw_ext[1:] if raw_ext.startswith('.') else raw_ext", "response": "The extension portion of this pack URI e. g. xml for\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the index of the singleton partname.", "response": "def idx(self):\n        \"\"\"\n        Return partname index as integer for tuple partname or None for\n        singleton partname, e.g. ``21`` for ``'/ppt/slides/slide21.xml'`` and\n        |None| for ``'/ppt/presentation.xml'``.\n        \"\"\"\n        filename = self.filename\n        if not filename:\n            return None\n        name_part = posixpath.splitext(filename)[0]  # filename w/ext removed\n        match = self._filename_re.match(name_part)\n        if match is None:\n            return None\n        if match.group(2):\n            return int(match.group(2))\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef relative_ref(self, baseURI):\n        # workaround for posixpath bug in 2.6, doesn't generate correct\n        # relative path when *start* (second) parameter is root ('/')\n        if baseURI == '/':\n            relpath = self[1:]\n        else:\n            relpath = posixpath.relpath(self, baseURI)\n        return relpath", "response": "Returns string containing relative reference to package item from baseURI. E. g. baseURI = baseURI. E. g. baseURI = baseURI. E. g. baseURI = baseURI. E. g. baseURI = baseURI. E. g. baseURI = baseURI. E. g. baseURI = baseURI. E. g. baseURI = baseURI."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the pack URI of the. rels part corresponding to the current pack URI.", "response": "def rels_uri(self):\n        \"\"\"\n        The pack URI of the .rels part corresponding to the current pack URI.\n        Only produces sensible output if the pack URI is a partname or the\n        package pseudo-partname '/'.\n        \"\"\"\n        rels_filename = '%s.rels' % self.filename\n        rels_uri_str = posixpath.join(self.baseURI, '_rels', rels_filename)\n        return PackURI(rels_uri_str)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef new(ext, content_type):\n        xml = '<Default xmlns=\"%s\"/>' % nsmap['ct']\n        default = parse_xml(xml)\n        default.set('Extension', ext)\n        default.set('ContentType', content_type)\n        return default", "response": "Return a new empty element with the specified extension and content type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new(partname, content_type):\n        xml = '<Override xmlns=\"%s\"/>' % nsmap['ct']\n        override = parse_xml(xml)\n        override.set('PartName', partname)\n        override.set('ContentType', content_type)\n        return override", "response": "Return a new element with the specified partname and content type set to parameter\n        values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef new(rId, reltype, target, target_mode=RTM.INTERNAL):\n        xml = '<Relationship xmlns=\"%s\"/>' % nsmap['pr']\n        relationship = parse_xml(xml)\n        relationship.set('Id', rId)\n        relationship.set('Type', reltype)\n        relationship.set('Target', target)\n        if target_mode == RTM.EXTERNAL:\n            relationship.set('TargetMode', RTM.EXTERNAL)\n        return relationship", "response": "Return a new Relationship element."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a child relationship element with attributes set according to parameter values.", "response": "def add_rel(self, rId, reltype, target, is_external=False):\n        \"\"\"\n        Add a child ``<Relationship>`` element with attributes set according\n        to parameter values.\n        \"\"\"\n        target_mode = RTM.EXTERNAL if is_external else RTM.INTERNAL\n        relationship = CT_Relationship.new(rId, reltype, target, target_mode)\n        self.append(relationship)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a child <Default > element with the specified extension and content type.", "response": "def add_default(self, ext, content_type):\n        \"\"\"\n        Add a child ``<Default>`` element with attributes set to parameter\n        values.\n        \"\"\"\n        default = CT_Default.new(ext, content_type)\n        self.append(default)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a child element with the specified partname and content type to the override list.", "response": "def add_override(self, partname, content_type):\n        \"\"\"\n        Add a child ``<Override>`` element with attributes set to parameter\n        values.\n        \"\"\"\n        override = CT_Override.new(partname, content_type)\n        self.append(override)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_paragraph(self, text='', style=None):\n        paragraph = self._add_paragraph()\n        if text:\n            paragraph.add_run(text)\n        if style is not None:\n            paragraph.style = style\n        return paragraph", "response": "Add a single paragraph to the end of the content in this Arc."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new table with rows and cols and width added to the content of this container.", "response": "def add_table(self, rows, cols, width):\n        \"\"\"\n        Return a table of *width* having *rows* rows and *cols* columns,\n        newly appended to the content in this container. *width* is evenly\n        distributed between the table columns.\n        \"\"\"\n        from .table import Table\n        tbl = CT_Tbl.new_tbl(rows, cols, width)\n        self._element._insert_tbl(tbl)\n        return Table(tbl, self)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ChunkFactory(chunk_type, stream_rdr, offset):\n    chunk_cls_map = {\n        PNG_CHUNK_TYPE.IHDR: _IHDRChunk,\n        PNG_CHUNK_TYPE.pHYs: _pHYsChunk,\n    }\n    chunk_cls = chunk_cls_map.get(chunk_type, _Chunk)\n    return chunk_cls.from_offset(chunk_type, stream_rdr, offset)", "response": "Returns a |_Chunk| subclass instance appropriate to chunk_type * parsed\n    from stream_rdr at offset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a |_Chunks| instance containing the PNG chunks in stream.", "response": "def from_stream(cls, stream):\n        \"\"\"\n        Return a |_Chunks| instance containing the PNG chunks in *stream*.\n        \"\"\"\n        chunk_parser = _ChunkParser.from_stream(stream)\n        chunks = [chunk for chunk in chunk_parser.iter_chunks()]\n        return cls(chunks)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pHYs(self):\n        match = lambda chunk: chunk.type_name == PNG_CHUNK_TYPE.pHYs  # noqa\n        return self._find_first(match)", "response": "Return the first pHYs chunk in PNG image or |None| if no pHYs chunk is present"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\niterates over the PNG chunks in this parser s internal cache.", "response": "def iter_chunks(self):\n        \"\"\"\n        Generate a |_Chunk| subclass instance for each chunk in this parser's\n        PNG stream, in the order encountered in the stream.\n        \"\"\"\n        for chunk_type, offset in self._iter_chunk_offsets():\n            chunk = _ChunkFactory(chunk_type, self._stream_rdr, offset)\n            yield chunk"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _iter_chunk_offsets(self):\n        chunk_offset = 8\n        while True:\n            chunk_data_len = self._stream_rdr.read_long(chunk_offset)\n            chunk_type = self._stream_rdr.read_str(4, chunk_offset, 4)\n            data_offset = chunk_offset + 8\n            yield chunk_type, data_offset\n            if chunk_type == 'IEND':\n                break\n            # incr offset for chunk len long, chunk type, chunk data, and CRC\n            chunk_offset += (4 + 4 + chunk_data_len + 4)", "response": "Iterate over the chunk offsets in the PNG image stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_offset(cls, chunk_type, stream_rdr, offset):\n        px_width = stream_rdr.read_long(offset)\n        px_height = stream_rdr.read_long(offset, 4)\n        return cls(chunk_type, px_width, px_height)", "response": "Return an _IHDRChunk instance containing the image dimensions and height extracted from the IHDR chunk at the given offset."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a _pHYsChunk instance from the given chunk type and offset.", "response": "def from_offset(cls, chunk_type, stream_rdr, offset):\n        \"\"\"\n        Return a _pHYsChunk instance containing the image resolution\n        extracted from the pHYs chunk in *stream* at *offset*.\n        \"\"\"\n        horz_px_per_unit = stream_rdr.read_long(offset)\n        vert_px_per_unit = stream_rdr.read_long(offset, 4)\n        units_specifier = stream_rdr.read_byte(offset, 8)\n        return cls(\n            chunk_type, horz_px_per_unit, vert_px_per_unit, units_specifier\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_byte(self, base, offset=0):\n        fmt = 'B'\n        return self._read_int(fmt, base, offset)", "response": "Read a byte from the file and return it as an integer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_long(self, base, offset=0):\n        fmt = '<L' if self._byte_order is LITTLE_ENDIAN else '>L'\n        return self._read_int(fmt, base, offset)", "response": "Reads a long value from the stream."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread a 2 byte short integer from the file and return it as a signed integer.", "response": "def read_short(self, base, offset=0):\n        \"\"\"\n        Return the int value of the two bytes at the file position determined\n        by *base* and *offset*, similarly to ``read_long()`` above.\n        \"\"\"\n        fmt = b'<H' if self._byte_order is LITTLE_ENDIAN else b'>H'\n        return self._read_int(fmt, base, offset)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_str(self, char_count, base, offset=0):\n        def str_struct(char_count):\n            format_ = '%ds' % char_count\n            return Struct(format_)\n        struct = str_struct(char_count)\n        chars = self._unpack_item(struct, base, offset)\n        unicode_str = chars.decode('UTF-8')\n        return unicode_str", "response": "Read the string from the file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload the image part collection with all the image parts in package.", "response": "def _gather_image_parts(self):\n        \"\"\"Load the image part collection with all the image parts in package.\"\"\"\n        for rel in self.iter_rels():\n            if rel.is_external:\n                continue\n            if rel.reltype != RT.IMAGE:\n                continue\n            if rel.target_part in self.image_parts:\n                continue\n            self.image_parts.append(rel.target_part)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_or_add_image_part(self, image_descriptor):\n        image = Image.from_file(image_descriptor)\n        matching_image_part = self._get_by_sha1(image.sha1)\n        if matching_image_part is not None:\n            return matching_image_part\n        return self._add_image_part(image)", "response": "Return |ImagePart| object containing image identified by image_descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds an |ImagePart| instance to the collection and return it appended.", "response": "def _add_image_part(self, image):\n        \"\"\"\n        Return an |ImagePart| instance newly created from image and appended\n        to the collection.\n        \"\"\"\n        partname = self._next_image_partname(image.ext)\n        image_part = ImagePart.from_image(image, partname)\n        self.append(image_part)\n        return image_part"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the image part with the given SHA1 hash or None if not found.", "response": "def _get_by_sha1(self, sha1):\n        \"\"\"\n        Return the image part in this collection having a SHA1 hash matching\n        *sha1*, or |None| if not found.\n        \"\"\"\n        for image_part in self._image_parts:\n            if image_part.sha1 == sha1:\n                return image_part\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_or_add_definition(self):\n        # ---note this method is called recursively to access inherited definitions---\n        # ---case-1: definition is not inherited---\n        if self._has_definition:\n            return self._definition\n        # ---case-2: definition is inherited and belongs to second-or-later section---\n        prior_headerfooter = self._prior_headerfooter\n        if prior_headerfooter:\n            return prior_headerfooter._get_or_add_definition()\n        # ---case-3: definition is inherited, but belongs to first section---\n        return self._add_definition()", "response": "Return HeaderPart or FooterPart object for this section."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _add_definition(self):\n        footer_part, rId = self._document_part.add_footer_part()\n        self._sectPr.add_footerReference(self._hdrftr_index, rId)\n        return footer_part", "response": "Return newly - added footer part."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _definition(self):\n        footerReference = self._sectPr.get_footerReference(self._hdrftr_index)\n        return self._document_part.footer_part(footerReference.rId)", "response": "Return |FooterPart| object containing content of this footer."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove footer definition associated with this section.", "response": "def _drop_definition(self):\n        \"\"\"Remove footer definition (footer part) associated with this section.\"\"\"\n        rId = self._sectPr.remove_footerReference(self._hdrftr_index)\n        self._document_part.drop_rel(rId)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntruing if a footer is defined for this section.", "response": "def _has_definition(self):\n        \"\"\"True if a footer is defined for this section.\"\"\"\n        footerReference = self._sectPr.get_footerReference(self._hdrftr_index)\n        return False if footerReference is None else True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _prior_headerfooter(self):\n        preceding_sectPr = self._sectPr.preceding_sectPr\n        return (\n            None if preceding_sectPr is None\n            else _Footer(preceding_sectPr, self._document_part, self._hdrftr_index)\n        )", "response": "|_Footer| proxy on prior sectPr element or None."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding the definition of the section to the header part.", "response": "def _add_definition(self):\n        \"\"\"Return newly-added header part.\"\"\"\n        header_part, rId = self._document_part.add_header_part()\n        self._sectPr.add_headerReference(self._hdrftr_index, rId)\n        return header_part"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _definition(self):\n        headerReference = self._sectPr.get_headerReference(self._hdrftr_index)\n        return self._document_part.header_part(headerReference.rId)", "response": "Return the definition of the header."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove header definition associated with this section.", "response": "def _drop_definition(self):\n        \"\"\"Remove header definition associated with this section.\"\"\"\n        rId = self._sectPr.remove_headerReference(self._hdrftr_index)\n        self._document_part.drop_header_part(rId)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _has_definition(self):\n        headerReference = self._sectPr.get_headerReference(self._hdrftr_index)\n        return False if headerReference is None else True", "response": "True if a header is explicitly defined for this section."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _prior_headerfooter(self):\n        preceding_sectPr = self._sectPr.preceding_sectPr\n        return (\n            None if preceding_sectPr is None\n            else _Header(preceding_sectPr, self._document_part, self._hdrftr_index)\n        )", "response": "|_Header| proxy on prior sectPr element or None."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_heading(self, text=\"\", level=1):\n        if not 0 <= level <= 9:\n            raise ValueError(\"level must be in range 0-9, got %d\" % level)\n        style = \"Title\" if level == 0 else \"Heading %d\" % level\n        return self.add_paragraph(text, style)", "response": "Return a heading paragraph newly added to the end of the document."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning newly |Paragraph| object containing only a page break.", "response": "def add_page_break(self):\n        \"\"\"Return newly |Paragraph| object containing only a page break.\"\"\"\n        paragraph = self.add_paragraph()\n        paragraph.add_run().add_break(WD_BREAK.PAGE)\n        return paragraph"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a picture shape to the document.", "response": "def add_picture(self, image_path_or_stream, width=None, height=None):\n        \"\"\"\n        Return a new picture shape added in its own paragraph at the end of\n        the document. The picture contains the image at\n        *image_path_or_stream*, scaled based on *width* and *height*. If\n        neither width nor height is specified, the picture appears at its\n        native size. If only one is specified, it is used to compute\n        a scaling factor that is then applied to the unspecified dimension,\n        preserving the aspect ratio of the image. The native size of the\n        picture is calculated using the dots-per-inch (dpi) value specified\n        in the image file, defaulting to 72 dpi if no value is specified, as\n        is often the case.\n        \"\"\"\n        run = self.add_paragraph().add_run()\n        return run.add_picture(image_path_or_stream, width, height)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a new section to the end of the document.", "response": "def add_section(self, start_type=WD_SECTION.NEW_PAGE):\n        \"\"\"\n        Return a |Section| object representing a new section added at the end\n        of the document. The optional *start_type* argument must be a member\n        of the :ref:`WdSectionStart` enumeration, and defaults to\n        ``WD_SECTION.NEW_PAGE`` if not provided.\n        \"\"\"\n        new_sectPr = self._element.body.add_section_break()\n        new_sectPr.start_type = start_type\n        return Section(new_sectPr, self._part)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a new table to the document.", "response": "def add_table(self, rows, cols, style=None):\n        \"\"\"\n        Add a table having row and column counts of *rows* and *cols*\n        respectively and table style of *style*. *style* may be a paragraph\n        style object or a paragraph style name. If *style* is |None|, the\n        table inherits the default table style of the document.\n        \"\"\"\n        table = self._body.add_table(rows, cols, self._block_width)\n        table.style = style\n        return table"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _block_width(self):\n        section = self.sections[-1]\n        return Emu(\n            section.page_width - section.left_margin - section.right_margin\n        )", "response": "Return a |Length| object specifying the width of available writing space between the margins of the last section of this document."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn |_Body| instance containing the content for this document.", "response": "def _body(self):\n        \"\"\"\n        The |_Body| instance containing the content for this document.\n        \"\"\"\n        if self.__body is None:\n            self.__body = _Body(self._element.body, self)\n        return self.__body"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nregisters cls to be constructed when the oxml parser encounters an analyzed element with matching tag.", "response": "def register_element_cls(tag, cls):\n    \"\"\"\n    Register *cls* to be constructed when the oxml parser encounters an\n    element with matching *tag*. *tag* is a string of the form\n    ``nspfx:tagroot``, e.g. ``'w:document'``.\n    \"\"\"\n    nspfx, tagroot = tag.split(':')\n    namespace = element_class_lookup.get_namespace(nsmap[nspfx])\n    namespace[tagroot] = cls"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef OxmlElement(nsptag_str, attrs=None, nsdecls=None):\n    nsptag = NamespacePrefixedTag(nsptag_str)\n    if nsdecls is None:\n        nsdecls = nsptag.nsmap\n    return oxml_parser.makeelement(\n        nsptag.clark_name, attrib=attrs, nsmap=nsdecls\n    )", "response": "Return a loose lxml element with the specified tag specified by nsptag_str."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an |_IfdEntry| subclass instance containing the value of the directory entry at offset in stream_rdr.", "response": "def _IfdEntryFactory(stream_rdr, offset):\n    \"\"\"\n    Return an |_IfdEntry| subclass instance containing the value of the\n    directory entry at *offset* in *stream_rdr*.\n    \"\"\"\n    ifd_entry_classes = {\n        TIFF_FLD.ASCII:    _AsciiIfdEntry,\n        TIFF_FLD.SHORT:    _ShortIfdEntry,\n        TIFF_FLD.LONG:     _LongIfdEntry,\n        TIFF_FLD.RATIONAL: _RationalIfdEntry,\n    }\n    field_type = stream_rdr.read_short(offset, 2)\n    if field_type in ifd_entry_classes:\n        entry_cls = ifd_entry_classes[field_type]\n    else:\n        entry_cls = _IfdEntry\n    return entry_cls.from_stream(stream_rdr, offset)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a |Tiff| instance from a file - like object.", "response": "def from_stream(cls, stream):\n        \"\"\"\n        Return a |Tiff| instance containing the properties of the TIFF image\n        in *stream*.\n        \"\"\"\n        parser = _TiffParser.parse(stream)\n\n        px_width = parser.px_width\n        px_height = parser.px_height\n        horz_dpi = parser.horz_dpi\n        vert_dpi = parser.vert_dpi\n\n        return cls(px_width, px_height, horz_dpi, vert_dpi)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the TIFF image in the given stream.", "response": "def parse(cls, stream):\n        \"\"\"\n        Return an instance of |_TiffParser| containing the properties parsed\n        from the TIFF image in *stream*.\n        \"\"\"\n        stream_rdr = cls._make_stream_reader(stream)\n        ifd0_offset = stream_rdr.read_long(4)\n        ifd_entries = _IfdEntries.from_stream(stream_rdr, ifd0_offset)\n        return cls(ifd_entries)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetects the endian of the TIFF header.", "response": "def _detect_endian(cls, stream):\n        \"\"\"\n        Return either BIG_ENDIAN or LITTLE_ENDIAN depending on the endian\n        indicator found in the TIFF *stream* header, either 'MM' or 'II'.\n        \"\"\"\n        stream.seek(0)\n        endian_str = stream.read(2)\n        return BIG_ENDIAN if endian_str == b'MM' else LITTLE_ENDIAN"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the dpi value for the given resolution tag.", "response": "def _dpi(self, resolution_tag):\n        \"\"\"\n        Return the dpi value calculated for *resolution_tag*, which can be\n        either TIFF_TAG.X_RESOLUTION or TIFF_TAG.Y_RESOLUTION. The\n        calculation is based on the values of both that tag and the\n        TIFF_TAG.RESOLUTION_UNIT tag in this parser's |_IfdEntries| instance.\n        \"\"\"\n        ifd_entries = self._ifd_entries\n\n        if resolution_tag not in ifd_entries:\n            return 72\n\n        # resolution unit defaults to inches (2)\n        resolution_unit = (\n            ifd_entries[TIFF_TAG.RESOLUTION_UNIT]\n            if TIFF_TAG.RESOLUTION_UNIT in ifd_entries else 2\n        )\n\n        if resolution_unit == 1:  # aspect ratio only\n            return 72\n        # resolution_unit == 2 for inches, 3 for centimeters\n        units_per_inch = 1 if resolution_unit == 2 else 2.54\n        dots_per_unit = ifd_entries[resolution_tag]\n        return int(round(dots_per_unit * units_per_inch))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a |StreamReader| instance with wrapping stream and having \"endian -ness determined by the MM or II indicator in the TIFF stream header.", "response": "def _make_stream_reader(cls, stream):\n        \"\"\"\n        Return a |StreamReader| instance with wrapping *stream* and having\n        \"endian-ness\" determined by the 'MM' or 'II' indicator in the TIFF\n        stream header.\n        \"\"\"\n        endian = cls._detect_endian(stream)\n        return StreamReader(stream, endian)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_stream(cls, stream, offset):\n        ifd_parser = _IfdParser(stream, offset)\n        entries = dict((e.tag, e.value) for e in ifd_parser.iter_entries())\n        return cls(entries)", "response": "Parse an IfdEntries object from a stream starting at the given offset."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef iter_entries(self):\n        for idx in range(self._entry_count):\n            dir_entry_offset = self._offset + 2 + (idx*12)\n            ifd_entry = _IfdEntryFactory(self._stream_rdr, dir_entry_offset)\n            yield ifd_entry", "response": "Iterates over the entries in the ISO."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing an entry from a stream.", "response": "def from_stream(cls, stream_rdr, offset):\n        \"\"\"\n        Return an |_IfdEntry| subclass instance containing the tag and value\n        of the tag parsed from *stream_rdr* at *offset*. Note this method is\n        common to all subclasses. Override the ``_parse_value()`` method to\n        provide distinctive behavior based on field type.\n        \"\"\"\n        tag_code = stream_rdr.read_short(offset, 0)\n        value_count = stream_rdr.read_long(offset, 4)\n        value_offset = stream_rdr.read_long(offset, 8)\n        value = cls._parse_value(\n            stream_rdr, offset, value_count, value_offset\n        )\n        return cls(tag_code, value)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a value from the stream_rdr at the given offset.", "response": "def _parse_value(cls, stream_rdr, offset, value_count, value_offset):\n        \"\"\"\n        Return the ASCII string parsed from *stream_rdr* at *value_offset*.\n        The length of the string, including a terminating '\\x00' (NUL)\n        character, is in *value_count*.\n        \"\"\"\n        return stream_rdr.read_str(value_count-1, value_offset)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the value of the entry.", "response": "def _parse_value(cls, stream_rdr, offset, value_count, value_offset):\n        \"\"\"\n        Return the short int value contained in the *value_offset* field of\n        this entry. Only supports single values at present.\n        \"\"\"\n        if value_count == 1:\n            return stream_rdr.read_short(offset, 8)\n        else:  # pragma: no cover\n            return 'Multi-value short integer NOT IMPLEMENTED'"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the value of the entry.", "response": "def _parse_value(cls, stream_rdr, offset, value_count, value_offset):\n        \"\"\"\n        Return the long int value contained in the *value_offset* field of\n        this entry. Only supports single values at present.\n        \"\"\"\n        if value_count == 1:\n            return stream_rdr.read_long(offset, 8)\n        else:  # pragma: no cover\n            return 'Multi-value long integer NOT IMPLEMENTED'"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a value from the stream.", "response": "def _parse_value(cls, stream_rdr, offset, value_count, value_offset):\n        \"\"\"\n        Return the rational (numerator / denominator) value at *value_offset*\n        in *stream_rdr* as a floating-point number. Only supports single\n        values at present.\n        \"\"\"\n        if value_count == 1:\n            numerator = stream_rdr.read_long(value_offset)\n            denominator = stream_rdr.read_long(value_offset, 4)\n            return numerator / denominator\n        else:  # pragma: no cover\n            return 'Multi-value Rational NOT IMPLEMENTED'"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns ( footer_part rId ) pair for newly - created footer part.", "response": "def add_footer_part(self):\n        \"\"\"Return (footer_part, rId) pair for newly-created footer part.\"\"\"\n        footer_part = FooterPart.new(self.package)\n        rId = self.relate_to(footer_part, RT.FOOTER)\n        return footer_part, rId"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn ( header_part rId ) pair for newly - created header part.", "response": "def add_header_part(self):\n        \"\"\"Return (header_part, rId) pair for newly-created header part.\"\"\"\n        header_part = HeaderPart.new(self.package)\n        rId = self.relate_to(header_part, RT.HEADER)\n        return header_part, rId"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef numbering_part(self):\n        try:\n            return self.part_related_by(RT.NUMBERING)\n        except KeyError:\n            numbering_part = NumberingPart.new()\n            self.relate_to(numbering_part, RT.NUMBERING)\n            return numbering_part", "response": "A |NumberingPart| object providing access to the numbering part definitions for this document."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _settings_part(self):\n        try:\n            return self.part_related_by(RT.SETTINGS)\n        except KeyError:\n            settings_part = SettingsPart.default(self.package)\n            self.relate_to(settings_part, RT.SETTINGS)\n            return settings_part", "response": "A |SettingsPart| object providing access to the document - level\n        settings for this document."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _styles_part(self):\n        try:\n            return self.part_related_by(RT.STYLES)\n        except KeyError:\n            styles_part = StylesPart.default(self.package)\n            self.relate_to(styles_part, RT.STYLES)\n            return styles_part", "response": "Returns |StylesPart| instance for this document. Creates an empty stylesPart part if one is not present."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the contents of the file at relpath relative to this file.", "response": "def text_of(relpath):\n    \"\"\"\n    Return string containing the contents of the file at *relpath* relative to\n    this file.\n    \"\"\"\n    thisdir = os.path.dirname(__file__)\n    file_path = os.path.join(thisdir, os.path.normpath(relpath))\n    with open(file_path) as f:\n        text = f.read()\n    return text"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting a physical package file containing the current naccesstoken entries to the given file.", "response": "def write(pkg_file, pkg_rels, parts):\n        \"\"\"\n        Write a physical package (.pptx file) to *pkg_file* containing\n        *pkg_rels* and *parts* and a content types stream based on the\n        content types of the parts.\n        \"\"\"\n        phys_writer = PhysPkgWriter(pkg_file)\n        PackageWriter._write_content_types_stream(phys_writer, parts)\n        PackageWriter._write_pkg_rels(phys_writer, pkg_rels)\n        PackageWriter._write_parts(phys_writer, parts)\n        phys_writer.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _write_content_types_stream(phys_writer, parts):\n        cti = _ContentTypesItem.from_parts(parts)\n        phys_writer.write(CONTENT_TYPES_URI, cti.blob)", "response": "Write the content types. xml part to the physical package with anonymization target for each part in parts."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites the blob of each part in parts to the package.", "response": "def _write_parts(phys_writer, parts):\n        \"\"\"\n        Write the blob of each part in *parts* to the package, along with a\n        rels item for its relationships if and only if it has any.\n        \"\"\"\n        for part in parts:\n            phys_writer.write(part.partname, part.blob)\n            if len(part._rels):\n                phys_writer.write(part.partname.rels_uri, part._rels.xml)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_parts(cls, parts):\n        cti = cls()\n        cti._defaults['rels'] = CT.OPC_RELATIONSHIPS\n        cti._defaults['xml'] = CT.XML\n        for part in parts:\n            cti._add_content_type(part.partname, part.content_type)\n        return cti", "response": "Create a new object from a list of parts."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add_content_type(self, partname, content_type):\n        ext = partname.ext\n        if (ext.lower(), content_type) in default_content_types:\n            self._defaults[ext] = content_type\n        else:\n            self._overrides[partname] = content_type", "response": "Add a content type for the given partname and content_type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _element(self):\n        _types_elm = CT_Types.new()\n        for ext in sorted(self._defaults.keys()):\n            _types_elm.add_default(ext, self._defaults[ext])\n        for partname in sorted(self._overrides.keys()):\n            _types_elm.add_override(partname, self._overrides[partname])\n        return _types_elm", "response": "Return the XML representation of this content types item suitable for storage as\n       . xml in an OPC package."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef given_a_run_having_mixed_text_content(context):\n    r_xml = \"\"\"\\\n        <w:r %s>\n          <w:t>abc</w:t>\n          <w:tab/>\n          <w:t>def</w:t>\n          <w:cr/>\n          <w:t>ghi</w:t>\n          <w:drawing/>\n          <w:br/>\n          <w:t>jkl</w:t>\n        </w:r>\"\"\" % nsdecls('w')\n    r = parse_xml(r_xml)\n    context.run = Run(r, None)", "response": "A run with mixed text content."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_section_break(self):\n        # ---get the sectPr at file-end, which controls last section (sections[-1])---\n        sentinel_sectPr = self.get_or_add_sectPr()\n        # ---add exact copy to new `w:p` element; that is now second-to last section---\n        self.add_p().set_sectPr(sentinel_sectPr.clone())\n        # ---remove any header or footer references from \"new\" last section---\n        for hdrftr_ref in sentinel_sectPr.xpath(\"w:headerReference|w:footerReference\"):\n            sentinel_sectPr.remove(hdrftr_ref)\n        # ---the sentinel `w:sectPr` now controls the new last section---\n        return sentinel_sectPr", "response": "Return the sentinel element for new section added at end of document."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clear_content(self):\n        if self.sectPr is not None:\n            content_elms = self[:-1]\n        else:\n            content_elms = self[:]\n        for content_elm in content_elms:\n            self.remove(content_elm)", "response": "Remove all content child elements from this element. Leave\n        the last element."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a |BaseImageHeader| subclass that knows how to parse the image in *stream*.", "response": "def _ImageHeaderFactory(stream):\n    \"\"\"\n    Return a |BaseImageHeader| subclass instance that knows how to parse the\n    headers of the image in *stream*.\n    \"\"\"\n    from docx.image import SIGNATURES\n\n    def read_32(stream):\n        stream.seek(0)\n        return stream.read(32)\n\n    header = read_32(stream)\n    for cls, offset, signature_bytes in SIGNATURES:\n        end = offset + len(signature_bytes)\n        found_bytes = header[offset:end]\n        if found_bytes == signature_bytes:\n            return cls.from_stream(stream)\n    raise UnrecognizedImageError"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing an image from a binary blob.", "response": "def from_blob(cls, blob):\n        \"\"\"\n        Return a new |Image| subclass instance parsed from the image binary\n        contained in *blob*.\n        \"\"\"\n        stream = BytesIO(blob)\n        return cls._from_stream(stream, blob)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a |Image| subclass instance loaded from the image file identified by image_descriptor.", "response": "def from_file(cls, image_descriptor):\n        \"\"\"\n        Return a new |Image| subclass instance loaded from the image file\n        identified by *image_descriptor*, a path or file-like object.\n        \"\"\"\n        if is_string(image_descriptor):\n            path = image_descriptor\n            with open(path, 'rb') as f:\n                blob = f.read()\n                stream = BytesIO(blob)\n            filename = os.path.basename(path)\n        else:\n            stream = image_descriptor\n            stream.seek(0)\n            blob = stream.read()\n            filename = None\n        return cls._from_stream(stream, blob, filename)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a tuple representing the width and height of the image.", "response": "def scaled_dimensions(self, width=None, height=None):\n        \"\"\"\n        Return a (cx, cy) 2-tuple representing the native dimensions of this\n        image scaled by applying the following rules to *width* and *height*.\n        If both *width* and *height* are specified, the return value is\n        (*width*, *height*); no scaling is performed. If only one is\n        specified, it is used to compute a scaling factor that is then\n        applied to the unspecified dimension, preserving the aspect ratio of\n        the image. If both *width* and *height* are |None|, the native\n        dimensions are returned. The native dimensions are calculated using\n        the dots-per-inch (dpi) value embedded in the image, defaulting to 72\n        dpi if no value is specified, as is often the case. The returned\n        values are both |Length| objects.\n        \"\"\"\n        if width is None and height is None:\n            return self.width, self.height\n\n        if width is None:\n            scaling_factor = float(height) / float(self.height)\n            width = round(self.width * scaling_factor)\n\n        if height is None:\n            scaling_factor = float(width) / float(self.width)\n            height = round(self.height * scaling_factor)\n\n        return Emu(width), Emu(height)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an |Image| instance from the given |Stream| object.", "response": "def _from_stream(cls, stream, blob, filename=None):\n        \"\"\"\n        Return an instance of the |Image| subclass corresponding to the\n        format of the image in *stream*.\n        \"\"\"\n        image_header = _ImageHeaderFactory(stream)\n        if filename is None:\n            filename = 'image.%s' % image_header.default_ext\n        return cls(blob, filename, image_header)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_run(self, text=None, style=None):\n        r = self._p.add_r()\n        run = Run(r, self)\n        if text:\n            run.text = text\n        if style:\n            run.style = style\n        return run", "response": "Append a run to this paragraph containing text and having character\n        style identified by style ID."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef insert_paragraph_before(self, text=None, style=None):\n        paragraph = self._insert_paragraph_before()\n        if text:\n            paragraph.add_run(text)\n        if style is not None:\n            paragraph.style = style\n        return paragraph", "response": "Insert a new paragraph before this one."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns |_ParagraphStyle| object representing the style assigned to this paragraph.", "response": "def style(self):\n        \"\"\"\n        Read/Write. |_ParagraphStyle| object representing the style assigned\n        to this paragraph. If no explicit style is assigned to this\n        paragraph, its value is the default paragraph style for the document.\n        A paragraph style name can be assigned in lieu of a paragraph style\n        object. Assigning |None| removes any applied style, making its\n        effective value the default paragraph style for the document.\n        \"\"\"\n        style_id = self._p.style\n        return self.part.get_style(style_id, WD_STYLE_TYPE.PARAGRAPH)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the text of the content of this paragraph.", "response": "def text(self):\n        \"\"\"\n        String formed by concatenating the text of each run in the paragraph.\n        Tabs and line breaks in the XML are mapped to ``\\\\t`` and ``\\\\n``\n        characters respectively.\n\n        Assigning text to this property causes all existing paragraph content\n        to be replaced with a single run containing the assigned text.\n        A ``\\\\t`` character in the text is mapped to a ``<w:tab/>`` element\n        and each ``\\\\n`` or ``\\\\r`` character is mapped to a line break.\n        Paragraph-level formatting, such as style, is preserved. All\n        run-level formatting, such as bold or italic, is removed.\n        \"\"\"\n        text = ''\n        for run in self.runs:\n            text += run.text\n        return text"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninserts a new paragraph before this one.", "response": "def _insert_paragraph_before(self):\n        \"\"\"\n        Return a newly created paragraph, inserted directly before this\n        paragraph.\n        \"\"\"\n        p = self._p.add_p_before()\n        return Paragraph(p, self._parent)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef serialize_for_reading(element):\n    xml = etree.tostring(element, encoding='unicode', pretty_print=True)\n    return XmlString(xml)", "response": "Serialize element to human - readable XML suitable for tests. No XML\n    declaration."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a sequence of attribute strings parsed from attrs.", "response": "def _attr_seq(self, attrs):\n        \"\"\"\n        Return a sequence of attribute strings parsed from *attrs*. Each\n        attribute string is stripped of whitespace on both ends.\n        \"\"\"\n        attrs = attrs.strip()\n        attr_lst = attrs.split()\n        return sorted(attr_lst)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if the element in line_2 is XML equivalent to the element in line.", "response": "def _eq_elm_strs(self, line, line_2):\n        \"\"\"\n        Return True if the element in *line_2* is XML equivalent to the\n        element in *line*.\n        \"\"\"\n        front, attrs, close, text = self._parse_line(line)\n        front_2, attrs_2, close_2, text_2 = self._parse_line(line_2)\n        if front != front_2:\n            return False\n        if self._attr_seq(attrs) != self._attr_seq(attrs_2):\n            return False\n        if close != close_2:\n            return False\n        if text != text_2:\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_line(cls, line):\n        match = cls._xml_elm_line_patt.match(line)\n        front, attrs, close, text = [match.group(n) for n in range(1, 5)]\n        return front, attrs, close, text", "response": "Parse a line of XML element - level attributes and text."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef populate_class_members(self, element_cls, prop_name):\n        self._element_cls = element_cls\n        self._prop_name = prop_name\n\n        self._add_attr_property()", "response": "Populates the internal state of the class members."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _add_attr_property(self):\n        property_ = property(self._getter, self._setter, None)\n        # assign unconditionally to overwrite element name definition\n        setattr(self._element_cls, self._prop_name, property_)", "response": "Add a read or write property to the element class that contains this attribute."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a function object suitable for the get side of the attribute property descriptor.", "response": "def _getter(self):\n        \"\"\"\n        Return a function object suitable for the \"get\" side of the attribute\n        property descriptor.\n        \"\"\"\n        def get_attr_value(obj):\n            attr_str_value = obj.get(self._clark_name)\n            if attr_str_value is None:\n                return self._default\n            return self._simple_type.from_xml(attr_str_value)\n        get_attr_value.__doc__ = self._docstring\n        return get_attr_value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _setter(self):\n        def set_attr_value(obj, value):\n            if value is None or value == self._default:\n                if self._clark_name in obj.attrib:\n                    del obj.attrib[self._clark_name]\n                return\n            str_value = self._simple_type.to_xml(value)\n            obj.set(self._clark_name, str_value)\n        return set_attr_value", "response": "Returns a function object suitable for the set side of the attribute\n            property descriptor."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _getter(self):\n        def get_attr_value(obj):\n            attr_str_value = obj.get(self._clark_name)\n            if attr_str_value is None:\n                raise InvalidXmlError(\n                    \"required '%s' attribute not present on element %s\" %\n                    (self._attr_name, obj.tag)\n                )\n            return self._simple_type.from_xml(attr_str_value)\n        get_attr_value.__doc__ = self._docstring\n        return get_attr_value", "response": "Return a function object suitable for the get side of the attribute\n            property descriptor."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a function object suitable for the set side of the attribute property descriptor.", "response": "def _setter(self):\n        \"\"\"\n        Return a function object suitable for the \"set\" side of the attribute\n        property descriptor.\n        \"\"\"\n        def set_attr_value(obj, value):\n            str_value = self._simple_type.to_xml(value)\n            obj.set(self._clark_name, str_value)\n        return set_attr_value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds an _add_x method to the element class for this childCOOKIE.", "response": "def _add_adder(self):\n        \"\"\"\n        Add an ``_add_x()`` method to the element class for this child\n        element.\n        \"\"\"\n        def _add_child(obj, **attrs):\n            new_method = getattr(obj, self._new_method_name)\n            child = new_method()\n            for key, value in attrs.items():\n                setattr(child, key, value)\n            insert_method = getattr(obj, self._insert_method_name)\n            insert_method(child)\n            return child\n\n        _add_child.__doc__ = (\n            'Add a new ``<%s>`` child element unconditionally, inserted in t'\n            'he correct sequence.' % self._nsptagname\n        )\n        self._add_to_class(self._add_method_name, _add_child)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _add_creator(self):\n        creator = self._creator\n        creator.__doc__ = (\n            'Return a \"loose\", newly created ``<%s>`` element having no attri'\n            'butes, text, or children.' % self._nsptagname\n        )\n        self._add_to_class(self._new_method_name, creator)", "response": "Add a creator method to the element class that creates\n        a new empty element of the correct type and no attributes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a read - only property to the element class for this child element.", "response": "def _add_getter(self):\n        \"\"\"\n        Add a read-only ``{prop_name}`` property to the element class for\n        this child element.\n        \"\"\"\n        property_ = property(self._getter, None, None)\n        # assign unconditionally to overwrite element name definition\n        setattr(self._element_cls, self._prop_name, property_)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_inserter(self):\n        def _insert_child(obj, child):\n            obj.insert_element_before(child, *self._successors)\n            return child\n\n        _insert_child.__doc__ = (\n            'Return the passed ``<%s>`` element after inserting it as a chil'\n            'd in the correct sequence.' % self._nsptagname\n        )\n        self._add_to_class(self._insert_method_name, _insert_child)", "response": "Add an insert_x method to the element class for this childCOOKIE."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a read - only property to the element class to retrieve a list of child elements matching this type.", "response": "def _add_list_getter(self):\n        \"\"\"\n        Add a read-only ``{prop_name}_lst`` property to the element class to\n        retrieve a list of child elements matching this type.\n        \"\"\"\n        prop_name = '%s_lst' % self._prop_name\n        property_ = property(self._list_getter, None, None)\n        setattr(self._element_cls, prop_name, property_)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_public_adder(self):\n        def add_child(obj):\n            private_add_method = getattr(obj, self._add_method_name)\n            child = private_add_method()\n            return child\n\n        add_child.__doc__ = (\n            'Add a new ``<%s>`` child element unconditionally, inserted in t'\n            'he correct sequence.' % self._nsptagname\n        )\n        self._add_to_class(self._public_add_method_name, add_child)", "response": "Add a public add_x method to the parent element class."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a method to the target class.", "response": "def _add_to_class(self, name, method):\n        \"\"\"\n        Add *method* to the target class as *name*, unless *name* is already\n        defined on the class.\n        \"\"\"\n        if hasattr(self._element_cls, name):\n            return\n        setattr(self._element_cls, name, method)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _getter(self):\n        def get_child_element(obj):\n            return obj.find(qn(self._nsptagname))\n        get_child_element.__doc__ = (\n            '``<%s>`` child element or |None| if not present.'\n            % self._nsptagname\n        )\n        return get_child_element", "response": "Return a function object suitable for the get side of the property\n        descriptor. This default getter returns the child element with the specified tag name or |None| if not present."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a function object suitable for the get side of a list property descriptor.", "response": "def _list_getter(self):\n        \"\"\"\n        Return a function object suitable for the \"get\" side of a list\n        property descriptor.\n        \"\"\"\n        def get_child_element_list(obj):\n            return obj.findall(qn(self._nsptagname))\n        get_child_element_list.__doc__ = (\n            'A list containing each of the ``<%s>`` child elements, in the o'\n            'rder they appear.' % self._nsptagname\n        )\n        return get_child_element_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npopulates the internal state of the object with the appropriate methods to the class members.", "response": "def populate_class_members(\n            self, element_cls, group_prop_name, successors):\n        \"\"\"\n        Add the appropriate methods to *element_cls*.\n        \"\"\"\n        self._element_cls = element_cls\n        self._group_prop_name = group_prop_name\n        self._successors = successors\n\n        self._add_getter()\n        self._add_creator()\n        self._add_inserter()\n        self._add_adder()\n        self._add_get_or_change_to_method()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a get_or_change_to_x method to the element class for thisCOOKIE.", "response": "def _add_get_or_change_to_method(self):\n        \"\"\"\n        Add a ``get_or_change_to_x()`` method to the element class for this\n        child element.\n        \"\"\"\n        def get_or_change_to_child(obj):\n            child = getattr(obj, self._prop_name)\n            if child is not None:\n                return child\n            remove_group_method = getattr(\n                obj, self._remove_group_method_name\n            )\n            remove_group_method()\n            add_method = getattr(obj, self._add_method_name)\n            child = add_method()\n            return child\n\n        get_or_change_to_child.__doc__ = (\n            'Return the ``<%s>`` child, replacing any other group element if'\n            ' found.'\n        ) % self._nsptagname\n        self._add_to_class(\n            self._get_or_change_to_method_name, get_or_change_to_child\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating property name from tag name e. g. a : schemeClr", "response": "def _prop_name(self):\n        \"\"\"\n        Calculate property name from tag name, e.g. a:schemeClr -> schemeClr.\n        \"\"\"\n        if ':' in self._nsptagname:\n            start = self._nsptagname.index(':') + 1\n        else:\n            start = 0\n        return self._nsptagname[start:]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef populate_class_members(self, element_cls, prop_name):\n        super(OneAndOnlyOne, self).populate_class_members(\n            element_cls, prop_name\n        )\n        self._add_getter()", "response": "Add the appropriate methods to element_cls."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef populate_class_members(self, element_cls, prop_name):\n        super(OneOrMore, self).populate_class_members(\n            element_cls, prop_name\n        )\n        self._add_list_getter()\n        self._add_creator()\n        self._add_inserter()\n        self._add_adder()\n        self._add_public_adder()\n        delattr(element_cls, prop_name)", "response": "Add the appropriate methods to element_cls."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding the appropriate methods to element_cls.", "response": "def populate_class_members(self, element_cls, prop_name):\n        \"\"\"\n        Add the appropriate methods to *element_cls*.\n        \"\"\"\n        super(ZeroOrOne, self).populate_class_members(element_cls, prop_name)\n        self._add_getter()\n        self._add_creator()\n        self._add_inserter()\n        self._add_adder()\n        self._add_get_or_adder()\n        self._add_remover()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a get_or_add_x method to the element class for thisCOOKIE.", "response": "def _add_get_or_adder(self):\n        \"\"\"\n        Add a ``get_or_add_x()`` method to the element class for this\n        child element.\n        \"\"\"\n        def get_or_add_child(obj):\n            child = getattr(obj, self._prop_name)\n            if child is None:\n                add_method = getattr(obj, self._add_method_name)\n                child = add_method()\n            return child\n        get_or_add_child.__doc__ = (\n            'Return the ``<%s>`` child element, newly added if not present.'\n        ) % self._nsptagname\n        self._add_to_class(self._get_or_add_method_name, get_or_add_child)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _add_remover(self):\n        def _remove_child(obj):\n            obj.remove_all(self._nsptagname)\n        _remove_child.__doc__ = (\n            'Remove all ``<%s>`` child elements.'\n        ) % self._nsptagname\n        self._add_to_class(self._remove_method_name, _remove_child)", "response": "Add a remove_all method to the element class for this childCOOKIE."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding the appropriate methods to element_cls.", "response": "def populate_class_members(self, element_cls, prop_name):\n        \"\"\"\n        Add the appropriate methods to *element_cls*.\n        \"\"\"\n        super(ZeroOrOneChoice, self).populate_class_members(\n            element_cls, prop_name\n        )\n        self._add_choice_getter()\n        for choice in self._choices:\n            choice.populate_class_members(\n                element_cls, self._prop_name, self._successors\n            )\n        self._add_group_remover()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _add_choice_getter(self):\n        property_ = property(self._choice_getter, None, None)\n        # assign unconditionally to overwrite element name definition\n        setattr(self._element_cls, self._prop_name, property_)", "response": "Add a read - only getter property to the element class that returns the present member of this group or None if none are\n        present."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a remove_eg_x method to the element class for this choice group.", "response": "def _add_group_remover(self):\n        \"\"\"\n        Add a ``_remove_eg_x()`` method to the element class for this choice\n        group.\n        \"\"\"\n        def _remove_choice_group(obj):\n            for tagname in self._member_nsptagnames:\n                obj.remove_all(tagname)\n\n        _remove_choice_group.__doc__ = (\n            'Remove the current choice group child element if present.'\n        )\n        self._add_to_class(\n            self._remove_choice_group_method_name, _remove_choice_group\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _choice_getter(self):\n        def get_group_member_element(obj):\n            return obj.first_child_found_in(*self._member_nsptagnames)\n        get_group_member_element.__doc__ = (\n            'Return the child element belonging to this element group, or '\n            '|None| if no member child is present.'\n        )\n        return get_group_member_element", "response": "Return a function object suitable for the get side of the property\n            descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef first_child_found_in(self, *tagnames):\n        for tagname in tagnames:\n            child = self.find(qn(tagname))\n            if child is not None:\n                return child\n        return None", "response": "Return the first child found with tag in tagnames."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves all child elements whose tagname appears in tagnames*.", "response": "def remove_all(self, *tagnames):\n        \"\"\"\n        Remove all child elements whose tagname (e.g. 'a:p') appears in\n        *tagnames*.\n        \"\"\"\n        for tagname in tagnames:\n            matching = self.findall(qn(tagname))\n            for child in matching:\n                self.remove(child)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\noverrides of lxml. _Element. xpath method to provide standard Open XML namespace mapping.", "response": "def xpath(self, xpath_str):\n        \"\"\"\n        Override of ``lxml`` _Element.xpath() method to provide standard Open\n        XML namespace mapping (``nsmap``) in centralized location.\n        \"\"\"\n        return super(BaseOxmlElement, self).xpath(\n            xpath_str, namespaces=nsmap\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the value of the RGB color property of the current locale.", "response": "def rgb(self):\n        \"\"\"\n        An |RGBColor| value or |None| if no RGB color is specified.\n\n        When :attr:`type` is `MSO_COLOR_TYPE.RGB`, the value of this property\n        will always be an |RGBColor| value. It may also be an |RGBColor|\n        value if :attr:`type` is `MSO_COLOR_TYPE.THEME`, as Word writes the\n        current value of a theme color when one is assigned. In that case,\n        the RGB value should be interpreted as no more than a good guess\n        however, as the theme color takes precedence at rendering time. Its\n        value is |None| whenever :attr:`type` is either |None| or\n        `MSO_COLOR_TYPE.AUTO`.\n\n        Assigning an |RGBColor| value causes :attr:`type` to become\n        `MSO_COLOR_TYPE.RGB` and any theme color is removed. Assigning |None|\n        causes any color to be removed such that the effective color is\n        inherited from the style hierarchy.\n        \"\"\"\n        color = self._color\n        if color is None:\n            return None\n        if color.val == ST_HexColorAuto.AUTO:\n            return None\n        return color.val"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the value of the themeColor property of the current locale.", "response": "def theme_color(self):\n        \"\"\"\n        A member of :ref:`MsoThemeColorIndex` or |None| if no theme color is\n        specified. When :attr:`type` is `MSO_COLOR_TYPE.THEME`, the value of\n        this property will always be a member of :ref:`MsoThemeColorIndex`.\n        When :attr:`type` has any other value, the value of this property is\n        |None|.\n\n        Assigning a member of :ref:`MsoThemeColorIndex` causes :attr:`type`\n        to become `MSO_COLOR_TYPE.THEME`. Any existing RGB value is retained\n        but ignored by Word. Assigning |None| causes any color specification\n        to be removed such that the effective color is inherited from the\n        style hierarchy.\n        \"\"\"\n        color = self._color\n        if color is None or color.themeColor is None:\n            return None\n        return color.themeColor"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads - only. A member of MsoColorType one of RGB THEME or MSO_COLOR_TYPE. AUTO or MSO_COLOR_TYPE. RGB or MSO_COLOR_TYPE. THEME or MSO_COLOR_TYPE. AUTO.", "response": "def type(self):\n        \"\"\"\n        Read-only. A member of :ref:`MsoColorType`, one of RGB, THEME, or\n        AUTO, corresponding to the way this color is defined. Its value is\n        |None| if no color is applied at this level, which causes the\n        effective color to be inherited from the style hierarchy.\n        \"\"\"\n        color = self._color\n        if color is None:\n            return None\n        if color.themeColor is not None:\n            return MSO_COLOR_TYPE.THEME\n        if color.val == ST_HexColorAuto.AUTO:\n            return MSO_COLOR_TYPE.AUTO\n        return MSO_COLOR_TYPE.RGB"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the type of this inline shape as a member of WD_INLINE_SHAPE.", "response": "def type(self):\n        \"\"\"\n        The type of this inline shape as a member of\n        ``docx.enum.shape.WD_INLINE_SHAPE``, e.g. ``LINKED_PICTURE``.\n        Read-only.\n        \"\"\"\n        graphicData = self._inline.graphic.graphicData\n        uri = graphicData.uri\n        if uri == nsmap['pic']:\n            blip = graphicData.pic.blipFill.blip\n            if blip.link is not None:\n                return WD_INLINE_SHAPE.LINKED_PICTURE\n            return WD_INLINE_SHAPE.PICTURE\n        if uri == nsmap['c']:\n            return WD_INLINE_SHAPE.CHART\n        if uri == nsmap['dgm']:\n            return WD_INLINE_SHAPE.SMART_ART\n        return WD_INLINE_SHAPE.NOT_IMPLEMENTED"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract the data from a Plotly Figure and returns a DataFrame or list of DataFrame objects.", "response": "def to_df(figure):\n    \"\"\"\n    Extracts the data from a Plotly Figure\n\n    Parameters\n    ----------\n        figure : plotly_figure\n            Figure from which data will be \n            extracted\n        \n        Returns a DataFrame or list of DataFrame\n    \"\"\"\n    dfs=[]\n    for trace in figure['data']:\n        if 'scatter' in trace['type']:\n            try:\n                if type(trace['x'][0])==float:\n                    index=trace['x']\n                else:\n                    index=pd.to_datetime(trace['x'])\n            except:\n                index=trace['x']\n            if 'marker' in trace:\n                d={}\n                if 'size' in trace['marker']:\n                    size=trace['marker']['size']\n                    if type(size)!=list:\n                        size=[size]*len(index)\n                    d['size']=size\n                if 'text' in trace:\n                    d['text']=trace['text']\n                if 'name' in trace:\n                    name=trace['name']\n                    if type(name)!=list:\n                        name=[name]*len(index)\n                    d['categories']=name\n                d['y']=trace['y']\n                d['x']=trace['x']\n                if 'z' in trace:\n                    d['z']=trace['z']\n                df=pd.DataFrame(d)\n            else:\n                df=pd.Series(trace['y'],index=index,name=trace['name'])\n            dfs.append(df)\n        elif trace['type'] in ('heatmap','surface'):\n            df=pd.DataFrame(trace['z'].transpose(),index=trace['x'],columns=trace['y'])\n            dfs.append(df)\n        elif trace['type'] in ('box','histogram'):\n            vals=trace['x'] if 'x' in trace else trace['y']\n            df=pd.DataFrame({trace['name']:vals})\n            dfs.append(df)\n    if max(list(map(len,dfs)))==min(list(map(len,dfs))):\n        if len(dfs)==1:\n            return dfs[0]\n        else:\n            if type(dfs[0])==pd.core.series.Series:\n                return pd.concat(dfs,axis=1)                \n            if all(dfs[0].columns==dfs[1].columns):\n                    return pd.concat(dfs,axis=0)\n            else:\n                return pd.concat(dfs,axis=1)\n    else:\n        try:\n            return pd.concat(dfs)\n        except:\n            return dfs"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts from hex or rgb to rgba", "response": "def to_rgba(color, alpha):\n    \"\"\"\n    Converts from hex|rgb to rgba\n\n    Parameters:\n    -----------\n            color : string\n                    Color representation on hex or rgb\n            alpha : float\n                    Value from 0 to 1.0 that represents the \n                    alpha value.\n\n    Example:\n            to_rgba('#E1E5ED',0.6)\n            to_rgba('#f03',0.7)\n            to_rgba('rgb(23,23,23)',.5)\n    \"\"\"\n    if type(color) == tuple:\n        color, alpha = color\n    color = color.lower()\n    if 'rgba' in color:\n        cl = list(eval(color.replace('rgba', '')))\n        if alpha:\n            cl[3] = alpha\n        return 'rgba' + str(tuple(cl))\n    elif 'rgb' in color:\n        r, g, b = eval(color.replace('rgb', ''))\n        return 'rgba' + str((r, g, b, alpha))\n    else:\n        return to_rgba(hex_to_rgb(color), alpha)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts from hex to rgb", "response": "def hex_to_rgb(color):\n    \"\"\"\n    Converts from hex to rgb\n\n    Parameters:\n    -----------\n            color : string\n                    Color representation on hex or rgb\n\n    Example:\n            hex_to_rgb('#E1E5ED')\n            hex_to_rgb('#f03')\n    \"\"\"\n    color = normalize(color)\n    color = color[1:]\n    # return 'rgb'+str(tuple(ord(c) for c in color.decode('hex')))\n    return 'rgb' + str((int(color[0:2], base=16), int(color[2:4], base=16), int(color[4:6], base=16)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nnormalize a string in rgba or rgb format", "response": "def normalize(color):\n    \"\"\"\n    Returns an hex color\n\n    Parameters:\n    -----------\n            color : string\n                    Color representation in rgba|rgb|hex\n\n    Example:\n            normalize('#f03')\n    \"\"\"\n    if type(color) == tuple:\n        color = to_rgba(*color)\n    if 'rgba' in color:\n        return rgb_to_hex(rgba_to_rgb(color))\n    elif 'rgb' in color:\n        return rgb_to_hex(color)\n    elif '#' in color:\n        if len(color) == 7:\n            return color\n        else:\n            color = color[1:]\n            return '#' + ''.join([x * 2 for x in list(color)])\n    else:\n        try:\n            return normalize(cnames[color.lower()])\n        except:\n            raise CufflinksError('Not a valid color: ' + color)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rgb_to_hex(color):\n    rgb = eval(color.replace('rgb', ''))\n    # return '#'+''.join(map(chr, rgb)).encode('hex')\n    return '#' + ''.join(['{0:02x}'.format(x).upper() for x in rgb])", "response": "Converts from rgb to hex"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rgba_to_rgb(color, bg='rgb(255,255,255)'):\n    def c_tup(c):\n        return eval(c[c.find('('):])\n    color = c_tup(color)\n    bg = hex_to_rgb(normalize(bg))\n    bg = c_tup(bg)\n    a = color[3]\n    r = [int((1 - a) * bg[i] + a * color[i]) for i in range(3)]\n    return 'rgb' + str(tuple(r))", "response": "Converts from rgba to rgb"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts from hex to hsv", "response": "def hex_to_hsv(color):\n    \"\"\"\n    Converts from hex to hsv\n\n    Parameters:\n    -----------\n            color : string\n                    Color representation on color\n\n    Example:\n            hex_to_hsv('#ff9933')\n    \"\"\"\n    color = normalize(color)\n    color = color[1:]\n    # color=tuple(ord(c)/255.0 for c in color.decode('hex'))\n    color = (int(color[0:2], base=16) / 255.0, int(color[2:4],\n                                                   base=16) / 255.0, int(color[4:6], base=16) / 255.0)\n    return colorsys.rgb_to_hsv(*color)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a scale of colours from a base colour.", "response": "def color_range(color, N=20):\n    \"\"\"\n    Generates a scale of colours from a base colour\n\n    Parameters:\n    -----------\n            color : string\n                    Color representation in hex\n            N   : int\n                    number of colours to generate\n\n    Example:\n            color_range('#ff9933',20)\n    \"\"\"\n    color = normalize(color)\n    org = color\n    color = hex_to_hsv(color)\n    HSV_tuples = [(color[0], x, color[2]) for x in np.arange(0, 1, 2.0 / N)]\n    HSV_tuples.extend([(color[0], color[1], x)\n                       for x in np.arange(0, 1, 2.0 / N)])\n    hex_out = []\n    for c in HSV_tuples:\n        c = colorsys.hsv_to_rgb(*c)\n        c = [int(_ * 255) for _ in c]\n        # hex_out.append(\"#\"+\"\".join([chr(x).encode('hex') for x in c]))\n        hex_out.append(\"#\" + \"\".join(['{0:02x}'.format(x) for x in c]))\n    if org not in hex_out:\n        hex_out.append(org)\n    hex_out.sort()\n    return hex_out"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef color_table(color, N=1, sort=False, sort_values=False, inline=False, as_html=False):\n    if isinstance(color, list):\n        c_ = ''\n        rgb_tup = [normalize(c) for c in color]\n        if sort:\n            rgb_tup.sort()\n    elif isinstance(color, dict):\n        c_ = ''\n        items = [(k, normalize(v), hex_to_hsv(normalize(v)))\n                 for k, v in list(color.items())]\n        if sort_values:\n            items = sorted(items, key=operator.itemgetter(2))\n        elif sort:\n            items = sorted(items, key=operator.itemgetter(0))\n        rgb_tup = [(k, v) for k, v, _ in items]\n    else:\n        c_ = normalize(color)\n        if N > 1:\n            rgb_tup = np.array(color_range(c_, N))[::-1]\n        else:\n            rgb_tup = [c_]\n\n    def _color(c):\n        if hex_to_hsv(c)[2] < .5:\n            color = \"#ffffff\"\n            shadow = '0 1px 0 #000'\n        else:\n            color = \"#000000\"\n            shadow = '0 1px 0 rgba(255,255,255,0.6)'\n        if c == c_:\n            border = \" border: 1px solid #ffffff;\"\n        else:\n            border = ''\n        return color, shadow, border\n\n    s = '<ul style=\"list-style-type: none;\">' if not inline else ''\n    for c in rgb_tup:\n        if isinstance(c, tuple):\n            k, c = c\n            k += ' : '\n        else:\n            k = ''\n        if inline:\n            s += '<div style=\"background-color:{0};height:20px;width:20px;display:inline-block;\"></div>'.format(\n                c)\n        else:\n            color, shadow, border = _color(c)\n            s += \"\"\"<li style=\"text-align:center;\"\"\" + border + \"\"\"line-height:30px;background-color:\"\"\" + c + \"\"\";\"> \n            <span style=\" text-shadow:\"\"\" + shadow + \"\"\"; color:\"\"\" + color + \"\"\";\">\"\"\" + k + c.upper() + \"\"\"</span>\n            </li>\"\"\"\n    s += '</ul>' if not inline else ''\n    if as_html:\n        return s\n    return display(HTML(s))", "response": "Generates a colour table for a single or multi - line color."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a generator that yields a list of colors and gradients of those colors.", "response": "def colorgen(colors=None, n=None, scale=None, theme=None):\n    \"\"\"\n    Returns a generator with a list of colors\n    and gradients of those colors\n\n    Parameters:\n    -----------\n            colors : list(colors)\n                    List of colors to use\n\n    Example:\n            colorgen()\n            colorgen(['blue','red','pink'])\n            colorgen(['#f03','rgb(23,25,25)'])\n    \"\"\"\n    from .themes import THEMES\n    step = .1\n    if not colors:\n        if not scale:\n            if not theme:\n                scale = get_config_file()['colorscale']\n            else:\n                scale = THEMES[theme]['colorscale']\n        colors = get_scales(scale)\n    dq = deque(colors)\n    if len(dq) == 0:\n        dq = deque(get_scales('ggplot'))\n    if n:\n        step = len(dq) * 0.8 / n if len(dq) * 8 < n else .1\n    for i in np.arange(.2, 1, step):\n        for y in dq:\n            yield to_rgba(y, 1 - i + .2)\n        dq.rotate(1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndisplay a list of available color scales for the current locale.", "response": "def scales(scale=None):\n    \"\"\"\n    Displays a color scale (HTML)\n\n    Parameters:\n    -----------\n            scale : str\n                    Color scale name\n                    If no scale name is provided then all scales are returned\n                            (max number for each scale)\n                    If scale='all' then all scale combinations available \n                            will be returned\n\n    Example:\n            scales('accent')\n            scales('all')\n            scales()\n    \"\"\"\n    if scale:\n        if scale == 'all':\n            display(HTML(cl.to_html(_scales)))\n        else:\n            display(HTML(cl.to_html(get_scales(scale))))\n    else:\n        s = ''\n        keys = list(_scales_names.keys())\n        keys.sort()\n        for k in keys:\n            scale = get_scales(k)\n            s += '<div style=\"display:inline-block;padding:10px;\"><div>{0}</div>{1}</div>'.format(\n                k, cl.to_html(scale))\n        display(HTML(s))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of colors available for a given scale.", "response": "def get_scales(scale=None, n=None):\n    \"\"\"\n    Returns a color scale \n\n    Parameters:\n    -----------\n            scale : str\n                    Color scale name\n                    If the color name is preceded by a minus (-) \n                    then the scale is inversed\n            n : int\n                    Number of colors \n                    If n < number of colors available for a given scale then \n                            the minimum number will be returned \n                    If n > number of colors available for a given scale then\n                            the maximum number will be returned \n\n    Example:\n            get_scales('accent',8)\n            get_scales('pastel1')\n    \"\"\"\n    if scale:\n        is_reverse = False\n        if scale[0] == '-':\n            scale = scale[1:]\n            is_reverse = True\n        d = copy.deepcopy(_scales_names[scale.lower()])\n        keys = list(map(int, list(d.keys())))\n        cs = None\n        if n:\n            if n in keys:\n                cs = d[str(n)]\n            elif n < min(keys):\n                cs = d[str(min(keys))]\n        if cs is None:\n            cs = d[str(max(keys))]\n        if is_reverse:\n            cs.reverse()\n        return cs\n    else:\n        d = {}\n        for k, v in list(_scales_names.items()):\n            if isinstance(v, dict):\n                keys = list(map(int, list(v.keys())))\n                d[k] = v[str(max(keys))]\n            else:\n                d[k] = v\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a color scale to be used for a plotly figure.", "response": "def get_colorscale(scale):\n    \"\"\"\n    Returns a color scale to be used for a plotly figure\n\n    Parameters:\n    -----------\n            scale : str or list\n                    Color scale name\n                    If the color name is preceded by a minus (-) \n                    then the scale is inversed.\n                    Also accepts a list of colors (rgb,rgba,hex)\n\n    Example:\n            get_colorscale('accent')\n            get_colorscale(['rgb(127,201,127)','rgb(190,174,212)','rgb(253,192,134)'])\n    \"\"\"\n\n    if type(scale) in string_types:\n        scale = get_scales(scale)\n    else:\n        if type(scale) != list:\n            raise Exception(\n                \"scale needs to be either a scale name or list of colors\")\n\n    cs = [[1.0 * c / (len(scale) - 1), scale[c]] for c in range(len(scale))]\n    cs.sort()\n    return cs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _to_iplot(self,colors=None,colorscale=None,kind='scatter',mode='lines',interpolation='linear',symbol='dot',size='12',fill=False,\n\t\twidth=3,dash='solid',sortbars=False,keys=False,bestfit=False,bestfit_colors=None,opacity=0.6,\n\t\tmean=False,mean_colors=None,asDates=False,asTimestamp=False,text=None,**kwargs):\n\t\"\"\"\n\tGenerates a plotly Data object \n\n\tParameters\n\t----------\n\t\tcolors : list or dict\n\t\t\t{key:color} to specify the color for each column\n\t\t\t[colors] to use the colors in the defined order\n\t\tcolorscale : str \n\t\t\tColor scale name\n\t\t\tOnly valid if 'colors' is null\n\t\t\tSee cufflinks.colors.scales() for available scales\n\t\tkind : string\n\t\t\tKind of chart\n\t\t\t\tscatter\n\t\t\t\tbar\n\t\tmode : string\n\t\t\tPlotting mode for scatter trace\n\t\t\t\tlines\n\t\t\t\tmarkers\n\t\t\t\tlines+markers\n\t\t\t\tlines+text\n\t\t\t\tmarkers+text\n\t\t\t\tlines+markers+text\n\t\tinterpolation : string\n\t\t\tPositioning of the connecting lines\n\t\t\t\tlinear\n\t\t\t\tspline\n\t\t\t\tvhv\n\t\t\t\thvh\n\t\t\t\tvh\n\t\t\t\thv\t\n\t\tsymbol : string\n\t\t\tThe symbol that is drawn on the plot for each marker\n\t\t\tValid only when mode includes markers\n\t\t\t\tcircle\n\t\t\t\tcircle-dot\n\t\t\t\tdiamond\n\t\t\t\tsquare\n\t\t\t\tand many more...(see plotly.validators.scatter.marker.SymbolValidator.values)\n\t\tsize : string or int \n\t\t\tSize of marker \n\t\t\tValid only if marker in mode\n\t\tfill : bool\n\t\t\tFilled Traces\n\t\twidth : int\n\t\t\tLine width\n\t\tdash : string\n\t\t\tDrawing style of lines\n\t\t\t\tsolid\n\t\t\t\tdash\n\t\t\t\tdashdot\n\t\t\t\tdot\n\t\tsortbars : boole\n\t\t\tSort bars in descending order\n\t\t\t* Only valid when kind='bar'\n\t\tkeys : list of columns\n\t\t\tList of columns to chart.\n\t\t\tAlso can be usded for custom sorting.\n\t\tbestfit : boolean or list\n\t\t\tIf True then a best fit line will be generated for \n\t\t\tall columns. \n\t\t\tIf list then a best fit line will be generated for \n\t\t\teach key on the list. \n\t\tbestfit_colors : list or dict\n\t\t\t{key:color} to specify the color for each column\n\t\t\t[colors] to use the colors in the defined order\n\t\tasDates : bool\n\t\t\tIf true it forces truncates times from a DatetimeIndex\n\t\t\n\t\"\"\" \n\tdf=self.copy()\n\n\tif asTimestamp:\n\t\tx=[_ for _ in df.index]\n\telif df.index.__class__.__name__ in ('PeriodIndex','DatetimeIndex'):\n\t\tif asDates:\n\t\t\tdf.index=df.index.date\n\t\tx=df.index.format()\n\telif isinstance(df.index,pd.MultiIndex):\n\t\tx=['({0})'.format(','.join([str(__) for __ in _])) for _ in df.index.values]\n\telse:\n\t\tx = df.index.values\n\tlines={}\n\tif type(df)==pd.core.series.Series:\n\t\tdf=pd.DataFrame({df.name:df})\n\n\tif not keys:\t\t\n\t\tif 'bar' in kind:\n\t\t\tif sortbars:\n\t\t\t\tkeys=list(df.sum().sort_values(ascending=False).keys())\n\t\t\telse:\n\t\t\t\tkeys=list(df.keys())\n\t\telse:\n\t\t\tkeys=list(df.keys())\n\n\tcolors=get_colors(colors,colorscale,keys)\n\tdash=get_items_as_list(dash,keys,'dash')\n\tsymbol=get_items_as_list(symbol,keys,'symbol')\n\tmode=get_items_as_list(mode,keys,'mode')\n\tinterpolation=get_items_as_list(interpolation,keys,'interpolation')\n\twidth=get_items_as_list(width,keys,'width')\n\tfor key in keys:\n\t\tlines[key]={}\n\t\tlines[key][\"x\"]=x\n\t\tlines[key][\"y\"]=df[key].fillna('').values\n\t\tlines[key][\"name\"]=str(key)\n\t\tif text is not None:\n\t\t\tlines[key][\"text\"]=text\n\t\tif 'bar' in kind:\n\t\t\tlines[key][\"marker\"]={'color':to_rgba(colors[key],opacity),'line':{'color':colors[key],'width':1}}\n\t\telse:\n\t\t\tlines[key][\"line\"]={'color':colors[key],'width':width[key],'dash':dash[key], 'shape':interpolation[key]}\n\t\t\tlines[key][\"mode\"]=mode[key]\n\t\t\tif 'marker' in mode[key]:\n\t\t\t\tlines[key][\"marker\"]=dict(symbol=symbol[key],size=size)\n\t\t\tif fill:\n\t\t\t\tlines[key][\"fill\"]='tonexty' if kind=='area' else 'tozeroy'\n\t\t\t\tlines[key][\"fillcolor\"]=to_rgba(colors[key],kwargs['opacity'] if 'opacity' in kwargs else .3\t\t)\n\tif 'bar' in kind:\n\t\tlines_plotly=[Bar(lines[key]).to_plotly_json() for key in keys]\n\telse:\n\t\tlines_plotly=[Scatter(lines[key]).to_plotly_json() for key in keys]\n\tfor trace in lines_plotly:\n\t\tif isinstance(trace['name'],pd.Timestamp):\n\t\t\ttrace.update(name=str(trace['name']))\n\n\tif bestfit:\n\t\tif type(bestfit)==list:\n\t\t\tkeys=bestfit\n\t\td={}\n\t\tfor key in keys:\n\t\t\tbestfit=df[key].bestfit()\n\t\t\td[bestfit.formula]=bestfit\n\t\tbestfit_lines=pd.DataFrame(d).to_iplot(bestfit=False,colors=bestfit_colors,kind='scatter',asTimestamp=asTimestamp)\n\t\tfor line in bestfit_lines:\n\t\t\tline['line']['dash']='dash'\n\t\t\tif not bestfit_colors:\n\t\t\t\tline['line']['color']=to_rgba(line['line']['color'],.6)\n\t\tdata=lines_plotly\n\t\tdata.extend(bestfit_lines)\n\t\treturn data\n\n\tif mean:\n\t\tif type(mean)==list:\n\t\t\tkeys=mean\n\t\td={}\n\t\tfor key in keys:\n\t\t\tmean=df[key].mean()\n\t\t\td['MEAN({key})'.format(key=key)]=pd.Series([mean]*len(df[key]),index=df[key].index)\n\t\tmean_lines=pd.DataFrame(d).to_iplot(mean=False,colors=mean_colors,kind='scatter',asTimestamp=asTimestamp)\n\t\tfor line in mean_lines:\n\t\t\tline['line']['dash']='dash'\n\t\t\tif not mean_colors:\n\t\t\t\tline['line']['color']=to_rgba(line['line']['color'],.6)\n\t\tdata=[lines_plotly]\n\t\tdata.extend(mean_lines)\n\t\treturn data\n\treturn lines_plotly", "response": "This function returns a string that can be used to create a plotly object for the given object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfunctions to plot a series of objects in the DataFrame.", "response": "def _iplot(self,kind='scatter',data=None,layout=None,filename='',sharing=None,title='',xTitle='',yTitle='',zTitle='',theme=None,colors=None,colorscale=None,fill=False,width=None,\n\t\t\tdash='solid',mode='',interpolation='linear',symbol='circle',size=12,barmode='',sortbars=False,bargap=None,bargroupgap=None,bins=None,histnorm='',\n\t\t\thistfunc='count',orientation='v',boxpoints=False,annotations=None,keys=False,bestfit=False,\n\t\t\tbestfit_colors=None,mean=False,mean_colors=None,categories='',x='',y='',z='',text='',gridcolor=None,\n\t\t\tzerolinecolor=None,margin=None,labels=None,values=None,secondary_y='',secondary_y_title='',subplots=False,shape=None,error_x=None,\n\t\t\terror_y=None,error_type='data',locations=None,lon=None,lat=None,asFrame=False,asDates=False,asFigure=False,\n\t\t\tasImage=False,dimensions=None,asPlot=False,asUrl=False,online=None,**kwargs):\n\t\"\"\"\n\tReturns a plotly chart either as inline chart, image of Figure object\n\n\tParameters:\n\t-----------\n\t\tkind : string\n\t\t\tKind of chart\n\t\t\t\tscatter\n\t\t\t\tbar\n\t\t\t\tbox\n\t\t\t\tspread\n\t\t\t\tratio\n\t\t\t\theatmap\n\t\t\t\tsurface\n\t\t\t\thistogram\n\t\t\t\tbubble\n\t\t\t\tbubble3d\n\t\t\t\tscatter3d\t\n\t\t\t\tscattergeo\n\t\t\t\tohlc\n\t\t\t\tcandle\n\t\t\t\tpie\n\t\t\t\tchoroplet\t\n\t\tdata : Data\n\t\t\tPlotly Data Object.\n\t\t\tIf not entered then the Data object will be automatically\n\t\t\tgenerated from the DataFrame.\n\t\tlayout : Layout\n\t\t\tPlotly layout Object\n\t\t\tIf not entered then the Layout objet will be automatically\n\t\t\tgenerated from the DataFrame.\n\t\tfilename : string\n\t\t\tFilename to be saved as in plotly account\n\t\tsharing : string\n\t\t\tSets the sharing level permission\n\t\t\t\tpublic - anyone can see this chart\n\t\t\t\tprivate - only you can see this chart\n\t\t\t\tsecret - only people with the link can see the chart\n\t\ttitle : string\n\t\t\tChart Title\t\t\t\t\n\t\txTitle : string\n\t\t\tX Axis Title\n\t\tyTitle : string\n\t\t\tY Axis Title\n\t\t\t\tzTitle : string\n\t\tzTitle : string\n\t\t\tZ Axis Title\n\t\t\tApplicable only for 3d charts\n\t\ttheme : string\n\t\t\tLayout Theme\n\t\t\t\tsolar\n\t\t\t\tpearl\n\t\t\t\twhite\t\t\n\t\t\tsee cufflinks.getThemes() for all \n\t\t\tavailable themes\n\t\tcolors : dict, list or string\n\t\t\t{key:color} to specify the color for each column\n\t\t\t[colors] to use the colors in the defined order\n\t\tcolorscale : string\n\t\t\tColor scale name\n\t\t\tIf the color name is preceded by a minus (-) \n\t\t\tthen the scale is inversed\n\t\t\tOnly valid if 'colors' is null\n\t\t\tSee cufflinks.colors.scales() for available scales\n\t\tfill : bool\n\t\t\tFilled Traces\t\t\n\t\twidth : dict, list or int\n\t\t\t\tint : applies to all traces\n\t\t\t\tlist : applies to each trace in the order \n\t\t\t\t\t\tspecified\n\t\t\t\tdict: {column:value} for each column in \n\t\t\t\t\t\tthe dataframe\n\t\t\tLine width\t\n\t\tdash : dict, list or string\n\t\t\t\tstring : applies to all traces\n\t\t\t\tlist : applies to each trace in the order \n\t\t\t\t\t\tspecified\n\t\t\t\tdict: {column:value} for each column in \n\t\t\t\t\t\tthe dataframe\n\t\t\tDrawing style of lines\n\t\t\t\tsolid\n\t\t\t\tdash\n\t\t\t\tdashdot\n\t\t\t\tdot\n\t\tmode : dict, list or string\n\t\t\t\tstring : applies to all traces\n\t\t\t\tlist : applies to each trace in the order \n\t\t\t\t\t\tspecified\n\t\t\t\tdict: {column:value} for each column in \n\t\t\t\t\t\tthe dataframe\n\t\t\tPlotting mode for scatter trace\n\t\t\t\tlines\n\t\t\t\tmarkers\n\t\t\t\tlines+markers\n\t\t\t\tlines+text\n\t\t\t\tmarkers+text\n\t\t\t\tlines+markers+text\n\t\tinterpolation : dict, list, or string\n\t\t\t\tstring : applies to all traces\n\t\t\t\tlist : applies to each trace in the order \n\t\t\t\t\t\tspecified\n\t\t\t\tdict: {column:value} for each column in \n\t\t\t\t\t\tthe dataframe\n\t\t\tPositioning of the connecting lines\n\t\t\t\tlinear\n\t\t\t\tspline\n\t\t\t\tvhv\n\t\t\t\thvh\n\t\t\t\tvh\n\t\t\t\thv\t\t\n\t\tsymbol : dict, list or string\n\t\t\t\tstring : applies to all traces\n\t\t\t\tlist : applies to each trace in the order \n\t\t\t\t\t\tspecified\n\t\t\t\tdict: {column:value} for each column in \n\t\t\t\t\t\tthe dataframe\n\t\t\tThe symbol that is drawn on the plot for each marker\n\t\t\tValid only when mode includes markers\n\t\t\t\tcircle\n\t\t\t\tcircle-dot\n\t\t\t\tdiamond\n\t\t\t\tsquare\n\t\t\t\tand many more...(see plotly.validators.scatter.marker.SymbolValidator.values)\n\t\tsize : string or int \n\t\t\tSize of marker \n\t\t\tValid only if marker in mode\n\t\tbarmode : string\n\t\t\tMode when displaying bars\n\t\t\t\tgroup\n\t\t\t\tstack\n\t\t\t\toverlay\n\t\t\t* Only valid when kind='bar'\n\t\tsortbars : bool\n\t\t\tSort bars in descending order\n\t\t\t* Only valid when kind='bar'\n\t\tbargap : float\n\t\t\tSets the gap between bars\n\t\t\t\t[0,1)\n\t\t\t* Only valid when kind is 'histogram' or 'bar'\n\t\tbargroupgap : float\n\t\t\tSet the gap between groups\n\t\t\t\t[0,1)\n\t\t\t* Only valid when kind is 'histogram' or 'bar'\t\t\n\t\tbins : int or tuple \n\t\t\tif int:\n\t\t\t\tSpecifies the number of bins \n\t\t\tif tuple:\n\t\t\t\t(start, end, size)\n\t\t\t\tstart : starting value\n\t\t\t\tend: end value\n\t\t\t\tsize: bin size\n\t\t\t* Only valid when kind='histogram'\n\n\t\thistnorm : string\n\t\t\t\t'' (frequency)\n\t\t\t\tpercent\n\t\t\t\tprobability\n\t\t\t\tdensity\n\t\t\t\tprobability density\n\t\t\tSets the type of normalization for an histogram trace. By default\n\t\t\tthe height of each bar displays the frequency of occurrence, i.e., \n\t\t\tthe number of times this value was found in the\n\t\t\tcorresponding bin. If set to 'percent', the height of each bar\n\t\t\tdisplays the percentage of total occurrences found within the\n\t\t\tcorresponding bin. If set to 'probability', the height of each bar\n\t\t\tdisplays the probability that an event will fall into the\n\t\t\tcorresponding bin. If set to 'density', the height of each bar is\n\t\t\tequal to the number of occurrences in a bin divided by the size of\n\t\t\tthe bin interval such that summing the area of all bins will yield\n\t\t\tthe total number of occurrences. If set to 'probability density',\n\t\t\tthe height of each bar is equal to the number of probability that an\n\t\t\tevent will fall into the corresponding bin divided by the size of\n\t\t\tthe bin interval such that summing the area of all bins will yield\n\t\t\t1.\n\t\t\t* Only valid when kind='histogram'\n\t\thistfunc : string\n\t\t\t\tcount\n\t\t\t\tsum\n\t\t\t\tavg\n\t\t\t\tmin\n\t\t\t\tmax\n\t\t   Sets the binning function used for an histogram trace. \n\t\t\t* Only valid when kind='histogram'           \n\t\torientation : string\n\t\t\t\th \n\t\t\t\tv\n\t\t\tSets the orientation of the bars. If set to 'v', the length of each\n |          bar will run vertically. If set to 'h', the length of each bar will\n |          run horizontally\n\t\t\t* Only valid when kind is 'histogram','bar' or 'box'\n\t\tboxpoints : string\n\t\t\tDisplays data points in a box plot\n\t\t\t\toutliers\n\t\t\t\tall\n\t\t\t\tsuspectedoutliers\n\t\t\t\tFalse\n\t\tannotations : dictionary\n\t\t\tDictionary of annotations\n\t\t\t{x_point : text}\n\t\tkeys : list of columns\n\t\t\tList of columns to chart.\n\t\t\tAlso can be used for custom sorting.\n\t\tbestfit : boolean or list\n\t\t\tIf True then a best fit line will be generated for\n\t\t\tall columns.\n\t\t\tIf list then a best fit line will be generated for\n\t\t\teach key on the list.\n\t\tbestfit_colors : list or dict\n\t\t\t{key:color} to specify the color for each column\n\t\t\t[colors] to use the colors in the defined order\t\n\t\tcategories : string\n\t\t\tName of the column that contains the categories\n\t\tx : string\n\t\t\tName of the column that contains the x axis values\t\t\n\t\ty : string\n\t\t\tName of the column that contains the y axis values\n\t\tz : string\n\t\t\tName of the column that contains the z axis values\t\t\t\t\t\n\t\ttext : string\n\t\t\tName of the column that contains the text values\t\n\t\tgridcolor : string\n\t\t\tGrid color\t\n\t\tzerolinecolor : string\n\t\t\tZero line color\n\t\tmargin : dict or tuple\n\t\t\tDictionary (l,r,b,t) or\n\t\t\tTuple containing the left,\n\t\t\tright, bottom and top margins\n\t\tlabels : string\n\t\t\tName of the column that contains the labels.\n\t\t\t* Only valid when kind='pie' \n\t\tvalues : string\n\t\t\tName of the column that contains the values.\n\t\t\t* Only valid when kind='pie'\n\t\tsecondary_y : string or list(string)\n\t\t\tName(s) of the column to be charted on the \n\t\t\tright hand side axis\n\t\tsecondary_y_title : string\n\t\t\tTitle of the secondary axis\n\t\tsubplots : bool\n\t\t\tIf true then each trace is placed in \n\t\t\tsubplot layout\n\t\tshape : (rows,cols)\n\t\t\tTuple indicating the size of rows and columns\n\t\t\tIf omitted then the layout is automatically set\n\t\t\t* Only valid when subplots=True\n\t\terror_x : int or float or [int or float]\n\t\t\terror values for the x axis\n\t\terror_y : int or float or [int or float]\n\t\t\terror values for the y axis\n\t\terror_type : string\n\t\t\ttype of error bars\n\t\t\t\t'data' \n\t\t\t\t'constant'\n\t\t\t\t'percent'\n\t\t\t\t'sqrt'\n\t\t\t\t'continuous'\n\t\t\t\t'continuous_percent'\n\t\tasFrame : bool\n\t\t\tIf true then the data component of Figure will\n\t\t\tbe of Pandas form (Series) otherwise they will \n\t\t\tbe index values\n\t\tasDates : bool\n\t\t\tIf true it truncates times from a DatetimeIndex\n\t\tasFigure : bool\n\t\t\tIf True returns plotly Figure\n\t\tasImage : bool\n\t\t\tIf True it returns an Image (png)\n\t\t\tIn ONLINE mode:\n\t\t\t\tImage file is saved in the working directory\t\t\t\t\n\t\t\t\t\tAccepts:\n\t\t\t\t\t\tfilename\n\t\t\t\t\t\tdimensions\n\t\t\t\t\t\tscale\n\t\t\t\t\t\tdisplay_image\n\t\t\tIn OFFLINE mode:\n\t\t\t\tImage file is downloaded (downloads folder) and a \n\t\t\t\tregular plotly chart is displayed in Jupyter\n\t\t\t\t\tAccepts:\n\t\t\t\t\t\tfilename\n\t\t\t\t\t\tdimensions\n\t\tdimensions : tuple(int,int)\n\t\t\tDimensions for image / chart\n\t\t\t\t(width,height)\t\t\n\t\tasPlot : bool\n\t\t\tIf True the chart opens in browser\n\t\tasUrl : bool\n\t\t\tIf True the chart url/path is returned. No chart is displayed. \n\t\t\t\tIf Online : the URL is returned\n\t\t\t\tIf Offline : the local path is returned\n\t\tonline : bool\n\t\t\tIf True then the chart/image is rendered on the server \n\t\t\teven when running in offline mode. \n\n\t\tOther Kwargs\n\t\t============\n\t\tLine, Scatter\n\t\t\tconnectgaps : bool\n\t\t\t\tIf True, empty values are connected \n\t\tPie charts\n\t\t\tsort : bool\n\t\t\t\tIf True it sorts the labels by value\n\t\t\tpull : float [0-1]\n\t\t\t\tPulls the slices from the centre \n\t\t\thole : float [0-1]\n\t\t\t\tSets the size of the inner hole\n\t\t\tlinecolor : string\n\t\t\t\tSets the color for the contour line of the slices\n\t\t\tlinewidth : string\n\t\t\t\tSets the width for the contour line of the slices\t\n\t\t\ttextcolor : string\n\t\t\t\tSets the color for the text in the slices\n\t\t\ttextposition : string \n\t\t\t\tSets the position of the legends for each slice\n\t\t\t\t\toutside\n\t\t\t\t\tinner\n\t\t\ttextinfo : string \n\t\t\t\tSets the information to be displayed on \n\t\t\t\tthe legends \n\t\t\t\t\tlabel\n\t\t\t\t\tpercent\n\t\t\t\t\tvalue\n\t\t\t\t\t* or ony combination of the above using \n\t\t\t\t\t  '+' between each item\n\t\t\t\t\t  ie 'label+percent'\n\n\t\tHistogram\n\t\t\tlinecolor : string\n\t\t\t\tspecifies the line color of the histogram\n\n\t\tHeatmap and Surface\n\t\t\tcenter_scale : float\n\t\t\t\tCenters the colorscale at a specific value\n\t\t\t\tAutomatically sets the (zmin,zmax) values\n\t\t\tzmin : float\n\t\t\t\tDefines the minimum range for the z values. \n\t\t\t\tThis affects the range for the colorscale\n\t\t\tzmax : float\n\t\t\t\tDefines the maximum range for the z values. \n\t\t\t\tThis affects the range for the colorscale\n\n\t\tError Bars\n\t\t\terror_trace : string\n\t\t\t\tName of the column for which error should be \n\t\t\t\tplotted. If omitted then errors apply to all \n\t\t\t\ttraces.\n\t\t\terror_values_minus : int or float or [int or float]\n\t\t\t\tValues corresponding to the span of the error bars \n\t\t\t\tbelow the trace coordinates\n\t\t\terror_color : string\n\t\t\t\tColor for error bars\n\t\t\terror_thickness : float \n\t\t\t\tSets the line thickness of the error bars\n\t\t\terror_width :  float\n\t\t\t\tSets the width (in pixels) of the cross-bar at both \n\t\t\t\tends of the error bars\n\t\t\terror_opacity : float [0,1]\n\t\t\t\tOpacity for the error bars\n\n\t\tSubplots\n\t\t\thorizontal_spacing : float [0,1]\n\t\t\t\tSpace between subplot columns.\n\t\t\tvertical_spacing : float [0,1]\n\t\t\t\tSpace between subplot rows.\n\t\t\tsubplot_titles : bool\n\t\t\t\tIf True, chart titles are plotted\n\t\t\t\tat the top of each subplot\n\t\t\tshared_xaxes : bool\n\t\t\t\tAssign shared x axes.\n\t\t\t\tIf True, subplots in the same grid column have one common\n\t\t\t\tshared x-axis at the bottom of the grid.\n\t\t\tshared_yaxes : bool\n\t\t\t\tAssign shared y axes.\n\t\t\t\tIf True, subplots in the same grid row have one common\n\t\t\t\tshared y-axis on the left-hand side of the grid.\n\n\t\tShapes\n\t\t\thline : float, list or dict\n\t\t\t\tDraws a horizontal line at the \n\t\t\t\tindicated y position(s)\n\t\t\t\tExtra parameters can be passed in\n\t\t\t\tthe form of a dictionary (see shapes)\n\t\t\tvline : float, list or dict\n\t\t\t\tDraws a vertical line at the \n\t\t\t\tindicated x position(s)\n\t\t\t\tExtra parameters can be passed in\n\t\t\t\tthe form of a dictionary (see shapes)\n\t\t\thpsan : (y0,y1)\n\t\t\t\tDraws a horizontal rectangle at the \n\t\t\t\tindicated (y0,y1) positions.\n\t\t\t\tExtra parameters can be passed in\n\t\t\t\tthe form of a dictionary (see shapes)\n\t\t\tvspan : (x0,x1)\n\t\t\t\tDraws a vertical rectangle at the \n\t\t\t\tindicated (x0,x1) positions.\n\t\t\t\tExtra parameters can be passed in\n\t\t\t\tthe form of a dictionary (see shapes)\n\t\t\tshapes : dict or list(dict)\n\t\t\t\tList of dictionaries with the \n\t\t\t\tspecifications of a given shape.\n\t\t\t\tSee help(cufflinks.tools.get_shape)\n\t\t\t\tfor more information\n\t\t\n\t\tAxis Ranges\n\t\t\txrange : [lower_bound,upper_bound]\n\t\t\t\tSets the range for the x axis\n\t\t\tyrange : [lower_bound,upper_bound]\n\t\t\t\tSets the range for the y axis\n\t\t\tzrange : [lower_bound,upper_bound]\n\t\t\t\tSets the range for the z axis\n\t\t\t\t\n\t\tExplicit Layout Updates\n\t\t\tlayout_update : dict\n\t\t\t\tThe layout will be modified with all \n\t\t\t\tthe explicit values stated in the \n\t\t\t\tdictionary. \n\t\t\t\tIt will not apply if layout is passed\n\t\t\t\tas parameter.\n\n\n\t\tRange Selector\n\t\t\trangeselector : dict\n\t\t\t\tDefines a rangeselector object\n\t\t\t\tsee help(cf.tools.get_range_selector) for more information\n\t\t\t\tExample:\n\t\t\t\t\t{'steps':['1y','2 months','5 weeks','ytd','2mtd'],\n\t\t\t\t\t 'axis':'xaxis', 'bgcolor' : ('blue',.3),\n\t\t\t\t\t 'x': 0.2 , 'y' : 0.9}\n\n\t\tRange Slider\n\t\t\trangeslider : bool or dict\n\t\t\t\tDefines if a rangeslider is displayed\n\t\t\t\tIf bool: \n\t\t\t\t\tTrue : Makes it visible\n\t\t\t\tif dict:\n\t\t\t\t\tRangeslider object\n\t\t\t\tExample:\n\t\t\t\t\t{'bgcolor':('blue',.3),'autorange':True}\n\n\t\tAnnotations\n\t\t\tfontcolor : str\n\t\t\t\tText color for annotations\n\t\t\tfontsize : int\n\t\t\t\tText size for annotations\n\t\t\ttextangle : int\n\t\t\t\tText angle \n\t\t\tSee https://plot.ly/python/reference/#layout-annotations \n\t\t\tfor a complete list of valid parameters.\n\n\t\tExports\n\t\t\tdisplay_image : bool\n\t\t\t\tIf True then the image if displayed after being saved\n\t\t\t\t** only valid if asImage=True\n\t\t\tscale : integer\n\t\t\t\tIncrease the resolution of the image by `scale` amount\n\t\t\t\tOnly valid when asImage=True\t\t\n\t\"\"\"\n\n\t# Valid Kwargs\n\tvalid_kwargs = ['color','opacity','column','columns','labels','text','world_readable','colorbar']\n\tBUBBLE_KWARGS = ['abs_size']\n\tTRACE_KWARGS = ['hoverinfo','connectgaps']\n\tHEATMAP_SURFACE_KWARGS = ['center_scale','zmin','zmax']\n\tPIE_KWARGS=['sort','pull','hole','textposition','textinfo','linecolor','linewidth','textcolor']\n\tOHLC_KWARGS=['up_color','down_color','open','high','low','close','volume','name','decreasing','increasing']\n\tSUBPLOT_KWARGS=['horizontal_spacing', 'vertical_spacing',\n\t\t\t\t\t'specs', 'insets','start_cell','shared_xaxes','shared_yaxes','subplot_titles','shared_xaxis','shared_yaxis']\n\tGEO_KWARGS=['locationmode','locationsrc','geo','lon','lat']\n\tERROR_KWARGS=['error_trace','error_values_minus','error_color','error_thickness',\n\t\t\t\t\t'error_width','error_opacity']\n\tEXPORT_KWARGS=['display_image','scale']\n\tFF_DISTPLOT=[\"group_labels\", \"bin_size\", \"curve_type\", \"rug_text\", \"show_hist\", \"show_curve\", \"show_rug\"]\n\tFF_VIOLIN=[\"data_header\",\"group_header\",\"show_rug\",\"sort\"]\n\tkwargs_list = [tools.__LAYOUT_KWARGS,BUBBLE_KWARGS,TRACE_KWARGS,\n\t\t\t\t   OHLC_KWARGS,PIE_KWARGS,HEATMAP_SURFACE_KWARGS,SUBPLOT_KWARGS,GEO_KWARGS,ERROR_KWARGS,EXPORT_KWARGS,\n\t\t\t\t   FF_DISTPLOT,FF_VIOLIN]\n\t[valid_kwargs.extend(_) for _ in kwargs_list]\n\n\tdict_modifiers_keys = ['line']\n\tdict_modifiers={}\n\n\tfor k in dict_modifiers_keys:\n\t\tdict_modifiers[k]=kwargs_from_keyword(kwargs,{},k,True)\n\t\n\n\tfor key in list(kwargs.keys()):\n\t\tif key not in valid_kwargs:\n\t\t\traise Exception(\"Invalid keyword : '{0}'\".format(key))\n\n\t# Setting default values\n\tif not colors:\n\t\tcolors=kwargs['color'] if 'color' in kwargs else colors\n\tif isinstance(colors,str):\n\t\tcolors=[colors]\n\topacity=kwargs['opacity'] if 'opacity' in kwargs else 0.8\n\tif not dimensions:\n\t\tdimensions=auth.get_config_file()['dimensions']\n\n\t# Get values from config theme\n\tif theme is None:\n\t\ttheme = auth.get_config_file()['theme']\n\ttheme_config=tools.getTheme(theme)\n\tif colorscale is None:\n\t\tconfig_colorscale=auth.get_config_file()['colorscale']\n\t\tif config_colorscale in ('dflt',None):\n\t\t\tcolorscale=theme_config['colorscale'] if 'colorscale' in theme_config else 'original'\n\t\telse:\n\t\t\tcolorscale=config_colorscale\n\tif width is None:\n\t\tif kind != 'pie':\n\t\t\twidth=theme_config['linewidth'] if 'linewidth' in theme_config else 2\n\tif margin is None:\n\t\tmargin=auth.get_config_file().get('margin',None)\n\n\n\t# In case column was used instead of keys\n\tif 'column' in kwargs:\n\t\tkeys=[kwargs['column']] if isinstance(kwargs['column'],str) else kwargs['column']\n\tif 'columns' in kwargs:\n\t\tkeys=[kwargs['columns']] if isinstance(kwargs['columns'],str) else kwargs['columns']\n\tkind='line' if kind=='lines' else kind\n\n\t# Figure generators\n\tdef get_marker(marker={}):\n\t\tif 'line' in dict_modifiers:\n\t\t\tif 'color' not in dict_modifiers['line']:\n\t\t\t\tif 'linecolor' in kwargs:\n\t\t\t\t\tlinecolor=kwargs.get('linecolor')\n\t\t\t\telse:\n\t\t\t\t\tif 'linecolor' in tools.getTheme(theme=theme):\n\t\t\t\t\t\tlinecolor=normalize(tools.getTheme(theme=theme)['linecolor'])\n\t\t\t\t\telse: \n\t\t\t\t\t\tlinecolor=tools.getLayout(theme=theme)['xaxis']['titlefont']['color']\n\t\t\t\tdict_modifiers['line']['color']=linecolor\t\t\t\n\t\t\tdict_modifiers['line']=tools.updateColors(dict_modifiers['line'])\n\t\t\tmarker['line']=deep_update(marker['line'],dict_modifiers['line'])\n\t\treturn marker\n\n\t# We assume we are good citizens\n\tvalidate=True\n\t\n\n\tif not layout:\n\t\tl_kwargs=dict([(k,kwargs[k]) for k in tools.__LAYOUT_KWARGS if k in kwargs])\n\t\tif annotations:\n\t\t\tann_kwargs=check_kwargs(kwargs,tools.__ANN_KWARGS,clean_origin=True)\n\t\t\tannotations=tools.get_annotations(self.copy(),annotations,kind=kind,theme=theme,**ann_kwargs)\n\n\n\t\tlayout=tools.getLayout(kind=kind,theme=theme,xTitle=xTitle,yTitle=yTitle,zTitle=zTitle,title=title,barmode=barmode,\n\t\t\t\t\t\t\t\tbargap=bargap,bargroupgap=bargroupgap,annotations=annotations,gridcolor=gridcolor,\n\t\t\t\t\t\t\t   dimensions=dimensions,\n\t\t\t\t\t\t\t\tzerolinecolor=zerolinecolor,margin=margin,is3d='3d' in kind,**l_kwargs)\n\telif isinstance(layout, Layout):\n\t\tlayout = layout.to_plotly_json()\n\n\tif not data:\n\t\tif categories and kind not in ('violin'):\n\t\t\tdata=[]\n\t\t\tif 'bar' in kind:\n\t\t\t\tdf=self.copy()\n\t\t\t\tdf=df.set_index(categories)\n\t\t\t\tfig=df.figure(kind=kind,colors=colors,colorscale=colorscale,fill=fill,width=width,sortbars=sortbars,opacity=opacity,\n\t\t\t\t\t\tasDates=asDates,mode=mode,symbol=symbol,size=size,text=text,barmode=barmode,orientation=orientation)\n\t\t\t\tdata=fig['data']\t\t\t\n\t\t\telse:\n\t\t\t\t_keys=pd.unique(self[categories])\n\t\t\t\tcolors=get_colors(colors,colorscale,_keys)\t\n\t\t\t\tmode='markers' if not mode else mode\n\t\t\t\tfor _ in _keys:\n\t\t\t\t\t__=self[self[categories]==_].copy()\n\t\t\t\t\tif text:\n\t\t\t\t\t\t_text=__[text] if asFrame else __[text].values\n\t\t\t\t\t_x=__[x] if asFrame else __[x].values\n\t\t\t\t\t_y=__[y] if asFrame else __[y].values\n\t\t\t\t\tif z:\n\t\t\t\t\t\t_z=__[z] if asFrame else __[z].values\n\t\t\t\t\tif 'bubble' in kind:\n\t\t\t\t\t\trg=__[size].values\n\t\t\t\t\t\trgo=self[size].values\n\t\t\t\t\t\tif not kwargs.get('abs_size',False):\n\t\t\t\t\t\t\tif len(rgo)>1:\n\t\t\t\t\t\t\t\t_size=[int(100*(float(i)-rgo.min( ))/(rgo.max()-rgo.min()))+12 for i in rg]\t\t\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t_size=[12] if len(rgo) else []\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t_size=rgo\n\t\t\t\t\telse:\n\t\t\t\t\t\t_size=size\n\t\t\t\t\t_data=Scatter3d(x=_x,y=_y,mode=mode,name=_,\n\t\t\t\t\t\t\t\tmarker=dict(color=colors[_],symbol=symbol,size=_size,opacity=opacity,\n\t\t\t\t\t\t\t\t\t\t\t\tline=dict(width=width)),textfont=tools.getLayout(theme=theme)['xaxis']['titlefont'])\n\t\t\t\t\tif '3d' in kind:\n\t\t\t\t\t\t_data=Scatter3d(x=_x,y=_y,z=_z,mode=mode,name=_,\n\t\t\t\t\t\t\t\tmarker=dict(color=colors[_],symbol=symbol,size=_size,opacity=opacity,\n\t\t\t\t\t\t\t\t\t\t\t\tline=dict(width=width)),textfont=tools.getLayout(theme=theme)['xaxis']['titlefont'])\n\t\t\t\t\telse:\n\t\t\t\t\t\t#see error 168\n\t\t\t\t\t\tif type(_x)==pd.np.ndarray:\n\t\t\t\t\t\t\tif '[ns]' in _x.dtype.str:\n\t\t\t\t\t\t\t\t_x=_x.astype(str)\n\t\t\t\t\t\tif type(_y)==pd.np.ndarray:\n\t\t\t\t\t\t\tif '[ns]' in _y.dtype.str:\n\t\t\t\t\t\t\t\t_y=_y.astype(str)\n\t\t\t\t\t\t\n\t\t\t\t\t\t_data=Scatter(x=_x,y=_y,mode=mode,name=_,\n\t\t\t\t\t\t\t\tmarker=dict(color=colors[_],symbol=symbol,size=_size,opacity=opacity,\n\t\t\t\t\t\t\t\t\t\t\t\tline=dict(width=width)),textfont=tools.getLayout(theme=theme)['xaxis']['titlefont'])\n\t\t\t\t\tif text:\n\t\t\t\t\t\t_data.update(text=_text)\n\t\t\t\t\tdata.append(_data)\n\t\telse:\n\t\t\tif kind in ('scatter','spread','ratio','bar','barh','area','line'):\n\t\t\t\tdf=self.copy()\n\t\t\t\tif type(df)==pd.core.series.Series:\n\t\t\t\t\tdf=pd.DataFrame({df.name:df})\n\t\t\t\tif x:\n\t\t\t\t\tdf=df.set_index(x)\n\t\t\t\tif y and secondary_y:\n\t\t\t\t\tif isinstance(secondary_y, str):\n\t\t\t\t\t\tdf=df[[y, secondary_y]]\n\t\t\t\t\telse:\n\t\t\t\t\t\tdf=df[[y] + secondary_y]\n\t\t\t\telif y:\n\t\t\t\t\tdf=df[y]\n\t\t\t\tif kind=='area':\n\t\t\t\t\tdf=df.transpose().fillna(0).cumsum().transpose()\n\t\t\t\tmode='lines' if not mode else mode\n\t\t\t\tif text:\n\t\t\t\t\tif not isinstance(text,list):\n\t\t\t\t\t\ttext=self[text].values\n\t\t\t\tdata=df.to_iplot(colors=colors,colorscale=colorscale,kind=kind,interpolation=interpolation,fill=fill,width=width,dash=dash,sortbars=sortbars,keys=keys,\n\t\t\t\t\t\tbestfit=bestfit,bestfit_colors=bestfit_colors,mean=mean,mean_colors=mean_colors,asDates=asDates,mode=mode,symbol=symbol,size=size,\n\t\t\t\t\t\ttext=text,**kwargs)\t\t\n\t\t\t\ttrace_kw=check_kwargs(kwargs,TRACE_KWARGS)\n\t\t\t\tfor trace in data:\n\t\t\t\t\ttrace.update(**trace_kw)\t\t\n\t\t\t\t\t\t\n\t\t\t\tif kind in ('spread','ratio'):\n\t\t\t\t\t\tif kind=='spread':\n\t\t\t\t\t\t\ttrace=self.apply(lambda x:x[0]-x[1],axis=1)\n\t\t\t\t\t\t\tpositive=trace.apply(lambda x:x if x>=0 else pd.np.nan)\n\t\t\t\t\t\t\tnegative=trace.apply(lambda x:x if x<0 else pd.np.nan)\n\t\t\t\t\t\t\ttrace=pd.DataFrame({'positive':positive,'negative':negative})\n\t\t\t\t\t\t\ttrace=trace.to_iplot(colors={'positive':'green','negative':'red'},width=0.5)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\ttrace=self.apply(lambda x:x[0]*1.0/x[1],axis=1).to_iplot(colors=['green'],width=1)\n\t\t\t\t\t\tfor t in trace:\n\t\t\t\t\t\t\tt.update({'xaxis':'x2','yaxis':'y2','fill':'tozeroy',\n\t\t\t\t\t\t\t\t\t\t\t'name':kind.capitalize(),'connectgaps':False,'showlegend':False})\n\t\t\t\t\t\tdata.append(trace[0])\n\t\t\t\t\t\tif kind=='spread':\n\t\t\t\t\t\t\tdata.append(trace[1])\n\t\t\t\t\t\tlayout['yaxis'].update({'domain':[.3,1]})\n\t\t\t\t\t\tlayout['yaxis2']=copy.deepcopy(layout['yaxis'])\n\t\t\t\t\t\tlayout['xaxis2']=copy.deepcopy(layout['xaxis'])\n\t\t\t\t\t\tlayout['yaxis2'].update(domain=[0,.25],title=kind.capitalize())\n\t\t\t\t\t\tlayout['xaxis2'].update(anchor='y2',showticklabels=False)\n\t\t\t\t\t\tlayout['hovermode']='x'\n\t\t\t\tif 'bar' in kind:\n\t\t\t\t\tif 'stack' in barmode:\n\t\t\t\t\t\tlayout['legend'].update(traceorder='normal')\n\t\t\t\t\torientation = 'h' if kind=='barh' else orientation\n\t\t\t\t\tfor trace in data:\n\t\t\t\t\t\ttrace.update(orientation=orientation)\n\t\t\t\t\t\tif orientation=='h':\n\t\t\t\t\t\t\ttrace['x'],trace['y']=trace['y'],trace['x']\t\n\t\t\t\t\t\t\n\t\t\telif kind=='bubble':\n\t\t\t\tmode='markers' if 'markers' not in mode else mode \n\t\t\t\tx=self[x].values.tolist()\n\t\t\t\ty=self[y].values.tolist()\n\t\t\t\tz=size if size else z\n\t\t\t\trg=self[z].values\n\t\t\t\tif not kwargs.get('abs_size',False):\n\t\t\t\t\tif len(rg) > 1:\n\t\t\t\t\t\tz=[int(100*(float(_)-rg.min())/(rg.max()-rg.min()))+12 for _ in rg]\n\t\t\t\t\telse:\n\t\t\t\t\t\tz=[12] if len(rg) else []\n\t\t\t\telse:\n\t\t\t\t\tz=rg\n\t\t\t\ttext=kwargs['labels'] if 'labels' in kwargs else text\n\t\t\t\tlabels=self[text].values.tolist() if text else ''\n\t\t\t\tclrs=colors if colors else get_scales(colorscale)\n\t\t\t\tclrs=[clrs] if not isinstance(clrs,list) else clrs\n\t\t\t\tclrs=[clrs[0]]*len(x) if len(clrs)==1 else clrs\n\t\t\t\tmarker=dict(color=clrs,size=z,symbol=symbol,\n\t\t\t\t\t\t\t\tline=dict(width=width))\n\t\t\t\ttrace=Scatter(x=x,y=y,marker=marker,mode='markers',text=labels)\n\t\t\t\tdata=[trace]\n\t\t\telif kind in ('box','histogram','hist'):\n\t\t\t\tif isinstance(self,pd.core.series.Series):\n\t\t\t\t\tdf=pd.DataFrame({self.name:self})\n\t\t\t\telse:\n\t\t\t\t\tdf=self.copy()\n\t\t\t\tdata=[]\n\t\t\t\tclrs=get_colors(colors,colorscale,df.columns)\n\t\t\t\tif 'hist' in kind:\n\t\t\t\t\tbarmode = 'overlay' if barmode=='' else\t barmode \n\t\t\t\t\tlayout.update(barmode=barmode) \n\t\t\t\tcolumns=keys if keys else df.columns\n\t\t\t\tfor _ in columns:\n\t\t\t\t\tif kind=='box':\n\t\t\t\t\t\t__=Box(y=df[_].values.tolist(),marker=dict(color=clrs[_]),name=_,\n\t\t\t\t\t\t\t\tline=dict(width=width),boxpoints=boxpoints)\n\t\t\t\t\t\t# 114 - Horizontal Box\n\t\t\t\t\t\t__['orientation']=orientation\n\t\t\t\t\t\tif orientation=='h':\n\t\t\t\t\t\t\t__['x'],__['y']=__['y'],__['x']\t\n\t\t\t\t\t\t\n\t\t\t\t\telse:\n\t\t\t\t\t\t__=dict(x=df[_].values.tolist(),name=_,\n\t\t\t\t\t\t\t\tmarker=dict(color=clrs[_], line=dict(width=width)),\n\t\t\t\t\t\t\t\torientation=orientation,\n\t\t\t\t\t\t\t\topacity=kwargs['opacity'] if 'opacity' in kwargs else .8, histfunc=histfunc,\n\t\t\t\t\t\t\t\thistnorm=histnorm)\n\n\t\t\t\t\t\t__['marker']=get_marker(__['marker'])\n\n\t\t\t\t\t\tif orientation=='h':\n\t\t\t\t\t\t\t__['y']=__['x']\n\t\t\t\t\t\t\tdel __['x']\n\t\t\t\t\t\t__ = Histogram(__)\n\t\t\t\t\t\tif bins:\n\t\t\t\t\t\t\tif type(bins) in (tuple,list):\n\t\t\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t\t\t_bins={'start':bins[0],'end':bins[1],'size':bins[2]}\n\t\t\t\t\t\t\t\t\tif orientation=='h':\n\t\t\t\t\t\t\t\t\t\t__.update(ybins=_bins,autobiny=False)\t\n\t\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\t\t__.update(xbins=_bins,autobinx=False)\t\n\t\t\t\t\t\t\t\texcept:\n\t\t\t\t\t\t\t\t\tprint(\"Invalid format for bins generation\")\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tif orientation=='h':\n\t\t\t\t\t\t\t\t\t__.update(nbinsy=bins)\t\n\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\t__.update(nbinsx=bins)\n\t\t\t\t\tdata.append(__)\n\n\t\t\telif kind in ('heatmap','surface'):\n\t\t\t\tif x:\n\t\t\t\t\tx=self[x].values.tolist()\n\t\t\t\telse:\n\t\t\t\t\tif self.index.__class__.__name__ in ('PeriodIndex','DatetimeIndex'):\n\t\t\t\t\t\tx=self.index.format()\n\t\t\t\t\telse:\n\t\t\t\t\t\tx=self.index.values.tolist()\n\t\t\t\ty=self[y].values.tolist() if y else self.columns.values.tolist()\n\t\t\t\tz=self[z].values.tolist() if z else self.values.transpose()\n\t\t\t\tscale=get_scales('rdbu') if not colorscale else get_scales(colorscale)\n\t\t\t\tcolorscale=[[float(_)/(len(scale)-1),scale[_]] for _ in range(len(scale))]\n\t\t\t\tcenter_scale = kwargs.get('center_scale',None)\n\t\t\t\t\n\t\t\t\tif is_list(z):\t\t\t\t\n\t\t\t\t\tzmin=min(z)\n\t\t\t\t\tzmax=max(z)\n\t\t\t\telse:\n\t\t\t\t\tzmin=z.min()\n\t\t\t\t\tzmax=z.max()\n\t\t\t\t\t\n\t\t\t\tif center_scale is not None:\n\t\t\t\t\tif center_scale<=zmin+(zmax-zmin)/2:\n\t\t\t\t\t\tzmin=center_scale*2-zmax\n\t\t\t\t\telse:\n\t\t\t\t\t\tzmax=center_scale*2-zmin\n\t\t\t\tzmin=kwargs.get('zmin',zmin)\n\t\t\t\tzmax=kwargs.get('zmax',zmax)\n\t\t\t\tif kind=='heatmap':\n\t\t\t\t\tdata=[Heatmap(z=z,x=x,y=y,zmin=zmin,zmax=zmax,colorscale=colorscale)]\n\t\t\t\telse:\n\t\t\t\t\tdata=[Surface(z=z,x=x,y=y,colorscale=colorscale)]\n\n\t\t\telif kind in ('scatter3d','bubble3d'):\n\t\t\t\tdata=[]\n\t\t\t\tkeys=self[text].values if text else list(range(len(self)))\n\t\t\t\tcolors=get_colors(colors,colorscale,keys,asList=True)\n\t\t\t\tmode='markers' if 'markers' not in mode else mode \n\t\t\t\tdf=self.copy()\n\t\t\t\tdf['index']=keys\n\t\t\t\tif kind=='bubble3d':\n\t\t\t\t\trg=self[size].values\n\t\t\t\t\tif not kwargs.get('abs_size',False):\n\t\t\t\t\t\tsize=[int(100*(float(_)-rg.min())/(rg.max()-rg.min()))+12 for _ in rg]\n\t\t\t\t\telse:\n\t\t\t\t\t\tsize=rg\n\t\t\t\telse:\n\t\t\t\t\tsize=[size for _ in range(len(keys))]\t\n\n\t\t\t\t_data=Scatter3d(x=df[x].values.tolist(),y=df[y].values.tolist(),z=df[z].values.tolist(),mode=mode,text=keys,\n\t\t\t\t\t\t\t\t\tmarker=dict(color=colors,symbol=symbol,size=size,opacity=.8))\n\t\t\t\tif text:\n\t\t\t\t\t_data.update(text=keys)\n\t\t\t\tdata.append(_data)\n\n\t\t\telif kind=='pie':\n\t\t\t\tif not labels:\n\t\t\t\t\traise CufflinksError('Missing: labels')\n\t\t\t\tif not values:\n\t\t\t\t\traise CufflinksError('Missing: values')\n\t\t\t\tlabels=self[labels].values.tolist()\n\t\t\t\tvalues=self[values].values.tolist()\n\t\t\t\tmarker=dict(colors=get_colors(colors,colorscale,labels,asList=True))\n\t\t\t\tmarker.update(line=dict(color=kwargs.pop('linecolor',None),width=kwargs.pop('linewidth',width)))\n\t\t\t\tpie=dict(values=values,labels=labels,name='',marker=marker)\n\t\t\t\t\n\t\t\t\tkw=check_kwargs(kwargs,PIE_KWARGS)\n\t\t\t\tkw['textfont']={'color':kw.pop('textcolor',None)}\n\t\t\t\tpie.update(kw)\n\t\t\t\tdata=[]\n\t\t\t\tdel layout['xaxis']\n\t\t\t\tdel layout['yaxis']\n\t\t\t\tdata.append(Pie(pie))\n\t\t\t\tvalidate=False\n\n\t\t\telif kind in ('old_candle','old_ohlc'):\n\t\t\t\td=ta._ohlc_dict(self)\n\t\t\t\tif len(list(d.keys()))!=4:\n\t\t\t\t\traise Exception(\"OHLC type of charts require an Open, High, Low and Close column\")\t\t\t\t\n\t\t\t\tohlc_kwargs=check_kwargs(kwargs,OHLC_KWARGS)\n\t\t\t\tif kind=='old_candle':\t\t\t\t\t\n\t\t\t\t\tfig=tools.get_candle(self,theme=theme,layout=layout,**ohlc_kwargs)\n\t\t\t\telse:\n\t\t\t\t\tfig=tools.get_ohlc(self,theme=theme,layout=layout,**ohlc_kwargs)\n\t\t\t\tif bestfit:\n\t\t\t\t\tdf=self.copy()\n\t\t\t\t\tbf=_to_iplot(self[d['close']],bestfit=True,bestfit_colors=bestfit_colors,asTimestamp=True)\n\t\t\t\t\tfig['data'].append(bf[1])\n\t\t\t\tdata=fig['data']\n\t\t\t\tlayout=fig['layout']\n\n\t\t\telif kind in ('candle','ohlc','candlestick'):\n\t\t\t\tkind='candlestick' if kind=='candle' else kind\n\t\t\t\tkw=check_kwargs(kwargs,OHLC_KWARGS)\n\t\t\t\td=ta._ohlc_dict(self,validate='ohlc',**kw)\n\t\t\t\t_d=dict(type=kind,\n\t\t\t\t\t\t\topen=self[d['open']].values.tolist(),\n\t\t\t\t\t\t\thigh=self[d['high']].values.tolist(),\n\t\t\t\t\t\t\tlow=self[d['low']].values.tolist(),\n\t\t\t\t\t\t\tclose=self[d['close']].values.tolist())\n\t\t\t\tif isinstance(self.index,pd.core.indexes.datetimes.DatetimeIndex):\n\t\t\t\t\t_d['x']=self.index.astype('str')\n\t\t\t\telse:\n\t\t\t\t\t_d['x']=self.index\n\t\t\t\tif 'name' in kw:\n\t\t\t\t\t_d['name']=kw['name']\n\t\t\t\t\n\t\t\t\tshowlegend=False\n\t\t\t\tif 'showlegend' in kwargs:\n\t\t\t\t\tshowlegend=kwargs['showlegend']\n\t\t\t\telse:\n\t\t\t\t\tif 'legend' in kwargs:\n\t\t\t\t\t\tif type(kwargs['legend'])==bool:\n\t\t\t\t\t\t\tshowlegend=kwargs['legend']\n\t\t\t\t# https://github.com/santosjorge/cufflinks/issues/113\n\t\t\t\t# _d['increasing']=dict(line=dict(color=kw['up_color']) if 'up_color' in kw else dict(),showlegend=showlegend)\n\t\t\t\t# _d['decreasing']=dict(line=dict(color=kw['down_color']) if 'down_color' in kw else dict(),showlegend=showlegend)\n\t\t\t\t_d['increasing']=dict(line=dict(color=kw['up_color']) if 'up_color' in kw else dict())\n\t\t\t\t_d['decreasing']=dict(line=dict(color=kw['down_color']) if 'down_color' in kw else dict())\n\t\t\t\tfor k in ('increasing','decreasing'):\n\t\t\t\t\tif k in kw:\n\t\t\t\t\t\t_d[k]=deep_update(_d[k],kw[k])\n\t\t\t\t\n\t\t\t\t_d['showlegend']=showlegend\n\t\t\t\t_d['yaxis']='y2'\n\t\t\t\tdata=[_d]\n\n\n\n\t\t\telif kind in ('choropleth','scattergeo'):\n\t\t\t\tkw=check_kwargs(kwargs,GEO_KWARGS)\n\t\t\t\tif kind=='choropleth':\n\t\t\t\t\tif not all([x!=None for x in (locations,z)]):\n\t\t\t\t\t\traise Exception(\"Choropleth maps require a 'location' and 'z' column names specified\")\n\t\t\t\t\tgeo_data={'type':'choropleth','locations':self[locations],'z':self[z],\n\t\t\t\t\t\t\t'colorscale':get_colorscale(colorscale),\n\t\t\t\t\t\t\t'marker':get_marker(dict(line=dict(width=width)))}\n\t\t\t\telif kind=='scattergeo':\n\t\t\t\t\tif not all([x!=None for x in (lon,lat)]):\n\t\t\t\t\t\traise Exception(\"Scattergeo maps require a 'lon' and 'lat' column names specified\")\n\t\t\t\t\tgeo_data={'type':'scattergeo','lat':self[lat],'lon':self[lon],\n\t\t\t\t\t\t\t'marker':get_marker(dict(line=dict(width=width),\n\t\t\t\t\t\t\t\t\t\t\t\tsymbol=symbol,colorscale=get_colorscale(colorscale),\n\t\t\t\t\t\t\t\t\t\t\t\tcolor=self[z] if z else None))}\n\t\t\t\tif 'colorbar' in kwargs:\n\t\t\t\t\tgeo_data['colorbar']=kwargs['colorbar']\n\t\t\t\tgeo_data.update(kw)\n\t\t\t\tif text:\n\t\t\t\t\tgeo_data.update(text=self[text])\n\t\t\t\tvalidate=False\n\t\t\t\tdata=[]\n\t\t\t\tdata.append(geo_data)\n\n\t\t\t# Figure Factory\n\t\t\telif kind in ('distplot'):\n\t\t\t\tcolors=get_colors(colors,colorscale,self.keys(),asList=True)\n\t\t\t\thist_data=self.transpose().values\n\t\t\t\tkw=check_kwargs(kwargs,FF_DISTPLOT)\n\t\t\t\tgroup_labels=kw.pop('group_labels',self.columns)\n\t\t\t\tif histnorm:\n\t\t\t\t\tkw['histnorm']=histnorm\n\t\t\t\tfig=ff.create_distplot(hist_data=hist_data,group_labels=group_labels,\n\t\t\t\t\t\t\t\t\t\t colors=colors,**kw)\n\t\t\t\tdata=fig.data\n\t\t\t\tlayout=tools.merge_dict(layout,fig.layout)\n\t\t\telif kind in ('violin'):\n\t\t\t\tdf=pd.DataFrame(self) if type(self)==pd.core.series.Series else self.copy()\n\t\t\t\tkw=check_kwargs(kwargs,FF_VIOLIN)\n\t\t\t\tkw['rugplot']=kw.pop('show_rug',True)\n\t\t\t\tkw['title']=title\n\t\t\t\tif 'group_header' not in kw:\n\t\t\t\t\tkw['group_header']=categories if categories else None\n\t\t\t\tcategories=kw.get('group_header')\n\t\t\t\tcolors=get_colors(colors,colorscale,df[categories].value_counts().values if categories else df.keys(),asList=True)\n\t\t\t\tkw['colors']=colors\n\t\t\t\tif categories:\n\t\t\t\t\tfor _ in range(2,df[categories].value_counts().size+1):\n\t\t\t\t\t\tlayout['xaxis{0}'.format(_)]=layout['xaxis'].copy()\n\t\t\t\t\tif categories not in df:\n\t\t\t\t\t\traise CufflinksError('Column \"{0}\" not found in DataFrame'.format(categories))\n\t\t\t\t\telif len(df.columns)==1:\n\t\t\t\t\t\traise CufflinksError('When \"categories\" are specified, two columns are expected. \\n Only one column was found.')\n\t\t\t\t\telif len(df.columns)==2:\n\t\t\t\t\t\tcols=list(df.columns)\n\t\t\t\t\t\tcols.remove(categories)\n\t\t\t\t\t\tkw['data_header']=cols[0]\n\t\t\t\t\telse: \n\t\t\t\t\t\tif 'data_header' not in kw:\n\t\t\t\t\t\t\traise CufflinksError('data_header must be the column name with the desired numeric data for the violin plot.')\n\t\t\t\telse:\n\t\t\t\t\tif len(df.columns)==1:\n\t\t\t\t\t\tkw['data_header']=df.columns[0]\n\t\t\t\t\telif len(df.columns)>1:\n\t\t\t\t\t\tif 'data_header' not in kw:\n\t\t\t\t\t\t\traise CufflinksError('data_header must be the column name with the desired numeric data for the violin plot.')\n\t\t\t\tfig=ff.create_violin(df,**kw).to_dict()\n\t\t\t\tdata=fig['data']\n\t\t\t\tlayout=tools.merge_dict(layout,fig['layout'])\n\n\t\t\t\t\n\n\t\n## Sharing Values\n\tif all(['world_readable' in kwargs,sharing is None]):\n\t\tsharing=kwargs['world_readable']\n\tif isinstance(sharing,bool):\n\t\t\tif sharing:\n\t\t\t\tsharing='public'\n\t\t\telse:\n\t\t\t\tsharing='private'\n\tif sharing is None:\n\t\tsharing=auth.get_config_file()['sharing']\n\n\tif not filename:\n\t\tif title:\n\t\t\tfilename=title\n\t\telse:\n\t\t\tfilename='Plotly Playground {0}'.format(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n\n\t\n\n## Figure defintion\n\tfigure={}\n\tfigure['data']=data\n\tfigure['layout']=layout\n\n## Check secondary axis\n\tif secondary_y:\n\t\tfigure=tools._set_axis(figure,secondary_y,side='right')\n\t\tif secondary_y_title:\n\t\t\tfigure.layout.yaxis2.title=secondary_y_title\n\n## Error Bars\n\tif kind in ('scatter','bar','barh','lines','line'):\n\t\tif any([error_x,error_y]):\n\t\t\tdef set_error(axis,**kwargs):\n\t\t\t\treturn tools.set_errors(figure,axis=axis,**kwargs)\n\t\t\tkw=check_kwargs(kwargs,ERROR_KWARGS)\n\t\t\tkw=dict([(k.replace('error_',''),v) for k,v in list(kw.items())])\n\t\t\tkw['type']=error_type\n\t\t\tif error_x:\n\t\t\t\tkw['values']=error_x\n\t\t\t\tfigure=set_error('x',**kw)\n\t\t\tif error_y:\n\t\t\t\tkw['values']=error_y\n\t\t\t\tfigure=set_error('y',**kw)\n## Subplots \n\n\tif subplots:\n\t\tfig=tools.strip_figures(figure)\n\t\tkw=check_kwargs(kwargs,SUBPLOT_KWARGS)\t\n\t\tfor _ in ['x','y']:\n\t\t\tif 'shared_{0}axes'.format(_) not in kw:\n\t\t\t\tkw['shared_{0}axes'.format(_)]=kw.pop('shared_{0}axis'.format(_),False)\n\t\tif 'subplot_titles' in kwargs:\n\t\t\tif kwargs['subplot_titles']==True:\n\t\t\t\tkw['subplot_titles']=[d['name'] for d in data]\n\t\t\telse:\n\t\t\t\tkw['subplot_titles']=kwargs['subplot_titles']\t\t\n\t\tfigure=tools.subplots(fig,shape,base_layout=layout,theme=theme,**kw)\n\n\n## Exports \n\tvalidate = False if 'shapes' in layout else validate\n\n\tif asFigure:\n\t\treturn Figure(figure)\n\telse:\n\t\treturn iplot(figure,validate=validate,sharing=sharing,filename=filename,\n\t\t\t online=online,asImage=asImage,asUrl=asUrl,asPlot=asPlot,\n\t\t\t dimensions=dimensions,display_image=kwargs.get('display_image',True))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dict with an item per key", "response": "def get_items_as_list(items,keys,items_names='styles'):\n\t\"\"\"\n\tReturns a dict with an item per key\n\n\tParameters:\n\t-----------\n\t\titems : string, list or dict\n\t\t\tItems (ie line styles)\n\t\tkeys: list \n\t\t\tList of keys\n\t\titems_names : string\n\t\t\tName of items \n\t\"\"\"\n\tif type(items)!=dict:\n\t\tif type(items)==list:\n\t\t\tif len(items)!=len(keys):\n\t\t\t\traise Exception('List of {0} is not the same length as keys'.format(items_names))\n\t\t\telse:\n\t\t\t\titems=dict(zip(keys,items))\n\t\telse:\n\t\t\titems=dict(zip(keys,[items]*len(keys)))\n\treturn items"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _scatter_matrix(self,theme=None,bins=10,color='grey',size=2, asFigure=False, **iplot_kwargs):\n\tsm=tools.scatter_matrix(self,theme=theme,bins=bins,color=color,size=size)\n\tif asFigure:\n\t\treturn sm\n\telse:\n\t\treturn iplot(sm,**iplot_kwargs)", "response": "Displays a matrix with scatter plot for each pair of \n\tSeries in the DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef iplot(figure,validate=True,sharing=None,filename='',\n\t\t\t online=None,asImage=False,asUrl=False,asPlot=False,\n\t\t\t dimensions=None,display_image=True,**kwargs):\n\t\"\"\"\n\tPlots a figure in IPython, creates an HTML or generates an Image\n\n\tfigure : figure\n\t\tPlotly figure to be charted\n\tvalidate : bool\n\t\tIf True then all values are validated before \n\t\tit is charted\n\tsharing : string\n\t\tSets the sharing level permission\n\t\t\tpublic - anyone can see this chart\n\t\t\tprivate - only you can see this chart\n\t\t\tsecret - only people with the link can see the chart\n\tfilename : string\n\t\tName to be used to save the file in the server, or as an image\n\tonline : bool\n\t\tIf True then the chart/image is rendered on the server \n\t\teven when running in offline mode. \n\tasImage : bool\n\t\t\tIf True it returns an Image (png)\n\t\t\tIn ONLINE mode:\n\t\t\t\tImage file is saved in the working directory\t\t\t\t\n\t\t\t\t\tAccepts:\n\t\t\t\t\t\tfilename\n\t\t\t\t\t\tdimensions\n\t\t\t\t\t\tscale\n\t\t\t\t\t\tdisplay_image\n\t\t\tIn OFFLINE mode:\n\t\t\t\tImage file is downloaded (downloads folder) and a \n\t\t\t\tregular plotly chart is displayed in Jupyter\n\t\t\t\t\tAccepts:\n\t\t\t\t\t\tfilename\n\t\t\t\t\t\tdimensions\n\tasUrl : bool\n\t\tIf True the chart url/path is returned. No chart is displayed. \n\t\t\tIf Online : the URL is returned\n\t\t\tIf Offline : the local path is returned\n\tasPlot : bool\n\t\tIf True the chart opens in browser\n\tdimensions : tuple(int,int)\n\t\tDimensions for image\n\t\t\t(width,height)\t\t\n\tdisplay_image : bool\n\t\tIf true, then the image is displayed after it has been saved\n\t\tRequires Jupyter Notebook\n\t\tOnly valid when asImage=True\n\n\tOther Kwargs\n\t============\n\t\tlegend : bool\n\t\t\tIf False then the legend will not be shown\n\t\tscale : integer\n\t\t\tIncrease the resolution of the image by `scale` amount\n\t\t\tOnly valid when asImage=True\t\t\n\t\"\"\"\n\tvalid_kwargs=['world_readable','legend','scale']\n\n\tfor key in list(kwargs.keys()):\n\t\tif key not in valid_kwargs:\n\t\t\traise Exception(\"Invalid keyword : '{0}'\".format(key))\n\t\n\tif 'legend' in kwargs:\n\t\tif 'layout' in figure:\n\t\t\tfigure['layout'].update(showlegend=kwargs['legend'])\n\n\t## Sharing Values\n\tif all(['world_readable' in kwargs,sharing is None]):\n\t\tsharing=kwargs['world_readable']\n\tif isinstance(sharing,bool):\n\t\t\tif sharing:\n\t\t\t\tsharing='public'\n\t\t\telse:\n\t\t\t\tsharing='private'\n\tif sharing is None:\n\t\tsharing=auth.get_config_file()['sharing']\n\n\t## Filename Handling\n\tif not filename:\n\t\t# if not figure.get('layout', None):\n\t\t# \tfigure['layout'] = {}\n\t\ttry:\n\t\t\tfilename=figure['layout']['title']\n\t\texcept:\n\t\t\tfilename='Plotly Playground {0}'.format(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n\n\t## Dimensions\n\tif not dimensions:\n\t\tdimensions=(800,500) if not auth.get_config_file()['dimensions'] else auth.get_config_file()['dimensions']\n\n\t## Offline Links\n\tshow_link = auth.get_config_file()['offline_show_link']\n\tlink_text = auth.get_config_file()['offline_link_text']\n\tconfig = auth.get_config_file()['offline_config']\n\n\t## Remove validation if shapes are present\n\tif 'layout' in figure:\n\t\tvalidate = False if 'shapes' in figure['layout'] else validate\n\n\t## asURL\n\tauto_open=True\n\tif asUrl:\n\t\tasPlot=True\n\t\tauto_open=False\n\n\t## Exports\n\tif asImage:\n\t\tif offline.is_offline() and not online:\n\t\t\treturn offline.py_offline.iplot(figure,validate=validate, filename=filename, show_link=show_link,link_text=link_text,\n\t\t\t\timage='png', image_width=dimensions[0], image_height=dimensions[1], config=config)\n\t\telse:\n\t\t\ttry:\n\t\t\t\tpy.image.save_as(figure,filename='img/'+filename,format='png',\n\t\t\t\t\twidth=dimensions[0],height=dimensions[1],scale=kwargs.get('scale',None))\n\t\t\t\tpath='img/'+filename+'.png'\n\t\t\texcept:\n\t\t\t\tpy.image.save_as(figure,filename=filename,format='png',\n\t\t\t\t\twidth=dimensions[0],height=dimensions[1],scale=kwargs.get('scale',None))\n\t\t\t\tpath=filename+'.png'\n\t\t\tif display_image:\n\t\t\t\treturn display(Image(path))\n\t\t\telse:\n\t\t\t\tprint('Image saved : {0}'.format(path))\n\t\t\t\treturn None\n\n\t## asPlot and asUrl\n\tif asPlot:\n\t\tfilename+='.html'\n\t\tif offline.is_offline() and not online:\n\t\t\treturn offline.py_offline.plot(figure, filename=filename, validate=validate,\n\t\t\t\t\t\t\t\tshow_link=show_link, link_text=link_text, auto_open=auto_open, config=config)\n\t\telse:\n\t\t\treturn py.plot(figure, sharing=sharing, filename=filename, validate=validate,\n\t\t\t\t\t\t\tauto_open=auto_open)\n\n\t## iplot\n\tif offline.is_offline() and not online:\t\n\t\treturn offline.py_offline.iplot(figure, validate=validate, filename=filename, show_link=show_link, link_text=link_text, config=config)\n\telse:\t\t\n\t\treturn py.iplot(figure,validate=validate,sharing=sharing,\n\t\t\t\t\t\tfilename=filename)", "response": "This function is used to plot a figure in IPython notebook."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ta_plot(self,study,periods=14,column=None,include=True,str='{name}({period})',detail=False,\n\t\t\t theme=None,sharing=None,filename='',asFigure=False,**iplot_kwargs):\n\t\"\"\"\n\tGenerates a Technical Study Chart\n\n\tParameters:\n\t-----------\n\t\t\tstudy : string\n\t\t\t\tTechnical Study to be charted\n\t\t\t\t\tsma - 'Simple Moving Average'\n\t\t\t\t\trsi - 'R Strength Indicator'\n\t\t\tperiods : int\n\t\t\t\tNumber of periods\n\t\t\tcolumn : string\n\t\t\t\tName of the column on which the\n\t\t\t\tstudy will be done\n\t\t\tinclude : bool\n\t\t\t\tIndicates if the input column(s)\n\t\t\t\tshould be included in the chart\n\t\t\tstr : string\n\t\t\t\tLabel factory for studies\n\t\t\t\tThe following wildcards can be used:\n\t\t\t\t\t{name} : Name of the column\n\t\t\t\t\t{study} : Name of the study\n\t\t\t\t\t{period} : Period used\n\t\t\t\tExamples:\n\t\t\t\t\t'study: {study} - period: {period}'\n\t\t\tdetail : bool\n\t\t\t\tIf True the supporting data/calculations\n\t\t\t\tare included in the chart \n\t\t\tstudy_colors : string or [string]\n\t\t\t\tColors to be used for the studies\n\n\t\tStudy Specific Parameters\n\t\t-------------------------\n\t\tRSI \n\t\t\trsi_upper : int (0,100]\n\t\t\t\tLevel for the upper rsi band\n\t\t\t\tdefault : 70\n\t\t\trsi_lower : int (0,100]\n\t\t\t\tLevel for the lower rsi band\n\t\t\t\tdefault : 30\n\t\tCCI \n\t\t\tcci_upper : int \n\t\t\t\tLevel for the upper cci band\n\t\t\t\tdefault : 100\n\t\t\tcci_lower : int \n\t\t\t\tLevel for the lower cci band\n\t\t\t\tdefault : -100\n\t\tBOLL\n\t\t\tboll_std : int or float\n\t\t\t\tNumber of standard deviations\n\t\tMACD\n\t\t\tfast_period : int\n\t\t\t\tNumber of periods for the fast moving average\n\t\t\tslow_period : int\n\t\t\t\tNumber of periods for the slow moving average\n\t\t\tsignal_period : int\n\t\t\t\tNumber of periods for the signal \n\t\tCORREL\n\t\t\thow : string\n\t\t\t\tMethod for the correlation calculation\n\t\t\t\t\tvalues\n\t\t\t\t\tpct_cht\n\t\t\t\t\tdiff\n\t\t\t\t\t\n\t\"\"\"\n\n\tif 'columns' in iplot_kwargs:\n\t\tcolumn=iplot_kwargs.pop('columns')\n\t\t\n\tif 'period' in iplot_kwargs:\n\t\tperiods=iplot_kwargs.pop('period')\n\t\t\n\tif 'world_readable' in iplot_kwargs:\n\t\tsharing=iplot_kwargs.pop('world_readable')\n\n\tif 'study_color' in iplot_kwargs:\n\t\tiplot_kwargs['study_colors']=iplot_kwargs.pop('study_color')\n\t\t\n\tif sharing is None:\n\t\t\tsharing = auth.get_config_file()['sharing']\n\tif isinstance(sharing,bool):\n\t\t\tif sharing:\n\t\t\t\tsharing='public'\n\t\t\telse:\n\t\t\t\tsharing='private'\n\tiplot_kwargs['sharing']=sharing\n\tif theme is None:\n\t\ttheme = iplot_kwargs.pop('study_theme',auth.get_config_file()['theme'])\n\n\tif not filename:\n\t\tif 'title' in iplot_kwargs:\n\t\t\tfilename=iplot_kwargs['title']\n\t\telse:\n\t\t\tfilename='Plotly Playground {0}'.format(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n\n\tdef get_subplots(figures):\n\t\tshape=(len(figures),1)\n\t\tlayout=tools.get_base_layout(figures)\n\t\tsubplots=tools.subplots(figures,shape=shape,shared_xaxes=True,base_layout=layout)\n\t\tif len(figures)==2:\n\t\t\tsubplots['layout']['yaxis']['domain']=[.27,1.0]\n\t\t\tsubplots['layout']['yaxis2']['domain']=[0,.25]\n\t\treturn subplots\n\n\tdef get_study(df,func,iplot_kwargs,iplot_study_kwargs,str=None,include=False,column=None,inset=False):\n\t\tdf=df.copy()\n\t\tif inset:\n\t\t\tif not column:\n\t\t\t\tif isinstance(df,pd.DataFrame):\n\t\t\t\t\tcolumn=df.keys().tolist()\n\t\t\t\telse:\n\t\t\t\t\tdf=pd.DataFrame(df)\n\t\t\t\t\tcolumn=df.keys().tolist()\n\t\tif 'legend' in iplot_kwargs:\n\t\t\tiplot_study_kwargs['legend']=iplot_kwargs['legend']\n\t\tfig_0=df.figure(**iplot_kwargs)\n\t\tdf_ta=func(df,column=column,include=False,str=str,**study_kwargs)\t\n\t\tkind=iplot_kwargs['kind'] if 'kind' in iplot_kwargs else ''\n\t\tiplot_study_kwargs['kind']='scatter'\n\t\tiplot_study_kwargs['colors']=iplot_study_kwargs.get('colors',['blue','green','red'] if study=='dmi' else 'blue')\n\t\tfig_1=df_ta.figure(theme=theme,**iplot_study_kwargs)\n\t\tif kind in ['candle','ohlc']:\n\t\t\t\tfor i in fig_1['data']:\n\t\t\t\t\ti['x']=[pd.Timestamp(_) for _ in i['x']]\n\t\tif inset:\n\t\t\tfigure=tools.merge_figures([fig_0,fig_1]) if include else fig_1\n\t\telse:\n\t\t\tfigure=get_subplots([fig_0,fig_1]) if include else fig_1\n\t\treturn figure\n\n\tstudy_kwargs={}  \n\tiplot_study_kwargs={}\n\n\tstudy_kwargs=check_kwargs(iplot_kwargs,__TA_KWARGS,{},clean_origin=True)\n\tiplot_study_kwargs=kwargs_from_keyword(iplot_kwargs,{},'study')\n\n\tstudy_kwargs.update({'periods':periods})\n\n\tta_func = eval('ta.{0}'.format(study))\n\n\tinset=study in ('sma','boll','ema','atr','ptps')\n\tfigure=get_study(self,ta_func,iplot_kwargs,iplot_study_kwargs,include=include,\n\t\t\t\t\t column=column,str=str,inset=inset)\n\n\t## Add Bands\n\tif study in ('rsi','cci'):\n\t\tbands= {'rsi':(30,70),\n\t\t\t\t'cci':(-100,100)}\n\t\t_upper=study_kwargs.get('{0}_upper'.format(study),bands[study][0])\n\t\t_lower=study_kwargs.get('{0}_lower'.format(study),bands[study][1])\n\t\tyref='y2' if include else 'y1'\n\t\tshapes=[tools.get_shape(y=i,yref=yref,color=j,dash='dash') for (i,j) in [(_lower,'green'),(_upper,'red')]]\n\t\tfigure['layout']['shapes']=shapes\n\n\t# if study=='rsi':\n\t# \trsi_upper=study_kwargs.get('rsi_upper',70)\n\t# \trsi_lower=study_kwargs.get('rsi_lower',30)\n\t# \tyref='y2' if include else 'y1'\n\t# \tshapes=[tools.get_shape(y=i,yref=yref,color=j,dash='dash') for (i,j) in [(rsi_lower,'green'),(rsi_upper,'red')]]\n\t# \tfigure['layout']['shapes']=shapes\n\t\n\t# if study=='cci':\n\t# \tcci_upper=study_kwargs.get('cci_upper',100)\n\t# \tcci_lower=study_kwargs.get('cci_lower',-100)\n\t# \tyref='y2' if include else 'y1'\n\t# \tshapes=[tools.get_shape(y=i,yref=yref,color=j,dash='dash') for (i,j) in [(cci_lower,'green'),(cci_upper,'red')]]\n\t# \tfigure['layout']['shapes']=shapes\n\n\t## Exports\n\n\tif asFigure:\n\t\treturn figure\n\telse: \n\t\treturn iplot(figure,sharing=sharing,filename=filename)", "response": "This function generates a TA chart for the given study and periods."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _fig_iplot(self,validate=True,sharing=None,filename='',\n\t\t\t online=None,asImage=False,asUrl=False,asPlot=False,\n\t\t\t dimensions=None,display_image=True,**kwargs):\n\t\"\"\"\n\tPlots a figure in IPython\n\n\tfigure : figure\n\t\tPlotly figure to be charted\n\tvalidate : bool\n\t\tIf True then all values are validated before \n\t\tit is charted\n\tsharing : string\n\t\tSets the sharing level permission\n\t\t\tpublic - anyone can see this chart\n\t\t\tprivate - only you can see this chart\n\t\t\tsecret - only people with the link can see the chart\n\tfilename : string\n\t\tName to be used to save the file in the server, or as an image\n\tonline : bool\n\t\tIf True then the chart is rendered on the server \n\t\teven when running in offline mode. \n\tasImage : bool\n\t\t\tIf True it returns an Image (png)\n\t\t\tIn ONLINE mode:\n\t\t\t\tImage file is saved in the working directory\t\t\t\t\n\t\t\t\t\tAccepts:\n\t\t\t\t\t\tfilename\n\t\t\t\t\t\tdimensions\n\t\t\t\t\t\tscale\n\t\t\t\t\t\tdisplay_image\n\t\t\tIn OFFLINE mode:\n\t\t\t\tImage file is downloaded (downloads folder) and a \n\t\t\t\tregular plotly chart is displayed in Jupyter\n\t\t\t\t\tAccepts:\n\t\t\t\t\t\tfilename\n\t\t\t\t\t\tdimensions\n\tasUrl : bool\n\t\tIf True the chart url/path is returned. No chart is displayed. \n\t\t\tIf Online : the URL is returned\n\t\t\tIf Offline : the local path is returned\n\tasPlot : bool\n\t\tIf True the chart opens in browser\n\tdimensions : tuple(int,int)\n\t\tDimensions for image\n\t\t\t(width,height)\t\t\n\tdisplay_image : bool\n\t\tIf true, then the image is displayed after it has been saved\n\t\tRequires Jupyter Notebook\n\t\tonlh valide when asImage=True\n\n\tOther Kwargs\n\t============\n\n\t\tlegend : bool\n\t\t\tIf False then the legend will not be shown\t\t\n\t\"\"\"\n\treturn iplot(self,validate=validate,sharing=sharing,filename=filename,\n\t\t\t online=online,asImage=asImage,asUrl=asUrl,asPlot=asPlot,\n\t\t\t dimensions=dimensions,display_image=display_image,**kwargs)", "response": "Function that returns a figure in IPython notebook."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nensures that all local files are set in a valid way.", "response": "def ensure_local_files():\n\t\"\"\"\n\tEnsure that filesystem is setup/filled out in a valid way\n\t\"\"\"\n\tif _file_permissions:\n\t\tif not os.path.isdir(AUTH_DIR):\n\t\t\tos.mkdir(AUTH_DIR)\n\t\tfor fn in [CONFIG_FILE]:\n\t\t\tcontents = load_json_dict(fn)\n\t\t\tfor key, val in list(_FILE_CONTENT[fn].items()):\n\t\t\t\tif key not in contents:\n\t\t\t\t\tcontents[key] = val\n\t\t\tcontents_keys = list(contents.keys())\n\t\t\tfor key in contents_keys:\n\t\t\t\tif key not in _FILE_CONTENT[fn]:\n\t\t\t\t\tdel contents[key]\n\t\t\tsave_json_dict(fn, contents)\n\telse:\n\t\twarnings.warn(\"Looks like you don't have 'read-write' permission to \"\n\t\t\t\t\t  \"your 'home' ('~') directory\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the config file for the current chart.", "response": "def set_config_file(sharing=None,theme=None,colorscale=None,offline=None,offline_connected=None,\n\t\t\t\t\toffline_url=None,offline_show_link=None,offline_link_text=None,\n\t\t\t\t\toffline_config=None,\n\t\t\t\t\tdatagen_mode=None,**kwargs):\n\t\"\"\"\n\tSet the keyword-value pairs in `~/.config`.\n\n\tsharing : string\n\t\t\tSets the sharing level permission\n\t\t\t\tpublic - anyone can see this chart\n\t\t\t\tprivate - only you can see this chart\n\t\t\t\tsecret - only people with the link can see the chart\n\ttheme : string\n\t\t\tSets the default theme\n\t\t\tSee cufflinks.getThemes() for available themes \n\tcolorscale : string\n\t\t\tSets the default colorscale\n\t\t\tSee cufflinks.scales()\n\toffline : bool\n\t\t\tIf true then the charts are rendered\n\t\t\tlocally. \n\toffline_connected : bool\n\t\t\tIf True, the plotly.js library will be loaded\n\t\t\tfrom an online CDN. If False, the plotly.js library will be loaded locally\n\t\t\tfrom the plotly python package\n\toffline_show_link : bool\n\t\t\tIf true then the chart will show a link to \n\t\t\tplot.ly at the bottom right of the chart \n\toffline_link_text : string\n\t\t\tText to display as link at the bottom \n\t\t\tright of the chart \n\toffline_config : dict\n\t\t\tAdditional configuration options\n\t\t\tFor the complete list of config options check out: \n\t\t\thttps://github.com/plotly/plotly.js/blob/master/src/plot_api/plot_config.js\n\tdatagen_mode : string\n\t\t\tMode in which the data is generated\n\t\t\tby the datagen module\n\t\t\t\tstocks : random stock names are used for the index\n\t\t\t\tabc : alphabet values are used for the index\n\tdimensions : tuple\n\t\t\tSets the default (width,height) of the chart\n\tmargin : dict or tuple\n\t\t\tDictionary (l,r,b,t) or\n\t\t\tTuple containing the left,\n\t\t\tright, bottom and top margins\n\t\"\"\"\n\tif not _file_permissions:\n\t\traise Exception(\"You don't have proper file permissions \"\n\t\t\t\t\t\t\t\t\t \"to run this function.\")\n\tvalid_kwargs=['world_readable','dimensions','margin','offline_config']\n\tfor key in list(kwargs.keys()):\n\t\tif key not in valid_kwargs:\n\t\t\traise Exception(\"Invalid keyword : '{0}'\".format(key))\n\tif all(['world_readable' in kwargs,sharing is None]):\n\t\tsharing=kwargs['world_readable']\n\tif isinstance(sharing,bool):\n\t\t\tif sharing:\n\t\t\t\tsharing='public'\n\t\t\telse:\n\t\t\t\tsharing='private'\n\tconfig = get_config_file()\n\tif sharing is not None:\n\t\tconfig['sharing'] = sharing\n\tif theme:\n\t\tconfig['theme']=theme\n\tif colorscale:\n\t\tconfig['colorscale']=colorscale\n\tif offline_connected:\n\t\tconfig['offline_connected']=offline_connected\n\tif offline is not None:\n\t\tconfig['offline']=offline\n\t\tif offline:\n\t\t\tgo_offline()\n\tif datagen_mode:\n\t\tconfig['datagen_mode']=datagen_mode\n\tif offline_url:\n\t\tconfig['offline_url']=offline_url\n\tif offline_show_link is not None:\n\t\tconfig['offline_show_link']=offline_show_link\n\tif offline_link_text:\n\t\tconfig['offline_link_text']=offline_link_text\n\tif offline_config:\n\t\tconfig['offline_config']=offline_config\n\tfor _ in valid_kwargs:\n\t\tif _ in kwargs:\n\t\t\tconfig[_]=kwargs[_]\n\tsave_json_dict(CONFIG_FILE, config)\n\tensure_local_files()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_json_dict(filename, *args):\n\tdata = {}\n\tif os.path.exists(filename):\n\t\twith open(filename, \"r\") as f:\n\t\t\ttry:\n\t\t\t\tdata = json.load(f)\n\t\t\t\tif not isinstance(data, dict):\n\t\t\t\t\tdata = {}\n\t\t\texcept:\n\t\t\t\tpass \n\t\tif args:\n\t\t\treturn {key: data[key] for key in args if key in data}\n\treturn data", "response": "Checks if file exists. Returns {} if something fails. Returns {} if something fails."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dictionary with the schema for a QuantFigure", "response": "def _get_schema(self):\n\t\t\"\"\"\n\t\tReturns a dictionary with the schema for a QuantFigure\n\n\t\t\"\"\"\n\t\td={}\n\t\tlayout_kwargs=dict((_,'') for _ in get_layout_kwargs())\n\t\tfor _ in ('data','layout','theme','panels'):\n\t\t\td[_]={}\n\t\t\tfor __ in eval('__QUANT_FIGURE_{0}'.format(_.upper())):\n\t\t\t\tlayout_kwargs.pop(__,None)\n\t\t\t\td[_][__]=None\n\t\td['layout'].update(annotations=dict(values=[],\n\t\t\t\t\t\t\t\t\t\t\tparams=utils.make_dict_from_list(get_annotation_kwargs())))\n\t\td['layout'].update(shapes=utils.make_dict_from_list(get_shapes_kwargs()))\n\t\t[layout_kwargs.pop(_,None) for _ in get_annotation_kwargs()+get_shapes_kwargs()]\n\t\td['layout'].update(**layout_kwargs)\n\t\treturn d"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_sliced(self,slice,df=None):\n\n\t\tdf=self.df.copy() if df==None else df\n\t\tif type(slice) not in (list,tuple):\n\t\t\traise Exception('Slice must be a tuple two values')\n\t\tif len(slice)!=2:\n\t\t\traise Exception('Slice must be a tuple two values')\n\t\ta,b=slice\n\t\ta=None if a in ('',None) else utils.make_string(a)\n\t\tb=None if b in ('',None) else utils.make_string(b)\n\t\treturn df.ix[a:b]", "response": "Returns a sliced DataFrame"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_resampled(self,rule,how={'ohlc':'last','volume':'sum'},df=None,**kwargs):\n\t\tdf=self.df.copy() if df is None else df\n\t\tif rule==None:\n\t\t\treturn df \n\t\telse:\n\t\t\tif isinstance(how,dict):\n\t\t\t\tif 'ohlc' in how:\n\t\t\t\t\tv=how.pop('ohlc')\n\t\t\t\t\tfor _ in ['open','high','low','close']: \n\t\t\t\t\t\thow[_]=v\n\t\t\t\t_how=how.copy()\n\t\t\t\tfor _ in _how:\n\t\t\t\t\tif _ not in self._d:\n\t\t\t\t\t\tdel how[_]\n\t\t\treturn df.resample(rule=rule,**kwargs).apply(how)", "response": "Returns a resampled DataFrame with the given rule"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(self,**kwargs):\n\t\tif 'columns' in kwargs:\n\t\t\tself._d=ta._ohlc_dict(self.df,columns=kwargs.pop('columns',None))\n\t\tschema=self._get_schema()\n\t\tannotations=kwargs.pop('annotations',None)\n\n\t\tif annotations:\n\t\t\tself.layout['annotations']['values']=utils.make_list(annotations)\n\t\tfor k,v in list(kwargs.items()):\n\t\t\ttry:\n\t\t\t\tutils.dict_update(self.__dict__,k,v,schema)\n\t\t\texcept:\n\t\t\t\tself.kwargs.update({k:v})", "response": "Updates the values for a QuantFigure\n\t"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes the values for a QuantFigure", "response": "def delete(self,*args):\n\t\t\"\"\"\n\t\tDeletes the values for a QuantFigure\n\t\tThe key-values are automatically deleted from the correct \n\t\tsection of the QuantFigure\n\t\t\"\"\"\n\t\tif args:\n\t\t\targs=args[0] if utils.is_list(args[0]) else args\n\t\t\tpath=utils.dict_path(self.__dict__)\n\t\t\tfor _ in args:\n\t\t\t\tif _ in self.__dict__.keys():\n\t\t\t\t\traise Exception('\"{0}\" cannot be deleted'.format(_))\n\n\t\t\tfor a in args:\n\t\t\t\ttry:\n\t\t\t\t\tif a in ('shapes'):\n\t\t\t\t\t\tself.layout[a].clear()\n\t\t\t\t\telif a=='annotations':\n\t\t\t\t\t\tself.layout['annotations']={'values':[],'params':{}}\n\t\t\t\t\telse:\n\t\t\t\t\t\tdel reduce(dict.get, path[a],self.__dict__)[a]\n\t\t\t\texcept:\n\t\t\t\t\traise Exception('Key: {0} not found'.format(a))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the panel domains for each axis.", "response": "def _panel_domains(self,n=2,min_panel_size=.15,spacing=0.08,top_margin=1,bottom_margin=0):\n\t\t\"\"\"\n\t\t\n\t\tReturns the panel domains for each axis\n\n\t\t\"\"\"\n\t\td={}\n\t\tfor _ in range(n+1,1,-1):\n\t\t\tlower=round(bottom_margin+(min_panel_size+spacing)*(n+1-_),2)\n\t\t\td['yaxis{0}'.format(_)]=dict(domain=(lower,lower+min_panel_size))\n\t\ttop=d['yaxis2']['domain']\n\t\td['yaxis2']['domain']=(top[0],top_margin)\n\t\treturn d"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_trendline(self,date0=None,date1=None,on=None,kind='trend',to_strfmt='%Y-%m-%d',from_strfmt='%d%b%y',**kwargs):\n\t\tann_values=copy.deepcopy(get_annotation_kwargs())\n\t\tann_values.extend(['x','y'])\n\t\tann_kwargs=utils.check_kwargs(kwargs,ann_values,{},clean_origin=True)\n\t\tdef position(d0,d1):\n\t\t\treturn d0+(d1-d0)/2\n\n\t\tdate0=kwargs.pop('date',date0)    \n\t\tdate0=date_tools.stringToString(date0,from_strfmt,to_strfmt) if '-' not in date0 else date0\n\t\t\n\t\tif kind=='trend':\n\t\t\tdate1=date_tools.stringToString(date1,from_strfmt,to_strfmt) if '-' not in date1 else date1\n\t\t\ton='close' if not on else on\n\t\t\tdf=pd.DataFrame(self.df[self._d[on]])\n\t\t\ty0=kwargs.get('y0',df.ix[date0].values[0])\n\t\t\ty1=kwargs.get('y1',df.ix[date1].values[0])\n\t\t\t\n\t\t\n\t\tif kind in ('support','resistance'):\n\t\t\tmode=kwargs.pop('mode','starttoend')\n\t\t\tif not on:\n\t\t\t\ton='low' if kind=='support' else 'high'\n\t\t\tdf=pd.DataFrame(self.df[self._d[on]])\n\t\t\ty0=kwargs.get('y0',df.ix[date0].values[0])\n\t\t\ty1=kwargs.get('y1',y0)\n\t\t\tif mode=='starttoend':\n\t\t\t\tdate0=df.index[0]\n\t\t\t\tdate1=df.index[-1]\n\t\t\telif mode=='toend':\n\t\t\t\tdate1=df.index[-1]\n\t\t\telif mode=='fromstart':\n\t\t\t\tdate1=date0\n\t\t\t\tdate0=df.index[0]\n\n\t\tif isinstance(date0,pd.Timestamp):\n\t\t\tdate0=date_tools.dateToString(date0,to_strfmt)\n\t\tif isinstance(date1,pd.Timestamp):\n\t\t\tdate1=date_tools.dateToString(date1,to_strfmt)\n\t\td={'x0':date0,'x1':date1,'y0':y0,'y1':y1}\n\t\td.update(**kwargs)\n\t\tshape=tools.get_shape(**d)        \n\n\t\t\n\t\tif ann_kwargs.get('text',False):\n\t\t\tann_kwargs['x']=ann_kwargs.get('x',date_tools.dateToString(position(date_tools.stringToDate(date0,to_strfmt),date_tools.stringToDate(date1,to_strfmt)),to_strfmt))\n\t\t\tann_kwargs['y']=ann_kwargs.get('y',position(shape['y0'],shape['y1']))\n\t\telse:\n\t\t\tann_kwargs={}\n\t\treturn {'shape':shape,'annotation':ann_kwargs}", "response": "Returns a trendline object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_trendline(self,date0,date1,on='close',text=None,**kwargs):\n\t\td={'kind':'trend','date0':date0,'date1':date1,'on':on,'text':text}\n\t\td.update(**kwargs)\n\t\tself.trendlines.append(d)", "response": "Adds a trendline to the QuantFigure."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a support line to the QuantFigure object.", "response": "def add_support(self,date,on='low',mode='starttoend',text=None,**kwargs):\n\t\t\"\"\"\n\t\tAdds a support line to the QuantFigure\n\n\t\tParameters:\n\t\t\tdate0 : string\n\t\t\t\tThe support line will be drawn at the 'y' level \n\t\t\t\tvalue that corresponds to this date. \n\t\t\ton : string\n\t\t\t\tIndicate the data series in which the \n\t\t\t\tsupport line should be based.\n\t\t\t\t\t'close'\n\t\t\t\t\t'high'\n\t\t\t\t\t'low'\n\t\t\t\t\t'open'\n\t\t\tmode : string\n\t\t\t\tDefines how the support/resistance will \n\t\t\t\tbe drawn\n\t\t\t\t\t'starttoened' : (x0,x1)\n\t\t\t\t\t'fromstart' : (x0,date)\n\t\t\t\t\t'toend' : (date,x1)\n\t\t\ttext : string\n\t\t\t\tIf passed, then an annotation will be added \n\t\t\t\tto the support line (at mid point)\n\t\t\n\t\tkwargs:\t\n\t\t\tfrom_strfmt : string\n\t\t\t\tDefines the date formating in which \n\t\t\t\tdate0 and date1 are stated. \n\t\t\t\tdefault: '%d%b%y'\t\t\t\t\t\n\t\t\tto_strfmt : string\n\t\t\t\tDefines the date formatting \n\t\t\t\tto which it should be converted. \n\t\t\t\tThis should match the same format as the timeseries index. \n\t\t\t\tdefault : '%Y-%m-%d'\t\t\n\t\t\"\"\"\n\t\td={'kind':'support','date':date,'mode':mode,'on':on,'text':text}\n\t\td.update(**kwargs)\n\t\tself.trendlines.append(d)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds an annotation to the QuantFigure.", "response": "def add_annotations(self,annotations,**kwargs):\n\t\t\"\"\"\n\t\tAdd an annotation to the QuantFigure. \n\n\t\tParameters:\n\t\t\tannotations : dict or list(dict,)\n\t\t\t\tAnnotations can be on the form form of \n\t\t\t\t\t{'date' : 'text'}\n\t\t\t\t\tand the text will automatically be placed at the \n\t\t\t\t\tright level on the chart \n\t\t\t\tor\n\t\t\t\t\tA Plotly fully defined annotation\n\n\t\tkwargs : \n\t\t\tfontcolor : str\n\t\t\t\tText color for annotations\n\t\t\tfontsize : int\n\t\t\t\tText size for annotations\n\t\t\ttextangle : int\n\t\t\t\tTextt angle \n\t\t\tSee https://plot.ly/python/reference/#layout-annotations \n\t\t\tfor a complete list of valid parameters.\n\n\t\t\"\"\"\n\t\tann_kwargs=utils.check_kwargs(kwargs,get_annotation_kwargs(),{},clean_origin=True)\n\t\tif type(annotations)==list:\n\t\t\tself.layout['annotations']['values'].extend(annotations)\n\t\telse:\n\t\t\tself.layout['annotations']['values'].append(annotations)\n\t\tif ann_kwargs:\n\t\t\tself.layout['annotations']['params'].update(**ann_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a shape to the QuantFigure.", "response": "def add_shapes(self,**kwargs):\n\t\t\"\"\"\n\t\tAdd a shape to the QuantFigure. \n\n\t\tkwargs : \n\t\t\thline : int, list or dict\n\t\t\t\tDraws a horizontal line at the\n\t\t\t\tindicated y position(s)\n\t\t\t\tExtra parameters can be passed in\n\t\t\t\tthe form of a dictionary (see shapes)\n\t\t\tvline : int, list or dict\n\t\t\t\tDraws a vertical line at the\n\t\t\t\tindicated x position(s)\n\t\t\t\tExtra parameters can be passed in\n\t\t\t\tthe form of a dictionary (see shapes)\n\t\t\thspan : (y0,y1)\n\t\t\t\tDraws a horizontal rectangle at the\n\t\t\t\tindicated (y0,y1) positions.\n\t\t\t\tExtra parameters can be passed in\n\t\t\t\tthe form of a dictionary (see shapes)\n\t\t\tvspan : (x0,x1)\n\t\t\t\tDraws a vertical rectangle at the\n\t\t\t\tindicated (x0,x1) positions.\n\t\t\t\tExtra parameters can be passed in\n\t\t\t\tthe form of a dictionary (see shapes)\n\t\t\tshapes : dict or list(dict)\n\t\t\t\tList of dictionaries with the\n\t\t\t\tspecifications of a given shape.\n\t\t\t\tSee help(cufflinks.tools.get_shape)\n\t\t\t\tfor more information\n\t\t\t\n\t\t\"\"\"\n\t\tkwargs=utils.check_kwargs(kwargs,get_shapes_kwargs(),{},clean_origin=True)\n\t\tfor k,v in list(kwargs.items()):\n\t\t\tif k in self.layout['shapes']:\n\t\t\t\tif utils.is_list(v):\n\t\t\t\t\tself.layout['shapes'][k].extend(v)\t\n\t\t\t\telse:\n\t\t\t\t\tself.layout['shapes'][k].append(v)\n\t\t\telse:\n\t\t\t\tself.layout['shapes'][k]=utils.make_list(v)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _add_study(self,study):\n\t\tstr='{study} {name}({period})' if study['params'].get('str',None)==None else study['params']['str']\n\t\tstudy['params']['str']=str\n\n\t\tif not study['name']:\n\t\t\tstudy['name']=ta.get_column_name(study['kind'].upper(),study=study['kind'],\n\t\t\t\t\t\t\t\t\t\t\t\tstr=str,\n\t\t\t\t\t\t\t\t\t\t\t\tperiod=study['params'].get('periods',None),\n\t\t\t\t\t\t\t\t\t\t\t\tcolumn=study['params'].get('column',None))\n\t\t\t\t\n\n\t\trestore=study['display'].pop('restore',False)\n\t\t\n\t\tif restore:\n\t\t\t_=self.studies.pop(study['kind'],None)\n\n\t\tif study['kind'] in self.studies:\n\t\t\ttry:\n\t\t\t\tid='{0} ({1})'.format(study['kind'],study['params']['periods'])\n\t\t\texcept:\n\t\t\t\tid='{0} ({1})'.format(study['kind'],'(2)')\n\t\telse:\n\t\t\tid=study['kind']\n\n\t\t_id=id\n\t\tn=1\n\t\twhile id in self.studies:\n\t\t\tid='{0} ({1})'.format(_id,n)\n\t\t\tn+=1\n\t\tself.studies[id]=study", "response": "Adds a study to QuantFigure. studyies\n\tif"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_volume(self,colorchange=True,column=None,name='',str='{name}',**kwargs):\n\t\tif not column:\n\t\t\tcolumn=self._d['volume']\n\t\tup_color=kwargs.pop('up_color',self.theme['up_color'])\n\t\tdown_color=kwargs.pop('down_color',self.theme['down_color'])\n\t\tstudy={'kind':'volume',\n\t\t\t   'name':name,\n\t\t\t   'params':{'colorchange':colorchange,'base':'close','column':column,\n\t\t\t\t\t\t 'str':None},\n\t\t\t  'display':utils.merge_dict({'up_color':up_color,'down_color':down_color},kwargs)}\n\t\tself._add_study(study)", "response": "Add a volume study to QuantFigure. studies\n\t"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a MACD study to QuantFigure. studies", "response": "def add_macd(self,fast_period=12,slow_period=26,signal_period=9,column=None,\n\t\t\t\t name='',str=None,**kwargs):\n\t\t\"\"\"\n\t\tAdd Moving Average Convergence Divergence (MACD) study to QuantFigure.studies\n\n\t\tParameters:\n\t\t\tfast_period : int\n\t\t\t\tMACD Fast Period\n\t\t\tslow_period : int\n\t\t\t\tMACD Slow Period\n\t\t\tsignal_period : int\n\t\t\t\tMACD Signal Period\n\t\t\tcolumn :string\n\t\t\t\tDefines the data column name that contains the \n\t\t\t\tdata over which the study will be applied. \n\t\t\t\tDefault: 'close'\n\t\t\tname : string\n\t\t\t\tName given to the study\n\t\t\tstr : string\n\t\t\t\tLabel factory for studies\n\t\t\t\tThe following wildcards can be used:\n\t\t\t\t\t{name} : Name of the column\n\t\t\t\t\t{study} : Name of the study\n\t\t\t\t\t{period} : Period used\n\t\t\t\tExamples:\n\t\t\t\t\t'study: {study} - period: {period}'\n\t\tkwargs: \n\t\t\tlegendgroup : bool\n\t\t\t\tIf true, all legend items are grouped into a \n\t\t\t\tsingle one\n\t\t\tAll formatting values available on iplot()\n\t\t\"\"\"\n\n\t\tif not column:\n\t\t\tcolumn=self._d['close']\n\t\tstudy={'kind':'macd',\n\t\t\t   'name':name,\n\t\t\t   'params':{'fast_period':fast_period,'slow_period':slow_period,\n\t\t\t\t\t\t 'signal_period':signal_period,'column':column,\n\t\t\t\t\t\t 'str':str},\n\t\t\t  'display':utils.merge_dict({'legendgroup':False,'colors':['blue','red']},kwargs)}\n\t\tstudy['params']['periods']='[{0},{1},{2}]'.format(fast_period,slow_period,signal_period)\n\t\tself._add_study(study)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a simple moving average study to QuantFigure. studies", "response": "def add_sma(self,periods=20,column=None,name='',\n\t\t\t\t\tstr=None,**kwargs):\n\t\t\"\"\"\n\t\tAdd Simple Moving Average (SMA) study to QuantFigure.studies\n\n\t\tParameters:\n\t\t\tperiods : int or list(int)\n\t\t\t\tNumber of periods\n\t\t\tcolumn :string\n\t\t\t\tDefines the data column name that contains the \n\t\t\t\tdata over which the study will be applied. \n\t\t\t\tDefault: 'close'\n\t\t\tname : string\n\t\t\t\tName given to the study\n\t\t\tstr : string\n\t\t\t\tLabel factory for studies\n\t\t\t\tThe following wildcards can be used:\n\t\t\t\t\t{name} : Name of the column\n\t\t\t\t\t{study} : Name of the study\n\t\t\t\t\t{period} : Period used\n\t\t\t\tExamples:\n\t\t\t\t\t'study: {study} - period: {period}'\n\t\tkwargs: \n\t\t\tlegendgroup : bool\n\t\t\t\tIf true, all legend items are grouped into a \n\t\t\t\tsingle one\n\t\t\tAll formatting values available on iplot()\n\t\t\"\"\"\n\t\tif not column:\n\t\t\tcolumn=self._d['close']\n\t\tstudy={'kind':'sma',\n\t\t\t   'name':name,\n\t\t\t   'params':{'periods':periods,'column':column,\n\t\t\t\t\t\t 'str':str},\n\t\t\t  'display':utils.merge_dict({'legendgroup':False},kwargs)}\n\t\tself._add_study(study)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_rsi(self,periods=20,rsi_upper=70,rsi_lower=30,showbands=True,column=None,\n\t\t\t\t\t\t   name='',str=None,**kwargs):\n\t\t\"\"\"\n\t\tAdd Relative Strength Indicator (RSI) study to QuantFigure.studies\n\n\t\tParameters:\n\t\t\tperiods : int or list(int)\n\t\t\t\tNumber of periods\n\t\t\trsi_upper : int \n\t\t\t\tbounds [0,100]\n\t\t\t\tUpper (overbought) level\n\t\t\trsi_lower : int\n\t\t\t\tbounds [0,100]\n\t\t\t\tLower (oversold) level\n\t\t\tshowbands : boolean\n\t\t\t\tIf True, then the rsi_upper and\n\t\t\t\trsi_lower levels are displayed\n\t\t\tcolumn :string\n\t\t\t\tDefines the data column name that contains the \n\t\t\t\tdata over which the study will be applied. \n\t\t\t\tDefault: 'close'\n\t\t\tname : string\n\t\t\t\tName given to the study\n\t\t\tstr : string\n\t\t\t\tLabel factory for studies\n\t\t\t\tThe following wildcards can be used:\n\t\t\t\t\t{name} : Name of the column\n\t\t\t\t\t{study} : Name of the study\n\t\t\t\t\t{period} : Period used\n\t\t\t\tExamples:\n\t\t\t\t\t'study: {study} - period: {period}'\n\t\tkwargs: \n\t\t\tlegendgroup : bool\n\t\t\t\tIf true, all legend items are grouped into a \n\t\t\t\tsingle one\n\t\t\tAll formatting values available on iplot()\n\t\t\"\"\"\n\t\tif not column:\n\t\t\tcolumn=self._d['close']\n\t\tstr=str if str else '{name}({column},{period})'\n\n\t\tstudy={'kind':'rsi',\n\t\t\t   'name':name,\n\t\t\t   'params':{'periods':periods,'column':column,\n\t\t\t\t\t\t 'str':str},\n\t\t\t  'display':utils.merge_dict({'legendgroup':True,'rsi_upper':rsi_upper,\n\t\t\t\t\t\t 'rsi_lower':rsi_lower,'showbands':showbands},kwargs)}\n\t\tself._add_study(study)", "response": "Add Relative Strength Indicator to QuantFigure. studies\n\t"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_bollinger_bands(self,periods=20,boll_std=2,fill=True,column=None,name='',\n\t\t\t\t\t\t   str='{name}({column},{period})',**kwargs):\n\t\t\"\"\"\n\t\tAdd Bollinger Bands (BOLL) study to QuantFigure.studies\n\n\t\tParameters:\n\t\t\tperiods : int or list(int)\n\t\t\t\tNumber of periods\n\t\t\tboll_std : int\n\t\t\t\tNumber of standard deviations for\n\t\t\t\tthe bollinger upper and lower bands\n\t\t\tfill : boolean\n\t\t\t\tIf True, then the innner area of the \n\t\t\t\tbands will filled\n\t\t\tcolumn :string\n\t\t\t\tDefines the data column name that contains the \n\t\t\t\tdata over which the study will be applied. \n\t\t\t\tDefault: 'close'\n\t\t\tname : string\n\t\t\t\tName given to the study\n\t\t\tstr : string\n\t\t\t\tLabel factory for studies\n\t\t\t\tThe following wildcards can be used:\n\t\t\t\t\t{name} : Name of the column\n\t\t\t\t\t{study} : Name of the study\n\t\t\t\t\t{period} : Period used\n\t\t\t\tExamples:\n\t\t\t\t\t'study: {study} - period: {period}'\n\t\tkwargs: \n\t\t\tlegendgroup : bool\n\t\t\t\tIf true, all legend items are grouped into a \n\t\t\t\tsingle one\n\t\t\tfillcolor : string\n\t\t\t\tColor to be used for the fill color.\n\t\t\t\tExample:\n\t\t\t\t\t'rgba(62, 111, 176, .4)'\n\t\t\tAll formatting values available on iplot()\n\t\t\"\"\"\n\t\tif not column:\n\t\t\tcolumn=self._d['close']\n\t\tstudy={'kind':'boll',\n\t\t\t   'name':name,\n\t\t\t   'params':{'periods':periods,'boll_std':boll_std,'column':column,\n\t\t\t\t\t\t'str':str},\n\t\t\t  'display':utils.merge_dict({'legendgroup':True,'fill':fill},kwargs)}\n\t\tself._add_study(study)", "response": "Adds a Bollinger Bands to the QuantFigure. studies\n\t"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_cci(self,periods=14,cci_upper=100,cci_lower=-100,\n\t\t\t    showbands=True,str=None,name='',**kwargs):\n\t\t\"\"\"\n\t\tCommodity Channel Indicator study to QuantFigure.studies\n\n\t\tParameters:\n\t\t\tperiods : int or list(int)\n\t\t\t\tNumber of periods\n\t\t\tcci_upper : int\n\t\t\t\tUpper bands level\n\t\t\t\tdefault : 100\n\t\t\tcci_lower : int\n\t\t\t\tLower band level\n\t\t\t\tdefault : -100\n\t\t\tshowbands : boolean\n\t\t\t\tIf True, then the cci_upper and\n\t\t\t\tcci_lower levels are displayed\n\t\t\tname : string\n\t\t\t\tName given to the study\n\t\t\tstr : string\n\t\t\t\tLabel factory for studies\n\t\t\t\tThe following wildcards can be used:\n\t\t\t\t\t{name} : Name of the column\n\t\t\t\t\t{study} : Name of the study\n\t\t\t\t\t{period} : Period used\n\t\t\t\tExamples:\n\t\t\t\t\t'study: {study} - period: {period}'\n\t\tkwargs: \n\t\t\tlegendgroup : bool\n\t\t\t\tIf true, all legend items are grouped into a \n\t\t\t\tsingle one\n\t\t\tAll formatting values available on iplot()\n\t\t\"\"\"\n\t\tstudy={'kind':'cci',\n\t\t\t   'name':name,\n\t\t\t   'params':{'periods':periods,'high':self._d['high'],'low':self._d['low'],'close':self._d['close'],\n\t\t\t\t\t\t 'str':str},\n\t\t\t  'display':utils.merge_dict({'legendgroup':True,'cci_upper':cci_upper,\n\t\t\t\t\t\t 'cci_lower':cci_lower,'showbands':showbands},kwargs)}\n\t\tself._add_study(study)", "response": "Add a new CCI column to QuantFigure. studies\n\t\t\t\t"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_ptps(self,periods=14,af=0.2,initial='long',str=None,name='',**kwargs):\n\t\tstudy={'kind':'ptps',\n\t\t\t   'name':name,\n\t\t\t   'params':{'periods':periods,'high':self._d['high'],'low':self._d['low'],'af':af,'initial':initial,\n\t\t\t\t\t\t 'str':str},\n\t\t\t  'display':utils.merge_dict({'legendgroup':False},kwargs)}\n\t\tself._add_study(study)", "response": "This method adds a Parabolic SAR study to QuantFigure. studies\n\t"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds Average True Range (ATR) study to QuantFigure.studies Parameters: periods : int or list(int) Number of periods name : string Name given to the study str : string Label factory for studies The following wildcards can be used: {name} : Name of the column {study} : Name of the study {period} : Period used Examples: 'study: {study} - period: {period}' kwargs: legendgroup : bool If true, all legend items are grouped into a single one All formatting values available on iplot()", "response": "def add_atr(self,periods=14,str=None,name='',**kwargs):\n\t\t\"\"\"\n\t\tAdd Average True Range (ATR) study to QuantFigure.studies\n\n\t\tParameters:\n\t\t\tperiods : int or list(int)\n\t\t\t\tNumber of periods\n\t\t\tname : string\n\t\t\t\tName given to the study\n\t\t\tstr : string\n\t\t\t\tLabel factory for studies\n\t\t\t\tThe following wildcards can be used:\n\t\t\t\t\t{name} : Name of the column\n\t\t\t\t\t{study} : Name of the study\n\t\t\t\t\t{period} : Period used\n\t\t\t\tExamples:\n\t\t\t\t\t'study: {study} - period: {period}'\n\t\tkwargs: \n\t\t\tlegendgroup : bool\n\t\t\t\tIf true, all legend items are grouped into a \n\t\t\t\tsingle one\n\t\t\tAll formatting values available on iplot()\n\t\t\"\"\"\n\t\tstudy={'kind':'atr',\n\t\t\t   'name':name,\n\t\t\t   'params':{'periods':periods,'high':self._d['high'],'low':self._d['low'],'close':self._d['close'],\n\t\t\t\t\t\t 'str':str},\n\t\t\t  'display':utils.merge_dict({'legendgroup':False},kwargs)}\n\t\tself._add_study(study)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef go_offline(connected=None):\n    from .auth import get_config_file\n    if connected is None:\n        try:\n            connected=True if get_config_file()['offline_connected'] is None else get_config_file()['offline_connected']\n        except:\n            connected=True\n    if run_from_ipython():\n        try:\n            py_offline.init_notebook_mode(connected)\n        except TypeError:\n            #For older versions of plotly\n            py_offline.init_notebook_mode()\n        py_offline.__PLOTLY_OFFLINE_INITIALIZED=True", "response": "connected : bool\n        If True, the plotly.js library will be loaded\n        from an online CDN. If False, the plotly.js library will be loaded locally\n        from the plotly python package"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _screen(self,include=True,**kwargs):\n\tdf=self.copy()\n\tfor k,v in list(kwargs.items()):\n\t\tv=[v] if type(v)!=list else v\n\t\tif include:\n\t\t\tdf=df[df[k].str.contains('|'.join(v),flags=re.IGNORECASE).fillna(False)]\n\t\telse:\n\t\t\tdf=df[df[k].str.contains('|'.join(v),flags=re.IGNORECASE).fillna(False)==False]\n\treturn df", "response": "Returns a DataFrame that contains the given strings in the screen."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a series with the bestfit values.", "response": "def bestfit(self):\n\t\"\"\"\n\tReturns a series with the bestfit values. \n\t\n\tExample:\n\t\tSeries.bestfit()\n\n\tReturns: series\n\t\tThe returned series contains a parameter \n\t\tcalled 'formula' which includes the string representation \n\t\tof the bestfit line. \n\t\"\"\"\n\t# statsmodel cannot be included on requirements.txt\n\t# see https://github.com/scikit-learn/scikit-learn/issues/4164\n\t# which shares the same issue as statsmodel\n\ttry:\n\t\timport statsmodels.api as sm\n\texcept:\n\t\traise Exception(\"statsmodels is required: \" \\\n\t\t\t\t\t\t\"please run \" \\\n\t\t\t\t\t\t\"pip install statsmodels\" )\n\n\tx=pd.Series(list(range(1,len(self)+1)),index=self.index)\n\tx=sm.add_constant(x)\n\tmodel=sm.OLS(self,x)\n\tfit=model.fit()\n\tvals=fit.params.values\n\tbest_fit=fit.fittedvalues\n\t# the below methods have been deprecated in Pandas\n\t# model=pd.ols(x=x,y=self,intercept=True)\n\t# best_fit=model.y_fitted\n\tbest_fit.formula='%.2f*x+%.2f' % (vals[0],vals[1])\n\treturn best_fit"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a normalized series or DataFrame", "response": "def normalize(self,asOf=None,multiplier=100):\n\t\"\"\"\n\tReturns a normalized series or DataFrame\n\t\n\tExample:\n\t\tSeries.normalize()\n\n\tReturns: series of DataFrame\n\t\n\tParameters:\n\t-----------\n\t\tasOf : string\n\t\t\tDate format\n\t\t\t'2015-02-29'\n\t\tmultiplier : int\n\t\t\tFactor by which the results will be adjusted\n\t\"\"\"\n\tif not asOf:\n\t\tx0=self.ix[0]\n\telse:\n\t\tx0=self.ix[asOf]\n\treturn self/x0*multiplier"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pp(el,preString=''):\n\ttab=' '*4\n\tif isinstance(el,dict):\n\t\tkeys=list(el.keys())\n\t\tkeys.sort()\n\t\tfor key in keys:\n\t\t\tval=el[key]\n\t\t\tif isinstance(val,dict) or isinstance(val,list):\n\t\t\t\tprint('%s%s :' % (preString,key))\n\t\t\t\tpp(val,preString+tab)\n\t\t\telse:\n\t\t\t\tprint('%s%s  =  %s' % (preString,key,val))\n\t\n\telif isinstance(el,list):\n\t\tprint(preString+tab+'[')\n\t\tpreString+=tab\t\n\t\tfor _ in el:\n\t\t\tif isinstance(_,dict):\n\t\t\t\tprint (preString+tab+'{')\n\t\t\t\tpp(_,preString+tab*2)\n\t\t\t\tprint(preString+tab+'}')\n\t\t\telif isinstance(_,list):\n\t\t\t\tprint(preString+tab+'[')\n\t\t\t\tpp(_,preString+tab)\n\t\t\t\tprint(preString+tab+']')\n\t\t\telse:\n\t\t\t\tpp(_,preString+tab)\n\t\t\t#print preString+'      '+str('-')*10            \n\t\tprint(preString\t+']')\n\telse:\n\t\tprint(preString+str(el))", "response": "prettyprint a concatenated dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dictionay indexed by values { value_k : key_k }", "response": "def inverseDict(d):\n\t\"\"\"\n\tReturns a dictionay indexed by values {value_k:key_k}\n\tParameters:\n\t-----------\n\t\td : dictionary\n\t\"\"\"\n\tdt={}\n\tfor k,v in list(d.items()):\n\t\tif type(v) in (list,tuple):\n\t\t\tfor i in v:\n\t\t\t\tdt[i]=k\n\t\telse:\n\t\t\tdt[v]=k\n\treturn dt"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlooks for keys of the format keyword_value. And return a dictionary with {keyword:value} format Parameters: ----------- from_kwargs : dict Original dictionary to_kwargs : dict Dictionary where the items will be appended keyword : string Keyword to look for in the orginal dictionary clean_origin : bool If True then the k,v pairs from the original dictionary are deleted", "response": "def kwargs_from_keyword(from_kwargs,to_kwargs,keyword,clean_origin=True):\n\t\"\"\"\n\tLooks for keys of the format keyword_value. \n\tAnd return a dictionary with {keyword:value} format\n\n\tParameters:\n\t-----------\n\t\tfrom_kwargs : dict\n\t\t\tOriginal dictionary\n\t\tto_kwargs : dict\n\t\t\tDictionary where the items will be appended\n\t\tkeyword : string\n\t\t\tKeyword to look for in the orginal dictionary\n\t\tclean_origin : bool\n\t\t\tIf True then the k,v pairs from the original \n\t\t\tdictionary are deleted\n\t\"\"\"\n\tfor k in list(from_kwargs.keys()):\n\t\tif '{0}_'.format(keyword) in k:\n\t\t\tto_kwargs[k.replace('{0}_'.format(keyword),'')]=from_kwargs[k]\n\t\t\tif clean_origin:\n\t\t\t\tdel from_kwargs[k]\n\treturn to_kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef deep_update(d,d_update):\n\tfor k,v in list(d_update.items()):\n\t\tif isinstance(v,dict):\n\t\t\tif k in d:\n\t\t\t\tdeep_update(d[k],v)\n\t\t\telse:\n\t\t\t\td[k]=v\n\t\telif isinstance(d,list):\n\t\t\td.append({k:v})\n\t\telse:\n\t\t\td[k]=v\n\treturn d", "response": "Updates the values of a given dictionary d with the values of d_update."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a google sheet from the specified URL.", "response": "def read_google(self,url,**kwargs):\n\t\"\"\"\n\tReads a google sheet\n\t\"\"\"\n\tif url[-1]!='/':\n\t\turl+='/'\n\treturn self.read_csv(url+'export?gid=0&format=csv',**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a string that represents a date n numbers of days from today.", "response": "def getDateFromToday(delta,strfmt='%Y%m%d'):\n\t\"\"\" Returns a string that represents a date n numbers of days from today.\n\tParameters:\n\t-----------\n\t\tdelta : int \n\t\t\tnumber of days\n\t\tstrfmt : string\n\t\t\tformat in which the date will be represented\n\t\"\"\"\n\treturn (dt.date.today() + dt.timedelta(delta)).strftime(strfmt)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dictionary with the actual column names that correspond to each of the OHLCV values.", "response": "def _ohlc_dict(df_or_figure,open='',high='',low='',close='',volume='',\n\t\t\t   validate='',**kwargs):\n\t\"\"\"\n\tReturns a dictionary with the actual column names that \n\tcorrespond to each of the OHLCV values.\n\n\tdf_or_figure :  DataFrame or Figure\n\topen : string\n\t\tColumn name to be used for OPEN values\n\thigh : string\n\t\tColumn name to be used for HIGH values\n\tlow : string\n\t\tColumn name to be used for LOW values\n\tclose : string\n\t\tColumn name to be used for CLOSE values\n\tvolume : string\n\t\tColumn name to be used for VOLUME values\n\tvalidate : string\n\t\tValidates that the stated column exists\n\t\tExample:\n\t\t\tvalidate='ohv' | Will ensure Open, High\n\t\t\t\t\t\t\t and close values exist. \n\t\"\"\"\n\tc_dir={}\n\tohlcv=['open','high','low','close','volume']\n\tif type(df_or_figure)==pd.DataFrame:\n\t\tcnames=df_or_figure.columns\n\telif type(df_or_figure)==Figure or type(df_or_figure) == dict:\n\t\tcnames=df_or_figure.axis['ref'].keys()\n\telif type(df_or_figure)==pd.Series:\n\t\tcnames=[df_or_figure.name]\n\tc_min=dict([(v.lower(),v) for v in cnames])\n\tfor _ in ohlcv:\n\t\tif _ in c_min.keys():\n\t\t\tc_dir[_]=c_min[_]\n\t\telse:\n\t\t\tfor c in cnames:\n\t\t\t\tif _ in c.lower():\n\t\t\t\t\tc_dir[_]=c\n\n\tif open:\n\t\tc_dir['open']=open\n\tif high:\n\t\tc_dir['high']=high\n\tif low:\n\t\tc_dir['low']=low\n\tif close:\n\t\tc_dir['close']=close\n\tif volume:\n\t\tc_dir['volume']=volume\n\t\t\n\tfor v in list(c_dir.values()):\n\t\tif v not in cnames:\n\t\t\traise StudyError('{0} is not a valid column name'.format(v))\n\n\tif validate:\n\t\t\terrs=[]\n\t\t\tval=validate.lower()\n\t\t\ts_names=dict([(_[0],_) for _ in ohlcv])\n\t\t\tcols=[_[0] for _ in c_dir.keys()]\n\t\t\tfor _ in val:\n\t\t\t\tif _ not in cols:\n\t\t\t\t\terrs.append(s_names[_])\n\t\t\tif errs:\n\t\t\t\traise StudyError('Missing Columns: {0}'.format(', '.join(errs)))\n\n\treturn c_dir"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a DataFrame with the data from the scattergeo. csv file", "response": "def scattergeo():\n\t\"\"\"\n\tReturns \n\t\"\"\"\n\tpath=os.path.join(os.path.dirname(__file__), '../data/scattergeo.csv')\n\tdf=pd.read_csv(path)\n\tdel df['Unnamed: 0']\n\tdf['text'] = df['airport'] + ' ' + df['city'] + ', ' + df['state'] + ' ' + 'Arrivals: ' + df['cnt'].astype(str)\n\tdf=df.rename(columns={'cnt':'z','long':'lon'})\n\treturn df"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef choropleth():\n\tpath=os.path.join(os.path.dirname(__file__), '../data/choropleth.csv')\n\tdf=pd.read_csv(path)\n\tdel df['Unnamed: 0']\n\tdf['z']=[np.random.randint(0,100) for _ in range(len(df))]\n\treturn df", "response": "Returns a DataFrame with the choropleth data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a DataFrame with the required format for a pie plot Parameters :", "response": "def pie(n_labels=5,mode=None):\n\t\"\"\"\n\tReturns a DataFrame with the required format for \n\ta pie plot\n\n\tParameters:\n\t-----------\n\t\tn_labels : int\n\t\t\tNumber of labels \n\t\tmode : string\n\t\t\tFormat for each item\n\t\t\t\t'abc' for alphabet columns\n\t\t\t\t'stocks' for random stock names\n\t\"\"\"\t\n\treturn pd.DataFrame({'values':np.random.randint(1,100,n_labels),\n\t\t\t\t\t\t 'labels':getName(n_labels,mode=mode)})"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a DataFrame with the required format for a scatter plot", "response": "def scatter(n_categories=5,n=10,prefix='category',mode=None):\n\t\"\"\"\n\tReturns a DataFrame with the required format for \n\ta scatter plot\n\n\tParameters:\n\t-----------\n\t\tn_categories : int\n\t\t\tNumber of categories \n\t\tn : int\n\t\t\tNumber of points for each category\n\t\tprefix : string\n\t\t\tName for each category\n\t\tmode : string\n\t\t\tFormat for each item\n\t\t\t\t'abc' for alphabet columns\n\t\t\t\t'stocks' for random stock names\n\t\"\"\"\t\n\tcategories=[]\n\tfor i in range(n_categories):\n\t\tcategories.extend([prefix+str(i+1)]*n)\n\treturn pd.DataFrame({'x':np.random.randn(n*n_categories),\n\t\t\t\t\t\t 'y':np.random.randn(n*n_categories),\n\t\t\t\t\t\t 'text':getName(n*n_categories,mode=mode),\n\t\t\t\t\t\t 'categories':categories})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a DataFrame with the required format for a heatmap plot", "response": "def heatmap(n_x=5,n_y=10):\n\t\"\"\"\n\tReturns a DataFrame with the required format for \n\ta heatmap plot\n\n\tParameters:\n\t-----------\n\t\tn_x : int\n\t\t\tNumber of x categories\n\t\tn_y : int\n\t\t\tNumber of y categories\n\t\"\"\"\t\n\tx=['x_'+str(_) for _ in range(n_x)]\n\ty=['y_'+str(_) for _ in range(n_y)]\n\treturn pd.DataFrame(surface(n_x-1,n_y-1).values,index=x,columns=y)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a DataFrame with the required format for a scatter plot", "response": "def lines(n_traces=5,n=100,columns=None,dateIndex=True,mode=None):\n\t\"\"\"\n\tReturns a DataFrame with the required format for \n\ta scatter (lines) plot\n\n\tParameters:\n\t-----------\n\t\tn_traces : int\n\t\t\tNumber of traces \n\t\tn : int\n\t\t\tNumber of points for each trace\n\t\tcolumns : [str]\n\t\t\tList of column names\n\t\tdateIndex : bool\n\t\t\tIf True it will return a datetime index\n\t\t\tif False it will return a enumerated index\n\t\tmode : string\n\t\t\tFormat for each item\n\t\t\t\t'abc' for alphabet columns\n\t\t\t\t'stocks' for random stock names\n\t\"\"\"\t\n\tindex=pd.date_range('1/1/15',periods=n) if dateIndex else list(range(n))\n\tdf=pd.DataFrame(np.random.randn(n,n_traces),index=index,\n\t\tcolumns=getName(n_traces,columns=columns,mode=mode))\n\treturn df.cumsum()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bars(n=3,n_categories=3,prefix='category',columns=None,mode='abc'):\n\tcategories=[]\n\tif not columns:\n\t\tcolumns=getName(n,mode=mode)\n\tfor i in range(n_categories):\n\t\tcategories.extend([prefix+str(i+1)])\n\tdata=dict([(x,np.random.randint(1,100,n_categories)) for x in columns])\n\treturn pd.DataFrame(data,index=categories)", "response": "Returns a DataFrame with the required format for \n\ta bar plot"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a DataFrame with the required format for a candlestick or ohlc plot", "response": "def ohlc(n=100):\n\t\"\"\"\n\tReturns a DataFrame with the required format for \n\ta candlestick or ohlc plot\n\tdf[['open','high','low','close']]\n\n\tParameters:\n\t-----------\n\t\tn : int\n\t\t\tNumber of ohlc points\n\t\t\n\t\"\"\"\t\n\tindex=pd.date_range('1/1/15',periods=n*288,freq='5min',tz='utc')\n\tdata=np.random.randn(n*288)\n\tdata[0]=np.array([100])\n\tdf=pd.DataFrame(data,index=index,\n\t\tcolumns=['a'])\n\tdf=df.cumsum()  \n\tdf=df.resample('1d').ohlc()\n\tdf.index=df.index.date\n\tdf.index=pd.to_datetime(df.index)\n\treturn df['a']"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ohlcv(n=100):\n\tdf=ohlc()\n\tdf['volume']=[np.random.randint(1000,10000) for _ in range(len(df))]\n\treturn df", "response": "Returns a DataFrame with the required format for \n\ta candlestick or ohlc plot"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef box(n_traces=5,n=100,mode=None):\n\tdf=pd.DataFrame([np.random.chisquare(np.random.randint(2,10),n_traces) for _ in range(n)],\n\t\tcolumns=getName(n_traces,mode=mode))\n\treturn df", "response": "Returns a DataFrame with the required format for \n\ta box plot\n\tn"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef histogram(n_traces=1,n=500,dispersion=2,mode=None):\n\tdf=pd.DataFrame(np.transpose([np.random.randn(n)+np.random.randint(-1*dispersion,dispersion) for _ in range(n_traces)]),\n\t\tcolumns=getName(n_traces,mode=mode))                     \n\treturn df", "response": "Returns a DataFrame with the required format for \n\ta histogram plot"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef distplot(n_traces=1,n=500,dispersion=3,mode=None):\n\treturn histogram(n_traces,n,dispersion,mode)", "response": "Returns a DataFrame with the required format for \n\ta distribution plot"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef violin(n=500,dispersion=3,categories=True,n_categories=5):\n\tdf = histogram(1,n,dispersion,'abc')\n\tdf=df.rename(columns={'a':'data'})\n\tif categories:\n\t\tdf['categories']=['category_{0}'.format(np.random.randint(n_categories)) for _ in range(n)]\n\treturn df", "response": "Returns a DataFrame with the required format for INDRA points in the distribution plot."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a DataFrame with the required format for a surface plot", "response": "def surface(n_x=20,n_y=20):\n\t\"\"\"\n\tReturns a DataFrame with the required format for \n\ta surface plot\n\n\tParameters:\n\t-----------\n\t\tn_x : int\n\t\t\tNumber of points along the X axis\n\t\tn_y : int\n\t\t\tNumber of points along the Y axis\n\t\"\"\"\t\n\tx=[float(np.random.randint(0,100))]\n\tfor i in range(n_x):\n\t\tx.append(x[:1][0]+np.random.randn()*np.random.randint(1,10))\n\tdf=pd.DataFrame(x)\n\tfor i in range(n_y):\n\t\tdf[i+1]=df[i].map(lambda x:x+np.random.randn()*np.random.randint(1,10))\n\treturn df"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sinwave(n=4,inc=.25):\n\tx=np.arange(-n,n,inc)\n\ty=np.arange(-n,n,inc)\n\tX,Y=np.meshgrid(x,y)\n\tR = np.sqrt(X**2 + Y**2)\n\tZ = np.sin(R)/(.5*R)\n\treturn pd.DataFrame(Z,index=x,columns=y)", "response": "Returns a DataFrame with the required format for \n\ta surface ( sine wave ) plot."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a theme definition.", "response": "def getTheme(theme=None):\n\t\"\"\"\n\tReturns a theme definition.\n\n\tTo see the colors translated (hex) use\n\tcufflinks.getLayout(theme) instead.\n\t\"\"\"\n\tif not theme:\n\t\ttheme = auth.get_config_file()['theme']\n\n\tif theme in THEMES:\n\t\treturn updateColors(copy.deepcopy(THEMES[theme]))\n\telse:\n\t\traise Exception(\"Invalid Theme: {0}\".format(theme))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getLayout(kind=None,theme=None,title='',xTitle='',yTitle='',zTitle='',barmode='',bargap=None,bargroupgap=None,\n\t\t\t  margin=None, dimensions=None, width=None, height=None,\n\t\t\t  annotations=None,is3d=False,**kwargs):\n\t\"\"\"\n\tGenerates a plotly Layout\n\n\tParameters:\n\t-----------\n\t\ttheme : string\n\t\t\tLayout Theme\n\t\t\t\tsolar\n\t\t\t\tpearl\n\t\t\t\twhite\n\t\ttitle : string\n\t\t\tChart Title\n\t\txTitle : string\n\t\t\tX Axis Title\n\t\tyTitle : string\n\t\t\tY Axis Title\n\t\tzTitle : string\n\t\t\tZ Axis Title\n\t\t\tApplicable only for 3d charts\n\t\tbarmode : string\n\t\t\tMode when displaying bars\n\t\t\t\tgroup\n\t\t\t\tstack\n\t\t\t\toverlay\n\t\tbargap : float\n\t\t\tSets the gap between bars\n\t\t\t\t[0,1)\n\t\t\tApplicabe for bar and histogram plots\n\t\tbargroupgap : float\n\t\t\tSet the gap between groups\n\t\t\t\t[0,1)\n\t\t\tApplicabe for bar and histogram plots\n\t\tgridcolor : string\n\t\t\t\tgrid color\n\t\tzerolinecolor : string\n\t\t\t\tzero line color\n\t\tmargin : dict or tuple\n\t\t\t\tDictionary (l,r,b,t) or\n\t\t\t\tTuple containing the left,\n\t\t\t\tright, bottom and top margins\n\t\tdimensions : tuple\n\t\t\tDimensions of figure\n\t\tannotations : dict or list\n\t\t\tDictionary of annotations\n\t\t\t\t{x_point : text}\n\t\t\tor\n\t\t\tList of Plotly Annotations\n\t\tis3d : bool\n\t\t\tIndicates if the layout is for a 3D chart\n\n\t\tOther Kwargs\n\t\t============\n\n\t\tShapes\n\t\t\thline : int, list or dict\n\t\t\t\tDraws a horizontal line at the\n\t\t\t\tindicated y position(s)\n\t\t\t\tExtra parameters can be passed in\n\t\t\t\tthe form of a dictionary (see shapes)\n\t\t\tvline : int, list or dict\n\t\t\t\tDraws a vertical line at the\n\t\t\t\tindicated x position(s)\n\t\t\t\tExtra parameters can be passed in\n\t\t\t\tthe form of a dictionary (see shapes)\n\t\t\thspan : (y0,y1)\n\t\t\t\tDraws a horizontal rectangle at the\n\t\t\t\tindicated (y0,y1) positions.\n\t\t\t\tExtra parameters can be passed in\n\t\t\t\tthe form of a dictionary (see shapes)\n\t\t\tvspan : (x0,x1)\n\t\t\t\tDraws a vertical rectangle at the\n\t\t\t\tindicated (x0,x1) positions.\n\t\t\t\tExtra parameters can be passed in\n\t\t\t\tthe form of a dictionary (see shapes)\n\t\t\tshapes : dict or list(dict)\n\t\t\t\tList of dictionaries with the\n\t\t\t\tspecifications of a given shape.\n\t\t\t\tSee help(cufflinks.tools.get_shape)\n\t\t\t\tfor more information\n\n\t\tAxis Ranges\n\t\t\txrange : [lower_bound,upper_bound]\n\t\t\t\tSets the range for the x axis\n\t\t\tyrange : [lower_bound,upper_bound]\n\t\t\t\tSets the range for the y axis\n\t\t\tzrange : [lower_bound,upper_bound]\n\t\t\t\tSets the range for the z axis\n\n\t\tExplicit Layout Updates\n\t\t\tlayout_update : dict\n\t\t\t\tThe layout will be modified with all\n\t\t\t\tthe explicit values stated in the\n\t\t\t\tdictionary\n\n\t\tRange Selector\n\t\t\trangeselector : dict\n\t\t\t\tDefines a rangeselector object\n\t\t\t\tsee help(cf.tools.get_range_selector) for more information\n\t\t\t\tExample:\n\t\t\t\t\t{'steps':['1y','2 months','5 weeks','ytd','2mtd'],\n\t\t\t\t\t 'axis':'xaxis', 'bgcolor' : ('blue',.3),\n\t\t\t\t\t 'x': 0.2 , 'y' : 0.9}\n\n\t\tRange Slider\n\t\t\trangeslider : bool or dict\n\t\t\t\tDefines if a rangeslider is displayed\n\t\t\t\tIf bool:\n\t\t\t\t\tTrue : Makes it visible\n\t\t\t\tif dict:\n\t\t\t\t\tRangeslider object\n\t\t\t\tExample:\n\t\t\t\t\t{'bgcolor':('blue',.3),'autorange':True}\n\n\t\tAnnotations\n\t\t\tfontcolor : str\n\t\t\t\tText color for annotations\n\t\t\tfontsize : int\n\t\t\t\tText size for annotations\n\t\t\ttextangle : int\n\t\t\t\tTextt angle\n\t\t\tSee https://plot.ly/python/reference/#layout-annotations\n\t\t\tfor a complete list of valid parameters.\n\t\"\"\"\n\n\n\tfor key in list(kwargs.keys()):\n\t\tif key not in __LAYOUT_KWARGS:\n\t\t\traise Exception(\"Invalid keyword : '{0}'\".format(key))\n\n\tif not theme:\n\t\ttheme = auth.get_config_file()['theme']\n\n\ttheme_data = getTheme(theme)\n\tlayout=theme_data['layout']\n\tlayout['xaxis'].update({'title':xTitle})\n\tlayout['yaxis'].update({'title':yTitle})\n\n\tfontfamily=kwargs.pop('fontfamily',None)\n\tif fontfamily:\n\t\tdeep_update(layout,{'font':{'family':fontfamily}})\n\n\n\tif barmode:\n\t\tlayout.update({'barmode':barmode})\n\tif bargroupgap:\n\t\tlayout.update({'bargroupgap':bargroupgap})\n\tif bargap:\n\t\tlayout.update(bargap=bargap)\n\tif title:\n\t\tlayout.update({'title':title})\n\tif annotations:\n\t\tlayout.update({'annotations':annotations})\n\n\n\tdef update_axis(layout,axis='xy',**vals):\n\t\tfor _x in axis:\n\t\t\tfor k,v in list(vals.items()):\n\t\t\t\tif v==None:\n\t\t\t\t\tvals.pop(k)\n\t\t\tfor k in layout:\n\t\t\t\tif '{0}{1}'.format(_x,'axis') in k:\n\t\t\t\t\tlayout[k].update(**vals)\n\t\treturn layout\n\n\taxis_kwargs=check_kwargs(kwargs,__LAYOUT_AXIS,{},True)\n\txaxis_kwargs=kwargs_from_keyword(kwargs,{},'xaxis',True)\n\tyaxis_kwargs=kwargs_from_keyword(kwargs,{},'yaxis',True)\n\n\tfor _x,_vals in (('xy',axis_kwargs),('x',xaxis_kwargs),('y',yaxis_kwargs)):\n\t\tlayout=update_axis(layout,_x,**_vals)\n\n\tif margin:\n\t\tif isinstance(margin,dict):\n\t\t\tmargin=margin\n\t\telse:\n\t\t\tmargin=dict(list(zip(('l','r','b','t'),margin)))\n\t\tlayout.update(margin=margin)\n\n\tif dimensions:\n\t\tlayout.update(width=dimensions[0])\n\t\tlayout.update(height=dimensions[1])\n\n\tif height:\n\t\tlayout.update(height=height)\n\tif width:\n\t\tlayout.update(width=width)\n\tif is3d:\n\t\tif '3d' in theme_data:\n\t\t\tlayout=deep_update(layout,theme_data['3d'])\n\t\tzaxis=layout['xaxis'].copy()\n\t\tzaxis.update(title=zTitle)\n\t\tscene=dict(xaxis=layout['xaxis'].copy(),yaxis=layout['yaxis'].copy(),zaxis=zaxis)\n\t\tlayout.update(scene=scene)\n\t\tdel layout['xaxis']\n\t\tdel layout['yaxis']\n\n\t## Axis Range\n\tfor r in ['x','y','z']:\n\t\tif '{0}range'.format(r) in kwargs:\n\t\t\tif is3d:\n\t\t\t\tlayout['scene']['{0}axis'.format(r)].update(range=kwargs['{0}range'.format(r)])\n\t\t\telse:\n\t\t\t\tlayout['{0}axis'.format(r)].update(range=kwargs['{0}range'.format(r)])\n\n\t# Need to update this for an add_axis approach.\n\tif kind in ('candlestick','ohlc','candle'):\n\t\tlayout['yaxis2']=layout['yaxis'].copy()\n\t\tlayout['yaxis'].update(showticklabels=False)\n\n\t## Kwargs\n\n\tif 'legend' in kwargs:\n\t\tif type(kwargs['legend'])==bool:\n\t\t\tlayout['showlegend']=kwargs['legend']\n\t\telif type(kwargs['legend'])==str:\n\t\t\tif kwargs['legend']=='top':\n\t\t\t\tlayout['legend'].update(orientation='h',yanchor='bottom',x=.3,y=.95)\n\t\t\telif kwargs['legend']=='bottom':\n\t\t\t\tlayout['legend'].update(orientation='h',yanchor='bottom',x=.3,y=-0.5)\n\t\t\tlayout['showlegend']=True\n\t\telse:\n\t\t\tlayout['legend']=kwargs['legend']\n\t\t\tlayout['showlegend']=True\n\n\tif 'showlegend' in kwargs:\n\t\tlayout['showlegend']=kwargs['showlegend']\n\n\t# Logarithmic Axis\n\tfor _ in ['x','y','z']:\n\t\tif 'log{0}'.format(_) in kwargs:\n\t\t\tif is3d:\n\t\t\t\tif kwargs['log{0}'.format(_)]:\n\t\t\t\t\tlayout['scene']['{0}axis'.format(_)]['type']='log'\n\t\t\telse:\n\t\t\t\tif kwargs['log{0}'.format(_)]:\n\t\t\t\t\tlayout['{0}axis'.format(_)]['type']='log'\n\n\t# Shapes\n\n\tif any(k in kwargs for k in ['vline','hline','shapes','hspan','vspan']):\n\t\tshapes=[]\n\n\t\tdef get_shapes(xline):\n\n\t\t\torientation=xline[0]\n\t\t\txline=kwargs[xline]\n\t\t\tif isinstance(xline,list):\n\t\t\t\tfor x_i in xline:\n\t\t\t\t\tif isinstance(x_i,dict):\n\t\t\t\t\t\tx_i['kind']='line'\n\t\t\t\t\t\tshapes.append(get_shape(**x_i))\n\t\t\t\t\telse:\n\t\t\t\t\t\tif orientation=='h':\n\t\t\t\t\t\t\tshapes.append(get_shape(kind='line',y=x_i))\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tshapes.append(get_shape(kind='line',x=x_i))\n\t\t\telif isinstance(xline,dict):\n\t\t\t\tshapes.append(get_shape(**xline))\n\t\t\telse:\n\t\t\t\tif orientation=='h':\n\t\t\t\t\tshapes.append(get_shape(kind='line',y=xline))\n\t\t\t\telse:\n\t\t\t\t\tshapes.append(get_shape(kind='line',x=xline))\n\n\t\tdef get_span(xspan):\n\t\t\torientation=xspan[0]\n\t\t\txspan=kwargs[xspan]\n\t\t\tif isinstance(xspan,list):\n\t\t\t\tfor x_i in xspan:\n\t\t\t\t\tif isinstance(x_i,dict):\n\t\t\t\t\t\tx_i['kind']='rect'\n\t\t\t\t\t\tshapes.append(get_shape(**x_i))\n\t\t\t\t\telse:\n\t\t\t\t\t\tv0,v1=x_i\n\t\t\t\t\t\tif orientation=='h':\n\t\t\t\t\t\t\tshapes.append(get_shape(kind='rect',y0=v0,y1=v1,fill=True,opacity=.5))\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tshapes.append(get_shape(kind='rect',x0=v0,x1=v1,fill=True,opacity=.5))\n\t\t\telif isinstance(xspan,dict):\n\t\t\t\txspan['kind']='rect'\n\t\t\t\tshapes.append(get_shape(**xspan))\n\t\t\telif isinstance(xspan,tuple):\n\t\t\t\tv0,v1=xspan\n\t\t\t\tif orientation=='h':\n\t\t\t\t\tshapes.append(get_shape(kind='rect',y0=v0,y1=v1,fill=True,opacity=.5))\n\t\t\t\telse:\n\t\t\t\t\tshapes.append(get_shape(kind='rect',x0=v0,x1=v1,fill=True,opacity=.5))\n\t\t\telse:\n\t\t\t\traise Exception('Invalid value for {0}span: {1}'.format(orientation,xspan))\n\n\t\tif 'hline' in kwargs:\n\t\t\tget_shapes('hline')\n\t\tif 'vline' in kwargs:\n\t\t\tget_shapes('vline')\n\t\tif 'hspan' in kwargs:\n\t\t\tget_span('hspan')\n\t\tif 'vspan' in kwargs:\n\t\t\tget_span('vspan')\n\t\tif 'shapes' in kwargs:\n\t\t\tshapes_=kwargs['shapes']\n\t\t\tif isinstance(shapes_,list):\n\t\t\t\tfor i in shapes_:\n\t\t\t\t\tshp=i if 'type' in i else get_shape(**i)\n\t\t\t\t\tshapes.append(shp)\n\t\t\telif isinstance(shapes_,dict):\n\t\t\t\t\tshp=shapes_ if 'type' in shapes_ else get_shape(**shapes_)\n\t\t\t\t\tshapes.append(shp)\n\t\t\telse:\n\t\t\t\traise Exception(\"Shapes need to be either a dict or list of dicts\")\n\n\n\t\tlayout['shapes']=shapes\n\n\t# Maps\n\tif kind in ('choropleth','scattergeo'):\n\t\tkw=check_kwargs(kwargs,__GEO_KWARGS)\n\t\tdefaults={'projection':{'type':'equirectangular'},'showframe':False,'showcoastlines':False}\n\t\tfor k,v in list(defaults.items()):\n\t\t\tif k not in kw:\n\t\t\t\tkw[k]=v\n\t\tkw_=kwargs_from_keyword(kw,{},'projection')\n\t\tdeep_update(kw,kw_)\n\t\tlayout['geo']=kw\n\t\tdel layout['xaxis']\n\t\tdel layout['yaxis']\n\t\tif not margin:\n\t\t\tlayout['margin']={'autoexpand':True}\n\n\t# Range Selector\n\tif 'rangeselector' in kwargs:\n\t\trs=kwargs['rangeselector']\n\t\tif 'axis' in rs:\n\t\t\taxis=rs['axis']\n\t\t\tdel rs['axis']\n\t\telse:\n\t\t\taxis='xaxis'\n\t\tlayout[axis]['rangeselector']=get_range_selector(**rs)\n\n\t# Range Slider\n\tif 'rangeslider' in kwargs:\n\t\tif type(kwargs['rangeslider'])==bool:\n\t\t\tif kwargs['rangeslider']:\n\t\t\t\tlayout['xaxis']['rangeslider']=dict(visible=kwargs['rangeslider'])\n\t\t\telse:\n\t\t\t\tlayout['xaxis']['rangeslider']=dict(visible=False)\n\t\t\t\t# layout['yaxis1'].update(domain=(0,0))\n\t\telse:\n\t\t\tlayout['xaxis']['rangeslider']=kwargs['rangeslider']\n\telse:\n\t\tif kind in ('ohlc','candle','candlestick'):\n\t\t\tlayout['xaxis']['rangeslider']=dict(visible=False)\n\t\t\t# layout['yaxis1'].update(domain=(0,0))\n\n\n\n\t# Explicit Updates\n\n\tif 'layout_update' in kwargs:\n\t\tlayout=deep_update(layout,kwargs['layout_update'])\n\n\treturn layout", "response": "This function returns a string that can be used to create a plotly layout."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate an annotation dictionary for a DataFrame", "response": "def get_annotations(df,annotations,kind='lines',theme=None,**kwargs):\n\t\"\"\"\n\tGenerates an annotations object\n\n\tParameters:\n\t-----------\n\t\tdf : DataFrame\n\t\t\tOriginal DataFrame of values\n\t\tannotations : dict or list\n\t\t\tDictionary of annotations\n\t\t\t{x_point : text}\n\t\t\tor\n\t\t\tList of Plotly annotations\n\t\"\"\"\n\n\n\n\n\n\tfor key in list(kwargs.keys()):\n\t\tif key not in __ANN_KWARGS:\n\t\t\traise Exception(\"Invalid keyword : '{0}'\".format(key))\n\n\ttheme_data = getTheme(theme)\n\n\tkwargs['fontcolor']=kwargs.pop('fontcolor',theme_data['annotations']['fontcolor'])\n\tkwargs['arrowcolor']=kwargs.pop('arrowcolor',theme_data['annotations']['arrowcolor'])\n\tkwargs['fontsize']=kwargs.pop('fontsize',12)\n\n\tdef check_ann(annotation):\n\t\tlocal_list=[]\n\n\t\tif 'title' in annotation:\n\t\t\tlocal_list.append(\n\t\t\t\t\tdict(\n\t\t\t\t\t\t\ttext=annotation['title'],\n\t\t\t\t\t\t\tshowarrow=False,\n\t\t\t\t\t\t\tx=0,\n\t\t\t\t\t\t\ty=1,\n\t\t\t\t\t\t\txref='paper',\n\t\t\t\t\t\t\tyref='paper',\n\t\t\t\t\t\t\tfont={'size':24 if not 'fontsize' in kwargs else kwargs['fontsize']}\n\t\t\t\t\t\t)\n\t\t\t\t)\n\n\t\t\tdel annotation['title']\t\n\t\t\tlocal_list.append(ann)\n\n\t\telif 'x' in annotation:\n\t\t\tann=dict(\n\t\t\t\t\t\t\t\tx=annotation['x'],\n\t\t\t\t\t\t\t\ty=annotation.get('y',.5),\n\t\t\t\t\t\t\t\txref=annotation.get('xref','x'),\n\t\t\t\t\t\t\t\tyref=annotation.get('yref',kwargs.get('yref','y1')),\n\t\t\t\t\t\t\t\ttext=annotation.get('text'),\n\t\t\t\t\t\t\t\tshowarrow=annotation.get('showarrow',True),\n\t\t\t\t\t\t\t\tarrowhead=annotation.get('arrowhead',7),\n\t\t\t\t\t\t\t\tarrowcolor=annotation.get('arrowcolor',kwargs.get('arrowcolor')),\n\t\t\t\t\t\t\t\tax=annotation.get('ax',0),\n\t\t\t\t\t\t\t\tay=annotation.get('ay',-100),\n\t\t\t\t\t\t\t\ttextangle=annotation.get('textangle',-90),\n\t\t\t\t\t\t\t\tfont = dict(\n\t\t\t\t\t\t\t\t\tcolor = annotation.get('fontcolor',annotation.get('color',kwargs.get('fontcolor'))),\n\t\t\t\t\t\t\t\t\tsize = annotation.get('fontsize',annotation.get('size',kwargs.get('fontsize')))\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t)\n\t\t\tlocal_list.append(ann)\n\n\t\telse:\n\t\t\tfor k,v in list(annotation.items()):\n\t\t\t\tif kind in ('candlestick','ohlc','candle'):\n\t\t\t\t\td=ta._ohlc_dict(df)\n\t\t\t\t\tmaxv=df[d['high']].ix[k]\n\t\t\t\t\tyref='y2'\n\t\t\t\telse:\n\t\t\t\t\tmaxv=df.ix[k].sum() if k in df.index else 0\n\t\t\t\t\tyref='y1'\n\t\t\t\tann=dict(\n\t\t\t\t\t\t\t\tx=k,\n\t\t\t\t\t\t\t\ty=maxv,\n\t\t\t\t\t\t\t\txref='x',\n\t\t\t\t\t\t\t\tyref=yref,\n\t\t\t\t\t\t\t\ttext=v,\n\t\t\t\t\t\t\t\tshowarrow=kwargs.get('showarrow',True),\n\t\t\t\t\t\t\t\tarrowhead=kwargs.get('arrowhead',7),\n\t\t\t\t\t\t\t\tarrowcolor = kwargs['arrowcolor'],\n\t\t\t\t\t\t\t\tax=kwargs.get('ax',0),\n\t\t\t\t\t\t\t\tay=kwargs.get('ay',-100),\n\t\t\t\t\t\t\t\ttextangle=kwargs.get('textangle',-90),\n\t\t\t\t\t\t\t\tfont = dict(\n\t\t\t\t\t\t\t\t\tcolor = kwargs['fontcolor'],\n\t\t\t\t\t\t\t\t\tsize=kwargs['fontsize']\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t)\n\t\t\t\tlocal_list.append(ann)\n\n\n\t\treturn local_list\n\n\tannotations = make_list(annotations)\n\t_list_ann=[]\n\tfor ann in annotations:\n\t\t_list_ann.extend(check_ann(ann))\n\treturn _list_ann"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_base_layout(figs):\n\tlayout={}\n\tfor fig in figs:\n\t\tif not isinstance(fig,dict):\n\t\t\tfig=fig.to_dict()\n\t\tfor k,v in list(fig['layout'].items()):\n\t\t\tlayout[k]=v\n\treturn layout", "response": "Generates a layout with the union of all properties of multiple\n\tfigures"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating multiple Plotly figures for a given DataFrame.", "response": "def figures(df,specs,asList=False):\n\t\"\"\"\n\tGenerates multiple Plotly figures for a given DataFrame\n\n\tParameters:\n\t-----------\n\t\tdf : DataFrame\n\t\t\tPandas DataFrame\n\t\tspecs : list(dict)\n\t\t\tList of dictionaries with the properties\n\t\t\tof each figure.\n\t\t\tAll properties avaialbe can be seen with\n\t\t\thelp(cufflinks.pd.DataFrame.iplot)\n\t\tasList : boolean\n\t\t\tIf True, then a list of figures is returned.\n\t\t\tOtherwise a single (merged) figure is returned.\n\t\t\tDefault : False\n\t\"\"\"\n\tfigs=[]\n\tfor spec in specs:\n\t\tfigs.append(df.figure(**spec))\n\tif asList:\n\t\treturn figs\n\telse:\n\t\treturn merge_figures(figs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef merge_figures(figures):\n\tfigure={}\n\tdata=[]\n\tfor fig in figures:\n\t\tfor trace in fig['data']:\n\t\t\tdata.append(trace)\n\tlayout=get_base_layout(figures)\n\tfigure['data']=data\n\tfigure['layout']=layout\n\treturn figure", "response": "Generates a single Figure from a list of figures\n\tParameters :\n\t-----------\n\tfigures : list of figures\n\tParameters"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a subplot view for a set of figures This is a wrapper for plotly.tools.make_subplots Parameters: ----------- figures : [Figures] List of Plotly Figures shape : (rows,cols) Tuple indicating the size of rows and columns If omitted then the layout is automatically set shared_xaxes : bool Assign shared x axes. If True, subplots in the same grid column have one common shared x-axis at the bottom of the grid. shared_yaxes : bool Assign shared y axes. If True, subplots in the same grid row have one common shared y-axis on the left-hand side of the grid. start_cell : string 'bottom-left' 'top-left' Choose the starting cell in the subplot grid used to set the domains of the subplots. theme : string Layout Theme solar pearl white see cufflinks.getThemes() for all available themes base_layout : layout (dict) Layout to be used as base where the subplots will be added subplot_titles : list(string) List of strings that contains the titles of each plot. horizontal_spacing : float [0,1] Space between subplot columns. vertical_spacing : float Space between subplot rows. specs : list of dicts Subplot specifications. ex1: specs=[[{}, {}], [{'colspan': 2}, None]] ex2: specs=[[{'rowspan': 2}, {}], [None, {}]] - Indices of the outer list correspond to subplot grid rows starting from the bottom. The number of rows in 'specs' must be equal to 'rows'. - Indices of the inner lists correspond to subplot grid columns starting from the left. The number of columns in 'specs' must be equal to 'cols'. - Each item in the 'specs' list corresponds to one subplot in a subplot grid. (N.B. The subplot grid has exactly 'rows' times 'cols' cells.) - Use None for blank a subplot cell (or to move pass a col/row span). - Note that specs[0][0] has the specs of the 'start_cell' subplot. - Each item in 'specs' is a dictionary. The available keys are: * is_3d (boolean, default=False): flag for 3d scenes * colspan (int, default=1): number of subplot columns for this subplot to span. * rowspan (int, default=1): number of subplot rows for this subplot to span. * l (float, default=0.0): padding left of cell * r (float, default=0.0): padding right of cell * t (float, default=0.0): padding right of cell * b (float, default=0.0): padding bottom of cell - Use 'horizontal_spacing' and 'vertical_spacing' to adjust the spacing in between the subplots. insets : list of dicts Inset specifications. - Each item in 'insets' is a dictionary. The available keys are: * cell (tuple, default=(1,1)): (row, col) index of the subplot cell to overlay inset axes onto. * is_3d (boolean, default=False): flag for 3d scenes * l (float, default=0.0): padding left of inset in fraction of cell width * w (float or 'to_end', default='to_end') inset width in fraction of cell width ('to_end': to cell right edge) * b (float, default=0.0): padding bottom of inset in fraction of cell height * h (float or 'to_end', default='to_end') inset height in fraction of cell height ('to_end': to cell top edge)", "response": "def subplots(figures,shape=None,\n\t\t\t\t  shared_xaxes=False, shared_yaxes=False,\n\t\t\t\t  start_cell='top-left', theme=None,base_layout=None,\n\t\t\t\t  **kwargs):\n\t\"\"\"\n\tGenerates a subplot view for a set of figures\n\tThis is a wrapper for plotly.tools.make_subplots\n\n\tParameters:\n\t-----------\n\t\tfigures : [Figures]\n\t\t\tList of Plotly Figures\n\t\tshape : (rows,cols)\n\t\t\tTuple indicating the size of rows and columns\n\t\t\tIf omitted then the layout is automatically set\n\t\tshared_xaxes : bool\n\t\t\tAssign shared x axes.\n\t\t\tIf True, subplots in the same grid column have one common\n\t\t\tshared x-axis at the bottom of the grid.\n\t\tshared_yaxes : bool\n\t\t\tAssign shared y axes.\n\t\t\tIf True, subplots in the same grid row have one common\n\t\t\tshared y-axis on the left-hand side of the grid.\n\t\tstart_cell : string\n\t\t\t\t'bottom-left'\n\t\t\t\t'top-left'\n\t\t\tChoose the starting cell in the subplot grid used to set the\n\t\t\tdomains of the subplots.\n\t\ttheme : string\n\t\t\tLayout Theme\n\t\t\t\tsolar\n\t\t\t\tpearl\n\t\t\t\twhite\n\t\t\tsee cufflinks.getThemes() for all\n\t\t\tavailable themes\n\t\tbase_layout : layout (dict)\n\t\t\tLayout to be used as base where the subplots will be\n\t\t\tadded\n\t\tsubplot_titles : list(string)\n\t\t\tList of strings that contains the titles of each\n\t\t\tplot.\n\t\thorizontal_spacing : float\n\t\t\t\t[0,1]\n\t\t\tSpace between subplot columns.\n\t\tvertical_spacing : float\n\t\t\tSpace between subplot rows.\n\t\tspecs : list of dicts\n\t\t\tSubplot specifications.\n\t\t\t\tex1: specs=[[{}, {}], [{'colspan': 2}, None]]\n\t\t\t\tex2: specs=[[{'rowspan': 2}, {}], [None, {}]]\n\n\t\t\t- Indices of the outer list correspond to subplot grid rows\n\t\t\t  starting from the bottom. The number of rows in 'specs'\n\t\t\t  must be equal to 'rows'.\n\n\t\t\t- Indices of the inner lists correspond to subplot grid columns\n\t\t\t  starting from the left. The number of columns in 'specs'\n\t\t\t  must be equal to 'cols'.\n\n\t\t\t- Each item in the 'specs' list corresponds to one subplot\n\t\t\t  in a subplot grid. (N.B. The subplot grid has exactly 'rows'\n\t\t\t  times 'cols' cells.)\n\n\t\t\t- Use None for blank a subplot cell (or to move pass a col/row span).\n\n\t\t\t- Note that specs[0][0] has the specs of the 'start_cell' subplot.\n\n\t\t\t- Each item in 'specs' is a dictionary.\n\t\t\t\tThe available keys are:\n\n\t\t\t\t* is_3d (boolean, default=False): flag for 3d scenes\n\t\t\t\t* colspan (int, default=1): number of subplot columns\n\t\t\t\t\tfor this subplot to span.\n\t\t\t\t* rowspan (int, default=1): number of subplot rows\n\t\t\t\t\tfor this subplot to span.\n\t\t\t\t* l (float, default=0.0): padding left of cell\n\t\t\t\t* r (float, default=0.0): padding right of cell\n\t\t\t\t* t (float, default=0.0): padding right of cell\n\t\t\t\t* b (float, default=0.0): padding bottom of cell\n\n\t\t\t- Use 'horizontal_spacing' and 'vertical_spacing' to adjust\n\t\t\t  the spacing in between the subplots.\n\n\t\tinsets : list of dicts\n\t\t\tInset specifications.\n\n\t\t\t- Each item in 'insets' is a dictionary.\n\t\t\t\tThe available keys are:\n\n\t\t\t\t* cell (tuple, default=(1,1)): (row, col) index of the\n\t\t\t\t\tsubplot cell to overlay inset axes onto.\n\t\t\t\t* is_3d (boolean, default=False): flag for 3d scenes\n\t\t\t\t* l (float, default=0.0): padding left of inset\n\t\t\t\t\t  in fraction of cell width\n\t\t\t\t* w (float or 'to_end', default='to_end') inset width\n\t\t\t\t\t  in fraction of cell width ('to_end': to cell right edge)\n\t\t\t\t* b (float, default=0.0): padding bottom of inset\n\t\t\t\t\t  in fraction of cell height\n\t\t\t\t* h (float or 'to_end', default='to_end') inset height\n\t\t\t\t\t  in fraction of cell height ('to_end': to cell top edge)\n\t\"\"\"\n\tif not isinstance(figures,list):\n\t\tfigures=[figures]\n\n\tif shape:\n\t\trows,cols=shape\n\t\tif len(figures)>rows*cols:\n\t\t\traise Exception(\"Invalid shape for the number of figures given\")\n\telse:\n\t\tif len(figures)==1:\n\t\t\tcols=1\n\t\t\trows=1\n\t\telif shared_xaxes:\n\t\t\tcols=1\n\t\t\trows=len(figures)\n\t\telif shared_yaxes:\n\t\t\tcols=len(figures)\n\t\t\trows=1\n\t\telse:\n\t\t\tcols=2\n\t\t\trows=len(figures)//2+len(figures)%2\n\tsp,grid_ref=get_subplots(rows=rows,cols=cols,\n\t\t\t\t             shared_xaxes=shared_xaxes, shared_yaxes=shared_yaxes,\n\t\t\t\t             start_cell=start_cell, theme=theme,base_layout=base_layout,\n\t\t\t\t             **kwargs)\n\tlist_ref=(col for row in grid_ref for col in row)\n\tfor i in range(len(figures)):\n\t\twhile True:\n\t\t\tlr=next(list_ref)\n\t\t\tif lr is not None:\n\t\t\t\tbreak\n\t\tfor _ in figures[i]['data']:\n\t\t\tfor axe in lr:\n\t\t\t\t_.update({'{0}axis'.format(axe[0]):axe})\n\t\t\tsp['data'].append(_)\n\t# Remove extra plots\n\tfor k in list(sp['layout'].keys()):\n\t\ttry:\n\t\t\tif int(k[-1])>len(figures):\n\t\t\t\tdel sp['layout'][k]\n\t\texcept:\n\t\t\tpass\n\n\t# Check for non-cartesian plots\n\tdata=sp['data']\n\tlayout=sp['layout']\n\tfor d in data:\n\t\tif d['type']=='pie':\n\t\t\td['domain']={}\n\t\t\td['domain']['x']=layout['xaxis{0}'.format(d['xaxis'][1:])]['domain']\n\t\t\td['domain']['y']=layout['yaxis{0}'.format(d['yaxis'][1:])]['domain']\n\treturn sp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a subplot view for a set of figures Parameters: ----------- rows : int Number of rows cols : int Number of cols shared_xaxes : bool Assign shared x axes. If True, subplots in the same grid column have one common shared x-axis at the bottom of the gird. shared_yaxes : bool Assign shared y axes. If True, subplots in the same grid row have one common shared y-axis on the left-hand side of the gird. start_cell : string 'bottom-left' 'top-left' Choose the starting cell in the subplot grid used to set the domains of the subplots. theme : string Layout Theme solar pearl white see cufflinks.getThemes() for all available themes horizontal_spacing : float [0,1] Space between subplot columns. vertical_spacing : float Space between subplot rows. specs : list of dicts Subplot specifications. ex1: specs=[[{}, {}], [{'colspan': 2}, None]] ex2: specs=[[{'rowspan': 2}, {}], [None, {}]] - Indices of the outer list correspond to subplot grid rows starting from the bottom. The number of rows in 'specs' must be equal to 'rows'. - Indices of the inner lists correspond to subplot grid columns starting from the left. The number of columns in 'specs' must be equal to 'cols'. - Each item in the 'specs' list corresponds to one subplot in a subplot grid. (N.B. The subplot grid has exactly 'rows' times 'cols' cells.) - Use None for blank a subplot cell (or to move pass a col/row span). - Note that specs[0][0] has the specs of the 'start_cell' subplot. - Each item in 'specs' is a dictionary. The available keys are: * is_3d (boolean, default=False): flag for 3d scenes * colspan (int, default=1): number of subplot columns for this subplot to span. * rowspan (int, default=1): number of subplot rows for this subplot to span. * l (float, default=0.0): padding left of cell * r (float, default=0.0): padding right of cell * t (float, default=0.0): padding right of cell * b (float, default=0.0): padding bottom of cell - Use 'horizontal_spacing' and 'vertical_spacing' to adjust the spacing in between the subplots. insets : list of dicts Inset specifications. - Each item in 'insets' is a dictionary. The available keys are: * cell (tuple, default=(1,1)): (row, col) index of the subplot cell to overlay inset axes onto. * is_3d (boolean, default=False): flag for 3d scenes * l (float, default=0.0): padding left of inset in fraction of cell width * w (float or 'to_end', default='to_end') inset width in fraction of cell width ('to_end': to cell right edge) * b (float, default=0.0): padding bottom of inset in fraction of cell height * h (float or 'to_end', default='to_end') inset height in fraction of cell height ('to_end': to cell top edge)", "response": "def get_subplots(rows=1,cols=1,\n\t\t\t\t  shared_xaxes=False, shared_yaxes=False,\n\t\t\t\t  start_cell='top-left', theme=None,base_layout=None,\n\t\t\t\t  **kwargs):\n\n\t\"\"\"\n\tGenerates a subplot view for a set of figures\n\n\tParameters:\n\t-----------\n\t\trows : int\n\t\t\tNumber of rows\n\t\tcols : int\n\t\t\tNumber of cols\n\t\tshared_xaxes : bool\n\t\t\tAssign shared x axes.\n\t\t\tIf True, subplots in the same grid column have one common\n\t\t\tshared x-axis at the bottom of the gird.\n\t\tshared_yaxes : bool\n\t\t\tAssign shared y axes.\n\t\t\tIf True, subplots in the same grid row have one common\n\t\t\tshared y-axis on the left-hand side of the gird.\n\t\tstart_cell : string\n\t\t\t\t'bottom-left'\n\t\t\t\t'top-left'\n\t\t\tChoose the starting cell in the subplot grid used to set the\n\t\t\tdomains of the subplots.\n\t\ttheme : string\n\t\t\tLayout Theme\n\t\t\t\tsolar\n\t\t\t\tpearl\n\t\t\t\twhite\n\t\t\tsee cufflinks.getThemes() for all\n\t\t\tavailable themes\n\t\thorizontal_spacing : float\n\t\t\t\t[0,1]\n\t\t\tSpace between subplot columns.\n\t\tvertical_spacing : float\n\t\t\tSpace between subplot rows.\n\t\tspecs : list of dicts\n\t\t\tSubplot specifications.\n\t\t\t\tex1: specs=[[{}, {}], [{'colspan': 2}, None]]\n\t\t\t\tex2: specs=[[{'rowspan': 2}, {}], [None, {}]]\n\n\t\t\t- Indices of the outer list correspond to subplot grid rows\n\t\t\t  starting from the bottom. The number of rows in 'specs'\n\t\t\t  must be equal to 'rows'.\n\n\t\t\t- Indices of the inner lists correspond to subplot grid columns\n\t\t\t  starting from the left. The number of columns in 'specs'\n\t\t\t  must be equal to 'cols'.\n\n\t\t\t- Each item in the 'specs' list corresponds to one subplot\n\t\t\t  in a subplot grid. (N.B. The subplot grid has exactly 'rows'\n\t\t\t  times 'cols' cells.)\n\n\t\t\t- Use None for blank a subplot cell (or to move pass a col/row span).\n\n\t\t\t- Note that specs[0][0] has the specs of the 'start_cell' subplot.\n\n\t\t\t- Each item in 'specs' is a dictionary.\n\t\t\t\tThe available keys are:\n\n\t\t\t\t* is_3d (boolean, default=False): flag for 3d scenes\n\t\t\t\t* colspan (int, default=1): number of subplot columns\n\t\t\t\t\tfor this subplot to span.\n\t\t\t\t* rowspan (int, default=1): number of subplot rows\n\t\t\t\t\tfor this subplot to span.\n\t\t\t\t* l (float, default=0.0): padding left of cell\n\t\t\t\t* r (float, default=0.0): padding right of cell\n\t\t\t\t* t (float, default=0.0): padding right of cell\n\t\t\t\t* b (float, default=0.0): padding bottom of cell\n\n\t\t\t- Use 'horizontal_spacing' and 'vertical_spacing' to adjust\n\t\t\t  the spacing in between the subplots.\n\n\t\tinsets : list of dicts\n\t\t\tInset specifications.\n\n\t\t\t- Each item in 'insets' is a dictionary.\n\t\t\t\tThe available keys are:\n\n\t\t\t\t* cell (tuple, default=(1,1)): (row, col) index of the\n\t\t\t\t\tsubplot cell to overlay inset axes onto.\n\t\t\t\t* is_3d (boolean, default=False): flag for 3d scenes\n\t\t\t\t* l (float, default=0.0): padding left of inset\n\t\t\t\t\t  in fraction of cell width\n\t\t\t\t* w (float or 'to_end', default='to_end') inset width\n\t\t\t\t\t  in fraction of cell width ('to_end': to cell right edge)\n\t\t\t\t* b (float, default=0.0): padding bottom of inset\n\t\t\t\t\t  in fraction of cell height\n\t\t\t\t* h (float or 'to_end', default='to_end') inset height\n\t\t\t\t\t  in fraction of cell height ('to_end': to cell top edge)\n\t\"\"\"\n\n\tif not theme:\n\t\ttheme = auth.get_config_file()['theme']\n\n\tlayout= base_layout if base_layout else getLayout(theme,**check_kwargs(kwargs,__LAYOUT_AXIS))\n\tsp=make_subplots(rows=rows,cols=cols,shared_xaxes=shared_xaxes,\n\t\t\t\t\t\t\t\t\t\t   shared_yaxes=shared_yaxes,print_grid=False,\n\t\t\t\t\t\t\t\t\t\t\tstart_cell=start_cell,**kwargs)\n\tsp, grid_ref = sp.to_dict(), sp._grid_ref\n\n\tfor k,v in list(layout.items()):\n\t\tif 'xaxis' not in k and 'yaxis' not in k:\n\t\t\tsp['layout'].update({k:v})\n\n\tdef update_axis(fig,layout):\n\t\tfor axis, n in list(Figure(fig).axis['len'].items()):\n\t\t\tfor _ in range(1,n+1):\n\t\t\t\tfor k,v in list(layout['{0}axis'.format(axis)].items()):\n\t\t\t\t\t_='' if _==1 else _\n\t\t\t\t\tif k not in fig['layout']['{0}axis{1}'.format(axis,_)]:\n\t\t\t\t\t\tfig['layout']['{0}axis{1}'.format(axis,_)][k]=v\n\n\tupdate_axis(sp,layout)\n\t# 124 - zeroline on the first figure\n\n\t# if 'subplot_titles' in kwargs:\n\t# \tif 'annotations' in layout:\n\t# \t\tannotation=sp['layout']['annotations'][0]\n\t# \telse:\n\t# \t\tannotation=getLayout(theme,annotations=[Annotation(text='')])['annotations']\n\t# \tfor ann in sp['layout']['annotations']:\n\t# \t\tann.update(font=dict(color=annotation['font']['color']))\n\n\t# def update_items(sp_item,layout,axis):\n\t# \tfor k,v in list(layout[axis].items()):\n\t# \t\tsp_item.update({k:v})\n\n\t# for k,v in list(sp['layout'].items()):\n\t# \tif isinstance(v,go.XAxis):\n\t# \t\tupdate_items(v,layout,'xaxis1')\n\t# \telif isinstance(v,go.YAxis):\n\t# \t\tupdate_items(v,layout,'xaxis1')\n\n\n\n\n\treturn sp, grid_ref"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndisplaying a matrix with scatter plot for each pair of Series in the DataFrame.", "response": "def scatter_matrix(df,theme=None,bins=10,color='grey',size=2):\n\t\"\"\"\n\tDisplays a matrix with scatter plot for each pair of\n\tSeries in the DataFrame.\n\tThe diagonal shows a histogram for each of the Series\n\n\tParameters:\n\t-----------\n\t\tdf : DataFrame\n\t\t\tPandas DataFrame\n\t\ttheme : string\n\t\t\tTheme to be used (if not the default)\n\t\tbins : int\n\t\t\tNumber of bins to use for histogram\n\t\tcolor : string\n\t\t\tColor to be used for each scatter plot\n\t\tsize : int\n\t\t\tSize for each marker on the scatter plot\n\t\"\"\"\n\tif not theme:\n\t\ttheme = auth.get_config_file()['theme']\n\n\tfigs=[]\n\tfor i in df.columns:\n\t\tfor j in df.columns:\n\t\t\tif i==j:\n\t\t\t\tfig=df.iplot(kind='histogram',keys=[i],asFigure=True,bins=bins)\n\t\t\t\tfigs.append(fig)\n\t\t\telse:\n\t\t\t\tfigs.append(df.iplot(kind='scatter',mode='markers',x=j,y=i,asFigure=True,size=size,colors=[color]))\n\tlayout=getLayout(theme)\n\tlayout['xaxis'].update(showgrid=False)\n\tlayout['yaxis'].update(showgrid=False)\n\tsm=subplots(figs,shape=(len(df.columns),len(df.columns)),shared_xaxes=False,shared_yaxes=False,\n\t\t\t\t\t  horizontal_spacing=.05,vertical_spacing=.07,base_layout=layout)\n\tsm['layout'].update(bargap=.02,showlegend=False)\n\treturn sm"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the axis of the plot", "response": "def _set_axis(self,traces,on=None,side='right',title=''):\n\t\"\"\"\n\tSets the axis in which each trace should appear\n\tIf the axis doesn't exist then a new axis is created\n\n\tParameters:\n\t-----------\n\t\ttraces : list(str)\n\t\t\tList of trace names\n\t\ton : string\n\t\t\tThe axis in which the traces should be placed.\n\t\t\tIf this is not indicated then a new axis will be\n\t\t\tcreated\n\t\tside : string\n\t\t\tSide where the axis will be placed\n\t\t\t\t'left'\n\t\t\t\t'right'\n\t\ttitle : string\n\t\t\tSets the title of the axis\n\t\t\tApplies only to new axis\n\t\"\"\"\n\tfig={}\n\tfig_cpy=fig_to_dict(self).copy()\n\tfig['data']=fig_cpy['data']\n\tfig['layout']=fig_cpy['layout']\n\tfig=Figure(fig)\n\ttraces=make_list(traces)\n\n\tdef update_data(trace,y):\n\t\tanchor=fig.axis['def'][y]['anchor'] if 'anchor' in fig.axis['def'][y] else 'x1'\n\t\tidx=fig.trace_dict[trace] if isinstance(trace,str) else trace\n\t\tfig['data'][idx]['xaxis']=anchor\n\t\tfig['data'][idx]['yaxis']=y\n\n\tfor trace in traces:\n\t\tif on:\n\t\t\tif on not in fig.axis['def']:\n\t\t\t\traise Exception('\"on\" axis does not exists: {0}'.format(on))\n\t\t\tupdate_data(trace,y=on)\n\t\telse:\n\t\t\tcurr_x,curr_y=fig.axis['ref'][trace]\n\t\t\tdomain='[0.0, 1.0]' if 'domain' not in fig.axis['def'][curr_y] else str(fig.axis['def'][curr_y]['domain'])\n\t\t\ttry:\n\t\t\t\tnew_axis=fig.axis['dom']['y'][domain][side]\n\t\t\texcept KeyError:\n\t\t\t\taxis=fig.axis['def'][curr_y].copy()\n\t\t\t\t### check overlaying values\n\t\t\t\taxis.update(title=title,overlaying=curr_y,side=side,anchor=curr_x)\n\t\t\t\taxis_idx=str(fig.axis['len']['y']+1)\n\t\t\t\tfig['layout']['yaxis{0}'.format(axis_idx)]=axis\n\t\t\t\tnew_axis='y{0}'.format(axis_idx)\n\t\t\tupdate_data(trace,y=new_axis)\n\n\n\tfor k in list(fig.axis['def'].keys()):\n\t\tid='{0}axis{1}'.format(k[0],k[-1:])\n\t\tif k not in fig.axis['ref_axis']:\n\t\t\ttry:\n\t\t\t\tdel fig['layout'][id]\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\n\treturn fig"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_shape(kind='line',x=None,y=None,x0=None,y0=None,x1=None,y1=None,span=0,color='red',dash='solid',width=1,\n\t\t\t\tfillcolor=None,fill=False,opacity=1,xref='x',yref='y'):\n\t\"\"\"\n\tReturns a plotly shape\n\n\tParameters:\n\t-----------\n\t\tkind : string\n\t\t\tShape kind\n\t\t\t\tline\n\t\t\t\trect\n\t\t\t\tcircle\n\t\tx : float\n\t\t\tx values for the shape.\n\t\t\tThis assumes x0=x1\n\t\tx0 : float\n\t\t\tx0 value for the shape\n\t\tx1 : float\n\t\t\tx1 value for the shape\n\t\ty : float\n\t\t\ty values for the shape.\n\t\t\tThis assumes y0=y1\n\t\ty0 : float\n\t\t\ty0 value for the shape\n\t\ty1 : float\n\t\t\ty1 value for the shape\n\t\tcolor : string\n\t\t\tcolor for shape line\n\t\tdash : string\n\t\t\tline style\n\t\t\t\tsolid\n\t\t\t\tdash\n\t\t\t\tdashdot\n\t\t\t\tdot\n\t\twidth : int\n\t\t\tline width\n\t\tfillcolor : string\n\t\t\tshape fill color\n\t\tfill : bool\n\t\t\tIf True then fill shape\n\t\t\tIf not fillcolor then the\n\t\t\tline color will be used\n\t\topacity : float [0,1]\n\t\t\topacity of the fill\n\t\txref : string\n\t\t\tSets the x coordinate system\n\t\t\twhich this object refers to\n\t\t\t\t'x'\n\t\t\t\t'paper'\n\t\t\t\t'x2' etc\n\t\tyref : string\n\t\t\tSets the y coordinate system\n\t\t\twhich this object refers to\n\t\t\t\t'y'\n\t\t\t\t'paper'\n\t\t\t\t'y2' etc\n\t\"\"\"\n\tif x1 is None:\n\t\tif x0 is None:\n\t\t\tif x is None:\n\t\t\t\txref='paper'\n\t\t\t\tx0=0\n\t\t\t\tx1=1\n\t\t\telse:\n\t\t\t\tx0=x1=x\n\t\telse:\n\t\t\tx1=x0\n\telse:\n\t\tx\n\tif y1 is None:\n\t\tif y0 is None:\n\t\t\tif y is None:\n\t\t\t\tyref='paper'\n\t\t\t\ty0=0\n\t\t\t\ty1=1\n\t\t\telse:\n\t\t\t\ty0=y1=y\n\t\telse:\n\t\t\ty1=y0\n\n\tshape = {\t'x0':x0,\n\t\t\t\t'y0':y0,\n\t\t\t\t'x1':x1,\n\t\t\t\t'y1':y1,\n\t\t\t\t'line' : {\n\t\t\t\t\t'color':normalize(color),\n\t\t\t\t\t'width':width,\n\t\t\t\t\t'dash':dash\n\t\t\t\t\t},\n\t\t\t\t'xref':xref,\n\t\t\t\t'yref':yref\n\t\t\t\t}\n\n\tif kind=='line':\n\t\tshape['type']='line'\n\n\telif kind=='circle':\n\t\tshape['type']='circle'\n\n\telif kind=='rect':\n\t\tshape['type']='rect'\n\telse:\n\t\traise Exception(\"Invalid or unkown shape type : {0}\".format(kind))\n\n\tif (fill or fillcolor) and kind!='line':\n\t\tfillcolor = color if not fillcolor else fillcolor\n\t\tfillcolor=to_rgba(normalize(fillcolor),opacity)\n\t\tshape['fillcolor']=fillcolor\n\n\treturn shape", "response": "Returns a plotly shape for the specified shape."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_range_selector(steps=['1m','1y'],bgcolor='rgba(150, 200, 250, 0.4)',x=0,y=0.9,\n\t\t\t\t\t\tvisible=True,**kwargs):\n\t\"\"\"\n\tReturns a range selector\n\tReference: https://plot.ly/python/reference/#layout-xaxis-rangeselector\n\n\tParameters:\n\t-----------\n\t\tsteps : string or list(string)\n\t\t\tSteps for the range\n\t\t\t\tExamples:\n\t\t\t\t\t['1y','2 months','5 weeks','ytd','2mtd']\n\t\tbgocolor : string or tuple(color,alpha)\n\t\t\tBackground color\n\t\t\t\tExamples:\n\t\t\t\t\t'cyan'\n\t\t\t\t\t'rgba(232,214,10,.5)'\n\t\t\t\t\t('blue',.3)\n\t\tfont_size : int\n\t\t\tFont size\n\t\tx : float\n\t\t\tPosition along the x axis\n\t\t\tDomain (0,1)\n\t\ty : float\n\t\t\tPosition along the y axis\n\t\t\tDomain (0,1)\n\t\"\"\"\n\timport string\n\n\tdef get_step(s):\n\n\n\t\tterm=[]\n\t\tstepmode='backward'\n\t\t_s=s\n\t\t_s=_s.replace(' ','')\n\t\t_s=_s.lower()\n\t\tif _s in ['reset','all']:\n\t\t\treturn {'count':1,'label':s,'step':'all'}\n\t\tif _s[-2:]=='td':\n\t\t\t_s=_s[:-2]\n\t\t\tstepmode='todate'\n\t\t\tif _s[0] not in string.digits:\n\t\t\t\t_s='1'+_s\n\t\tif _s[0] not in string.digits:\n\t\t\traise Exception('Invalid step format: {0}'.format(s))\n\t\twhile _s[-1] not in string.digits:\n\t\t\tterm.append(_s[-1])\n\t\t\t_s=_s[:-1]\n\t\tterm.reverse()\n\t\tterm=''.join(term)\n\t\tcnt=int(_s)\n\t\tterm=term[:-1] if (term[-1]=='s' and len(term)>1) else term\n\t\tif term in ['y','year','yr']:\n\t\t\t\t\tsteps='year'\n\t\telif term in ['w','week','wk']:\n\t\t\tsteps='week'\n\t\telif term in ['m','month','mth','mnth','mo']:\n\t\t\tsteps='month'\n\t\telif term in ['hr','hour']:\n\t\t\tsteps='hour'\n\t\telif term in ['min','minute','mn']:\n\t\t\tsteps='minute'\n\t\telif term in ['sec','sc','s']:\n\t\t\tsteps='second'\n\t\telse:\n\t\t\traise Exception('Invalid step format: {0}'.format(s))\n\t\treturn {'count':cnt,'label':s,'step':steps,'stepmode':stepmode}\n\n\trangeselector={\n\t\t'bgcolor':to_rgba(bgcolor,1),\n\t\t'x':x,\n\t\t'y':y,\n\t\t'visible':visible\n\t}\n\n\tkwargs['fontsize']=kwargs.get('fontsize',13)\n\n\trangeselector=dict_replace_keyword(rangeselector,'font',kwargs,False)\n\n\tbuttons=[]\n\tif type(steps) not in (list,tuple):\n\t\tsteps=[steps]\n\tfor s in steps:\n\t\tbuttons.append(get_step(s))\n\trangeselector['buttons']=buttons\n\treturn rangeselector", "response": "Returns a range selector for the current plot."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef view_label_matrix(L, colorbar=True):\n    L = L.todense() if sparse.issparse(L) else L\n    plt.imshow(L, aspect=\"auto\")\n    plt.title(\"Label Matrix\")\n    if colorbar:\n        labels = sorted(np.unique(np.asarray(L).reshape(-1, 1).squeeze()))\n        boundaries = np.array(labels + [max(labels) + 1]) - 0.5\n        plt.colorbar(boundaries=boundaries, ticks=labels)\n    plt.show()", "response": "Display an n m matrix of labels"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef view_overlaps(L, self_overlaps=False, normalize=True, colorbar=True):\n    L = L.todense() if sparse.issparse(L) else L\n    G = _get_overlaps_matrix(L, normalize=normalize)\n    if not self_overlaps:\n        np.fill_diagonal(G, 0)  # Zero out self-overlaps\n    plt.imshow(G, aspect=\"auto\")\n    plt.title(\"Overlaps\")\n    if colorbar:\n        plt.colorbar()\n    plt.show()", "response": "Display an matrix of overlaps"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef view_conflicts(L, normalize=True, colorbar=True):\n    L = L.todense() if sparse.issparse(L) else L\n    C = _get_conflicts_matrix(L, normalize=normalize)\n    plt.imshow(C, aspect=\"auto\")\n    plt.title(\"Conflicts\")\n    if colorbar:\n        plt.colorbar()\n    plt.show()", "response": "Display an matrix of conflicts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot a histogram from a numpy array of probabilities", "response": "def plot_probabilities_histogram(Y_p, title=None):\n    \"\"\"Plot a histogram from a numpy array of probabilities\n\n    Args:\n        Y_p: An [n] or [n, 1] np.ndarray of probabilities (floats in [0,1])\n    \"\"\"\n    if Y_p.ndim > 1:\n        msg = (\n            f\"Arg Y_p should be a 1-dimensional np.ndarray, not of shape \"\n            f\"{Y_p.shape}.\"\n        )\n        raise ValueError(msg)\n    plt.hist(Y_p, bins=20)\n    plt.xlim((0, 1.025))\n    plt.xlabel(\"Probability\")\n    plt.ylabel(\"# Predictions\")\n    if isinstance(title, str):\n        plt.title(title)\n    plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot_predictions_histogram(Y_ph, Y, title=None):\n    labels = list(set(Y).union(set(Y_ph)))\n    edges = [x - 0.5 for x in range(min(labels), max(labels) + 2)]\n\n    plt.hist([Y_ph, Y], bins=edges, label=[\"Predicted\", \"Gold\"])\n    ax = plt.gca()\n    ax.set_xticks(labels)\n    plt.xlabel(\"Label\")\n    plt.ylabel(\"# Predictions\")\n    plt.legend(loc=\"upper right\")\n    if isinstance(title, str):\n        plt.title(title)\n    plt.show()", "response": "Plot a histogram comparing int predictions vs true labels by class\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the logging frequency has been met.", "response": "def check(self, batch_size):\n        \"\"\"Returns True if the logging frequency has been met.\"\"\"\n        self.increment(batch_size)\n        return self.unit_count >= self.config[\"log_train_every\"]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef increment(self, batch_size):\n        self.example_count += batch_size\n        self.example_total += batch_size\n        if self.log_unit == \"seconds\":\n            self.unit_count = int(self.timer.elapsed())\n            self.unit_total = int(self.timer.total_elapsed())\n        elif self.log_unit == \"examples\":\n            self.unit_count = self.example_count\n            self.unit_total = self.example_total\n        elif self.log_unit == \"batches\":\n            self.unit_count += 1\n            self.unit_total += 1\n        elif self.log_unit == \"epochs\":\n            # Track epoch by example count because otherwise we only know when\n            # a new epoch starts, not when an epoch ends\n            if self.example_count >= self.epoch_size:\n                self.unit_count += 1\n                self.unit_total += 1\n        else:\n            raise Exception(f\"Unrecognized log_unit: {self.log_unit}\")", "response": "Update the total and relative unit counts based on the log_unit."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calculate_metrics(self, model, train_loader, valid_loader, metrics_dict):\n        # Check whether or not it's time for validation as well\n        self.log_count += 1\n        log_valid = (\n            valid_loader is not None\n            and self.valid_every_X\n            and not (self.log_count % self.valid_every_X)\n        )\n\n        metrics_dict = {}\n\n        # Calculate custom metrics\n        if self.config[\"log_train_metrics_func\"] is not None:\n            func = self.config[\"log_train_metrics_func\"]\n            func_list = func if isinstance(func, list) else [func]\n            for func in func_list:\n                metrics_dict = self._calculate_custom_metrics(\n                    model, train_loader, func, metrics_dict, split=\"train\"\n                )\n        if self.config[\"log_valid_metrics_func\"] is not None and log_valid:\n            func = self.config[\"log_valid_metrics_func\"]\n            func_list = func if isinstance(func, list) else [func]\n            for func in func_list:\n                metrics_dict = self._calculate_custom_metrics(\n                    model, valid_loader, func, metrics_dict, split=\"valid\"\n                )\n\n        # Calculate standard metrics\n        metrics_dict = self._calculate_standard_metrics(\n            model, train_loader, self.log_train_metrics, metrics_dict, \"train\"\n        )\n\n        if log_valid:\n            metrics_dict = self._calculate_standard_metrics(\n                model, valid_loader, self.log_valid_metrics, metrics_dict, \"valid\"\n            )\n\n        return metrics_dict", "response": "Calculate metrics for the current log count and train_loader."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef log(self, metrics_dict):\n        if self.writer:\n            self.write_to_file(metrics_dict)\n\n        if self.verbose:\n            self.print_to_screen(metrics_dict)\n        self.reset()", "response": "Print calculated metrics and optionally write to file and print to screen."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef print_to_screen(self, metrics_dict):\n        score_strings = defaultdict(list)\n        for split_metric, value in metrics_dict.items():\n            split, metric = split_metric.split(\"/\", 1)\n\n            if isinstance(value, float):\n                score_strings[split].append(f\"{metric}={value:0.3f}\")\n            else:\n                score_strings[split].append(f\"{metric}={value}\")\n\n        header = f\"{self.unit_total} {self.log_unit[:3]}\"\n        if self.log_unit != \"epochs\":\n            epochs = self.example_total / self.epoch_size\n            header += f\" ({epochs:0.2f} epo)\"\n        string = f\"[{header}]:\"\n\n        if score_strings[\"train\"]:\n            train_scores = f\"{', '.join(score_strings['train'])}\"\n            string += f\" TRAIN:[{train_scores}]\"\n        if score_strings[\"valid\"]:\n            valid_scores = f\"{', '.join(score_strings['valid'])}\"\n            string += f\" VALID:[{valid_scores}]\"\n        print(string)", "response": "Print all metrics in metrics_dict to screen"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _reduce_output(self, outputs, seq_lengths):\n        batch_size = outputs.shape[0]\n        reduced = []\n        # Necessary to iterate over batch because of different sequence lengths\n        for i in range(batch_size):\n            if self.lstm_reduction == \"mean\":\n                # Average over all non-padding reduced\n                # Use dim=0 because first dimension disappears after indexing\n                reduced.append(outputs[i, : seq_lengths[i], :].mean(dim=0))\n            elif self.lstm_reduction == \"max\":\n                # Max-pool over all non-padding reduced\n                # Use dim=0 because first dimension disappears after indexing\n                reduced.append(outputs[i, : seq_lengths[i], :].max(dim=0)[0])\n            elif self.lstm_reduction == \"last\":\n                # Take the last output of the sequence (before padding starts)\n                # NOTE: maybe better to take first and last?\n                reduced.append(outputs[i, seq_lengths[i] - 1, :])\n            elif self.lstm_reduction == \"attention\":\n                reduced.append(self._attention(outputs[i, : seq_lengths[i], :]))\n            else:\n                msg = (\n                    f\"Did not recognize lstm kwarg 'lstm_reduction' == \"\n                    f\"{self.lstm_reduction}\"\n                )\n                raise ValueError(msg)\n        return torch.stack(reduced, dim=0)", "response": "Reduces the output of an LSTM step into the hidden state outputs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\napplying one step of an lstm to the input X which is handled by self. encoder.", "response": "def forward(self, X):\n        \"\"\"Applies one step of an lstm (plus reduction) to the input X, which\n        is handled by self.encoder\"\"\"\n        # Identify the first non-zero integer from the right (i.e., the length\n        # of the sequence before padding starts).\n        batch_size, max_seq = X.shape[0], X.shape[1]\n        seq_lengths = torch.zeros(batch_size, dtype=torch.long)\n        for i in range(batch_size):\n            for j in range(max_seq - 1, -1, -1):\n                if not torch.all(X[i, j] == 0):\n                    seq_lengths[i] = j + 1\n                    break\n\n        # Sort by length because pack_padded_sequence requires it\n        # Save original order to restore before returning\n        seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n        X = X[perm_idx]\n        inv_perm_idx = torch.tensor(\n            [i for i, _ in sorted(enumerate(perm_idx), key=lambda idx: idx[1])],\n            dtype=torch.long,\n        )\n\n        # Encode and pack input sequence\n        X_packed = rnn_utils.pack_padded_sequence(\n            self.encoder.encode(X), seq_lengths, batch_first=True\n        )\n\n        # Run LSTM\n        outputs, (h_t, c_t) = self.lstm(X_packed)\n\n        # Unpack and reduce outputs\n        outputs_unpacked, _ = rnn_utils.pad_packed_sequence(outputs, batch_first=True)\n        reduced = self._reduce_output(outputs_unpacked, seq_lengths)\n        return reduced[inv_perm_idx, :]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef choose_other_label(k, y):\n    return choice(list(set(range(1, k + 1)) - set([y])))", "response": "Given a cardinality k and true label y return random value in\n    { 1... k } \\ y."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef gaussian_bags_of_words(Y, vocab=vocab1k, sigma=1, bag_size=[25, 50], **kwargs):\n\n    def make_distribution(sigma, num_words):\n        p = abs(np.random.normal(0, sigma, num_words))\n        return p / sum(p)\n\n    num_words = len(vocab)\n    word_dists = {y: make_distribution(sigma, num_words) for y in set(Y)}\n    bag_sizes = np.random.choice(range(min(bag_size), max(bag_size)), len(Y))\n\n    X = []\n    items = []\n    for i, (y, length) in enumerate(zip(Y, bag_sizes)):\n        x = torch.from_numpy(np.random.choice(num_words, length, p=word_dists[y]))\n        X.append(x)\n        items.append(\" \".join(vocab[j] for j in x))\n\n    return X, items", "response": "Generates a list of bags of words based on label assignments Y."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _generate_edges(self, edge_prob):\n        self.E, self.parent = [], {}\n        for i in range(self.m):\n            if random() < edge_prob and i > 0:\n                p_i = choice(i)\n                self.E.append((p_i, i))\n                self.parent[i] = p_i", "response": "Generate a random tree - structured dependency graph based on a specific edge probability."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the conditional probability that LF i outputs li and that LF j output lj and Y = y.", "response": "def P_conditional(self, i, li, j, lj, y):\n        \"\"\"Compute the conditional probability\n            P_\\theta(li | lj, y)\n            =\n            Z^{-1} exp(\n                theta_{i|y} \\indpm{ \\lambda_i = Y }\n                + \\theta_{i,j} \\indpm{ \\lambda_i = \\lambda_j }\n            )\n        In other words, compute the conditional probability that LF i outputs\n        li given that LF j output lj, and Y = y, parameterized by\n            - a class-conditional LF accuracy parameter \\theta_{i|y}\n            - a symmetric LF correlation paramter \\theta_{i,j}\n        \"\"\"\n        Z = np.sum([self._P(i, _li, j, lj, y) for _li in range(self.k + 1)])\n        return self._P(i, li, j, lj, y) / Z"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating an n m label matrix with entries in 0... k", "response": "def _generate_label_matrix(self):\n        \"\"\"Generate an [n,m] label matrix with entries in {0,...,k}\"\"\"\n        self.L = np.zeros((self.n, self.m))\n        self.Y = np.zeros(self.n, dtype=np.int64)\n        for i in range(self.n):\n            y = choice(self.k, p=self.p) + 1  # Note that y \\in {1,...,k}\n            self.Y[i] = y\n            for j in range(self.m):\n                p_j = self.parent.get(j, 0)\n                prob_y = self.P_conditional(j, y, p_j, self.L[i, p_j], y)\n                prob_0 = self.P_conditional(j, 0, p_j, self.L[i, p_j], y)\n                p = np.ones(self.k + 1) * (1 - prob_y - prob_0) / (self.k - 1)\n                p[0] = prob_0\n                p[y] = prob_y\n                self.L[i, j] = choice(self.k + 1, p=p)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the true clique conditional probabilities P \\ lC | Y by counting given L Y and then computing the ground truth for that cluster.", "response": "def _get_conditional_probs(self):\n        \"\"\"Compute the true clique conditional probabilities P(\\lC | Y) by\n        counting given L, Y; we'll use this as ground truth to compare to.\n\n        Note that this generates an attribute, self.c_probs, that has the same\n        definition as returned by `LabelModel.get_conditional_probs`.\n\n        TODO: Can compute these exactly if we want to implement that.\n        \"\"\"\n        # TODO: Extend to higher-order cliques again\n        self.c_probs = np.zeros((self.m * (self.k + 1), self.k))\n        for y in range(1, self.k + 1):\n            Ly = self.L[self.Y == y]\n            for ly in range(self.k + 1):\n                self.c_probs[ly :: (self.k + 1), y - 1] = (\n                    np.where(Ly == ly, 1, 0).sum(axis=0) / Ly.shape[0]\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fit_transform(self, input, **fit_kwargs):\n        self.fit(input, **fit_kwargs)\n        X = self.transform(input)\n        return X", "response": "Execute fit and transform in sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fit(self, sents, **kwargs):\n        tokens = list(itertools.chain.from_iterable(sents))\n        counter = Counter(tokens)\n        self.vocab = self.build_vocab(counter, **kwargs)", "response": "Builds a vocabulary object based on the tokens in the input."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting lists of tokens into a Tensor of embedding indices.", "response": "def transform(self, sents):\n        \"\"\"Converts lists of tokens into a Tensor of embedding indices.\n\n        Args:\n            sents: A list of lists of tokens (representing sentences)\n                NOTE: These sentences should already be marked using the\n                mark_entities() helper.\n        Returns:\n            X: A Tensor of shape (num_items, max_seq_len)\n        \"\"\"\n\n        def convert(tokens):\n            return torch.tensor([self.vocab.stoi[t] for t in tokens], dtype=torch.long)\n\n        if self.vocab is None:\n            raise Exception(\n                \"Must run .fit() for .fit_transform() before \" \"calling .transform().\"\n            )\n\n        seqs = sorted([convert(s) for s in sents], key=lambda x: -len(x))\n        X = torch.LongTensor(pad_sequence(seqs, batch_first=True))\n        return X"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pred_to_prob(Y_h, k):\n    Y_h = Y_h.clone()\n    if Y_h.dim() > 1:\n        Y_h = Y_h.squeeze()\n    assert Y_h.dim() == 1\n    assert (Y_h >= 1).all()\n    assert (Y_h <= k).all()\n    n = Y_h.shape[0]\n    Y_s = torch.zeros((n, k), dtype=Y_h.dtype, device=Y_h.device)\n    for i, j in enumerate(Y_h):\n        Y_s[i, j - 1] = 1.0\n    return Y_s", "response": "Converts a 1D tensor of predicted labels into a 2D tensor of probabilistic labels for item i and label k"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef arraylike_to_numpy(array_like):\n\n    orig_type = type(array_like)\n\n    # Convert to np.ndarray\n    if isinstance(array_like, np.ndarray):\n        pass\n    elif isinstance(array_like, list):\n        array_like = np.array(array_like)\n    elif issparse(array_like):\n        array_like = array_like.toarray()\n    elif isinstance(array_like, torch.Tensor):\n        array_like = array_like.numpy()\n    elif not isinstance(array_like, np.ndarray):\n        array_like = np.array(array_like)\n    else:\n        msg = f\"Input of type {orig_type} could not be converted to 1d \" \"np.ndarray\"\n        raise ValueError(msg)\n\n    # Correct shape\n    if (array_like.ndim > 1) and (1 in array_like.shape):\n        array_like = array_like.flatten()\n    if array_like.ndim != 1:\n        raise ValueError(\"Input could not be converted to 1d np.array\")\n\n    # Convert to ints\n    if any(array_like % 1):\n        raise ValueError(\"Input contains at least one non-integer value.\")\n    array_like = array_like.astype(np.dtype(int))\n\n    return array_like", "response": "Convert a 1d array - like ( e. g. list tensor etc. to a np. ndarray"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a matrix from one label type to another.", "response": "def convert_labels(Y, source, dest):\n    \"\"\"Convert a matrix from one label type to another\n\n    Args:\n        Y: A np.ndarray or torch.Tensor of labels (ints)\n        source: The convention the labels are currently expressed in\n        dest: The convention to convert the labels to\n\n    Conventions:\n        'categorical': [0: abstain, 1: positive, 2: negative]\n        'plusminus': [0: abstain, 1: positive, -1: negative]\n        'onezero': [0: negative, 1: positive]\n\n    Note that converting to 'onezero' will combine abstain and negative labels.\n    \"\"\"\n    if Y is None:\n        return Y\n    if isinstance(Y, np.ndarray):\n        Y = Y.copy()\n        assert isinstance(Y, int)\n    elif isinstance(Y, torch.Tensor):\n        Y = Y.clone()\n        assert np.sum(Y.numpy() - Y.numpy().astype(int)) == 0.0\n    else:\n        raise ValueError(\"Unrecognized label data type.\")\n    negative_map = {\"categorical\": 2, \"plusminus\": -1, \"onezero\": 0}\n    Y[Y == negative_map[source]] = negative_map[dest]\n    return Y"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a 2D label matrix L into a 3D tensor where each element in L is a one hot class.", "response": "def label_matrix_to_one_hot(L, k=None):\n    \"\"\"Converts a 2D [n,m] label matrix into an [n,m,k] one hot 3D tensor\n\n    Note that in the returned 3D matrix, abstain votes continue to be\n    represented by 0s, not 1s.\n\n    Args:\n        L: a [n,m] label matrix with categorical labels (0 = abstain)\n        k: the number of classes that could appear in L\n            if None, k is inferred as the max element in L\n    \"\"\"\n    n, m = L.shape\n    if k is None:\n        k = L.max()\n    L_onehot = torch.zeros(n, m, k + 1)\n    for i, row in enumerate(L):\n        for j, k in enumerate(row):\n            if k > 0:\n                L_onehot[i, j, k - 1] = 1\n    return L_onehot"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmerges dictionary y into a copy of x, overwriting elements of x when there is a conflict, except if the element is a dictionary, in which case recurse. misses: what to do if a key in y is not in x 'insert' -> set x[key] = value 'exception' -> raise an exception 'report' -> report the name of the missing key 'ignore' -> do nothing TODO: give example here (pull from tests)", "response": "def recursive_merge_dicts(x, y, misses=\"report\", verbose=None):\n    \"\"\"\n    Merge dictionary y into a copy of x, overwriting elements of x when there\n    is a conflict, except if the element is a dictionary, in which case recurse.\n\n    misses: what to do if a key in y is not in x\n        'insert'    -> set x[key] = value\n        'exception' -> raise an exception\n        'report'    -> report the name of the missing key\n        'ignore'    -> do nothing\n\n    TODO: give example here (pull from tests)\n    \"\"\"\n\n    def recurse(x, y, misses=\"report\", verbose=1):\n        found = True\n        for k, v in y.items():\n            found = False\n            if k in x:\n                found = True\n                if isinstance(x[k], dict):\n                    if not isinstance(v, dict):\n                        msg = f\"Attempted to overwrite dict {k} with \" f\"non-dict: {v}\"\n                        raise ValueError(msg)\n                    recurse(x[k], v, misses, verbose)\n                else:\n                    if x[k] == v:\n                        msg = f\"Reaffirming {k}={x[k]}\"\n                    else:\n                        msg = f\"Overwriting {k}={x[k]} to {k}={v}\"\n                        x[k] = v\n                    if verbose > 1 and k != \"verbose\":\n                        print(msg)\n            else:\n                for kx, vx in x.items():\n                    if isinstance(vx, dict):\n                        found = recurse(vx, {k: v}, misses=\"ignore\", verbose=verbose)\n                    if found:\n                        break\n            if not found:\n                msg = f'Could not find kwarg \"{k}\" in destination dict.'\n                if misses == \"insert\":\n                    x[k] = v\n                    if verbose > 1:\n                        print(f\"Added {k}={v} from second dict to first\")\n                elif misses == \"exception\":\n                    raise ValueError(msg)\n                elif misses == \"report\":\n                    print(msg)\n                else:\n                    pass\n        return found\n\n    # If verbose is not provided, look for an value in y first, then x\n    # (Do this because 'verbose' kwarg is often inside one or both of x and y)\n    if verbose is None:\n        verbose = y.get(\"verbose\", x.get(\"verbose\", 1))\n\n    z = copy.deepcopy(x)\n    recurse(z, y, misses, verbose)\n    return z"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef split_data(\n    *inputs,\n    splits=[0.5, 0.5],\n    shuffle=True,\n    stratify_by=None,\n    index_only=False,\n    seed=None,\n):\n    \"\"\"Splits inputs into multiple splits of defined sizes\n\n    Args:\n        inputs: correlated tuples/lists/arrays/matrices/tensors to split\n        splits: list containing split sizes (fractions or counts);\n        shuffle: if True, shuffle the data before splitting\n        stratify_by: (None or an input) if not None, use these labels to\n            stratify the splits (separating the data into groups by these\n            labels and sampling from those, rather than from the population at\n            large); overrides shuffle\n        index_only: if True, return only the indices of the new splits, not the\n            split data itself\n        seed: (int) random seed\n\n    Example usage:\n        Ls, Xs, Ys = split_data(L, X, Y, splits=[0.8, 0.1, 0.1])\n        OR\n        assignments = split_data(Y, splits=[0.8, 0.1, 0.1], index_only=True)\n\n    Note: This is very similar to scikit-learn's train_test_split() method,\n        but with support for more than two splits.\n    \"\"\"\n\n    def fractions_to_counts(fracs, n):\n        \"\"\"Converts a list of fractions to a list of counts that sum to n\"\"\"\n        counts = [int(np.round(n * frac)) for frac in fracs]\n        # Ensure sum of split counts sums to n\n        counts[-1] = n - sum(counts[:-1])\n        return counts\n\n    def slice_data(data, indices):\n        if isinstance(data, list) or isinstance(data, tuple):\n            return [d for i, d in enumerate(data) if i in set(indices)]\n        else:\n            try:\n                # Works for np.ndarray, scipy.sparse, torch.Tensor\n                return data[indices]\n            except TypeError:\n                raise Exception(\n                    f\"split_data() currently only accepts inputs \"\n                    f\"of type tuple, list, np.ndarray, scipy.sparse, or \"\n                    f\"torch.Tensor; not {type(data)}\"\n                )\n\n    # Setting random seed\n    if seed is not None:\n        random.seed(seed)\n\n    try:\n        n = len(inputs[0])\n    except TypeError:\n        n = inputs[0].shape[0]\n    num_splits = len(splits)\n\n    # Check splits for validity and convert to fractions\n    if all(isinstance(x, int) for x in splits):\n        if not sum(splits) == n:\n            raise ValueError(\n                f\"Provided split counts must sum to n ({n}), not {sum(splits)}.\"\n            )\n        fracs = [count / n for count in splits]\n\n    elif all(isinstance(x, float) for x in splits):\n        if not sum(splits) == 1.0:\n            raise ValueError(f\"Split fractions must sum to 1.0, not {sum(splits)}.\")\n        fracs = splits\n\n    else:\n        raise ValueError(\"Splits must contain all ints or all floats.\")\n\n    # Make sampling pools\n    if stratify_by is None:\n        pools = [np.arange(n)]\n    else:\n        pools = defaultdict(list)\n        for i, val in enumerate(stratify_by):\n            pools[val].append(i)\n        pools = list(pools.values())\n\n    # Make index assignments\n    assignments = [[] for _ in range(num_splits)]\n    for pool in pools:\n        if shuffle or stratify_by is not None:\n            random.shuffle(pool)\n\n        counts = fractions_to_counts(fracs, len(pool))\n        counts.insert(0, 0)\n        cum_counts = np.cumsum(counts)\n        for i in range(num_splits):\n            assignments[i].extend(pool[cum_counts[i] : cum_counts[i + 1]])\n\n    if index_only:\n        return assignments\n    else:\n        outputs = []\n        for data in inputs:\n            data_splits = []\n            for split in range(num_splits):\n                data_splits.append(slice_data(data, assignments[split]))\n            outputs.append(data_splits)\n\n        if len(outputs) == 1:\n            return outputs[0]\n        else:\n            return outputs", "response": "Splits the data into multiple sets of items for the given size."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating and attaches task_heads to the appropriate network layers.", "response": "def _build_task_heads(self, head_modules):\n        \"\"\"Creates and attaches task_heads to the appropriate network layers\"\"\"\n        # Make task head layer assignments\n        num_layers = len(self.config[\"layer_out_dims\"])\n        task_head_layers = self._set_task_head_layers(num_layers)\n\n        # task_head_layers stores the layer whose output is input to task head t\n        # task_map stores the task heads that appear at each layer\n        self.task_map = defaultdict(list)\n        for t, l in enumerate(task_head_layers):\n            self.task_map[l].append(t)\n\n        if any(l == 0 for l in task_head_layers) and head_modules is None:\n            raise Exception(\n                \"If any task head is being attached to layer 0 \"\n                \"(the input modules), then you must provide a t-length list of \"\n                \"head_modules, since the output dimension of each input_module \"\n                \"cannot be inferred.\"\n            )\n\n        # Construct heads\n        head_dims = [self.K[t] for t in range(self.t)]\n\n        heads = nn.ModuleList()\n        for t in range(self.t):\n            input_dim = self.config[\"layer_out_dims\"][task_head_layers[t]]\n            if self.config[\"pass_predictions\"]:\n                for p in self.task_graph.parents[t]:\n                    input_dim += head_dims[p]\n            output_dim = head_dims[t]\n\n            if head_modules is None:\n                head = nn.Linear(input_dim, output_dim)\n            elif isinstance(head_modules, list):\n                head = head_modules[t]\n            else:\n                head = copy.deepcopy(head_modules)\n            heads.append(head)\n        return heads"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef forward(self, x):\n        head_outputs = [None] * self.t\n\n        # Execute input layer\n        if isinstance(self.input_layer, list):  # One input_module per task\n            input_outputs = [mod(x) for mod, x in zip(self.input_layer, x)]\n            x = torch.stack(input_outputs, dim=1)\n\n            # Execute level-0 task heads from their respective input modules\n            for t in self.task_map[0]:\n                head = self.heads[t]\n                head_outputs[t] = head(input_outputs[t])\n        else:  # One input_module for all tasks\n            x = self.input_layer(x)\n\n            # Execute level-0 task heads from the single input module\n            for t in self.task_map[0]:\n                head = self.heads[t]\n                head_outputs[t] = head(x)\n\n        # Execute middle layers\n        for i, layer in enumerate(self.middle_layers, start=1):\n            x = layer(x)\n\n            # Attach level-i task heads from the ith middle module\n            for t in self.task_map[i]:\n                head = self.heads[t]\n                # Optionally include as input the predictions of parent tasks\n                if self.config[\"pass_predictions\"] and bool(self.task_graph.parents[t]):\n                    task_input = [x]\n                    for p in self.task_graph.parents[t]:\n                        task_input.append(head_outputs[p])\n                    task_input = torch.stack(task_input, dim=1)\n                else:\n                    task_input = x\n                head_outputs[t] = head(task_input)\n        return head_outputs", "response": "Returns a list of outputs for tasks 0... t - 1."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts Y to t - length list of probabilistic labels if necessary", "response": "def _preprocess_Y(self, Y, k=None):\n        \"\"\"Convert Y to t-length list of probabilistic labels if necessary\"\"\"\n        # If not a list, convert to a singleton list\n        if not isinstance(Y, list):\n            if self.t != 1:\n                msg = \"For t > 1, Y must be a list of n-dim or [n, K_t] tensors\"\n                raise ValueError(msg)\n            Y = [Y]\n\n        if not len(Y) == self.t:\n            msg = f\"Expected Y to be a t-length list (t={self.t}), not {len(Y)}\"\n            raise ValueError(msg)\n\n        return [EndModel._preprocess_Y(self, Y_t, self.K[t]) for t, Y_t in enumerate(Y)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the loss function to use in the train_model routine", "response": "def _get_loss_fn(self):\n        \"\"\"Returns the loss function to use in the train_model routine\"\"\"\n        criteria = self.criteria.to(self.config[\"device\"])\n        loss_fn = lambda X, Y: sum(\n            criteria(Y_tp, Y_t) for Y_tp, Y_t in zip(self.forward(X), Y)\n        )\n        return loss_fn"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _create_L_ind(self, L):\n        # TODO: Update LabelModel to keep L, L_ind, L_aug as sparse matrices\n        # throughout and remove this line.\n        if issparse(L[0]):\n            L = [L_t.todense() for L_t in L]\n\n        # Make sure converted to numpy here\n        L = self._to_numpy(L)\n\n        L_ind = np.ones((self.n, self.m * self.k))\n        for yi, y in enumerate(self.task_graph.feasible_set()):\n            for t in range(self.t):\n                # A[x::y] slices A starting at x at intervals of y\n                # e.g., np.arange(9)[0::3] == np.array([0,3,6])\n                L_ind[:, yi :: self.k] *= np.where(\n                    np.logical_or(L[t] == y[t], L[t] == 0), 1, 0\n                )\n\n            # Set LFs that abstained on all feasible label vectors to all 0s\n            L_ind[:, yi :: self.k] *= np.where(sum(L) != 0, 1, 0)\n\n        return L_ind", "response": "Convert T label matrices with labels in 0... K_t to a one - hot format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the probability distribution of the cluster class entries in the task graph.", "response": "def predict_proba(self, L):\n        \"\"\"Returns the task marginals estimated by the model: a t-length list of\n        [n,k_t] matrices where the (i,j) entry of the sth matrix represents the\n        estimated P((Y_i)_s | \\lambda_j(x_i))\n\n        Args:\n            L: A t-length list of [n,m] scipy.sparse label matrices with values\n                in {0,1,...,k}\n        \"\"\"\n        # First, get the estimated probability distribution over the feasible\n        # set defined by the TaskGraph\n        # This is an [n,k] array, where k = |(feasible set)|\n        Y_pf = LabelModel.predict_proba(self, L)\n        n, k = Y_pf.shape\n\n        # Now get the per-task marginals\n        # TODO: Make this optional, versus just returning the above\n        Y_p = [np.zeros((n, k_t)) for k_t in self.task_graph.K]\n        for yi, y in enumerate(self.task_graph.feasible_set()):\n            for t in range(self.t):\n                k_t = int(y[t])\n                Y_p[t][:, k_t - 1] += Y_pf[:, yi]\n        return Y_p"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npredicting labels for an input X on all tasks.", "response": "def predict(self, X, break_ties=\"random\", return_probs=False, **kwargs):\n        \"\"\"Predicts (int) labels for an input X on all tasks\n\n        Args:\n            X: The input for the predict_proba method\n            break_ties: A tie-breaking policy (see Classifier._break_ties())\n            return_probs: Return the predicted probabilities as well\n\n        Returns:\n            Y_p: An n-dim np.ndarray of predictions in {1,...k}\n            [Optionally: Y_s: An [n, k] np.ndarray of predicted probabilities]\n        \"\"\"\n        Y_s = self._to_numpy(self.predict_proba(X, **kwargs))\n        Y_p = self._break_ties(Y_s, break_ties).astype(np.int)\n        if return_probs:\n            return Y_p, Y_s\n        else:\n            return Y_p"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nscoring the predictive performance of the Classifier on all tasks in the cluster.", "response": "def score(\n        self,\n        data,\n        metric=\"accuracy\",\n        break_ties=\"random\",\n        verbose=True,\n        print_confusion_matrix=True,\n        **kwargs,\n    ):\n        \"\"\"Scores the predictive performance of the Classifier on all tasks\n\n        Args:\n            data: a Pytorch DataLoader, Dataset, or tuple with Tensors (X,Y):\n                X: The input for the predict method\n                Y: An [n] or [n, 1] torch.Tensor or np.ndarray of target labels\n                    in {1,...,k}\n            metric: A metric (string) with which to score performance or a\n                list of such metrics\n            break_ties: A tie-breaking policy (see Classifier._break_ties())\n            verbose: The verbosity for just this score method; it will not\n                update the class config.\n            print_confusion_matrix: Print confusion matrix (overwritten to False if\n                verbose=False)\n\n        Returns:\n            scores: A (float) score or a list of such scores if kwarg metric\n                is a list\n        \"\"\"\n        Y_p, Y, Y_s = self._get_predictions(\n            data, break_ties=break_ties, return_probs=True, **kwargs\n        )\n\n        # Evaluate on the specified metrics\n        return_list = isinstance(metric, list)\n        metric_list = metric if isinstance(metric, list) else [metric]\n        scores = []\n        for metric in metric_list:\n            score = metric_score(Y, Y_p, metric, probs=Y_s, ignore_in_gold=[0])\n            scores.append(score)\n            if verbose:\n                print(f\"{metric.capitalize()}: {score:.3f}\")\n\n        # Optionally print confusion matrix\n        if print_confusion_matrix and verbose:\n            confusion_matrix(Y, Y_p, pretty_print=True)\n\n        # If a single metric was given as a string (not list), return a float\n        if len(scores) == 1 and not return_list:\n            return scores[0]\n        else:\n            return scores"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _train_model(\n        self, train_data, loss_fn, valid_data=None, log_writer=None, restore_state={}\n    ):\n        \"\"\"The internal training routine called by train_model() after setup\n\n        Args:\n            train_data: a tuple of Tensors (X,Y), a Dataset, or a DataLoader of\n                X (data) and Y (labels) for the train split\n            loss_fn: the loss function to minimize (maps *data -> loss)\n            valid_data: a tuple of Tensors (X,Y), a Dataset, or a DataLoader of\n                X (data) and Y (labels) for the dev split\n            restore_state: a dictionary containing model weights (optimizer, main network) and training information\n\n        If valid_data is not provided, then no checkpointing or\n        evaluation on the dev set will occur.\n        \"\"\"\n        # Set model to train mode\n        self.train()\n        train_config = self.config[\"train_config\"]\n\n        # Convert data to DataLoaders\n        train_loader = self._create_data_loader(train_data)\n        valid_loader = self._create_data_loader(valid_data)\n        epoch_size = len(train_loader.dataset)\n\n        # Move model to GPU\n        if self.config[\"verbose\"] and self.config[\"device\"] != \"cpu\":\n            print(\"Using GPU...\")\n        self.to(self.config[\"device\"])\n\n        # Set training components\n        self._set_writer(train_config)\n        self._set_logger(train_config, epoch_size)\n        self._set_checkpointer(train_config)\n        self._set_optimizer(train_config)\n        self._set_scheduler(train_config)\n\n        # Restore model if necessary\n        if restore_state:\n            start_iteration = self._restore_training_state(restore_state)\n        else:\n            start_iteration = 0\n\n        # Train the model\n        metrics_hist = {}  # The most recently seen value for all metrics\n        for epoch in range(start_iteration, train_config[\"n_epochs\"]):\n            progress_bar = (\n                train_config[\"progress_bar\"]\n                and self.config[\"verbose\"]\n                and self.logger.log_unit == \"epochs\"\n            )\n\n            t = tqdm(\n                enumerate(train_loader),\n                total=len(train_loader),\n                disable=(not progress_bar),\n            )\n\n            self.running_loss = 0.0\n            self.running_examples = 0\n            for batch_num, data in t:\n                # NOTE: actual batch_size may not equal config's target batch_size\n                batch_size = len(data[0])\n\n                # Moving data to device\n                if self.config[\"device\"] != \"cpu\":\n                    data = place_on_gpu(data)\n\n                # Zero the parameter gradients\n                self.optimizer.zero_grad()\n\n                # Forward pass to calculate the average loss per example\n                loss = loss_fn(*data)\n                if torch.isnan(loss):\n                    msg = \"Loss is NaN. Consider reducing learning rate.\"\n                    raise Exception(msg)\n\n                # Backward pass to calculate gradients\n                # Loss is an average loss per example\n                loss.backward()\n\n                # Perform optimizer step\n                self.optimizer.step()\n\n                # Calculate metrics, log, and checkpoint as necessary\n                metrics_dict = self._execute_logging(\n                    train_loader, valid_loader, loss, batch_size\n                )\n                metrics_hist.update(metrics_dict)\n\n                # tqdm output\n                t.set_postfix(loss=metrics_dict[\"train/loss\"])\n\n            # Apply learning rate scheduler\n            self._update_scheduler(epoch, metrics_hist)\n\n        self.eval()\n\n        # Restore best model if applicable\n        if self.checkpointer:\n            self.checkpointer.load_best_model(model=self)\n\n        # Write log if applicable\n        if self.writer:\n            if self.writer.include_config:\n                self.writer.add_config(self.config)\n            self.writer.close()\n\n        # Print confusion matrix if applicable\n        if self.config[\"verbose\"]:\n            print(\"Finished Training\")\n            if valid_loader is not None:\n                self.score(\n                    valid_loader,\n                    metric=train_config[\"validation_metric\"],\n                    verbose=True,\n                    print_confusion_matrix=True,\n                )", "response": "This function is called by train_model to train the model and store the training information in the internal object self. train_data and self. loss_fn."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save(self, destination, **kwargs):\n        with open(destination, \"wb\") as f:\n            torch.save(self, f, **kwargs)", "response": "Serialize and save a model."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load(source, **kwargs):\n        with open(source, \"rb\") as f:\n            return torch.load(f, **kwargs)", "response": "Deserialize and load a model.\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresumes training of a classifier by reloading the appropriate state_dicts for each model.", "response": "def resume_training(self, train_data, model_path, valid_data=None):\n        \"\"\"This model resume training of a classifier by reloading the appropriate state_dicts for each model\n\n        Args:\n           train_data: a tuple of Tensors (X,Y), a Dataset, or a DataLoader of\n                X (data) and Y (labels) for the train split\n            model_path: the path to the saved checpoint for resuming training\n            valid_data: a tuple of Tensors (X,Y), a Dataset, or a DataLoader of\n                X (data) and Y (labels) for the dev split\n        \"\"\"\n        restore_state = self.checkpointer.restore(model_path)\n        loss_fn = self._get_loss_fn()\n        self.train()\n        self._train_model(\n            train_data=train_data,\n            loss_fn=loss_fn,\n            valid_data=valid_data,\n            restore_state=restore_state,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrestoring the model and optimizer states This helper function restores the model's state to a given iteration so that a user can resume training at any epoch. Args: restore_state: a state_dict dictionary", "response": "def _restore_training_state(self, restore_state):\n        \"\"\"Restores the model and optimizer states\n\n        This helper function restores the model's state to a given iteration so\n        that a user can resume training at any epoch.\n\n        Args:\n            restore_state: a state_dict dictionary\n        \"\"\"\n        self.load_state_dict(restore_state[\"model\"])\n        self.optimizer.load_state_dict(restore_state[\"optimizer\"])\n        self.lr_scheduler.load_state_dict(restore_state[\"lr_scheduler\"])\n        start_iteration = restore_state[\"iteration\"] + 1\n        if self.config[\"verbose\"]:\n            print(f\"Restored checkpoint to iteration {start_iteration}.\")\n\n        if restore_state[\"best_model_found\"]:\n            # Update checkpointer with appropriate information about best model\n            # Note that the best model found so far may not be the model in the\n            # checkpoint that is currently being loaded.\n            self.checkpointer.best_model_found = True\n            self.checkpointer.best_iteration = restore_state[\"best_iteration\"]\n            self.checkpointer.best_score = restore_state[\"best_score\"]\n            if self.config[\"verbose\"]:\n                print(\n                    f\"Updated checkpointer: \"\n                    f\"best_score={self.checkpointer.best_score:.3f}, \"\n                    f\"best_iteration={self.checkpointer.best_iteration}\"\n                )\n        return start_iteration"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_dataset(self, *data):\n        # Make sure data is a tuple of dense tensors\n        data = [self._to_torch(x, dtype=torch.FloatTensor) for x in data]\n        return TensorDataset(*data)", "response": "Converts input data to the appropriate Dataset"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts input data into a DataLoader", "response": "def _create_data_loader(self, data, **kwargs):\n        \"\"\"Converts input data into a DataLoader\"\"\"\n        if data is None:\n            return None\n\n        # Set DataLoader config\n        # NOTE: Not applicable if data is already a DataLoader\n        config = {\n            **self.config[\"train_config\"][\"data_loader_config\"],\n            **kwargs,\n            \"pin_memory\": self.config[\"device\"] != \"cpu\",\n        }\n        # Return data as DataLoader\n        if isinstance(data, DataLoader):\n            return data\n        elif isinstance(data, Dataset):\n            return DataLoader(data, **config)\n        elif isinstance(data, (tuple, list)):\n            return DataLoader(self._create_dataset(*data), **config)\n        else:\n            raise ValueError(\"Input data type not recognized.\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing predictions in batch given a labeled dataset or tuple with Tensors X Y and Y_s.", "response": "def _get_predictions(self, data, break_ties=\"random\", return_probs=False, **kwargs):\n        \"\"\"Computes predictions in batch, given a labeled dataset\n\n        Args:\n            data: a Pytorch DataLoader, Dataset, or tuple with Tensors (X,Y):\n                X: The input for the predict method\n                Y: An [n] or [n, 1] torch.Tensor or np.ndarray of target labels\n                    in {1,...,k}\n            break_ties: How to break ties when making predictions\n            return_probs: Return the predicted probabilities as well\n\n        Returns:\n            Y_p: A Tensor of predictions\n            Y: A Tensor of labels\n            [Optionally: Y_s: An [n, k] np.ndarray of predicted probabilities]\n        \"\"\"\n        data_loader = self._create_data_loader(data)\n        Y_p = []\n        Y = []\n        Y_s = []\n\n        # Do batch evaluation by default, getting the predictions and labels\n        for batch_num, data in enumerate(data_loader):\n            Xb, Yb = data\n            Y.append(self._to_numpy(Yb))\n\n            # Optionally move to device\n            if self.config[\"device\"] != \"cpu\":\n                Xb = place_on_gpu(Xb)\n\n            # Append predictions and labels from DataLoader\n            Y_pb, Y_sb = self.predict(\n                Xb, break_ties=break_ties, return_probs=True, **kwargs\n            )\n            Y_p.append(self._to_numpy(Y_pb))\n            Y_s.append(self._to_numpy(Y_sb))\n        Y_p, Y, Y_s = map(self._stack_batches, [Y_p, Y, Y_s])\n        if return_probs:\n            return Y_p, Y, Y_s\n        else:\n            return Y_p, Y"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _break_ties(self, Y_s, break_ties=\"random\"):\n        n, k = Y_s.shape\n        Y_h = np.zeros(n)\n        diffs = np.abs(Y_s - Y_s.max(axis=1).reshape(-1, 1))\n\n        TOL = 1e-5\n        for i in range(n):\n            max_idxs = np.where(diffs[i, :] < TOL)[0]\n            if len(max_idxs) == 1:\n                Y_h[i] = max_idxs[0] + 1\n            # Deal with \"tie votes\" according to the specified policy\n            elif break_ties == \"random\":\n                Y_h[i] = np.random.choice(max_idxs) + 1\n            elif break_ties == \"abstain\":\n                Y_h[i] = 0\n            elif isinstance(break_ties, int):\n                Y_h[i] = break_ties\n            else:\n                ValueError(f\"break_ties={break_ties} policy not recognized.\")\n        return Y_h", "response": "Break ties in each row of a tensor according to the specified policy\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _to_numpy(Z):\n        if Z is None:\n            return Z\n        elif issparse(Z):\n            return Z.toarray()\n        elif isinstance(Z, np.ndarray):\n            return Z\n        elif isinstance(Z, list):\n            return np.array(Z)\n        elif isinstance(Z, torch.Tensor):\n            return Z.cpu().numpy()\n        else:\n            msg = (\n                f\"Expected None, list, numpy.ndarray or torch.Tensor, \"\n                f\"got {type(Z)} instead.\"\n            )\n            raise Exception(msg)", "response": "Converts a None list np. ndarray or torch. Tensor to np. ndarray ; also handles converting sparse input to dense."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a None list np. ndarray or torch. Tensor to torch. Tensor ; also handles converting sparse input to dense.", "response": "def _to_torch(Z, dtype=None):\n        \"\"\"Converts a None, list, np.ndarray, or torch.Tensor to torch.Tensor;\n        also handles converting sparse input to dense.\"\"\"\n        if Z is None:\n            return None\n        elif issparse(Z):\n            Z = torch.from_numpy(Z.toarray())\n        elif isinstance(Z, torch.Tensor):\n            pass\n        elif isinstance(Z, list):\n            Z = torch.from_numpy(np.array(Z))\n        elif isinstance(Z, np.ndarray):\n            Z = torch.from_numpy(Z)\n        else:\n            msg = (\n                f\"Expected list, numpy.ndarray or torch.Tensor, \"\n                f\"got {type(Z)} instead.\"\n            )\n            raise Exception(msg)\n\n        return Z.type(dtype) if dtype else Z"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting a warning statement just once.", "response": "def warn_once(self, msg, msg_name=None):\n        \"\"\"Prints a warning statement just once\n\n        Args:\n            msg: The warning message\n            msg_name: [optional] The name of the warning. If None, the msg_name\n                will be the msg itself.\n        \"\"\"\n        assert isinstance(msg, str)\n        msg_name = msg_name if msg_name else msg\n        if msg_name not in warnings_given:\n            warnings.warn(msg)\n        warnings_given.add(msg_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _stack_batches(X):\n        X = [Classifier._to_numpy(Xb) for Xb in X]\n        if len(X[0].shape) == 1:\n            return np.hstack(X)\n        elif len(X[0].shape) == 2:\n            return np.vstack(X)\n        else:\n            raise ValueError(f\"Can't stack {len(X[0].shape)}-dim batches.\")", "response": "Stack a list of np. ndarrays along the first axis returning an an\n        np. ndarray ; note this is only for smooth hanlding of the multi - task\n        setting."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _preprocess_Y(self, Y, k):\n        Y = Y.clone()\n\n        # If preds, convert to probs\n        if Y.dim() == 1 or Y.shape[1] == 1:\n            Y = pred_to_prob(Y.long(), k=k)\n        return Y", "response": "Convert Y to prob labels if necessary"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a [ n k ) tensor of probs ( probabilistic labels ).", "response": "def predict_proba(self, X):\n        \"\"\"Returns a [n, k] tensor of probs (probabilistic labels).\"\"\"\n        return F.softmax(self.forward(X), dim=1).data.cpu().numpy()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting sparse linear layer", "response": "def forward(self, X):\n        \"\"\"Execute sparse linear layer\n\n        Args:\n            X: an [n, h] torch.LongTensor containing up to h indices of features\n                whose weights should be looked up and used in a sparse linear\n                multiplication.\n        \"\"\"\n        return self.W(X).sum(dim=1) + self.b"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef splits(\n        cls,\n        conn_str,\n        candidate_def,\n        word_dict=None,\n        train=0,\n        dev=1,\n        test=2,\n        use_lfs=(0, 0, 0),\n        pretrained_word_dict=None,\n        max_seq_len=125,\n    ):\n        \"\"\"\n        Create train/dev/test splits (mapped to split numbers)\n\n        :param conn_str:\n        :param candidate_def:\n        :param word_dict:\n        :param train:\n        :param dev:\n        :param test:\n        :param use_lfs:\n        :param pretrained_word_dict:\n        :param max_seq_len:\n        :return:\n\n        \"\"\"\n        # initialize word_dict if needed\n        train_set = cls(\n            conn_str,\n            candidate_def,\n            word_dict=word_dict,\n            split=train,\n            use_lfs=use_lfs[train],\n            pretrained_word_dict=pretrained_word_dict,\n            max_seq_len=max_seq_len,\n        )\n        return (\n            train_set,\n            cls(\n                conn_str,\n                candidate_def,\n                word_dict=train_set.word_dict,\n                split=dev,\n                use_lfs=use_lfs[dev],\n                max_seq_len=max_seq_len,\n            ),\n            cls(\n                conn_str,\n                candidate_def,\n                word_dict=train_set.word_dict,\n                split=test,\n                use_lfs=use_lfs[test],\n                max_seq_len=max_seq_len,\n            ),\n        )", "response": "Create train dev and test sets for a single entry in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _mark_entities(self, c, markers):\n        sent = c.get_parent().words\n        positions = [\n            [c[i].get_word_start(), c[i].get_word_end()]\n            for i in range(self.cardinality)\n        ]\n        seq = mark_entities(sent, positions, markers=markers, style=\"insert\")\n        return [w for w in seq if w.strip()]", "response": "Convert Snorkel candidates to marked up sequences\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _include_pretrained_vocab(self, pretrained_word_dict, candidates):\n        terms = Counter()\n        for c in candidates:\n            for w in c.get_parent().words:\n                if w in pretrained_word_dict:\n                    terms[w] += 1\n        list(map(self.word_dict.get, terms))", "response": "Include terms available via pretrained embeddings\nCOOKIES."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build_vocab(self, sentences, markers=[]):\n        from snorkel.learning.pytorch.rnn.utils import SymbolTable\n\n        vocab = Counter()\n        for sent in sentences:\n            for w in sent:\n                vocab[w] += 1\n        word_dict = SymbolTable()\n        list(map(word_dict.get, vocab))\n        list(map(word_dict.get, markers))\n        return word_dict", "response": "Build a dictionary of symbol tables based on the sentences and markers."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _check_L(self, L):\n        # TODO: Take this out?\n        if issparse(L):\n            L = L.todense()\n\n        # Check for correct values, e.g. warning if in {-1,0,1}\n        if np.any(L < 0):\n            raise ValueError(\"L must have values in {0,1,...,k}.\")", "response": "Run some basic checks on the given log matrix L."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_L_ind(self, L):\n        # TODO: Update LabelModel to keep L variants as sparse matrices\n        # throughout and remove this line.\n        if issparse(L):\n            L = L.todense()\n\n        L_ind = np.zeros((self.n, self.m * self.k))\n        for y in range(1, self.k + 1):\n            # A[x::y] slices A starting at x at intervals of y\n            # e.g., np.arange(9)[0::3] == np.array([0,3,6])\n            L_ind[:, (y - 1) :: self.k] = np.where(L == y, 1, 0)\n        return L_ind", "response": "Convert a label matrix with labels in 0... k to a one - hot format."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an augmented version of L where each column is an indicator for whether a certain source or clique of sources voted in a certain pattern.", "response": "def _get_augmented_label_matrix(self, L, higher_order=False):\n        \"\"\"Returns an augmented version of L where each column is an indicator\n        for whether a certain source or clique of sources voted in a certain\n        pattern.\n\n        Args:\n            L: An [n,m] scipy.sparse label matrix with values in {0,1,...,k}\n        \"\"\"\n        # Create a helper data structure which maps cliques (as tuples of member\n        # sources) --> {start_index, end_index, maximal_cliques}, where\n        # the last value is a set of indices in this data structure\n        self.c_data = {}\n        for i in range(self.m):\n            self.c_data[i] = {\n                \"start_index\": i * self.k,\n                \"end_index\": (i + 1) * self.k,\n                \"max_cliques\": set(\n                    [\n                        j\n                        for j in self.c_tree.nodes()\n                        if i in self.c_tree.node[j][\"members\"]\n                    ]\n                ),\n            }\n\n        L_ind = self._create_L_ind(L)\n\n        # Get the higher-order clique statistics based on the clique tree\n        # First, iterate over the maximal cliques (nodes of c_tree) and\n        # separator sets (edges of c_tree)\n        if higher_order:\n            L_aug = np.copy(L_ind)\n            for item in chain(self.c_tree.nodes(), self.c_tree.edges()):\n                if isinstance(item, int):\n                    C = self.c_tree.node[item]\n                    C_type = \"node\"\n                elif isinstance(item, tuple):\n                    C = self.c_tree[item[0]][item[1]]\n                    C_type = \"edge\"\n                else:\n                    raise ValueError(item)\n                members = list(C[\"members\"])\n                nc = len(members)\n\n                # If a unary maximal clique, just store its existing index\n                if nc == 1:\n                    C[\"start_index\"] = members[0] * self.k\n                    C[\"end_index\"] = (members[0] + 1) * self.k\n\n                # Else add one column for each possible value\n                else:\n                    L_C = np.ones((self.n, self.k ** nc))\n                    for i, vals in enumerate(product(range(self.k), repeat=nc)):\n                        for j, v in enumerate(vals):\n                            L_C[:, i] *= L_ind[:, members[j] * self.k + v]\n\n                    # Add to L_aug and store the indices\n                    if L_aug is not None:\n                        C[\"start_index\"] = L_aug.shape[1]\n                        C[\"end_index\"] = L_aug.shape[1] + L_C.shape[1]\n                        L_aug = np.hstack([L_aug, L_C])\n                    else:\n                        C[\"start_index\"] = 0\n                        C[\"end_index\"] = L_C.shape[1]\n                        L_aug = L_C\n\n                    # Add to self.c_data as well\n                    id = tuple(members) if len(members) > 1 else members[0]\n                    self.c_data[id] = {\n                        \"start_index\": C[\"start_index\"],\n                        \"end_index\": C[\"end_index\"],\n                        \"max_cliques\": set([item]) if C_type == \"node\" else set(item),\n                    }\n            return L_aug\n        else:\n            return L_ind"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _build_mask(self):\n        self.mask = torch.ones(self.d, self.d).byte()\n        for ci in self.c_data.values():\n            si, ei = ci[\"start_index\"], ci[\"end_index\"]\n            for cj in self.c_data.values():\n                sj, ej = cj[\"start_index\"], cj[\"end_index\"]\n\n                # Check if ci and cj are part of the same maximal clique\n                # If so, mask out their corresponding blocks in O^{-1}\n                if len(ci[\"max_cliques\"].intersection(cj[\"max_cliques\"])) > 0:\n                    self.mask[si:ei, sj:ej] = 0\n                    self.mask[sj:ej, si:ei] = 0", "response": "Build the mask applied to O^{ - 1 } O for the matrix approx constraint"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _generate_O(self, L):\n        L_aug = self._get_augmented_label_matrix(L)\n        self.d = L_aug.shape[1]\n        self.O = torch.from_numpy(L_aug.T @ L_aug / self.n).float()", "response": "Form the overlaps matrix which is just all the different observed\n        combinations of values of pairs of sources\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _generate_O_inv(self, L):\n        self._generate_O(L)\n        self.O_inv = torch.from_numpy(np.linalg.inv(self.O.numpy())).float()", "response": "Form the inverse overlaps matrix"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes the learned params for the cluster - level parameters.", "response": "def _init_params(self):\n        \"\"\"Initialize the learned params\n\n        - \\mu is the primary learned parameter, where each row corresponds to\n        the probability of a clique C emitting a specific combination of labels,\n        conditioned on different values of Y (for each column); that is:\n\n            self.mu[i*self.k + j, y] = P(\\lambda_i = j | Y = y)\n\n        and similarly for higher-order cliques.\n        - Z is the inverse form version of \\mu.\n        \"\"\"\n        train_config = self.config[\"train_config\"]\n\n        # Initialize mu so as to break basic reflective symmetry\n        # Note that we are given either a single or per-LF initial precision\n        # value, prec_i = P(Y=y|\\lf=y), and use:\n        #   mu_init = P(\\lf=y|Y=y) = P(\\lf=y) * prec_i / P(Y=y)\n\n        # Handle single or per-LF values\n        if isinstance(train_config[\"prec_init\"], (int, float)):\n            prec_init = train_config[\"prec_init\"] * torch.ones(self.m)\n        else:\n            prec_init = torch.from_numpy(train_config[\"prec_init\"])\n            if prec_init.shape[0] != self.m:\n                raise ValueError(f\"prec_init must have shape {self.m}.\")\n\n        # Get the per-value labeling propensities\n        # Note that self.O must have been computed already!\n        lps = torch.diag(self.O).numpy()\n\n        # TODO: Update for higher-order cliques!\n        self.mu_init = torch.zeros(self.d, self.k)\n        for i in range(self.m):\n            for y in range(self.k):\n                idx = i * self.k + y\n                mu_init = torch.clamp(lps[idx] * prec_init[i] / self.p[y], 0, 1)\n                self.mu_init[idx, y] += mu_init\n\n        # Initialize randomly based on self.mu_init\n        self.mu = nn.Parameter(self.mu_init.clone() * np.random.random()).float()\n\n        if self.inv_form:\n            self.Z = nn.Parameter(torch.randn(self.d, self.k)).float()\n\n        # Build the mask over O^{-1}\n        # TODO: Put this elsewhere?\n        self._build_mask()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the full conditional probabilities table as a numpy array.", "response": "def get_conditional_probs(self, source=None):\n        \"\"\"Returns the full conditional probabilities table as a numpy array,\n        where row i*(k+1) + ly is the conditional probabilities of source i\n        emmiting label ly (including abstains 0), conditioned on different\n        values of Y, i.e.:\n\n            c_probs[i*(k+1) + ly, y] = P(\\lambda_i = ly | Y = y)\n\n        Note that this simply involves inferring the kth row by law of total\n        probability and adding in to mu.\n\n        If `source` is not None, returns only the corresponding block.\n        \"\"\"\n        c_probs = np.zeros((self.m * (self.k + 1), self.k))\n        mu = self.mu.detach().clone().numpy()\n\n        for i in range(self.m):\n            # si = self.c_data[(i,)]['start_index']\n            # ei = self.c_data[(i,)]['end_index']\n            # mu_i = mu[si:ei, :]\n            mu_i = mu[i * self.k : (i + 1) * self.k, :]\n            c_probs[i * (self.k + 1) + 1 : (i + 1) * (self.k + 1), :] = mu_i\n\n            # The 0th row (corresponding to abstains) is the difference between\n            # the sums of the other rows and one, by law of total prob\n            c_probs[i * (self.k + 1), :] = 1 - mu_i.sum(axis=0)\n        c_probs = np.clip(c_probs, 0.01, 0.99)\n\n        if source is not None:\n            return c_probs[source * (self.k + 1) : (source + 1) * (self.k + 1)]\n        else:\n            return c_probs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef predict_proba(self, L):\n        self._set_constants(L)\n\n        L_aug = self._get_augmented_label_matrix(L)\n        mu = np.clip(self.mu.detach().clone().numpy(), 0.01, 0.99)\n\n        # Create a \"junction tree mask\" over the columns of L_aug / mu\n        if len(self.deps) > 0:\n            jtm = np.zeros(L_aug.shape[1])\n\n            # All maximal cliques are +1\n            for i in self.c_tree.nodes():\n                node = self.c_tree.node[i]\n                jtm[node[\"start_index\"] : node[\"end_index\"]] = 1\n\n            # All separator sets are -1\n            for i, j in self.c_tree.edges():\n                edge = self.c_tree[i][j]\n                jtm[edge[\"start_index\"] : edge[\"end_index\"]] = 1\n        else:\n            jtm = np.ones(L_aug.shape[1])\n\n        # Note: We omit abstains, effectively assuming uniform distribution here\n        X = np.exp(L_aug @ np.diag(jtm) @ np.log(mu) + np.log(self.p))\n        Z = np.tile(X.sum(axis=1).reshape(-1, 1), self.k)\n        return X / Z", "response": "Returns the [ n k ) matrix of label probabilities P Y | \\ lambda"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_Q(self):\n        Z = self.Z.detach().clone().numpy()\n        O = self.O.numpy()\n        I_k = np.eye(self.k)\n        return O @ Z @ np.linalg.inv(I_k + Z.T @ O @ Z) @ Z.T @ O", "response": "Get the model s estimate of Q = \\ mu P \\ mu^T\n        We can then separately extract \\ mu subject to additional constraints and return \\ mu P \\ mu^T\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _set_class_balance(self, class_balance, Y_dev):\n        if class_balance is not None:\n            self.p = np.array(class_balance)\n        elif Y_dev is not None:\n            class_counts = Counter(Y_dev)\n            sorted_counts = np.array([v for k, v in sorted(class_counts.items())])\n            self.p = sorted_counts / sum(sorted_counts)\n        else:\n            self.p = (1 / self.k) * np.ones(self.k)\n        self.P = torch.diag(torch.from_numpy(self.p)).float()", "response": "Set a prior for the class balance from the user - provided class_balance Y_dev"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef train_model(\n        self,\n        L_train,\n        Y_dev=None,\n        deps=[],\n        class_balance=None,\n        log_writer=None,\n        **kwargs,\n    ):\n        \"\"\"Train the model (i.e. estimate mu) in one of two ways, depending on\n        whether source dependencies are provided or not:\n\n        Args:\n            L_train: An [n,m] scipy.sparse matrix with values in {0,1,...,k}\n                corresponding to labels from supervision sources on the\n                training set\n            Y_dev: Target labels for the dev set, for estimating class_balance\n            deps: (list of tuples) known dependencies between supervision\n                sources. If not provided, sources are assumed to be independent.\n                TODO: add automatic dependency-learning code\n            class_balance: (np.array) each class's percentage of the population\n\n        (1) No dependencies (conditionally independent sources): Estimate mu\n        subject to constraints:\n            (1a) O_{B(i,j)} - (mu P mu.T)_{B(i,j)} = 0, for i != j, where B(i,j)\n                is the block of entries corresponding to sources i,j\n            (1b) np.sum( mu P, 1 ) = diag(O)\n\n        (2) Source dependencies:\n            - First, estimate Z subject to the inverse form\n            constraint:\n                (2a) O_\\Omega + (ZZ.T)_\\Omega = 0, \\Omega is the deps mask\n            - Then, compute Q = mu P mu.T\n            - Finally, estimate mu subject to mu P mu.T = Q and (1b)\n        \"\"\"\n        self.config = recursive_merge_dicts(self.config, kwargs, misses=\"ignore\")\n        train_config = self.config[\"train_config\"]\n\n        # TODO: Implement logging for label model?\n        if log_writer is not None:\n            raise NotImplementedError(\"Logging for LabelModel.\")\n\n        # Note that the LabelModel class implements its own (centered) L2 reg.\n        l2 = train_config.get(\"l2\", 0)\n\n        self._set_class_balance(class_balance, Y_dev)\n        self._set_constants(L_train)\n        self._set_dependencies(deps)\n        self._check_L(L_train)\n\n        # Whether to take the simple conditionally independent approach, or the\n        # \"inverse form\" approach for handling dependencies\n        # This flag allows us to eg test the latter even with no deps present\n        self.inv_form = len(self.deps) > 0\n\n        # Creating this faux dataset is necessary for now because the LabelModel\n        # loss functions do not accept inputs, but Classifer._train_model()\n        # expects training data to feed to the loss functions.\n        dataset = MetalDataset([0], [0])\n        train_loader = DataLoader(dataset)\n        if self.inv_form:\n            # Compute O, O^{-1}, and initialize params\n            if self.config[\"verbose\"]:\n                print(\"Computing O^{-1}...\")\n            self._generate_O_inv(L_train)\n            self._init_params()\n\n            # Estimate Z, compute Q = \\mu P \\mu^T\n            if self.config[\"verbose\"]:\n                print(\"Estimating Z...\")\n            self._train_model(train_loader, self.loss_inv_Z)\n            self.Q = torch.from_numpy(self.get_Q()).float()\n\n            # Estimate \\mu\n            if self.config[\"verbose\"]:\n                print(\"Estimating \\mu...\")\n            self._train_model(train_loader, partial(self.loss_inv_mu, l2=l2))\n        else:\n            # Compute O and initialize params\n            if self.config[\"verbose\"]:\n                print(\"Computing O...\")\n            self._generate_O(L_train)\n            self._init_params()\n\n            # Estimate \\mu\n            if self.config[\"verbose\"]:\n                print(\"Estimating \\mu...\")\n            self._train_model(train_loader, partial(self.loss_mu, l2=l2))", "response": "Train the label model on the given set of known classes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntransforming the input label matrix L to a three - way overlaps tensor.", "response": "def _get_overlaps_tensor(self, L):\n        \"\"\"Transforms the input label matrix to a three-way overlaps tensor.\n\n        Args:\n            L: (np.array) An n x m array of LF output labels, in {0,...,k} if\n                self.abstains, else in {1,...,k}, generated by m conditionally\n                independent LFs on n data points\n\n        Outputs:\n            O: (torch.Tensor) A (m, m, m, k, k, k) tensor of the label-specific\n            empirical overlap rates; that is,\n\n                O[i,j,k,y1,y2,y3] = P(\\lf_i = y1, \\lf_j = y2, \\lf_k = y3)\n\n            where this quantity is computed empirically by this function, based\n            on the label matrix L.\n        \"\"\"\n        n, m = L.shape\n\n        # Convert from a (n,m) matrix of ints to a (k_lf, n, m) indicator tensor\n        LY = np.array([np.where(L == y, 1, 0) for y in range(self.k_0, self.k + 1)])\n\n        # Form the three-way overlaps matrix\n        O = np.einsum(\"abc,dbe,fbg->cegadf\", LY, LY, LY) / n\n        return torch.from_numpy(O).float()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_mask(self, m):\n        mask = torch.ones((m, m, m, self.k_lf, self.k_lf, self.k_lf)).byte()\n        for i, j, k in product(range(m), repeat=3):\n            if len(set((i, j, k))) < 3:\n                mask[i, j, k, :, :, :] = 0\n        return mask", "response": "Get the three - way overlaps matrix O which is 0 when indices i j k are not unique"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_char_offsets(self, char_offsets):\n        if char_offsets:\n            char_offsets = char_offsets\n            char_offsets.append(len(self.text))\n        else:\n            char_offsets = np.zeros(len(self.tokens) + 1)\n            for i, tok in enumerate(self.tokens):\n                # Add 1 to account for the spaces between tokens\n                char_offsets[i + 1] = char_offsets[i] + len(tok) + 1\n            char_offsets[-1] = len(self.text)\n        return np.array(char_offsets)", "response": "Store or calculate char_offsets"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npredicts the probabilistic labels for a set of class entry.", "response": "def predict_proba(self, L):\n        \"\"\"\n        Args:\n            L: An [n, m] scipy.sparse matrix of labels\n        Returns:\n            output: A [n, k] np.ndarray of probabilistic labels\n        \"\"\"\n        n = L.shape[0]\n        Y_p = np.random.rand(n, self.k)\n        Y_p /= Y_p.sum(axis=1).reshape(-1, 1)\n        return Y_p"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef train_model(self, balance, *args, **kwargs):\n        self.balance = np.array(balance)", "response": "Train the model for the given resource class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef feasible_set(self):\n        for y in itertools.product(*[range(1, k + 1) for k in self.K]):\n            yield np.array(y)", "response": "Iterator over values in feasible set"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the accuracy score of a node.", "response": "def accuracy_score(gold, pred, ignore_in_gold=[], ignore_in_pred=[]):\n    \"\"\"\n    Calculate (micro) accuracy.\n    Args:\n        gold: A 1d array-like of gold labels\n        pred: A 1d array-like of predicted labels (assuming abstain = 0)\n        ignore_in_gold: A list of labels for which elements having that gold\n            label will be ignored.\n        ignore_in_pred: A list of labels for which elements having that pred\n            label will be ignored.\n\n    Returns:\n        A float, the (micro) accuracy score\n    \"\"\"\n    gold, pred = _preprocess(gold, pred, ignore_in_gold, ignore_in_pred)\n\n    if len(gold) and len(pred):\n        acc = np.sum(gold == pred) / len(gold)\n    else:\n        acc = 0\n\n    return acc"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef coverage_score(gold, pred, ignore_in_gold=[], ignore_in_pred=[]):\n    gold, pred = _preprocess(gold, pred, ignore_in_gold, ignore_in_pred)\n\n    return np.sum(pred != 0) / len(pred)", "response": "Calculate the coverage score for a node."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate precision score for a single class.", "response": "def precision_score(gold, pred, pos_label=1, ignore_in_gold=[], ignore_in_pred=[]):\n    \"\"\"\n    Calculate precision for a single class.\n    Args:\n        gold: A 1d array-like of gold labels\n        pred: A 1d array-like of predicted labels (assuming abstain = 0)\n        ignore_in_gold: A list of labels for which elements having that gold\n            label will be ignored.\n        ignore_in_pred: A list of labels for which elements having that pred\n            label will be ignored.\n        pos_label: The class label to treat as positive for precision\n\n    Returns:\n        pre: The (float) precision score\n    \"\"\"\n    gold, pred = _preprocess(gold, pred, ignore_in_gold, ignore_in_pred)\n\n    positives = np.where(pred == pos_label, 1, 0).astype(bool)\n    trues = np.where(gold == pos_label, 1, 0).astype(bool)\n    TP = np.sum(positives * trues)\n    FP = np.sum(positives * np.logical_not(trues))\n\n    if TP or FP:\n        pre = TP / (TP + FP)\n    else:\n        pre = 0\n\n    return pre"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef recall_score(gold, pred, pos_label=1, ignore_in_gold=[], ignore_in_pred=[]):\n    gold, pred = _preprocess(gold, pred, ignore_in_gold, ignore_in_pred)\n\n    positives = np.where(pred == pos_label, 1, 0).astype(bool)\n    trues = np.where(gold == pos_label, 1, 0).astype(bool)\n    TP = np.sum(positives * trues)\n    FN = np.sum(np.logical_not(positives) * trues)\n\n    if TP or FN:\n        rec = TP / (TP + FN)\n    else:\n        rec = 0\n\n    return rec", "response": "Calculate recall score for a single class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the f - beta score for a single class.", "response": "def fbeta_score(\n    gold, pred, pos_label=1, beta=1.0, ignore_in_gold=[], ignore_in_pred=[]\n):\n    \"\"\"\n    Calculate recall for a single class.\n    Args:\n        gold: A 1d array-like of gold labels\n        pred: A 1d array-like of predicted labels (assuming abstain = 0)\n        ignore_in_gold: A list of labels for which elements having that gold\n            label will be ignored.\n        ignore_in_pred: A list of labels for which elements having that pred\n            label will be ignored.\n        pos_label: The class label to treat as positive for f-beta\n        beta: The beta to use in the f-beta score calculation\n\n    Returns:\n        fbeta: The (float) f-beta score\n    \"\"\"\n    gold, pred = _preprocess(gold, pred, ignore_in_gold, ignore_in_pred)\n    pre = precision_score(gold, pred, pos_label=pos_label)\n    rec = recall_score(gold, pred, pos_label=pos_label)\n\n    if pre or rec:\n        fbeta = (1 + beta ** 2) * (pre * rec) / ((beta ** 2 * pre) + rec)\n    else:\n        fbeta = 0\n\n    return fbeta"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the ROC - AUC score given the gold labels and predicted probabilities.", "response": "def roc_auc_score(gold, probs, ignore_in_gold=[], ignore_in_pred=[]):\n    \"\"\"Compute the ROC AUC score, given the gold labels and predicted probs.\n\n    Args:\n        gold: A 1d array-like of gold labels\n        probs: A 2d array-like of predicted probabilities\n        ignore_in_gold: A list of labels for which elements having that gold\n            label will be ignored.\n\n    Returns:\n        roc_auc_score: The (float) roc_auc score\n    \"\"\"\n    gold = arraylike_to_numpy(gold)\n\n    # Filter out the ignore_in_gold (but not ignore_in_pred)\n    # Note the current sub-functions (below) do not handle this...\n    if len(ignore_in_pred) > 0:\n        raise ValueError(\"ignore_in_pred not defined for ROC-AUC score.\")\n    keep = [x not in ignore_in_gold for x in gold]\n    gold = gold[keep]\n    probs = probs[keep, :]\n\n    # Convert gold to one-hot indicator format, using the k inferred from probs\n    gold_s = pred_to_prob(torch.from_numpy(gold), k=probs.shape[1]).numpy()\n    return skm.roc_auc_score(gold_s, probs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove from gold and pred all items with labels designated to ignore.", "response": "def _drop_ignored(gold, pred, ignore_in_gold, ignore_in_pred):\n    \"\"\"Remove from gold and pred all items with labels designated to ignore.\"\"\"\n    keepers = np.ones_like(gold).astype(bool)\n    for x in ignore_in_gold:\n        keepers *= np.where(gold != x, 1, 0).astype(bool)\n    for x in ignore_in_pred:\n        keepers *= np.where(pred != x, 1, 0).astype(bool)\n\n    gold = gold[keepers]\n    pred = pred[keepers]\n    return gold, pred"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndumping the log to file", "response": "def write(self):\n        \"\"\"Dump JSON to file\"\"\"\n        with open(self.log_path, \"w\") as f:\n            json.dump(self.log_dict, f, indent=1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npredict int labels for an input X on all tasks and returns the predicted probabilities.", "response": "def predict(self, X, break_ties=\"random\", return_probs=False, **kwargs):\n        \"\"\"Predicts int labels for an input X on all tasks\n\n        Args:\n            X: The input for the predict_proba method\n            break_ties: A tie-breaking policy\n            return_probs: Return the predicted probabilities as well\n\n        Returns:\n            Y_p: A t-length list of n-dim np.ndarrays of predictions in [1, K_t]\n            [Optionally: Y_s: A t-length list of [n, K_t] np.ndarrays of\n                predicted probabilities]\n        \"\"\"\n        Y_s = self.predict_proba(X, **kwargs)\n        self._check(Y_s, typ=list)\n        self._check(Y_s[0], typ=np.ndarray)\n\n        Y_p = []\n        for Y_ts in Y_s:\n            Y_tp = self._break_ties(Y_ts, break_ties)\n            Y_p.append(Y_tp.astype(np.int))\n\n        if return_probs:\n            return Y_p, Y_s\n        else:\n            return Y_p"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nscore the predictive performance of the Classifier on all tasks in the cluster.", "response": "def score(\n        self,\n        data,\n        metric=\"accuracy\",\n        validation_task=None,\n        reduce=\"mean\",\n        break_ties=\"random\",\n        verbose=True,\n        print_confusion_matrix=False,\n        **kwargs,\n    ):\n        \"\"\"Scores the predictive performance of the Classifier on all tasks\n        Args:\n            data: either a Pytorch Dataset, DataLoader or tuple supplying (X,Y):\n                X: The input for the predict method\n                Y: A t-length list of [n] or [n, 1] np.ndarrays or\n                   torch.Tensors of gold labels in {1,...,K_t}\n            metric: The metric with which to score performance on each task\n            validation_task:\n                int: returns score for specific task number.\n            reduce: How to reduce the scores of multiple tasks:\n                 None : return a t-length list of scores\n                'mean': return the mean score across tasks\n            break_ties: How to break ties when making predictions\n        Returns:\n            scores: A (float) score or a t-length list of such scores if\n                reduce=None\n        \"\"\"\n        Y_p, Y, Y_s = self._get_predictions(\n            data, break_ties=break_ties, return_probs=True, **kwargs\n        )\n\n        # TODO: Handle multiple metrics...\n        metric_list = metric if isinstance(metric, list) else [metric]\n        if len(metric_list) > 1:\n            raise NotImplementedError(\n                \"Multiple metrics for multi-task score() not yet supported.\"\n            )\n        metric = metric_list[0]\n\n        # Return score for task t only.\n        if validation_task is not None:\n            score = metric_score(\n                Y[validation_task],\n                Y_p[validation_task],\n                metric,\n                probs=Y_s[validation_task],\n                ignore_in_gold=[0],\n            )\n            if verbose:\n                print(f\"{metric.capitalize()}: {score:.3f}\")\n            return score\n\n        task_scores = []\n        for t, Y_tp in enumerate(Y_p):\n            score = metric_score(Y[t], Y_tp, metric, probs=Y_s[t], ignore_in_gold=[0])\n            task_scores.append(score)\n\n        # TODO: Other options for reduce, including scoring only certain\n        # primary tasks, and converting to end labels using TaskGraph...\n        if reduce is None:\n            score = task_scores\n        elif reduce == \"mean\":\n            score = np.mean(task_scores)\n        else:\n            raise Exception(f\"Keyword reduce='{reduce}' not recognized.\")\n\n        if verbose:\n            if reduce is None:\n                for t, score_t in enumerate(score):\n                    print(f\"{metric.capitalize()} (t={t}): {score_t:0.3f}\")\n            else:\n                print(f\"{metric.capitalize()}: {score:.3f}\")\n\n        return score"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef score_task(self, X, Y, t=0, metric=\"accuracy\", verbose=True, **kwargs):\n        Y = self._to_numpy(Y)\n        Y_tp = self.predict_task(X, t=t, **kwargs)\n        probs = self.predict_proba(X)[t]\n        score = metric_score(\n            Y[t], Y_tp, metric, ignore_in_gold=[0], probs=probs, **kwargs\n        )\n        if verbose:\n            print(f\"[t={t}] {metric.capitalize()}: {score:.3f}\")\n        return score", "response": "Scores the predictive performance of the Classifier on the specified task t\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef predict_task(self, X, t=0, break_ties=\"random\", **kwargs):\n        Y_tp = self.predict_task_proba(X, t=t, **kwargs)\n        Y_tph = self._break_ties(Y_tp, break_ties)\n        return Y_tph", "response": "Predicts int labels for an input X on task t"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npredicting probabilistic labels for an input X on task t.", "response": "def predict_task_proba(self, X, t=0, **kwargs):\n        \"\"\"Predicts probabilistic labels for an input X on task t\n\n        Args:\n            X: The input for the predict_proba method\n            t: The task index to predict for which to predict probabilities\n        Returns:\n            An [n, K_t] tensor of predictions for task t\n        NOTE: By default, this method calls predict_proba and extracts element\n        t. If it is possible to predict individual tasks in isolation, however,\n        this method may be overriden for efficiency's sake.\n        \"\"\"\n        return self.predict_proba(X, **kwargs)[t]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _to_torch(Z, dtype=None):\n        if isinstance(Z, list):\n            return [Classifier._to_torch(z, dtype=dtype) for z in Z]\n        else:\n            return Classifier._to_torch(Z)", "response": "Converts a None list np. ndarray or torch. Tensor to torch. Tensor"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _to_numpy(Z):\n        if isinstance(Z, list):\n            return [Classifier._to_numpy(z) for z in Z]\n        else:\n            return Classifier._to_numpy(Z)", "response": "Converts a None list np. ndarray or torch. Tensor to np. ndarray"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pretty_print_schedule(self, hyperband_schedule, describe_hyperband=True):\n        print(\"=========================================\")\n        print(\"|           Hyperband Schedule          |\")\n        print(\"=========================================\")\n        if describe_hyperband:\n            # Print a message indicating what the below schedule means\n            print(\n                \"Table consists of tuples of \"\n                \"(num configs, num_resources_per_config) \"\n                \"which specify how many configs to run and \"\n                \"for how many epochs. \"\n            )\n            print(\n                \"Each bracket starts with a list of random \"\n                \"configurations which is successively halved \"\n                \"according the schedule.\"\n            )\n            print(\n                \"See the Hyperband paper \"\n                \"(https://arxiv.org/pdf/1603.06560.pdf) for more details.\"\n            )\n            print(\"-----------------------------------------\")\n        for bracket_index, bracket in enumerate(hyperband_schedule):\n            bracket_string = \"Bracket %d:\" % bracket_index\n            for n_i, r_i in bracket:\n                bracket_string += \" (%d, %d)\" % (n_i, r_i)\n            print(bracket_string)\n        print(\"-----------------------------------------\")", "response": "Prints the schedule for the user to read."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_largest_schedule_within_budget(self, budget, proportion_discard):\n\n        # Exhaustively generate schedules and check if\n        # they're within budget, adding to a list.\n        valid_schedules_and_costs = []\n        for R in range(1, budget):\n            schedule = self.generate_hyperband_schedule(R, proportion_discard)\n            cost = self.compute_schedule_cost(schedule)\n            if cost <= budget:\n                valid_schedules_and_costs.append((schedule, cost))\n\n        # Choose a valid schedule that maximizes usage of the budget.\n        valid_schedules_and_costs.sort(key=lambda x: x[1], reverse=True)\n        return valid_schedules_and_costs[0][0]", "response": "Returns the largest hyperband schedule within target budget."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_hyperband_schedule(self, R, eta):\n        schedule = []\n        s_max = int(math.floor(math.log(R, eta)))\n        # B = (s_max + 1) * R\n        for s in range(0, s_max + 1):\n            n = math.ceil(int((s_max + 1) / (s + 1)) * eta ** s)\n            r = R * eta ** (-s)\n            bracket = []\n            for i in range(0, s + 1):\n                n_i = int(math.floor(n * eta ** (-i)))\n                r_i = int(r * eta ** i)\n                bracket.append((n_i, r_i))\n            schedule = [bracket] + schedule\n        return schedule", "response": "Generate hyperband schedule according to the paper."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef search(\n        self,\n        search_space,\n        valid_data,\n        init_args=[],\n        train_args=[],\n        init_kwargs={},\n        train_kwargs={},\n        module_args={},\n        module_kwargs={},\n        max_search=None,\n        shuffle=True,\n        verbose=True,\n        seed=None,\n        **score_kwargs,\n    ):\n        \"\"\"\n        Performs hyperband search according to the generated schedule.\n\n        At the beginning of each bracket, we generate a\n        list of random configurations and perform\n        successive halving on it; we repeat this process\n        for the number of brackets in the schedule.\n\n        Args:\n            init_args: (list) positional args for initializing the model\n            train_args: (list) positional args for training the model\n            valid_data: a tuple of Tensors (X,Y), a Dataset, or a DataLoader of\n                X (data) and Y (labels) for the dev split\n            search_space: see ModelTuner's config_generator() documentation\n            max_search: see ModelTuner's config_generator() documentation\n            shuffle: see ModelTuner's config_generator() documentation\n\n        Returns:\n            best_model: the highest performing trained model found by Hyperband\n            best_config: (dict) the config corresponding to the best model\n\n        Note: Initialization is performed by ModelTuner instead of passing a\n        pre-initialized model so that tuning may be performed over all model\n        parameters, including the network architecture (which is defined before\n        the train loop).\n        \"\"\"\n        self._clear_state(seed)\n        self.search_space = search_space\n\n        # Loop over each bracket\n        n_models_scored = 0\n        for bracket_index, bracket in enumerate(self.hyperband_schedule):\n\n            # Sample random configurations to seed SuccessiveHalving\n            n_starting_configurations, _ = bracket[0]\n            configurations = list(\n                self.config_generator(\n                    search_space,\n                    max_search=n_starting_configurations,\n                    rng=self.rng,\n                    shuffle=True,\n                )\n            )\n\n            # Successive Halving\n            for band_index, (n_i, r_i) in enumerate(bracket):\n\n                assert len(configurations) <= n_i\n\n                # Evaluate each configuration for r_i epochs\n                scored_configurations = []\n                for i, configuration in enumerate(configurations):\n\n                    cur_model_index = n_models_scored\n\n                    # Set epochs of the configuration\n                    configuration[\"n_epochs\"] = r_i\n\n                    # Train model and get the score\n                    score, model = self._test_model_config(\n                        f\"{band_index}_{i}\",\n                        configuration,\n                        valid_data,\n                        init_args=init_args,\n                        train_args=train_args,\n                        init_kwargs=init_kwargs,\n                        train_kwargs=train_kwargs,\n                        module_args=module_args,\n                        module_kwargs=module_kwargs,\n                        verbose=verbose,\n                        **score_kwargs,\n                    )\n\n                    # Add score and model to list\n                    scored_configurations.append(\n                        (score, cur_model_index, configuration)\n                    )\n                    n_models_scored += 1\n\n                # Sort scored configurations by score\n                scored_configurations.sort(key=lambda x: x[0], reverse=True)\n\n                # Successively halve the configurations\n                if band_index + 1 < len(bracket):\n                    n_to_keep, _ = bracket[band_index + 1]\n                    configurations = [x[2] for x in scored_configurations][:n_to_keep]\n\n        print(\"=\" * 60)\n        print(f\"[SUMMARY]\")\n        print(f\"Best model: [{self.best_index}]\")\n        print(f\"Best config: {self.best_config}\")\n        print(f\"Best score: {self.best_score}\")\n        print(\"=\" * 60)\n\n        # Return best model\n        return self._load_best_model(clean_up=True)", "response": "This function searches the modelTuner for the given set of models in the given set of models."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmark the entities at specific positions in the given list of tokens.", "response": "def mark_entities(tokens, positions, markers=[], style=\"insert\"):\n    \"\"\"Adds special markers around tokens at specific positions (e.g., entities)\n\n    Args:\n        tokens: A list of tokens (the sentence)\n        positions:\n            1) A list of inclusive ranges (tuples) corresponding to the\n            token ranges of the entities in order. (Assumes each entity\n            has only one corresponding mention.)\n            OR\n            2) A dict of lists with keys corresponding to mention indices and\n            values corresponding to one or more inclusive ranges corresponding\n            to that mention. (Allows entities to potentially have multiple\n            mentions)\n        markers: A list of strings (length of 2 * the number of entities) to\n            use as markers of the entities.\n        style: Where to apply the markers:\n            'insert': Insert the markers as new tokens before/after each entity\n            'concatenate': Prepend/append the markers to the first/last token\n                of each entity\n            If the tokens are going to be input to an LSTM, then it is usually\n            best to use the 'insert' option; 'concatenate' may be better for\n            viewing.\n\n    Returns:\n        toks: An extended list of tokens with markers around the mentions\n\n    WARNING: if the marked token set will be used with pretrained embeddings,\n        provide markers that will not result in UNK embeddings!\n\n    Example:\n        Input:  (['The', 'cat', 'sat'], [(1,1)])\n        Output: ['The', '[[BEGIN0]]', 'cat', '[[END0]]', 'sat']\n    \"\"\"\n    if markers and len(markers) != 2 * len(positions):\n        msg = (\n            f\"Expected len(markers) == 2 * len(positions), \"\n            f\"but {len(markers)} != {2 * len(positions)}.\"\n        )\n        raise ValueError(msg)\n\n    toks = list(tokens)\n\n    # markings will be of the form:\n    # [(position, entity_idx), (position, entity_idx), ...]\n    if isinstance(positions, list):\n        markings = [(position, idx) for idx, position in enumerate(positions)]\n    elif isinstance(positions, dict):\n        markings = []\n        for idx, v in positions.items():\n            for position in v:\n                markings.append((position, idx))\n    else:\n        msg = (\n            f\"Argument _positions_ must be a list or dict. \"\n            f\"Instead, got {type(positions)}\"\n        )\n        raise ValueError(msg)\n\n    markings = sorted(markings)\n    for i, ((si, ei), idx) in enumerate(markings):\n        if markers:\n            start_marker = markers[2 * idx]\n            end_marker = markers[2 * idx + 1]\n        else:\n            start_marker = f\"[[BEGIN{idx}]]\"\n            end_marker = f\"[[END{idx}]]\"\n        if style == \"insert\":\n            toks.insert(si + 2 * i, start_marker)\n            toks.insert(ei + 2 * (i + 1), end_marker)\n        elif style == \"concatenate\":\n            toks[si] = start_marker + toks[si]\n            toks[ei] = toks[ei] + end_marker\n        else:\n            raise NotImplementedError\n    return toks"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclearing the state of the current object", "response": "def _clear_state(self, seed=None):\n        \"\"\"Clears the state, starts clock\"\"\"\n        self.start_time = time()\n        self.run_stats = []\n        self.best_index = -1\n        self.best_score = -1\n        self.best_config = None\n\n        # Note: These must be set at the start of self.search()\n        self.search_space = None\n\n        # Reset the seed\n        if seed is not None:\n            self.rng = random.Random(seed)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_stats_df(self):\n\n        run_stats_df = []\n        for x in self.run_stats:\n            search_results = {**x[\"search_params\"]}\n            search_results[\"score\"] = x[\"score\"]\n            run_stats_df.append(search_results)\n        return pd.DataFrame(run_stats_df)", "response": "Returns self. run_stats over search params as pandas dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef search(\n        self,\n        search_space,\n        valid_data,\n        init_args=[],\n        train_args=[],\n        init_kwargs={},\n        train_kwargs={},\n        module_args={},\n        module_kwargs={},\n        max_search=None,\n        shuffle=True,\n        verbose=True,\n        **score_kwargs,\n    ):\n        \"\"\"\n        Args:\n            search_space: see config_generator() documentation\n            valid_data: a tuple of Tensors (X,Y), a Dataset, or a DataLoader of\n                X (data) and Y (labels) for the dev split\n            init_args: (list) positional args for initializing the model\n            train_args: (list) positional args for training the model\n            init_kwargs: (dict) keyword args for initializing the model\n            train_kwargs: (dict) keyword args for training the model\n            module_args: (dict) Dictionary of lists of module args\n            module_kwargs: (dict) Dictionary of dictionaries of module kwargs\n            max_search: see config_generator() documentation\n            shuffle: see config_generator() documentation\n        Returns:\n            best_model: the highest performing trained model\n\n        Note: Initialization is performed by ModelTuner instead of passing a\n        pre-initialized model so that tuning may be performed over all model\n        parameters, including the network architecture (which is defined before\n        the train loop).\n        \"\"\"\n        raise NotImplementedError()", "response": "Search the modelTuner for the given set of modules in the given search space."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef config_generator(search_space, max_search, rng, shuffle=True):\n\n        def dict_product(d):\n            keys = d.keys()\n            for element in product(*d.values()):\n                yield dict(zip(keys, element))\n\n        def range_param_func(v):\n            scale = v.get(\"scale\", \"linear\")\n            mini = min(v[\"range\"])\n            maxi = max(v[\"range\"])\n            if scale == \"linear\":\n                func = lambda rand: mini + (maxi - mini) * rand\n            elif scale == \"log\":\n                mini = np.log(mini)\n                maxi = np.log(maxi)\n                func = lambda rand: np.exp(mini + (maxi - mini) * rand)\n            else:\n                raise ValueError(f\"Unrecognized scale '{scale}' for \" \"parameter {k}\")\n            return func\n\n        discretes = {}\n        ranges = {}\n        for k, v in search_space.items():\n            if isinstance(v, dict):\n                ranges[k] = range_param_func(v)\n            elif isinstance(v, list):\n                discretes[k] = v\n            else:\n                discretes[k] = [v]\n\n        discrete_configs = list(dict_product(discretes))\n\n        if shuffle:\n            rng.shuffle(discrete_configs)\n\n        # If there are range parameters and a non-None max_search, cycle\n        # through the discrete_configs (with new range values) until\n        # max_search is met\n        if ranges and max_search:\n            discrete_configs = cycle(discrete_configs)\n\n        for i, config in enumerate(discrete_configs):\n            # We may see the same config twice due to cycle\n            config = config.copy()\n            if max_search and i == max_search:\n                break\n            for k, v in ranges.items():\n                config[k] = float(v(rng.random()))\n            yield config", "response": "Generates a list of config dicts for the given search space and max_search parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compute_mu(L_aug, Y, k, p):\n    n, d = L_aug.shape\n    assert Y.shape[0] == n\n\n    # Compute mu\n    mu = np.zeros((d, k))\n    for y in range(1, k + 1):\n        L_y = L_aug[Y == y]\n        mu[:, y - 1] = L_y.sum(axis=0) / L_y.shape[0]\n    return mu", "response": "Given label matrix L_aug and labels Y compute the true mu params."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving label matrix L_aug and labels Y compute the covariance.", "response": "def compute_covariance(L_aug, Y, k, p):\n    \"\"\"Given label matrix L_aug and labels Y, compute the covariance.\n\n    Args:\n        L: (np.array {0,1}) [n, d] The augmented (indicator) label matrix\n        Y: (np.array int) [n] The true labels in {1,...,k}\n        k: (int) Cardinality\n        p: (np.array float) [k] The class balance\n    \"\"\"\n    n, d = L_aug.shape\n    assert Y.shape[0] == n\n    mu = compute_mu(L_aug, Y, k, p)\n    return (L_aug.T @ L_aug) / n - mu @ np.diag(p) @ mu.T"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compute_inv_covariance(L_aug, Y, k, p):\n    return np.linalg.inv(compute_covariance(L_aug, Y, k, p))", "response": "Given label matrix L and labels Y compute the covariance matrix."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _conflicted_data_points(L):\n    m = sparse.diags(np.ravel(L.max(axis=1).todense()))\n    return np.ravel(np.max(m @ (L != 0) != L, axis=1).astype(int).todense())", "response": "Returns an indicator vector where ith element = 1 if x_i is labeled by\n    at least two LFs that give it disagreeing labels."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lf_polarities(L):\n    polarities = [sorted(list(set(L[:, i].data))) for i in range(L.shape[1])]\n    return [p[0] if len(p) == 1 else p for p in polarities]", "response": "Return the polarities of each LF based on evidence in a label matrix."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lf_overlaps(L, normalize_by_coverage=False):\n    overlaps = (L != 0).T @ _overlapped_data_points(L) / L.shape[0]\n    if normalize_by_coverage:\n        overlaps /= lf_coverages(L)\n    return np.nan_to_num(overlaps)", "response": "Returns the fraction of items each LF label is also labeled by at\n     least one other LF."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef lf_conflicts(L, normalize_by_overlaps=False):\n    conflicts = (L != 0).T @ _conflicted_data_points(L) / L.shape[0]\n    if normalize_by_overlaps:\n        conflicts /= lf_overlaps(L)\n    return np.nan_to_num(conflicts)", "response": "Returns the fraction of items each LF label is also given by at least one other LF."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the empirical accuracy against a set of labels Y", "response": "def lf_empirical_accuracies(L, Y):\n    \"\"\"Return the **empirical accuracy** against a set of labels Y (e.g. dev\n    set) for each LF.\n    Args:\n        L: an n x m scipy.sparse matrix where L_{i,j} is the label given by the\n            jth LF to the ith candidate\n        Y: an [n] or [n, 1] np.ndarray of gold labels\n    \"\"\"\n    # Assume labeled set is small, work with dense matrices\n    Y = arraylike_to_numpy(Y)\n    L = L.toarray()\n    X = np.where(L == 0, 0, np.where(L == np.vstack([Y] * L.shape[1]).T, 1, -1))\n    return 0.5 * (X.sum(axis=0) / (L != 0).sum(axis=0) + 1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef lf_summary(L, Y=None, lf_names=None, est_accs=None):\n    n, m = L.shape\n    if lf_names is not None:\n        col_names = [\"j\"]\n        d = {\"j\": list(range(m))}\n    else:\n        lf_names = list(range(m))\n        col_names = []\n        d = {}\n\n    # Default LF stats\n    col_names.extend([\"Polarity\", \"Coverage\", \"Overlaps\", \"Conflicts\"])\n    d[\"Polarity\"] = Series(data=lf_polarities(L), index=lf_names)\n    d[\"Coverage\"] = Series(data=lf_coverages(L), index=lf_names)\n    d[\"Overlaps\"] = Series(data=lf_overlaps(L), index=lf_names)\n    d[\"Conflicts\"] = Series(data=lf_conflicts(L), index=lf_names)\n\n    if Y is not None:\n        col_names.extend([\"Correct\", \"Incorrect\", \"Emp. Acc.\"])\n        confusions = [\n            confusion_matrix(Y, L[:, i], pretty_print=False) for i in range(m)\n        ]\n        corrects = [np.diagonal(conf).sum() for conf in confusions]\n        incorrects = [\n            conf.sum() - correct for conf, correct in zip(confusions, corrects)\n        ]\n        accs = lf_empirical_accuracies(L, Y)\n        d[\"Correct\"] = Series(data=corrects, index=lf_names)\n        d[\"Incorrect\"] = Series(data=incorrects, index=lf_names)\n        d[\"Emp. Acc.\"] = Series(data=accs, index=lf_names)\n\n    if est_accs is not None:\n        col_names.append(\"Learned Acc.\")\n        d[\"Learned Acc.\"] = Series(est_accs, index=lf_names)\n\n    return DataFrame(data=d, index=lf_names)[col_names]", "response": "Returns a pandas DataFrame with the various per - LF statistics."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates coverage overlap conflicts and accuracy for a single LF", "response": "def single_lf_summary(Y_p, Y=None):\n    \"\"\"Calculates coverage, overlap, conflicts, and accuracy for a single LF\n\n    Args:\n        Y_p: a np.array or torch.Tensor of predicted labels\n        Y: a np.array or torch.Tensor of true labels (if known)\n    \"\"\"\n    L = sparse.csr_matrix(arraylike_to_numpy(Y_p).reshape(-1, 1))\n    return lf_summary(L, Y)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef error_buckets(gold, pred, X=None):\n    buckets = defaultdict(list)\n    gold = arraylike_to_numpy(gold)\n    pred = arraylike_to_numpy(pred)\n    for i, (y, l) in enumerate(zip(pred, gold)):\n        buckets[y, l].append(X[i] if X is not None else i)\n    return buckets", "response": "Group items by error buckets."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef confusion_matrix(\n    gold, pred, null_pred=False, null_gold=False, normalize=False, pretty_print=True\n):\n    \"\"\"A shortcut method for building a confusion matrix all at once.\n\n    Args:\n        gold: an array-like of gold labels (ints)\n        pred: an array-like of predictions (ints)\n        null_pred: If True, include the row corresponding to null predictions\n        null_gold: If True, include the col corresponding to null gold labels\n        normalize: if True, divide counts by the total number of items\n        pretty_print: if True, pretty-print the matrix before returning\n    \"\"\"\n    conf = ConfusionMatrix(null_pred=null_pred, null_gold=null_gold)\n    gold = arraylike_to_numpy(gold)\n    pred = arraylike_to_numpy(pred)\n    conf.add(gold, pred)\n    mat = conf.compile()\n\n    if normalize:\n        mat = mat / len(gold)\n\n    if pretty_print:\n        conf.display(normalize=normalize)\n\n    return mat", "response": "A shortcut method for building a confusion matrix all at once."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding gold and pred to the set of known classes.", "response": "def add(self, gold, pred):\n        \"\"\"\n        Args:\n            gold: a np.ndarray of gold labels (ints)\n            pred: a np.ndarray of predictions (ints)\n        \"\"\"\n        self.counter.update(zip(gold, pred))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsearching the model for the given set of modules.", "response": "def search(\n        self,\n        search_space,\n        valid_data,\n        init_args=[],\n        train_args=[],\n        init_kwargs={},\n        train_kwargs={},\n        module_args={},\n        module_kwargs={},\n        max_search=None,\n        shuffle=True,\n        verbose=True,\n        clean_up=True,\n        seed=None,\n        **score_kwargs,\n    ):\n        \"\"\"\n        Args:\n            search_space: see config_generator() documentation\n            valid_data: a tuple of Tensors (X,Y), a Dataset, or a DataLoader of\n                X (data) and Y (labels) for the dev split\n            init_args: (list) positional args for initializing the model\n            train_args: (list) positional args for training the model\n            init_kwargs: (dict) keyword args for initializing the model\n            train_kwargs: (dict) keyword args for training the model\n            module_args: (dict) Dictionary of lists of module args\n            module_kwargs: (dict) Dictionary of dictionaries of module kwargs\n            max_search: see config_generator() documentation\n            shuffle: see config_generator() documentation\n\n        Returns:\n            best_model: the highest performing trained model\n\n        Note: Initialization is performed by ModelTuner instead of passing a\n        pre-initialized model so that tuning may be performed over all model\n        parameters, including the network architecture (which is defined before\n        the train loop).\n        \"\"\"\n        self._clear_state(seed)\n        self.search_space = search_space\n\n        # Generate configs\n        configs = self.config_generator(search_space, max_search, self.rng, shuffle)\n\n        # Commence search\n        for i, config in enumerate(configs):\n            score, model = self._test_model_config(\n                i,\n                config,\n                valid_data,\n                init_args=init_args,\n                train_args=train_args,\n                init_kwargs=init_kwargs,\n                train_kwargs=train_kwargs,\n                module_args=module_args,\n                module_kwargs=module_kwargs,\n                verbose=verbose,\n                **score_kwargs,\n            )\n\n        if verbose:\n            print(\"=\" * 60)\n            print(f\"[SUMMARY]\")\n            print(f\"Best model: [{self.best_index}]\")\n            print(f\"Best config: {self.best_config}\")\n            print(f\"Best score: {self.best_score}\")\n            print(\"=\" * 60)\n\n        self._save_report()\n\n        # Return best model\n        return self._load_best_model(clean_up=clean_up)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconfiguring osmnx by setting the default global vars to desired values.", "response": "def config(data_folder=settings.data_folder,\n           logs_folder=settings.logs_folder,\n           imgs_folder=settings.imgs_folder,\n           cache_folder=settings.cache_folder,\n           use_cache=settings.use_cache,\n           log_file=settings.log_file,\n           log_console=settings.log_console,\n           log_level=settings.log_level,\n           log_name=settings.log_name,\n           log_filename=settings.log_filename,\n           useful_tags_node=settings.useful_tags_node,\n           useful_tags_path=settings.useful_tags_path,\n           osm_xml_node_attrs=settings.osm_xml_node_attrs,\n           osm_xml_node_tags=settings.osm_xml_node_tags,\n           osm_xml_way_attrs=settings.osm_xml_way_attrs,\n           osm_xml_way_tags=settings.osm_xml_way_tags,\n           default_access=settings.default_access,\n           default_crs=settings.default_crs,\n           default_user_agent=settings.default_user_agent,\n           default_referer=settings.default_referer,\n           default_accept_language=settings.default_accept_language):\n    \"\"\"\n    Configure osmnx by setting the default global vars to desired values.\n\n    Parameters\n    ---------\n    data_folder : string\n        where to save and load data files\n    logs_folder : string\n        where to write the log files\n    imgs_folder : string\n        where to save figures\n    cache_folder : string\n        where to save the http response cache\n    use_cache : bool\n        if True, use a local cache to save/retrieve http responses instead of\n        calling API repetitively for the same request URL\n    log_file : bool\n        if true, save log output to a log file in logs_folder\n    log_console : bool\n        if true, print log output to the console\n    log_level : int\n        one of the logger.level constants\n    log_name : string\n        name of the logger\n    useful_tags_node : list\n        a list of useful OSM tags to attempt to save from node elements\n    useful_tags_path : list\n        a list of useful OSM tags to attempt to save from path elements\n    default_access : string\n        default filter for OSM \"access\" key\n    default_crs : string\n        default CRS to set when creating graphs\n    default_user_agent : string\n        HTTP header user-agent\n    default_referer : string\n        HTTP header referer\n    default_accept_language : string\n        HTTP header accept-language\n\n    Returns\n    -------\n    None\n    \"\"\"\n\n    # set each global variable to the passed-in parameter value\n    settings.use_cache = use_cache\n    settings.cache_folder = cache_folder\n    settings.data_folder = data_folder\n    settings.imgs_folder = imgs_folder\n    settings.logs_folder = logs_folder\n    settings.log_console = log_console\n    settings.log_file = log_file\n    settings.log_level = log_level\n    settings.log_name = log_name\n    settings.log_filename = log_filename\n    settings.useful_tags_node = useful_tags_node\n    settings.useful_tags_path = useful_tags_path\n    settings.useful_tags_node = list(set(\n        useful_tags_node + osm_xml_node_attrs + osm_xml_node_tags))\n    settings.useful_tags_path = list(set(\n        useful_tags_path + osm_xml_way_attrs + osm_xml_way_tags))\n    settings.osm_xml_node_attrs = osm_xml_node_attrs\n    settings.osm_xml_node_tags = osm_xml_node_tags\n    settings.osm_xml_way_attrs = osm_xml_way_attrs\n    settings.osm_xml_way_tags = osm_xml_way_tags\n    settings.default_access = default_access\n    settings.default_crs = default_crs\n    settings.default_user_agent = default_user_agent\n    settings.default_referer = default_referer\n    settings.default_accept_language = default_accept_language\n\n    # if logging is turned on, log that we are configured\n    if settings.log_file or settings.log_console:\n        log('Configured osmnx')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite a message to the log file and or to the console.", "response": "def log(message, level=None, name=None, filename=None):\n    \"\"\"\n    Write a message to the log file and/or print to the the console.\n\n    Parameters\n    ----------\n    message : string\n        the content of the message to log\n    level : int\n        one of the logger.level constants\n    name : string\n        name of the logger\n    filename : string\n        name of the log file\n\n    Returns\n    -------\n    None\n    \"\"\"\n\n    if level is None:\n        level = settings.log_level\n    if name is None:\n        name = settings.log_name\n    if filename is None:\n        filename = settings.log_filename\n\n    # if logging to file is turned on\n    if settings.log_file:\n        # get the current logger (or create a new one, if none), then log\n        # message at requested level\n        logger = get_logger(level=level, name=name, filename=filename)\n        if level == lg.DEBUG:\n            logger.debug(message)\n        elif level == lg.INFO:\n            logger.info(message)\n        elif level == lg.WARNING:\n            logger.warning(message)\n        elif level == lg.ERROR:\n            logger.error(message)\n\n    # if logging to console is turned on, convert message to ascii and print to\n    # the console\n    if settings.log_console:\n        # capture current stdout, then switch it to the console, print the\n        # message, then switch back to what had been the stdout. this prevents\n        # logging to notebook - instead, it goes to console\n        standard_out = sys.stdout\n        sys.stdout = sys.__stdout__\n\n        # convert message to ascii for console display so it doesn't break\n        # windows terminals\n        message = unicodedata.normalize('NFKD', make_str(message)).encode('ascii', errors='replace').decode()\n        print(message)\n        sys.stdout = standard_out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a logger or return the current one if already instantiated.", "response": "def get_logger(level=None, name=None, filename=None):\n    \"\"\"\n    Create a logger or return the current one if already instantiated.\n\n    Parameters\n    ----------\n    level : int\n        one of the logger.level constants\n    name : string\n        name of the logger\n    filename : string\n        name of the log file\n\n    Returns\n    -------\n    logger.logger\n    \"\"\"\n\n    if level is None:\n        level = settings.log_level\n    if name is None:\n        name = settings.log_name\n    if filename is None:\n        filename = settings.log_filename\n\n    logger = lg.getLogger(name)\n\n    # if a logger with this name is not already set up\n    if not getattr(logger, 'handler_set', None):\n\n        # get today's date and construct a log filename\n        todays_date = dt.datetime.today().strftime('%Y_%m_%d')\n        log_filename = os.path.join(settings.logs_folder, '{}_{}.log'.format(filename, todays_date))\n\n        # if the logs folder does not already exist, create it\n        if not os.path.exists(settings.logs_folder):\n            os.makedirs(settings.logs_folder)\n\n        # create file handler and log formatter and set them up\n        handler = lg.FileHandler(log_filename, encoding='utf-8')\n        formatter = lg.Formatter('%(asctime)s %(levelname)s %(name)s %(message)s')\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n        logger.setLevel(level)\n        logger.handler_set = True\n\n    return logger"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef induce_subgraph(G, node_subset):\n\n    node_subset = set(node_subset)\n\n    # copy nodes into new graph\n    G2 = G.__class__()\n    G2.add_nodes_from((n, G.nodes[n]) for n in node_subset)\n    \n    # copy edges to new graph, including parallel edges\n    if G2.is_multigraph:\n        G2.add_edges_from((n, nbr, key, d)\n            for n, nbrs in G.adj.items() if n in node_subset\n            for nbr, keydict in nbrs.items() if nbr in node_subset\n            for key, d in keydict.items())\n    else:\n        G2.add_edges_from((n, nbr, d)\n            for n, nbrs in G.adj.items() if n in node_subset\n            for nbr, d in nbrs.items() if nbr in node_subset)\n    \n    # update graph attribute dict, and return graph\n    G2.graph.update(G.graph)\n    return G2", "response": "Induce a subgraph of G."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a subgraph of the largest weakly or strongly connected component from a directed graph.", "response": "def get_largest_component(G, strongly=False):\n    \"\"\"\n    Return a subgraph of the largest weakly or strongly connected component\n    from a directed graph.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n    strongly : bool\n        if True, return the largest strongly instead of weakly connected\n        component\n\n    Returns\n    -------\n    G : networkx multidigraph\n        the largest connected component subgraph from the original graph\n    \"\"\"\n\n    start_time = time.time()\n    original_len = len(list(G.nodes()))\n\n    if strongly:\n        # if the graph is not connected retain only the largest strongly connected component\n        if not nx.is_strongly_connected(G):\n            \n            # get all the strongly connected components in graph then identify the largest\n            sccs = nx.strongly_connected_components(G)\n            largest_scc = max(sccs, key=len)\n            G = induce_subgraph(G, largest_scc)\n            \n            msg = ('Graph was not connected, retained only the largest strongly '\n                   'connected component ({:,} of {:,} total nodes) in {:.2f} seconds')\n            log(msg.format(len(list(G.nodes())), original_len, time.time()-start_time))\n    else:\n        # if the graph is not connected retain only the largest weakly connected component\n        if not nx.is_weakly_connected(G):\n            \n            # get all the weakly connected components in graph then identify the largest\n            wccs = nx.weakly_connected_components(G)\n            largest_wcc = max(wccs, key=len)\n            G = induce_subgraph(G, largest_wcc)\n            \n            msg = ('Graph was not connected, retained only the largest weakly '\n                   'connected component ({:,} of {:,} total nodes) in {:.2f} seconds')\n            log(msg.format(len(list(G.nodes())), original_len, time.time()-start_time))\n\n    return G"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef great_circle_vec(lat1, lng1, lat2, lng2, earth_radius=6371009):\n\n    phi1 = np.deg2rad(lat1)\n    phi2 = np.deg2rad(lat2)\n    d_phi = phi2 - phi1\n\n    theta1 = np.deg2rad(lng1)\n    theta2 = np.deg2rad(lng2)\n    d_theta = theta2 - theta1\n\n    h = np.sin(d_phi / 2) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(d_theta / 2) ** 2 \n    h = np.minimum(1.0, h) # protect against floating point errors\n\n    arc = 2 * np.arcsin(np.sqrt(h))\n\n    # return distance in units of earth_radius\n    distance = arc * earth_radius\n    return distance", "response": "Vectorized function to calculate the great circle distance between two points or between vectors of points using haversine."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef euclidean_dist_vec(y1, x1, y2, x2):\n\n    # euclid's formula\n    distance = ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5\n    return distance", "response": "Vectorized function to calculate the euclidean distance between two points y1 and x2."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_nearest_node(G, point, method='haversine', return_dist=False):\n    start_time = time.time()\n    \n    if not G or (G.number_of_nodes() == 0):\n        raise ValueError('G argument must be not be empty or should contain at least one node')\n\n    # dump graph node coordinates into a pandas dataframe indexed by node id\n    # with x and y columns\n    coords = [[node, data['x'], data['y']] for node, data in G.nodes(data=True)]\n    df = pd.DataFrame(coords, columns=['node', 'x', 'y']).set_index('node')\n\n    # add columns to the dataframe representing the (constant) coordinates of\n    # the reference point\n    df['reference_y'] = point[0]\n    df['reference_x'] = point[1]\n\n    # calculate the distance between each node and the reference point\n    if method == 'haversine':\n        # calculate distance vector using haversine (ie, for\n        # spherical lat-long geometries)\n        distances = great_circle_vec(lat1=df['reference_y'],\n                                     lng1=df['reference_x'],\n                                     lat2=df['y'],\n                                     lng2=df['x'])\n\n    elif method == 'euclidean':\n        # calculate distance vector using euclidean distances (ie, for projected\n        # planar geometries)\n        distances = euclidean_dist_vec(y1=df['reference_y'],\n                                       x1=df['reference_x'],\n                                       y2=df['y'],\n                                       x2=df['x'])\n\n    else:\n        raise ValueError('method argument must be either \"haversine\" or \"euclidean\"')\n\n    # nearest node's ID is the index label of the minimum distance\n    nearest_node = distances.idxmin()\n    log('Found nearest node ({}) to point {} in {:,.2f} seconds'.format(nearest_node, point, time.time()-start_time))\n\n    # if caller requested return_dist, return distance between the point and the\n    # nearest node as well\n    if return_dist:\n        return nearest_node, distances.loc[nearest_node]\n    else:\n        return nearest_node", "response": "Returns the nearest node in a networkx multidigraph."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_nearest_edge(G, point):\n    start_time = time.time()\n\n    gdf = graph_to_gdfs(G, nodes=False, fill_edge_geometry=True)\n    graph_edges = gdf[[\"geometry\", \"u\", \"v\"]].values.tolist()\n\n    edges_with_distances = [\n        (\n            graph_edge,\n            Point(tuple(reversed(point))).distance(graph_edge[0])\n        )\n        for graph_edge in graph_edges\n    ]\n\n    edges_with_distances = sorted(edges_with_distances, key=lambda x: x[1])\n    closest_edge_to_point = edges_with_distances[0][0]\n\n    geometry, u, v = closest_edge_to_point\n\n    log('Found nearest edge ({}) to point {} in {:,.2f} seconds'.format((u, v), point, time.time() - start_time))\n\n    return geometry, u, v", "response": "Returns the nearest edge to a pair of coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the nearest nodes to a list of points.", "response": "def get_nearest_nodes(G, X, Y, method=None):\n    \"\"\"\n    Return the graph nodes nearest to a list of points. Pass in points\n    as separate vectors of X and Y coordinates. The 'kdtree' method\n    is by far the fastest with large data sets, but only finds approximate\n    nearest nodes if working in unprojected coordinates like lat-lng (it\n    precisely finds the nearest node if working in projected coordinates).\n    The 'balltree' method is second fastest with large data sets, but it \n    is precise if working in unprojected coordinates like lat-lng.\n    \n    Parameters\n    ----------\n    G : networkx multidigraph\n    X : list-like\n        The vector of longitudes or x's for which we will find the nearest\n        node in the graph\n    Y : list-like\n        The vector of latitudes or y's for which we will find the nearest\n        node in the graph\n    method : str {None, 'kdtree', 'balltree'}\n        Which method to use for finding nearest node to each point.\n        If None, we manually find each node one at a time using \n        osmnx.utils.get_nearest_node and haversine. If 'kdtree' we use \n        scipy.spatial.cKDTree for very fast euclidean search. If\n        'balltree', we use sklearn.neighbors.BallTree for fast \n        haversine search.\n        \n    Returns\n    -------\n    nn : array\n        list of nearest node IDs\n    \"\"\"\n    \n    start_time = time.time()\n\n    if method is None:\n        \n        # calculate nearest node one at a time for each point\n        nn = [get_nearest_node(G, (y, x), method='haversine') for x, y in zip(X, Y)]\n    \n    elif method == 'kdtree':\n        \n        # check if we were able to import scipy.spatial.cKDTree successfully\n        if not cKDTree:\n            raise ImportError('The scipy package must be installed to use this optional feature.')\n        \n        # build a k-d tree for euclidean nearest node search\n        nodes = pd.DataFrame({'x':nx.get_node_attributes(G, 'x'),\n                              'y':nx.get_node_attributes(G, 'y')})\n        tree = cKDTree(data=nodes[['x', 'y']], compact_nodes=True, balanced_tree=True)\n        \n        # query the tree for nearest node to each point\n        points = np.array([X, Y]).T\n        dist, idx = tree.query(points, k=1)\n        nn = nodes.iloc[idx].index\n        \n    elif method == 'balltree':\n        \n        # check if we were able to import sklearn.neighbors.BallTree successfully\n        if not BallTree:\n            raise ImportError('The scikit-learn package must be installed to use this optional feature.')\n        \n        # haversine requires data in form of [lat, lng] and inputs/outputs in units of radians\n        nodes = pd.DataFrame({'x':nx.get_node_attributes(G, 'x'),\n                              'y':nx.get_node_attributes(G, 'y')})\n        nodes_rad = np.deg2rad(nodes[['y', 'x']].astype(np.float))\n        points = np.array([Y.astype(np.float), X.astype(np.float)]).T\n        points_rad = np.deg2rad(points)\n\n        # build a ball tree for haversine nearest node search\n        tree = BallTree(nodes_rad, metric='haversine')\n\n        # query the tree for nearest node to each point\n        idx = tree.query(points_rad, k=1, return_distance=False)\n        nn = nodes.iloc[idx[:,0]].index\n    \n    else:\n        raise ValueError('You must pass a valid method name, or None.')\n\n    log('Found nearest nodes to {:,} points in {:,.2f} seconds'.format(len(X), time.time()-start_time))\n\n    return np.array(nn)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the nearest edges to a list of points.", "response": "def get_nearest_edges(G, X, Y, method=None, dist=0.0001):\n    \"\"\"\n    Return the graph edges nearest to a list of points. Pass in points\n    as separate vectors of X and Y coordinates. The 'kdtree' method\n    is by far the fastest with large data sets, but only finds approximate\n    nearest edges if working in unprojected coordinates like lat-lng (it\n    precisely finds the nearest edge if working in projected coordinates).\n    The 'balltree' method is second fastest with large data sets, but it\n    is precise if working in unprojected coordinates like lat-lng.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n    X : list-like\n        The vector of longitudes or x's for which we will find the nearest\n        edge in the graph. For projected graphs, use the projected coordinates,\n        usually in meters.\n    Y : list-like\n        The vector of latitudes or y's for which we will find the nearest\n        edge in the graph. For projected graphs, use the projected coordinates,\n        usually in meters.\n    method : str {None, 'kdtree', 'balltree'}\n        Which method to use for finding nearest edge to each point.\n        If None, we manually find each edge one at a time using\n        osmnx.utils.get_nearest_edge. If 'kdtree' we use\n        scipy.spatial.cKDTree for very fast euclidean search. Recommended for\n        projected graphs. If 'balltree', we use sklearn.neighbors.BallTree for\n        fast haversine search. Recommended for unprojected graphs.\n\n    dist : float\n        spacing length along edges. Units are the same as the geom; Degrees for\n        unprojected geometries and meters for projected geometries. The smaller\n        the value, the more points are created.\n\n    Returns\n    -------\n    ne : ndarray\n        array of nearest edges represented by their startpoint and endpoint ids,\n        u and v, the OSM ids of the nodes.\n\n    Info\n    ----\n    The method creates equally distanced points along the edges of the network.\n    Then, these points are used in a kdTree or BallTree search to identify which\n    is nearest.Note that this method will not give the exact perpendicular point\n    along the edge, but the smaller the *dist* parameter, the closer the solution\n    will be.\n\n    Code is adapted from an answer by JHuw from this original question:\n    https://gis.stackexchange.com/questions/222315/geopandas-find-nearest-point\n    -in-other-dataframe\n    \"\"\"\n    start_time = time.time()\n\n    if method is None:\n        # calculate nearest edge one at a time for each point\n        ne = [get_nearest_edge(G, (x, y)) for x, y in zip(X, Y)]\n        ne = [(u, v) for _, u, v in ne]\n\n    elif method == 'kdtree':\n\n        # check if we were able to import scipy.spatial.cKDTree successfully\n        if not cKDTree:\n            raise ImportError('The scipy package must be installed to use this optional feature.')\n\n        # transform graph into DataFrame\n        edges = graph_to_gdfs(G, nodes=False, fill_edge_geometry=True)\n\n        # transform edges into evenly spaced points\n        edges['points'] = edges.apply(lambda x: redistribute_vertices(x.geometry, dist), axis=1)\n\n        # develop edges data for each created points\n        extended = edges['points'].apply([pd.Series]).stack().reset_index(level=1, drop=True).join(edges).reset_index()\n\n        # Prepare btree arrays\n        nbdata = np.array(list(zip(extended['Series'].apply(lambda x: x.x),\n                                   extended['Series'].apply(lambda x: x.y))))\n\n        # build a k-d tree for euclidean nearest node search\n        btree = cKDTree(data=nbdata, compact_nodes=True, balanced_tree=True)\n\n        # query the tree for nearest node to each point\n        points = np.array([X, Y]).T\n        dist, idx = btree.query(points, k=1)  # Returns ids of closest point\n        eidx = extended.loc[idx, 'index']\n        ne = edges.loc[eidx, ['u', 'v']]\n\n    elif method == 'balltree':\n\n        # check if we were able to import sklearn.neighbors.BallTree successfully\n        if not BallTree:\n            raise ImportError('The scikit-learn package must be installed to use this optional feature.')\n\n        # transform graph into DataFrame\n        edges = graph_to_gdfs(G, nodes=False, fill_edge_geometry=True)\n\n        # transform edges into evenly spaced points\n        edges['points'] = edges.apply(lambda x: redistribute_vertices(x.geometry, dist), axis=1)\n\n        # develop edges data for each created points\n        extended = edges['points'].apply([pd.Series]).stack().reset_index(level=1, drop=True).join(edges).reset_index()\n\n        # haversine requires data in form of [lat, lng] and inputs/outputs in units of radians\n        nodes = pd.DataFrame({'x': extended['Series'].apply(lambda x: x.x),\n                              'y': extended['Series'].apply(lambda x: x.y)})\n        nodes_rad = np.deg2rad(nodes[['y', 'x']].values.astype(np.float))\n        points = np.array([Y, X]).T\n        points_rad = np.deg2rad(points)\n\n        # build a ball tree for haversine nearest node search\n        tree = BallTree(nodes_rad, metric='haversine')\n\n        # query the tree for nearest node to each point\n        idx = tree.query(points_rad, k=1, return_distance=False)\n        eidx = extended.loc[idx[:, 0], 'index']\n        ne = edges.loc[eidx, ['u', 'v']]\n\n    else:\n        raise ValueError('You must pass a valid method name, or None.')\n\n    log('Found nearest edges to {:,} points in {:,.2f} seconds'.format(len(X), time.time() - start_time))\n\n    return np.array(ne)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef redistribute_vertices(geom, dist):\n    if geom.geom_type == 'LineString':\n        num_vert = int(round(geom.length / dist))\n        if num_vert == 0:\n            num_vert = 1\n        return [geom.interpolate(float(n) / num_vert, normalized=True)\n                for n in range(num_vert + 1)]\n    elif geom.geom_type == 'MultiLineString':\n        parts = [redistribute_vertices(part, dist)\n                 for part in geom]\n        return type(geom)([p for p in parts if not p.is_empty])\n    else:\n        raise ValueError('unhandled geometry {}'.format(geom.geom_type))", "response": "Redistribute vertices on a projected LineString or MultiLineString."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_bearing(origin_point, destination_point):\n\n    if not (isinstance(origin_point, tuple) and isinstance(destination_point, tuple)):\n        raise TypeError('origin_point and destination_point must be (lat, lng) tuples')\n\n    # get latitudes and the difference in longitude, as radians\n    lat1 = math.radians(origin_point[0])\n    lat2 = math.radians(destination_point[0])\n    diff_lng = math.radians(destination_point[1] - origin_point[1])\n\n    # calculate initial bearing from -180 degrees to +180 degrees\n    x = math.sin(diff_lng) * math.cos(lat2)\n    y = math.cos(lat1) * math.sin(lat2) - (math.sin(lat1) * math.cos(lat2) * math.cos(diff_lng))\n    initial_bearing = math.atan2(x, y)\n\n    # normalize initial bearing to 0 degrees to 360 degrees to get compass bearing\n    initial_bearing = math.degrees(initial_bearing)\n    bearing = (initial_bearing + 360) % 360\n\n    return bearing", "response": "Calculate the bearing between two lat - long points. Each tuple should\n    represent ( lat lng ) as decimal degrees. Each tuple should\n    represent ( lat lng ) as decimal degrees."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_edge_bearings(G):\n\n    for u, v, data in G.edges(keys=False, data=True):\n\n        if u == v:\n            # a self-loop has an undefined compass bearing\n            data['bearing'] = np.nan\n        \n        else:\n            # calculate bearing from edge's origin to its destination\n            origin_point = (G.nodes[u]['y'], G.nodes[u]['x'])\n            destination_point = (G.nodes[v]['y'], G.nodes[v]['x'])\n            bearing = get_bearing(origin_point, destination_point)\n            \n            # round to thousandth of a degree\n            data['bearing'] = round(bearing, 3)\n\n    return G", "response": "Adds the compass bearing from origin node to destination node for each edge in the directed graph G."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef geocode(query):\n\n    # send the query to the nominatim geocoder and parse the json response\n    url_template = 'https://nominatim.openstreetmap.org/search?format=json&limit=1&q={}'\n    url = url_template.format(query)\n    response = requests.get(url, timeout=60)\n    results = response.json()\n\n    # if results were returned, parse lat and long out of the result\n    if len(results) > 0 and 'lat' in results[0] and 'lon' in results[0]:\n        lat = float(results[0]['lat'])\n        lon = float(results[0]['lon'])\n        point = (lat, lon)\n        log('Geocoded \"{}\" to {}'.format(query, point))\n        return point\n    else:\n        raise Exception('Nominatim geocoder returned no results for query \"{}\"'.format(query))", "response": "Geocode a query string to a tuple of lat lon"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_route_edge_attributes(G, route, attribute=None, minimize_key='length', retrieve_default=None):\n\n    attribute_values = []\n    for u, v in zip(route[:-1], route[1:]):\n        # if there are parallel edges between two nodes, select the one with the\n        # lowest value of minimize_key\n        data = min(G.get_edge_data(u, v).values(), key=lambda x: x[minimize_key])\n        if attribute is None:\n            attribute_value = data\n        elif retrieve_default is not None:\n            attribute_value = data.get(attribute, retrieve_default(u, v))\n        else:\n            attribute_value = data[attribute]\n        attribute_values.append(attribute_value)\n    return attribute_values", "response": "Get a list of attribute values for each edge in a multidigraph."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef count_streets_per_node(G, nodes=None):\n\n    start_time = time.time()\n\n    # to calculate the counts, get undirected representation of the graph. for\n    # each node, get the list of the set of unique u,v,key edges, including\n    # parallel edges but excluding self-loop parallel edges (this is necessary\n    # because bi-directional self-loops will appear twice in the undirected\n    # graph as you have u,v,key0 and u,v,key1 where u==v when you convert from\n    # MultiDiGraph to MultiGraph - BUT, one-way self-loops will appear only\n    # once. to get consistent accurate counts of physical streets, ignoring\n    # directionality, we need the list of the set of unique edges...). then,\n    # count how many times the node appears in the u,v tuples in the list. this\n    # is the count of how many street segments emanate from this node. finally,\n    # create a dict of node id:count\n    G_undir = G.to_undirected(reciprocal=False)\n    all_edges = G_undir.edges(keys=False)\n    if nodes is None:\n        nodes = G_undir.nodes()\n\n    # get all unique edges - this throws away any parallel edges (including\n    # those in self-loops)\n    all_unique_edges = set(all_edges)\n\n    # get all edges (including parallel edges) that are not self-loops\n    non_self_loop_edges = [e for e in all_edges if not e[0]==e[1]]\n\n    # get a single copy of each self-loop edge (ie, if it's bi-directional, we\n    # ignore the parallel edge going the reverse direction and keep only one\n    # copy)\n    set_non_self_loop_edges = set(non_self_loop_edges)\n    self_loop_edges = [e for e in all_unique_edges if e not in set_non_self_loop_edges]\n\n    # final list contains all unique edges, including each parallel edge, unless\n    # the parallel edge is a self-loop, in which case it doesn't double-count\n    # the self-loop\n    edges = non_self_loop_edges + self_loop_edges\n\n    # flatten the list of (u,v) tuples\n    edges_flat = list(chain.from_iterable(edges))\n\n    # count how often each node appears in the list of flattened edge endpoints\n    counts = Counter(edges_flat)\n    streets_per_node = {node:counts[node] for node in nodes}\n    msg = ('Got the counts of undirected street segments incident to each node '\n           '(before removing peripheral edges) in {:,.2f} seconds')\n    log(msg.format(time.time()-start_time))\n    return streets_per_node", "response": "Counts how many streets emanate from each node in a multi - graph."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef round_polygon_coords(p, precision):\n    \n    # round the coordinates of the Polygon exterior\n    new_exterior = [[round(x, precision) for x in c] for c in p.exterior.coords]\n\n    # round the coordinates of the (possibly multiple, possibly none) Polygon interior(s)\n    new_interiors = []\n    for interior in p.interiors:\n        new_interiors.append([[round(x, precision) for x in c] for c in interior.coords])\n    \n    # construct a new Polygon with the rounded coordinates\n    # buffer by zero to clean self-touching or self-crossing polygons\n    new_poly = Polygon(shell=new_exterior, holes=new_interiors).buffer(0)\n    return new_poly", "response": "Round the coordinates of a shapely Polygon to some decimal precision."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrounding the coordinates of a shapely Point to some decimal precision.", "response": "def round_point_coords(pt, precision):\n    \"\"\"\n    Round the coordinates of a shapely Point to some decimal precision.\n\n    Parameters\n    ----------\n    pt : shapely Point\n        the Point to round the coordinates of\n    precision : int\n        decimal precision to round coordinates to\n\n    Returns\n    -------\n    Point\n    \"\"\"\n    \n    return Point([round(x, precision) for x in pt.coords[0]])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrounds the coordinates of a shapely LineString to some decimal precision.", "response": "def round_linestring_coords(ls, precision):\n    \"\"\"\n    Round the coordinates of a shapely LineString to some decimal precision.\n\n    Parameters\n    ----------\n    ls : shapely LineString\n        the LineString to round the coordinates of\n    precision : int\n        decimal precision to round coordinates to\n\n    Returns\n    -------\n    LineString\n    \"\"\"\n    \n    return LineString([[round(x, precision) for x in c] for c in ls.coords])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrounding the coordinates of a shapely geometry to some decimal precision.", "response": "def round_shape_coords(shape, precision):\n    \"\"\"\n    Round the coordinates of a shapely geometry to some decimal precision.\n\n    Parameters\n    ----------\n    shape : shapely geometry, one of Point, MultiPoint, LineString,\n            MultiLineString, Polygon, or MultiPolygon\n        the geometry to round the coordinates of\n    precision : int\n        decimal precision to round coordinates to\n\n    Returns\n    -------\n    shapely geometry\n    \"\"\"\n    \n    if isinstance(shape, Point):\n        return round_point_coords(shape, precision)\n    \n    elif isinstance(shape, MultiPoint):\n        return round_multipoint_coords(shape, precision)\n    \n    elif isinstance(shape, LineString):\n        return round_linestring_coords(shape, precision)\n    \n    elif isinstance(shape, MultiLineString):\n        return round_multilinestring_coords(shape, precision)\n    \n    elif isinstance(shape, Polygon):\n        return round_polygon_coords(shape, precision)\n    \n    elif isinstance(shape, MultiPolygon):\n        return round_multipolygon_coords(shape, precision)\n    \n    else:\n        raise TypeError('cannot round coordinates of unhandled geometry type: {}'.format(type(shape)))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef overpass_json_from_file(filename):\n\n    _, ext = os.path.splitext(filename)\n    \n    if ext == '.bz2':\n        # Use Python 2/3 compatible BZ2File()\n        opener = lambda fn: bz2.BZ2File(fn)\n    else:\n        # Assume an unrecognized file extension is just XML\n        opener = lambda fn: open(fn, mode='rb')\n    \n    with opener(filename) as file:\n        handler = OSMContentHandler()\n        xml.sax.parse(file, handler)\n        return handler.object", "response": "Read OSM XML from input filename and return Overpass - like JSON."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bbox_to_poly(north, south, east, west):\n    \n    return Polygon([(west, south), (east, south), (east, north), (west, north)])", "response": "Convert bounding box to polygon"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_poi_query(north, south, east, west, amenities=None, timeout=180, maxsize=''):\n    if amenities:\n        # Overpass QL template\n        query_template = ('[out:json][timeout:{timeout}]{maxsize};((node[\"amenity\"~\"{amenities}\"]({south:.6f},'\n                          '{west:.6f},{north:.6f},{east:.6f});(._;>;););(way[\"amenity\"~\"{amenities}\"]({south:.6f},'\n                          '{west:.6f},{north:.6f},{east:.6f});(._;>;););(relation[\"amenity\"~\"{amenities}\"]'\n                          '({south:.6f},{west:.6f},{north:.6f},{east:.6f});(._;>;);););out;')\n\n        # Parse amenties\n        query_str = query_template.format(amenities=\"|\".join(amenities), north=north, south=south, east=east, west=west,\n                                          timeout=timeout, maxsize=maxsize)\n    else:\n        # Overpass QL template\n        query_template = ('[out:json][timeout:{timeout}]{maxsize};((node[\"amenity\"]({south:.6f},'\n                          '{west:.6f},{north:.6f},{east:.6f});(._;>;););(way[\"amenity\"]({south:.6f},'\n                          '{west:.6f},{north:.6f},{east:.6f});(._;>;););(relation[\"amenity\"]'\n                          '({south:.6f},{west:.6f},{north:.6f},{east:.6f});(._;>;);););out;')\n\n        # Parse amenties\n        query_str = query_template.format(north=north, south=south, east=east, west=west,\n                                          timeout=timeout, maxsize=maxsize)\n\n    return query_str", "response": "Parse the Overpass QL query based on the list of amenities."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndownloading POIs from OpenStreetMap based on selected amenities.", "response": "def osm_poi_download(polygon=None, amenities=None, north=None, south=None, east=None, west=None,\n                     timeout=180, max_query_area_size=50*1000*50*1000):\n    \"\"\"\n    Get points of interests (POIs) from OpenStreetMap based on selected amenity types.\n\n    Parameters\n    ----------\n    poly : shapely.geometry.Polygon\n        Polygon that will be used to limit the POI search. \n    amenities : list\n        List of amenities that will be used for finding the POIs from the selected area.\n\n    Returns\n    -------\n    gdf : geopandas.GeoDataFrame\n        Points of interest and the tags associated with them as geopandas GeoDataFrame.\n    \"\"\"\n\n    if polygon:\n        # Bounds\n        west, south, east, north = polygon.bounds\n\n        # Parse the Overpass QL query\n        query = parse_poi_query(amenities=amenities, west=west, south=south, east=east, north=north)\n\n    elif not (north is None or south is None or east is None or west is None):\n        # TODO: Add functionality for subdividing search area geometry based on max_query_area_size\n        # Parse Polygon from bbox\n        #polygon = bbox_to_poly(north=north, south=south, east=east, west=west)\n\n        # Parse the Overpass QL query\n        query = parse_poi_query(amenities=amenities, west=west, south=south, east=east, north=north)\n\n    else:\n        raise ValueError('You must pass a polygon or north, south, east, and west')\n\n    # Get the POIs\n    responses = overpass_request(data={'data': query}, timeout=timeout)\n\n    return responses"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_nodes_coords(osm_response):\n\n    coords = {}\n    for result in osm_response['elements']:\n        if 'type' in result and result['type'] == 'node':\n            coords[result['id']] = {'lat': result['lat'],\n                                    'lon': result['lon']}\n    return coords", "response": "Parse node coordinates from OSM response."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_polygonal_poi(coords, response):\n\n    if 'type' in response and response['type'] == 'way':\n        nodes = response['nodes']\n        try:\n            polygon = Polygon([(coords[node]['lon'], coords[node]['lat']) for node in nodes])\n\n            poi = {'nodes': nodes,\n                   'geometry': polygon,\n                   'osmid': response['id']}\n\n            if 'tags' in response:\n                for tag in response['tags']:\n                    poi[tag] = response['tags'][tag]\n            return poi\n\n        except Exception:\n            log('Polygon has invalid geometry: {}'.format(nodes))\n    \n    return None", "response": "Parses the areal POI way polygons from OSM node coords."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses OSM nodes and returns a dict of vertex IDs and their lat lon coordinates.", "response": "def parse_osm_node(response):\n    \"\"\"\n    Parse points from OSM nodes.\n\n    Parameters\n    ----------\n    response : JSON \n        Nodes from OSM response.  \n\n    Returns\n    -------\n    Dict of vertex IDs and their lat, lon coordinates.\n    \"\"\"\n\n    try:\n        point = Point(response['lon'], response['lat'])\n\n        poi = {\n            'osmid': response['id'],\n            'geometry': point\n        }\n        if 'tags' in response:\n            for tag in response['tags']:\n                poi[tag] = response['tags'][tag]\n\n    except Exception:\n        log('Point has invalid geometry: {}'.format(response['id']))\n\n    return poi"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef invalid_multipoly_handler(gdf, relation, way_ids):\n\n    try:\n        gdf_clean = gdf.dropna(subset=['geometry'])\n        multipoly = MultiPolygon(list(gdf_clean['geometry']))\n        return multipoly\n\n    except Exception:\n        log(\"Invalid geometry at relation id %s.\\nWay-ids of the invalid MultiPolygon:\" % (\n        relation['id'], str(way_ids)))\n        return None", "response": "Handles invalid multipolygon geometries when there exists a feature with NaN geometry"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the OSM relations and returns a GeoDataFrame with the MultiPolygon representations of the ways and nodes of those relations.", "response": "def parse_osm_relations(relations, osm_way_df):\n    \"\"\"\n    Parses the osm relations (multipolygons) from osm \n    ways and nodes. See more information about relations \n    from OSM documentation: http://wiki.openstreetmap.org/wiki/Relation \n         \n    Parameters\n    ----------\n    relations : list\n        OSM 'relation' items (dictionaries) in a list. \n    osm_way_df : gpd.GeoDataFrame\n        OSM 'way' features as a GeoDataFrame that contains all the \n        'way' features that will constitute the multipolygon relations.\n     \n    Returns\n    -------\n    gpd.GeoDataFrame\n        A GeoDataFrame with MultiPolygon representations of the \n        relations and the attributes associated with them.   \n    \"\"\"\n\n    gdf_relations = gpd.GeoDataFrame()\n\n    # Iterate over relations and extract the items\n    for relation in relations:\n        if relation['tags']['type'] == 'multipolygon':\n            try:\n                # Parse member 'way' ids\n                member_way_ids = [member['ref'] for member in relation['members'] if member['type'] == 'way']\n                # Extract the ways\n                member_ways = osm_way_df.reindex(member_way_ids)\n                # Extract the nodes of those ways\n                member_nodes = list(member_ways['nodes'].values)\n                try:\n                    # Create MultiPolygon from geometries (exclude NaNs)\n                    multipoly = MultiPolygon(list(member_ways['geometry']))\n                except Exception:\n                    multipoly = invalid_multipoly_handler(gdf=member_ways, relation=relation, way_ids=member_way_ids)\n\n                if multipoly:\n                    # Create GeoDataFrame with the tags and the MultiPolygon and its 'ways' (ids), and the 'nodes' of those ways\n                    geo = gpd.GeoDataFrame(relation['tags'], index=[relation['id']])\n                    # Initialize columns (needed for .loc inserts)\n                    geo = geo.assign(geometry=None, ways=None, nodes=None, element_type=None, osmid=None)\n                    # Add attributes\n                    geo.loc[relation['id'], 'geometry'] = multipoly\n                    geo.loc[relation['id'], 'ways'] = member_way_ids\n                    geo.loc[relation['id'], 'nodes'] = member_nodes\n                    geo.loc[relation['id'], 'element_type'] = 'relation'\n                    geo.loc[relation['id'], 'osmid'] = relation['id']\n\n                    # Append to relation GeoDataFrame\n                    gdf_relations = gdf_relations.append(geo, sort=False)\n                    # Remove such 'ways' from 'osm_way_df' that are part of the 'relation'\n                    osm_way_df = osm_way_df.drop(member_way_ids)\n            except Exception:\n                log(\"Could not handle OSM 'relation': {}\".format(relation['id']))\n\n    # Merge 'osm_way_df' and the 'gdf_relations'\n    osm_way_df = osm_way_df.append(gdf_relations, sort=False)\n    return osm_way_df"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a GeoDataFrame from the OSM POI json file.", "response": "def create_poi_gdf(polygon=None, amenities=None, north=None, south=None, east=None, west=None):\n    \"\"\"\n    Parse GeoDataFrames from POI json that was returned by Overpass API.\n\n    Parameters\n    ----------\n    polygon : shapely Polygon or MultiPolygon\n        geographic shape to fetch the POIs within\n    amenities: list\n        List of amenities that will be used for finding the POIs from the selected area. \n        See available amenities from: http://wiki.openstreetmap.org/wiki/Key:amenity \n    north : float\n        northern latitude of bounding box\n    south : float\n        southern latitude of bounding box\n    east : float\n        eastern longitude of bounding box\n    west : float\n        western longitude of bounding box\n        \n    Returns\n    -------\n    Geopandas GeoDataFrame with POIs and the associated attributes. \n    \"\"\"\n\n    responses = osm_poi_download(polygon=polygon, amenities=amenities, north=north, south=south, east=east, west=west)\n\n    # Parse coordinates from all the nodes in the response\n    coords = parse_nodes_coords(responses)\n\n    # POI nodes\n    poi_nodes = {}\n\n    # POI ways\n    poi_ways = {}\n\n    # A list of POI relations\n    relations = []\n\n    for result in responses['elements']:\n        if result['type'] == 'node' and 'tags' in result:\n            poi = parse_osm_node(response=result)\n            # Add element_type\n            poi['element_type'] = 'node'\n            # Add to 'pois'\n            poi_nodes[result['id']] = poi\n        elif result['type'] == 'way':\n            # Parse POI area Polygon\n            poi_area = parse_polygonal_poi(coords=coords, response=result)\n            if poi_area:\n                # Add element_type\n                poi_area['element_type'] = 'way'\n                # Add to 'poi_ways'\n                poi_ways[result['id']] = poi_area\n\n        elif result['type'] == 'relation':\n            # Add relation to a relation list (needs to be parsed after all nodes and ways have been parsed)\n            relations.append(result)\n\n    # Create GeoDataFrames\n    gdf_nodes = gpd.GeoDataFrame(poi_nodes).T\n    gdf_nodes.crs = settings.default_crs\n\n    gdf_ways = gpd.GeoDataFrame(poi_ways).T\n    gdf_ways.crs = settings.default_crs\n\n    # Parse relations (MultiPolygons) from 'ways'\n    gdf_ways = parse_osm_relations(relations=relations, osm_way_df=gdf_ways)\n\n    # Combine GeoDataFrames\n    gdf = gdf_nodes.append(gdf_ways, sort=False)\n\n    return gdf"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a GeoDataFrame containing all the POIs within some distance north south east and west of a point.", "response": "def pois_from_point(point, distance=None, amenities=None):\n    \"\"\"\n    Get point of interests (POIs) within some distance north, south, east, and west of\n    a lat-long point.\n\n    Parameters\n    ----------\n    point : tuple\n        a lat-long point\n    distance : numeric\n        distance in meters\n    amenities : list\n        List of amenities that will be used for finding the POIs from the selected area. \n        See available amenities from: http://wiki.openstreetmap.org/wiki/Key:amenity\n\n    Returns\n    -------\n    GeoDataFrame \n    \"\"\"\n\n    bbox = bbox_from_point(point=point, distance=distance)\n    north, south, east, west = bbox\n    return create_poi_gdf(amenities=amenities, north=north, south=south, east=east, west=west)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pois_from_address(address, distance, amenities=None):\n\n    # geocode the address string to a (lat, lon) point\n    point = geocode(query=address)\n\n    # get POIs within distance of this point\n    return pois_from_point(point=point, amenities=amenities, distance=distance)", "response": "Returns a GeoDataFrame containing OSM points of Interests within some distance north south east and west of the address."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pois_from_place(place, amenities=None):\n\n    city = gdf_from_place(place)\n    polygon = city['geometry'].iloc[0]\n    return create_poi_gdf(polygon=polygon, amenities=amenities)", "response": "Returns a GeoDataFrame containing the POIs within the boundaries of some place."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if the node is a real endpoint of an edge in the network.", "response": "def is_endpoint(G, node, strict=True):\n    \"\"\"\n    Return True if the node is a \"real\" endpoint of an edge in the network, \\\n    otherwise False. OSM data includes lots of nodes that exist only as points \\\n    to help streets bend around curves. An end point is a node that either: \\\n    1) is its own neighbor, ie, it self-loops. \\\n    2) or, has no incoming edges or no outgoing edges, ie, all its incident \\\n        edges point inward or all its incident edges point outward. \\\n    3) or, it does not have exactly two neighbors and degree of 2 or 4. \\\n    4) or, if strict mode is false, if its edges have different OSM IDs. \\\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n\n    node : int\n        the node to examine\n    strict : bool\n        if False, allow nodes to be end points even if they fail all other rules \\\n        but have edges with different OSM IDs\n\n    Returns\n    -------\n    bool\n\n    \"\"\"\n    neighbors = set(list(G.predecessors(node)) + list(G.successors(node)))\n    n = len(neighbors)\n    d = G.degree(node)\n\n    if node in neighbors:\n        # if the node appears in its list of neighbors, it self-loops. this is\n        # always an endpoint.\n        return True\n\n    # if node has no incoming edges or no outgoing edges, it must be an endpoint\n    elif G.out_degree(node)==0 or G.in_degree(node)==0:\n        return True\n\n    elif not (n==2 and (d==2 or d==4)):\n        # else, if it does NOT have 2 neighbors AND either 2 or 4 directed\n        # edges, it is an endpoint. either it has 1 or 3+ neighbors, in which\n        # case it is a dead-end or an intersection of multiple streets or it has\n        # 2 neighbors but 3 degree (indicating a change from oneway to twoway)\n        # or more than 4 degree (indicating a parallel edge) and thus is an\n        # endpoint\n        return True\n\n    elif not strict:\n        # non-strict mode\n        osmids = []\n\n        # add all the edge OSM IDs for incoming edges\n        for u in G.predecessors(node):\n            for key in G[u][node]:\n                osmids.append(G.edges[u, node, key]['osmid'])\n\n        # add all the edge OSM IDs for outgoing edges\n        for v in G.successors(node):\n            for key in G[node][v]:\n                osmids.append(G.edges[node, v, key]['osmid'])\n\n        # if there is more than 1 OSM ID in the list of edge OSM IDs then it is\n        # an endpoint, if not, it isn't\n        return len(set(osmids)) > 1\n\n    else:\n        # if none of the preceding rules returned true, then it is not an endpoint\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_path(G, node, endpoints, path):\n    # for each successor in the passed-in node\n    for successor in G.successors(node):\n        if successor not in path:\n            # if this successor is already in the path, ignore it, otherwise add\n            # it to the path\n            path.append(successor)\n            if successor not in endpoints:\n                # if this successor is not an endpoint, recursively call\n                # build_path until you find an endpoint\n                path = build_path(G, successor, endpoints, path)\n            else:\n                # if this successor is an endpoint, we've completed the path,\n                # so return it\n                return path\n\n    if (path[-1] not in endpoints) and (path[0] in G.successors(path[-1])):\n        # if the end of the path is not actually an endpoint and the path's\n        # first node is a successor of the path's final node, then this is\n        # actually a self loop, so add path's first node to end of path to\n        # close it\n        path.append(path[0])\n\n    return path", "response": "Recursively build a path of nodes that are in the given set of endpoints."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_paths_to_simplify(G, strict=True):\n\n    # first identify all the nodes that are endpoints\n    start_time = time.time()\n    endpoints = set([node for node in G.nodes() if is_endpoint(G, node, strict=strict)])\n    log('Identified {:,} edge endpoints in {:,.2f} seconds'.format(len(endpoints), time.time()-start_time))\n\n    start_time = time.time()\n    paths_to_simplify = []\n\n    # for each endpoint node, look at each of its successor nodes\n    for node in endpoints:\n        for successor in G.successors(node):\n            if successor not in endpoints:\n                # if the successor is not an endpoint, build a path from the\n                # endpoint node to the next endpoint node\n                try:\n                    path = build_path(G, successor, endpoints, path=[node, successor])\n                    paths_to_simplify.append(path)\n                except RuntimeError:\n                    log('Recursion error: exceeded max depth, moving on to next endpoint successor', level=lg.WARNING)\n                    # recursion errors occur if some connected component is a\n                    # self-contained ring in which all nodes are not end points.\n                    # could also occur in extremely long street segments (eg, in\n                    # rural areas) with too many nodes between true endpoints.\n                    # handle it by just ignoring that component and letting its\n                    # topology remain intact (this should be a rare occurrence)\n                    # RuntimeError is what Python <3.5 will throw, Py3.5+ throws\n                    # RecursionError but it is a subtype of RuntimeError so it\n                    # still gets handled\n\n    log('Constructed all paths to simplify in {:,.2f} seconds'.format(time.time()-start_time))\n    return paths_to_simplify", "response": "Returns a list of all the paths to be simplified between endpoint nodes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsimplify a networkx graph by removing all nodes that are not intersections and optionally adding all edges that are not intersections or dead - ends.", "response": "def simplify_graph(G, strict=True):\n    \"\"\"\n    Simplify a graph's topology by removing all nodes that are not intersections\n    or dead-ends.\n\n    Create an edge directly between the end points that encapsulate them,\n    but retain the geometry of the original edges, saved as attribute in new\n    edge.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n    strict : bool\n        if False, allow nodes to be end points even if they fail all other rules\n        but have edges with different OSM IDs\n\n    Returns\n    -------\n    networkx multidigraph\n    \"\"\"\n\n    if is_simplified(G):\n        raise Exception('This graph has already been simplified, cannot simplify it again.')\n\n    log('Begin topologically simplifying the graph...')\n    G = G.copy()\n    initial_node_count = len(list(G.nodes()))\n    initial_edge_count = len(list(G.edges()))\n    all_nodes_to_remove = []\n    all_edges_to_add = []\n\n    # construct a list of all the paths that need to be simplified\n    paths = get_paths_to_simplify(G, strict=strict)\n\n    start_time = time.time()\n    for path in paths:\n\n        # add the interstitial edges we're removing to a list so we can retain\n        # their spatial geometry\n        edge_attributes = {}\n        for u, v in zip(path[:-1], path[1:]):\n\n            # there shouldn't be multiple edges between interstitial nodes\n            if not G.number_of_edges(u, v) == 1:\n                log('Multiple edges between \"{}\" and \"{}\" found when simplifying'.format(u, v), level=lg.WARNING)\n\n            # the only element in this list as long as above check is True\n            # (MultiGraphs use keys (the 0 here), indexed with ints from 0 and\n            # up)\n            edge = G.edges[u, v, 0]\n            for key in edge:\n                if key in edge_attributes:\n                    # if this key already exists in the dict, append it to the\n                    # value list\n                    edge_attributes[key].append(edge[key])\n                else:\n                    # if this key doesn't already exist, set the value to a list\n                    # containing the one value\n                    edge_attributes[key] = [edge[key]]\n\n        for key in edge_attributes:\n            # don't touch the length attribute, we'll sum it at the end\n            if len(set(edge_attributes[key])) == 1 and not key == 'length':\n                # if there's only 1 unique value in this attribute list,\n                # consolidate it to the single value (the zero-th)\n                edge_attributes[key] = edge_attributes[key][0]\n            elif not key == 'length':\n                # otherwise, if there are multiple values, keep one of each value\n                edge_attributes[key] = list(set(edge_attributes[key]))\n\n        # construct the geometry and sum the lengths of the segments\n        edge_attributes['geometry'] = LineString([Point((G.nodes[node]['x'], G.nodes[node]['y'])) for node in path])\n        edge_attributes['length'] = sum(edge_attributes['length'])\n\n        # add the nodes and edges to their lists for processing at the end\n        all_nodes_to_remove.extend(path[1:-1])\n        all_edges_to_add.append({'origin':path[0],\n                                 'destination':path[-1],\n                                 'attr_dict':edge_attributes})\n\n    # for each edge to add in the list we assembled, create a new edge between\n    # the origin and destination\n    for edge in all_edges_to_add:\n        G.add_edge(edge['origin'], edge['destination'], **edge['attr_dict'])\n\n    # finally remove all the interstitial nodes between the new edges\n    G.remove_nodes_from(set(all_nodes_to_remove))\n\n    G.graph['simplified'] = True\n\n    msg = 'Simplified graph (from {:,} to {:,} nodes and from {:,} to {:,} edges) in {:,.2f} seconds'\n    log(msg.format(initial_node_count, len(list(G.nodes())), initial_edge_count, len(list(G.edges())), time.time()-start_time))\n    return G"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclean - up intersections comprising clusters of nodes and edges of nodes and edges of nodes and edges of edges.", "response": "def clean_intersections(G, tolerance=15, dead_ends=False):\n    \"\"\"\n    Clean-up intersections comprising clusters of nodes by merging them and\n    returning their centroids.\n\n    Divided roads are represented by separate centerline edges. The intersection\n    of two divided roads thus creates 4 nodes, representing where each edge\n    intersects a perpendicular edge. These 4 nodes represent a single\n    intersection in the real world. This function cleans them up by buffering\n    their points to an arbitrary distance, merging overlapping buffers, and\n    taking their centroid. For best results, the tolerance argument should be\n    adjusted to approximately match street design standards in the specific\n    street network.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n    tolerance : float\n        nodes within this distance (in graph's geometry's units) will be\n        dissolved into a single intersection\n    dead_ends : bool\n        if False, discard dead-end nodes to return only street-intersection\n        points\n\n    Returns\n    ----------\n    intersection_centroids : geopandas.GeoSeries\n        a GeoSeries of shapely Points representing the centroids of street\n        intersections\n    \"\"\"\n\n    # if dead_ends is False, discard dead-end nodes to only work with edge\n    # intersections\n    if not dead_ends:\n        if 'streets_per_node' in G.graph:\n            streets_per_node = G.graph['streets_per_node']\n        else:\n            streets_per_node = count_streets_per_node(G)\n\n        dead_end_nodes = [node for node, count in streets_per_node.items() if count <= 1]\n        G = G.copy()\n        G.remove_nodes_from(dead_end_nodes)\n\n    # create a GeoDataFrame of nodes, buffer to passed-in distance, merge\n    # overlaps\n    gdf_nodes = graph_to_gdfs(G, edges=False)\n    buffered_nodes = gdf_nodes.buffer(tolerance).unary_union\n    if isinstance(buffered_nodes, Polygon):\n        # if only a single node results, make it iterable so we can turn it into\n        # a GeoSeries\n        buffered_nodes = [buffered_nodes]\n\n    # get the centroids of the merged intersection polygons\n    unified_intersections = gpd.GeoSeries(list(buffered_nodes))\n    intersection_centroids = unified_intersections.centroid\n    return intersection_centroids"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef project_geometry(geometry, crs=None, to_crs=None, to_latlong=False):\n\n    if crs is None:\n        crs = settings.default_crs\n\n    gdf = gpd.GeoDataFrame()\n    gdf.crs = crs\n    gdf.gdf_name = 'geometry to project'\n    gdf['geometry'] = None\n    gdf.loc[0, 'geometry'] = geometry\n    gdf_proj = project_gdf(gdf, to_crs=to_crs, to_latlong=to_latlong)\n    geometry_proj = gdf_proj['geometry'].iloc[0]\n    return geometry_proj, gdf_proj.crs", "response": "Project a shapely geometry to UTM or a shapely polygon or MultiPolygon from lat - long to UTM"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef project_gdf(gdf, to_crs=None, to_latlong=False):\n    assert len(gdf) > 0, 'You cannot project an empty GeoDataFrame.'\n    start_time = time.time()\n\n    # if gdf has no gdf_name attribute, create one now\n    if not hasattr(gdf, 'gdf_name'):\n        gdf.gdf_name = 'unnamed'\n\n    # if to_crs was passed-in, use this value to project the gdf\n    if to_crs is not None:\n        projected_gdf = gdf.to_crs(to_crs)\n\n    # if to_crs was not passed-in, calculate the centroid of the geometry to\n    # determine UTM zone\n    else:\n        if to_latlong:\n            # if to_latlong is True, project the gdf to latlong\n            latlong_crs = settings.default_crs\n            projected_gdf = gdf.to_crs(latlong_crs)\n            log('Projected the GeoDataFrame \"{}\" to default_crs in {:,.2f} seconds'.format(gdf.gdf_name, time.time()-start_time))\n        else:\n            # else, project the gdf to UTM\n            # if GeoDataFrame is already in UTM, just return it\n            if (gdf.crs is not None) and ('proj' in gdf.crs) and (gdf.crs['proj'] == 'utm'):\n                return gdf\n\n            # calculate the centroid of the union of all the geometries in the\n            # GeoDataFrame\n            avg_longitude = gdf['geometry'].unary_union.centroid.x\n\n            # calculate the UTM zone from this avg longitude and define the UTM\n            # CRS to project\n            utm_zone = int(math.floor((avg_longitude + 180) / 6.) + 1)\n            utm_crs = {'datum': 'WGS84',\n                       'ellps': 'WGS84',\n                       'proj' : 'utm',\n                       'zone' : utm_zone,\n                       'units': 'm'}\n\n            # project the GeoDataFrame to the UTM CRS\n            projected_gdf = gdf.to_crs(utm_crs)\n            log('Projected the GeoDataFrame \"{}\" to UTM-{} in {:,.2f} seconds'.format(gdf.gdf_name, utm_zone, time.time()-start_time))\n\n    projected_gdf.gdf_name = gdf.gdf_name\n    return projected_gdf", "response": "Project a GeoDataFrame to the UTM zone appropriate for its geometries."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef project_graph(G, to_crs=None):\n\n    G_proj = G.copy()\n    start_time = time.time()\n\n    # create a GeoDataFrame of the nodes, name it, convert osmid to str\n    nodes, data = zip(*G_proj.nodes(data=True))\n    gdf_nodes = gpd.GeoDataFrame(list(data), index=nodes)\n    gdf_nodes.crs = G_proj.graph['crs']\n    gdf_nodes.gdf_name = '{}_nodes'.format(G_proj.name)\n\n    # create new lat/lon columns just to save that data for later, and create a\n    # geometry column from x/y\n    gdf_nodes['lon'] = gdf_nodes['x']\n    gdf_nodes['lat'] = gdf_nodes['y']\n    gdf_nodes['geometry'] = gdf_nodes.apply(lambda row: Point(row['x'], row['y']), axis=1)\n    log('Created a GeoDataFrame from graph in {:,.2f} seconds'.format(time.time()-start_time))\n\n    # project the nodes GeoDataFrame to UTM\n    gdf_nodes_utm = project_gdf(gdf_nodes, to_crs=to_crs)\n\n    # extract data for all edges that have geometry attribute\n    edges_with_geom = []\n    for u, v, key, data in G_proj.edges(keys=True, data=True):\n        if 'geometry' in data:\n            edges_with_geom.append({'u':u, 'v':v, 'key':key, 'geometry':data['geometry']})\n\n    # create an edges GeoDataFrame and project to UTM, if there were any edges\n    # with a geometry attribute. geom attr only exists if graph has been\n    # simplified, otherwise you don't have to project anything for the edges\n    # because the nodes still contain all spatial data\n    if len(edges_with_geom) > 0:\n        gdf_edges = gpd.GeoDataFrame(edges_with_geom)\n        gdf_edges.crs = G_proj.graph['crs']\n        gdf_edges.gdf_name = '{}_edges'.format(G_proj.name)\n        gdf_edges_utm = project_gdf(gdf_edges, to_crs=to_crs)\n\n    # extract projected x and y values from the nodes' geometry column\n    start_time = time.time()\n    gdf_nodes_utm['x'] = gdf_nodes_utm['geometry'].map(lambda point: point.x)\n    gdf_nodes_utm['y'] = gdf_nodes_utm['geometry'].map(lambda point: point.y)\n    gdf_nodes_utm = gdf_nodes_utm.drop('geometry', axis=1)\n    log('Extracted projected node geometries from GeoDataFrame in {:,.2f} seconds'.format(time.time()-start_time))\n\n    # clear the graph to make it a blank slate for the projected data\n    start_time = time.time()\n    edges = list(G_proj.edges(keys=True, data=True))\n    graph_name = G_proj.graph['name']\n    G_proj.clear()\n\n    # add the projected nodes and all their attributes to the graph\n    G_proj.add_nodes_from(gdf_nodes_utm.index)\n    attributes = gdf_nodes_utm.to_dict()\n    for label in gdf_nodes_utm.columns:\n        nx.set_node_attributes(G_proj, name=label, values=attributes[label])\n\n    # add the edges and all their attributes (including reconstructed geometry,\n    # when it exists) to the graph\n    for u, v, key, attributes in edges:\n        if 'geometry' in attributes:\n            row = gdf_edges_utm[(gdf_edges_utm['u']==u) & (gdf_edges_utm['v']==v) & (gdf_edges_utm['key']==key)]\n            attributes['geometry'] = row['geometry'].iloc[0]\n\n        # attributes dict contains key, so we don't need to explicitly pass it here\n        G_proj.add_edge(u, v, **attributes)\n\n    # set the graph's CRS attribute to the new, projected CRS and return the\n    # projected graph\n    G_proj.graph['crs'] = gdf_nodes_utm.crs\n    G_proj.graph['name'] = '{}_UTM'.format(graph_name)\n    if 'streets_per_node' in G.graph:\n        G_proj.graph['streets_per_node'] = G.graph['streets_per_node']\n    log('Rebuilt projected graph in {:,.2f} seconds'.format(time.time()-start_time))\n    return G_proj", "response": "Project a networkx graph to UTM zone appropriate for its geographic\n    location."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nplotting a GeoDataFrame of place boundary geometries.", "response": "def plot_shape(gdf, fc='#cbe0f0', ec='#999999', linewidth=1, alpha=1,\n               figsize=(6,6), margin=0.02, axis_off=True):\n    \"\"\"\n    Plot a GeoDataFrame of place boundary geometries.\n\n    Parameters\n    ----------\n    gdf : GeoDataFrame\n        the gdf containing the geometries to plot\n    fc : string or list\n        the facecolor (or list of facecolors) for the polygons\n    ec : string or list\n        the edgecolor (or list of edgecolors) for the polygons\n    linewidth : numeric\n        the width of the polygon edge lines\n    alpha : numeric\n        the opacity\n    figsize : tuple\n        the size of the plotting figure\n    margin : numeric\n        the size of the figure margins\n    axis_off : bool\n        if True, disable the matplotlib axes display\n\n    Returns\n    -------\n    fig, ax : tuple\n    \"\"\"\n\n    # if facecolor or edgecolor is a string instead of a list, make sure we have\n    # as many colors as gdf elements\n    if isinstance(fc, str):\n        fc = [fc] * len(gdf)\n    if isinstance(ec, str):\n        ec = [ec] * len(gdf)\n\n    # plot the geometries one at a time\n    fig, ax = plt.subplots(figsize=figsize)\n    for geometry, facecolor, edgecolor in zip(gdf['geometry'], fc, ec):\n        if isinstance(geometry, (Polygon, MultiPolygon)):\n            if isinstance(geometry, Polygon):\n                geometry = MultiPolygon([geometry])\n            for polygon in geometry:\n                patch = PolygonPatch(polygon, fc=facecolor, ec=edgecolor, linewidth=linewidth, alpha=alpha)\n                ax.add_patch(patch)\n        else:\n            raise ValueError('All geometries in GeoDataFrame must be shapely Polygons or MultiPolygons')\n\n    # adjust the axis margins and limits around the image and make axes\n    # equal-aspect\n    west, south, east, north = gdf.unary_union.bounds\n    margin_ns = (north - south) * margin\n    margin_ew = (east - west) * margin\n    ax.set_ylim((south - margin_ns, north + margin_ns))\n    ax.set_xlim((west - margin_ew, east + margin_ew))\n    ax.set_aspect(aspect='equal', adjustable='box')\n    if axis_off:\n        ax.axis('off')\n\n    plt.show()\n    return fig, ax"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rgb_color_list_to_hex(color_list):\n    color_list_rgb = [[int(x*255) for x in c[0:3]] for c in color_list]\n    color_list_hex = ['#{:02X}{:02X}{:02X}'.format(rgb[0], rgb[1], rgb[2]) for rgb in color_list_rgb]\n    return color_list_hex", "response": "Convert a list of RGBa colors to a list of hexadecimal color codes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn n - length list of RGBa colors from the passed colormap name and alpha.", "response": "def get_colors(n, cmap='viridis', start=0., stop=1., alpha=1., return_hex=False):\n    \"\"\"\n    Return n-length list of RGBa colors from the passed colormap name and alpha.\n\n    Parameters\n    ----------\n    n : int\n        number of colors\n    cmap : string\n        name of a colormap\n    start : float\n        where to start in the colorspace\n    stop : float\n        where to end in the colorspace\n    alpha : float\n        opacity, the alpha channel for the RGBa colors\n    return_hex : bool\n        if True, convert RGBa colors to a hexadecimal string\n\n    Returns\n    -------\n    colors : list\n    \"\"\"\n    colors = [cm.get_cmap(cmap)(x) for x in np.linspace(start, stop, n)]\n    colors = [(r, g, b, alpha) for r, g, b, _ in colors]\n    if return_hex:\n        colors = rgb_color_list_to_hex(colors)\n    return colors"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_node_colors_by_attr(G, attr, num_bins=None, cmap='viridis', start=0, stop=1, na_color='none'):\n    if num_bins is None:\n        num_bins=len(G.nodes())\n    bin_labels = range(num_bins)\n    attr_values = pd.Series([data[attr] for node, data in G.nodes(data=True)])\n    cats = pd.qcut(x=attr_values, q=num_bins, labels=bin_labels)\n    colors = get_colors(num_bins, cmap, start, stop)\n    node_colors = [colors[int(cat)] if pd.notnull(cat) else na_color for cat in cats]\n    return node_colors", "response": "Get a list of node colors by binning some continuous - variable attribute into a random variety of quantiles."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_edge_colors_by_attr(G, attr, num_bins=5, cmap='viridis', start=0, stop=1, na_color='none'):\n    if num_bins is None:\n        num_bins=len(G.edges())\n    bin_labels = range(num_bins)\n    attr_values = pd.Series([data[attr] for u, v, key, data in G.edges(keys=True, data=True)])\n    cats = pd.qcut(x=attr_values, q=num_bins, labels=bin_labels)\n    colors = get_colors(num_bins, cmap, start, stop)\n    edge_colors = [colors[int(cat)] if pd.notnull(cat) else na_color for cat in cats]\n    return edge_colors", "response": "Get a list of edge colors by binning some continuous - variable attribute into a random network."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save_and_show(fig, ax, save, show, close, filename, file_format, dpi, axis_off):\n    # save the figure if specified\n    if save:\n        start_time = time.time()\n\n        # create the save folder if it doesn't already exist\n        if not os.path.exists(settings.imgs_folder):\n            os.makedirs(settings.imgs_folder)\n        path_filename = os.path.join(settings.imgs_folder, os.extsep.join([filename, file_format]))\n\n        if file_format == 'svg':\n            # if the file_format is svg, prep the fig/ax a bit for saving\n            ax.axis('off')\n            ax.set_position([0, 0, 1, 1])\n            ax.patch.set_alpha(0.)\n            fig.patch.set_alpha(0.)\n            fig.savefig(path_filename, bbox_inches=0, format=file_format, facecolor=fig.get_facecolor(), transparent=True)\n        else:\n            if axis_off:\n                # if axis is turned off, constrain the saved figure's extent to\n                # the interior of the axis\n                extent = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n            else:\n                extent = 'tight'\n            fig.savefig(path_filename, dpi=dpi, bbox_inches=extent, format=file_format, facecolor=fig.get_facecolor(), transparent=True)\n        log('Saved the figure to disk in {:,.2f} seconds'.format(time.time()-start_time))\n\n    # show the figure if specified\n    if show:\n        start_time = time.time()\n        plt.show()\n        log('Showed the plot in {:,.2f} seconds'.format(time.time()-start_time))\n    # if show=False, close the figure if close=True to prevent display\n    elif close:\n        plt.close()\n\n    return fig, ax", "response": "Save a figure to disk and show it as specified."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplot a networkx spatial graph.", "response": "def plot_graph(G, bbox=None, fig_height=6, fig_width=None, margin=0.02,\n               axis_off=True, equal_aspect=False, bgcolor='w', show=True,\n               save=False, close=True, file_format='png', filename='temp',\n               dpi=300, annotate=False, node_color='#66ccff', node_size=15,\n               node_alpha=1, node_edgecolor='none', node_zorder=1,\n               edge_color='#999999', edge_linewidth=1, edge_alpha=1,\n               use_geom=True):\n    \"\"\"\n    Plot a networkx spatial graph.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n    bbox : tuple\n        bounding box as north,south,east,west - if None will calculate from\n        spatial extents of data. if passing a bbox, you probably also want to\n        pass margin=0 to constrain it.\n    fig_height : int\n        matplotlib figure height in inches\n    fig_width : int\n        matplotlib figure width in inches\n    margin : float\n        relative margin around the figure\n    axis_off : bool\n        if True turn off the matplotlib axis\n    equal_aspect : bool\n        if True set the axis aspect ratio equal\n    bgcolor : string\n        the background color of the figure and axis\n    show : bool\n        if True, show the figure\n    save : bool\n        if True, save the figure as an image file to disk\n    close : bool\n        close the figure (only if show equals False) to prevent display\n    file_format : string\n        the format of the file to save (e.g., 'jpg', 'png', 'svg')\n    filename : string\n        the name of the file if saving\n    dpi : int\n        the resolution of the image file if saving\n    annotate : bool\n        if True, annotate the nodes in the figure\n    node_color : string\n        the color of the nodes\n    node_size : int\n        the size of the nodes\n    node_alpha : float\n        the opacity of the nodes\n    node_edgecolor : string\n        the color of the node's marker's border\n    node_zorder : int\n        zorder to plot nodes, edges are always 2, so make node_zorder 1 to plot\n        nodes beneath them or 3 to plot nodes atop them\n    edge_color : string\n        the color of the edges' lines\n    edge_linewidth : float\n        the width of the edges' lines\n    edge_alpha : float\n        the opacity of the edges' lines\n    use_geom : bool\n        if True, use the spatial geometry attribute of the edges to draw\n        geographically accurate edges, rather than just lines straight from node\n        to node\n\n    Returns\n    -------\n    fig, ax : tuple\n    \"\"\"\n\n    log('Begin plotting the graph...')\n    node_Xs = [float(x) for _, x in G.nodes(data='x')]\n    node_Ys = [float(y) for _, y in G.nodes(data='y')]\n\n    # get north, south, east, west values either from bbox parameter or from the\n    # spatial extent of the edges' geometries\n    if bbox is None:\n        edges = graph_to_gdfs(G, nodes=False, fill_edge_geometry=True)\n        west, south, east, north = edges.total_bounds\n    else:\n        north, south, east, west = bbox\n\n    # if caller did not pass in a fig_width, calculate it proportionately from\n    # the fig_height and bounding box aspect ratio\n    bbox_aspect_ratio = (north-south)/(east-west)\n    if fig_width is None:\n        fig_width = fig_height / bbox_aspect_ratio\n\n    # create the figure and axis\n    fig, ax = plt.subplots(figsize=(fig_width, fig_height), facecolor=bgcolor)\n    ax.set_facecolor(bgcolor)\n\n    # draw the edges as lines from node to node\n    start_time = time.time()\n    lines = []\n    for u, v, data in G.edges(keys=False, data=True):\n        if 'geometry' in data and use_geom:\n            # if it has a geometry attribute (a list of line segments), add them\n            # to the list of lines to plot\n            xs, ys = data['geometry'].xy\n            lines.append(list(zip(xs, ys)))\n        else:\n            # if it doesn't have a geometry attribute, the edge is a straight\n            # line from node to node\n            x1 = G.nodes[u]['x']\n            y1 = G.nodes[u]['y']\n            x2 = G.nodes[v]['x']\n            y2 = G.nodes[v]['y']\n            line = [(x1, y1), (x2, y2)]\n            lines.append(line)\n\n    # add the lines to the axis as a linecollection\n    lc = LineCollection(lines, colors=edge_color, linewidths=edge_linewidth, alpha=edge_alpha, zorder=2)\n    ax.add_collection(lc)\n    log('Drew the graph edges in {:,.2f} seconds'.format(time.time()-start_time))\n\n    # scatter plot the nodes\n    ax.scatter(node_Xs, node_Ys, s=node_size, c=node_color, alpha=node_alpha, edgecolor=node_edgecolor, zorder=node_zorder)\n\n    # set the extent of the figure\n    margin_ns = (north - south) * margin\n    margin_ew = (east - west) * margin\n    ax.set_ylim((south - margin_ns, north + margin_ns))\n    ax.set_xlim((west - margin_ew, east + margin_ew))\n\n    # configure axis appearance\n    xaxis = ax.get_xaxis()\n    yaxis = ax.get_yaxis()\n\n    xaxis.get_major_formatter().set_useOffset(False)\n    yaxis.get_major_formatter().set_useOffset(False)\n\n    # if axis_off, turn off the axis display set the margins to zero and point\n    # the ticks in so there's no space around the plot\n    if axis_off:\n        ax.axis('off')\n        ax.margins(0)\n        ax.tick_params(which='both', direction='in')\n        xaxis.set_visible(False)\n        yaxis.set_visible(False)\n        fig.canvas.draw()\n\n    if equal_aspect:\n        # make everything square\n        ax.set_aspect('equal')\n        fig.canvas.draw()\n    else:\n        # if the graph is not projected, conform the aspect ratio to not stretch the plot\n        if G.graph['crs'] == settings.default_crs:\n            coslat = np.cos((min(node_Ys) + max(node_Ys)) / 2. / 180. * np.pi)\n            ax.set_aspect(1. / coslat)\n            fig.canvas.draw()\n\n    # annotate the axis with node IDs if annotate=True\n    if annotate:\n        for node, data in G.nodes(data=True):\n            ax.annotate(node, xy=(data['x'], data['y']))\n\n    # save and show the figure as specified\n    fig, ax = save_and_show(fig, ax, save, show, close, filename, file_format, dpi, axis_off)\n    return fig, ax"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a list of nodes return a list of lines that together follow the path .", "response": "def node_list_to_coordinate_lines(G, node_list, use_geom=True):\n    \"\"\"\n    Given a list of nodes, return a list of lines that together follow the path\n    defined by the list of nodes.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n    route : list\n        the route as a list of nodes\n    use_geom : bool\n        if True, use the spatial geometry attribute of the edges to draw\n        geographically accurate edges, rather than just lines straight from node\n        to node\n\n    Returns\n    -------\n    lines : list of lines given as pairs ( (x_start, y_start), (x_stop, y_stop) )\n    \"\"\"\n    edge_nodes = list(zip(node_list[:-1], node_list[1:]))\n    lines = []\n    for u, v in edge_nodes:\n        # if there are parallel edges, select the shortest in length\n        data = min(G.get_edge_data(u, v).values(), key=lambda x: x['length'])\n\n        # if it has a geometry attribute (ie, a list of line segments)\n        if 'geometry' in data and use_geom:\n            # add them to the list of lines to plot\n            xs, ys = data['geometry'].xy\n            lines.append(list(zip(xs, ys)))\n        else:\n            # if it doesn't have a geometry attribute, the edge is a straight\n            # line from node to node\n            x1 = G.nodes[u]['x']\n            y1 = G.nodes[u]['y']\n            x2 = G.nodes[v]['x']\n            y2 = G.nodes[v]['y']\n            line = [(x1, y1), (x2, y2)]\n            lines.append(line)\n    return lines"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplotting a route along a networkx spatial graph.", "response": "def plot_graph_route(G, route, bbox=None, fig_height=6, fig_width=None,\n                     margin=0.02, bgcolor='w', axis_off=True, show=True,\n                     save=False, close=True, file_format='png', filename='temp',\n                     dpi=300, annotate=False, node_color='#999999',\n                     node_size=15, node_alpha=1, node_edgecolor='none',\n                     node_zorder=1, edge_color='#999999', edge_linewidth=1,\n                     edge_alpha=1, use_geom=True, origin_point=None,\n                     destination_point=None, route_color='r', route_linewidth=4,\n                     route_alpha=0.5, orig_dest_node_alpha=0.5,\n                     orig_dest_node_size=100, orig_dest_node_color='r',\n                     orig_dest_point_color='b'):\n    \"\"\"\n    Plot a route along a networkx spatial graph.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n    route : list\n        the route as a list of nodes\n    bbox : tuple\n        bounding box as north,south,east,west - if None will calculate from\n        spatial extents of data. if passing a bbox, you probably also want to\n        pass margin=0 to constrain it.\n    fig_height : int\n        matplotlib figure height in inches\n    fig_width : int\n        matplotlib figure width in inches\n    margin : float\n        relative margin around the figure\n    axis_off : bool\n        if True turn off the matplotlib axis\n    bgcolor : string\n        the background color of the figure and axis\n    show : bool\n        if True, show the figure\n    save : bool\n        if True, save the figure as an image file to disk\n    close : bool\n        close the figure (only if show equals False) to prevent display\n    file_format : string\n        the format of the file to save (e.g., 'jpg', 'png', 'svg')\n    filename : string\n        the name of the file if saving\n    dpi : int\n        the resolution of the image file if saving\n    annotate : bool\n        if True, annotate the nodes in the figure\n    node_color : string\n        the color of the nodes\n    node_size : int\n        the size of the nodes\n    node_alpha : float\n        the opacity of the nodes\n    node_edgecolor : string\n        the color of the node's marker's border\n    node_zorder : int\n        zorder to plot nodes, edges are always 2, so make node_zorder 1 to plot\n        nodes beneath them or 3 to plot nodes atop them\n    edge_color : string\n        the color of the edges' lines\n    edge_linewidth : float\n        the width of the edges' lines\n    edge_alpha : float\n        the opacity of the edges' lines\n    use_geom : bool\n        if True, use the spatial geometry attribute of the edges to draw\n        geographically accurate edges, rather than just lines straight from node\n        to node\n    origin_point : tuple\n        optional, an origin (lat, lon) point to plot instead of the origin node\n    destination_point : tuple\n        optional, a destination (lat, lon) point to plot instead of the\n        destination node\n    route_color : string\n        the color of the route\n    route_linewidth : int\n        the width of the route line\n    route_alpha : float\n        the opacity of the route line\n    orig_dest_node_alpha : float\n        the opacity of the origin and destination nodes\n    orig_dest_node_size : int\n        the size of the origin and destination nodes\n    orig_dest_node_color : string\n        the color of the origin and destination nodes\n    orig_dest_point_color : string\n        the color of the origin and destination points if being plotted instead\n        of nodes\n\n    Returns\n    -------\n    fig, ax : tuple\n    \"\"\"\n\n    # plot the graph but not the route\n    fig, ax = plot_graph(G, bbox=bbox, fig_height=fig_height, fig_width=fig_width,\n                         margin=margin, axis_off=axis_off, bgcolor=bgcolor,\n                         show=False, save=False, close=False, filename=filename,\n                         dpi=dpi, annotate=annotate, node_color=node_color,\n                         node_size=node_size, node_alpha=node_alpha,\n                         node_edgecolor=node_edgecolor, node_zorder=node_zorder,\n                         edge_color=edge_color, edge_linewidth=edge_linewidth,\n                         edge_alpha=edge_alpha, use_geom=use_geom)\n\n    # the origin and destination nodes are the first and last nodes in the route\n    origin_node = route[0]\n    destination_node = route[-1]\n\n    if origin_point is None or destination_point is None:\n        # if caller didn't pass points, use the first and last node in route as\n        # origin/destination\n        origin_destination_lats = (G.nodes[origin_node]['y'], G.nodes[destination_node]['y'])\n        origin_destination_lons = (G.nodes[origin_node]['x'], G.nodes[destination_node]['x'])\n    else:\n        # otherwise, use the passed points as origin/destination\n        origin_destination_lats = (origin_point[0], destination_point[0])\n        origin_destination_lons = (origin_point[1], destination_point[1])\n        orig_dest_node_color = orig_dest_point_color\n\n    # scatter the origin and destination points\n    ax.scatter(origin_destination_lons, origin_destination_lats, s=orig_dest_node_size,\n               c=orig_dest_node_color, alpha=orig_dest_node_alpha, edgecolor=node_edgecolor, zorder=4)\n\n    # plot the route lines\n    lines = node_list_to_coordinate_lines(G, route, use_geom)\n\n    # add the lines to the axis as a linecollection\n    lc = LineCollection(lines, colors=route_color, linewidths=route_linewidth, alpha=route_alpha, zorder=3)\n    ax.add_collection(lc)\n\n    # save and show the figure as specified\n    fig, ax = save_and_show(fig, ax, save, show, close, filename, file_format, dpi, axis_off)\n    return fig, ax"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_folium_polyline(edge, edge_color, edge_width, edge_opacity, popup_attribute=None):\n\n    \"\"\"\n    Turn a row from the gdf_edges GeoDataFrame into a folium PolyLine with\n    attributes.\n\n    Parameters\n    ----------\n    edge : GeoSeries\n        a row from the gdf_edges GeoDataFrame\n    edge_color : string\n        color of the edge lines\n    edge_width : numeric\n        width of the edge lines\n    edge_opacity : numeric\n        opacity of the edge lines\n    popup_attribute : string\n        edge attribute to display in a pop-up when an edge is clicked, if None,\n        no popup\n\n    Returns\n    -------\n    pl : folium.PolyLine\n    \"\"\"\n\n    # check if we were able to import folium successfully\n    if not folium:\n        raise ImportError('The folium package must be installed to use this optional feature.')\n\n    # locations is a list of points for the polyline\n    # folium takes coords in lat,lon but geopandas provides them in lon,lat\n    # so we have to flip them around\n    locations = list([(lat, lon) for lon, lat in edge['geometry'].coords])\n\n    # if popup_attribute is None, then create no pop-up\n    if popup_attribute is None:\n        popup = None\n    else:\n        # folium doesn't interpret html in the html argument (weird), so can't\n        # do newlines without an iframe\n        popup_text = json.dumps(edge[popup_attribute])\n        popup = folium.Popup(html=popup_text)\n\n    # create a folium polyline with attributes\n    pl = folium.PolyLine(locations=locations, popup=popup,\n                         color=edge_color, weight=edge_width, opacity=edge_opacity)\n    return pl", "response": "Turn a row from the gdf_edges GeoDataFrame into a folium PolyLine with optional attributes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot_graph_folium(G, graph_map=None, popup_attribute=None,\n                      tiles='cartodbpositron', zoom=1, fit_bounds=True,\n                      edge_color='#333333', edge_width=5, edge_opacity=1):\n    \"\"\"\n    Plot a graph on an interactive folium web map.\n\n    Note that anything larger than a small city can take a long time to plot and\n    create a large web map file that is very slow to load as JavaScript.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n    graph_map : folium.folium.Map\n        if not None, plot the graph on this preexisting folium map object\n    popup_attribute : string\n        edge attribute to display in a pop-up when an edge is clicked\n    tiles : string\n        name of a folium tileset\n    zoom : int\n        initial zoom level for the map\n    fit_bounds : bool\n        if True, fit the map to the boundaries of the route's edges\n    edge_color : string\n        color of the edge lines\n    edge_width : numeric\n        width of the edge lines\n    edge_opacity : numeric\n        opacity of the edge lines\n\n    Returns\n    -------\n    graph_map : folium.folium.Map\n    \"\"\"\n\n    # check if we were able to import folium successfully\n    if not folium:\n        raise ImportError('The folium package must be installed to use this optional feature.')\n\n    # create gdf of the graph edges\n    gdf_edges = graph_to_gdfs(G, nodes=False, fill_edge_geometry=True)\n\n    # get graph centroid\n    x, y = gdf_edges.unary_union.centroid.xy\n    graph_centroid = (y[0], x[0])\n\n    # create the folium web map if one wasn't passed-in\n    if graph_map is None:\n        graph_map = folium.Map(location=graph_centroid, zoom_start=zoom, tiles=tiles)\n\n    # add each graph edge to the map\n    for _, row in gdf_edges.iterrows():\n        pl = make_folium_polyline(edge=row, edge_color=edge_color, edge_width=edge_width,\n                                  edge_opacity=edge_opacity, popup_attribute=popup_attribute)\n        pl.add_to(graph_map)\n\n    # if fit_bounds is True, fit the map to the bounds of the route by passing\n    # list of lat-lng points as [southwest, northeast]\n    if fit_bounds:\n        tb = gdf_edges.total_bounds\n        bounds = [(tb[1], tb[0]), (tb[3], tb[2])]\n        graph_map.fit_bounds(bounds)\n\n    return graph_map", "response": "Plot a graph on an interactive folium web map."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplots a route on an interactive folium web map.", "response": "def plot_route_folium(G, route, route_map=None, popup_attribute=None,\n                      tiles='cartodbpositron', zoom=1, fit_bounds=True,\n                      route_color='#cc0000', route_width=5, route_opacity=1):\n    \"\"\"\n    Plot a route on an interactive folium web map.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n    route : list\n        the route as a list of nodes\n    route_map : folium.folium.Map\n        if not None, plot the route on this preexisting folium map object\n    popup_attribute : string\n        edge attribute to display in a pop-up when an edge is clicked\n    tiles : string\n        name of a folium tileset\n    zoom : int\n        initial zoom level for the map\n    fit_bounds : bool\n        if True, fit the map to the boundaries of the route's edges\n    route_color : string\n        color of the route's line\n    route_width : numeric\n        width of the route's line\n    route_opacity : numeric\n        opacity of the route lines\n\n    Returns\n    -------\n    route_map : folium.folium.Map\n    \"\"\"\n\n    # check if we were able to import folium successfully\n    if not folium:\n        raise ImportError('The folium package must be installed to use this optional feature.')\n\n    # create gdf of the route edges\n    gdf_edges = graph_to_gdfs(G, nodes=False, fill_edge_geometry=True)\n    route_nodes = list(zip(route[:-1], route[1:]))\n    index = [gdf_edges[(gdf_edges['u']==u) & (gdf_edges['v']==v)].index[0] for u, v in route_nodes]\n    gdf_route_edges = gdf_edges.loc[index]\n\n    # get route centroid\n    x, y = gdf_route_edges.unary_union.centroid.xy\n    route_centroid = (y[0], x[0])\n\n    # create the folium web map if one wasn't passed-in\n    if route_map is None:\n        route_map = folium.Map(location=route_centroid, zoom_start=zoom, tiles=tiles)\n\n    # add each route edge to the map\n    for _, row in gdf_route_edges.iterrows():\n        pl = make_folium_polyline(edge=row, edge_color=route_color, edge_width=route_width,\n                                  edge_opacity=route_opacity, popup_attribute=popup_attribute)\n        pl.add_to(route_map)\n\n    # if fit_bounds is True, fit the map to the bounds of the route by passing\n    # list of lat-lng points as [southwest, northeast]\n    if fit_bounds:\n        tb = gdf_route_edges.total_bounds\n        bounds = [(tb[1], tb[0]), (tb[3], tb[2])]\n        route_map.fit_bounds(bounds)\n\n    return route_map"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplot a figure - ground diagram of a street network.", "response": "def plot_figure_ground(G=None, address=None, point=None, dist=805,\n                       network_type='drive_service', street_widths=None,\n                       default_width=4, fig_length=8, edge_color='w',\n                       bgcolor='#333333', smooth_joints=True, filename=None,\n                       file_format='png', show=False, save=True, close=True,\n                       dpi=300):\n    \"\"\"\n    Plot a figure-ground diagram of a street network, defaulting to one square\n    mile.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n    address : string\n        the address to geocode as the center point if G is not passed in\n    point : tuple\n        the center point if address and G are not passed in\n    dist : numeric\n        how many meters to extend north, south, east, and west from the center\n        point\n    network_type : string\n        what type of network to get\n    street_widths : dict\n        where keys are street types and values are widths to plot in pixels\n    default_width : numeric\n        the default street width in pixels for any street type not found in\n        street_widths dict\n    fig_length : numeric\n        the height and width of this square diagram\n    edge_color : string\n        the color of the streets\n    bgcolor : string\n        the color of the background\n    smooth_joints : bool\n        if True, plot nodes same width as streets to smooth line joints and\n        prevent cracks between them from showing\n    filename : string\n        filename to save the image as\n    file_format : string\n        the format of the file to save (e.g., 'jpg', 'png', 'svg')\n    show : bool\n        if True, show the figure\n    save : bool\n        if True, save the figure as an image file to disk\n    close : bool\n        close the figure (only if show equals False) to prevent display\n    dpi : int\n        the resolution of the image file if saving\n\n    Returns\n    -------\n    fig, ax : tuple\n    \"\"\"\n\n    multiplier = 1.2\n\n    # if G was passed-in, use this graph in the plot, centered on the centroid\n    # of its nodes\n    if G is not None:\n        gdf_nodes = graph_to_gdfs(G, edges=False, node_geometry=True)\n        lnglat_point = gdf_nodes.unary_union.centroid.coords[0]\n        point = tuple(reversed(lnglat_point))\n\n    # otherwise, get the network by either address or point, whichever was\n    # passed-in, using a distance multiplier to make sure we get more than\n    # enough network. simplify in non-strict mode to not combine multiple street\n    # types into single edge\n    elif address is not None:\n        G, point = graph_from_address(address, distance=dist*multiplier, distance_type='bbox', network_type=network_type,\n                                      simplify=False, truncate_by_edge=True, return_coords=True)\n        G = simplify_graph(G, strict=False)\n    elif point is not None:\n        G = graph_from_point(point, distance=dist*multiplier, distance_type='bbox', network_type=network_type,\n                             simplify=False, truncate_by_edge=True)\n        G = simplify_graph(G, strict=False)\n    else:\n        raise ValueError('You must pass an address or lat-long point or graph.')\n\n    # if user did not pass in custom street widths, create a dict of default\n    # values\n    if street_widths is None:\n        street_widths = {'footway' : 1.5,\n                         'steps' : 1.5,\n                         'pedestrian' : 1.5,\n                         'service' : 1.5,\n                         'path' : 1.5,\n                         'track' : 1.5,\n                         'motorway' : 6}\n\n    # we need an undirected graph to find every edge incident to a node\n    G_undir = G.to_undirected()\n\n    # for each network edge, get a linewidth according to street type (the OSM\n    # 'highway' value)\n    edge_linewidths = []\n    for _, _, data in G_undir.edges(keys=False, data=True):\n        street_type = data['highway'][0] if isinstance(data['highway'], list) else data['highway']\n        if street_type in street_widths:\n            edge_linewidths.append(street_widths[street_type])\n        else:\n            edge_linewidths.append(default_width)\n\n    if smooth_joints:\n        # for each node, get a nodesize according to the narrowest incident edge\n        node_widths = {}\n        for node in G_undir.nodes():\n            # first, identify all the highway types of this node's incident edges\n            incident_edges_data = [G_undir.get_edge_data(node, neighbor) for neighbor in G_undir.neighbors(node)]\n            edge_types = [data[0]['highway'] for data in incident_edges_data]\n            if len(edge_types) < 1:\n                # if node has no incident edges, make size zero\n                node_widths[node] = 0\n            else:\n                # flatten the list of edge types\n                edge_types_flat = []\n                for et in edge_types:\n                    if isinstance(et, list):\n                        edge_types_flat.extend(et)\n                    else:\n                        edge_types_flat.append(et)\n\n                # for each edge type in the flattened list, lookup the\n                # corresponding width\n                edge_widths = [street_widths[edge_type] if edge_type in street_widths else default_width for edge_type in edge_types_flat]\n\n                # the node diameter will be the biggest of the edge widths, to make joints perfectly smooth\n                # alternatively, use min (?) to pervent anything larger from extending past smallest street's line\n                circle_diameter = max(edge_widths)\n\n                # mpl circle marker sizes are in area, so it is the diameter\n                # squared\n                circle_area = circle_diameter ** 2\n                node_widths[node] = circle_area\n\n        # assign the node size to each node in the graph\n        node_sizes = [node_widths[node] for node in G_undir.nodes()]\n    else:\n        node_sizes = 0\n\n    # define the spatial extents of the plotting figure to make it square, in\n    # projected units, and cropped to the desired area\n    bbox = bbox_from_point(point, dist, project_utm=False)\n\n    # create a filename if one was not passed\n    if filename is None and save:\n        filename = 'figure_ground_{}_{}'.format(point, network_type)\n\n    # plot the figure\n    fig, ax = plot_graph(G_undir, bbox=bbox, fig_height=fig_length,\n                         margin=0, axis_off=True, equal_aspect=False,\n                         bgcolor=bgcolor, node_size=node_sizes,\n                         node_color=edge_color, edge_linewidth=edge_linewidths,\n                         edge_color=edge_color, show=show, save=save,\n                         close=close, filename=filename, file_format=file_format,\n                         dpi=dpi)\n\n    return fig, ax"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving a response json object to the cache.", "response": "def save_to_cache(url, response_json):\n    \"\"\"\n    Save an HTTP response json object to the cache.\n\n    If the request was sent to server via POST instead of GET, then URL should\n    be a GET-style representation of request. Users should always pass\n    OrderedDicts instead of dicts of parameters into request functions, so that\n    the parameters stay in the same order each time, producing the same URL\n    string, and thus the same hash. Otherwise the cache will eventually contain\n    multiple saved responses for the same request because the URL's parameters\n    appeared in a different order each time.\n\n    Parameters\n    ----------\n    url : string\n        the url of the request\n    response_json : dict\n        the json response\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if settings.use_cache:\n        if response_json is None:\n            log('Saved nothing to cache because response_json is None')\n        else:\n            # create the folder on the disk if it doesn't already exist\n            if not os.path.exists(settings.cache_folder):\n                os.makedirs(settings.cache_folder)\n\n            # hash the url (to make filename shorter than the often extremely\n            # long url)\n            filename = hashlib.md5(url.encode('utf-8')).hexdigest()\n            cache_path_filename = os.path.join(settings.cache_folder, os.extsep.join([filename, 'json']))\n\n            # dump to json, and save to file\n            json_str = make_str(json.dumps(response_json))\n            with io.open(cache_path_filename, 'w', encoding='utf-8') as cache_file:\n                cache_file.write(json_str)\n\n            log('Saved response to cache file \"{}\"'.format(cache_path_filename))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_from_cache(url):\n    # if the tool is configured to use the cache\n    if settings.use_cache:\n        # determine the filename by hashing the url\n        filename = hashlib.md5(url.encode('utf-8')).hexdigest()\n\n        cache_path_filename = os.path.join(settings.cache_folder, os.extsep.join([filename, 'json']))\n        # open the cache file for this url hash if it already exists, otherwise\n        # return None\n        if os.path.isfile(cache_path_filename):\n            with io.open(cache_path_filename, encoding='utf-8') as cache_file:\n                response_json = json.load(cache_file)\n            log('Retrieved response from cache file \"{}\" for URL \"{}\"'.format(cache_path_filename, url))\n            return response_json", "response": "Retrieve a HTTP response json object from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_http_headers(user_agent=None, referer=None, accept_language=None):\n\n    if user_agent is None:\n        user_agent = settings.default_user_agent\n    if referer is None:\n        referer = settings.default_referer\n    if accept_language is None:\n        accept_language = settings.default_accept_language\n\n    headers = requests.utils.default_headers()\n    headers.update({'User-Agent': user_agent, 'referer': referer, 'Accept-Language': accept_language})\n    return headers", "response": "Returns the HTTP headers for the current OSMnx server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef nominatim_request(params, type = \"search\", pause_duration=1, timeout=30, error_pause_duration=180):\n\n    known_requests = {\"search\", \"reverse\", \"lookup\"}\n    if type not in known_requests:\n        raise ValueError(\"The type of Nominatim request is invalid. Please choose one of {{'search', 'reverse', 'lookup'}}\")\n\n    # prepare the Nominatim API URL and see if request already exists in the\n    # cache\n    url = 'https://nominatim.openstreetmap.org/{}'.format(type)\n    prepared_url = requests.Request('GET', url, params=params).prepare().url\n    cached_response_json = get_from_cache(prepared_url)\n\n    if cached_response_json is not None:\n        # found this request in the cache, just return it instead of making a\n        # new HTTP call\n        return cached_response_json\n\n    else:\n        # if this URL is not already in the cache, pause, then request it\n        log('Pausing {:,.2f} seconds before making API GET request'.format(pause_duration))\n        time.sleep(pause_duration)\n        start_time = time.time()\n        log('Requesting {} with timeout={}'.format(prepared_url, timeout))\n        response = requests.get(url, params=params, timeout=timeout, headers=get_http_headers())\n\n        # get the response size and the domain, log result\n        size_kb = len(response.content) / 1000.\n        domain = re.findall(r'//(?s)(.*?)/', url)[0]\n        log('Downloaded {:,.1f}KB from {} in {:,.2f} seconds'.format(size_kb, domain, time.time()-start_time))\n\n        try:\n            response_json = response.json()\n            save_to_cache(prepared_url, response_json)\n        except Exception:\n            #429 is 'too many requests' and 504 is 'gateway timeout' from server\n            # overload - handle these errors by recursively calling\n            # nominatim_request until we get a valid response\n            if response.status_code in [429, 504]:\n                # pause for error_pause_duration seconds before re-trying request\n                log('Server at {} returned status code {} and no JSON data. Re-trying request in {:.2f} seconds.'.format(domain,\n                                                                                                                         response.status_code,\n                                                                                                                         error_pause_duration),\n                                                                                                                         level=lg.WARNING)\n                time.sleep(error_pause_duration)\n                response_json = nominatim_request(params=params, pause_duration=pause_duration, timeout=timeout)\n\n            # else, this was an unhandled status_code, throw an exception\n            else:\n                log('Server at {} returned status code {} and no JSON data'.format(domain, response.status_code), level=lg.ERROR)\n                raise Exception('Server returned no JSON data.\\n{} {}\\n{}'.format(response, response.reason, response.text))\n\n        return response_json", "response": "Send a request to the Nominatim API and return the JSON response."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a request to the Overpass API and return the JSON response.", "response": "def overpass_request(data, pause_duration=None, timeout=180, error_pause_duration=None):\n    \"\"\"\n    Send a request to the Overpass API via HTTP POST and return the JSON\n    response.\n\n    Parameters\n    ----------\n    data : dict or OrderedDict\n        key-value pairs of parameters to post to the API\n    pause_duration : int\n        how long to pause in seconds before requests, if None, will query API\n        status endpoint to find when next slot is available\n    timeout : int\n        the timeout interval for the requests library\n    error_pause_duration : int\n        how long to pause in seconds before re-trying requests if error\n\n    Returns\n    -------\n    dict\n    \"\"\"\n\n    # define the Overpass API URL, then construct a GET-style URL as a string to\n    # hash to look up/save to cache\n    url = 'http://overpass-api.de/api/interpreter'\n    prepared_url = requests.Request('GET', url, params=data).prepare().url\n    cached_response_json = get_from_cache(prepared_url)\n\n    if cached_response_json is not None:\n        # found this request in the cache, just return it instead of making a\n        # new HTTP call\n        return cached_response_json\n\n    else:\n        # if this URL is not already in the cache, pause, then request it\n        if pause_duration is None:\n            this_pause_duration = get_pause_duration()\n        log('Pausing {:,.2f} seconds before making API POST request'.format(this_pause_duration))\n        time.sleep(this_pause_duration)\n        start_time = time.time()\n        log('Posting to {} with timeout={}, \"{}\"'.format(url, timeout, data))\n        response = requests.post(url, data=data, timeout=timeout, headers=get_http_headers())\n\n        # get the response size and the domain, log result\n        size_kb = len(response.content) / 1000.\n        domain = re.findall(r'//(?s)(.*?)/', url)[0]\n        log('Downloaded {:,.1f}KB from {} in {:,.2f} seconds'.format(size_kb, domain, time.time()-start_time))\n\n        try:\n            response_json = response.json()\n            if 'remark' in response_json:\n                log('Server remark: \"{}\"'.format(response_json['remark'], level=lg.WARNING))\n            save_to_cache(prepared_url, response_json)\n        except Exception:\n            #429 is 'too many requests' and 504 is 'gateway timeout' from server\n            # overload - handle these errors by recursively calling\n            # overpass_request until we get a valid response\n            if response.status_code in [429, 504]:\n                # pause for error_pause_duration seconds before re-trying request\n                if error_pause_duration is None:\n                    error_pause_duration = get_pause_duration()\n                log('Server at {} returned status code {} and no JSON data. Re-trying request in {:.2f} seconds.'.format(domain,\n                                                                                                                         response.status_code,\n                                                                                                                         error_pause_duration),\n                                                                                                                         level=lg.WARNING)\n                time.sleep(error_pause_duration)\n                response_json = overpass_request(data=data, pause_duration=pause_duration, timeout=timeout)\n\n            # else, this was an unhandled status_code, throw an exception\n            else:\n                log('Server at {} returned status code {} and no JSON data'.format(domain, response.status_code), level=lg.ERROR)\n                raise Exception('Server returned no JSON data.\\n{} {}\\n{}'.format(response, response.reason, response.text))\n\n        return response_json"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a GeoDataFrame from a single place name query.", "response": "def gdf_from_place(query, gdf_name=None, which_result=1, buffer_dist=None):\n    \"\"\"\n    Create a GeoDataFrame from a single place name query.\n\n    Parameters\n    ----------\n    query : string or dict\n        query string or structured query dict to geocode/download\n    gdf_name : string\n        name attribute metadata for GeoDataFrame (this is used to save shapefile\n        later)\n    which_result : int\n        max number of results to return and which to process upon receipt\n    buffer_dist : float\n        distance to buffer around the place geometry, in meters\n\n    Returns\n    -------\n    GeoDataFrame\n    \"\"\"\n    # if no gdf_name is passed, just use the query\n    assert (isinstance(query, dict) or isinstance(query, str)), 'query must be a dict or a string'\n    if (gdf_name is None) and isinstance(query, dict):\n        gdf_name = ', '.join(list(query.values()))\n    elif (gdf_name is None) and isinstance(query, str):\n        gdf_name = query\n\n    # get the data from OSM\n    data = osm_polygon_download(query, limit=which_result)\n    if len(data) >= which_result:\n\n        # extract data elements from the JSON response\n        result = data[which_result - 1]\n        bbox_south, bbox_north, bbox_west, bbox_east = [float(x) for x in result['boundingbox']]\n        geometry = result['geojson']\n        place = result['display_name']\n        features = [{'type': 'Feature',\n                     'geometry': geometry,\n                     'properties': {'place_name': place,\n                                    'bbox_north': bbox_north,\n                                    'bbox_south': bbox_south,\n                                    'bbox_east': bbox_east,\n                                    'bbox_west': bbox_west}}]\n\n        # if we got an unexpected geometry type (like a point), log a warning\n        if geometry['type'] not in ['Polygon', 'MultiPolygon']:\n            log('OSM returned a {} as the geometry.'.format(geometry['type']), level=lg.WARNING)\n\n        # create the GeoDataFrame, name it, and set its original CRS to default_crs\n        gdf = gpd.GeoDataFrame.from_features(features)\n        gdf.gdf_name = gdf_name\n        gdf.crs = settings.default_crs\n\n        # if buffer_dist was passed in, project the geometry to UTM, buffer it\n        # in meters, then project it back to lat-long\n        if buffer_dist is not None:\n            gdf_utm = project_gdf(gdf)\n            gdf_utm['geometry'] = gdf_utm['geometry'].buffer(buffer_dist)\n            gdf = project_gdf(gdf_utm, to_latlong=True)\n            log('Buffered the GeoDataFrame \"{}\" to {} meters'.format(gdf.gdf_name, buffer_dist))\n\n        # return the gdf\n        log('Created GeoDataFrame with {} row for query \"{}\"'.format(len(gdf), query))\n        return gdf\n    else:\n        # if there was no data returned (or fewer results than which_result\n        # specified)\n        log('OSM returned no results (or fewer than which_result) for query \"{}\"'.format(query), level=lg.WARNING)\n        gdf = gpd.GeoDataFrame()\n        gdf.gdf_name = gdf_name\n        return gdf"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gdf_from_places(queries, gdf_name='unnamed', buffer_dist=None):\n    # create an empty GeoDataFrame then append each result as a new row\n    gdf = gpd.GeoDataFrame()\n    for query in queries:\n        gdf = gdf.append(gdf_from_place(query, buffer_dist=buffer_dist))\n\n    # reset the index, name the GeoDataFrame\n    gdf = gdf.reset_index().drop(labels='index', axis=1)\n    gdf.gdf_name = gdf_name\n\n    # set the original CRS of the GeoDataFrame to default_crs, and return it\n    gdf.crs = settings.default_crs\n    log('Finished creating GeoDataFrame with {} rows from {} queries'.format(len(gdf), len(queries)))\n    return gdf", "response": "Create a GeoDataFrame from a list of query strings or structured query dicts to geocode and download them."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_osm_filter(network_type):\n    filters = {}\n\n    # driving: filter out un-drivable roads, service roads, private ways, and\n    # anything specifying motor=no. also filter out any non-service roads that\n    # are tagged as providing parking, driveway, private, or emergency-access\n    # services\n    filters['drive'] = ('[\"area\"!~\"yes\"][\"highway\"!~\"cycleway|footway|path|pedestrian|steps|track|corridor|'\n                        'proposed|construction|bridleway|abandoned|platform|raceway|service\"]'\n                        '[\"motor_vehicle\"!~\"no\"][\"motorcar\"!~\"no\"]{}'\n                        '[\"service\"!~\"parking|parking_aisle|driveway|private|emergency_access\"]').format(settings.default_access)\n\n    # drive+service: allow ways tagged 'service' but filter out certain types of\n    # service ways\n    filters['drive_service'] = ('[\"area\"!~\"yes\"][\"highway\"!~\"cycleway|footway|path|pedestrian|steps|track|corridor|'\n                                'proposed|construction|bridleway|abandoned|platform|raceway\"]'\n                                '[\"motor_vehicle\"!~\"no\"][\"motorcar\"!~\"no\"]{}'\n                                '[\"service\"!~\"parking|parking_aisle|private|emergency_access\"]').format(settings.default_access)\n\n    # walking: filter out cycle ways, motor ways, private ways, and anything\n    # specifying foot=no. allow service roads, permitting things like parking\n    # lot lanes, alleys, etc that you *can* walk on even if they're not exactly\n    # pleasant walks. some cycleways may allow pedestrians, but this filter ignores\n    # such cycleways.\n    filters['walk'] = ('[\"area\"!~\"yes\"][\"highway\"!~\"cycleway|motor|proposed|construction|abandoned|platform|raceway\"]'\n                       '[\"foot\"!~\"no\"][\"service\"!~\"private\"]{}').format(settings.default_access)\n\n    # biking: filter out foot ways, motor ways, private ways, and anything\n    # specifying biking=no\n    filters['bike'] = ('[\"area\"!~\"yes\"][\"highway\"!~\"footway|steps|corridor|motor|proposed|construction|abandoned|platform|raceway\"]'\n                       '[\"bicycle\"!~\"no\"][\"service\"!~\"private\"]{}').format(settings.default_access)\n\n    # to download all ways, just filter out everything not currently in use or\n    # that is private-access only\n    filters['all'] = ('[\"area\"!~\"yes\"][\"highway\"!~\"proposed|construction|abandoned|platform|raceway\"]'\n                      '[\"service\"!~\"private\"]{}').format(settings.default_access)\n\n    # to download all ways, including private-access ones, just filter out\n    # everything not currently in use\n    filters['all_private'] = '[\"area\"!~\"yes\"][\"highway\"!~\"proposed|construction|abandoned|platform|raceway\"]'\n\n    # no filter, needed for infrastructures other than \"highway\"\n    filters['none'] = ''\n\n    if network_type in filters:\n        osm_filter = filters[network_type]\n    else:\n        raise UnknownNetworkType('unknown network_type \"{}\"'.format(network_type))\n\n    return osm_filter", "response": "Returns a filter to query OSM for the specified network type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef osm_net_download(polygon=None, north=None, south=None, east=None, west=None,\n                     network_type='all_private', timeout=180, memory=None,\n                     max_query_area_size=50*1000*50*1000, infrastructure='way[\"highway\"]',\n                     custom_filter=None):\n    \"\"\"\n    Download OSM ways and nodes within some bounding box from the Overpass API.\n\n    Parameters\n    ----------\n    polygon : shapely Polygon or MultiPolygon\n        geographic shape to fetch the street network within\n    north : float\n        northern latitude of bounding box\n    south : float\n        southern latitude of bounding box\n    east : float\n        eastern longitude of bounding box\n    west : float\n        western longitude of bounding box\n    network_type : string\n        {'walk', 'bike', 'drive', 'drive_service', 'all', 'all_private'} what\n        type of street network to get\n    timeout : int\n        the timeout interval for requests and to pass to API\n    memory : int\n        server memory allocation size for the query, in bytes. If none, server\n        will use its default allocation size\n    max_query_area_size : float\n        max area for any part of the geometry, in the units the geometry is in:\n        any polygon bigger will get divided up for multiple queries to API\n        (default is 50,000 * 50,000 units [ie, 50km x 50km in area, if units are\n        meters])\n    infrastructure : string\n        download infrastructure of given type. default is streets, ie,\n        'way[\"highway\"]') but other infrastructures may be selected like power\n        grids, ie, 'way[\"power\"~\"line\"]'\n    custom_filter : string\n        a custom network filter to be used instead of the network_type presets\n\n    Returns\n    -------\n    response_jsons : list\n    \"\"\"\n\n    # check if we're querying by polygon or by bounding box based on which\n    # argument(s) where passed into this function\n    by_poly = polygon is not None\n    by_bbox = not (north is None or south is None or east is None or west is None)\n    if not (by_poly or by_bbox):\n        raise InsufficientNetworkQueryArguments(\n            'You must pass a polygon or north, south, east, and west')\n\n    # create a filter to exclude certain kinds of ways based on the requested\n    # network_type\n    if custom_filter:\n        osm_filter = custom_filter\n    else:\n        osm_filter = get_osm_filter(network_type)\n    response_jsons = []\n\n    # pass server memory allocation in bytes for the query to the API\n    # if None, pass nothing so the server will use its default allocation size\n    # otherwise, define the query's maxsize parameter value as whatever the\n    # caller passed in\n    if memory is None:\n        maxsize = ''\n    else:\n        maxsize = '[maxsize:{}]'.format(memory)\n\n    # define the query to send the API\n    # specifying way[\"highway\"] means that all ways returned must have a highway\n    # key. the {filters} then remove ways by key/value. the '>' makes it recurse\n    # so we get ways and way nodes. maxsize is in bytes.\n    if by_bbox:\n        # turn bbox into a polygon and project to local UTM\n        polygon = Polygon([(west, south), (east, south), (east, north), (west, north)])\n        geometry_proj, crs_proj = project_geometry(polygon)\n\n        # subdivide it if it exceeds the max area size (in meters), then project\n        # back to lat-long\n        geometry_proj_consolidated_subdivided = consolidate_subdivide_geometry(geometry_proj, max_query_area_size=max_query_area_size)\n        geometry, _ = project_geometry(geometry_proj_consolidated_subdivided, crs=crs_proj, to_latlong=True)\n        log('Requesting network data within bounding box from API in {:,} request(s)'.format(len(geometry)))\n        start_time = time.time()\n\n        # loop through each polygon rectangle in the geometry (there will only\n        # be one if original bbox didn't exceed max area size)\n        for poly in geometry:\n            # represent bbox as south,west,north,east and round lat-longs to 6\n            # decimal places (ie, ~100 mm) so URL strings aren't different\n            # due to float rounding issues (for consistent caching)\n            west, south, east, north = poly.bounds\n            query_template = '[out:json][timeout:{timeout}]{maxsize};({infrastructure}{filters}({south:.6f},{west:.6f},{north:.6f},{east:.6f});>;);out;'\n            query_str = query_template.format(north=north, south=south,\n                                              east=east, west=west,\n                                              infrastructure=infrastructure,\n                                              filters=osm_filter,\n                                              timeout=timeout, maxsize=maxsize)\n            response_json = overpass_request(data={'data':query_str}, timeout=timeout)\n            response_jsons.append(response_json)\n        log('Got all network data within bounding box from API in {:,} request(s) and {:,.2f} seconds'.format(len(geometry), time.time()-start_time))\n\n    elif by_poly:\n        # project to utm, divide polygon up into sub-polygons if area exceeds a\n        # max size (in meters), project back to lat-long, then get a list of\n        # polygon(s) exterior coordinates\n        geometry_proj, crs_proj = project_geometry(polygon)\n        geometry_proj_consolidated_subdivided = consolidate_subdivide_geometry(geometry_proj, max_query_area_size=max_query_area_size)\n        geometry, _ = project_geometry(geometry_proj_consolidated_subdivided, crs=crs_proj, to_latlong=True)\n        polygon_coord_strs = get_polygons_coordinates(geometry)\n        log('Requesting network data within polygon from API in {:,} request(s)'.format(len(polygon_coord_strs)))\n        start_time = time.time()\n\n        # pass each polygon exterior coordinates in the list to the API, one at\n        # a time\n        for polygon_coord_str in polygon_coord_strs:\n            query_template = '[out:json][timeout:{timeout}]{maxsize};({infrastructure}{filters}(poly:\"{polygon}\");>;);out;'\n            query_str = query_template.format(polygon=polygon_coord_str, infrastructure=infrastructure, filters=osm_filter, timeout=timeout, maxsize=maxsize)\n            response_json = overpass_request(data={'data':query_str}, timeout=timeout)\n            response_jsons.append(response_json)\n        log('Got all network data within polygon from API in {:,} request(s) and {:,.2f} seconds'.format(len(polygon_coord_strs), time.time()-start_time))\n\n    return response_jsons", "response": "Download OSM ways and nodes within some bounding box from Overpass API."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts exterior coordinates from polygon or multiPolygons and return a list of strings", "response": "def get_polygons_coordinates(geometry):\n    \"\"\"\n    Extract exterior coordinates from polygon(s) to pass to OSM in a query by\n    polygon. Ignore the interior (\"holes\") coordinates.\n\n    Parameters\n    ----------\n    geometry : shapely Polygon or MultiPolygon\n        the geometry to extract exterior coordinates from\n\n    Returns\n    -------\n    polygon_coord_strs : list\n    \"\"\"\n\n    # extract the exterior coordinates of the geometry to pass to the API later\n    polygons_coords = []\n    if isinstance(geometry, Polygon):\n        x, y = geometry.exterior.xy\n        polygons_coords.append(list(zip(x, y)))\n    elif isinstance(geometry, MultiPolygon):\n        for polygon in geometry:\n            x, y = polygon.exterior.xy\n            polygons_coords.append(list(zip(x, y)))\n    else:\n        raise TypeError('Geometry must be a shapely Polygon or MultiPolygon')\n\n    # convert the exterior coordinates of the polygon(s) to the string format\n    # the API expects\n    polygon_coord_strs = []\n    for coords in polygons_coords:\n        s = ''\n        separator = ' '\n        for coord in list(coords):\n            # round floating point lats and longs to 6 decimal places (ie, ~100 mm),\n            # so we can hash and cache strings consistently\n            s = '{}{}{:.6f}{}{:.6f}'.format(s, separator, coord[1], separator, coord[0])\n        polygon_coord_strs.append(s.strip(separator))\n\n    return polygon_coord_strs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_node(element):\n\n    node = {}\n    node['y'] = element['lat']\n    node['x'] = element['lon']\n    node['osmid'] = element['id']\n    if 'tags' in element:\n        for useful_tag in settings.useful_tags_node:\n            if useful_tag in element['tags']:\n                node[useful_tag] = element['tags'][useful_tag]\n    return node", "response": "Convert an OSM node element into the format for a networkx node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting an OSM way element into a networkx graph path.", "response": "def get_path(element):\n    \"\"\"\n    Convert an OSM way element into the format for a networkx graph path.\n\n    Parameters\n    ----------\n    element : dict\n        an OSM way element\n\n    Returns\n    -------\n    dict\n    \"\"\"\n\n    path = {}\n    path['osmid'] = element['id']\n\n    # remove any consecutive duplicate elements in the list of nodes\n    grouped_list = groupby(element['nodes'])\n    path['nodes'] = [group[0] for group in grouped_list]\n\n    if 'tags' in element:\n        for useful_tag in settings.useful_tags_path:\n            if useful_tag in element['tags']:\n                path[useful_tag] = element['tags'][useful_tag]\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the Osm nodes and paths from the Overpass API response.", "response": "def parse_osm_nodes_paths(osm_data):\n    \"\"\"\n    Construct dicts of nodes and paths with key=osmid and value=dict of\n    attributes.\n\n    Parameters\n    ----------\n    osm_data : dict\n        JSON response from from the Overpass API\n\n    Returns\n    -------\n    nodes, paths : tuple\n    \"\"\"\n\n    nodes = {}\n    paths = {}\n    for element in osm_data['elements']:\n        if element['type'] == 'node':\n            key = element['id']\n            nodes[key] = get_node(element)\n        elif element['type'] == 'way': #osm calls network paths 'ways'\n            key = element['id']\n            paths[key] = get_path(element)\n\n    return nodes, paths"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_isolated_nodes(G):\n\n    isolated_nodes = [node for node, degree in dict(G.degree()).items() if degree < 1]\n    G.remove_nodes_from(isolated_nodes)\n    log('Removed {:,} isolated nodes'.format(len(isolated_nodes)))\n    return G", "response": "Removes isolated nodes from a networkx multidigraph G."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef truncate_graph_dist(G, source_node, max_distance=1000, weight='length', retain_all=False):\n\n    # get the shortest distance between the node and every other node, then\n    # remove every node further than max_distance away\n    start_time = time.time()\n    G = G.copy()\n    distances = nx.shortest_path_length(G, source=source_node, weight=weight)\n    distant_nodes = {key:value for key, value in dict(distances).items() if value > max_distance}\n    G.remove_nodes_from(distant_nodes.keys())\n    log('Truncated graph by weighted network distance in {:,.2f} seconds'.format(time.time()-start_time))\n\n    # remove any isolated nodes and retain only the largest component (if\n    # retain_all is True)\n    if not retain_all:\n        G = remove_isolated_nodes(G)\n        G = get_largest_component(G)\n\n    return G", "response": "Truncate a networkx graph by weighted network distance."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove every node in graph that falls outside a bounding box. Needed because overpass returns entire ways that also include nodes outside the bbox if the way (that is, a way with a single OSM ID) has a node inside the bbox at some point. Parameters ---------- G : networkx multidigraph north : float northern latitude of bounding box south : float southern latitude of bounding box east : float eastern longitude of bounding box west : float western longitude of bounding box truncate_by_edge : bool if True retain node if it's outside bbox but at least one of node's neighbors are within bbox retain_all : bool if True, return the entire graph even if it is not connected Returns ------- networkx multidigraph", "response": "def truncate_graph_bbox(G, north, south, east, west, truncate_by_edge=False, retain_all=False):\n    \"\"\"\n    Remove every node in graph that falls outside a bounding box.\n\n    Needed because overpass returns entire ways that also include nodes outside\n    the bbox if the way (that is, a way with a single OSM ID) has a node inside\n    the bbox at some point.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n    north : float\n        northern latitude of bounding box\n    south : float\n        southern latitude of bounding box\n    east : float\n        eastern longitude of bounding box\n    west : float\n        western longitude of bounding box\n    truncate_by_edge : bool\n        if True retain node if it's outside bbox but at least one of node's\n        neighbors are within bbox\n    retain_all : bool\n        if True, return the entire graph even if it is not connected\n\n    Returns\n    -------\n    networkx multidigraph\n    \"\"\"\n\n    start_time = time.time()\n    G = G.copy()\n    nodes_outside_bbox = []\n\n    for node, data in G.nodes(data=True):\n        if data['y'] > north or data['y'] < south or data['x'] > east or data['x'] < west:\n            # this node is outside the bounding box\n            if not truncate_by_edge:\n                # if we're not truncating by edge, add node to list of nodes\n                # outside the bounding box\n                nodes_outside_bbox.append(node)\n            else:\n                # if we're truncating by edge, see if any of node's neighbors\n                # are within bounding box\n                any_neighbors_in_bbox = False\n                neighbors = list(G.successors(node)) + list(G.predecessors(node))\n                for neighbor in neighbors:\n                    x = G.nodes[neighbor]['x']\n                    y = G.nodes[neighbor]['y']\n                    if y < north and y > south and x < east and x > west:\n                        any_neighbors_in_bbox = True\n                        break\n\n                # if none of its neighbors are within the bounding box, add node\n                # to list of nodes outside the bounding box\n                if not any_neighbors_in_bbox:\n                    nodes_outside_bbox.append(node)\n\n    G.remove_nodes_from(nodes_outside_bbox)\n    log('Truncated graph by bounding box in {:,.2f} seconds'.format(time.time()-start_time))\n\n    # remove any isolated nodes and retain only the largest component (if\n    # retain_all is True)\n    if not retain_all:\n        G = remove_isolated_nodes(G)\n        G = get_largest_component(G)\n\n    return G"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef quadrat_cut_geometry(geometry, quadrat_width, min_num=3, buffer_amount=1e-9):\n\n    # create n evenly spaced points between the min and max x and y bounds\n    west, south, east, north = geometry.bounds\n    x_num = math.ceil((east-west) / quadrat_width) + 1\n    y_num = math.ceil((north-south) / quadrat_width) + 1\n    x_points = np.linspace(west, east, num=max(x_num, min_num))\n    y_points = np.linspace(south, north, num=max(y_num, min_num))\n\n    # create a quadrat grid of lines at each of the evenly spaced points\n    vertical_lines = [LineString([(x, y_points[0]), (x, y_points[-1])]) for x in x_points]\n    horizont_lines = [LineString([(x_points[0], y), (x_points[-1], y)]) for y in y_points]\n    lines = vertical_lines + horizont_lines\n\n    # buffer each line to distance of the quadrat width divided by 1 billion,\n    # take their union, then cut geometry into pieces by these quadrats\n    buffer_size = quadrat_width * buffer_amount\n    lines_buffered = [line.buffer(buffer_size) for line in lines]\n    quadrats = unary_union(lines_buffered)\n    multipoly = geometry.difference(quadrats)\n\n    return multipoly", "response": "Splits a shapely Polygon or MultiPolygon into smaller sub - polygons of a specified size and buffer them into smaller quadrats."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef intersect_index_quadrats(gdf, geometry, quadrat_width=0.05, min_num=3, buffer_amount=1e-9):\n\n    # create an empty dataframe to append matches to\n    points_within_geometry = pd.DataFrame()\n\n    # cut the geometry into chunks for r-tree spatial index intersecting\n    multipoly = quadrat_cut_geometry(geometry, quadrat_width=quadrat_width, buffer_amount=buffer_amount, min_num=min_num)\n\n    # create an r-tree spatial index for the nodes (ie, points)\n    start_time = time.time()\n    sindex = gdf['geometry'].sindex\n    log('Created r-tree spatial index for {:,} points in {:,.2f} seconds'.format(len(gdf), time.time()-start_time))\n\n    # loop through each chunk of the geometry to find approximate and then\n    # precisely intersecting points\n    start_time = time.time()\n    for poly in multipoly:\n\n        # buffer by the tiny distance to account for any space lost in the\n        # quadrat cutting, otherwise may miss point(s) that lay directly on\n        # quadrat line\n        buffer_size = quadrat_width * buffer_amount\n        poly = poly.buffer(buffer_size).buffer(0)\n\n        # find approximate matches with r-tree, then precise matches from those\n        # approximate ones\n        if poly.is_valid and poly.area > 0:\n            possible_matches_index = list(sindex.intersection(poly.bounds))\n            possible_matches = gdf.iloc[possible_matches_index]\n            precise_matches = possible_matches[possible_matches.intersects(poly)]\n            points_within_geometry = points_within_geometry.append(precise_matches)\n\n    if len(points_within_geometry) > 0:\n        # drop duplicate points, if buffered poly caused an overlap on point(s)\n        # that lay directly on a quadrat line\n        points_within_geometry = points_within_geometry.drop_duplicates(subset='node')\n    else:\n        # after simplifying the graph, and given the requested network type,\n        # there are no nodes inside the polygon - can't create graph from that\n        # so throw error\n        raise Exception('There are no nodes within the requested geometry')\n\n    log('Identified {:,} nodes inside polygon in {:,.2f} seconds'.format(len(points_within_geometry), time.time()-start_time))\n    return points_within_geometry", "response": "Intersects points with a polygon using an r - tree spatial index and cutting the polygon up into smaller sub - polygons for r - tree acceleration"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves every node in graph that lie outside the polygon or MultiPolygon.", "response": "def truncate_graph_polygon(G, polygon, retain_all=False, truncate_by_edge=False, quadrat_width=0.05, min_num=3, buffer_amount=1e-9):\n    \"\"\"\n    Remove every node in graph that falls outside some shapely Polygon or\n    MultiPolygon.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n    polygon : Polygon or MultiPolygon\n        only retain nodes in graph that lie within this geometry\n    retain_all : bool\n        if True, return the entire graph even if it is not connected\n    truncate_by_edge : bool\n        if True retain node if it's outside polygon but at least one of node's\n        neighbors are within polygon (NOT CURRENTLY IMPLEMENTED)\n    quadrat_width : numeric\n        passed on to intersect_index_quadrats: the linear length (in degrees) of\n        the quadrats with which to cut up the geometry (default = 0.05, approx\n        4km at NYC's latitude)\n    min_num : int\n        passed on to intersect_index_quadrats: the minimum number of linear\n        quadrat lines (e.g., min_num=3 would produce a quadrat grid of 4\n        squares)\n    buffer_amount : numeric\n        passed on to intersect_index_quadrats: buffer the quadrat grid lines by\n        quadrat_width times buffer_amount\n\n    Returns\n    -------\n    networkx multidigraph\n    \"\"\"\n\n    start_time = time.time()\n    G = G.copy()\n    log('Identifying all nodes that lie outside the polygon...')\n\n    # get a GeoDataFrame of all the nodes, for spatial analysis\n    node_geom = [Point(data['x'], data['y']) for _, data in G.nodes(data=True)]\n    gdf_nodes = gpd.GeoDataFrame({'node':pd.Series(G.nodes()), 'geometry':node_geom})\n    gdf_nodes.crs = G.graph['crs']\n\n    # find all the nodes in the graph that lie outside the polygon\n    points_within_geometry = intersect_index_quadrats(gdf_nodes, polygon, quadrat_width=quadrat_width, min_num=min_num, buffer_amount=buffer_amount)\n    nodes_outside_polygon = gdf_nodes[~gdf_nodes.index.isin(points_within_geometry.index)]\n\n    # now remove from the graph all those nodes that lie outside the place\n    # polygon\n    start_time = time.time()\n    G.remove_nodes_from(nodes_outside_polygon['node'])\n    log('Removed {:,} nodes outside polygon in {:,.2f} seconds'.format(len(nodes_outside_polygon), time.time()-start_time))\n\n    # remove any isolated nodes and retain only the largest component (if retain_all is False)\n    if not retain_all:\n        G = remove_isolated_nodes(G)\n        G = get_largest_component(G)\n\n    return G"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_edge_lengths(G):\n\n    start_time = time.time()\n\n    # first load all the edges' origin and destination coordinates as a\n    # dataframe indexed by u, v, key\n    coords = np.array([[u, v, k, G.nodes[u]['y'], G.nodes[u]['x'], G.nodes[v]['y'], G.nodes[v]['x']] for u, v, k in G.edges(keys=True)])\n    df_coords = pd.DataFrame(coords, columns=['u', 'v', 'k', 'u_y', 'u_x', 'v_y', 'v_x'])\n    df_coords[['u', 'v', 'k']] = df_coords[['u', 'v', 'k']].astype(np.int64)\n    df_coords = df_coords.set_index(['u', 'v', 'k'])\n\n    # then calculate the great circle distance with the vectorized function\n    gc_distances = great_circle_vec(lat1=df_coords['u_y'],\n                                    lng1=df_coords['u_x'],\n                                    lat2=df_coords['v_y'],\n                                    lng2=df_coords['v_x'])\n\n    # fill nulls with zeros and round to the millimeter\n    gc_distances = gc_distances.fillna(value=0).round(3)\n    nx.set_edge_attributes(G, name='length', values=gc_distances.to_dict())\n\n    log('Added edge lengths to graph in {:,.2f} seconds'.format(time.time()-start_time))\n    return G", "response": "Adds length attribute to each edge in the graph G by great circle distance between nodes u and v."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_path(G, data, one_way):\n\n    # extract the ordered list of nodes from this path element, then delete it\n    # so we don't add it as an attribute to the edge later\n    path_nodes = data['nodes']\n    del data['nodes']\n\n    # set the oneway attribute to the passed-in value, to make it consistent\n    # True/False values\n    data['oneway'] = one_way\n\n    # zip together the path nodes so you get tuples like (0,1), (1,2), (2,3)\n    # and so on\n    path_edges = list(zip(path_nodes[:-1], path_nodes[1:]))\n    G.add_edges_from(path_edges, **data)\n\n    # if the path is NOT one-way\n    if not one_way:\n        # reverse the direction of each edge and add this path going the\n        # opposite direction\n        path_edges_opposite_direction = [(v, u) for u, v in path_edges]\n        G.add_edges_from(path_edges_opposite_direction, **data)", "response": "Adds a path to the multidigraph G."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_paths(G, paths, bidirectional=False):\n\n    # the list of values OSM uses in its 'oneway' tag to denote True\n    osm_oneway_values = ['yes', 'true', '1', '-1']\n\n    for data in paths.values():\n\n        # if this path is tagged as one-way and if it is not a walking network,\n        # then we'll add the path in one direction only\n        if ('oneway' in data and data['oneway'] in osm_oneway_values) and not bidirectional:\n            if data['oneway'] == '-1':\n                # paths with a one-way value of -1 are one-way, but in the\n                # reverse direction of the nodes' order, see osm documentation\n                data['nodes'] = list(reversed(data['nodes']))\n            # add this path (in only one direction) to the graph\n            add_path(G, data, one_way=True)\n\n        elif ('junction' in data and data['junction'] == 'roundabout') and not bidirectional:\n            # roundabout are also oneway but not tagged as is\n            add_path(G, data, one_way=True)\n\n        # else, this path is not tagged as one-way or it is a walking network\n        # (you can walk both directions on a one-way street)\n        else:\n            # add this path (in both directions) to the graph and set its\n            # 'oneway' attribute to False. if this is a walking network, this\n            # may very well be a one-way street (as cars/bikes go), but in a\n            # walking-only network it is a bi-directional edge\n            add_path(G, data, one_way=False)\n\n    return G", "response": "Adds a collection of paths to the OSM tree G."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_graph(response_jsons, name='unnamed', retain_all=False, bidirectional=False):\n\n    log('Creating networkx graph from downloaded OSM data...')\n    start_time = time.time()\n\n    # make sure we got data back from the server requests\n    elements = []\n    for response_json in response_jsons:\n        elements.extend(response_json['elements'])\n    if len(elements) < 1:\n        raise EmptyOverpassResponse('There are no data elements in the response JSON objects')\n\n    # create the graph as a MultiDiGraph and set the original CRS to default_crs\n    G = nx.MultiDiGraph(name=name, crs=settings.default_crs)\n\n    # extract nodes and paths from the downloaded osm data\n    nodes = {}\n    paths = {}\n    for osm_data in response_jsons:\n        nodes_temp, paths_temp = parse_osm_nodes_paths(osm_data)\n        for key, value in nodes_temp.items():\n            nodes[key] = value\n        for key, value in paths_temp.items():\n            paths[key] = value\n\n    # add each osm node to the graph\n    for node, data in nodes.items():\n        G.add_node(node, **data)\n\n    # add each osm way (aka, path) to the graph\n    G = add_paths(G, paths, bidirectional=bidirectional)\n\n    # retain only the largest connected component, if caller did not\n    # set retain_all=True\n    if not retain_all:\n        G = get_largest_component(G)\n\n    log('Created graph with {:,} nodes and {:,} edges in {:,.2f} seconds'.format(len(list(G.nodes())), len(list(G.edges())), time.time()-start_time))\n\n    # add length (great circle distance between nodes) attribute to each edge to\n    # use as weight\n    if len(G.edges) > 0:\n        G = add_edge_lengths(G)\n\n    return G", "response": "Create a networkx graph from the OSM data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bbox_from_point(point, distance=1000, project_utm=False, return_crs=False):\n\n    # reverse the order of the (lat,lng) point so it is (x,y) for shapely, then\n    # project to UTM and buffer in meters\n    lat, lng = point\n    point_proj, crs_proj = project_geometry(Point((lng, lat)))\n    buffer_proj = point_proj.buffer(distance)\n\n    if project_utm:\n        west, south, east, north = buffer_proj.bounds\n        log('Created bounding box {} meters in each direction from {} and projected it: {},{},{},{}'.format(distance, point, north, south, east, west))\n    else:\n        # if project_utm is False, project back to lat-long then get the\n        # bounding coordinates\n        buffer_latlong, _ = project_geometry(buffer_proj, crs=crs_proj, to_latlong=True)\n        west, south, east, north = buffer_latlong.bounds\n        log('Created bounding box {} meters in each direction from {}: {},{},{},{}'.format(distance, point, north, south, east, west))\n\n    if return_crs:\n        return north, south, east, west, crs_proj\n    else:\n        return north, south, east, west", "response": "Create a bounding box around a point"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef graph_from_bbox(north, south, east, west, network_type='all_private',\n                    simplify=True, retain_all=False, truncate_by_edge=False,\n                    name='unnamed', timeout=180, memory=None,\n                    max_query_area_size=50*1000*50*1000, clean_periphery=True,\n                    infrastructure='way[\"highway\"]', custom_filter=None):\n    \"\"\"\n    Create a networkx graph from OSM data within some bounding box.\n\n    Parameters\n    ----------\n    north : float\n        northern latitude of bounding box\n    south : float\n        southern latitude of bounding box\n    east : float\n        eastern longitude of bounding box\n    west : float\n        western longitude of bounding box\n    network_type : string\n        what type of street network to get\n    simplify : bool\n        if true, simplify the graph topology\n    retain_all : bool\n        if True, return the entire graph even if it is not connected\n    truncate_by_edge : bool\n        if True retain node if it's outside bbox but at least one of node's\n        neighbors are within bbox\n    name : string\n        the name of the graph\n    timeout : int\n        the timeout interval for requests and to pass to API\n    memory : int\n        server memory allocation size for the query, in bytes. If none, server\n        will use its default allocation size\n    max_query_area_size : float\n        max size for any part of the geometry, in square degrees: any polygon\n        bigger will get divided up for multiple queries to API\n    clean_periphery : bool\n        if True (and simplify=True), buffer 0.5km to get a graph larger than\n        requested, then simplify, then truncate it to requested spatial extent\n    infrastructure : string\n        download infrastructure of given type (default is streets (ie, 'way[\"highway\"]') but other\n        infrastructures may be selected like power grids (ie, 'way[\"power\"~\"line\"]'))\n    custom_filter : string\n        a custom network filter to be used instead of the network_type presets\n\n    Returns\n    -------\n    networkx multidigraph\n    \"\"\"\n\n    if clean_periphery and simplify:\n        # create a new buffered bbox 0.5km around the desired one\n        buffer_dist = 500\n        polygon = Polygon([(west, north), (west, south), (east, south), (east, north)])\n        polygon_utm, crs_utm = project_geometry(geometry=polygon)\n        polygon_proj_buff = polygon_utm.buffer(buffer_dist)\n        polygon_buff, _ = project_geometry(geometry=polygon_proj_buff, crs=crs_utm, to_latlong=True)\n        west_buffered, south_buffered, east_buffered, north_buffered = polygon_buff.bounds\n\n        # get the network data from OSM then create the graph\n        response_jsons = osm_net_download(north=north_buffered, south=south_buffered,\n                                          east=east_buffered, west=west_buffered,\n                                          network_type=network_type, timeout=timeout,\n                                          memory=memory, max_query_area_size=max_query_area_size,\n                                          infrastructure=infrastructure, custom_filter=custom_filter)\n        G_buffered = create_graph(response_jsons, name=name, retain_all=retain_all,\n                                  bidirectional=network_type in settings.bidirectional_network_types)\n        G = truncate_graph_bbox(G_buffered, north, south, east, west, retain_all=True, truncate_by_edge=truncate_by_edge)\n\n        # simplify the graph topology\n        G_buffered = simplify_graph(G_buffered)\n\n        # truncate graph by desired bbox to return the graph within the bbox\n        # caller wants\n        G = truncate_graph_bbox(G_buffered, north, south, east, west, retain_all=retain_all, truncate_by_edge=truncate_by_edge)\n\n        # count how many street segments in buffered graph emanate from each\n        # intersection in un-buffered graph, to retain true counts for each\n        # intersection, even if some of its neighbors are outside the bbox\n        G.graph['streets_per_node'] = count_streets_per_node(G_buffered, nodes=G.nodes())\n\n    else:\n        # get the network data from OSM\n        response_jsons = osm_net_download(north=north, south=south, east=east,\n                                          west=west, network_type=network_type,\n                                          timeout=timeout, memory=memory,\n                                          max_query_area_size=max_query_area_size,\n                                          infrastructure=infrastructure, custom_filter=custom_filter)\n\n        # create the graph, then truncate to the bounding box\n        G = create_graph(response_jsons, name=name, retain_all=retain_all,\n                         bidirectional=network_type in settings.bidirectional_network_types)\n        G = truncate_graph_bbox(G, north, south, east, west, retain_all=retain_all, truncate_by_edge=truncate_by_edge)\n\n        # simplify the graph topology as the last step. don't truncate after\n        # simplifying or you may have simplified out to an endpoint\n        # beyond the truncation distance, in which case you will then strip out\n        # your entire edge\n        if simplify:\n            G = simplify_graph(G)\n\n    log('graph_from_bbox() returning graph with {:,} nodes and {:,} edges'.format(len(list(G.nodes())), len(list(G.edges()))))\n    return  G", "response": "Create a networkx graph from OSM data within some bounding box."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a networkx graph from OSM data within some distance of some center point.", "response": "def graph_from_point(center_point, distance=1000, distance_type='bbox',\n                     network_type='all_private', simplify=True, retain_all=False,\n                     truncate_by_edge=False, name='unnamed', timeout=180,\n                     memory=None, max_query_area_size=50*1000*50*1000,\n                     clean_periphery=True, infrastructure='way[\"highway\"]',\n                     custom_filter=None):\n    \"\"\"\n    Create a networkx graph from OSM data within some distance of some (lat,\n    lon) center point.\n\n    Parameters\n    ----------\n    center_point : tuple\n        the (lat, lon) central point around which to construct the graph\n    distance : int\n        retain only those nodes within this many meters of the center of the\n        graph, with distance determined according to distance_type argument\n    distance_type : string\n        {'network', 'bbox'} if 'bbox', retain only those nodes within a bounding\n        box of the distance parameter. if 'network', retain only those nodes\n        within some network distance from the center-most node.\n    network_type : string\n        what type of street network to get\n    simplify : bool\n        if true, simplify the graph topology\n    retain_all : bool\n        if True, return the entire graph even if it is not connected\n    truncate_by_edge : bool\n        if True retain node if it's outside bbox but at least one of node's\n        neighbors are within bbox\n    name : string\n        the name of the graph\n    timeout : int\n        the timeout interval for requests and to pass to API\n    memory : int\n        server memory allocation size for the query, in bytes. If none, server\n        will use its default allocation size\n    max_query_area_size : float\n        max size for any part of the geometry, in square degrees: any polygon\n        bigger will get divided up for multiple queries to API\n    clean_periphery : bool,\n        if True (and simplify=True), buffer 0.5km to get a graph larger than\n        requested, then simplify, then truncate it to requested spatial extent\n    infrastructure : string\n        download infrastructure of given type (default is streets (ie, 'way[\"highway\"]') but other\n        infrastructures may be selected like power grids (ie, 'way[\"power\"~\"line\"]'))\n    custom_filter : string\n        a custom network filter to be used instead of the network_type presets\n\n    Returns\n    -------\n    networkx multidigraph\n    \"\"\"\n\n    if distance_type not in ['bbox', 'network']:\n        raise InvalidDistanceType('distance_type must be \"bbox\" or \"network\"')\n\n    # create a bounding box from the center point and the distance in each\n    # direction\n    north, south, east, west = bbox_from_point(center_point, distance)\n\n    # create a graph from the bounding box\n    G = graph_from_bbox(north, south, east, west, network_type=network_type, simplify=simplify,\n                        retain_all=retain_all, truncate_by_edge=truncate_by_edge, name=name,\n                        timeout=timeout, memory=memory, max_query_area_size=max_query_area_size,\n                        clean_periphery=clean_periphery, infrastructure=infrastructure,\n                        custom_filter=custom_filter)\n\n    # if the network distance_type is network, find the node in the graph\n    # nearest to the center point, and truncate the graph by network distance\n    # from this node\n    if distance_type == 'network':\n        centermost_node = get_nearest_node(G, center_point)\n        G = truncate_graph_dist(G, centermost_node, max_distance=distance)\n\n    log('graph_from_point() returning graph with {:,} nodes and {:,} edges'.format(len(list(G.nodes())), len(list(G.edges()))))\n    return G"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a networkx graph from OSM data within some distance of some address.", "response": "def graph_from_address(address, distance=1000, distance_type='bbox',\n                       network_type='all_private', simplify=True, retain_all=False,\n                       truncate_by_edge=False, return_coords=False,\n                       name='unnamed', timeout=180, memory=None,\n                       max_query_area_size=50*1000*50*1000,\n                       clean_periphery=True, infrastructure='way[\"highway\"]',\n                       custom_filter=None):\n    \"\"\"\n    Create a networkx graph from OSM data within some distance of some address.\n\n    Parameters\n    ----------\n    address : string\n        the address to geocode and use as the central point around which to\n        construct the graph\n    distance : int\n        retain only those nodes within this many meters of the center of the\n        graph\n    distance_type : string\n        {'network', 'bbox'} if 'bbox', retain only those nodes within a bounding\n        box of the distance parameter.\n        if 'network', retain only those nodes within some network distance from\n        the center-most node.\n    network_type : string\n        what type of street network to get\n    simplify : bool\n        if true, simplify the graph topology\n    retain_all : bool\n        if True, return the entire graph even if it is not connected\n    truncate_by_edge : bool\n        if True retain node if it's outside bbox but at least one of node's\n        neighbors are within bbox\n    return_coords : bool\n        optionally also return the geocoded coordinates of the address\n    name : string\n        the name of the graph\n    timeout : int\n        the timeout interval for requests and to pass to API\n    memory : int\n        server memory allocation size for the query, in bytes. If none, server\n        will use its default allocation size\n    max_query_area_size\n        float, max size for any part of the geometry, in square degrees: any\n        polygon bigger will get divided up for multiple queries to API\n    clean_periphery : bool,\n        if True (and simplify=True), buffer 0.5km to get a graph larger than\n        requested, then simplify, then truncate it to requested spatial extent\n    infrastructure : string\n        download infrastructure of given type (default is streets (ie, 'way[\"highway\"]') but other\n        infrastructures may be selected like power grids (ie, 'way[\"power\"~\"line\"]'))\n    custom_filter : string\n        a custom network filter to be used instead of the network_type presets\n\n    Returns\n    -------\n    networkx multidigraph or tuple\n        multidigraph or optionally (multidigraph, tuple)\n    \"\"\"\n\n    # geocode the address string to a (lat, lon) point\n    point = geocode(query=address)\n\n    # then create a graph from this point\n    G = graph_from_point(point, distance, distance_type, network_type=network_type,\n                         simplify=simplify, retain_all=retain_all, truncate_by_edge=truncate_by_edge,\n                         name=name, timeout=timeout, memory=memory,\n                         max_query_area_size=max_query_area_size,\n                         clean_periphery=clean_periphery, infrastructure=infrastructure,\n                         custom_filter=custom_filter)\n    log('graph_from_address() returning graph with {:,} nodes and {:,} edges'.format(len(list(G.nodes())), len(list(G.edges()))))\n\n    if return_coords:\n        return G, point\n    else:\n        return G"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef graph_from_polygon(polygon, network_type='all_private', simplify=True,\n                       retain_all=False, truncate_by_edge=False, name='unnamed',\n                       timeout=180, memory=None,\n                       max_query_area_size=50*1000*50*1000,\n                       clean_periphery=True, infrastructure='way[\"highway\"]',\n                       custom_filter=None):\n    \"\"\"\n    Create a networkx graph from OSM data within the spatial boundaries of the\n    passed-in shapely polygon.\n\n    Parameters\n    ----------\n    polygon : shapely Polygon or MultiPolygon\n        the shape to get network data within. coordinates should be in units of\n        latitude-longitude degrees.\n    network_type : string\n        what type of street network to get\n    simplify : bool\n        if true, simplify the graph topology\n    retain_all : bool\n        if True, return the entire graph even if it is not connected\n    truncate_by_edge : bool\n        if True retain node if it's outside bbox but at least one of node's\n        neighbors are within bbox\n    name : string\n        the name of the graph\n    timeout : int\n        the timeout interval for requests and to pass to API\n    memory : int\n        server memory allocation size for the query, in bytes. If none, server\n        will use its default allocation size\n    max_query_area_size : float\n        max size for any part of the geometry, in square degrees: any polygon\n        bigger will get divided up for multiple queries to API\n    clean_periphery : bool\n        if True (and simplify=True), buffer 0.5km to get a graph larger than\n        requested, then simplify, then truncate it to requested spatial extent\n    infrastructure : string\n        download infrastructure of given type (default is streets\n        (ie, 'way[\"highway\"]') but other infrastructures may be selected\n        like power grids (ie, 'way[\"power\"~\"line\"]'))\n    custom_filter : string\n        a custom network filter to be used instead of the network_type presets\n\n    Returns\n    -------\n    networkx multidigraph\n    \"\"\"\n\n    # verify that the geometry is valid and is a shapely Polygon/MultiPolygon\n    # before proceeding\n    if not polygon.is_valid:\n        raise TypeError('Shape does not have a valid geometry')\n    if not isinstance(polygon, (Polygon, MultiPolygon)):\n        raise TypeError('Geometry must be a shapely Polygon or MultiPolygon. If you requested '\n                         'graph from place name or address, make sure your query resolves to a '\n                         'Polygon or MultiPolygon, and not some other geometry, like a Point. '\n                         'See OSMnx documentation for details.')\n\n    if clean_periphery and simplify:\n        # create a new buffered polygon 0.5km around the desired one\n        buffer_dist = 500\n        polygon_utm, crs_utm = project_geometry(geometry=polygon)\n        polygon_proj_buff = polygon_utm.buffer(buffer_dist)\n        polygon_buffered, _ = project_geometry(geometry=polygon_proj_buff, crs=crs_utm, to_latlong=True)\n\n        # get the network data from OSM,  create the buffered graph, then\n        # truncate it to the buffered polygon\n        response_jsons = osm_net_download(polygon=polygon_buffered, network_type=network_type,\n                                          timeout=timeout, memory=memory,\n                                          max_query_area_size=max_query_area_size,\n                                          infrastructure=infrastructure, custom_filter=custom_filter)\n        G_buffered = create_graph(response_jsons, name=name, retain_all=True,\n                                  bidirectional=network_type in settings.bidirectional_network_types)\n        G_buffered = truncate_graph_polygon(G_buffered, polygon_buffered, retain_all=True, truncate_by_edge=truncate_by_edge)\n\n        # simplify the graph topology\n        G_buffered = simplify_graph(G_buffered)\n\n        # truncate graph by polygon to return the graph within the polygon that\n        # caller wants. don't simplify again - this allows us to retain\n        # intersections along the street that may now only connect 2 street\n        # segments in the network, but in reality also connect to an\n        # intersection just outside the polygon\n        G = truncate_graph_polygon(G_buffered, polygon, retain_all=retain_all, truncate_by_edge=truncate_by_edge)\n\n        # count how many street segments in buffered graph emanate from each\n        # intersection in un-buffered graph, to retain true counts for each\n        # intersection, even if some of its neighbors are outside the polygon\n        G.graph['streets_per_node'] = count_streets_per_node(G_buffered, nodes=G.nodes())\n\n    else:\n        # download a list of API responses for the polygon/multipolygon\n        response_jsons = osm_net_download(polygon=polygon, network_type=network_type,\n                                          timeout=timeout, memory=memory,\n                                          max_query_area_size=max_query_area_size,\n                                          infrastructure=infrastructure, custom_filter=custom_filter)\n\n        # create the graph from the downloaded data\n        G = create_graph(response_jsons, name=name, retain_all=True,\n                         bidirectional=network_type in settings.bidirectional_network_types)\n\n        # truncate the graph to the extent of the polygon\n        G = truncate_graph_polygon(G, polygon, retain_all=retain_all, truncate_by_edge=truncate_by_edge)\n\n        # simplify the graph topology as the last step. don't truncate after\n        # simplifying or you may have simplified out to an endpoint beyond the\n        # truncation distance, in which case you will then strip out your entire\n        # edge\n        if simplify:\n            G = simplify_graph(G)\n\n    log('graph_from_polygon() returning graph with {:,} nodes and {:,} edges'.format(len(list(G.nodes())), len(list(G.edges()))))\n    return G", "response": "Create a networkx graph from OSM data within the specified shapely polygon."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef graph_from_place(query, network_type='all_private', simplify=True,\n                     retain_all=False, truncate_by_edge=False, name='unnamed',\n                     which_result=1, buffer_dist=None, timeout=180, memory=None,\n                     max_query_area_size=50*1000*50*1000, clean_periphery=True,\n                     infrastructure='way[\"highway\"]', custom_filter=None):\n    \"\"\"\n    Create a networkx graph from OSM data within the spatial boundaries of some\n    geocodable place(s).\n\n    The query must be geocodable and OSM must have polygon boundaries for the\n    geocode result. If OSM does not have a polygon for this place, you can\n    instead get its street network using the graph_from_address function, which\n    geocodes the place name to a point and gets the network within some distance\n    of that point. Alternatively, you might try to vary the which_result\n    parameter to use a different geocode result. For example, the first geocode\n    result (ie, the default) might resolve to a point geometry, but the second\n    geocode result for this query might resolve to a polygon, in which case you\n    can use graph_from_place with which_result=2.\n\n    Parameters\n    ----------\n    query : string or dict or list\n        the place(s) to geocode/download data for\n    network_type : string\n        what type of street network to get\n    simplify : bool\n        if true, simplify the graph topology\n    retain_all : bool\n        if True, return the entire graph even if it is not connected\n    truncate_by_edge : bool\n        if True retain node if it's outside bbox but at least one of node's\n        neighbors are within bbox\n    name : string\n        the name of the graph\n    which_result : int\n        max number of results to return and which to process upon receipt\n    buffer_dist : float\n        distance to buffer around the place geometry, in meters\n    timeout : int\n        the timeout interval for requests and to pass to API\n    memory : int\n        server memory allocation size for the query, in bytes. If none, server\n        will use its default allocation size\n    max_query_area_size : float\n        max size for any part of the geometry, in square degrees: any polygon\n        bigger will get divided up for multiple queries to API\n    clean_periphery : bool\n        if True (and simplify=True), buffer 0.5km to get a graph larger than\n        requested, then simplify, then truncate it to requested spatial extent\n    infrastructure : string\n        download infrastructure of given type (default is streets (ie, 'way[\"highway\"]') but other\n        infrastructures may be selected like power grids (ie, 'way[\"power\"~\"line\"]'))\n    custom_filter : string\n        a custom network filter to be used instead of the network_type presets\n\n    Returns\n    -------\n    networkx multidigraph\n    \"\"\"\n\n    # create a GeoDataFrame with the spatial boundaries of the place(s)\n    if isinstance(query, str) or isinstance(query, dict):\n        # if it is a string (place name) or dict (structured place query), then\n        # it is a single place\n        gdf_place = gdf_from_place(query, which_result=which_result, buffer_dist=buffer_dist)\n        name = query\n    elif isinstance(query, list):\n        # if it is a list, it contains multiple places to get\n        gdf_place = gdf_from_places(query, buffer_dist=buffer_dist)\n    else:\n        raise TypeError('query must be a string or a list of query strings')\n\n    # extract the geometry from the GeoDataFrame to use in API query\n    polygon = gdf_place['geometry'].unary_union\n    log('Constructed place geometry polygon(s) to query API')\n\n    # create graph using this polygon(s) geometry\n    G = graph_from_polygon(polygon, network_type=network_type, simplify=simplify,\n                           retain_all=retain_all, truncate_by_edge=truncate_by_edge,\n                           name=name, timeout=timeout, memory=memory,\n                           max_query_area_size=max_query_area_size,\n                           clean_periphery=clean_periphery, infrastructure=infrastructure,\n                           custom_filter=custom_filter)\n\n    log('graph_from_place() returning graph with {:,} nodes and {:,} edges'.format(len(list(G.nodes())), len(list(G.edges()))))\n    return G", "response": "Create a networkx graph from OSM data within a given place."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef graph_from_file(filename, bidirectional=False, simplify=True,\n                    retain_all=False, name='unnamed'):\n    \"\"\"\n    Create a networkx graph from OSM data in an XML file.\n\n    Parameters\n    ----------\n    filename : string\n        the name of a file containing OSM XML data\n    bidirectional : bool\n        if True, create bidirectional edges for one-way streets\n    simplify : bool\n        if True, simplify the graph topology\n    retain_all : bool\n        if True, return the entire graph even if it is not connected\n    name : string\n        the name of the graph\n\n    Returns\n    -------\n    networkx multidigraph\n    \"\"\"\n    # transmogrify file of OSM XML data into JSON\n    response_jsons = [overpass_json_from_file(filename)]\n\n    # create graph using this response JSON\n    G = create_graph(response_jsons, bidirectional=bidirectional,\n                     retain_all=retain_all, name=name)\n\n    # simplify the graph topology as the last step.\n    if simplify:\n        G = simplify_graph(G)\n\n    log('graph_from_file() returning graph with {:,} nodes and {:,} edges'.format(len(list(G.nodes())), len(list(G.edges()))))\n    return G", "response": "Create a networkx graph from OSM XML data in an XML file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndownload OpenStreetMap footprints from the server.", "response": "def osm_footprints_download(polygon=None, north=None, south=None, east=None, west=None,\n                            footprint_type='building', timeout=180, memory=None, \n                            max_query_area_size=50*1000*50*1000):\n    \"\"\"\n    Download OpenStreetMap footprint data.\n\n    Parameters\n    ----------\n    polygon : shapely Polygon or MultiPolygon\n        geographic shape to fetch the footprints within\n    north : float\n        northern latitude of bounding box\n    south : float\n        southern latitude of bounding box\n    east : float\n        eastern longitude of bounding box\n    west : float\n        western longitude of bounding box\n    footprint_type : string\n        type of footprint to be downloaded. OSM tag key e.g. 'building', 'landuse', 'place', etc.\n    timeout : int\n        the timeout interval for requests and to pass to API\n    memory : int\n        server memory allocation size for the query, in bytes. If none, server\n        will use its default allocation size\n    max_query_area_size : float\n        max area for any part of the geometry, in the units the geometry is in:\n        any polygon bigger will get divided up for multiple queries to API\n        (default is 50,000 * 50,000 units (ie, 50km x 50km in area, if units are\n        meters))\n\n    Returns\n    -------\n    list\n        list of response_json dicts\n    \"\"\"\n\n    # check if we're querying by polygon or by bounding box based on which\n    # argument(s) where passed into this function\n    by_poly = polygon is not None\n    by_bbox = not (north is None or south is None or east is None or west is None)\n    if not (by_poly or by_bbox):\n        raise ValueError('You must pass a polygon or north, south, east, and west')\n\n    response_jsons = []\n\n    # pass server memory allocation in bytes for the query to the API\n    # if None, pass nothing so the server will use its default allocation size\n    # otherwise, define the query's maxsize parameter value as whatever the\n    # caller passed in\n    if memory is None:\n        maxsize = ''\n    else:\n        maxsize = '[maxsize:{}]'.format(memory)\n\n    # define the query to send the API\n    if by_bbox:\n        # turn bbox into a polygon and project to local UTM\n        polygon = Polygon([(west, south), (east, south), (east, north), (west, north)])\n        geometry_proj, crs_proj = project_geometry(polygon)\n\n        # subdivide it if it exceeds the max area size (in meters), then project\n        # back to lat-long\n        geometry_proj_consolidated_subdivided = consolidate_subdivide_geometry(geometry_proj, max_query_area_size=max_query_area_size)\n        geometry, _ = project_geometry(geometry_proj_consolidated_subdivided, crs=crs_proj, to_latlong=True)\n        log('Requesting footprints data within bounding box from API in {:,} request(s)'.format(len(geometry)))\n        start_time = time.time()\n\n        # loop through each polygon rectangle in the geometry (there will only\n        # be one if original bbox didn't exceed max area size)\n        for poly in geometry:\n            # represent bbox as south,west,north,east and round lat-longs to 8\n            # decimal places (ie, within 1 mm) so URL strings aren't different\n            # due to float rounding issues (for consistent caching)\n            west, south, east, north = poly.bounds\n            query_template = ('[out:json][timeout:{timeout}]{maxsize};'\n                              '((way[\"{footprint_type}\"]({south:.8f},{west:.8f},{north:.8f},{east:.8f});'\n                              '(._;>;););'\n                              '(relation[\"{footprint_type}\"]({south:.8f},{west:.8f},{north:.8f},{east:.8f});'\n                              '(._;>;);););out;')\n            query_str = query_template.format(north=north, south=south, east=east, west=west, timeout=timeout,\n                                              maxsize=maxsize, footprint_type=footprint_type)\n            response_json = overpass_request(data={'data':query_str}, timeout=timeout)\n            response_jsons.append(response_json)\n        msg = ('Got all footprint data within bounding box from '\n               'API in {:,} request(s) and {:,.2f} seconds')\n        log(msg.format(len(geometry), time.time()-start_time))\n\n    elif by_poly:\n        # project to utm, divide polygon up into sub-polygons if area exceeds a\n        # max size (in meters), project back to lat-long, then get a list of polygon(s) exterior coordinates\n        geometry_proj, crs_proj = project_geometry(polygon)\n        geometry_proj_consolidated_subdivided = consolidate_subdivide_geometry(geometry_proj, max_query_area_size=max_query_area_size)\n        geometry, _ = project_geometry(geometry_proj_consolidated_subdivided, crs=crs_proj, to_latlong=True)\n        polygon_coord_strs = get_polygons_coordinates(geometry)\n        log('Requesting footprint data within polygon from API in {:,} request(s)'.format(len(polygon_coord_strs)))\n        start_time = time.time()\n\n        # pass each polygon exterior coordinates in the list to the API, one at\n        # a time\n        for polygon_coord_str in polygon_coord_strs:\n            query_template = ('[out:json][timeout:{timeout}]{maxsize};('\n                              'way(poly:\"{polygon}\")[\"{footprint_type}\"];(._;>;);'\n                              'relation(poly:\"{polygon}\")[\"{footprint_type}\"];(._;>;););out;')\n            query_str = query_template.format(polygon=polygon_coord_str, timeout=timeout, maxsize=maxsize,\n                                              footprint_type=footprint_type)\n            response_json = overpass_request(data={'data':query_str}, timeout=timeout)\n            response_jsons.append(response_json)\n        msg = ('Got all footprint data within polygon from API in '\n               '{:,} request(s) and {:,.2f} seconds')\n        log(msg.format(len(polygon_coord_strs), time.time()-start_time))\n\n    return response_jsons"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a GeoDataFrame containing the footprints from OSM.", "response": "def create_footprints_gdf(polygon=None, north=None, south=None, east=None, west=None, \n                          footprint_type='building', retain_invalid=False):\n    \"\"\"\n    Get footprint data from OSM then assemble it into a GeoDataFrame.\n\n    Parameters\n    ----------\n    polygon : shapely Polygon or MultiPolygon\n        geographic shape to fetch the footprints within\n    north : float\n        northern latitude of bounding box\n    south : float\n        southern latitude of bounding box\n    east : float\n        eastern longitude of bounding box\n    west : float\n        western longitude of bounding box\n    footprint_type : string\n        type of footprint to be downloaded. OSM tag key e.g. 'building', 'landuse', 'place', etc.\n    retain_invalid : bool\n        if False discard any footprints with an invalid geometry\n\n    Returns\n    -------\n    GeoDataFrame\n    \"\"\"\n\n    responses = osm_footprints_download(polygon, north, south, east, west, footprint_type)\n\n    # list of polygons to removed at the end of the process\n    pop_list = []\n\n    vertices = {}\n    for response in responses:\n        for result in response['elements']:\n            if 'type' in result and result['type']=='node':\n                vertices[result['id']] = {'lat' : result['lat'],\n                                          'lon' : result['lon']}\n\n    footprints = {}\n    for response in responses:\n        for result in response['elements']:\n            if 'type' in result and result['type']=='way':\n                nodes = result['nodes']\n                try:\n                    polygon = Polygon([(vertices[node]['lon'], vertices[node]['lat']) for node in nodes])\n                except Exception:\n                    log('Polygon has invalid geometry: {}'.format(nodes))\n                footprint = {'nodes' : nodes,\n                            'geometry' : polygon}\n\n                if 'tags' in result:\n                    for tag in result['tags']:\n                        footprint[tag] = result['tags'][tag]\n\n                # if polygons are untagged or not tagged with the footprint_type\n                # add them to pop_list to be removed from the final dictionary\n                if 'tags' not in result:\n                    pop_list.append(result['id'])\n                elif footprint_type not in result['tags']:\n                    pop_list.append(result['id'])\n\n                footprints[result['id']] = footprint\n\n    # Create multipolygon footprints and pop untagged supporting polygons from footprints\n    for response in responses:\n        for result in response['elements']:\n            if 'type' in result and result['type']=='relation':\n\n                outer_polys = []\n                inner_polys = []\n                multipoly = []\n\n                for member in result['members']:\n                    if 'role' in member and member['role']=='outer':\n                        outer_polys.append(member['ref'])\n                    if 'role' in member and member['role']=='inner':\n                        inner_polys.append(member['ref'])\n\n                # osm allows multiple outer polygons in a relation\n                for outer_poly in outer_polys:\n                    temp_poly=footprints[outer_poly]['geometry']\n\n                    for inner_poly in inner_polys:\n                        temp_poly=temp_poly.difference(footprints[inner_poly]['geometry'])\n\n                    multipoly.append(temp_poly)\n\n                footprint = {'geometry' : MultiPolygon(multipoly)}\n\n                if 'tags' in result:\n                    for tag in result['tags']:\n                        footprint[tag] = result['tags'][tag]\n\n                footprints[result['id']] = footprint\n\n    # remove supporting geometry from footprints dictionary\n    for item in pop_list:\n        footprints.pop(item)\n\n    gdf = gpd.GeoDataFrame(footprints).T\n    gdf.crs = settings.default_crs\n\n    if not retain_invalid:\n        # drop all invalid geometries\n        gdf = gdf[gdf['geometry'].is_valid]\n\n    return gdf"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a GeoDataFrame containing footprints within some distance north south east and west of a point.", "response": "def footprints_from_point(point, distance, footprint_type='building', retain_invalid=False):\n    \"\"\"\n    Get footprints within some distance north, south, east, and west of\n    a lat-long point.\n\n    Parameters\n    ----------\n    point : tuple\n        a lat-long point\n    distance : numeric\n        distance in meters\n    footprint_type : string\n        type of footprint to be downloaded. OSM tag key e.g. 'building', 'landuse', 'place', etc.\n    retain_invalid : bool\n        if False discard any footprints with an invalid geometry\n\n    Returns\n    -------\n    GeoDataFrame\n    \"\"\"\n\n    bbox = bbox_from_point(point=point, distance=distance)\n    north, south, east, west = bbox\n    return create_footprints_gdf(north=north, south=south, east=east, west=west, \n                                 footprint_type=footprint_type, retain_invalid=retain_invalid)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting footprints within some distance north south east and west of a node.", "response": "def footprints_from_address(address, distance, footprint_type='building', retain_invalid=False):\n    \"\"\"\n    Get footprints within some distance north, south, east, and west of\n    an address.\n\n    Parameters\n    ----------\n    address : string\n        the address to geocode to a lat-long point\n    distance : numeric\n        distance in meters\n    footprint_type : string\n        type of footprint to be downloaded. OSM tag key e.g. 'building', 'landuse', 'place', etc.\n    retain_invalid : bool\n        if False discard any footprints with an invalid geometry\n\n    Returns\n    -------\n    GeoDataFrame\n    \"\"\"\n\n    # geocode the address string to a (lat, lon) point\n    point = geocode(query=address)\n\n    # get footprints within distance of this point\n    return footprints_from_point(point, distance, footprint_type=footprint_type, \n                                 retain_invalid=retain_invalid)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a GeoDataFrame containing footprints within some polygon.", "response": "def footprints_from_polygon(polygon, footprint_type='building', retain_invalid=False):\n    \"\"\"\n    Get footprints within some polygon.\n\n    Parameters\n    ----------\n    polygon : shapely Polygon or MultiPolygon\n        the shape to get data within. coordinates should be in units of\n        latitude-longitude degrees.\n    footprint_type : string\n        type of footprint to be downloaded. OSM tag key e.g. 'building', 'landuse', 'place', etc.\n    retain_invalid : bool\n        if False discard any footprints with an invalid geometry\n\n    Returns\n    -------\n    GeoDataFrame\n    \"\"\"\n\n    return create_footprints_gdf(polygon=polygon, footprint_type=footprint_type, \n                                 retain_invalid=retain_invalid)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef footprints_from_place(place, footprint_type='building', retain_invalid=False):\n\n    city = gdf_from_place(place)\n    polygon = city['geometry'].iloc[0]\n    return create_footprints_gdf(polygon, retain_invalid=retain_invalid,\n                                 footprint_type=footprint_type)", "response": "Returns a list of footprints within the boundaries of some place."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_footprints(gdf, fig=None, ax=None, figsize=None, color='#333333', bgcolor='w',\n                   set_bounds=True, bbox=None, save=False, show=True, close=False, \n                   filename='image', file_format='png', dpi=600):\n    \"\"\"\n    Plot a GeoDataFrame of footprints.\n\n    Parameters\n    ----------\n    gdf : GeoDataFrame\n        footprints\n    fig : figure\n    ax : axis\n    figsize : tuple\n    color : string\n        the color of the footprints\n    bgcolor : string\n        the background color of the plot\n    set_bounds : bool\n        if True, set bounds from either passed-in bbox or the spatial extent of the gdf\n    bbox : tuple\n        if True and if set_bounds is True, set the display bounds to this bbox\n    save : bool\n        whether to save the figure to disk or not\n    show : bool\n        whether to display the figure or not\n    close : bool\n        close the figure (only if show equals False) to prevent display\n    filename : string\n        the name of the file to save\n    file_format : string\n        the format of the file to save (e.g., 'jpg', 'png', 'svg')\n    dpi : int\n        the resolution of the image file if saving\n\n    Returns\n    -------\n    fig, ax : tuple\n\n    \"\"\"\n\n    if fig is None or ax is None:\n        fig, ax = plt.subplots(figsize=figsize, facecolor=bgcolor)\n        ax.set_facecolor(bgcolor)\n\n    # extract each polygon as a descartes patch, and add to a matplotlib patch\n    # collection\n    patches = []\n    for geometry in gdf['geometry']:\n        if isinstance(geometry, Polygon):\n            patches.append(PolygonPatch(geometry))\n        elif isinstance(geometry, MultiPolygon):\n            for subpolygon in geometry: #if geometry is multipolygon, go through each constituent subpolygon\n                patches.append(PolygonPatch(subpolygon))\n    pc = PatchCollection(patches, facecolor=color, edgecolor=color, linewidth=0, alpha=1)\n    ax.add_collection(pc)\n\n    if set_bounds:\n        if bbox is None:\n            # set the figure bounds to the polygons' bounds\n            left, bottom, right, top = gdf.total_bounds\n        else:\n            top, bottom, right, left = bbox\n        ax.set_xlim((left, right))\n        ax.set_ylim((bottom, top))\n\n    # turn off the axis display set the margins to zero and point the ticks in\n    # so there's no space around the plot\n    ax.axis('off')\n    ax.margins(0)\n    ax.tick_params(which='both', direction='in')\n    fig.canvas.draw()\n\n    # make everything square\n    ax.set_aspect('equal')\n    fig.canvas.draw()\n\n    fig, ax = save_and_show(fig=fig, ax=ax, save=save, show=show, close=close,\n                            filename=filename, file_format=file_format, dpi=dpi, axis_off=True)\n\n    return fig, ax", "response": "Plots a GeoDataFrame of footprints."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_node_elevations(G, api_key, max_locations_per_batch=350,\n                        pause_duration=0.02): # pragma: no cover\n    \"\"\"\n    Get the elevation (meters) of each node in the network and add it to the\n    node as an attribute.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n    api_key : string\n        your google maps elevation API key\n    max_locations_per_batch : int\n        max number of coordinate pairs to submit in each API call (if this is\n        too high, the server will reject the request because its character\n        limit exceeds the max)\n    pause_duration : float\n        time to pause between API calls\n\n    Returns\n    -------\n    G : networkx multidigraph\n    \"\"\"\n\n    # google maps elevation API endpoint\n    url_template = 'https://maps.googleapis.com/maps/api/elevation/json?locations={}&key={}'\n\n    # make a pandas series of all the nodes' coordinates as 'lat,lng'\n    # round coorindates to 5 decimal places (approx 1 meter) to be able to fit\n    # in more locations per API call\n    node_points = pd.Series({node:'{:.5f},{:.5f}'.format(data['y'], data['x']) for node, data in G.nodes(data=True)})\n    log('Requesting node elevations from the API in {} calls.'.format(math.ceil(len(node_points) / max_locations_per_batch)))\n\n    # break the series of coordinates into chunks of size max_locations_per_batch\n    # API format is locations=lat,lng|lat,lng|lat,lng|lat,lng...\n    results = []\n    for i in range(0, len(node_points), max_locations_per_batch):\n        chunk = node_points.iloc[i : i + max_locations_per_batch]\n        locations = '|'.join(chunk)\n        url = url_template.format(locations, api_key)\n\n        # check if this request is already in the cache (if global use_cache=True)\n        cached_response_json = get_from_cache(url)\n        if cached_response_json is not None:\n            response_json = cached_response_json\n        else:\n            try:\n                # request the elevations from the API\n                log('Requesting node elevations: {}'.format(url))\n                time.sleep(pause_duration)\n                response = requests.get(url)\n                response_json = response.json()\n                save_to_cache(url, response_json)\n            except Exception as e:\n                log(e)\n                log('Server responded with {}: {}'.format(response.status_code, response.reason))\n\n        # append these elevation results to the list of all results\n        results.extend(response_json['results'])\n\n    # sanity check that all our vectors have the same number of elements\n    if not (len(results) == len(G.nodes()) == len(node_points)):\n        raise Exception('Graph has {} nodes but we received {} results from the elevation API.'.format(len(G.nodes()), len(results)))\n    else:\n        log('Graph has {} nodes and we received {} results from the elevation API.'.format(len(G.nodes()), len(results)))\n\n    # add elevation as an attribute to the nodes\n    df = pd.DataFrame(node_points, columns=['node_points'])\n    df['elevation'] = [result['elevation'] for result in results]\n    df['elevation'] = df['elevation'].round(3) # round to millimeter\n    nx.set_node_attributes(G, name='elevation', values=df['elevation'].to_dict())\n    log('Added elevation data to all nodes.')\n\n    return G", "response": "Add node elevations to the multidigraph G."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_edge_grades(G, add_absolute=True): # pragma: no cover\n\n    # for each edge, calculate the difference in elevation from origin to\n    # destination, then divide by edge length\n    for u, v, data in G.edges(keys=False, data=True):\n        elevation_change = G.nodes[v]['elevation'] - G.nodes[u]['elevation']\n        \n        # round to ten-thousandths decimal place\n        grade = round(elevation_change / data['length'], 4)\n        data['grade'] = grade\n        if add_absolute:\n            data['grade_abs'] = abs(grade)\n\n    log('Added grade data to all edges.')\n    return G", "response": "Adds the directed grade to each edge in the network and returns the resulting networkx multidigraph."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_gdf_shapefile(gdf, filename=None, folder=None):\n\n    if folder is None:\n        folder = settings.data_folder\n\n    if filename is None:\n        filename = make_shp_filename(gdf.gdf_name)\n\n    # give the save folder a filename subfolder to make the full path to the\n    # files\n    folder_path = os.path.join(folder, filename)\n\n    # make everything but geometry column a string\n    for col in [c for c in gdf.columns if not c == 'geometry']:\n        gdf[col] = gdf[col].fillna('').map(make_str)\n\n    # if the save folder does not already exist, create it with a filename\n    # subfolder\n    if not os.path.exists(folder_path):\n        os.makedirs(folder_path)\n    gdf.to_file(folder_path)\n\n    if not hasattr(gdf, 'gdf_name'):\n        gdf.gdf_name = 'unnamed'\n    log('Saved the GeoDataFrame \"{}\" as shapefile \"{}\"'.format(gdf.gdf_name, folder_path))", "response": "Save a GeoDataFrame of place shapes or footprints as an ESRI\n    shapefile."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save_graph_shapefile(G, filename='graph', folder=None, encoding='utf-8'):\n\n    start_time = time.time()\n    if folder is None:\n        folder = settings.data_folder\n\n    # convert directed graph G to an undirected graph for saving as a shapefile\n    G_save = get_undirected(G.copy())\n\n    # create a GeoDataFrame of the nodes and set CRS\n    nodes, data = zip(*G_save.nodes(data=True))\n    gdf_nodes = gpd.GeoDataFrame(list(data), index=nodes)\n    gdf_nodes.crs = G_save.graph['crs']\n\n    # create a geometry column then drop the x and y columns\n    gdf_nodes['geometry'] = gdf_nodes.apply(lambda row: Point(row['x'], row['y']), axis=1)\n    gdf_nodes = gdf_nodes.drop(['x', 'y'], axis=1)\n\n    # make everything but geometry column a string\n    for col in [c for c in gdf_nodes.columns if not c == 'geometry']:\n        gdf_nodes[col] = gdf_nodes[col].fillna('').map(make_str)\n\n    # create a list to hold our edges, then loop through each edge in the graph\n    edges = []\n    for u, v, key, data in G_save.edges(keys=True, data=True):\n\n        # for each edge, add key and all attributes in data dict to the\n        # edge_details\n        edge_details = {'key':key}\n        for attr_key in data:\n            edge_details[attr_key] = data[attr_key]\n\n        # if edge doesn't already have a geometry attribute, create one now\n        if 'geometry' not in data:\n            point_u = Point((G_save.nodes[u]['x'], G_save.nodes[u]['y']))\n            point_v = Point((G_save.nodes[v]['x'], G_save.nodes[v]['y']))\n            edge_details['geometry'] = LineString([point_u, point_v])\n\n        edges.append(edge_details)\n\n    # create a geodataframe from the list of edges and set the CRS\n    gdf_edges = gpd.GeoDataFrame(edges)\n    gdf_edges.crs = G_save.graph['crs']\n\n    # make everything but geometry column a string\n    for col in [c for c in gdf_edges.columns if not c == 'geometry']:\n        gdf_edges[col] = gdf_edges[col].fillna('').map(make_str)\n\n    # if the save folder does not already exist, create it with a filename\n    # subfolder\n    folder = os.path.join(folder, filename)\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n\n    # save the nodes and edges as separate ESRI shapefiles\n    gdf_nodes.to_file('{}/nodes'.format(folder), encoding=encoding)\n    gdf_edges.to_file('{}/edges'.format(folder), encoding=encoding)\n    log('Saved graph \"{}\" to disk as shapefiles at \"{}\" in {:,.2f} seconds'.format(G_save.name, folder, time.time()-start_time))", "response": "Save a networkx multidigraph G as ESRI shapefiles to disk."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsave a networkx graph as an OSM XML formatted file.", "response": "def save_graph_osm(G, node_tags=settings.osm_xml_node_tags,\n                   node_attrs=settings.osm_xml_node_attrs,\n                   edge_tags=settings.osm_xml_way_tags,\n                   edge_attrs=settings.osm_xml_way_attrs,\n                   oneway=True, filename='graph.osm',\n                   folder=None):\n    \"\"\"\n    Save a graph as an OSM XML formatted file. NOTE: for very large\n    networks this method can take upwards of 30+ minutes to finish.\n\n    Parameters\n    __________\n    G : networkx multidigraph or multigraph\n    filename : string\n        the name of the osm file (including file extension)\n    folder : string\n        the folder to contain the file, if None, use default data folder\n\n    Returns\n    -------\n    None\n    \"\"\"\n    start_time = time.time()\n    if folder is None:\n        folder = settings.data_folder\n\n    # create a copy to convert all the node/edge attribute values to string\n    G_save = G.copy()\n\n    gdf_nodes, gdf_edges = graph_to_gdfs(\n        G_save, node_geometry=False, fill_edge_geometry=False)\n\n    # rename columns per osm specification\n    gdf_nodes.rename(\n        columns={'osmid': 'id', 'x': 'lon', 'y': 'lat'}, inplace=True)\n    if 'uniqueid' in gdf_edges.columns:\n        gdf_edges = gdf_edges.rename(columns={'uniqueid': 'id'})\n    else:\n        gdf_edges = gdf_edges.reset_index().rename(columns={'index': 'id'})\n\n    # add default values for required attributes\n    for table in [gdf_nodes, gdf_edges]:\n        table['uid'] = '1'\n        table['user'] = 'osmnx'\n        table['version'] = '1'\n        table['changeset'] = '1'\n        table['timestamp'] = '2017-01-01T00:00:00Z'\n\n    # convert all datatypes to str\n    nodes = gdf_nodes.applymap(str)\n    edges = gdf_edges.applymap(str)\n\n    # misc. string replacements to meet OSM XML spec\n    if 'oneway' in edges.columns:\n        edges.loc[:, 'oneway'] = oneway\n        edges.loc[:, 'oneway'] = edges['oneway'].astype(str)\n        edges.loc[:, 'oneway'] = edges['oneway'].str.replace(\n            'False', 'no').replace('True', 'yes')\n\n    # initialize XML tree with an OSM root element\n    root = etree.Element('osm')\n\n    # append nodes to the XML tree\n    for i, row in nodes.iterrows():\n        node = etree.SubElement(\n            root, 'node', attrib=row[node_attrs].dropna().to_dict())\n        for tag in node_tags:\n            etree.SubElement(\n                node, 'tag', attrib={'k': tag, 'v': row[tag]})\n\n    # append edges to the XML tree\n    for i, row in edges.iterrows():\n        edge = etree.SubElement(\n            root, 'way', attrib=row[edge_attrs].dropna().to_dict())\n        etree.SubElement(edge, 'nd', attrib={'ref': row['u']})\n        etree.SubElement(edge, 'nd', attrib={'ref': row['v']})\n        for tag in edge_tags:\n            etree.SubElement(\n                edge, 'tag', attrib={'k': tag, 'v': row[tag]})\n\n    et = etree.ElementTree(root)\n\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n\n    et.write(os.path.join(folder, filename))\n\n    log('Saved graph \"{}\" to disk as OSM at \"{}\" in {:,.2f} seconds'.format(\n        G_save.name, os.path.join(folder, filename), time.time() - start_time))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves a networkx multidigraph to disk.", "response": "def save_graphml(G, filename='graph.graphml', folder=None, gephi=False):\n    \"\"\"\n    Save graph as GraphML file to disk.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n    filename : string\n        the name of the graphml file (including file extension)\n    folder : string\n        the folder to contain the file, if None, use default data folder\n    gephi : bool\n        if True, give each edge a unique key to work around Gephi's\n        restrictive interpretation of the GraphML specification\n\n    Returns\n    -------\n    None\n    \"\"\"\n\n    start_time = time.time()\n    if folder is None:\n        folder = settings.data_folder\n\n    # create a copy to convert all the node/edge attribute values to string\n    G_save = G.copy()\n\n    if gephi:\n        \n        gdf_nodes, gdf_edges = graph_to_gdfs(G_save, nodes=True, edges=True, node_geometry=True,\n                                             fill_edge_geometry=True)\n        \n        # turn each edge's key into a unique ID for Gephi compatibility\n        gdf_edges['key'] = range(len(gdf_edges))\n        \n        # gephi doesn't handle node attrs named x and y well, so rename\n        gdf_nodes['xcoord'] = gdf_nodes['x']\n        gdf_nodes['ycoord'] = gdf_nodes['y']\n        G_save = gdfs_to_graph(gdf_nodes, gdf_edges)\n\n        # remove graph attributes as Gephi only accepts node and edge attrs\n        G_save.graph = {}\n\n    else:\n        # if not gephi, keep graph attrs and stringify them\n        for dict_key in G_save.graph:\n            # convert all the graph attribute values to strings\n            G_save.graph[dict_key] = make_str(G_save.graph[dict_key])\n\n    # stringify node and edge attributes\n    for _, data in G_save.nodes(data=True):\n        for dict_key in data:\n            if gephi and dict_key in ['xcoord', 'ycoord']:\n                # don't convert x y values to string if saving for gephi\n                continue\n            else:\n                # convert all the node attribute values to strings\n                data[dict_key] = make_str(data[dict_key])\n    \n    for _, _, data in G_save.edges(keys=False, data=True):\n        for dict_key in data:\n            # convert all the edge attribute values to strings\n            data[dict_key] = make_str(data[dict_key])\n\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n\n    nx.write_graphml(G_save, os.path.join(folder, filename))\n    log('Saved graph \"{}\" to disk as GraphML at \"{}\" in {:,.2f} seconds'.format(G_save.name, os.path.join(folder, filename), time.time()-start_time))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload a GraphML file from disk and convert the node and edge attributes to correct data types.", "response": "def load_graphml(filename, folder=None, node_type=int):\n    \"\"\"\n    Load a GraphML file from disk and convert the node/edge attributes to\n    correct data types.\n\n    Parameters\n    ----------\n    filename : string\n        the name of the graphml file (including file extension)\n    folder : string\n        the folder containing the file, if None, use default data folder\n    node_type : type\n        (Python type (default: int)) - Convert node ids to this type\n\n    Returns\n    -------\n    networkx multidigraph\n    \"\"\"\n    start_time = time.time()\n\n    # read the graph from disk\n    if folder is None:\n        folder = settings.data_folder\n    path = os.path.join(folder, filename)\n    G = nx.MultiDiGraph(nx.read_graphml(path, node_type=node_type))\n\n    # convert graph crs attribute from saved string to correct dict data type\n    G.graph['crs'] = ast.literal_eval(G.graph['crs'])\n\n    if 'streets_per_node' in G.graph:\n        G.graph['streets_per_node'] = ast.literal_eval(G.graph['streets_per_node'])\n\n    # convert numeric node tags from string to numeric data types\n    log('Converting node and edge attribute data types')\n    for _, data in G.nodes(data=True):\n        data['osmid'] = node_type(data['osmid'])\n        data['x'] = float(data['x'])\n        data['y'] = float(data['y'])\n\n    # convert numeric, bool, and list node tags from string to correct data types\n    for _, _, data in G.edges(data=True, keys=False):\n\n        # first parse oneway to bool and length to float - they should always\n        # have only 1 value each\n        data['oneway'] = ast.literal_eval(data['oneway'])\n        data['length'] = float(data['length'])\n\n        # these attributes might have a single value, or a list if edge's\n        # topology was simplified\n        for attr in ['highway', 'name', 'bridge', 'tunnel', 'lanes', 'ref', 'maxspeed', 'service', 'access', 'area', 'landuse', 'width', 'est_width']:\n            # if this edge has this attribute, and it starts with '[' and ends\n            # with ']', then it's a list to be parsed\n            if attr in data and data[attr][0] == '[' and data[attr][-1] == ']':\n                # try to convert the string list to a list type, else leave as\n                # single-value string (and leave as string if error)\n                try:\n                    data[attr] = ast.literal_eval(data[attr])\n                except:\n                    pass\n\n        # osmid might have a single value or a list\n        if 'osmid' in data:\n            if data['osmid'][0] == '[' and data['osmid'][-1] == ']':\n                # if it's a list, eval the list then convert each element to node_type\n                data['osmid'] = [node_type(i) for i in ast.literal_eval(data['osmid'])]\n            else:\n                # if it's not a list, convert it to the node_type\n                data['osmid'] = node_type(data['osmid'])\n\n        # if geometry attribute exists, load the string as well-known text to\n        # shapely LineString\n        if 'geometry' in data:\n            data['geometry'] = wkt.loads(data['geometry'])\n\n    # remove node_default and edge_default metadata keys if they exist\n    if 'node_default' in G.graph:\n        del G.graph['node_default']\n    if 'edge_default' in G.graph:\n        del G.graph['edge_default']\n\n    log('Loaded graph with {:,} nodes and {:,} edges in {:,.2f} seconds from \"{}\"'.format(len(list(G.nodes())),\n                                                                                          len(list(G.edges())),\n                                                                                          time.time()-start_time,\n                                                                                          path))\n    return G"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_duplicate_edge(data, data_other):\n\n    is_dupe = False\n\n    # if either edge's OSM ID contains multiple values (due to simplification), we want\n    # to compare as sets so they are order-invariant, otherwise uv does not match vu\n    osmid = set(data['osmid']) if isinstance(data['osmid'], list) else data['osmid']\n    osmid_other = set(data_other['osmid']) if isinstance(data_other['osmid'], list) else data_other['osmid']\n\n    if osmid == osmid_other:\n        # if they contain the same OSM ID or set of OSM IDs (due to simplification)\n        if ('geometry' in data) and ('geometry' in data_other):\n            # if both edges have a geometry attribute\n            if is_same_geometry(data['geometry'], data_other['geometry']):\n                # if their edge geometries have the same coordinates\n                is_dupe = True\n        elif ('geometry' in data) and ('geometry' in data_other):\n            # if neither edge has a geometry attribute\n            is_dupe = True\n        else:\n            # if one edge has geometry attribute but the other doesn't, keep it\n            pass\n\n    return is_dupe", "response": "Checks if two edge data dictionaries are the same based on OSM ID and geometry attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if the geometries in two edges are the same in normal or reversed order of points.", "response": "def is_same_geometry(ls1, ls2):\n    \"\"\"\n    Check if LineString geometries in two edges are the same, in\n    normal or reversed order of points.\n\n    Parameters\n    ----------\n    ls1 : LineString\n        the first edge's geometry\n    ls2 : LineString\n        the second edge's geometry\n\n    Returns\n    -------\n    bool\n    \"\"\"\n\n    # extract geometries from each edge data dict\n    geom1 = [list(coords) for coords in ls1.xy]\n    geom2 = [list(coords) for coords in ls2.xy]\n\n    # reverse the first edge's list of x's and y's to look for a match in\n    # either order\n    geom1_r = [list(reversed(list(coords))) for coords in ls1.xy]\n\n    # if the edge's geometry matches its reverse's geometry in either order,\n    # return True\n    return (geom1 == geom2 or geom1_r == geom2)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the keys of edges that share a u v with another edge but differ in geometry.", "response": "def update_edge_keys(G):\n    \"\"\"\n    Update the keys of edges that share a u, v with another edge but differ in\n    geometry. For example, two one-way streets from u to v that bow away from\n    each other as separate streets, rather than opposite direction edges of a \n    single street.\n    \n    Parameters\n    ----------\n    G : networkx multidigraph\n\n    Returns\n    -------\n    networkx multigraph\n    \"\"\"\n\n    # identify all the edges that are duplicates based on a sorted combination\n    # of their origin, destination, and key. that is, edge uv will match edge vu\n    # as a duplicate, but only if they have the same key\n    edges = graph_to_gdfs(G, nodes=False, fill_edge_geometry=False)\n    edges['uvk'] = edges.apply(lambda row: '_'.join(sorted([str(row['u']), str(row['v'])]) + [str(row['key'])]), axis=1)\n    edges['dupe'] = edges['uvk'].duplicated(keep=False)\n    dupes = edges[edges['dupe']==True].dropna(subset=['geometry'])\n\n    different_streets = []\n    groups = dupes[['geometry', 'uvk', 'u', 'v', 'key', 'dupe']].groupby('uvk')\n    \n    # for each set of duplicate edges\n    for label, group in groups:\n        \n        # if there are more than 2 edges here, make sure to compare all\n        if len(group['geometry']) > 2:\n            l = group['geometry'].tolist()\n            l.append(l[0])\n            geom_pairs = list(zip(l[:-1], l[1:]))\n        # otherwise, just compare the first edge to the second edge\n        else:\n            geom_pairs = [(group['geometry'].iloc[0], group['geometry'].iloc[1])]\n        \n        # for each pair of edges to compare\n        for geom1, geom2 in geom_pairs:\n            # if they don't have the same geometry, flag them as different streets\n            if not is_same_geometry(geom1, geom2):\n                # add edge uvk, but not edge vuk, otherwise we'll iterate both their keys\n                # and they'll still duplicate each other at the end of this process\n                different_streets.append((group['u'].iloc[0], group['v'].iloc[0], group['key'].iloc[0]))\n\n    # for each unique different street, iterate its key + 1 so it's unique\n    for u, v, k in set(different_streets):\n        # filter out key if it appears in data dict as we'll pass it explicitly\n        attributes = {k:v for k, v in G[u][v][k].items() if k != 'key'}\n        G.add_edge(u, v, key=k+1, **attributes)\n        G.remove_edge(u, v, key=k)\n\n    return G"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a directed graph to an undirected graph that maintains parallel edges if geometries differ.", "response": "def get_undirected(G):\n    \"\"\"\n    Convert a directed graph to an undirected graph that maintains parallel\n    edges if geometries differ.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n\n    Returns\n    -------\n    networkx multigraph\n    \"\"\"\n\n    start_time = time.time()\n\n    # set from/to nodes before making graph undirected\n    G = G.copy()\n    for u, v, k, data in G.edges(keys=True, data=True):\n        G.edges[u, v, k]['from'] = u\n        G.edges[u, v, k]['to'] = v\n        \n        # add geometry if it doesn't already exist, to retain parallel\n        # edges' distinct geometries\n        if 'geometry' not in data:\n            point_u = Point((G.nodes[u]['x'], G.nodes[u]['y']))\n            point_v = Point((G.nodes[v]['x'], G.nodes[v]['y']))\n            data['geometry'] = LineString([point_u, point_v])\n\n    # update edge keys so we don't retain only one edge of sets of parallel edges\n    # when we convert from a multidigraph to a multigraph\n    G = update_edge_keys(G)\n\n    # now convert multidigraph to a multigraph, retaining all edges in both\n    # directions for now, as well as all graph attributes\n    H = nx.MultiGraph()\n    H.add_nodes_from(G.nodes(data=True))\n    H.add_edges_from(G.edges(keys=True, data=True))\n    H.graph = G.graph\n    H.name = G.name\n\n    # the previous operation added all directed edges from G as undirected\n    # edges in H. this means we have duplicate edges for every bi-directional\n    # street. so, look through the edges and remove any duplicates\n    duplicate_edges = []\n    for u, v, key, data in H.edges(keys=True, data=True):\n\n        # if we haven't already flagged this edge as a duplicate\n        if not (u, v, key) in duplicate_edges:\n\n            # look at every other edge between u and v, one at a time\n            for key_other in H[u][v]:\n\n                # don't compare this edge to itself\n                if not key_other == key:\n\n                    # compare the first edge's data to the second's to see if\n                    # they are duplicates\n                    data_other = H.edges[u, v, key_other]\n                    if is_duplicate_edge(data, data_other):\n\n                        # if they match up, flag the duplicate for removal\n                        duplicate_edges.append((u, v, key_other))\n\n    H.remove_edges_from(duplicate_edges)\n    log('Made undirected graph in {:,.2f} seconds'.format(time.time() - start_time))\n\n    return H"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a networkx multidigraph into a list of GeoDataFrames.", "response": "def graph_to_gdfs(G, nodes=True, edges=True, node_geometry=True, fill_edge_geometry=True):\n    \"\"\"\n    Convert a graph into node and/or edge GeoDataFrames\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n    nodes : bool\n        if True, convert graph nodes to a GeoDataFrame and return it\n    edges : bool\n        if True, convert graph edges to a GeoDataFrame and return it\n    node_geometry : bool\n        if True, create a geometry column from node x and y data\n    fill_edge_geometry : bool\n        if True, fill in missing edge geometry fields using origin and\n        destination nodes\n\n    Returns\n    -------\n    GeoDataFrame or tuple\n        gdf_nodes or gdf_edges or both as a tuple\n    \"\"\"\n\n    if not (nodes or edges):\n        raise ValueError('You must request nodes or edges, or both.')\n\n    to_return = []\n\n    if nodes:\n\n        start_time = time.time()\n\n        nodes, data = zip(*G.nodes(data=True))\n        gdf_nodes = gpd.GeoDataFrame(list(data), index=nodes)\n        if node_geometry:\n            gdf_nodes['geometry'] = gdf_nodes.apply(lambda row: Point(row['x'], row['y']), axis=1)\n        gdf_nodes.crs = G.graph['crs']\n        gdf_nodes.gdf_name = '{}_nodes'.format(G.graph['name'])\n\n        to_return.append(gdf_nodes)\n        log('Created GeoDataFrame \"{}\" from graph in {:,.2f} seconds'.format(gdf_nodes.gdf_name, time.time()-start_time))\n\n    if edges:\n\n        start_time = time.time()\n\n        # create a list to hold our edges, then loop through each edge in the\n        # graph\n        edges = []\n        for u, v, key, data in G.edges(keys=True, data=True):\n\n            # for each edge, add key and all attributes in data dict to the\n            # edge_details\n            edge_details = {'u':u, 'v':v, 'key':key}\n            for attr_key in data:\n                edge_details[attr_key] = data[attr_key]\n\n            # if edge doesn't already have a geometry attribute, create one now\n            # if fill_edge_geometry==True\n            if 'geometry' not in data:\n                if fill_edge_geometry:\n                    point_u = Point((G.nodes[u]['x'], G.nodes[u]['y']))\n                    point_v = Point((G.nodes[v]['x'], G.nodes[v]['y']))\n                    edge_details['geometry'] = LineString([point_u, point_v])\n                else:\n                    edge_details['geometry'] = np.nan\n\n            edges.append(edge_details)\n\n        # create a GeoDataFrame from the list of edges and set the CRS\n        gdf_edges = gpd.GeoDataFrame(edges)\n        gdf_edges.crs = G.graph['crs']\n        gdf_edges.gdf_name = '{}_edges'.format(G.graph['name'])\n\n        to_return.append(gdf_edges)\n        log('Created GeoDataFrame \"{}\" from graph in {:,.2f} seconds'.format(gdf_edges.gdf_name, time.time()-start_time))\n\n    if len(to_return) > 1:\n        return tuple(to_return)\n    else:\n        return to_return[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gdfs_to_graph(gdf_nodes, gdf_edges):\n\n    G = nx.MultiDiGraph()\n    G.graph['crs'] = gdf_nodes.crs\n    G.graph['name'] = gdf_nodes.gdf_name.rstrip('_nodes')\n\n    # add the nodes and their attributes to the graph\n    G.add_nodes_from(gdf_nodes.index)\n    attributes = gdf_nodes.to_dict()\n    for attribute_name in gdf_nodes.columns:\n        # only add this attribute to nodes which have a non-null value for it\n        attribute_values = {k:v for k, v in attributes[attribute_name].items() if pd.notnull(v)}\n        nx.set_node_attributes(G, name=attribute_name, values=attribute_values)\n\n    # add the edges and attributes that are not u, v, key (as they're added\n    # separately) or null\n    for _, row in gdf_edges.iterrows():\n        attrs = {}\n        for label, value in row.iteritems():\n            if (label not in ['u', 'v', 'key']) and (isinstance(value, list) or pd.notnull(value)):\n                attrs[label] = value\n        G.add_edge(row['u'], row['v'], key=row['key'], **attrs)\n\n    return G", "response": "Convert node and edge GeoDataFrames into a networkx graphx object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_shp_filename(place_name):\n    name_pieces = list(reversed(place_name.split(', ')))\n    filename = '-'.join(name_pieces).lower().replace(' ','_')\n    filename = re.sub('[^0-9a-zA-Z_-]+', '', filename)\n    return filename", "response": "Create a filename string from a place name string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef basic_stats(G, area=None, clean_intersects=False, tolerance=15,\n                circuity_dist='gc'):\n    \"\"\"\n    Calculate basic descriptive metric and topological stats for a graph.\n\n    For an unprojected lat-lng graph, tolerance and graph units should be in\n    degrees, and circuity_dist should be 'gc'. For a projected graph, tolerance\n    and graph units should be in meters (or similar) and circuity_dist should be\n    'euclidean'.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n    area : numeric\n        the area covered by the street network, in square meters (typically land\n        area); if none, will skip all density-based metrics\n    clean_intersects : bool\n        if True, calculate clean intersections count (and density, if area is\n        provided)\n    tolerance : numeric\n        tolerance value passed along if clean_intersects=True, see\n        clean_intersections() function documentation for details and usage\n    circuity_dist : str\n        'gc' or 'euclidean', how to calculate straight-line distances for\n        circuity measurement; use former for lat-lng networks and latter for\n        projected networks\n\n    Returns\n    -------\n    stats : dict\n        dictionary of network measures containing the following elements (some\n        keys may not be present, based on the arguments passed into the function):\n\n          - n = number of nodes in the graph\n          - m = number of edges in the graph\n          - k_avg = average node degree of the graph\n          - intersection_count = number of intersections in graph, that is,\n                nodes with >1 street emanating from them\n          - streets_per_node_avg = how many streets (edges in the undirected\n                representation of the graph) emanate from each node (ie,\n                intersection or dead-end) on average (mean)\n          - streets_per_node_counts = dict, with keys of number of streets\n                emanating from the node, and values of number of nodes with this\n                count\n          - streets_per_node_proportion = dict, same as previous, but as a\n                proportion of the total, rather than counts\n          - edge_length_total = sum of all edge lengths in the graph, in meters\n          - edge_length_avg = mean edge length in the graph, in meters\n          - street_length_total = sum of all edges in the undirected\n                representation of the graph\n          - street_length_avg = mean edge length in the undirected\n                representation of the graph, in meters\n          - street_segments_count = number of edges in the undirected\n                representation of the graph\n          - node_density_km = n divided by area in square kilometers\n          - intersection_density_km = intersection_count divided by area in\n                square kilometers\n          - edge_density_km = edge_length_total divided by area in square\n                kilometers\n          - street_density_km = street_length_total divided by area in square\n                kilometers\n          - circuity_avg = edge_length_total divided by the sum of the great\n                circle distances between the nodes of each edge\n          - self_loop_proportion = proportion of edges that have a single node\n                as its two endpoints (ie, the edge links nodes u and v, and u==v)\n          - clean_intersection_count = number of intersections in street\n                network, merging complex ones into single points\n          - clean_intersection_density_km = clean_intersection_count divided by\n                area in square kilometers\n    \"\"\"\n\n    sq_m_in_sq_km = 1e6 #there are 1 million sq meters in 1 sq km\n    G_undirected = None\n\n    # calculate the number of nodes, n, and the number of edges, m, in the graph\n    n = len(list(G.nodes()))\n    m = len(list(G.edges()))\n\n    # calculate the average degree of the graph\n    k_avg = 2 * m / n\n\n    if 'streets_per_node' in G.graph:\n        # get the degrees saved as a graph attribute (from an undirected\n        # representation of the graph). this is not the degree of the nodes in\n        # the directed graph, but rather represents the number of streets\n        # (unidirected edges) emanating from each node. see\n        # count_streets_per_node function.\n        streets_per_node = G.graph['streets_per_node']\n    else:\n        # count how many street segments emanate from each node in this graph\n        streets_per_node = count_streets_per_node(G)\n\n    # count number of intersections in graph, as nodes with >1 street emanating\n    # from them\n    node_ids = set(G.nodes())\n    intersection_count = len([True for node, count in streets_per_node.items() if (count > 1) and (node in node_ids)])\n\n    # calculate the average number of streets (unidirected edges) incident to\n    # each node\n    streets_per_node_avg = sum(streets_per_node.values()) / n\n\n    # create a dict where key = number of streets (unidirected edges) incident\n    # to each node, and value = how many nodes are of this number in the graph\n    streets_per_node_counts = {num:list(streets_per_node.values()).count(num) for num in range(max(streets_per_node.values()) + 1)}\n\n    # degree proportions: dict where key = each degree and value = what\n    # proportion of nodes are of this degree in the graph\n    streets_per_node_proportion = {num:count/n for num, count in streets_per_node_counts.items()}\n\n    # calculate the total and average edge lengths\n    edge_length_total = sum([d['length'] for u, v, d in G.edges(data=True)])\n    edge_length_avg = edge_length_total / m\n\n    # calculate the total and average street segment lengths (so, edges without\n    # double-counting two-way streets)\n    if G_undirected is None:\n        G_undirected = G.to_undirected(reciprocal=False)\n    street_length_total = sum([d['length'] for u, v, d in G_undirected.edges(data=True)])\n    street_segments_count = len(list(G_undirected.edges(keys=True)))\n    street_length_avg = street_length_total / street_segments_count\n\n    # calculate clean intersection counts\n    if clean_intersects:\n        clean_intersection_points = clean_intersections(G, tolerance=tolerance, dead_ends=False )\n        clean_intersection_count = len(clean_intersection_points)\n    else:\n        clean_intersection_count = None\n\n    # we can calculate density metrics only if area is not null\n    if area is not None:\n        area_km = area / sq_m_in_sq_km\n\n        # calculate node density as nodes per sq km\n        node_density_km = n / area_km\n\n        # calculate intersection density as nodes with >1 street emanating from\n        # them, per sq km\n        intersection_density_km = intersection_count / area_km\n\n        # calculate edge density as linear meters per sq km\n        edge_density_km = edge_length_total / area_km\n\n        # calculate street density as linear meters per sq km\n        street_density_km = street_length_total / area_km\n\n        if clean_intersects:\n            clean_intersection_density_km = clean_intersection_count / area_km\n        else:\n            clean_intersection_density_km = None\n    else:\n        # if area is None, then we cannot calculate density\n        node_density_km = None\n        intersection_density_km = None\n        edge_density_km = None\n        street_density_km = None\n        clean_intersection_density_km = None\n\n    # average circuity: sum of edge lengths divided by sum of straight-line\n    # distance between edge endpoints. first load all the edges origin and\n    # destination coordinates as a dataframe, then calculate the straight-line\n    # distance\n    coords = np.array([[G.nodes[u]['y'], G.nodes[u]['x'], G.nodes[v]['y'], G.nodes[v]['x']] for u, v, k in G.edges(keys=True)])\n    df_coords = pd.DataFrame(coords, columns=['u_y', 'u_x', 'v_y', 'v_x'])\n    if circuity_dist == 'gc':\n        gc_distances = great_circle_vec(lat1=df_coords['u_y'],\n                                        lng1=df_coords['u_x'],\n                                        lat2=df_coords['v_y'],\n                                        lng2=df_coords['v_x'])\n    elif circuity_dist == 'euclidean':\n        gc_distances = euclidean_dist_vec(y1=df_coords['u_y'],\n                                          x1=df_coords['u_x'],\n                                          y2=df_coords['v_y'],\n                                          x2=df_coords['v_x'])\n    else:\n        raise ValueError('circuity_dist must be \"gc\" or \"euclidean\"')\n\n    gc_distances = gc_distances.fillna(value=0)\n    try:\n        circuity_avg = edge_length_total / gc_distances.sum()\n    except ZeroDivisionError:\n        circuity_avg = np.nan\n\n    # percent of edges that are self-loops, ie both endpoints are the same node\n    self_loops = [True for u, v, k in G.edges(keys=True) if u == v]\n    self_loops_count = len(self_loops)\n    self_loop_proportion = self_loops_count / m\n\n    # assemble the results\n    stats = {'n':n,\n             'm':m,\n             'k_avg':k_avg,\n             'intersection_count':intersection_count,\n             'streets_per_node_avg':streets_per_node_avg,\n             'streets_per_node_counts':streets_per_node_counts,\n             'streets_per_node_proportion':streets_per_node_proportion,\n             'edge_length_total':edge_length_total,\n             'edge_length_avg':edge_length_avg,\n             'street_length_total':street_length_total,\n             'street_length_avg':street_length_avg,\n             'street_segments_count':street_segments_count,\n             'node_density_km':node_density_km,\n             'intersection_density_km':intersection_density_km,\n             'edge_density_km':edge_density_km,\n             'street_density_km':street_density_km,\n             'circuity_avg':circuity_avg,\n             'self_loop_proportion':self_loop_proportion,\n             'clean_intersection_count':clean_intersection_count,\n             'clean_intersection_density_km':clean_intersection_density_km}\n\n    # return the results\n    return stats", "response": "Calculates basic descriptive metric and topological stats for a single undirected network."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the extended stats and metrics for a single network.", "response": "def extended_stats(G, connectivity=False, anc=False, ecc=False, bc=False, cc=False):\n    \"\"\"\n    Calculate extended topological stats and metrics for a graph.\n\n    Many of these algorithms have an inherently high time complexity. Global\n    topological analysis of large complex networks is extremely time consuming\n    and may exhaust computer memory. Consider using function arguments to not\n    run metrics that require computation of a full matrix of paths if they\n    will not be needed.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n    connectivity : bool\n        if True, calculate node and edge connectivity\n    anc : bool\n        if True, calculate average node connectivity\n    ecc : bool\n        if True, calculate shortest paths, eccentricity, and topological metrics\n        that use eccentricity\n    bc : bool\n        if True, calculate node betweenness centrality\n    cc : bool\n        if True, calculate node closeness centrality\n\n    Returns\n    -------\n    stats : dict\n        dictionary of network measures containing the following elements (some\n        only calculated/returned optionally, based on passed parameters):\n\n          - avg_neighbor_degree\n          - avg_neighbor_degree_avg\n          - avg_weighted_neighbor_degree\n          - avg_weighted_neighbor_degree_avg\n          - degree_centrality\n          - degree_centrality_avg\n          - clustering_coefficient\n          - clustering_coefficient_avg\n          - clustering_coefficient_weighted\n          - clustering_coefficient_weighted_avg\n          - pagerank\n          - pagerank_max_node\n          - pagerank_max\n          - pagerank_min_node\n          - pagerank_min\n          - node_connectivity\n          - node_connectivity_avg\n          - edge_connectivity\n          - eccentricity\n          - diameter\n          - radius\n          - center\n          - periphery\n          - closeness_centrality\n          - closeness_centrality_avg\n          - betweenness_centrality\n          - betweenness_centrality_avg\n\n    \"\"\"\n\n    stats = {}\n    full_start_time = time.time()\n\n    # create a DiGraph from the MultiDiGraph, for those metrics that require it\n    G_dir = nx.DiGraph(G)\n\n    # create an undirected Graph from the MultiDiGraph, for those metrics that\n    # require it\n    G_undir = nx.Graph(G)\n\n    # get the largest strongly connected component, for those metrics that\n    # require strongly connected graphs\n    G_strong = get_largest_component(G, strongly=True)\n\n    # average degree of the neighborhood of each node, and average for the graph\n    avg_neighbor_degree = nx.average_neighbor_degree(G)\n    stats['avg_neighbor_degree'] = avg_neighbor_degree\n    stats['avg_neighbor_degree_avg'] = sum(avg_neighbor_degree.values())/len(avg_neighbor_degree)\n\n    # average weighted degree of the neighborhood of each node, and average for\n    # the graph\n    avg_weighted_neighbor_degree = nx.average_neighbor_degree(G, weight='length')\n    stats['avg_weighted_neighbor_degree'] = avg_weighted_neighbor_degree\n    stats['avg_weighted_neighbor_degree_avg'] = sum(avg_weighted_neighbor_degree.values())/len(avg_weighted_neighbor_degree)\n\n    # degree centrality for a node is the fraction of nodes it is connected to\n    degree_centrality = nx.degree_centrality(G)\n    stats['degree_centrality'] = degree_centrality\n    stats['degree_centrality_avg'] = sum(degree_centrality.values())/len(degree_centrality)\n\n    # calculate clustering coefficient for the nodes\n    stats['clustering_coefficient'] = nx.clustering(G_undir)\n\n    # average clustering coefficient for the graph\n    stats['clustering_coefficient_avg'] = nx.average_clustering(G_undir)\n\n    # calculate weighted clustering coefficient for the nodes\n    stats['clustering_coefficient_weighted'] = nx.clustering(G_undir, weight='length')\n\n    # average clustering coefficient (weighted) for the graph\n    stats['clustering_coefficient_weighted_avg'] = nx.average_clustering(G_undir, weight='length')\n\n    # pagerank: a ranking of the nodes in the graph based on the structure of\n    # the incoming links\n    pagerank = nx.pagerank(G_dir, weight='length')\n    stats['pagerank'] = pagerank\n\n    # node with the highest page rank, and its value\n    pagerank_max_node = max(pagerank, key=lambda x: pagerank[x])\n    stats['pagerank_max_node'] = pagerank_max_node\n    stats['pagerank_max'] = pagerank[pagerank_max_node]\n\n    # node with the lowest page rank, and its value\n    pagerank_min_node = min(pagerank, key=lambda x: pagerank[x])\n    stats['pagerank_min_node'] = pagerank_min_node\n    stats['pagerank_min'] = pagerank[pagerank_min_node]\n\n    # if True, calculate node and edge connectivity\n    if connectivity:\n        start_time = time.time()\n\n        # node connectivity is the minimum number of nodes that must be removed\n        # to disconnect G or render it trivial\n        stats['node_connectivity'] = nx.node_connectivity(G_strong)\n\n        # edge connectivity is equal to the minimum number of edges that must be\n        # removed to disconnect G or render it trivial\n        stats['edge_connectivity'] = nx.edge_connectivity(G_strong)\n        log('Calculated node and edge connectivity in {:,.2f} seconds'.format(time.time() - start_time))\n\n    # if True, calculate average node connectivity\n    if anc:\n        # mean number of internally node-disjoint paths between each pair of\n        # nodes in G, i.e., the expected number of nodes that must be removed to\n        # disconnect a randomly selected pair of non-adjacent nodes\n        start_time = time.time()\n        stats['node_connectivity_avg'] = nx.average_node_connectivity(G)\n        log('Calculated average node connectivity in {:,.2f} seconds'.format(time.time() - start_time))\n\n    # if True, calculate shortest paths, eccentricity, and topological metrics\n    # that use eccentricity\n    if ecc:\n        # precompute shortest paths between all nodes for eccentricity-based\n        # stats\n        start_time = time.time()\n        sp = {source:dict(nx.single_source_dijkstra_path_length(G_strong, source, weight='length')) for source in G_strong.nodes()}\n\n        log('Calculated shortest path lengths in {:,.2f} seconds'.format(time.time() - start_time))\n\n        # eccentricity of a node v is the maximum distance from v to all other\n        # nodes in G\n        eccentricity = nx.eccentricity(G_strong, sp=sp)\n        stats['eccentricity'] = eccentricity\n\n        # diameter is the maximum eccentricity\n        diameter = nx.diameter(G_strong, e=eccentricity)\n        stats['diameter'] = diameter\n\n        # radius is the minimum eccentricity\n        radius = nx.radius(G_strong, e=eccentricity)\n        stats['radius'] = radius\n\n        # center is the set of nodes with eccentricity equal to radius\n        center = nx.center(G_strong, e=eccentricity)\n        stats['center'] = center\n\n        # periphery is the set of nodes with eccentricity equal to the diameter\n        periphery = nx.periphery(G_strong, e=eccentricity)\n        stats['periphery'] = periphery\n\n    # if True, calculate node closeness centrality\n    if cc:\n        # closeness centrality of a node is the reciprocal of the sum of the\n        # shortest path distances from u to all other nodes\n        start_time = time.time()\n        closeness_centrality = nx.closeness_centrality(G, distance='length')\n        stats['closeness_centrality'] = closeness_centrality\n        stats['closeness_centrality_avg'] = sum(closeness_centrality.values())/len(closeness_centrality)\n        log('Calculated closeness centrality in {:,.2f} seconds'.format(time.time() - start_time))\n\n    # if True, calculate node betweenness centrality\n    if bc:\n        # betweenness centrality of a node is the sum of the fraction of\n        # all-pairs shortest paths that pass through node\n        start_time = time.time()\n        betweenness_centrality = nx.betweenness_centrality(G, weight='length')\n        stats['betweenness_centrality'] = betweenness_centrality\n        stats['betweenness_centrality_avg'] = sum(betweenness_centrality.values())/len(betweenness_centrality)\n        log('Calculated betweenness centrality in {:,.2f} seconds'.format(time.time() - start_time))\n\n    log('Calculated extended stats in {:,.2f} seconds'.format(time.time()-full_start_time))\n    return stats"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef backward_induction(ddp, T, v_term=None):\n    n = ddp.num_states\n    vs = np.empty((T+1, n))\n    sigmas = np.empty((T, n), dtype=int)\n\n    if v_term is None:\n        v_term = np.zeros(n)\n    vs[T, :] = v_term\n\n    for t in range(T, 0, -1):\n        ddp.bellman_operator(vs[t, :], Tv=vs[t-1, :], sigma=sigmas[t-1, :])\n\n    return vs, sigmas", "response": "r Solve by backward induction a : math:`T - finite horizon\n    discrete dynamic program with stationary reward transition and probability arrays and discount factor."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking whether s_indices and a_indices are sorted in lexicographic order.", "response": "def _has_sorted_sa_indices(s_indices, a_indices):\n    \"\"\"\n    Check whether `s_indices` and `a_indices` are sorted in\n    lexicographic order.\n\n    Parameters\n    ----------\n    s_indices, a_indices : ndarray(ndim=1)\n\n    Returns\n    -------\n    bool\n        Whether `s_indices` and `a_indices` are sorted.\n\n    \"\"\"\n    L = len(s_indices)\n    for i in range(L-1):\n        if s_indices[i] > s_indices[i+1]:\n            return False\n        if s_indices[i] == s_indices[i+1]:\n            if a_indices[i] >= a_indices[i+1]:\n                return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a_indptr array for the next state in the sequence s_indices.", "response": "def _generate_a_indptr(num_states, s_indices, out):\n    \"\"\"\n    Generate `a_indptr`; stored in `out`. `s_indices` is assumed to be\n    in sorted order.\n\n    Parameters\n    ----------\n    num_states : scalar(int)\n\n    s_indices : ndarray(int, ndim=1)\n\n    out : ndarray(int, ndim=1)\n        Length must be num_states+1.\n\n    \"\"\"\n    idx = 0\n    out[0] = 0\n    for s in range(num_states-1):\n        while(s_indices[idx] == s):\n            idx += 1\n        out[s+1] = idx\n    out[num_states] = len(s_indices)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_action_feasibility(self):\n        # Check that for every state, reward is finite for some action\n        R_max = self.s_wise_max(self.R)\n        if (R_max == -np.inf).any():\n            # First state index such that all actions yield -inf\n            s = np.where(R_max == -np.inf)[0][0]\n            raise ValueError(\n                'for every state the reward must be finite for some action: '\n                'violated for state {s}'.format(s=s)\n            )\n\n        if self._sa_pair:\n            # Check that for every state there is at least one action available\n            diff = np.diff(self.a_indptr)\n            if (diff == 0).any():\n                # First state index such that no action is available\n                s = np.where(diff == 0)[0][0]\n                raise ValueError(\n                    'for every state at least one action must be available: '\n                    'violated for state {s}'.format(s=s)\n                )", "response": "Checks that the action feasibility of the action is not in use."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_sa_pair_form(self, sparse=True):\n\n        if self._sa_pair:\n            return self\n        else:\n            s_ind, a_ind = np.where(self.R > - np.inf)\n            RL = self.R[s_ind, a_ind]\n            if sparse:\n                QL = sp.csr_matrix(self.Q[s_ind, a_ind])\n            else:\n                QL = self.Q[s_ind, a_ind]\n            return DiscreteDP(RL, QL, self.beta, s_ind, a_ind)", "response": "Convert this instance of DiscreteDP to SA - pair form."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_product_form(self):\n        if self._sa_pair:\n            ns = self.num_states\n            na = self.a_indices.max() + 1\n            R = np.full((ns, na), -np.inf)\n            R[self.s_indices, self.a_indices] = self.R\n            Q = np.zeros((ns, na, ns))\n            if self._sparse:\n                _fill_dense_Q(self.s_indices, self.a_indices,\n                              self.Q.toarray(), Q)\n            else:\n                _fill_dense_Q(self.s_indices, self.a_indices, self.Q, Q)\n            return DiscreteDP(R, Q, self.beta)\n        else:\n            return self", "response": "Convert this instance of DiscreteDP to the product form."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a policy vector sigma return the reward vector R_sigma and transition probability matrix Q_sigma.", "response": "def RQ_sigma(self, sigma):\n        \"\"\"\n        Given a policy `sigma`, return the reward vector `R_sigma` and\n        the transition probability matrix `Q_sigma`.\n\n        Parameters\n        ----------\n        sigma : array_like(int, ndim=1)\n            Policy vector, of length n.\n\n        Returns\n        -------\n        R_sigma : ndarray(float, ndim=1)\n            Reward vector for `sigma`, of length n.\n\n        Q_sigma : ndarray(float, ndim=2)\n            Transition probability matrix for `sigma`, of shape (n, n).\n\n        \"\"\"\n        if self._sa_pair:\n            sigma = np.asarray(sigma)\n            sigma_indices = np.empty(self.num_states, dtype=int)\n            _find_indices(self.a_indices, self.a_indptr, sigma,\n                          out=sigma_indices)\n            R_sigma, Q_sigma = self.R[sigma_indices], self.Q[sigma_indices]\n        else:\n            R_sigma = self.R[np.arange(self.num_states), sigma]\n            Q_sigma = self.Q[np.arange(self.num_states), sigma]\n\n        return R_sigma, Q_sigma"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a function that returns the T_sigma operator.", "response": "def T_sigma(self, sigma):\n        \"\"\"\n        Given a policy `sigma`, return the T_sigma operator.\n\n        Parameters\n        ----------\n        sigma : array_like(int, ndim=1)\n            Policy vector, of length n.\n\n        Returns\n        -------\n        callable\n            The T_sigma operator.\n\n        \"\"\"\n        R_sigma, Q_sigma = self.RQ_sigma(sigma)\n        return lambda v: R_sigma + self.beta * Q_sigma.dot(v)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the v - greedy policy.", "response": "def compute_greedy(self, v, sigma=None):\n        \"\"\"\n        Compute the v-greedy policy.\n\n        Parameters\n        ----------\n        v : array_like(float, ndim=1)\n            Value function vector, of length n.\n\n        sigma : ndarray(int, ndim=1), optional(default=None)\n            Optional output array for `sigma`.\n\n        Returns\n        -------\n        sigma : ndarray(int, ndim=1)\n            v-greedy policy vector, of length n.\n\n        \"\"\"\n        if sigma is None:\n            sigma = np.empty(self.num_states, dtype=int)\n        self.bellman_operator(v, sigma=sigma)\n        return sigma"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nevaluates the value of a policy.", "response": "def evaluate_policy(self, sigma):\n        \"\"\"\n        Compute the value of a policy.\n\n        Parameters\n        ----------\n        sigma : array_like(int, ndim=1)\n            Policy vector, of length n.\n\n        Returns\n        -------\n        v_sigma : ndarray(float, ndim=1)\n            Value vector of `sigma`, of length n.\n\n        \"\"\"\n        if self.beta == 1:\n            raise NotImplementedError(self._error_msg_no_discounting)\n\n        # Solve (I - beta * Q_sigma) v = R_sigma for v\n        R_sigma, Q_sigma = self.RQ_sigma(sigma)\n        b = R_sigma\n\n        A = self._I - self.beta * Q_sigma\n\n        v_sigma = self._lineq_solve(A, b)\n\n        return v_sigma"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef operator_iteration(self, T, v, max_iter, tol=None, *args, **kwargs):\n        # May be replaced with quantecon.compute_fixed_point\n        if max_iter <= 0:\n            return v, 0\n\n        for i in range(max_iter):\n            new_v = T(v, *args, **kwargs)\n            if tol is not None and np.abs(new_v - v).max() < tol:\n                v[:] = new_v\n                break\n            v[:] = new_v\n\n        num_iter = i + 1\n\n        return num_iter", "response": "Iteratively apply the operator T to v."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsolves the dynamic programming problem.", "response": "def solve(self, method='policy_iteration',\n              v_init=None, epsilon=None, max_iter=None, k=20):\n        \"\"\"\n        Solve the dynamic programming problem.\n\n        Parameters\n        ----------\n        method : str, optinal(default='policy_iteration')\n            Solution method, str in {'value_iteration', 'vi',\n            'policy_iteration', 'pi', 'modified_policy_iteration',\n            'mpi'}.\n\n        v_init : array_like(float, ndim=1), optional(default=None)\n            Initial value function, of length n. If None, `v_init` is\n            set such that v_init(s) = max_a r(s, a) for value iteration\n            and policy iteration; for modified policy iteration,\n            v_init(s) = min_(s_next, a) r(s_next, a)/(1 - beta) to guarantee\n            convergence.\n\n        epsilon : scalar(float), optional(default=None)\n            Value for epsilon-optimality. If None, the value stored in\n            the attribute `epsilon` is used.\n\n        max_iter : scalar(int), optional(default=None)\n            Maximum number of iterations. If None, the value stored in\n            the attribute `max_iter` is used.\n\n        k : scalar(int), optional(default=20)\n            Number of iterations for partial policy evaluation in\n            modified policy iteration (irrelevant for other methods).\n\n        Returns\n        -------\n        res : DPSolveResult\n            Optimization result represetned as a DPSolveResult. See\n            `DPSolveResult` for details.\n\n        \"\"\"\n        if method in ['value_iteration', 'vi']:\n            res = self.value_iteration(v_init=v_init,\n                                       epsilon=epsilon,\n                                       max_iter=max_iter)\n        elif method in ['policy_iteration', 'pi']:\n            res = self.policy_iteration(v_init=v_init,\n                                        max_iter=max_iter)\n        elif method in ['modified_policy_iteration', 'mpi']:\n            res = self.modified_policy_iteration(v_init=v_init,\n                                                 epsilon=epsilon,\n                                                 max_iter=max_iter,\n                                                 k=k)\n        else:\n            raise ValueError('invalid method')\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsolving the optimization problem by value iteration.", "response": "def value_iteration(self, v_init=None, epsilon=None, max_iter=None):\n        \"\"\"\n        Solve the optimization problem by value iteration. See the\n        `solve` method.\n\n        \"\"\"\n        if self.beta == 1:\n            raise NotImplementedError(self._error_msg_no_discounting)\n\n        if max_iter is None:\n            max_iter = self.max_iter\n        if epsilon is None:\n            epsilon = self.epsilon\n\n        try:\n            tol = epsilon * (1-self.beta) / (2*self.beta)\n        except ZeroDivisionError:  # Raised if beta = 0\n            tol = np.inf\n\n        v = np.empty(self.num_states)\n        if v_init is None:\n            self.s_wise_max(self.R, out=v)\n        else:\n            v[:] = v_init\n\n        # Storage array for self.bellman_operator\n        Tv = np.empty(self.num_states)\n\n        num_iter = self.operator_iteration(T=self.bellman_operator,\n                                           v=v, max_iter=max_iter, tol=tol,\n                                           Tv=Tv)\n        sigma = self.compute_greedy(v)\n\n        res = DPSolveResult(v=v,\n                            sigma=sigma,\n                            num_iter=num_iter,\n                            mc=self.controlled_mc(sigma),\n                            method='value iteration',\n                            epsilon=epsilon,\n                            max_iter=max_iter)\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsolving the optimization problem by policy iteration.", "response": "def policy_iteration(self, v_init=None, max_iter=None):\n        \"\"\"\n        Solve the optimization problem by policy iteration. See the\n        `solve` method.\n\n        \"\"\"\n        if self.beta == 1:\n            raise NotImplementedError(self._error_msg_no_discounting)\n\n        if max_iter is None:\n            max_iter = self.max_iter\n\n        # What for initial condition?\n        if v_init is None:\n            v_init = self.s_wise_max(self.R)\n\n        sigma = self.compute_greedy(v_init)\n        new_sigma = np.empty(self.num_states, dtype=int)\n\n        for i in range(max_iter):\n            # Policy evaluation\n            v_sigma = self.evaluate_policy(sigma)\n            # Policy improvement\n            self.compute_greedy(v_sigma, sigma=new_sigma)\n            if np.array_equal(new_sigma, sigma):\n                break\n            sigma[:] = new_sigma\n\n        num_iter = i + 1\n\n        res = DPSolveResult(v=v_sigma,\n                            sigma=sigma,\n                            num_iter=num_iter,\n                            mc=self.controlled_mc(sigma),\n                            method='policy iteration',\n                            max_iter=max_iter)\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsolves the optimization problem by modified policy iteration.", "response": "def modified_policy_iteration(self, v_init=None, epsilon=None,\n                                  max_iter=None, k=20):\n        \"\"\"\n        Solve the optimization problem by modified policy iteration. See\n        the `solve` method.\n\n        \"\"\"\n        if self.beta == 1:\n            raise NotImplementedError(self._error_msg_no_discounting)\n\n        if max_iter is None:\n            max_iter = self.max_iter\n        if epsilon is None:\n            epsilon = self.epsilon\n\n        def span(z):\n            return z.max() - z.min()\n\n        def midrange(z):\n            return (z.min() + z.max()) / 2\n\n        v = np.empty(self.num_states)\n        if v_init is None:\n            v[:] = self.R[self.R > -np.inf].min() / (1 - self.beta)\n        else:\n            v[:] = v_init\n\n        u = np.empty(self.num_states)\n        sigma = np.empty(self.num_states, dtype=int)\n\n        try:\n            tol = epsilon * (1-self.beta) / self.beta\n        except ZeroDivisionError:  # Raised if beta = 0\n            tol = np.inf\n\n        for i in range(max_iter):\n            # Policy improvement\n            self.bellman_operator(v, Tv=u, sigma=sigma)\n            diff = u - v\n            if span(diff) < tol:\n                v[:] = u + midrange(diff) * self.beta / (1 - self.beta)\n                break\n            # Partial policy evaluation with k iterations\n            self.operator_iteration(T=self.T_sigma(sigma), v=u, max_iter=k)\n            v[:] = u\n\n        num_iter = i + 1\n\n        res = DPSolveResult(v=v,\n                            sigma=sigma,\n                            num_iter=num_iter,\n                            mc=self.controlled_mc(sigma),\n                            method='modified policy iteration',\n                            epsilon=epsilon,\n                            max_iter=max_iter,\n                            k=k)\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the controlled Markov chain for a given policy vector sigma.", "response": "def controlled_mc(self, sigma):\n        \"\"\"\n        Returns the controlled Markov chain for a given policy `sigma`.\n\n        Parameters\n        ----------\n        sigma : array_like(int, ndim=1)\n            Policy vector, of length n.\n\n        Returns\n        -------\n        mc : MarkovChain\n            Controlled Markov chain.\n\n        \"\"\"\n        _, Q_sigma = self.RQ_sigma(sigma)\n        return MarkovChain(Q_sigma)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsmoothing the data in x using convolution with a window of requested size and type.", "response": "def smooth(x, window_len=7, window='hanning'):\n    \"\"\"\n    Smooth the data in x using convolution with a window of requested\n    size and type.\n\n    Parameters\n    ----------\n    x : array_like(float)\n        A flat NumPy array containing the data to smooth\n    window_len : scalar(int), optional\n        An odd integer giving the length of the window.  Defaults to 7.\n    window : string\n        A string giving the window type. Possible values are 'flat',\n        'hanning', 'hamming', 'bartlett' or 'blackman'\n\n    Returns\n    -------\n    array_like(float)\n        The smoothed values\n\n    Notes\n    -----\n    Application of the smoothing window at the top and bottom of x is\n    done by reflecting x around these points to extend it sufficiently\n    in each direction.\n\n    \"\"\"\n    if len(x) < window_len:\n        raise ValueError(\"Input vector length must be >= window length.\")\n\n    if window_len < 3:\n        raise ValueError(\"Window length must be at least 3.\")\n\n    if not window_len % 2:  # window_len is even\n        window_len += 1\n        print(\"Window length reset to {}\".format(window_len))\n\n    windows = {'hanning': np.hanning,\n               'hamming': np.hamming,\n               'bartlett': np.bartlett,\n               'blackman': np.blackman,\n               'flat': np.ones  # moving average\n               }\n\n    # === Reflect x around x[0] and x[-1] prior to convolution === #\n    k = int(window_len / 2)\n    xb = x[:k]   # First k elements\n    xt = x[-k:]  # Last k elements\n    s = np.concatenate((xb[::-1], x, xt[::-1]))\n\n    # === Select window values === #\n    if window in windows.keys():\n        w = windows[window](window_len)\n    else:\n        msg = \"Unrecognized window type '{}'\".format(window)\n        print(msg + \" Defaulting to hanning\")\n        w = windows['hanning'](window_len)\n\n    return np.convolve(w / w.sum(), s, mode='valid')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ar_periodogram(x, window='hanning', window_len=7):\n    # === run regression === #\n    x_lag = x[:-1]  # lagged x\n    X = np.array([np.ones(len(x_lag)), x_lag]).T  # add constant\n\n    y = np.array(x[1:])  # current x\n\n    beta_hat = np.linalg.solve(X.T @ X, X.T @ y)  # solve for beta hat\n    e_hat = y - X @ beta_hat  # compute residuals\n    phi = beta_hat[1]  # pull out phi parameter\n\n    # === compute periodogram on residuals === #\n    w, I_w = periodogram(e_hat, window=window, window_len=window_len)\n\n    # === recolor and return === #\n    I_w = I_w / np.abs(1 - phi * np.exp(1j * w))**2\n\n    return w, I_w", "response": "Compute periodograms at the current Fourier frequences at which periodogram is evaluated."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _support_enumeration_gen(payoff_matrices):\n    nums_actions = payoff_matrices[0].shape\n    n_min = min(nums_actions)\n\n    for k in range(1, n_min+1):\n        supps = (np.arange(0, k, 1, np.int_), np.empty(k, np.int_))\n        actions = (np.empty(k+1), np.empty(k+1))\n        A = np.empty((k+1, k+1))\n\n        while supps[0][-1] < nums_actions[0]:\n            supps[1][:] = np.arange(k)\n            while supps[1][-1] < nums_actions[1]:\n                if _indiff_mixed_action(\n                    payoff_matrices[0], supps[0], supps[1], A, actions[1]\n                ):\n                    if _indiff_mixed_action(\n                        payoff_matrices[1], supps[1], supps[0], A, actions[0]\n                    ):\n                        out = (np.zeros(nums_actions[0]),\n                               np.zeros(nums_actions[1]))\n                        for p, (supp, action) in enumerate(zip(supps,\n                                                               actions)):\n                            out[p][supp] = action[:-1]\n                        yield out\n                next_k_array(supps[1])\n            next_k_array(supps[0])", "response": "Main body of `support_enumeration_gen`.\n\n    Parameters\n    ----------\n    payoff_matrices : tuple(ndarray(float, ndim=2))\n        Tuple of payoff matrices, of shapes (m, n) and (n, m),\n        respectively.\n\n    Yields\n    ------\n    out : tuple(ndarray(float, ndim=1))\n        Tuple of Nash equilibrium mixed actions, of lengths m and n,\n        respectively."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _indiff_mixed_action(payoff_matrix, own_supp, opp_supp, A, out):\n    m = payoff_matrix.shape[0]\n    k = len(own_supp)\n\n    for i in range(k):\n        for j in range(k):\n            A[j, i] = payoff_matrix[own_supp[i], opp_supp[j]]  # transpose\n    A[:-1, -1] = 1\n    A[-1, :-1] = -1\n    A[-1, -1] = 0\n    out[:-1] = 0\n    out[-1] = 1\n\n    r = _numba_linalg_solve(A, out)\n    if r != 0:  # A: singular\n        return False\n    for i in range(k):\n        if out[i] <= 0:\n            return False\n    val = out[-1]\n\n    if k == m:\n        return True\n\n    own_supp_flags = np.zeros(m, np.bool_)\n    own_supp_flags[own_supp] = True\n\n    for i in range(m):\n        if not own_supp_flags[i]:\n            payoff = 0\n            for j in range(k):\n                payoff += payoff_matrix[i, opp_supp[j]] * out[j]\n            if payoff > val:\n                return False\n    return True", "response": "Compute the indifferent mixed action for a player."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pure2mixed(num_actions, action):\n    mixed_action = np.zeros(num_actions)\n    mixed_action[action] = 1\n    return mixed_action", "response": "Convert a pure action to the corresponding mixed action."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes the action set of a specific player.", "response": "def delete_action(self, action, player_idx=0):\n        \"\"\"\n        Return a new `Player` instance with the action(s) specified by\n        `action` deleted from the action set of the player specified by\n        `player_idx`. Deletion is not performed in place.\n\n        Parameters\n        ----------\n        action : scalar(int) or array_like(int)\n            Integer or array like of integers representing the action(s)\n            to be deleted.\n\n        player_idx : scalar(int), optional(default=0)\n            Index of the player to delete action(s) for.\n\n        Returns\n        -------\n        Player\n            Copy of `self` with the action(s) deleted as specified.\n\n        Examples\n        --------\n        >>> player = Player([[3, 0], [0, 3], [1, 1]])\n        >>> player\n        Player([[3, 0],\n                [0, 3],\n                [1, 1]])\n        >>> player.delete_action(2)\n        Player([[3, 0],\n                [0, 3]])\n        >>> player.delete_action(0, player_idx=1)\n        Player([[0],\n                [3],\n                [1]])\n\n        \"\"\"\n        payoff_array_new = np.delete(self.payoff_array, action, player_idx)\n        return Player(payoff_array_new)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an array of payoff values one for each own action given the profile of the opponents actions.", "response": "def payoff_vector(self, opponents_actions):\n        \"\"\"\n        Return an array of payoff values, one for each own action, given\n        a profile of the opponents' actions.\n\n        Parameters\n        ----------\n        opponents_actions : see `best_response`.\n\n        Returns\n        -------\n        payoff_vector : ndarray(float, ndim=1)\n            An array representing the player's payoff vector given the\n            profile of the opponents' actions.\n\n        \"\"\"\n        def reduce_last_player(payoff_array, action):\n            \"\"\"\n            Given `payoff_array` with ndim=M, return the payoff array\n            with ndim=M-1 fixing the last player's action to be `action`.\n\n            \"\"\"\n            if isinstance(action, numbers.Integral):  # pure action\n                return payoff_array.take(action, axis=-1)\n            else:  # mixed action\n                return payoff_array.dot(action)\n\n        if self.num_opponents == 1:\n            payoff_vector = \\\n                reduce_last_player(self.payoff_array, opponents_actions)\n        elif self.num_opponents >= 2:\n            payoff_vector = self.payoff_array\n            for i in reversed(range(self.num_opponents)):\n                payoff_vector = \\\n                    reduce_last_player(payoff_vector, opponents_actions[i])\n        else:  # Trivial case with self.num_opponents == 0\n            payoff_vector = self.payoff_array\n\n        return payoff_vector"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_best_response(self, own_action, opponents_actions, tol=None):\n        if tol is None:\n            tol = self.tol\n\n        payoff_vector = self.payoff_vector(opponents_actions)\n        payoff_max = payoff_vector.max()\n\n        if isinstance(own_action, numbers.Integral):\n            return payoff_vector[own_action] >= payoff_max - tol\n        else:\n            return np.dot(own_action, payoff_vector) >= payoff_max - tol", "response": "Return True if own_action is a best response to opponents_actions."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the best response action to opponents_actions.", "response": "def best_response(self, opponents_actions, tie_breaking='smallest',\n                      payoff_perturbation=None, tol=None, random_state=None):\n        \"\"\"\n        Return the best response action(s) to `opponents_actions`.\n\n        Parameters\n        ----------\n        opponents_actions : scalar(int) or array_like\n            A profile of N-1 opponents' actions, represented by either\n            scalar(int), array_like(float), array_like(int), or\n            array_like(array_like(float)). If N=2, then it must be a\n            scalar of integer (in which case it is treated as the\n            opponent's pure action) or a 1-dimensional array of floats\n            (in which case it is treated as the opponent's mixed\n            action). If N>2, then it must be an array of N-1 objects,\n            where each object must be an integer (pure action) or an\n            array of floats (mixed action).\n\n        tie_breaking : str, optional(default='smallest')\n            str in {'smallest', 'random', False}. Control how, or\n            whether, to break a tie (see Returns for details).\n\n        payoff_perturbation : array_like(float), optional(default=None)\n            Array of length equal to the number of actions of the player\n            containing the values (\"noises\") to be added to the payoffs\n            in determining the best response.\n\n        tol : scalar(float), optional(default=None)\n            Tolerance level used in determining best responses. If None,\n            default to the value of the `tol` attribute.\n\n        random_state : int or np.random.RandomState, optional\n            Random seed (integer) or np.random.RandomState instance to\n            set the initial state of the random number generator for\n            reproducibility. If None, a randomly initialized RandomState\n            is used. Relevant only when tie_breaking='random'.\n\n        Returns\n        -------\n        scalar(int) or ndarray(int, ndim=1)\n            If tie_breaking=False, returns an array containing all the\n            best response pure actions. If tie_breaking='smallest',\n            returns the best response action with the smallest index; if\n            tie_breaking='random', returns an action randomly chosen\n            from the best response actions.\n\n        \"\"\"\n        if tol is None:\n            tol = self.tol\n\n        payoff_vector = self.payoff_vector(opponents_actions)\n        if payoff_perturbation is not None:\n            try:\n                payoff_vector += payoff_perturbation\n            except TypeError:  # type mismatch\n                payoff_vector = payoff_vector + payoff_perturbation\n\n        best_responses = \\\n            np.where(payoff_vector >= payoff_vector.max() - tol)[0]\n\n        if tie_breaking == 'smallest':\n            return best_responses[0]\n        elif tie_breaking == 'random':\n            return self.random_choice(best_responses,\n                                      random_state=random_state)\n        elif tie_breaking is False:\n            return best_responses\n        else:\n            msg = \"tie_breaking must be one of 'smallest', 'random', or False\"\n            raise ValueError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a random action from the actions array.", "response": "def random_choice(self, actions=None, random_state=None):\n        \"\"\"\n        Return a pure action chosen randomly from `actions`.\n\n        Parameters\n        ----------\n        actions : array_like(int), optional(default=None)\n            An array of integers representing pure actions.\n\n        random_state : int or np.random.RandomState, optional\n            Random seed (integer) or np.random.RandomState instance to\n            set the initial state of the random number generator for\n            reproducibility. If None, a randomly initialized RandomState\n            is used.\n\n        Returns\n        -------\n        scalar(int)\n            If `actions` is given, returns an integer representing a\n            pure action chosen randomly from `actions`; if not, an\n            action is chosen randomly from the player's all actions.\n\n        \"\"\"\n        random_state = check_random_state(random_state)\n\n        if actions is not None:\n            n = len(actions)\n        else:\n            n = self.num_actions\n\n        if n == 1:\n            idx = 0\n        else:\n            idx = random_state.randint(n)\n\n        if actions is not None:\n            return actions[idx]\n        else:\n            return idx"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine if an action is strictly dominated by some mixed action.", "response": "def is_dominated(self, action, tol=None, method=None):\n        \"\"\"\n        Determine whether `action` is strictly dominated by some mixed\n        action.\n\n        Parameters\n        ----------\n        action : scalar(int)\n            Integer representing a pure action.\n\n        tol : scalar(float), optional(default=None)\n            Tolerance level used in determining domination. If None,\n            default to the value of the `tol` attribute.\n\n        method : str, optional(default=None)\n            If None, `lemke_howson` from `quantecon.game_theory` is used\n            to solve for a Nash equilibrium of an auxiliary zero-sum\n            game. If `method` is set to `'simplex'` or\n            `'interior-point'`, `scipy.optimize.linprog` is used with\n            the method as specified by `method`.\n\n        Returns\n        -------\n        bool\n            True if `action` is strictly dominated by some mixed action;\n            False otherwise.\n\n        \"\"\"\n        if tol is None:\n            tol = self.tol\n\n        payoff_array = self.payoff_array\n\n        if self.num_opponents == 0:\n            return payoff_array.max() > payoff_array[action] + tol\n\n        ind = np.ones(self.num_actions, dtype=bool)\n        ind[action] = False\n        D = payoff_array[ind]\n        D -= payoff_array[action]\n        if D.shape[0] == 0:  # num_actions == 1\n            return False\n        if self.num_opponents >= 2:\n            D.shape = (D.shape[0], np.prod(D.shape[1:]))\n\n        if method is None:\n            from .lemke_howson import lemke_howson\n            g_zero_sum = NormalFormGame([Player(D), Player(-D.T)])\n            NE = lemke_howson(g_zero_sum)\n            return NE[0] @ D @ NE[1] > tol\n        elif method in ['simplex', 'interior-point']:\n            from scipy.optimize import linprog\n            m, n = D.shape\n            A = np.empty((n+2, m+1))\n            A[:n, :m] = -D.T\n            A[:n, -1] = 1  # Slack variable\n            A[n, :m], A[n+1, :m] = 1, -1  # Equality constraint\n            A[n:, -1] = 0\n            b = np.empty(n+2)\n            b[:n] = 0\n            b[n], b[n+1] = 1, -1\n            c = np.zeros(m+1)\n            c[-1] = -1\n            res = linprog(c, A_ub=A, b_ub=b, method=method)\n            if res.success:\n                return res.x[-1] > tol\n            elif res.status == 2:  # infeasible\n                return False\n            else:  # pragma: no cover\n                msg = 'scipy.optimize.linprog returned {0}'.format(res.status)\n                raise RuntimeError(msg)\n        else:\n            raise ValueError('Unknown method {0}'.format(method))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dominated_actions(self, tol=None, method=None):\n        out = []\n        for action in range(self.num_actions):\n            if self.is_dominated(action, tol=tol, method=method):\n                out.append(action)\n        return out", "response": "Returns a list of actions that are strictly dominated by some mixed action."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_action(self, player_idx, action):\n        # Allow negative indexing\n        if -self.N <= player_idx < 0:\n            player_idx = player_idx + self.N\n\n        players_new = tuple(\n            player.delete_action(action, player_idx-i)\n            for i, player in enumerate(self.players)\n        )\n        return NormalFormGame(players_new)", "response": "Delete the action set of the player with the specified index."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_nash(self, action_profile, tol=None):\n        if self.N == 2:\n            for i, player in enumerate(self.players):\n                own_action, opponent_action = \\\n                    action_profile[i], action_profile[1-i]\n                if not player.is_best_response(own_action, opponent_action,\n                                               tol):\n                    return False\n\n        elif self.N >= 3:\n            for i, player in enumerate(self.players):\n                own_action = action_profile[i]\n                opponents_actions = \\\n                    tuple(action_profile[i+1:]) + tuple(action_profile[:i])\n\n                if not player.is_best_response(own_action, opponents_actions,\n                                               tol):\n                    return False\n\n        else:  # Trivial case with self.N == 1\n            if not self.players[0].is_best_response(action_profile[0], None,\n                                                    tol):\n                return False\n\n        return True", "response": "Return True if the action_profile is a Nash equilibrium."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mclennan_tourky(g, init=None, epsilon=1e-3, max_iter=200,\n                    full_output=False):\n    r\"\"\"\n    Find one mixed-action epsilon-Nash equilibrium of an N-player normal\n    form game by the fixed point computation algorithm by McLennan and\n    Tourky [1]_.\n\n    Parameters\n    ----------\n    g : NormalFormGame\n        NormalFormGame instance.\n\n    init : array_like(int or array_like(float, ndim=1)), optional\n        Initial action profile, an array of N objects, where each object\n        must be an iteger (pure action) or an array of floats (mixed\n        action). If None, default to an array of zeros (the zero-th\n        action for each player).\n\n    epsilon : scalar(float), optional(default=1e-3)\n        Value of epsilon-optimality.\n\n    max_iter : scalar(int), optional(default=100)\n        Maximum number of iterations.\n\n    full_output : bool, optional(default=False)\n        If False, only the computed Nash equilibrium is returned. If\n        True, the return value is `(NE, res)`, where `NE` is the Nash\n        equilibrium and `res` is a `NashResult` object.\n\n    Returns\n    -------\n    NE : tuple(ndarray(float, ndim=1))\n        Tuple of computed Nash equilibrium mixed actions.\n\n    res : NashResult\n        Object containing information about the computation. Returned\n        only when `full_output` is True. See `NashResult` for details.\n\n    Examples\n    --------\n    Consider the following version of 3-player \"anti-coordination\" game,\n    where action 0 is a safe action which yields payoff 1, while action\n    1 yields payoff :math:`v` if no other player plays 1 and payoff 0\n    otherwise:\n\n    >>> N = 3\n    >>> v = 2\n    >>> payoff_array = np.empty((2,)*n)\n    >>> payoff_array[0, :] = 1\n    >>> payoff_array[1, :] = 0\n    >>> payoff_array[1].flat[0] = v\n    >>> g = NormalFormGame((Player(payoff_array),)*N)\n    >>> print(g)\n    3-player NormalFormGame with payoff profile array:\n    [[[[ 1.,  1.,  1.],   [ 1.,  1.,  2.]],\n      [[ 1.,  2.,  1.],   [ 1.,  0.,  0.]]],\n     [[[ 2.,  1.,  1.],   [ 0.,  1.,  0.]],\n      [[ 0.,  0.,  1.],   [ 0.,  0.,  0.]]]]\n\n    This game has a unique symmetric Nash equilibrium, where the\n    equilibrium action is given by :math:`(p^*, 1-p^*)` with :math:`p^*\n    = 1/v^{1/(N-1)}`:\n\n    >>> p_star = 1/(v**(1/(N-1)))\n    >>> [p_star, 1 - p_star]\n    [0.7071067811865475, 0.29289321881345254]\n\n    Obtain an approximate Nash equilibrium of this game by\n    `mclennan_tourky`:\n\n    >>> epsilon = 1e-5  # Value of epsilon-optimality\n    >>> NE = mclennan_tourky(g, epsilon=epsilon)\n    >>> print(NE[0], NE[1], NE[2], sep='\\n')\n    [ 0.70710754  0.29289246]\n    [ 0.70710754  0.29289246]\n    [ 0.70710754  0.29289246]\n    >>> g.is_nash(NE, tol=epsilon)\n    True\n\n    Additional information is returned if `full_output` is set True:\n\n    >>> NE, res = mclennan_tourky(g, epsilon=epsilon, full_output=True)\n    >>> res.converged\n    True\n    >>> res.num_iter\n    18\n\n    References\n    ----------\n    .. [1] A. McLennan and R. Tourky, \"From Imitation Games to\n       Kakutani,\" 2006.\n\n    \"\"\"\n    try:\n        N = g.N\n    except:\n        raise TypeError('g must be a NormalFormGame')\n    if N < 2:\n        raise NotImplementedError('Not implemented for 1-player games')\n\n    if init is None:\n        init = (0,) * N\n    try:\n        l = len(init)\n    except TypeError:\n        raise TypeError('init must be array_like')\n    if l != N:\n        raise ValueError(\n            'init must be of length {N}'.format(N=N)\n        )\n\n    indptr = np.empty(N+1, dtype=int)\n    indptr[0] = 0\n    indptr[1:] = np.cumsum(g.nums_actions)\n    x_init = _flatten_action_profile(init, indptr)\n\n    is_approx_fp = lambda x: _is_epsilon_nash(x, g, epsilon, indptr)\n    x_star, converged, num_iter = \\\n        _compute_fixed_point_ig(_best_response_selection, x_init, max_iter,\n                                verbose=0, print_skip=1,\n                                is_approx_fp=is_approx_fp,\n                                g=g, indptr=indptr)\n    NE = _get_action_profile(x_star, indptr)\n\n    if not full_output:\n        return NE\n\n    res = NashResult(NE=NE,\n                     converged=converged,\n                     num_iter=num_iter,\n                     max_iter=max_iter,\n                     init=init,\n                     epsilon=epsilon)\n\n    return NE, res", "response": "r This function is used to compute mixed - action epsilon - Nash equilibriums of a normal - form game by McLennan and McTourky."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _is_epsilon_nash(x, g, epsilon, indptr=None):\n    if indptr is None:\n        indptr = np.empty(g.N+1, dtype=int)\n        indptr[0] = 0\n        indptr[1:] = np.cumsum(g.nums_actions)\n\n    action_profile = _get_action_profile(x, indptr)\n    return g.is_nash(action_profile, tol=epsilon)", "response": "Determines whether x is an epsilon - Nash equilibrium of g."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a tuple of mixed actions from a flattened action profile.", "response": "def _get_action_profile(x, indptr):\n    \"\"\"\n    Obtain a tuple of mixed actions from a flattened action profile.\n\n    Parameters\n    ----------\n    x : array_like(float, ndim=1)\n        Array of flattened mixed action profile of length equal to n_0 +\n        ... + n_N-1, where `out[indptr[i]:indptr[i+1]]` contains player\n        i's mixed action.\n\n    indptr : array_like(int, ndim=1)\n        Array of index pointers of length N+1, where `indptr[0] = 0` and\n        `indptr[i+1] = indptr[i] + n_i`.\n\n    Returns\n    -------\n    action_profile : tuple(ndarray(float, ndim=1))\n        Tuple of N mixed actions, each of length n_i.\n\n    \"\"\"\n    N = len(indptr) - 1\n    action_profile = tuple(x[indptr[i]:indptr[i+1]] for i in range(N))\n    return action_profile"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nflattens the given action profile into a single array of mixed actions.", "response": "def _flatten_action_profile(action_profile, indptr):\n    \"\"\"\n    Flatten the given action profile.\n\n    Parameters\n    ----------\n    action_profile : array_like(int or array_like(float, ndim=1))\n        Profile of actions of the N players, where each player i' action\n        is a pure action (int) or a mixed action (array_like of floats\n        of length n_i).\n\n    indptr : array_like(int, ndim=1)\n        Array of index pointers of length N+1, where `indptr[0] = 0` and\n        `indptr[i+1] = indptr[i] + n_i`.\n\n    Returns\n    -------\n    out : ndarray(float, ndim=1)\n        Array of flattened mixed action profile of length equal to n_0 +\n        ... + n_N-1, where `out[indptr[i]:indptr[i+1]]` contains player\n        i's mixed action.\n\n    \"\"\"\n    N = len(indptr) - 1\n    out = np.empty(indptr[-1])\n\n    for i in range(N):\n        if isinstance(action_profile[i], numbers.Integral):  # pure action\n            num_actions = indptr[i+1] - indptr[i]\n            mixed_action = pure2mixed(num_actions, action_profile[i])\n        else:  # mixed action\n            mixed_action = action_profile[i]\n        out[indptr[i]:indptr[i+1]] = mixed_action\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a NormalFormGame instance of a 2 - player non - zero sum Colonel", "response": "def blotto_game(h, t, rho, mu=0, random_state=None):\n    \"\"\"\n    Return a NormalFormGame instance of a 2-player non-zero sum Colonel\n    Blotto game (Hortala-Vallve and Llorente-Saguer, 2012), where the\n    players have an equal number `t` of troops to assign to `h` hills\n    (so that the number of actions for each player is equal to\n    (t+h-1) choose (h-1) = (t+h-1)!/(t!*(h-1)!)). Each player has a\n    value for each hill that he receives if he assigns strictly more\n    troops to the hill than his opponent (ties are broken uniformly at\n    random), where the values are drawn from a multivariate normal\n    distribution with covariance `rho`. Each player\u2019s payoff is the sum\n    of the values of the hills won by that player.\n\n    Parameters\n    ----------\n    h : scalar(int)\n        Number of hills.\n    t : scalar(int)\n        Number of troops.\n    rho : scalar(float)\n        Covariance of the players' values of each hill. Must be in\n        [-1, 1].\n    mu : scalar(float), optional(default=0)\n        Mean of the players' values of each hill.\n    random_state : int or np.random.RandomState, optional\n        Random seed (integer) or np.random.RandomState instance to set\n        the initial state of the random number generator for\n        reproducibility. If None, a randomly initialized RandomState is\n        used.\n\n    Returns\n    -------\n    g : NormalFormGame\n\n    Examples\n    --------\n    >>> g = blotto_game(2, 3, 0.5, random_state=1234)\n    >>> g.players[0]\n    Player([[-0.44861083, -1.08443468, -1.08443468, -1.08443468],\n            [ 0.18721302, -0.44861083, -1.08443468, -1.08443468],\n            [ 0.18721302,  0.18721302, -0.44861083, -1.08443468],\n            [ 0.18721302,  0.18721302,  0.18721302, -0.44861083]])\n    >>> g.players[1]\n    Player([[-1.20042463, -1.39708658, -1.39708658, -1.39708658],\n            [-1.00376268, -1.20042463, -1.39708658, -1.39708658],\n            [-1.00376268, -1.00376268, -1.20042463, -1.39708658],\n            [-1.00376268, -1.00376268, -1.00376268, -1.20042463]])\n\n    \"\"\"\n    actions = simplex_grid(h, t)\n    n = actions.shape[0]\n    payoff_arrays = tuple(np.empty((n, n)) for i in range(2))\n    mean = np.array([mu, mu])\n    cov = np.array([[1, rho], [rho, 1]])\n    random_state = check_random_state(random_state)\n    values = random_state.multivariate_normal(mean, cov, h)\n    _populate_blotto_payoff_arrays(payoff_arrays, actions, values)\n    g = NormalFormGame(\n        [Player(payoff_array) for payoff_array in payoff_arrays]\n    )\n    return g"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _populate_blotto_payoff_arrays(payoff_arrays, actions, values):\n    n, h = actions.shape\n    payoffs = np.empty(2)\n    for i in range(n):\n        for j in range(n):\n            payoffs[:] = 0\n            for k in range(h):\n                if actions[i, k] == actions[j, k]:\n                    for p in range(2):\n                        payoffs[p] += values[k, p] / 2\n                else:\n                    winner = np.int(actions[i, k] < actions[j, k])\n                    payoffs[winner] += values[k, winner]\n            payoff_arrays[0][i, j], payoff_arrays[1][j, i] = payoffs", "response": "Populate the ndarrays in the payoff_arrays with the payoff values of the current node."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ranking_game(n, steps=10, random_state=None):\n    payoff_arrays = tuple(np.empty((n, n)) for i in range(2))\n    random_state = check_random_state(random_state)\n\n    scores = random_state.randint(1, steps+1, size=(2, n))\n    scores.cumsum(axis=1, out=scores)\n\n    costs = np.empty((2, n-1))\n    costs[:] = random_state.randint(1, steps+1, size=(2, n-1))\n    costs.cumsum(axis=1, out=costs)\n    costs[:] /= (n * steps)\n\n    _populate_ranking_payoff_arrays(payoff_arrays, scores, costs)\n    g = NormalFormGame(\n        [Player(payoff_array) for payoff_array in payoff_arrays]\n    )\n    return g", "response": "Returns a ranking game for the given number of steps and random number generator."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _populate_ranking_payoff_arrays(payoff_arrays, scores, costs):\n    n = payoff_arrays[0].shape[0]\n    for p, payoff_array in enumerate(payoff_arrays):\n        payoff_array[0, :] = 0\n        for i in range(1, n):\n            for j in range(n):\n                payoff_array[i, j] = -costs[p, i-1]\n\n    prize = 1.\n    for i in range(n):\n        for j in range(n):\n            if scores[0, i] > scores[1, j]:\n                payoff_arrays[0][i, j] += prize\n            elif scores[0, i] < scores[1, j]:\n                payoff_arrays[1][j, i] += prize\n            else:\n                payoff_arrays[0][i, j] += prize / 2\n                payoff_arrays[1][j, i] += prize / 2", "response": "Populate the ndarrays in the ranking_payoff_arrays with the payoff values of\nApps that are given by the scores and costs."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a NormalFormGame instance of the 2 - player game introduced by the given nash .", "response": "def sgc_game(k):\n    \"\"\"\n    Return a NormalFormGame instance of the 2-player game introduced by\n    Sandholm, Gilpin, and Conitzer (2005), which has a unique Nash\n    equilibrium, where each player plays half of the actions with\n    positive probabilities. Payoffs are normalized so that the minimum\n    and the maximum payoffs are 0 and 1, respectively.\n\n    Parameters\n    ----------\n    k : scalar(int)\n        Positive integer determining the number of actions. The returned\n        game will have `4*k-1` actions for each player.\n\n    Returns\n    -------\n    g : NormalFormGame\n\n    Examples\n    --------\n    >>> g = sgc_game(2)\n    >>> g.players[0]\n    Player([[ 0.75,  0.5 ,  1.  ,  0.5 ,  0.5 ,  0.5 ,  0.5 ],\n            [ 1.  ,  0.75,  0.5 ,  0.5 ,  0.5 ,  0.5 ,  0.5 ],\n            [ 0.5 ,  1.  ,  0.75,  0.5 ,  0.5 ,  0.5 ,  0.5 ],\n            [ 0.  ,  0.  ,  0.  ,  0.75,  0.  ,  0.  ,  0.  ],\n            [ 0.  ,  0.  ,  0.  ,  0.  ,  0.75,  0.  ,  0.  ],\n            [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.75,  0.  ],\n            [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.75]])\n    >>> g.players[1]\n    Player([[ 0.75,  0.5 ,  1.  ,  0.5 ,  0.5 ,  0.5 ,  0.5 ],\n            [ 1.  ,  0.75,  0.5 ,  0.5 ,  0.5 ,  0.5 ,  0.5 ],\n            [ 0.5 ,  1.  ,  0.75,  0.5 ,  0.5 ,  0.5 ,  0.5 ],\n            [ 0.  ,  0.  ,  0.  ,  0.  ,  0.75,  0.  ,  0.  ],\n            [ 0.  ,  0.  ,  0.  ,  0.75,  0.  ,  0.  ,  0.  ],\n            [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.75],\n            [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.75,  0.  ]])\n\n    \"\"\"\n    payoff_arrays = tuple(np.empty((4*k-1, 4*k-1)) for i in range(2))\n    _populate_sgc_payoff_arrays(payoff_arrays)\n    g = NormalFormGame(\n        [Player(payoff_array) for payoff_array in payoff_arrays]\n    )\n    return g"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _populate_sgc_payoff_arrays(payoff_arrays):\n    n = payoff_arrays[0].shape[0]  # 4*k-1\n    m = (n+1)//2 - 1  # 2*k-1\n    for payoff_array in payoff_arrays:\n        for i in range(m):\n            for j in range(m):\n                payoff_array[i, j] = 0.75\n            for j in range(m, n):\n                payoff_array[i, j] = 0.5\n        for i in range(m, n):\n            for j in range(n):\n                payoff_array[i, j] = 0\n\n        payoff_array[0, m-1] = 1\n        payoff_array[0, 1] = 0.5\n        for i in range(1, m-1):\n            payoff_array[i, i-1] = 1\n            payoff_array[i, i+1] = 0.5\n        payoff_array[m-1, m-2] = 1\n        payoff_array[m-1, 0] = 0.5\n\n    k = (m+1)//2\n    for h in range(k):\n        i, j = m + 2*h, m + 2*h\n        payoff_arrays[0][i, j] = 0.75\n        payoff_arrays[0][i+1, j+1] = 0.75\n        payoff_arrays[1][j, i+1] = 0.75\n        payoff_arrays[1][j+1, i] = 0.75", "response": "Populate the ndarrays in the SGC game with the payoff values of the SGC game."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a NormalFormGame instance of the 2 - player win - lose game with the payoff of 0 and 1 for all of the k - subset nodes.", "response": "def tournament_game(n, k, random_state=None):\n    \"\"\"\n    Return a NormalFormGame instance of the 2-player win-lose game,\n    whose payoffs are either 0 or 1, introduced by Anbalagan et al.\n    (2013). Player 0 has n actions, which constitute the set of nodes\n    {0, ..., n-1}, while player 1 has n choose k actions, each\n    corresponding to a subset of k elements of the set of n nodes. Given\n    a randomly generated tournament graph on the n nodes, the payoff for\n    player 0 is 1 if, in the tournament, the node chosen by player 0\n    dominates all the nodes in the k-subset chosen by player 1. The\n    payoff for player 1 is 1 if player 1's k-subset contains player 0's\n    chosen node.\n\n    Parameters\n    ----------\n    n : scalar(int)\n        Number of nodes in the tournament graph.\n    k : scalar(int)\n        Size of subsets of nodes in the tournament graph.\n    random_state : int or np.random.RandomState, optional\n        Random seed (integer) or np.random.RandomState instance to set\n        the initial state of the random number generator for\n        reproducibility. If None, a randomly initialized RandomState is\n        used.\n\n    Returns\n    -------\n    g : NormalFormGame\n\n    Notes\n    -----\n    The actions of player 1 are ordered according to the combinatorial\n    number system [1]_, which is different from the order used in the\n    original library in C.\n\n    Examples\n    --------\n    >>> g = tournament_game(5, 2, random_state=1234)\n    >>> g.players[0]\n    Player([[ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n            [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n            [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n            [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n            [ 0.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.]])\n    >>> g.players[1]\n    Player([[ 1.,  1.,  0.,  0.,  0.],\n            [ 1.,  0.,  1.,  0.,  0.],\n            [ 0.,  1.,  1.,  0.,  0.],\n            [ 1.,  0.,  0.,  1.,  0.],\n            [ 0.,  1.,  0.,  1.,  0.],\n            [ 0.,  0.,  1.,  1.,  0.],\n            [ 1.,  0.,  0.,  0.,  1.],\n            [ 0.,  1.,  0.,  0.,  1.],\n            [ 0.,  0.,  1.,  0.,  1.],\n            [ 0.,  0.,  0.,  1.,  1.]])\n\n    References\n    ----------\n    .. [1] `Combinatorial number system\n       <https://en.wikipedia.org/wiki/Combinatorial_number_system>`_,\n       Wikipedia.\n\n    \"\"\"\n    m = scipy.special.comb(n, k, exact=True)\n    if m > np.iinfo(np.intp).max:\n        raise ValueError('Maximum allowed size exceeded')\n\n    payoff_arrays = tuple(np.zeros(shape) for shape in [(n, m), (m, n)])\n    tourn = random_tournament_graph(n, random_state=random_state)\n    indices, indptr = tourn.csgraph.indices, tourn.csgraph.indptr\n    _populate_tournament_payoff_array0(payoff_arrays[0], k, indices, indptr)\n    _populate_tournament_payoff_array1(payoff_arrays[1], k)\n    g = NormalFormGame(\n        [Player(payoff_array) for payoff_array in payoff_arrays]\n    )\n    return g"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _populate_tournament_payoff_array0(payoff_array, k, indices, indptr):\n    n = payoff_array.shape[0]\n    X = np.empty(k, dtype=np.int_)\n    a = np.empty(k, dtype=np.int_)\n    for i in range(n):\n        d = indptr[i+1] - indptr[i]\n        if d >= k:\n            for j in range(k):\n                a[j] = j\n            while a[-1] < d:\n                for j in range(k):\n                    X[j] = indices[indptr[i]+a[j]]\n                payoff_array[i, k_array_rank_jit(X)] = 1\n                a = next_k_array(a)", "response": "Populate payoff_array with payoff values for player 0 in the tournament game given a random tournament graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _populate_tournament_payoff_array1(payoff_array, k):\n    m = payoff_array.shape[0]\n    X = np.arange(k)\n    for j in range(m):\n        for i in range(k):\n            payoff_array[j, X[i]] = 1\n        X = next_k_array(X)", "response": "Populate payoff_array with the payoff values for player 1 in the tournament game."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a NormalFormGame instance of the 2 - player game unit vector game.", "response": "def unit_vector_game(n, avoid_pure_nash=False, random_state=None):\n    \"\"\"\n    Return a NormalFormGame instance of the 2-player game \"unit vector\n    game\" (Savani and von Stengel, 2016). Payoffs for player 1 are\n    chosen randomly from the [0, 1) range. For player 0, each column\n    contains exactly one 1 payoff and the rest is 0.\n\n    Parameters\n    ----------\n    n : scalar(int)\n        Number of actions.\n    avoid_pure_nash : bool, optional(default=False)\n        If True, player 0's payoffs will be placed in order to avoid\n        pure Nash equilibria. (If necessary, the payoffs for player 1\n        are redrawn so as not to have a dominant action.)\n    random_state : int or np.random.RandomState, optional\n        Random seed (integer) or np.random.RandomState instance to set\n        the initial state of the random number generator for\n        reproducibility. If None, a randomly initialized RandomState is\n        used.\n\n    Returns\n    -------\n    g : NormalFormGame\n\n    Examples\n    --------\n    >>> g = unit_vector_game(4, random_state=1234)\n    >>> g.players[0]\n    Player([[ 1.,  0.,  1.,  0.],\n            [ 0.,  0.,  0.,  1.],\n            [ 0.,  0.,  0.,  0.],\n            [ 0.,  1.,  0.,  0.]])\n    >>> g.players[1]\n    Player([[ 0.19151945,  0.62210877,  0.43772774,  0.78535858],\n            [ 0.77997581,  0.27259261,  0.27646426,  0.80187218],\n            [ 0.95813935,  0.87593263,  0.35781727,  0.50099513],\n            [ 0.68346294,  0.71270203,  0.37025075,  0.56119619]])\n\n    With `avoid_pure_nash=True`:\n\n    >>> g = unit_vector_game(4, avoid_pure_nash=True, random_state=1234)\n    >>> g.players[0]\n    Player([[ 1.,  1.,  0.,  0.],\n            [ 0.,  0.,  0.,  0.],\n            [ 0.,  0.,  1.,  1.],\n            [ 0.,  0.,  0.,  0.]])\n    >>> g.players[1]\n    Player([[ 0.19151945,  0.62210877,  0.43772774,  0.78535858],\n            [ 0.77997581,  0.27259261,  0.27646426,  0.80187218],\n            [ 0.95813935,  0.87593263,  0.35781727,  0.50099513],\n            [ 0.68346294,  0.71270203,  0.37025075,  0.56119619]])\n    >>> pure_nash_brute(g)\n    []\n\n    \"\"\"\n    random_state = check_random_state(random_state)\n    payoff_arrays = (np.zeros((n, n)), random_state.random_sample((n, n)))\n\n    if not avoid_pure_nash:\n        ones_ind = random_state.randint(n, size=n)\n        payoff_arrays[0][ones_ind, np.arange(n)] = 1\n    else:\n        if n == 1:\n            raise ValueError('Cannot avoid pure Nash with n=1')\n        maxes = payoff_arrays[1].max(axis=0)\n        is_suboptimal = payoff_arrays[1] < maxes\n        nums_suboptimal = is_suboptimal.sum(axis=1)\n\n        while (nums_suboptimal==0).any():\n            payoff_arrays[1][:] = random_state.random_sample((n, n))\n            payoff_arrays[1].max(axis=0, out=maxes)\n            np.less(payoff_arrays[1], maxes, out=is_suboptimal)\n            is_suboptimal.sum(axis=1, out=nums_suboptimal)\n\n        for i in range(n):\n            one_ind = random_state.randint(n)\n            while not is_suboptimal[i, one_ind]:\n                one_ind = random_state.randint(n)\n            payoff_arrays[0][one_ind, i] = 1\n\n    g = NormalFormGame(\n        [Player(payoff_array) for payoff_array in payoff_arrays]\n    )\n    return g"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef gth_solve(A, overwrite=False, use_jit=True):\n    A1 = np.array(A, dtype=float, copy=not overwrite, order='C')\n    # `order='C'` is for use with Numba <= 0.18.2\n    # See issue github.com/numba/numba/issues/1103\n\n    if len(A1.shape) != 2 or A1.shape[0] != A1.shape[1]:\n        raise ValueError('matrix must be square')\n\n    n = A1.shape[0]\n    x = np.zeros(n)\n\n    if use_jit:\n        _gth_solve_jit(A1, x)\n        return x\n\n    # if not using jit\n    # === Reduction === #\n    for k in range(n-1):\n        scale = np.sum(A1[k, k+1:n])\n        if scale <= 0:\n            # There is one (and only one) recurrent class contained in\n            # {0, ..., k};\n            # compute the solution associated with that recurrent class.\n            n = k+1\n            break\n        A1[k+1:n, k] /= scale\n\n        A1[k+1:n, k+1:n] += np.dot(A1[k+1:n, k:k+1], A1[k:k+1, k+1:n])\n\n    # === Backward substitution === #\n    x[n-1] = 1\n    for k in range(n-2, -1, -1):\n        x[k] = np.dot(x[k+1:n], A1[k+1:n, k])\n\n    # === Normalization === #\n    x /= np.sum(x)\n\n    return x", "response": "r Solve the Grassmann - Taksar - Heyman - GTH algorithm for a given irreducible markov transition matrix or generator matrix."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _csr_matrix_indices(S):\n    m, n = S.shape\n\n    for i in range(m):\n        for j in range(S.indptr[i], S.indptr[i+1]):\n            row_index, col_index = i, S.indices[j]\n            yield row_index, col_index", "response": "Generate the indices of nonzero entries of a csr_matrix S"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef random_tournament_graph(n, random_state=None):\n    random_state = check_random_state(random_state)\n    num_edges = n * (n-1) // 2\n    r = random_state.random_sample(num_edges)\n    row = np.empty(num_edges, dtype=int)\n    col = np.empty(num_edges, dtype=int)\n    _populate_random_tournament_row_col(n, r, row, col)\n    data = np.ones(num_edges, dtype=bool)\n    adj_matrix = sparse.coo_matrix((data, (row, col)), shape=(n, n))\n    return DiGraph(adj_matrix)", "response": "Generate a random tournament graph with n nodes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _populate_random_tournament_row_col(n, r, row, col):\n    k = 0\n    for i in range(n):\n        for j in range(i+1, n):\n            if r[k] < 0.5:\n                row[k], col[k] = i, j\n            else:\n                row[k], col[k] = j, i\n            k += 1", "response": "Populate row and col with random numbers in r."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind strongly connected components for this node.", "response": "def _find_scc(self):\n        \"\"\"\n        Set ``self._num_scc`` and ``self._scc_proj``\n        by calling ``scipy.sparse.csgraph.connected_components``:\n        * docs.scipy.org/doc/scipy/reference/sparse.csgraph.html\n        * github.com/scipy/scipy/blob/master/scipy/sparse/csgraph/_traversal.pyx\n\n        ``self._scc_proj`` is a list of length `n` that assigns to each node\n        the label of the strongly connected component to which it belongs.\n\n        \"\"\"\n        # Find the strongly connected components\n        self._num_scc, self._scc_proj = \\\n            csgraph.connected_components(self.csgraph, connection='strong')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _condensation_lil(self):\n        condensation_lil = sparse.lil_matrix(\n            (self.num_strongly_connected_components,\n             self.num_strongly_connected_components), dtype=bool\n        )\n\n        scc_proj = self.scc_proj\n        for node_from, node_to in _csr_matrix_indices(self.csgraph):\n            scc_from, scc_to = scc_proj[node_from], scc_proj[node_to]\n            if scc_from != scc_to:\n                condensation_lil[scc_from, scc_to] = True\n\n        return condensation_lil", "response": "Return the sparse matrix representation of the condensation digraph\n        in lil format."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind the sink SCC that is strongly connected to the other SCCs.", "response": "def _find_sink_scc(self):\n        \"\"\"\n        Set self._sink_scc_labels, which is a list containing the labels of\n        the strongly connected components.\n\n        \"\"\"\n        condensation_lil = self._condensation_lil()\n\n        # A sink SCC is a SCC such that none of its members is strongly\n        # connected to nodes in other SCCs\n        # Those k's such that graph_condensed_lil.rows[k] == []\n        self._sink_scc_labels = \\\n            np.where(np.logical_not(condensation_lil.rows))[0]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _compute_period(self):\n        # Degenerate graph with a single node (which is strongly connected)\n        # csgraph.reconstruct_path would raise an exception\n        # github.com/scipy/scipy/issues/4018\n        if self.n == 1:\n            if self.csgraph[0, 0] == 0:  # No edge: \"trivial graph\"\n                self._period = 1  # Any universally accepted definition?\n                self._cyclic_components_proj = np.zeros(self.n, dtype=int)\n                return None\n            else:  # Self loop\n                self._period = 1\n                self._cyclic_components_proj = np.zeros(self.n, dtype=int)\n                return None\n\n        if not self.is_strongly_connected:\n            raise NotImplementedError(\n                'Not defined for a non strongly-connected digraph'\n            )\n\n        if np.any(self.csgraph.diagonal() > 0):\n            self._period = 1\n            self._cyclic_components_proj = np.zeros(self.n, dtype=int)\n            return None\n\n        # Construct a breadth-first search tree rooted at 0\n        node_order, predecessors = \\\n            csgraph.breadth_first_order(self.csgraph, i_start=0)\n        bfs_tree_csr = \\\n            csgraph.reconstruct_path(self.csgraph, predecessors)\n\n        # Edges not belonging to tree_csr\n        non_bfs_tree_csr = self.csgraph - bfs_tree_csr\n        non_bfs_tree_csr.eliminate_zeros()\n\n        # Distance to 0\n        level = np.zeros(self.n, dtype=int)\n        for i in range(1, self.n):\n            level[node_order[i]] = level[predecessors[node_order[i]]] + 1\n\n        # Determine the period\n        d = 0\n        for node_from, node_to in _csr_matrix_indices(non_bfs_tree_csr):\n            value = level[node_from] - level[node_to] + 1\n            d = gcd(d, value)\n            if d == 1:\n                self._period = 1\n                self._cyclic_components_proj = np.zeros(self.n, dtype=int)\n                return None\n\n        self._period = d\n        self._cyclic_components_proj = level % d", "response": "Compute the period of the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef subgraph(self, nodes):\n        adj_matrix = self.csgraph[np.ix_(nodes, nodes)]\n\n        weighted = True  # To copy the dtype\n\n        if self.node_labels is not None:\n            node_labels = self.node_labels[nodes]\n        else:\n            node_labels = None\n\n        return DiGraph(adj_matrix, weighted=weighted, node_labels=node_labels)", "response": "Returns the subgraph consisting of the given nodes and edges between thses nodes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nestimating the rank of a matrix.", "response": "def rank_est(A, atol=1e-13, rtol=0):\n    \"\"\"\n    Estimate the rank (i.e. the dimension of the nullspace) of a matrix.\n\n    The algorithm used by this function is based on the singular value\n    decomposition of `A`.\n\n    Parameters\n    ----------\n    A : array_like(float, ndim=1 or 2)\n        A should be at most 2-D.  A 1-D array with length n will be\n        treated as a 2-D with shape (1, n)\n    atol : scalar(float), optional(default=1e-13)\n        The absolute tolerance for a zero singular value.  Singular\n        values smaller than `atol` are considered to be zero.\n    rtol : scalar(float), optional(default=0)\n        The relative tolerance.  Singular values less than rtol*smax are\n        considered to be zero, where smax is the largest singular value.\n\n    Returns\n    -------\n    r : scalar(int)\n        The estimated rank of the matrix.\n\n    Note: If both `atol` and `rtol` are positive, the combined tolerance\n    is the maximum of the two; that is:\n\n        tol = max(atol, rtol * smax)\n\n    Note: Singular values smaller than `tol` are considered to be zero.\n\n    See also\n    --------\n    numpy.linalg.matrix_rank\n        matrix_rank is basically the same as this function, but it does\n        not provide the option of the absolute tolerance.\n\n    \"\"\"\n\n    A = np.atleast_2d(A)\n    s = svd(A, compute_uv=False)\n    tol = max(atol, rtol * s[0])\n    rank = int((s >= tol).sum())\n\n    return rank"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate s_indices and a_indices for the DiscreteDP case.", "response": "def sa_indices(num_states, num_actions):\n    \"\"\"\n    Generate `s_indices` and `a_indices` for `DiscreteDP`, for the case\n    where all the actions are feasible at every state.\n\n    Parameters\n    ----------\n    num_states : scalar(int)\n        Number of states.\n\n    num_actions : scalar(int)\n        Number of actions.\n\n    Returns\n    -------\n    s_indices : ndarray(int, ndim=1)\n        Array containing the state indices.\n\n    a_indices : ndarray(int, ndim=1)\n        Array containing the action indices.\n\n    Examples\n    --------\n    >>> s_indices, a_indices = qe.markov.sa_indices(4, 3)\n    >>> s_indices\n    array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3])\n    >>> a_indices\n    array([0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2])\n\n    \"\"\"\n    L = num_states * num_actions\n    dtype = np.int_\n    s_indices = np.empty(L, dtype=dtype)\n    a_indices = np.empty(L, dtype=dtype)\n\n    i = 0\n    for s in range(num_states):\n        for a in range(num_actions):\n            s_indices[i] = s\n            a_indices[i] = a\n            i += 1\n\n    return s_indices, a_indices"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _generate_sample_paths(P_cdfs, init_states, random_values, out):\n    num_reps, ts_length = out.shape\n\n    for i in range(num_reps):\n        out[i, 0] = init_states[i]\n        for t in range(ts_length-1):\n            out[i, t+1] = searchsorted(P_cdfs[out[i, t]], random_values[i, t])", "response": "Generate sample paths for the next state transition."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating sample paths for sparse matrix.", "response": "def _generate_sample_paths_sparse(P_cdfs1d, indices, indptr, init_states,\n                                  random_values, out):\n    \"\"\"\n    For sparse matrix.\n\n    Generate num_reps sample paths of length ts_length, where num_reps =\n    out.shape[0] and ts_length = out.shape[1].\n\n    Parameters\n    ----------\n    P_cdfs1d : ndarray(float, ndim=1)\n        1D array containing the CDFs of the state transition.\n\n    indices : ndarray(int, ndim=1)\n        CSR format index array.\n\n    indptr : ndarray(int, ndim=1)\n        CSR format index pointer array.\n\n    init_states : array_like(int, ndim=1)\n        Array containing the initial states. Its length must be equal to\n        num_reps.\n\n    random_values : ndarray(float, ndim=2)\n        Array containing random values from [0, 1). Its shape must be\n        equal to (num_reps, ts_length-1)\n\n    out : ndarray(int, ndim=2)\n        Array to store the sample paths.\n\n    Notes\n    -----\n    This routine is jit-complied by Numba.\n\n    \"\"\"\n    num_reps, ts_length = out.shape\n\n    for i in range(num_reps):\n        out[i, 0] = init_states[i]\n        for t in range(ts_length-1):\n            k = searchsorted(P_cdfs1d[indptr[out[i, t]]:indptr[out[i, t]+1]],\n                             random_values[i, t])\n            out[i, t+1] = indices[indptr[out[i, t]]+k]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mc_sample_path(P, init=0, sample_size=1000, random_state=None):\n    random_state = check_random_state(random_state)\n\n    if isinstance(init, numbers.Integral):\n        X_0 = init\n    else:\n        cdf0 = np.cumsum(init)\n        u_0 = random_state.random_sample()\n        X_0 = searchsorted(cdf0, u_0)\n\n    mc = MarkovChain(P)\n    return mc.simulate(ts_length=sample_size, init=X_0,\n                       random_state=random_state)", "response": "Generates one sample path from the Markov chain represented by a Markov transition matrix P on state space S."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_index(self, value):\n        if self.state_values is None:\n            state_values_ndim = 1\n        else:\n            state_values_ndim = self.state_values.ndim\n\n        values = np.asarray(value)\n\n        if values.ndim <= state_values_ndim - 1:\n            return self._get_index(value)\n        elif values.ndim == state_values_ndim:  # array of values\n            k = values.shape[0]\n            idx = np.empty(k, dtype=int)\n            for i in range(k):\n                idx[i] = self._get_index(values[i])\n            return idx\n        else:\n            raise ValueError('invalid value')", "response": "Returns the index of the given value or values in the state_values array."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the index of the given value in state_values.", "response": "def _get_index(self, value):\n        \"\"\"\n        Return the index of the given value in `state_values`.\n\n        Parameters\n        ----------\n        value\n            Value to get the index for.\n\n        Returns\n        -------\n        idx : int\n            Index of `value`.\n\n        \"\"\"\n        error_msg = 'value {0} not found'.format(value)\n\n        if self.state_values is None:\n            if isinstance(value, numbers.Integral) and (0 <= value < self.n):\n                return value\n            else:\n                raise ValueError(error_msg)\n\n        # if self.state_values is not None:\n        if self.state_values.ndim == 1:\n            try:\n                idx = np.where(self.state_values == value)[0][0]\n                return idx\n            except IndexError:\n                raise ValueError(error_msg)\n        else:\n            idx = 0\n            while idx < self.n:\n                if np.array_equal(self.state_values[idx], value):\n                    return idx\n                idx += 1\n            raise ValueError(error_msg)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the stationary distributions for the current class.", "response": "def _compute_stationary(self):\n        \"\"\"\n        Store the stationary distributions in self._stationary_distributions.\n\n        \"\"\"\n        if self.is_irreducible:\n            if not self.is_sparse:  # Dense\n                stationary_dists = gth_solve(self.P).reshape(1, self.n)\n            else:  # Sparse\n                stationary_dists = \\\n                    gth_solve(self.P.toarray(),\n                              overwrite=True).reshape(1, self.n)\n        else:\n            rec_classes = self.recurrent_classes_indices\n            stationary_dists = np.zeros((len(rec_classes), self.n))\n            for i, rec_class in enumerate(rec_classes):\n                P_rec_class = self.P[np.ix_(rec_class, rec_class)]\n                if self.is_sparse:\n                    P_rec_class = P_rec_class.toarray()\n                stationary_dists[i, rec_class] = \\\n                    gth_solve(P_rec_class, overwrite=True)\n\n        self._stationary_dists = stationary_dists"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsimulates time series of state transitions where state indices are returned.", "response": "def simulate_indices(self, ts_length, init=None, num_reps=None,\n                         random_state=None):\n        \"\"\"\n        Simulate time series of state transitions, where state indices\n        are returned.\n\n        Parameters\n        ----------\n        ts_length : scalar(int)\n            Length of each simulation.\n\n        init : int or array_like(int, ndim=1), optional\n            Initial state(s). If None, the initial state is randomly\n            drawn.\n\n        num_reps : scalar(int), optional(default=None)\n            Number of repetitions of simulation.\n\n        random_state : int or np.random.RandomState, optional\n            Random seed (integer) or np.random.RandomState instance to\n            set the initial state of the random number generator for\n            reproducibility. If None, a randomly initialized RandomState\n            is used.\n\n        Returns\n        -------\n        X : ndarray(ndim=1 or 2)\n            Array containing the state values of the sample path(s). See\n            the `simulate` method for more information.\n\n        \"\"\"\n        random_state = check_random_state(random_state)\n        dim = 1  # Dimension of the returned array: 1 or 2\n\n        msg_out_of_range = 'index {init} is out of the state space'\n\n        try:\n            k = len(init)  # init is an array\n            dim = 2\n            init_states = np.asarray(init, dtype=int)\n            # Check init_states are in the state space\n            if (init_states >= self.n).any() or (init_states < -self.n).any():\n                idx = np.where(\n                    (init_states >= self.n) + (init_states < -self.n)\n                )[0][0]\n                raise ValueError(msg_out_of_range.format(init=idx))\n            if num_reps is not None:\n                k *= num_reps\n                init_states = np.tile(init_states, num_reps)\n        except TypeError:  # init is a scalar(int) or None\n            k = 1\n            if num_reps is not None:\n                dim = 2\n                k = num_reps\n            if init is None:\n                init_states = random_state.randint(self.n, size=k)\n            elif isinstance(init, numbers.Integral):\n                # Check init is in the state space\n                if init >= self.n or init < -self.n:\n                    raise ValueError(msg_out_of_range.format(init=init))\n                init_states = np.ones(k, dtype=int) * init\n            else:\n                raise ValueError(\n                    'init must be int, array_like of ints, or None'\n                )\n\n        # === set up array to store output === #\n        X = np.empty((k, ts_length), dtype=int)\n\n        # Random values, uniformly sampled from [0, 1)\n        random_values = random_state.random_sample(size=(k, ts_length-1))\n\n        # Generate sample paths and store in X\n        if not self.is_sparse:  # Dense\n            _generate_sample_paths(\n                self.cdfs, init_states, random_values, out=X\n            )\n        else:  # Sparse\n            _generate_sample_paths_sparse(\n                self.cdfs1d, self.P.indices, self.P.indptr, init_states,\n                random_values, out=X\n            )\n\n        if dim == 1:\n            return X[0]\n        else:\n            return X"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsimulate the time series of state transitions.", "response": "def simulate(self, ts_length, init=None, num_reps=None, random_state=None):\n        \"\"\"\n        Simulate time series of state transitions, where the states are\n        annotated with their values (if `state_values` is not None).\n\n        Parameters\n        ----------\n        ts_length : scalar(int)\n            Length of each simulation.\n\n        init : scalar or array_like, optional(default=None)\n            Initial state values(s). If None, the initial state is\n            randomly drawn.\n\n        num_reps : scalar(int), optional(default=None)\n            Number of repetitions of simulation.\n\n        random_state : int or np.random.RandomState, optional\n            Random seed (integer) or np.random.RandomState instance to\n            set the initial state of the random number generator for\n            reproducibility. If None, a randomly initialized RandomState\n            is used.\n\n        Returns\n        -------\n        X : ndarray(ndim=1 or 2)\n            Array containing the sample path(s), of shape (ts_length,)\n            if init is a scalar (integer) or None and num_reps is None;\n            of shape (k, ts_length) otherwise, where k = len(init) if\n            (init, num_reps) = (array, None), k = num_reps if (init,\n            num_reps) = (int or None, int), and k = len(init)*num_reps\n            if (init, num_reps) = (array, int).\n\n        \"\"\"\n        if init is not None:\n            init_idx = self.get_index(init)\n        else:\n            init_idx = None\n        X = self.simulate_indices(ts_length, init=init_idx, num_reps=num_reps,\n                                  random_state=random_state)\n\n        # Annotate states\n        if self.state_values is not None:\n            X = self.state_values[X]\n\n        return X"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the stationary values of the control set entry for the current entry.", "response": "def stationary_values(self, method='doubling'):\n        \"\"\"\n        Computes the matrix :math:`P` and scalar :math:`d` that represent\n        the value function\n\n        .. math::\n\n             V(x) = x' P x + d\n\n        in the infinite horizon case.  Also computes the control matrix\n        :math:`F` from :math:`u = - Fx`. Computation is via the solution\n        algorithm as specified by the `method` option (default to the\n        doubling algorithm) (see the documentation in\n        `matrix_eqn.solve_discrete_riccati`).\n\n        Parameters\n        ----------\n        method : str, optional(default='doubling')\n            Solution method used in solving the associated Riccati\n            equation, str in {'doubling', 'qz'}.\n\n        Returns\n        -------\n        P : array_like(float)\n            P is part of the value function representation of\n            :math:`V(x) = x'Px + d`\n        F : array_like(float)\n            F is the policy rule that determines the choice of control\n            in each period.\n        d : array_like(float)\n            d is part of the value function representation of\n            :math:`V(x) = x'Px + d`\n\n        \"\"\"\n        # === simplify notation === #\n        Q, R, A, B, N, C = self.Q, self.R, self.A, self.B, self.N, self.C\n\n        # === solve Riccati equation, obtain P === #\n        A0, B0 = np.sqrt(self.beta) * A, np.sqrt(self.beta) * B\n        P = solve_discrete_riccati(A0, B0, R, Q, N, method=method)\n\n        # == Compute F == #\n        S1 = Q + self.beta * dot(B.T, dot(P, B))\n        S2 = self.beta * dot(B.T, dot(P, A)) + N\n        F = solve(S1, S2)\n\n        # == Compute d == #\n        d = self.beta * np.trace(dot(P, dot(C, C.T))) / (1 - self.beta)\n\n        # == Bind states and return values == #\n        self.P, self.F, self.d = P, F, d\n\n        return P, F, d"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pure_nash_brute_gen(g, tol=None):\n    for a in np.ndindex(*g.nums_actions):\n        if g.is_nash(a, tol=tol):\n            yield a", "response": "Generator version of pure_nash_brute."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef qnwequi(n, a, b, kind=\"N\", equidist_pp=None, random_state=None):\n    random_state = check_random_state(random_state)\n\n    if equidist_pp is None:\n        import sympy as sym\n        equidist_pp = np.sqrt(np.array(list(sym.primerange(0, 7920))))\n\n    n, a, b = list(map(np.atleast_1d, list(map(np.asarray, [n, a, b]))))\n\n    d = max(list(map(len, [n, a, b])))\n    n = np.prod(n)\n\n    if a.size == 1:\n        a = np.repeat(a, d)\n\n    if b.size == 1:\n        b = np.repeat(b, d)\n\n    i = np.arange(1, n + 1)\n\n    if kind.upper() == \"N\":  # Neiderreiter\n        j = 2.0 ** (np.arange(1, d+1) / (d+1))\n        nodes = np.outer(i, j)\n        nodes = (nodes - fix(nodes)).squeeze()\n    elif kind.upper() == \"W\":  # Weyl\n        j = equidist_pp[:d]\n        nodes = np.outer(i, j)\n        nodes = (nodes - fix(nodes)).squeeze()\n    elif kind.upper() == \"H\":  # Haber\n        j = equidist_pp[:d]\n        nodes = np.outer(i * (i+1) / 2, j)\n        nodes = (nodes - fix(nodes)).squeeze()\n    elif kind.upper() == \"R\":  # pseudo-random\n        nodes = random_state.rand(n, d).squeeze()\n    else:\n        raise ValueError(\"Unknown sequence requested\")\n\n    # compute nodes and weights\n    r = b - a\n    nodes = a + nodes * r\n    weights = (np.prod(r) / n) * np.ones(n)\n\n    return nodes, weights", "response": "Generates equidistributed sequences with property that averages over the integral as n goes to infinity."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the multivariate normal distribution of a multivariate normal distribution.", "response": "def qnwnorm(n, mu=None, sig2=None, usesqrtm=False):\n    \"\"\"\n    Computes nodes and weights for multivariate normal distribution\n\n    Parameters\n    ----------\n    n : int or array_like(float)\n        A length-d iterable of the number of nodes in each dimension\n\n    mu : scalar or array_like(float), optional(default=zeros(d))\n        The means of each dimension of the random variable. If a scalar\n        is given, that constant is repeated d times, where d is the\n        number of dimensions\n\n    sig2 : array_like(float), optional(default=eye(d))\n        A d x d array representing the variance-covariance matrix of the\n        multivariate normal distribution.\n\n    Returns\n    -------\n    nodes : np.ndarray(dtype=float)\n        Quadrature nodes\n\n    weights : np.ndarray(dtype=float)\n        Weights for quadrature nodes\n\n    Notes\n    -----\n    Based of original function ``qnwnorm`` in CompEcon toolbox by\n    Miranda and Fackler\n\n    References\n    ----------\n    Miranda, Mario J, and Paul L Fackler. Applied Computational\n    Economics and Finance, MIT Press, 2002.\n\n    \"\"\"\n    n = np.atleast_1d(n)\n    d = n.size\n\n    if mu is None:\n        mu = np.zeros(d)\n    else:\n        mu = np.atleast_1d(mu)\n\n    if sig2 is None:\n        sig2 = np.eye(d)\n    else:\n        sig2 = np.atleast_1d(sig2).reshape(d, d)\n\n    if all([x.size == 1 for x in [n, mu, sig2]]):\n        nodes, weights = _qnwnorm1(n[0])\n    else:\n        nodes = []\n        weights = []\n\n        for i in range(d):\n            _1d = _qnwnorm1(n[i])\n            nodes.append(_1d[0])\n            weights.append(_1d[1])\n\n        nodes = gridmake(*nodes)\n        weights = ckron(*weights[::-1])\n\n    if usesqrtm:\n        new_sig2 = la.sqrtm(sig2)\n    else:  # cholesky\n        new_sig2 = la.cholesky(sig2)\n\n    if d > 1:\n        nodes = nodes.dot(new_sig2) + mu  # Broadcast ok\n    else:  # nodes.dot(sig) will not be aligned in scalar case.\n        nodes = nodes * new_sig2 + mu\n\n    return nodes.squeeze(), weights"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the nodes and weights for a multivariate lognormal distribution.", "response": "def qnwlogn(n, mu=None, sig2=None):\n    \"\"\"\n    Computes nodes and weights for multivariate lognormal distribution\n\n    Parameters\n    ----------\n    n : int or array_like(float)\n        A length-d iterable of the number of nodes in each dimension\n\n    mu : scalar or array_like(float), optional(default=zeros(d))\n        The means of each dimension of the random variable. If a scalar\n        is given, that constant is repeated d times, where d is the\n        number of dimensions\n\n    sig2 : array_like(float), optional(default=eye(d))\n        A d x d array representing the variance-covariance matrix of the\n        multivariate normal distribution.\n\n    Returns\n    -------\n    nodes : np.ndarray(dtype=float)\n        Quadrature nodes\n\n    weights : np.ndarray(dtype=float)\n        Weights for quadrature nodes\n\n    Notes\n    -----\n    Based of original function ``qnwlogn`` in CompEcon toolbox by\n    Miranda and Fackler\n\n    References\n    ----------\n    Miranda, Mario J, and Paul L Fackler. Applied Computational\n    Economics and Finance, MIT Press, 2002.\n\n    \"\"\"\n    nodes, weights = qnwnorm(n, mu, sig2)\n    return np.exp(nodes), weights"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the quadrature nodes and weights for a multivariate uniform distribution.", "response": "def qnwunif(n, a, b):\n    \"\"\"\n    Computes quadrature nodes and weights for multivariate uniform\n    distribution\n\n    Parameters\n    ----------\n    n : int or array_like(float)\n        A length-d iterable of the number of nodes in each dimension\n\n    a : scalar or array_like(float)\n        A length-d iterable of lower endpoints. If a scalar is given,\n        that constant is repeated d times, where d is the number of\n        dimensions\n\n    b : scalar or array_like(float)\n        A length-d iterable of upper endpoints. If a scalar is given,\n        that constant is repeated d times, where d is the number of\n        dimensions\n\n    Returns\n    -------\n    nodes : np.ndarray(dtype=float)\n        Quadrature nodes\n\n    weights : np.ndarray(dtype=float)\n        Weights for quadrature nodes\n\n    Notes\n    -----\n    Based of original function ``qnwunif`` in CompEcon toolbox by\n    Miranda and Fackler\n\n    References\n    ----------\n    Miranda, Mario J, and Paul L Fackler. Applied Computational\n    Economics and Finance, MIT Press, 2002.\n\n    \"\"\"\n    n, a, b = list(map(np.asarray, [n, a, b]))\n    nodes, weights = qnwlege(n, a, b)\n    weights = weights / np.prod(b - a)\n    return nodes, weights"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nintegrate a d - dimensional function f on a rectangle with lower and upper bound for dimension a and b.", "response": "def quadrect(f, n, a, b, kind='lege', *args, **kwargs):\n    \"\"\"\n    Integrate the d-dimensional function f on a rectangle with lower and\n    upper bound for dimension i defined by a[i] and b[i], respectively;\n    using n[i] points.\n\n    Parameters\n    ----------\n    f : function\n        The function to integrate over. This should be a function\n        that accepts as its first argument a matrix representing points\n        along each dimension (each dimension is a column). Other\n        arguments that need to be passed to the function are caught by\n        `*args` and `**kwargs`\n\n    n : int or array_like(float)\n        A length-d iterable of the number of nodes in each dimension\n\n    a : scalar or array_like(float)\n        A length-d iterable of lower endpoints. If a scalar is given,\n        that constant is repeated d times, where d is the number of\n        dimensions\n\n    b : scalar or array_like(float)\n        A length-d iterable of upper endpoints. If a scalar is given,\n        that constant is repeated d times, where d is the number of\n        dimensions\n\n    kind : string, optional(default='lege')\n        Specifies which type of integration to perform. Valid\n        values are:\n\n        lege - Gauss-Legendre\n        cheb - Gauss-Chebyshev\n        trap - trapezoid rule\n        simp - Simpson rule\n        N    - Neiderreiter equidistributed sequence\n        W    - Weyl equidistributed sequence\n        H    - Haber  equidistributed sequence\n        R    - Monte Carlo\n\n    *args, **kwargs :\n        Other arguments passed to the function f\n\n    Returns\n    -------\n    out : scalar (float)\n        The value of the integral on the region [a, b]\n\n    Notes\n    -----\n    Based of original function ``quadrect`` in CompEcon toolbox by\n    Miranda and Fackler\n\n    References\n    ----------\n    Miranda, Mario J, and Paul L Fackler. Applied Computational\n    Economics and Finance, MIT Press, 2002.\n\n    \"\"\"\n    if kind.lower() == \"lege\":\n        nodes, weights = qnwlege(n, a, b)\n    elif kind.lower() == \"cheb\":\n        nodes, weights = qnwcheb(n, a, b)\n    elif kind.lower() == \"trap\":\n        nodes, weights = qnwtrap(n, a, b)\n    elif kind.lower() == \"simp\":\n        nodes, weights = qnwsimp(n, a, b)\n    else:\n        nodes, weights = qnwequi(n, a, b, kind)\n\n    out = weights.dot(f(nodes, *args, **kwargs))\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a function that computes the nodes and weights for a gamma distribution.", "response": "def qnwgamma(n, a=1.0, b=1.0, tol=3e-14):\n    \"\"\"\n    Computes nodes and weights for gamma distribution\n\n    Parameters\n    ----------\n    n : int or array_like(float)\n        A length-d iterable of the number of nodes in each dimension\n\n    a : scalar or array_like(float) : optional(default=ones(d))\n        Shape parameter of the gamma distribution parameter. Must be positive\n\n    b : scalar or array_like(float) : optional(default=ones(d))\n        Scale parameter of the gamma distribution parameter. Must be positive\n\n    tol : scalar or array_like(float) : optional(default=ones(d) * 3e-14)\n        Tolerance parameter for newton iterations for each node\n\n    Returns\n    -------\n    nodes : np.ndarray(dtype=float)\n        Quadrature nodes\n\n    weights : np.ndarray(dtype=float)\n        Weights for quadrature nodes\n\n    Notes\n    -----\n    Based of original function ``qnwgamma`` in CompEcon toolbox by\n    Miranda and Fackler\n\n    References\n    ----------\n    Miranda, Mario J, and Paul L Fackler. Applied Computational\n    Economics and Finance, MIT Press, 2002.\n\n    \"\"\"\n    return _make_multidim_func(_qnwgamma1, n, a, b, tol)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _qnwcheb1(n, a, b):\n    nodes = (b+a)/2 - (b-a)/2 * np.cos(np.pi/n * np.linspace(0.5, n-0.5, n))\n\n    # Create temporary arrays to be used in computing weights\n    t1 = np.arange(1, n+1) - 0.5\n    t2 = np.arange(0.0, n, 2)\n    t3 = np.concatenate((np.array([1.0]),\n                        -2.0/(np.arange(1.0, n-1, 2)*np.arange(3.0, n+1, 2))))\n\n    # compute weights and return\n    weights = ((b-a)/n)*np.cos(np.pi/n*np.outer(t1, t2)) @ t3\n\n    return nodes, weights", "response": "Compute univariate Guass - Checbychev quadrature nodes and weights"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing univariate Guass - Legendre quadrature nodes and weights for a given interval.", "response": "def _qnwlege1(n, a, b):\n    \"\"\"\n    Compute univariate Guass-Legendre quadrature nodes and weights\n\n    Parameters\n    ----------\n    n : int\n        The number of nodes\n\n    a : int\n        The lower endpoint\n\n    b : int\n        The upper endpoint\n\n    Returns\n    -------\n    nodes : np.ndarray(dtype=float)\n        An n element array of nodes\n\n    nodes : np.ndarray(dtype=float)\n        An n element array of weights\n\n    Notes\n    -----\n    Based of original function ``qnwlege1`` in CompEcon toolbox by\n    Miranda and Fackler\n\n    References\n    ----------\n    Miranda, Mario J, and Paul L Fackler. Applied Computational\n    Economics and Finance, MIT Press, 2002.\n\n    \"\"\"\n    # import ipdb; ipdb.set_trace()\n    maxit = 100\n    m = int(fix((n + 1) / 2.0))\n    xm = 0.5 * (b + a)\n    xl = 0.5 * (b - a)\n    nodes = np.zeros(n)\n\n    weights = nodes.copy()\n    i = np.arange(m)\n\n    z = np.cos(np.pi * ((i + 1.0) - 0.25) / (n + 0.5))\n\n    for its in range(maxit):\n        p1 = np.ones_like(z)\n        p2 = np.zeros_like(z)\n        for j in range(1, n+1):\n            p3 = p2\n            p2 = p1\n            p1 = ((2 * j - 1) * z * p2 - (j - 1) * p3) / j\n\n        pp = n * (z * p1 - p2)/(z * z - 1.0)\n        z1 = z.copy()\n        z = z1 - p1/pp\n        if np.all(np.abs(z - z1) < 1e-14):\n            break\n\n    if its == maxit - 1:\n        raise ValueError(\"Maximum iterations in _qnwlege1\")\n\n    nodes[i] = xm - xl * z\n    nodes[- i - 1] = xm + xl * z\n\n    weights[i] = 2 * xl / ((1 - z * z) * pp * pp)\n    weights[- i - 1] = weights[i]\n\n    return nodes, weights"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _qnwnorm1(n):\n    maxit = 100\n    pim4 = 1 / np.pi**(0.25)\n    m = int(fix((n + 1) / 2))\n    nodes = np.zeros(n)\n    weights = np.zeros(n)\n\n    for i in range(m):\n        if i == 0:\n            z = np.sqrt(2*n+1) - 1.85575 * ((2 * n + 1)**(-1 / 6.1))\n        elif i == 1:\n            z = z - 1.14 * (n ** 0.426) / z\n        elif i == 2:\n            z = 1.86 * z + 0.86 * nodes[0]\n        elif i == 3:\n            z = 1.91 * z + 0.91 * nodes[1]\n        else:\n            z = 2 * z + nodes[i-2]\n\n        its = 0\n\n        while its < maxit:\n            its += 1\n            p1 = pim4\n            p2 = 0\n            for j in range(1, n+1):\n                p3 = p2\n                p2 = p1\n                p1 = z * math.sqrt(2.0/j) * p2 - math.sqrt((j - 1.0) / j) * p3\n\n            pp = math.sqrt(2 * n) * p2\n            z1 = z\n            z = z1 - p1/pp\n            if abs(z - z1) < 1e-14:\n                break\n\n        if its == maxit:\n            raise ValueError(\"Failed to converge in _qnwnorm1\")\n\n        nodes[n - 1 - i] = z\n        nodes[i] = -z\n        weights[i] = 2 / (pp*pp)\n        weights[n - 1 - i] = weights[i]\n\n    weights /= math.sqrt(math.pi)\n    nodes = nodes * math.sqrt(2.0)\n\n    return nodes, weights", "response": "Compute nodes and weights for univariate standard distribution."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes univariate Simpson quadrature nodes and weights", "response": "def _qnwsimp1(n, a, b):\n    \"\"\"\n    Compute univariate Simpson quadrature nodes and weights\n\n    Parameters\n    ----------\n    n : int\n        The number of nodes\n\n    a : int\n        The lower endpoint\n\n    b : int\n        The upper endpoint\n\n    Returns\n    -------\n    nodes : np.ndarray(dtype=float)\n        An n element array of nodes\n\n    nodes : np.ndarray(dtype=float)\n        An n element array of weights\n\n    Notes\n    -----\n    Based of original function ``qnwsimp1`` in CompEcon toolbox by\n    Miranda and Fackler\n\n    References\n    ----------\n    Miranda, Mario J, and Paul L Fackler. Applied Computational\n    Economics and Finance, MIT Press, 2002.\n\n    \"\"\"\n    if n % 2 == 0:\n        print(\"WARNING qnwsimp: n must be an odd integer. Increasing by 1\")\n        n += 1\n\n    nodes = np.linspace(a, b, n)\n    dx = nodes[1] - nodes[0]\n    weights = np.kron(np.ones((n+1) // 2), np.array([2.0, 4.0]))\n    weights = weights[:n]\n    weights[0] = weights[-1] = 1\n    weights = (dx / 3.0) * weights\n\n    return nodes, weights"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _qnwtrap1(n, a, b):\n    if n < 1:\n        raise ValueError(\"n must be at least one\")\n\n    nodes = np.linspace(a, b, n)\n    dx = nodes[1] - nodes[0]\n\n    weights = dx * np.ones(n)\n    weights[0] *= 0.5\n    weights[-1] *= 0.5\n\n    return nodes, weights", "response": "Compute univariate trapezoid rule quadrature nodes and weights"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes nodes and weights for quadrature on the beta distribution. Default is a=b=1 which is just a uniform distribution NOTE: For now I am just following compecon; would be much better to find a different way since I don't know what they are doing. Parameters ---------- n : scalar : int The number of quadrature points a : scalar : float, optional(default=1) First Beta distribution parameter b : scalar : float, optional(default=1) Second Beta distribution parameter Returns ------- nodes : np.ndarray(dtype=float, ndim=1) The quadrature points weights : np.ndarray(dtype=float, ndim=1) The quadrature weights that correspond to nodes Notes ----- Based of original function ``_qnwbeta1`` in CompEcon toolbox by Miranda and Fackler References ---------- Miranda, Mario J, and Paul L Fackler. Applied Computational Economics and Finance, MIT Press, 2002.", "response": "def _qnwbeta1(n, a=1.0, b=1.0):\n    \"\"\"\n    Computes nodes and weights for quadrature on the beta distribution.\n    Default is a=b=1 which is just a uniform distribution\n\n    NOTE: For now I am just following compecon; would be much better to\n    find a different way since I don't know what they are doing.\n\n    Parameters\n    ----------\n    n : scalar : int\n        The number of quadrature points\n\n    a : scalar : float, optional(default=1)\n        First Beta distribution parameter\n\n    b : scalar : float, optional(default=1)\n        Second Beta distribution parameter\n\n    Returns\n    -------\n    nodes : np.ndarray(dtype=float, ndim=1)\n        The quadrature points\n\n    weights : np.ndarray(dtype=float, ndim=1)\n        The quadrature weights that correspond to nodes\n\n    Notes\n    -----\n    Based of original function ``_qnwbeta1`` in CompEcon toolbox by\n    Miranda and Fackler\n\n    References\n    ----------\n    Miranda, Mario J, and Paul L Fackler. Applied Computational\n    Economics and Finance, MIT Press, 2002.\n\n    \"\"\"\n    # We subtract one and write a + 1 where we actually want a, and a\n    # where we want a - 1\n    a = a - 1\n    b = b - 1\n\n    maxiter = 25\n\n    # Allocate empty space\n    nodes = np.zeros(n)\n    weights = np.zeros(n)\n\n    # Find \"reasonable\" starting values.  Why these numbers?\n    for i in range(n):\n        if i == 0:\n            an = a/n\n            bn = b/n\n            r1 = (1+a) * (2.78/(4+n*n) + .768*an/n)\n            r2 = 1 + 1.48*an + .96*bn + .452*an*an + .83*an*bn\n            z = 1 - r1/r2\n        elif i == 1:\n            r1 = (4.1+a) / ((1+a)*(1+0.156*a))\n            r2 = 1 + 0.06 * (n-8) * (1+0.12*a)/n\n            r3 = 1 + 0.012*b * (1+0.25*abs(a))/n\n            z = z - (1-z) * r1 * r2 * r3\n        elif i == 2:\n            r1 = (1.67+0.28*a)/(1+0.37*a)\n            r2 = 1+0.22*(n-8)/n\n            r3 = 1+8*b/((6.28+b)*n*n)\n            z = z-(nodes[0]-z)*r1*r2*r3\n        elif i == n - 2:\n            r1 = (1+0.235*b)/(0.766+0.119*b)\n            r2 = 1/(1+0.639*(n-4)/(1+0.71*(n-4)))\n            r3 = 1/(1+20*a/((7.5+a)*n*n))\n            z = z+(z-nodes[-4])*r1*r2*r3\n        elif i == n - 1:\n            r1 = (1+0.37*b) / (1.67+0.28*b)\n            r2 = 1 / (1+0.22*(n-8)/n)\n            r3 = 1 / (1+8*a/((6.28+a)*n*n))\n            z = z+(z-nodes[-3])*r1*r2*r3\n        else:\n            z = 3*nodes[i-1] - 3*nodes[i-2] + nodes[i-3]\n\n        ab = a+b\n\n        # Root finding\n        its = 0\n        z1 = -100\n        while abs(z - z1) > 1e-10 and its < maxiter:\n            temp = 2 + ab\n            p1 = (a-b + temp*z)/2\n            p2 = 1\n\n            for j in range(2, n+1):\n                p3 = p2\n                p2 = p1\n                temp = 2*j + ab\n                aa = 2*j * (j+ab)*(temp-2)\n                bb = (temp-1) * (a*a - b*b + temp*(temp-2) * z)\n                c = 2 * (j - 1 + a) * (j - 1 + b) * temp\n                p1 = (bb*p2 - c*p3)/aa\n\n            pp = (n*(a-b-temp*z) * p1 + 2*(n+a)*(n+b)*p2)/(temp*(1 - z*z))\n            z1 = z\n            z = z1 - p1/pp\n\n            if abs(z - z1) < 1e-12:\n                break\n\n            its += 1\n\n        if its == maxiter:\n            raise ValueError(\"Max Iteration reached.  Failed to converge\")\n\n        nodes[i] = z\n        weights[i] = temp/(pp*p2)\n\n    nodes = (1-nodes)/2\n    weights = weights * math.exp(gammaln(a+n) + gammaln(b+n)\n                                 - gammaln(n+1) - gammaln(n+ab+1))\n    weights = weights / (2*math.exp(gammaln(a+1) + gammaln(b+1)\n                         - gammaln(ab+2)))\n\n    return nodes, weights"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfunctions to create a newton random variable from a gamma distribution.", "response": "def _qnwgamma1(n, a=1.0, b=1.0, tol=3e-14):\n    \"\"\"\n    1d quadrature weights and nodes for Gamma distributed random variable\n\n    Parameters\n    ----------\n    n : scalar : int\n        The number of quadrature points\n\n    a : scalar : float, optional(default=1.0)\n        Shape parameter of the gamma distribution parameter. Must be positive\n\n    b : scalar : float, optional(default=1.0)\n        Scale parameter of the gamma distribution parameter. Must be positive\n\n    tol : scalar : float, optional(default=3e-14)\n        Tolerance parameter for newton iterations for each node\n\n    Returns\n    -------\n    nodes : np.ndarray(dtype=float, ndim=1)\n        The quadrature points\n\n    weights : np.ndarray(dtype=float, ndim=1)\n        The quadrature weights that correspond to nodes\n\n    Notes\n    -----\n    Based of original function ``qnwgamma1`` in CompEcon toolbox by\n    Miranda and Fackler\n\n    References\n    ----------\n    Miranda, Mario J, and Paul L Fackler. Applied Computational\n    Economics and Finance, MIT Press, 2002.\n\n    \"\"\"\n    a -= 1\n\n    maxit = 25\n\n    factor = -math.exp(gammaln(a+n) - gammaln(n) - gammaln(a+1))\n    nodes = np.zeros(n)\n    weights = np.zeros(n)\n\n    # Create nodes\n    for i in range(n):\n        # Reasonable starting values\n        if i == 0:\n            z = (1+a) * (3+0.92*a) / (1 + 2.4*n + 1.8*a)\n        elif i == 1:\n            z = z + (15 + 6.25*a) / (1 + 0.9*a + 2.5*n)\n        else:\n            j = i-1\n            z = z + ((1 + 2.55*j) / (1.9*j) + 1.26*j*a / (1 + 3.5*j)) * \\\n                (z - nodes[j-1]) / (1 + 0.3*a)\n\n        # root finding iterations\n        its = 0\n        z1 = -10000\n        while abs(z - z1) > tol and its < maxit:\n            p1 = 1.0\n            p2 = 0.0\n            for j in range(1, n+1):\n                # Recurrance relation for Laguerre polynomials\n                p3 = p2\n                p2 = p1\n                p1 = ((2*j - 1 + a - z)*p2 - (j - 1 + a)*p3) / j\n\n            pp = (n*p1 - (n+a)*p2) / z\n            z1 = z\n            z = z1 - p1/pp\n            its += 1\n\n        if its == maxit:\n            raise ValueError('Failure to converge')\n\n        nodes[i] = z\n        weights[i] = factor / (pp*n*p2)\n\n    return nodes*b, weights"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a vertex_enumeration generator for a NormalFormGame instance.", "response": "def vertex_enumeration_gen(g, qhull_options=None):\n    \"\"\"\n    Generator version of `vertex_enumeration`.\n\n    Parameters\n    ----------\n    g : NormalFormGame\n        NormalFormGame instance with 2 players.\n\n    qhull_options : str, optional(default=None)\n        Options to pass to `scipy.spatial.ConvexHull`. See the `Qhull\n        manual <http://www.qhull.org>`_  for details.\n\n    Yields\n    -------\n    tuple(ndarray(float, ndim=1))\n        Tuple of Nash equilibrium mixed actions.\n\n    \"\"\"\n    try:\n        N = g.N\n    except AttributeError:\n        raise TypeError('input must be a 2-player NormalFormGame')\n    if N != 2:\n        raise NotImplementedError('Implemented only for 2-player games')\n\n    brps = [_BestResponsePolytope(\n        g.players[1-i], idx=i, qhull_options=qhull_options\n    ) for i in range(N)]\n\n    labelings_bits_tup = \\\n        tuple(_ints_arr_to_bits(brps[i].labelings) for i in range(N))\n    equations_tup = tuple(brps[i].equations for i in range(N))\n    trans_recips = tuple(brps[i].trans_recip for i in range(N))\n\n    return _vertex_enumeration_gen(labelings_bits_tup, equations_tup,\n                                   trans_recips)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _vertex_enumeration_gen(labelings_bits_tup, equations_tup, trans_recips):\n    m, n = equations_tup[0].shape[1] - 1, equations_tup[1].shape[1] - 1\n    num_vertices0, num_vertices1 = \\\n        equations_tup[0].shape[0], equations_tup[1].shape[0]\n    ZERO_LABELING0_BITS = (np.uint64(1) << np.uint64(m)) - np.uint64(1)\n    COMPLETE_LABELING_BITS = (np.uint64(1) << np.uint64(m+n)) - np.uint64(1)\n\n    for i in range(num_vertices0):\n        if labelings_bits_tup[0][i] == ZERO_LABELING0_BITS:\n            continue\n        for j in range(num_vertices1):\n            xor = labelings_bits_tup[0][i] ^ labelings_bits_tup[1][j]\n            if xor == COMPLETE_LABELING_BITS:\n                yield _get_mixed_actions(\n                    labelings_bits_tup[0][i],\n                    (equations_tup[0][i], equations_tup[1][j]),\n                    trans_recips\n                )\n                break", "response": "Generator for the vertex_enumeration_gen."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting an array of integers representing the set bits into the corresponding integer.", "response": "def _ints_arr_to_bits(ints_arr, out):\n    \"\"\"\n    Convert an array of integers representing the set bits into the\n    corresponding integer.\n\n    Compiled as a ufunc by Numba's `@guvectorize`: if the input is a\n    2-dim array with shape[0]=K, the function returns a 1-dim array of\n    K converted integers.\n\n    Parameters\n    ----------\n    ints_arr : ndarray(int32, ndim=1)\n        Array of distinct integers from 0, ..., 63.\n\n    Returns\n    -------\n    np.uint64\n        Integer with set bits represented by the input integers.\n\n    Examples\n    --------\n    >>> ints_arr = np.array([0, 1, 2], dtype=np.int32)\n    >>> _ints_arr_to_bits(ints_arr)\n    7\n    >>> ints_arr2d = np.array([[0, 1, 2], [3, 0, 1]], dtype=np.int32)\n    >>> _ints_arr_to_bits(ints_arr2d)\n    array([ 7, 11], dtype=uint64)\n\n    \"\"\"\n    m = ints_arr.shape[0]\n    out[0] = 0\n    for i in range(m):\n        out[0] |= np.uint64(1) << np.uint64(ints_arr[i])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_mixed_actions(labeling_bits, equation_tup, trans_recips):\n    m, n = equation_tup[0].shape[0] - 1, equation_tup[1].shape[0] - 1\n    out = np.empty(m+n)\n\n    for pl, (start, stop, skip) in enumerate([(0, m, np.uint64(1)),\n                                              (m, m+n, np.uint64(0))]):\n        sum_ = 0.\n        for i in range(start, stop):\n            if (labeling_bits & np.uint64(1)) == skip:\n                out[i] = 0\n            else:\n                out[i] = equation_tup[pl][i-start] * trans_recips[pl] - \\\n                    equation_tup[pl][-1]\n                sum_ += out[i]\n            labeling_bits = labeling_bits >> np.uint64(1)\n        if sum_ != 0:\n            out[start:stop] /= sum_\n\n    return out[:m], out[m:]", "response": "This function returns a tuple of mixed actions for the current language."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a matrix where rows span the cartesian product of combinations of the input arrays.", "response": "def gridmake(*arrays):\n    \"\"\"\n    Expands one or more vectors (or matrices) into a matrix where rows span the\n    cartesian product of combinations of the input arrays. Each column of the\n    input arrays will correspond to one column of the output matrix.\n\n    Parameters\n    ----------\n    *arrays : tuple/list of np.ndarray\n              Tuple/list of vectors to be expanded.\n\n    Returns\n    -------\n    out : np.ndarray\n          The cartesian product of combinations of the input arrays.\n\n    Notes\n    -----\n    Based of original function ``gridmake`` in CompEcon toolbox by\n    Miranda and Fackler\n\n    References\n    ----------\n    Miranda, Mario J, and Paul L Fackler. Applied Computational Economics\n    and Finance, MIT Press, 2002.\n\n    \"\"\"\n    if all([i.ndim == 1 for i in arrays]):\n        d = len(arrays)\n        if d == 2:\n            out = _gridmake2(*arrays)\n        else:\n            out = _gridmake2(arrays[0], arrays[1])\n            for arr in arrays[2:]:\n                out = _gridmake2(out, arr)\n\n        return out\n    else:\n        raise NotImplementedError(\"Come back here\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _gridmake2(x1, x2):\n    if x1.ndim == 1 and x2.ndim == 1:\n        return np.column_stack([np.tile(x1, x2.shape[0]),\n                               np.repeat(x2, x1.shape[0])])\n    elif x1.ndim > 1 and x2.ndim == 1:\n        first = np.tile(x1, (x2.shape[0], 1))\n        second = np.repeat(x2, x1.shape[0])\n        return np.column_stack([first, second])\n    else:\n        raise NotImplementedError(\"Come back here\")", "response": "This function is used to expand two vectors or matrices into a matrix where rows span the\n    cartesian product of combinations of x1 and x2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn m randomly sampled probability vectors of dimension k.", "response": "def probvec(m, k, random_state=None, parallel=True):\n    \"\"\"\n    Return m randomly sampled probability vectors of dimension k.\n\n    Parameters\n    ----------\n    m : scalar(int)\n        Number of probability vectors.\n\n    k : scalar(int)\n        Dimension of each probability vectors.\n\n    random_state : int or np.random.RandomState, optional\n        Random seed (integer) or np.random.RandomState instance to set\n        the initial state of the random number generator for\n        reproducibility. If None, a randomly initialized RandomState is\n        used.\n\n    parallel : bool(default=True)\n        Whether to use multi-core CPU (parallel=True) or single-threaded\n        CPU (parallel=False). (Internally the code is executed through\n        Numba.guvectorize.)\n\n    Returns\n    -------\n    x : ndarray(float, ndim=2)\n        Array of shape (m, k) containing probability vectors as rows.\n\n    Examples\n    --------\n    >>> qe.random.probvec(2, 3, random_state=1234)\n    array([[ 0.19151945,  0.43058932,  0.37789123],\n           [ 0.43772774,  0.34763084,  0.21464142]])\n\n    \"\"\"\n    if k == 1:\n        return np.ones((m, k))\n\n    # if k >= 2\n    random_state = check_random_state(random_state)\n    r = random_state.random_sample(size=(m, k-1))\n    x = np.empty((m, k))\n\n    # Parse Parallel Option #\n    if parallel:\n        _probvec_parallel(r, x)\n    else:\n        _probvec_cpu(r, x)\n\n    return x"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _probvec(r, out):\n    n = r.shape[0]\n    r.sort()\n    out[0] = r[0]\n    for i in range(1, n):\n        out[i] = r[i] - r[i-1]\n    out[n] = 1 - r[n-1]", "response": "Fill out with randomly sampled probability vectors as rows."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sample_without_replacement(n, k, num_trials=None, random_state=None):\n    if n <= 0:\n        raise ValueError('n must be greater than 0')\n    if k > n:\n        raise ValueError('k must be smaller than or equal to n')\n\n    size = k if num_trials is None else (num_trials, k)\n\n    random_state = check_random_state(random_state)\n    r = random_state.random_sample(size=size)\n    result = _sample_without_replacement(n, r)\n\n    return result", "response": "Randomly choose k integers without replacement from 0... n - 1."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsamples n random elements from the Numba random variates.", "response": "def _sample_without_replacement(n, r, out):\n    \"\"\"\n    Main body of `sample_without_replacement`. To be complied as a ufunc\n    by guvectorize of Numba.\n\n    \"\"\"\n    k = r.shape[0]\n\n    # Logic taken from random.sample in the standard library\n    pool = np.arange(n)\n    for j in range(k):\n        idx = int(np.floor(r[j] * (n-j)))  # np.floor returns a float\n        out[j] = pool[idx]\n        pool[idx] = pool[n-j-1]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a random sample according to the cumulative distribution of a node.", "response": "def draw(cdf, size=None):\n    \"\"\"\n    Generate a random sample according to the cumulative distribution\n    given by `cdf`. Jit-complied by Numba in nopython mode.\n\n    Parameters\n    ----------\n    cdf : array_like(float, ndim=1)\n        Array containing the cumulative distribution.\n\n    size : scalar(int), optional(default=None)\n        Size of the sample. If an integer is supplied, an ndarray of\n        `size` independent draws is returned; otherwise, a single draw\n        is returned as a scalar.\n\n    Returns\n    -------\n    scalar(int) or ndarray(int, ndim=1)\n\n    Examples\n    --------\n    >>> cdf = np.cumsum([0.4, 0.6])\n    >>> qe.random.draw(cdf)\n    1\n    >>> qe.random.draw(cdf, 10)\n    array([1, 0, 1, 0, 1, 0, 0, 0, 1, 0])\n\n    \"\"\"\n    if isinstance(size, types.Integer):\n        def draw_impl(cdf, size):\n            rs = np.random.random_sample(size)\n            out = np.empty(size, dtype=np.int_)\n            for i in range(size):\n                out[i] = searchsorted(cdf, rs[i])\n            return out\n    else:\n        def draw_impl(cdf, size):\n            r = np.random.random_sample()\n            return searchsorted(cdf, r)\n    return draw_impl"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cartesian(nodes, order='C'):\n    '''\n    Cartesian product of a list of arrays\n\n    Parameters\n    ----------\n    nodes : list(array_like(ndim=1))\n\n    order : str, optional(default='C')\n        ('C' or 'F') order in which the product is enumerated\n\n    Returns\n    -------\n    out : ndarray(ndim=2)\n        each line corresponds to one point of the product space\n    '''\n\n    nodes = [np.array(e) for e in nodes]\n    shapes = [e.shape[0] for e in nodes]\n\n    dtype = nodes[0].dtype\n\n    n = len(nodes)\n    l = np.prod(shapes)\n    out = np.zeros((l, n), dtype=dtype)\n\n    if order == 'C':\n        repetitions = np.cumprod([1] + shapes[:-1])\n    else:\n        shapes.reverse()\n        sh = [1] + shapes[:-1]\n        repetitions = np.cumprod(sh)\n        repetitions = repetitions.tolist()\n        repetitions.reverse()\n\n    for i in range(n):\n        _repeat_1d(nodes[i], repetitions[i], out[:, i])\n\n    return out", "response": "Returns the cartesian product of a list of arrays"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mlinspace(a, b, nums, order='C'):\n    '''\n    Constructs a regular cartesian grid\n\n    Parameters\n    ----------\n    a : array_like(ndim=1)\n        lower bounds in each dimension\n\n    b : array_like(ndim=1)\n        upper bounds in each dimension\n\n    nums : array_like(ndim=1)\n        number of nodes along each dimension\n\n    order : str, optional(default='C')\n        ('C' or 'F') order in which the product is enumerated\n\n    Returns\n    -------\n    out : ndarray(ndim=2)\n        each line corresponds to one point of the product space\n    '''\n\n    a = np.array(a, dtype='float64')\n    b = np.array(b, dtype='float64')\n    nums = np.array(nums, dtype='int64')\n    nodes = [np.linspace(a[i], b[i], nums[i]) for i in range(len(nums))]\n\n    return cartesian(nodes, order=order)", "response": "Returns a regular cartesian grid of the given numbers"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _repeat_1d(x, K, out):\n    '''\n    Repeats each element of a vector many times and repeats the whole\n    result many times\n\n    Parameters\n    ----------\n    x : ndarray(ndim=1)\n        vector to be repeated\n\n    K : scalar(int)\n        number of times each element of x is repeated (inner iterations)\n\n    out : ndarray(ndim=1)\n        placeholder for the result\n\n    Returns\n    -------\n    None\n    '''\n\n    N = x.shape[0]\n    L = out.shape[0] // (K*N)  # number of outer iterations\n    # K                        # number of inner iterations\n\n    # the result out should enumerate in C-order the elements\n    # of a 3-dimensional array T of dimensions (K,N,L)\n    # such that for all k,n,l, we have T[k,n,l] == x[n]\n\n    for n in range(N):\n        val = x[n]\n        for k in range(K):\n            for l in range(L):\n                ind = k*N*L + n*L + l\n                out[ind] = val", "response": "Repeats each element of a vector many times and repeats the whole\n    result many times\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef simplex_grid(m, n):\n    L = num_compositions_jit(m, n)\n    if L == 0:  # Overflow occured\n    \traise ValueError(_msg_max_size_exceeded)\n    out = np.empty((L, m), dtype=np.int_)\n\n    x = np.zeros(m, dtype=np.int_)\n    x[m-1] = n\n\n    for j in range(m):\n        out[0, j] = x[j]\n\n    h = m\n\n    for i in range(1, L):\n        h -= 1\n\n        val = x[h]\n        x[h] = 0\n        x[m-1] = val - 1\n        x[h-1] += 1\n\n        for j in range(m):\n            out[i, j] = x[j]\n\n        if val != 1:\n            h = m\n\n    return out", "response": "r Constructs a simplex grid of the n - dimensional m - part compositions of the n - dimensional simplex."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef simplex_index(x, m, n):\n    if m == 1:\n        return 0\n\n    decumsum = np.cumsum(x[-1:0:-1])[::-1]\n    idx = num_compositions(m, n) - 1\n    for i in range(m-1):\n        if decumsum[i] == 0:\n            break\n        idx -= num_compositions(m-i, decumsum[i]-1)\n    return idx", "response": "r Returns the index of the point x in the lexicographic order of the simplex."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the total number of m - part compositions of n.", "response": "def num_compositions(m, n):\n    \"\"\"\n    The total number of m-part compositions of n, which is equal to\n    (n+m-1) choose (m-1).\n\n    Parameters\n    ----------\n    m : scalar(int)\n        Number of parts of composition.\n\n    n : scalar(int)\n        Integer to decompose.\n\n    Returns\n    -------\n    scalar(int)\n        Total number of m-part compositions of n.\n\n    \"\"\"\n    # docs.scipy.org/doc/scipy/reference/generated/scipy.special.comb.html\n    return scipy.special.comb(n+m-1, m-1, exact=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_params(self):\n        # === set up ma_poly === #\n        ma_poly = np.asarray(self._theta)\n        self.ma_poly = np.insert(ma_poly, 0, 1)  # The array (1, theta)\n\n        # === set up ar_poly === #\n        if np.isscalar(self._phi):\n            ar_poly = np.array(-self._phi)\n        else:\n            ar_poly = -np.asarray(self._phi)\n        self.ar_poly = np.insert(ar_poly, 0, 1)  # The array (1, -phi)\n\n        # === pad ar_poly with zeros if required === #\n        if len(self.ar_poly) < len(self.ma_poly):\n            temp = np.zeros(len(self.ma_poly) - len(self.ar_poly))\n            self.ar_poly = np.hstack((self.ar_poly, temp))", "response": "r Sets the parameters of the object to match the parameters of the object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef impulse_response(self, impulse_length=30):\n        from scipy.signal import dimpulse\n        sys = self.ma_poly, self.ar_poly, 1\n        times, psi = dimpulse(sys, n=impulse_length)\n        psi = psi[0].flatten()  # Simplify return value into flat array\n\n        return psi", "response": "Get the impulse response corresponding to our model."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef spectral_density(self, two_pi=True, res=1200):\n        from scipy.signal import freqz\n        w, h = freqz(self.ma_poly, self.ar_poly, worN=res, whole=two_pi)\n        spect = h * conj(h) * self.sigma**2\n\n        return w, spect", "response": "r Compute the spectral density of the entry in the set of unique entries."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef autocovariance(self, num_autocov=16):\n        spect = self.spectral_density()[1]\n        acov = np.fft.ifft(spect).real\n\n        # num_autocov should be <= len(acov) / 2\n        return acov[:num_autocov]", "response": "Calculates the autocovariance function from the ARMA parameters\n        and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef simulation(self, ts_length=90, random_state=None):\n        from scipy.signal import dlsim\n        random_state = check_random_state(random_state)\n\n        sys = self.ma_poly, self.ar_poly, 1\n        u = random_state.randn(ts_length, 1) * self.sigma\n        vals = dlsim(sys, u)[1]\n\n        return vals.flatten()", "response": "Compute a simulated sample path assuming Gaussian shocks."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute_steadystate(self, nnc=2):\n        zx = np.eye(self.A0.shape[0])-self.A0\n        self.zz = nullspace(zx)\n        self.zz /= self.zz[nnc]\n        self.css = self.Sc.dot(self.zz)\n        self.sss = self.Ss.dot(self.zz)\n        self.iss = self.Si.dot(self.zz)\n        self.dss = self.Sd.dot(self.zz)\n        self.bss = self.Sb.dot(self.zz)\n        self.kss = self.Sk.dot(self.zz)\n        self.hss = self.Sh.dot(self.zz)", "response": "Computes the non - stochastic steady - state of the economy."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the quantities and prices for the economy.", "response": "def compute_sequence(self, x0, ts_length=None, Pay=None):\n        \"\"\"\n        Simulate quantities and prices for the economy\n\n        Parameters\n        ----------\n        x0 : array_like(float)\n            The initial state\n\n        ts_length : scalar(int)\n            Length of the simulation\n\n        Pay : array_like(float)\n            Vector to price an asset whose payout is Pay*xt\n\n        \"\"\"\n        lq = LQ(self.Q, self.R, self.A, self.B,\n                self.C, N=self.W, beta=self.beta)\n        xp, up, wp = lq.compute_sequence(x0, ts_length)\n        self.h = self.Sh.dot(xp)\n        self.k = self.Sk.dot(xp)\n        self.i = self.Si.dot(xp)\n        self.b = self.Sb.dot(xp)\n        self.d = self.Sd.dot(xp)\n        self.c = self.Sc.dot(xp)\n        self.g = self.Sg.dot(xp)\n        self.s = self.Ss.dot(xp)\n\n        # === Value of J-period risk-free bonds === #\n        # === See p.145: Equation (7.11.2) === #\n        e1 = np.zeros((1, self.nc))\n        e1[0, 0] = 1\n        self.R1_Price = np.empty((ts_length + 1, 1))\n        self.R2_Price = np.empty((ts_length + 1, 1))\n        self.R5_Price = np.empty((ts_length + 1, 1))\n        for i in range(ts_length + 1):\n            self.R1_Price[i, 0] = self.beta * e1.dot(self.Mc).dot(np.linalg.matrix_power(\n                self.A0, 1)).dot(xp[:, i]) / e1.dot(self.Mc).dot(xp[:, i])\n            self.R2_Price[i, 0] = self.beta**2 * e1.dot(self.Mc).dot(\n                np.linalg.matrix_power(self.A0, 2)).dot(xp[:, i]) / e1.dot(self.Mc).dot(xp[:, i])\n            self.R5_Price[i, 0] = self.beta**5 * e1.dot(self.Mc).dot(\n                np.linalg.matrix_power(self.A0, 5)).dot(xp[:, i]) / e1.dot(self.Mc).dot(xp[:, i])\n\n        # === Gross rates of return on 1-period risk-free bonds === #\n        self.R1_Gross = 1 / self.R1_Price\n\n        # === Net rates of return on J-period risk-free bonds === #\n        # === See p.148: log of gross rate of return, divided by j === #\n        self.R1_Net = np.log(1 / self.R1_Price) / 1\n        self.R2_Net = np.log(1 / self.R2_Price) / 2\n        self.R5_Net = np.log(1 / self.R5_Price) / 5\n\n        # === Value of asset whose payout vector is Pay*xt === #\n        # See p.145: Equation (7.11.1)\n        if isinstance(Pay, np.ndarray) == True:\n            self.Za = Pay.T.dot(self.Mc)\n            self.Q = solve_discrete_lyapunov(\n                self.A0.T * self.beta**0.5, self.Za)\n            self.q = self.beta / (1 - self.beta) * \\\n                np.trace(self.C.T.dot(self.Q).dot(self.C))\n            self.Pay_Price = np.empty((ts_length + 1, 1))\n            self.Pay_Gross = np.empty((ts_length + 1, 1))\n            self.Pay_Gross[0, 0] = np.nan\n            for i in range(ts_length + 1):\n                self.Pay_Price[i, 0] = (xp[:, i].T.dot(self.Q).dot(\n                    xp[:, i]) + self.q) / e1.dot(self.Mc).dot(xp[:, i])\n            for i in range(ts_length):\n                self.Pay_Gross[i + 1, 0] = self.Pay_Price[i + 1,\n                                                          0] / (self.Pay_Price[i, 0] - Pay.dot(xp[:, i]))\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef irf(self, ts_length=100, shock=None):\n\n        if type(shock) != np.ndarray:\n            # Default is to select first element of w\n            shock = np.vstack((np.ones((1, 1)), np.zeros((self.nw - 1, 1))))\n\n        self.c_irf = np.empty((ts_length, self.nc))\n        self.s_irf = np.empty((ts_length, self.nb))\n        self.i_irf = np.empty((ts_length, self.ni))\n        self.k_irf = np.empty((ts_length, self.nk))\n        self.h_irf = np.empty((ts_length, self.nh))\n        self.g_irf = np.empty((ts_length, self.ng))\n        self.d_irf = np.empty((ts_length, self.nd))\n        self.b_irf = np.empty((ts_length, self.nb))\n\n        for i in range(ts_length):\n            self.c_irf[i, :] = self.Sc.dot(\n                np.linalg.matrix_power(self.A0, i)).dot(self.C).dot(shock).T\n            self.s_irf[i, :] = self.Ss.dot(\n                np.linalg.matrix_power(self.A0, i)).dot(self.C).dot(shock).T\n            self.i_irf[i, :] = self.Si.dot(\n                np.linalg.matrix_power(self.A0, i)).dot(self.C).dot(shock).T\n            self.k_irf[i, :] = self.Sk.dot(\n                np.linalg.matrix_power(self.A0, i)).dot(self.C).dot(shock).T\n            self.h_irf[i, :] = self.Sh.dot(\n                np.linalg.matrix_power(self.A0, i)).dot(self.C).dot(shock).T\n            self.g_irf[i, :] = self.Sg.dot(\n                np.linalg.matrix_power(self.A0, i)).dot(self.C).dot(shock).T\n            self.d_irf[i, :] = self.Sd.dot(\n                np.linalg.matrix_power(self.A0, i)).dot(self.C).dot(shock).T\n            self.b_irf[i, :] = self.Sb.dot(\n                np.linalg.matrix_power(self.A0, i)).dot(self.C).dot(shock).T\n\n        return", "response": "Calculates the IRF of the current set of IAFs for the current set of periods and shocks."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef canonical(self):\n        Ac1 = np.hstack((self.deltah, np.zeros((self.nh, self.nz))))\n        Ac2 = np.hstack((np.zeros((self.nz, self.nh)), self.a22))\n        Ac = np.vstack((Ac1, Ac2))\n        Bc = np.vstack((self.thetah, np.zeros((self.nz, self.nc))))\n        Cc = np.vstack((np.zeros((self.nh, self.nw)), self.c2))\n        Rc1 = np.hstack((self.llambda.T.dot(self.llambda), -\n                         self.llambda.T.dot(self.ub)))\n        Rc2 = np.hstack((-self.ub.T.dot(self.llambda), self.ub.T.dot(self.ub)))\n        Rc = np.vstack((Rc1, Rc2))\n        Qc = self.pih.T.dot(self.pih)\n        Nc = np.hstack(\n            (self.pih.T.dot(self.llambda), -self.pih.T.dot(self.ub)))\n\n        lq_aux = LQ(Qc, Rc, Ac, Bc, N=Nc, beta=self.beta)\n\n        P1, F1, d1 = lq_aux.stationary_values()\n\n        self.F_b = F1[:, 0:self.nh]\n        self.F_f = F1[:, self.nh:]\n\n        self.pihat = np.linalg.cholesky(self.pih.T.dot(\n            self.pih) + self.beta.dot(self.thetah.T).dot(P1[0:self.nh, 0:self.nh]).dot(self.thetah)).T\n        self.llambdahat = self.pihat.dot(self.F_b)\n        self.ubhat = - self.pihat.dot(self.F_f)\n\n        return", "response": "Compute the canonical preference representation of the current household technology."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filtered_to_forecast(self):\n        # === simplify notation === #\n        A, C = self.ss.A, self.ss.C\n        Q = np.dot(C, C.T)\n\n        # === and then update === #\n        self.x_hat = dot(A, self.x_hat)\n        self.Sigma = dot(A, dot(self.Sigma, A.T)) + Q", "response": "Updates the moments of the time t filtering distribution to the moments of the predictive distribution which becomes the time\n        t + 1 prior\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stationary_values(self, method='doubling'):\n\n        # === simplify notation === #\n        A, C, G, H = self.ss.A, self.ss.C, self.ss.G, self.ss.H\n        Q, R = np.dot(C, C.T), np.dot(H, H.T)\n\n        # === solve Riccati equation, obtain Kalman gain === #\n        Sigma_infinity = solve_discrete_riccati(A.T, G.T, Q, R, method=method)\n        temp1 = dot(dot(A, Sigma_infinity), G.T)\n        temp2 = inv(dot(G, dot(Sigma_infinity, G.T)) + R)\n        K_infinity = dot(temp1, temp2)\n\n        # == record as attributes and return == #\n        self._Sigma_infinity, self._K_infinity = Sigma_infinity, K_infinity\n        return Sigma_infinity, K_infinity", "response": "Computes the stationary values of the entry - time sets for the current entry - time set."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind one mixed-action Nash equilibrium of a 2-player normal form game by the Lemke-Howson algorithm [2]_, implemented with \"complementary pivoting\" (see, e.g., von Stengel [3]_ for details). Parameters ---------- g : NormalFormGame NormalFormGame instance with 2 players. init_pivot : scalar(int), optional(default=0) Initial pivot, an integer k such that 0 <= k < m+n, where integers 0, ..., m-1 and m, ..., m+n-1 correspond to the actions of players 0 and 1, respectively. max_iter : scalar(int), optional(default=10**6) Maximum number of pivoting steps. capping : scalar(int), optional(default=None) If supplied, the routine is executed with the heuristics proposed by Codenotti et al. [1]_; see Notes below for details. full_output : bool, optional(default=False) If False, only the computed Nash equilibrium is returned. If True, the return value is `(NE, res)`, where `NE` is the Nash equilibrium and `res` is a `NashResult` object. Returns ------- NE : tuple(ndarray(float, ndim=1)) Tuple of computed Nash equilibrium mixed actions. res : NashResult Object containing information about the computation. Returned only when `full_output` is True. See `NashResult` for details. Examples -------- Consider the following game from von Stengel [3]_: >>> np.set_printoptions(precision=4) # Reduce the digits printed >>> bimatrix = [[(3, 3), (3, 2)], ... [(2, 2), (5, 6)], ... [(0, 3), (6, 1)]] >>> g = NormalFormGame(bimatrix) Obtain a Nash equilibrium of this game by `lemke_howson` with player 0's action 1 (out of the three actions 0, 1, and 2) as the initial pivot: >>> lemke_howson(g, init_pivot=1) (array([ 0. , 0.3333, 0.6667]), array([ 0.3333, 0.6667])) >>> g.is_nash(_) True Additional information is returned if `full_output` is set True: >>> NE, res = lemke_howson(g, init_pivot=1, full_output=True) >>> res.converged # Whether the routine has converged True >>> res.num_iter # Number of pivoting steps performed 4 Notes ----- * This routine is implemented with floating point arithmetic and thus is subject to numerical instability. * If `capping` is set to a positive integer, the routine is executed with the heuristics proposed by [1]_: * For k = `init_pivot`, `init_pivot` + 1, ..., `init_pivot` + (m+n-2), (modulo m+n), the Lemke-Howson algorithm is executed with k as the initial pivot and `capping` as the maximum number of pivoting steps. If the algorithm converges during this loop, then the Nash equilibrium found is returned. * Otherwise, the Lemke-Howson algorithm is executed with `init_pivot` + (m+n-1) (modulo m+n) as the initial pivot, with a limit `max_iter` on the total number of pivoting steps. Accoding to the simulation results for *uniformly random games*, for medium- to large-size games this heuristics outperforms the basic Lemke-Howson algorithm with a fixed initial pivot, where [1]_ suggests that `capping` be set to 10. References ---------- .. [1] B. Codenotti, S. De Rossi, and M. Pagan, \"An Experimental Analysis of Lemke-Howson Algorithm,\" arXiv:0811.3247, 2008. .. [2] C. E. Lemke and J. T. Howson, \"Equilibrium Points of Bimatrix Games,\" Journal of the Society for Industrial and Applied Mathematics (1964), 413-423. .. [3] B. von Stengel, \"Equilibrium Computation for Two-Player Games in Strategic and Extensive Form,\" Chapter 3, N. Nisan, T. Roughgarden, E. Tardos, and V. Vazirani eds., Algorithmic Game Theory, 2007.", "response": "def lemke_howson(g, init_pivot=0, max_iter=10**6, capping=None,\n                 full_output=False):\n    \"\"\"\n    Find one mixed-action Nash equilibrium of a 2-player normal form\n    game by the Lemke-Howson algorithm [2]_, implemented with\n    \"complementary pivoting\" (see, e.g., von Stengel [3]_ for details).\n\n    Parameters\n    ----------\n    g : NormalFormGame\n        NormalFormGame instance with 2 players.\n\n    init_pivot : scalar(int), optional(default=0)\n        Initial pivot, an integer k such that 0 <= k < m+n, where\n        integers 0, ..., m-1 and m, ..., m+n-1 correspond to the actions\n        of players 0 and 1, respectively.\n\n    max_iter : scalar(int), optional(default=10**6)\n        Maximum number of pivoting steps.\n\n    capping : scalar(int), optional(default=None)\n        If supplied, the routine is executed with the heuristics\n        proposed by Codenotti et al. [1]_; see Notes below for details.\n\n    full_output : bool, optional(default=False)\n        If False, only the computed Nash equilibrium is returned. If\n        True, the return value is `(NE, res)`, where `NE` is the Nash\n        equilibrium and `res` is a `NashResult` object.\n\n    Returns\n    -------\n    NE : tuple(ndarray(float, ndim=1))\n        Tuple of computed Nash equilibrium mixed actions.\n\n    res : NashResult\n        Object containing information about the computation. Returned\n        only when `full_output` is True. See `NashResult` for details.\n\n    Examples\n    --------\n    Consider the following game from von Stengel [3]_:\n\n    >>> np.set_printoptions(precision=4)  # Reduce the digits printed\n    >>> bimatrix = [[(3, 3), (3, 2)],\n    ...             [(2, 2), (5, 6)],\n    ...             [(0, 3), (6, 1)]]\n    >>> g = NormalFormGame(bimatrix)\n\n    Obtain a Nash equilibrium of this game by `lemke_howson` with player\n    0's action 1 (out of the three actions 0, 1, and 2) as the initial\n    pivot:\n\n    >>> lemke_howson(g, init_pivot=1)\n    (array([ 0.    ,  0.3333,  0.6667]), array([ 0.3333,  0.6667]))\n    >>> g.is_nash(_)\n    True\n\n    Additional information is returned if `full_output` is set True:\n\n    >>> NE, res = lemke_howson(g, init_pivot=1, full_output=True)\n    >>> res.converged  # Whether the routine has converged\n    True\n    >>> res.num_iter  # Number of pivoting steps performed\n    4\n\n    Notes\n    -----\n    * This routine is implemented with floating point arithmetic and\n      thus is subject to numerical instability.\n\n    * If `capping` is set to a positive integer, the routine is executed\n      with the heuristics proposed by [1]_:\n\n      * For k = `init_pivot`, `init_pivot` + 1, ..., `init_pivot` +\n        (m+n-2), (modulo m+n), the Lemke-Howson algorithm is executed\n        with k as the initial pivot and `capping` as the maximum number\n        of pivoting steps. If the algorithm converges during this loop,\n        then the Nash equilibrium found is returned.\n\n      * Otherwise, the Lemke-Howson algorithm is executed with\n        `init_pivot` + (m+n-1) (modulo m+n) as the initial pivot, with a\n        limit `max_iter` on the total number of pivoting steps.\n\n      Accoding to the simulation results for *uniformly random games*,\n      for medium- to large-size games this heuristics outperforms the\n      basic Lemke-Howson algorithm with a fixed initial pivot, where\n      [1]_ suggests that `capping` be set to 10.\n\n    References\n    ----------\n    .. [1] B. Codenotti, S. De Rossi, and M. Pagan, \"An Experimental\n       Analysis of Lemke-Howson Algorithm,\" arXiv:0811.3247, 2008.\n\n    .. [2] C. E. Lemke and J. T. Howson, \"Equilibrium Points of Bimatrix\n       Games,\" Journal of the Society for Industrial and Applied\n       Mathematics (1964), 413-423.\n\n    .. [3] B. von Stengel, \"Equilibrium Computation for Two-Player Games\n       in Strategic and Extensive Form,\" Chapter 3, N. Nisan, T.\n       Roughgarden, E. Tardos, and V. Vazirani eds., Algorithmic Game\n       Theory, 2007.\n\n    \"\"\"\n    try:\n        N = g.N\n    except:\n        raise TypeError('g must be a 2-player NormalFormGame')\n    if N != 2:\n        raise NotImplementedError('Implemented only for 2-player games')\n\n    payoff_matrices = g.payoff_arrays\n    nums_actions = g.nums_actions\n    total_num = sum(nums_actions)\n\n    msg = '`init_pivot` must be an integer k' + \\\n          'such that 0 <= k < {0}'.format(total_num)\n\n    if not isinstance(init_pivot, numbers.Integral):\n        raise TypeError(msg)\n\n    if not (0 <= init_pivot < total_num):\n        raise ValueError(msg)\n\n    if capping is None:\n        capping = max_iter\n\n    tableaux = tuple(\n        np.empty((nums_actions[1-i], total_num+1)) for i in range(N)\n    )\n    bases = tuple(np.empty(nums_actions[1-i], dtype=int) for i in range(N))\n\n    converged, num_iter, init_pivot_used = \\\n        _lemke_howson_capping(payoff_matrices, tableaux, bases, init_pivot,\n                              max_iter, capping)\n    NE = _get_mixed_actions(tableaux, bases)\n\n    if not full_output:\n        return NE\n\n    res = NashResult(NE=NE,\n                     converged=converged,\n                     num_iter=num_iter,\n                     max_iter=max_iter,\n                     init=init_pivot_used)\n\n    return NE, res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _lemke_howson_capping(payoff_matrices, tableaux, bases, init_pivot,\n                          max_iter, capping):\n    \"\"\"\n    Execute the Lemke-Howson algorithm with the heuristics proposed by\n    Codenotti et al.\n\n    Parameters\n    ----------\n    payoff_matrices : tuple(ndarray(ndim=2))\n        Tuple of two arrays representing payoff matrices, of shape\n        (m, n) and (n, m), respectively.\n\n    tableaux : tuple(ndarray(float, ndim=2))\n        Tuple of two arrays to be used to store the tableaux, of shape\n        (n, m+n+1) and (m, m+n+1), respectively. Modified in place.\n\n    bases : tuple(ndarray(int, ndim=1))\n        Tuple of two arrays to be used to store the bases, of shape (n,)\n        and (m,), respectively. Modified in place.\n\n    init_pivot : scalar(int)\n        Integer k such that 0 <= k < m+n.\n\n    max_iter : scalar(int)\n        Maximum number of pivoting steps.\n\n    capping : scalar(int)\n        Value for capping. If set equal to `max_iter`, then the routine\n        is equivalent to the standard Lemke-Howson algorithm.\n\n    \"\"\"\n    m, n = tableaux[1].shape[0], tableaux[0].shape[0]\n    init_pivot_curr = init_pivot\n    max_iter_curr = max_iter\n    total_num_iter = 0\n\n    for k in range(m+n-1):\n        capping_curr = min(max_iter_curr, capping)\n\n        _initialize_tableaux(payoff_matrices, tableaux, bases)\n        converged, num_iter = \\\n            _lemke_howson_tbl(tableaux, bases, init_pivot_curr, capping_curr)\n\n        total_num_iter += num_iter\n\n        if converged or total_num_iter >= max_iter:\n            return converged, total_num_iter, init_pivot_curr\n\n        init_pivot_curr += 1\n        if init_pivot_curr >= m + n:\n            init_pivot_curr -= m + n\n        max_iter_curr -= num_iter\n\n    _initialize_tableaux(payoff_matrices, tableaux, bases)\n    converged, num_iter = \\\n        _lemke_howson_tbl(tableaux, bases, init_pivot_curr, max_iter_curr)\n    total_num_iter += num_iter\n\n    return converged, total_num_iter, init_pivot_curr", "response": "Execute the Lemke - Howson algorithm with the heuristics proposed by the Codenotti et al."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _initialize_tableaux(payoff_matrices, tableaux, bases):\n    nums_actions = payoff_matrices[0].shape\n\n    consts = np.zeros(2)  # To be added to payoffs if min <= 0\n    for pl in range(2):\n        min_ = payoff_matrices[pl].min()\n        if min_ <= 0:\n            consts[pl] = min_ * (-1) + 1\n\n    for pl, (py_start, sl_start) in enumerate(zip((0, nums_actions[0]),\n                                                  (nums_actions[0], 0))):\n        for i in range(nums_actions[1-pl]):\n            for j in range(nums_actions[pl]):\n                tableaux[pl][i, py_start+j] = \\\n                    payoff_matrices[1-pl][i, j] + consts[1-pl]\n            for j in range(nums_actions[1-pl]):\n                if j == i:\n                    tableaux[pl][i, sl_start+j] = 1\n                else:\n                    tableaux[pl][i, sl_start+j] = 0\n            tableaux[pl][i, -1] = 1\n\n        for i in range(nums_actions[1-pl]):\n            bases[pl][i] = sl_start + i\n\n    return tableaux, bases", "response": "Initialize the basis and tableaux arrays in place."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming a pivoting step. Modify tableau in place.", "response": "def _pivoting(tableau, pivot, pivot_row):\n    \"\"\"\n    Perform a pivoting step. Modify `tableau` in place.\n\n    Parameters\n    ----------\n    tableau : ndarray(float, ndim=2)\n        Array containing the tableau.\n\n    pivot : scalar(int)\n        Pivot.\n\n    pivot_row : scalar(int)\n        Pivot row index.\n\n    Returns\n    -------\n    tableau : ndarray(float, ndim=2)\n        View to `tableau`.\n\n    \"\"\"\n    nrows, ncols = tableau.shape\n\n    pivot_elt = tableau[pivot_row, pivot]\n    for j in range(ncols):\n        tableau[pivot_row, j] /= pivot_elt\n\n    for i in range(nrows):\n        if i == pivot_row:\n            continue\n        multiplier = tableau[i, pivot]\n        if multiplier == 0:\n            continue\n        for j in range(ncols):\n            tableau[i, j] -= tableau[pivot_row, j] * multiplier\n\n    return tableau"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract mixed actions from tableaux and bases and return a tuple of the corresponding normalized mixed actions.", "response": "def _get_mixed_actions(tableaux, bases):\n    \"\"\"\n    From `tableaux` and `bases`, extract non-slack basic variables and\n    return a tuple of the corresponding, normalized mixed actions.\n\n    Parameters\n    ----------\n    tableaux : tuple(ndarray(float, ndim=2))\n        Tuple of two arrays containing the tableaux, of shape (n, m+n+1)\n        and (m, m+n+1), respectively.\n\n    bases : tuple(ndarray(int, ndim=1))\n        Tuple of two arrays containing the bases, of shape (n,) and\n        (m,), respectively.\n\n    Returns\n    -------\n    tuple(ndarray(float, ndim=1))\n        Tuple of mixed actions as given by the non-slack basic variables\n        in the tableaux.\n\n    \"\"\"\n    nums_actions = tableaux[1].shape[0], tableaux[0].shape[0]\n    num = nums_actions[0] + nums_actions[1]\n    out = np.zeros(num)\n\n    for pl, (start, stop) in enumerate(zip((0, nums_actions[0]),\n                                           (nums_actions[0], num))):\n        sum_ = 0.\n        for i in range(nums_actions[1-pl]):\n            k = bases[pl][i]\n            if start <= k < stop:\n                out[k] = tableaux[pl][i, -1]\n                sum_ += tableaux[pl][i, -1]\n        if sum_ != 0:\n            out[start:stop] /= sum_\n\n    return out[:nums_actions[0]], out[nums_actions[0]:]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef var_quadratic_sum(A, C, H, beta, x0):\n    # == Make sure that A, C, H and x0 are array_like == #\n\n    A, C, H = list(map(np.atleast_2d, (A, C, H)))\n    x0 = np.atleast_1d(x0)\n    # == Start computations == #\n    Q = scipy.linalg.solve_discrete_lyapunov(sqrt(beta) * A.T, H)\n    cq = dot(dot(C.T, Q), C)\n    v = np.trace(cq) * beta / (1 - beta)\n    q0 = dot(dot(x0.T, Q), x0) + v\n\n    return q0", "response": "r Computes the expected discounted quadratic sum of A C H and x0."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef m_quadratic_sum(A, B, max_it=50):\n\n    gamma1 = solve_discrete_lyapunov(A, B, max_it)\n\n    return gamma1", "response": "r Computes the quadratic sum of A and B."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tauchen(rho, sigma_u, m=3, n=7):\n\n    # standard deviation of y_t\n    std_y = np.sqrt(sigma_u**2 / (1 - rho**2))\n\n    # top of discrete state space\n    x_max = m * std_y\n\n    # bottom of discrete state space\n    x_min = -x_max\n\n    # discretized state space\n    x = np.linspace(x_min, x_max, n)\n\n    step = (x_max - x_min) / (n - 1)\n    half_step = 0.5 * step\n    P = np.empty((n, n))\n\n    _fill_tauchen(x, P, n, rho, sigma_u, half_step)\n\n    mc = MarkovChain(P, state_values=x)\n    return mc", "response": "r This function computes a linear Gaussian AR ( 1 ) process and returns a MarkovChain object that stores the transition matrix and state values of the next state."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef nelder_mead(fun, x0, bounds=np.array([[], []]).T, args=(), tol_f=1e-10,\n                tol_x=1e-10, max_iter=1000):\n    \"\"\"\n    .. highlight:: none\n\n    Maximize a scalar-valued function with one or more variables using the\n    Nelder-Mead method.\n\n    This function is JIT-compiled in `nopython` mode using Numba.\n\n    Parameters\n    ----------\n    fun : callable\n        The objective function to be maximized: `fun(x, *args) -> float`\n        where x is an 1-D array with shape (n,) and args is a tuple of the\n        fixed parameters needed to completely specify the function. This\n        function must be JIT-compiled in `nopython` mode using Numba.\n\n    x0 : ndarray(float, ndim=1)\n        Initial guess. Array of real elements of size (n,), where \u2018n\u2019 is the\n        number of independent variables.\n\n    bounds: ndarray(float, ndim=2), optional\n        Bounds for each variable for proposed solution, encoded as a sequence\n        of (min, max) pairs for each element in x. The default option is used\n        to specify no bounds on x.\n\n    args : tuple, optional\n        Extra arguments passed to the objective function.\n\n    tol_f : scalar(float), optional(default=1e-10)\n        Tolerance to be used for the function value convergence test.\n\n    tol_x : scalar(float), optional(default=1e-10)\n        Tolerance to be used for the function domain convergence test.\n\n    max_iter : scalar(float), optional(default=1000)\n        The maximum number of allowed iterations.\n\n    Returns\n    ----------\n    results : namedtuple\n        A namedtuple containing the following items:\n        ::\n\n            \"x\" : Approximate local maximizer\n            \"fun\" : Approximate local maximum value\n            \"success\" : 1 if the algorithm successfully terminated, 0 otherwise\n            \"nit\" : Number of iterations\n            \"final_simplex\" : Vertices of the final simplex\n\n    Examples\n    --------\n    >>> @njit\n    ... def rosenbrock(x):\n    ...     return -(100 * (x[1] - x[0] ** 2) ** 2 + (1 - x[0])**2)\n    ...\n    >>> x0 = np.array([-2, 1])\n    >>> qe.optimize.nelder_mead(rosenbrock, x0)\n    results(x=array([0.99999814, 0.99999756]), fun=-1.6936258239463265e-10,\n            success=True, nit=110,\n            final_simplex=array([[0.99998652, 0.9999727],\n                                 [1.00000218, 1.00000301],\n                                 [0.99999814, 0.99999756]]))\n\n    Notes\n    --------\n    This algorithm has a long history of successful use in applications, but it\n    will usually be slower than an algorithm that uses first or second\n    derivative information. In practice, it can have poor performance in\n    high-dimensional problems and is not robust to minimizing complicated\n    functions. Additionally, there currently is no complete theory describing\n    when the algorithm will successfully converge to the minimum, or how fast\n    it will if it does.\n\n    References\n    ----------\n\n    .. [1] J. C. Lagarias, J. A. Reeds, M. H. Wright and P. E. Wright,\n           Convergence Properties of the Nelder\u2013Mead Simplex Method in Low\n           Dimensions, SIAM. J. Optim. 9, 112\u2013147 (1998).\n\n    .. [2] S. Singer and S. Singer, Efficient implementation of the Nelder\u2013Mead\n           search algorithm, Appl. Numer. Anal. Comput. Math., vol. 1, no. 2,\n           pp. 524\u2013534, 2004.\n\n    .. [3] J. A. Nelder and R. Mead, A simplex method for function\n           minimization, Comput. J. 7, 308\u2013313 (1965).\n\n    .. [4] Gao, F. and Han, L., Implementing the Nelder-Mead simplex algorithm\n           with adaptive parameters, Comput Optim Appl (2012) 51: 259.\n\n    .. [5] http://www.scholarpedia.org/article/Nelder-Mead_algorithm\n\n    .. [6] http://www.brnt.eu/phd/node10.html#SECTION00622200000000000000\n\n    .. [7] Chase Coleman's tutorial on Nelder Mead\n\n    .. [8] SciPy's Nelder-Mead implementation\n\n    \"\"\"\n    vertices = _initialize_simplex(x0, bounds)\n\n    results = _nelder_mead_algorithm(fun, vertices, bounds, args=args,\n                                     tol_f=tol_f, tol_x=tol_x,\n                                     max_iter=max_iter)\n\n    return results", "response": "Maximize a scalar - valued function with one or more independent variables using Nelder - Mead method."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating an initial simplex for the Nelder - Mead method using Numba.", "response": "def _initialize_simplex(x0, bounds):\n    \"\"\"\n    Generates an initial simplex for the Nelder-Mead method. JIT-compiled in\n    `nopython` mode using Numba.\n\n    Parameters\n    ----------\n    x0 : ndarray(float, ndim=1)\n        Initial guess. Array of real elements of size (n,), where \u2018n\u2019 is the\n        number of independent variables.\n\n    bounds: ndarray(float, ndim=2)\n        Sequence of (min, max) pairs for each element in x0.\n\n    Returns\n    ----------\n    vertices : ndarray(float, ndim=2)\n        Initial simplex with shape (n+1, n).\n\n    \"\"\"\n    n = x0.size\n\n    vertices = np.empty((n + 1, n), dtype=np.float64)\n\n    # Broadcast x0 on row dimension\n    vertices[:] = x0\n\n    nonzdelt = 0.05\n    zdelt = 0.00025\n\n    for i in range(n):\n        # Generate candidate coordinate\n        if vertices[i + 1, i] != 0.:\n            vertices[i + 1, i] *= (1 + nonzdelt)\n        else:\n            vertices[i + 1, i] = zdelt\n\n    return vertices"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _check_params(\u03c1, \u03c7, \u03b3, \u03c3, bounds, n):\n    if \u03c1 < 0:\n        raise ValueError(\"\u03c1 must be strictly greater than 0.\")\n    if \u03c7 < 1:\n        raise ValueError(\"\u03c7 must be strictly greater than 1.\")\n    if \u03c7 < \u03c1:\n        raise ValueError(\"\u03c7 must be strictly greater than \u03c1.\")\n    if \u03b3 < 0 or \u03b3 > 1:\n        raise ValueError(\"\u03b3 must be strictly between 0 and 1.\")\n    if \u03c3 < 0 or \u03c3 > 1:\n        raise ValueError(\"\u03c3 must be strictly between 0 and 1.\")\n\n    if not (bounds.shape == (0, 2) or bounds.shape == (n, 2)):\n        raise ValueError(\"The shape of `bounds` is not valid.\")\n    if (np.atleast_2d(bounds)[:, 0] > np.atleast_2d(bounds)[:, 1]).any():\n        raise ValueError(\"Lower bounds must be greater than upper bounds.\")", "response": "Checks whether the parameters for the Nelder - Mead algorithm are valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks whether x is within bounds. JIT - compiled in nopython mode uses Numba.", "response": "def _check_bounds(x, bounds):\n    \"\"\"\n    Checks whether `x` is within `bounds`. JIT-compiled in `nopython` mode\n    using Numba.\n\n    Parameters\n    ----------\n    x : ndarray(float, ndim=1)\n        1-D array with shape (n,) of independent variables.\n\n    bounds: ndarray(float, ndim=2)\n        Sequence of (min, max) pairs for each element in x.\n\n    Returns\n    ----------\n    bool\n        `True` if `x` is within `bounds`, `False` otherwise.\n\n    \"\"\"\n    if bounds.shape == (0, 2):\n        return True\n    else:\n        return ((np.atleast_2d(bounds)[:, 0] <= x).all() and\n                (x <= np.atleast_2d(bounds)[:, 1]).all())"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps for bounding and taking the negative of fun for the base Nelder - Mead algorithm.", "response": "def _neg_bounded_fun(fun, bounds, x, args=()):\n    \"\"\"\n    Wrapper for bounding and taking the negative of `fun` for the\n    Nelder-Mead algorithm. JIT-compiled in `nopython` mode using Numba.\n\n    Parameters\n    ----------\n    fun : callable\n        The objective function to be minimized.\n            `fun(x, *args) -> float`\n        where x is an 1-D array with shape (n,) and args is a tuple of the\n        fixed parameters needed to completely specify the function. This\n        function must be JIT-compiled in `nopython` mode using Numba.\n\n    bounds: ndarray(float, ndim=2)\n        Sequence of (min, max) pairs for each element in x.\n\n    x : ndarray(float, ndim=1)\n        1-D array with shape (n,) of independent variables at which `fun` is\n        to be evaluated.\n\n    args : tuple, optional\n        Extra arguments passed to the objective function.\n\n    Returns\n    ----------\n    scalar\n        `-fun(x, *args)` if x is within `bounds`, `np.inf` otherwise.\n\n    \"\"\"\n    if _check_bounds(x, bounds):\n        return -fun(x, *args)\n    else:\n        return np.inf"}
