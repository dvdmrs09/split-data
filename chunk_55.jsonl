{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetects the MIME type of the stream and return it in text / plain.", "response": "def detectType1(option, urlOrPath, serverEndpoint=ServerEndpoint, verbose=Verbose, tikaServerJar=TikaServerJar, \n               responseMimeType='text/plain',\n               services={'type': '/detect/stream'}, config_path=None):\n    '''\n    Detect the MIME/media type of the stream and return it in text/plain.\n    :param option:\n    :param urlOrPath:\n    :param serverEndpoint:\n    :param verbose:\n    :param tikaServerJar:\n    :param responseMimeType:\n    :param services:\n    :return:\n    '''\n    path, mode = getRemoteFile(urlOrPath, TikaFilesPath)\n    if option not in services:\n        log.exception('Detect option must be one of %s' % binary_string(services.keys()))\n        raise TikaException('Detect option must be one of %s' % binary_string(services.keys()))\n    service = services[option]\n    status, response = callServer('put', serverEndpoint, service, open(path, 'rb'),\n            {\n                'Accept': responseMimeType,\n                'Content-Disposition': make_content_disposition_header(path)\n            },\n            verbose, tikaServerJar, config_path=config_path)\n    if csvOutput == 1:\n        return(status, urlOrPath.decode(\"UTF-8\") + \",\" + response)\n    else:\n        return (status, response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getConfig(option, serverEndpoint=ServerEndpoint, verbose=Verbose, tikaServerJar=TikaServerJar, responseMimeType='application/json',\n              services={'mime-types': '/mime-types', 'detectors': '/detectors', 'parsers': '/parsers/details'}):\n    '''\n    Get the configuration of the Tika Server (parsers, detectors, etc.) and return it in JSON format.\n    :param option:\n    :param serverEndpoint:\n    :param verbose:\n    :param tikaServerJar:\n    :param responseMimeType:\n    :param services:\n    :return:\n    '''\n    if option not in services:\n        die('config option must be one of mime-types, detectors, or parsers')\n    service = services[option]\n    status, response = callServer('get', serverEndpoint, service, None, {'Accept': responseMimeType}, verbose, tikaServerJar)\n    return (status, response)", "response": "Get the configuration of the Tika Server"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef callServer(verb, serverEndpoint, service, data, headers, verbose=Verbose, tikaServerJar=TikaServerJar,\n               httpVerbs={'get': requests.get, 'put': requests.put, 'post': requests.post}, classpath=None,\n                rawResponse=False,config_path=None):\n    '''\n    Call the Tika Server, do some error checking, and return the response.\n    :param verb:\n    :param serverEndpoint:\n    :param service:\n    :param data:\n    :param headers:\n    :param verbose:\n    :param tikaServerJar:\n    :param httpVerbs:\n    :param classpath:\n    :return:\n    '''\n    parsedUrl = urlparse(serverEndpoint) \n    serverHost = parsedUrl.hostname\n    scheme = parsedUrl.scheme\n\n    port = parsedUrl.port\n    if classpath is None:\n        classpath = TikaServerClasspath\n    \n    global TikaClientOnly\n    if not TikaClientOnly:\n        serverEndpoint = checkTikaServer(scheme, serverHost, port, tikaServerJar, classpath, config_path)\n\n    serviceUrl  = serverEndpoint + service\n    if verb not in httpVerbs:\n        log.exception('Tika Server call must be one of %s' % binary_string(httpVerbs.keys()))\n        raise TikaException('Tika Server call must be one of %s' % binary_string(httpVerbs.keys()))\n    verbFn = httpVerbs[verb]\n\n    if Windows and hasattr(data, \"read\"):\n        data = data.read()\n        \n    encodedData = data\n    if type(data) is unicode_string:\n        encodedData = data.encode('utf-8')\n\n    resp = verbFn(serviceUrl, encodedData, headers=headers, verify=False)\n    if verbose:\n        print(sys.stderr, \"Request headers: \", headers)\n        print(sys.stderr, \"Response headers: \", resp.headers)\n    if resp.status_code != 200:\n        log.warning('Tika server returned status: %d', resp.status_code)\n\n    resp.encoding = \"utf-8\"\n    if rawResponse:\n        return (resp.status_code, resp.content)\n    else:\n        return (resp.status_code, resp.text)", "response": "Call the Tika Server and return the response."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef checkTikaServer(scheme=\"http\", serverHost=ServerHost, port=Port, tikaServerJar=TikaServerJar, classpath=None, config_path=None):\n    '''\n    Check that tika-server is running.  If not, download JAR file and start it up.\n    :param scheme: e.g. http or https\n    :param serverHost:\n    :param port:\n    :param tikaServerJar:\n    :param classpath:\n    :return:\n    '''\n    if classpath is None:\n        classpath = TikaServerClasspath\n    if port is None:\n        port = '443' if scheme == 'https' else '80'\n\n    urlp = urlparse(tikaServerJar)\n    serverEndpoint = '%s://%s:%s' % (scheme, serverHost, port)\n    jarPath = os.path.join(TikaJarPath, 'tika-server.jar')\n    if 'localhost' in serverEndpoint or '127.0.0.1' in serverEndpoint:\n        alreadyRunning = checkPortIsOpen(serverHost, port)\n\n        if not alreadyRunning:\n            if not os.path.isfile(jarPath) and urlp.scheme != '':\n                getRemoteJar(tikaServerJar, jarPath)\n\n            if not checkJarSig(tikaServerJar, jarPath):\n                os.remove(jarPath)\n                tikaServerJar = getRemoteJar(tikaServerJar, jarPath)\n\n            status = startServer(jarPath, TikaJava, serverHost, port, classpath, config_path)\n            if not status:\n                log.error(\"Failed to receive startup confirmation from startServer.\")\n                raise RuntimeError(\"Unable to start Tika server.\")\n    return serverEndpoint", "response": "Check that Tika - server is running."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef checkJarSig(tikaServerJar, jarPath):\n    '''\n    Checks the signature of Jar\n    :param tikaServerJar:\n    :param jarPath:\n    :return: ``True`` if the signature of the jar matches\n    '''\n    if not os.path.isfile(jarPath + \".md5\"):\n        getRemoteJar(tikaServerJar + \".md5\", jarPath + \".md5\")\n    m = hashlib.md5()\n    with open(jarPath, 'rb') as f:\n        binContents = f.read()\n        m.update(binContents)\n        with open(jarPath + \".md5\", \"r\") as em:\n            existingContents = em.read()\n            return existingContents == m.hexdigest()", "response": "Checks the signature of the jar."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstarting Tika Server :param tikaServerJar: path to tika server jar :param serverHost: the host interface address to be used for binding the service :param port: the host port to be used for binding the service :param classpath: Class path value to pass to JVM :return: None", "response": "def startServer(tikaServerJar, java_path = TikaJava, serverHost = ServerHost, port = Port, classpath=None, config_path=None):\n    '''\n    Starts Tika Server\n    :param tikaServerJar: path to tika server jar\n    :param serverHost: the host interface address to be used for binding the service\n    :param port: the host port to be used for binding the service\n    :param classpath: Class path value to pass to JVM\n    :return: None\n    '''\n    if classpath is None:\n        classpath = TikaServerClasspath\n\n    host = \"localhost\"\n    if Windows:\n        host = \"0.0.0.0\"\n\n    if classpath:\n        classpath += \":\" + tikaServerJar\n    else:\n        classpath = tikaServerJar\n\n    # setup command string\n    cmd_string = \"\"\n    if not config_path:\n        cmd_string = '%s -cp %s org.apache.tika.server.TikaServerCli --port %s --host %s &' \\\n                     % (java_path, classpath, port, host)\n    else:\n        cmd_string = '%s -cp %s org.apache.tika.server.TikaServerCli --port %s --host %s --config %s &' \\\n                     % (java_path, classpath, port, host, config_path)\n\n    # Check that we can write to log path\n    try:\n        tika_log_file_path = os.path.join(TikaServerLogFilePath, 'tika-server.log')\n        logFile = open(tika_log_file_path, 'w')\n    except PermissionError as e:\n        log.error(\"Unable to create tika-server.log at %s due to permission error.\" % (TikaServerLogFilePath))\n        return False\n\n    # Check that specified java binary is available on path\n    try:\n        _ = Popen(java_path, stdout=open(os.devnull, \"w\"), stderr=open(os.devnull, \"w\"))\n    except FileNotFoundError as e:\n        log.error(\"Unable to run java; is it installed?\")\n        return False\n\n    # Run java with jar args\n    cmd = Popen(cmd_string, stdout=logFile, stderr=STDOUT, shell=True)\n\n    # Check logs and retry as configured\n    try_count = 0\n    is_started = False\n    while try_count < TikaStartupMaxRetry:\n        with open(tika_log_file_path, \"r\") as tika_log_file_tmp:\n            # check for INFO string to confirm listening endpoint\n            if \"Started Apache Tika server at\" in tika_log_file_tmp.read():\n                is_started = True\n            else:\n                log.warning(\"Failed to see startup log message; retrying...\")\n        time.sleep(TikaStartupSleep)\n        try_count += 1\n\n    if not is_started:\n        log.error(\"Tika startup log message not received after %d tries.\" % (TikaStartupMaxRetry))\n        return False\n    else:\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef toFilename(url):\n    '''\n    gets url and returns filename\n    '''\n    urlp = urlparse(url)\n    path = urlp.path\n    if not path:\n        path = \"file_{}\".format(int(time.time()))\n    value = re.sub(r'[^\\w\\s\\.\\-]', '-', path).strip().lower()\n    return re.sub(r'[-\\s]+', '-', value).strip(\"-\")[-200:]", "response": "gets url and returns filename"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getRemoteFile(urlOrPath, destPath):\n    '''\n    Fetches URL to local path or just returns absolute path.\n    :param urlOrPath: resource locator, generally URL or path\n    :param destPath: path to store the resource, usually a path on file system\n    :return: tuple having (path, 'local'/'remote')\n    '''\n    urlp = urlparse(urlOrPath)\n    if urlp.scheme == '':\n        return (os.path.abspath(urlOrPath), 'local')\n    elif urlp.scheme not in ('http', 'https'):\n        return (urlOrPath, 'local')\n    else:\n        filename = toFilename(urlOrPath)\n        destPath = destPath + '/' + filename\n        log.info('Retrieving %s to %s.' % (urlOrPath, destPath))\n        try:\n            urlretrieve(urlOrPath, destPath)\n        except IOError:\n            # monkey patch fix for SSL/Windows per Tika-Python #54 \n            # https://github.com/chrismattmann/tika-python/issues/54\n            import ssl\n            if hasattr(ssl, '_create_unverified_context'):\n                ssl._create_default_https_context = ssl._create_unverified_context\n            # delete whatever we had there\n            if os.path.exists(destPath) and os.path.isfile(destPath):\n                os.remove(destPath)\n            urlretrieve(urlOrPath, destPath)\n        return (destPath, 'remote')", "response": "Fetches URL to local path or just returns absolute path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if the specified port is open.", "response": "def checkPortIsOpen(remoteServerHost=ServerHost, port = Port):\n    '''\n    Checks if the specified port is open\n    :param remoteServerHost: the host address\n    :param port: port which needs to be checked\n    :return: ``True`` if port is open, ``False`` otherwise\n    '''\n    remoteServerIP  = socket.gethostbyname(remoteServerHost)\n    try:\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        result = sock.connect_ex((remoteServerIP, int(port)))\n        if result == 0:\n            return True\n        else :\n            return False\n        sock.close()\n        #FIXME: the above line is unreachable\n\n    except KeyboardInterrupt:\n        print(\"You pressed Ctrl+C\")\n        sys.exit()\n\n    except socket.gaierror:\n        print('Hostname could not be resolved. Exiting')\n        sys.exit()\n\n    except socket.error:\n        print(\"Couldn't connect to server\")\n        sys.exit()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun Tika from command line according to USAGE.", "response": "def main(argv=None):\n    \"\"\"Run Tika from command line according to USAGE.\"\"\"\n    global Verbose\n    global EncodeUtf8\n    global csvOutput\n    if argv is None:\n        argv = sys.argv\n\n    if (len(argv) < 3 and not (('-h' in argv) or ('--help' in argv))):\n        log.exception('Bad args')\n        raise TikaException('Bad args')\n    try:\n        opts, argv = getopt.getopt(argv[1:], 'hi:s:o:p:v:e:c',\n          ['help', 'install=', 'server=', 'output=', 'port=', 'verbose', 'encode', 'csv'])\n    except getopt.GetoptError as opt_error:\n        msg, bad_opt = opt_error\n        log.exception(\"%s error: Bad option: %s, %s\" % (argv[0], bad_opt, msg))\n        raise TikaException(\"%s error: Bad option: %s, %s\" % (argv[0], bad_opt, msg))\n\n    tikaServerJar = TikaServerJar\n    serverHost = ServerHost\n    outDir = '.'\n    port = Port\n    for opt, val in opts:\n        if opt   in ('-h', '--help'):    echo2(USAGE); sys.exit()\n        elif opt in ('--install'):       tikaServerJar = val\n        elif opt in ('--server'):        serverHost = val\n        elif opt in ('-o', '--output'):  outDir = val\n        elif opt in ('--port'):          port = val\n        elif opt in ('-v', '--verbose'): Verbose = 1\n        elif opt in ('-e', '--encode'): EncodeUtf8 = 1\n        elif opt in ('-c', '--csv'): csvOutput = 1\n        else:\n            raise TikaException(USAGE)\n\n    cmd = argv[0]\n    option = argv[1]\n    try:\n        paths = argv[2:]\n    except:\n        paths = None\n    return runCommand(cmd, option, paths, port, outDir, serverHost=serverHost, tikaServerJar=tikaServerJar, verbose=Verbose, encode=EncodeUtf8)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_buffer(string, config_path=None):\n    '''\n    Detects MIME type of the buffered content\n    :param string: buffered content whose type needs to be detected\n    :return:\n    '''\n    status, response = callServer('put', ServerEndpoint, '/detect/stream', string,\n                                  {'Accept': 'text/plain'}, False, config_path=config_path)\n    return response", "response": "Detects MIME type of the buffered content and returns the response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a tar file into a sequence of Tika objects.", "response": "def from_file(filename, serverEndpoint=ServerEndpoint):\n    '''\n    Parse from file\n    :param filename: file\n    :param serverEndpoint: Tika server end point (optional)\n    :return:\n    '''\n    tarOutput = parse1('unpack', filename, serverEndpoint,\n                       responseMimeType='application/x-tar',\n                       services={'meta': '/meta', 'text': '/tika',\n                                 'all': '/rmeta/xml', 'unpack': '/unpack/all'},\n                       rawResponse=True)\n    return _parse(tarOutput)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_buffer(string, serverEndpoint=ServerEndpoint):\n    '''\n    Parse from buffered content\n    :param string:  buffered content\n    :param serverEndpoint: Tika server URL (Optional)\n    :return: parsed content\n    '''\n    status, response = callServer('put', serverEndpoint, '/unpack/all', string,\n                                  {'Accept': 'application/x-tar'}, False,\n                                  rawResponse=True)\n\n    return _parse((status, response))", "response": "Parse a buffered content into a new object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_file(filename, srcLang, destLang, serverEndpoint=ServerEndpoint):\n    '''\n    Traslates the content of source file to destination language\n    :param filename: file whose contents needs translation\n    :param srcLang: name of language of input file\n    :param destLang: name of language of desired language\n    :param serverEndpoint: Tika server end point (Optional)\n    :return: translated content\n    '''\n    jsonOutput = doTranslate1(srcLang+':'+destLang, filename, serverEndpoint)\n    return jsonOutput[1]", "response": "Translate a file to another language"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_buffer(string, srcLang, destLang, serverEndpoint=ServerEndpoint):\n    '''\n    Translates content from source language to desired destination language\n    :param string: input content which needs translation\n    :param srcLang: name of language of the input content\n    :param destLang: name of the desired language for translation\n    :param serverEndpoint:\n    :return:\n    '''\n    status, response = callServer('put', ServerEndpoint, '/translate/all/'+Translator+'/'+srcLang+'/'+destLang, \n                                  string, {'Accept': 'text/plain'}, False)\n    return response", "response": "Translate a string from source language to destination language."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef auto_from_file(filename, destLang, serverEndpoint=ServerEndpoint):\n    '''\n    Translates contents of a file to desired language by auto detecting the source language\n    :param filename: file whose contents needs translation\n    :param destLang: name of the desired language for translation\n    :param serverEndpoint: Tika server end point (Optional)\n    :return:\n    '''\n    jsonOutput = doTranslate1(destLang, filename, serverEndpoint)\n    return jsonOutput[1]", "response": "Auto - detects the source language of a file and returns the contents of the file in the desired language."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef auto_from_buffer(string, destLang, serverEndpoint=ServerEndpoint):\n    '''\n    Translates content to desired language by auto detecting the source language\n    :param string: input content which needs translation\n    :param destLang: name of the desired language for translation\n    :param serverEndpoint: Tika server end point (Optional)\n    :return:\n    '''\n    status, response = callServer('put', ServerEndpoint, '/translate/all/'+Translator+'/'+destLang, \n                                  string, {'Accept': 'text/plain'}, False)\n    return response", "response": "Auto detects the source language of the input content and translates it to the destination language"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_file(filename, mime=False):\n    m = _get_magic_type(mime)\n    return m.from_file(filename)", "response": "Accepts a filename and returns the detected filetype."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_buffer(buffer, mime=False):\n    m = _get_magic_type(mime)\n    return m.from_buffer(buffer)", "response": "Takes a binary string and returns the detected filetype."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndecrease the saturation channel of a color by some percent.", "response": "def desaturate(color, prop):\n    \"\"\"Decrease the saturation channel of a color by some percent.\n\n    Parameters\n    ----------\n    color : matplotlib color\n        hex, rgb-tuple, or html color name\n    prop : float\n        saturation channel of color will be multiplied by this value\n\n    Returns\n    -------\n    new_color : rgb tuple\n        desaturated color code in RGB tuple representation\n\n    \"\"\"\n    # Check inputs\n    if not 0 <= prop <= 1:\n        raise ValueError(\"prop must be between 0 and 1\")\n\n    # Get rgb tuple rep\n    rgb = mplcol.colorConverter.to_rgb(color)\n\n    # Convert to hls\n    h, l, s = colorsys.rgb_to_hls(*rgb)\n\n    # Desaturate the saturation channel\n    s *= prop\n\n    # Convert back to rgb\n    new_color = colorsys.hls_to_rgb(h, l, s)\n\n    return new_color"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of colors defining a color palette.", "response": "def color_palette(name=None, n_colors=6, desat=None):\n    \"\"\"Return a list of colors defining a color palette.\n\n    Availible seaborn palette names:\n        deep, muted, bright, pastel, dark, colorblind\n\n    Other options:\n        hls, husl, any matplotlib palette\n\n    Matplotlib paletes can be specified as reversed palettes by appending\n    \"_r\" to the name or as dark palettes by appending \"_d\" to the name.\n\n    This function can also be used in a ``with`` statement to temporarily\n    set the color cycle for a plot or set of plots.\n\n    Parameters\n    ----------\n    name: None, string, or sequence\n        Name of palette or None to return current palette. If a\n        sequence, input colors are used but possibly cycled and\n        desaturated.\n    n_colors : int\n        Number of colors in the palette. If larger than the number of\n        colors in the palette, they will cycle.\n    desat : float\n        Value to desaturate each color by.\n\n    Returns\n    -------\n    palette : list of RGB tuples.\n        Color palette.\n\n    Examples\n    --------\n    >>> p = color_palette(\"muted\")\n\n    >>> p = color_palette(\"Blues_d\", 10)\n\n    >>> p = color_palette(\"Set1\", desat=.7)\n\n    >>> import matplotlib.pyplot as plt\n    >>> with color_palette(\"husl\", 8):\n    ...     f, ax = plt.subplots()\n    ...     ax.plot(x, y)                  # doctest: +SKIP\n\n    See Also\n    --------\n    set_palette : set the default color cycle for all plots.\n    axes_style : define parameters to set the style of plots\n    plotting_context : define parameters to scale plot elements\n\n    \"\"\"\n    seaborn_palettes = dict(\n        deep=[\"#4C72B0\", \"#55A868\", \"#C44E52\",\n              \"#8172B2\", \"#CCB974\", \"#64B5CD\"],\n        muted=[\"#4878CF\", \"#6ACC65\", \"#D65F5F\",\n               \"#B47CC7\", \"#C4AD66\", \"#77BEDB\"],\n        pastel=[\"#92C6FF\", \"#97F0AA\", \"#FF9F9A\",\n                \"#D0BBFF\", \"#FFFEA3\", \"#B0E0E6\"],\n        bright=[\"#003FFF\", \"#03ED3A\", \"#E8000B\",\n                \"#8A2BE2\", \"#FFC400\", \"#00D7FF\"],\n        dark=[\"#001C7F\", \"#017517\", \"#8C0900\",\n              \"#7600A1\", \"#B8860B\", \"#006374\"],\n        colorblind=[\"#0072B2\", \"#009E73\", \"#D55E00\",\n                    \"#CC79A7\", \"#F0E442\", \"#56B4E9\"],\n    )\n\n    if name is None:\n        palette = mpl.rcParams[\"axes.color_cycle\"]\n    elif not isinstance(name, string_types):\n        palette = name\n    elif name == \"hls\":\n        palette = hls_palette(n_colors)\n    elif name == \"husl\":\n        palette = husl_palette(n_colors)\n    elif name in seaborn_palettes:\n        palette = seaborn_palettes[name]\n    elif name in dir(mpl.cm):\n        palette = mpl_palette(name, n_colors)\n    elif name[:-2] in dir(mpl.cm):\n        palette = mpl_palette(name, n_colors)\n    else:\n        raise ValueError(\"%s is not a valid palette name\" % name)\n\n    if desat is not None:\n        palette = [desaturate(c, desat) for c in palette]\n\n    # Always return as many colors as we asked for\n    pal_cycle = cycle(palette)\n    palette = [next(pal_cycle) for _ in range(n_colors)]\n\n    # Always return in r, g, b tuple format\n    try:\n        palette = map(mpl.colors.colorConverter.to_rgb, palette)\n        palette = _ColorPalette(palette)\n    except ValueError:\n        raise ValueError(\"Could not generate a palette for %s\" % str(name))\n\n    return palette"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mpl_palette(name, n_colors=6):\n    brewer_qual_pals = {\"Accent\": 8, \"Dark2\": 8, \"Paired\": 12,\n                        \"Pastel1\": 9, \"Pastel2\": 8,\n                        \"Set1\": 9, \"Set2\": 8, \"Set3\": 12}\n\n    if name.endswith(\"_d\"):\n        pal = [\"#333333\"]\n        pal.extend(color_palette(name.replace(\"_d\", \"_r\"), 2))\n        cmap = blend_palette(pal, n_colors, as_cmap=True)\n    else:\n        cmap = getattr(mpl.cm, name)\n    if name in brewer_qual_pals:\n        bins = np.linspace(0, 1, brewer_qual_pals[name])[:n_colors]\n    else:\n        bins = np.linspace(0, 1, n_colors + 2)[1:-1]\n    palette = list(map(tuple, cmap(bins)[:, :3]))\n\n    return palette", "response": "Return discrete colors from a matplotlib palette."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a palette that blends from a deep gray to color.", "response": "def dark_palette(color, n_colors=6, reverse=False, as_cmap=False):\n    \"\"\"Make a palette that blends from a deep gray to `color`.\n\n    Parameters\n    ----------\n    color : matplotlib color\n        hex, rgb-tuple, or html color name\n    n_colors : int, optional\n        number of colors in the palette\n    reverse : bool, optional\n        if True, reverse the direction of the blend\n    as_cmap : bool, optional\n        if True, return as a matplotlib colormap instead of list\n\n    Returns\n    -------\n    palette : list or colormap\n\n    \"\"\"\n    gray = \"#222222\"\n    colors = [color, gray] if reverse else [gray, color]\n    return blend_palette(colors, n_colors, as_cmap)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef blend_palette(colors, n_colors=6, as_cmap=False):\n    name = \"-\".join(map(str, colors))\n    pal = mpl.colors.LinearSegmentedColormap.from_list(name, colors)\n    if not as_cmap:\n        pal = pal(np.linspace(0, 1, n_colors))\n    return pal", "response": "Make a palette that blends between a list of colors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef xkcd_palette(colors):\n    palette = [xkcd_rgb[name] for name in colors]\n    return color_palette(palette, len(palette))", "response": "Make a palette with color names from the xkcd color survey."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake a sequential palette from the cubehelix system. This produces a colormap with linearly-decreasing (or increasing) brightness. That means that information will be preserved if printed to black and white or viewed by someone who is colorblind. \"cubehelix\" is also availible as a matplotlib-based palette, but this function gives the user more control over the look of the palette and has a different set of defaults. Parameters ---------- n_colors : int Number of colors in the palette. start : float, 0 <= start <= 3 The hue at the start of the helix. rot : float Rotations around the hue wheel over the range of the palette. gamma : float 0 <= gamma Gamma factor to emphasize darker (gamma < 1) or lighter (gamma > 1) colors. hue : float, 0 <= hue <= 1 Saturation of the colors. dark : float 0 <= dark <= 1 Intensity of the darkest color in the palette. light : float 0 <= light <= 1 Intensity of the lightest color in the palette. reverse : bool If True, the palette will go from dark to light. as_cmap : bool If True, return a matplotlib colormap instead of a list of colors. Returns ------- palette : list or colormap References ---------- Green, D. A. (2011). \"A colour scheme for the display of astronomical intensity images\". Bulletin of the Astromical Society of India, Vol. 39, p. 289-295.", "response": "def cubehelix_palette(n_colors=6, start=0, rot=.4, gamma=1.0, hue=0.8,\n                      light=.85, dark=.15, reverse=False, as_cmap=False):\n    \"\"\"Make a sequential palette from the cubehelix system.\n\n    This produces a colormap with linearly-decreasing (or increasing)\n    brightness. That means that information will be preserved if printed to\n    black and white or viewed by someone who is colorblind.  \"cubehelix\" is\n    also availible as a matplotlib-based palette, but this function gives the\n    user more control over the look of the palette and has a different set of\n    defaults.\n\n    Parameters\n    ----------\n    n_colors : int\n        Number of colors in the palette.\n    start : float, 0 <= start <= 3\n        The hue at the start of the helix.\n    rot : float\n        Rotations around the hue wheel over the range of the palette.\n    gamma : float 0 <= gamma\n        Gamma factor to emphasize darker (gamma < 1) or lighter (gamma > 1)\n        colors.\n    hue : float, 0 <= hue <= 1\n        Saturation of the colors.\n    dark : float 0 <= dark <= 1\n        Intensity of the darkest color in the palette.\n    light : float 0 <= light <= 1\n        Intensity of the lightest color in the palette.\n    reverse : bool\n        If True, the palette will go from dark to light.\n    as_cmap : bool\n        If True, return a matplotlib colormap instead of a list of colors.\n\n    Returns\n    -------\n    palette : list or colormap\n\n    References\n    ----------\n    Green, D. A. (2011). \"A colour scheme for the display of astronomical\n    intensity images\". Bulletin of the Astromical Society of India, Vol. 39,\n    p. 289-295.\n\n    \"\"\"\n    cdict = mpl._cm.cubehelix(gamma, start, rot, hue)\n    cmap = mpl.colors.LinearSegmentedColormap(\"cubehelix\", cdict)\n\n    x = np.linspace(light, dark, n_colors)\n    pal = cmap(x)[:, :3].tolist()\n    if reverse:\n        pal = pal[::-1]\n\n    if as_cmap:\n        x_256 = np.linspace(light, dark, 256)\n        if reverse:\n            x_256 = x_256[::-1]\n        pal_256 = cmap(x_256)\n        cmap = mpl.colors.ListedColormap(pal_256)\n        return cmap\n    else:\n        return pal"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds command - line arguments to be passed to ffmpeg.", "response": "def get_args(stream_spec, overwrite_output=False):\n    \"\"\"Build command-line arguments to be passed to ffmpeg.\"\"\"\n    nodes = get_stream_spec_nodes(stream_spec)\n    args = []\n    # TODO: group nodes together, e.g. `-i somefile -r somerate`.\n    sorted_nodes, outgoing_edge_maps = topo_sort(nodes)\n    input_nodes = [node for node in sorted_nodes if isinstance(node, InputNode)]\n    output_nodes = [node for node in sorted_nodes if isinstance(node, OutputNode)]\n    global_nodes = [node for node in sorted_nodes if isinstance(node, GlobalNode)]\n    filter_nodes = [node for node in sorted_nodes if isinstance(node, FilterNode)]\n    stream_name_map = {(node, None): str(i) for i, node in enumerate(input_nodes)}\n    filter_arg = _get_filter_arg(filter_nodes, outgoing_edge_maps, stream_name_map)\n    args += reduce(operator.add, [_get_input_args(node) for node in input_nodes])\n    if filter_arg:\n        args += ['-filter_complex', filter_arg]\n    args += reduce(operator.add, [_get_output_args(node, stream_name_map) for node in output_nodes])\n    args += reduce(operator.add, [_get_global_args(node) for node in global_nodes], [])\n    if overwrite_output:\n        args += ['-y']\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds the command - line for invoking ffmpeg.", "response": "def compile(stream_spec, cmd='ffmpeg', overwrite_output=False):\n    \"\"\"Build command-line for invoking ffmpeg.\n\n    The :meth:`run` function uses this to build the commnad line\n    arguments and should work in most cases, but calling this function\n    directly is useful for debugging or if you need to invoke ffmpeg\n    manually for whatever reason.\n\n    This is the same as calling :meth:`get_args` except that it also\n    includes the ``ffmpeg`` command as the first argument.\n    \"\"\"\n    if isinstance(cmd, basestring):\n        cmd = [cmd]\n    elif type(cmd) != list:\n        cmd = list(cmd)\n    return cmd + get_args(stream_spec, overwrite_output=overwrite_output)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninvoke ffmpeg for the supplied node graph.", "response": "def run(\n        stream_spec, cmd='ffmpeg', capture_stdout=False, capture_stderr=False, input=None,\n        quiet=False, overwrite_output=False):\n    \"\"\"Invoke ffmpeg for the supplied node graph.\n\n    Args:\n        capture_stdout: if True, capture stdout (to be used with\n            ``pipe:`` ffmpeg outputs).\n        capture_stderr: if True, capture stderr.\n        quiet: shorthand for setting ``capture_stdout`` and ``capture_stderr``.\n        input: text to be sent to stdin (to be used with ``pipe:``\n            ffmpeg inputs)\n        **kwargs: keyword-arguments passed to ``get_args()`` (e.g.\n            ``overwrite_output=True``).\n\n    Returns: (out, err) tuple containing captured stdout and stderr data.\n    \"\"\"\n    process = run_async(\n        stream_spec,\n        cmd,\n        pipe_stdin=input is not None,\n        pipe_stdout=capture_stdout,\n        pipe_stderr=capture_stderr,\n        quiet=quiet,\n        overwrite_output=overwrite_output,\n    )\n    out, err = process.communicate(input)\n    retcode = process.poll()\n    if retcode:\n        raise Error('ffmpeg', out, err)\n    return out, err"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef input(filename, **kwargs):\n    kwargs['filename'] = filename\n    fmt = kwargs.pop('f', None)\n    if fmt:\n        if 'format' in kwargs:\n            raise ValueError(\"Can't specify both `format` and `f` kwargs\")\n        kwargs['format'] = fmt\n    return InputNode(input.__name__, kwargs=kwargs).stream()", "response": "Input file URL (ffmpeg ``-i`` option)\n\n    Any supplied kwargs are passed to ffmpeg verbatim (e.g. ``t=20``,\n    ``f='mp4'``, ``acodec='pcm'``, etc.).\n\n    To tell ffmpeg to read from stdin, use ``pipe:`` as the filename.\n\n    Official documentation: `Main options <https://ffmpeg.org/ffmpeg.html#Main-options>`__"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef output(*streams_and_filename, **kwargs):\n    streams_and_filename = list(streams_and_filename)\n    if 'filename' not in kwargs:\n        if not isinstance(streams_and_filename[-1], basestring):\n            raise ValueError('A filename must be provided')\n        kwargs['filename'] = streams_and_filename.pop(-1)\n    streams = streams_and_filename\n\n    fmt = kwargs.pop('f', None)\n    if fmt:\n        if 'format' in kwargs:\n            raise ValueError(\"Can't specify both `format` and `f` kwargs\")\n        kwargs['format'] = fmt\n    return OutputNode(streams, output.__name__, kwargs=kwargs).stream()", "response": "Output a list of streams and filenames to a file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfunction to run in a separate greenlet to read progress events from a unix - domain socket.", "response": "def _do_watch_progress(filename, sock, handler):\n    \"\"\"Function to run in a separate gevent greenlet to read progress\n    events from a unix-domain socket.\"\"\"\n    connection, client_address = sock.accept()\n    data = b''\n    try:\n        while True:\n            more_data = connection.recv(16)\n            if not more_data:\n                break\n            data += more_data\n            lines = data.split(b'\\n')\n            for line in lines[:-1]:\n                line = line.decode()\n                parts = line.split('=')\n                key = parts[0] if len(parts) > 0 else None\n                value = parts[1] if len(parts) > 1 else None\n                handler(key, value)\n            data = lines[-1]\n    finally:\n        connection.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef show_progress(total_duration):\n    with tqdm(total=round(total_duration, 2)) as bar:\n        def handler(key, value):\n            if key == 'out_time_ms':\n                time = round(float(value) / 1000000., 2)\n                bar.update(time - bar.n)\n            elif key == 'progress' and value == 'end':\n                bar.update(bar.total - bar.n)\n        with _watch_progress(handler) as socket_filename:\n            yield socket_filename", "response": "Create a unix - domain socket to watch progress and render tqdm\n    progress bar."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns ffprobe on the specified file and return a JSON representation of the output.", "response": "def probe(filename, cmd='ffprobe', **kwargs):\n    \"\"\"Run ffprobe on the specified file and return a JSON representation of the output.\n\n    Raises:\n        :class:`ffmpeg.Error`: if ffprobe returns a non-zero exit code,\n            an :class:`Error` is returned with a generic error message.\n            The stderr output can be retrieved by accessing the\n            ``stderr`` property of the exception.\n    \"\"\"\n    args = [cmd, '-show_format', '-show_streams', '-of', 'json']\n    args += convert_kwargs_to_cmd_line_args(kwargs)\n    args += [filename]\n\n    p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    out, err = p.communicate()\n    if p.returncode != 0:\n        raise Error('ffprobe', out, err)\n    return json.loads(out.decode('utf-8'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter_multi_output(stream_spec, filter_name, *args, **kwargs):\n    return FilterNode(stream_spec, filter_name, args=args, kwargs=kwargs, max_inputs=None)", "response": "Apply custom filter with one or more outputs."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\napplies custom filter. ``filter_`` is normally used by higher-level filter functions such as ``hflip``, but if a filter implementation is missing from ``fmpeg-python``, you can call ``filter_`` directly to have ``fmpeg-python`` pass the filter name and arguments to ffmpeg verbatim. Args: stream_spec: a Stream, list of Streams, or label-to-Stream dictionary mapping filter_name: ffmpeg filter name, e.g. `colorchannelmixer` *args: list of args to pass to ffmpeg verbatim **kwargs: list of keyword-args to pass to ffmpeg verbatim The function name is suffixed with ``_`` in order avoid confusion with the standard python ``filter`` function. Example: ``ffmpeg.input('in.mp4').filter('hflip').output('out.mp4').run()``", "response": "def filter(stream_spec, filter_name, *args, **kwargs):\n    \"\"\"Apply custom filter.\n\n    ``filter_`` is normally used by higher-level filter functions such as ``hflip``, but if a filter implementation\n    is missing from ``fmpeg-python``, you can call ``filter_`` directly to have ``fmpeg-python`` pass the filter name\n    and arguments to ffmpeg verbatim.\n\n    Args:\n        stream_spec: a Stream, list of Streams, or label-to-Stream dictionary mapping\n        filter_name: ffmpeg filter name, e.g. `colorchannelmixer`\n        *args: list of args to pass to ffmpeg verbatim\n        **kwargs: list of keyword-args to pass to ffmpeg verbatim\n\n    The function name is suffixed with ``_`` in order avoid confusion with the standard python ``filter`` function.\n\n    Example:\n\n        ``ffmpeg.input('in.mp4').filter('hflip').output('out.mp4').run()``\n    \"\"\"\n    return filter_multi_output(stream_spec, filter_name, *args, **kwargs).stream()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfilter a stream spec using a filter function.", "response": "def filter_(stream_spec, filter_name, *args, **kwargs):\n    \"\"\"Alternate name for ``filter``, so as to not collide with the\n    built-in python ``filter`` operator.\n    \"\"\"\n    return filter(stream_spec, filter_name, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setpts(stream, expr):\n    return FilterNode(stream, setpts.__name__, args=[expr]).stream()", "response": "Change the PTS of the input frames."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntrimming the input so that the output contains one continuous subpart of the input.", "response": "def trim(stream, **kwargs):\n    \"\"\"Trim the input so that the output contains one continuous subpart of the input.\n\n    Args:\n        start: Specify the time of the start of the kept section, i.e. the frame with the timestamp start will be the\n            first frame in the output.\n        end: Specify the time of the first frame that will be dropped, i.e. the frame immediately preceding the one\n            with the timestamp end will be the last frame in the output.\n        start_pts: This is the same as start, except this option sets the start timestamp in timebase units instead of\n            seconds.\n        end_pts: This is the same as end, except this option sets the end timestamp in timebase units instead of\n            seconds.\n        duration: The maximum duration of the output in seconds.\n        start_frame: The number of the first frame that should be passed to the output.\n        end_frame: The number of the first frame that should be dropped.\n\n    Official documentation: `trim <https://ffmpeg.org/ffmpeg-filters.html#trim>`__\n    \"\"\"\n    return FilterNode(stream, trim.__name__, kwargs=kwargs).stream()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef overlay(main_parent_node, overlay_parent_node, eof_action='repeat', **kwargs):\n    kwargs['eof_action'] = eof_action\n    return FilterNode([main_parent_node, overlay_parent_node], overlay.__name__, kwargs=kwargs, max_inputs=2).stream()", "response": "Overlay one video on top of another."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncrop the input video.", "response": "def crop(stream, x, y, width, height, **kwargs):\n    \"\"\"Crop the input video.\n\n    Args:\n        x: The horizontal position, in the input video, of the left edge of\n           the output video.\n        y: The vertical position, in the input video, of the top edge of the\n           output video.\n        width: The width of the output video. Must be greater than 0.\n        heigth: The height of the output video. Must be greater than 0.\n\n    Official documentation: `crop <https://ffmpeg.org/ffmpeg-filters.html#crop>`__\n    \"\"\"\n    return FilterNode(\n        stream,\n        crop.__name__,\n        args=[width, height, x, y],\n        kwargs=kwargs\n    ).stream()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndrawing a colored box on the input image.", "response": "def drawbox(stream, x, y, width, height, color, thickness=None, **kwargs):\n    \"\"\"Draw a colored box on the input image.\n\n    Args:\n        x: The expression which specifies the top left corner x coordinate of the box. It defaults to 0.\n        y: The expression which specifies the top left corner y coordinate of the box. It defaults to 0.\n        width: Specify the width of the box; if 0 interpreted as the input width. It defaults to 0.\n        heigth: Specify the height of the box; if 0 interpreted as the input height. It defaults to 0.\n        color: Specify the color of the box to write. For the general syntax of this option, check the \"Color\" section\n            in the ffmpeg-utils manual. If the special value invert is used, the box edge color is the same as the\n            video with inverted luma.\n        thickness: The expression which sets the thickness of the box edge. Default value is 3.\n        w: Alias for ``width``.\n        h: Alias for ``height``.\n        c: Alias for ``color``.\n        t: Alias for ``thickness``.\n\n    Official documentation: `drawbox <https://ffmpeg.org/ffmpeg-filters.html#drawbox>`__\n    \"\"\"\n    if thickness:\n        kwargs['t'] = thickness\n    return FilterNode(stream, drawbox.__name__, args=[x, y, width, height, color], kwargs=kwargs).stream()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef drawtext(stream, text=None, x=0, y=0, escape_text=True, **kwargs):\n    if text is not None:\n        if escape_text:\n            text = escape_chars(text, '\\\\\\'%')\n        kwargs['text'] = text\n    if x != 0:\n        kwargs['x'] = x\n    if y != 0:\n        kwargs['y'] = y\n    return filter(stream, drawtext.__name__, **kwargs)", "response": "Draw a text string or text from a specified file on top of a video."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconcatenating audio and video streams together one after the other.", "response": "def concat(*streams, **kwargs):\n    \"\"\"Concatenate audio and video streams, joining them together one after the other.\n\n    The filter works on segments of synchronized video and audio streams. All segments must have the same number of\n    streams of each type, and that will also be the number of streams at output.\n\n    Args:\n        unsafe: Activate unsafe mode: do not fail if segments have a different format.\n\n    Related streams do not always have exactly the same duration, for various reasons including codec frame size or\n    sloppy authoring. For that reason, related synchronized streams (e.g. a video and its audio track) should be\n    concatenated at once. The concat filter will use the duration of the longest stream in each segment (except the\n    last one), and if necessary pad shorter audio streams with silence.\n\n    For this filter to work correctly, all segments must start at timestamp 0.\n\n    All corresponding streams must have the same parameters in all segments; the filtering system will automatically\n    select a common pixel format for video streams, and a common sample format, sample rate and channel layout for\n    audio streams, but other settings, such as resolution, must be converted explicitly by the user.\n\n    Different frame rates are acceptable but will result in variable frame rate at output; be sure to configure the\n    output file to handle it.\n\n    Official documentation: `concat <https://ffmpeg.org/ffmpeg-filters.html#concat>`__\n    \"\"\"\n    video_stream_count = kwargs.get('v', 1)\n    audio_stream_count = kwargs.get('a', 0)\n    stream_count = video_stream_count + audio_stream_count\n    if len(streams) % stream_count != 0:\n        raise ValueError(\n            'Expected concat input streams to have length multiple of {} (v={}, a={}); got {}'\n                .format(stream_count, video_stream_count, audio_stream_count, len(streams)))\n    kwargs['n'] = int(len(streams) / stream_count)\n    return FilterNode(streams, concat.__name__, kwargs=kwargs, max_inputs=None).stream()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\napplies Zoom & Pan effect.", "response": "def zoompan(stream, **kwargs):\n    \"\"\"Apply Zoom & Pan effect.\n\n    Args:\n        zoom: Set the zoom expression. Default is 1.\n        x: Set the x expression. Default is 0.\n        y: Set the y expression. Default is 0.\n        d: Set the duration expression in number of frames. This sets for how many number of frames effect will last\n            for single input image.\n        s: Set the output image size, default is ``hd720``.\n        fps: Set the output frame rate, default is 25.\n        z: Alias for ``zoom``.\n\n    Official documentation: `zoompan <https://ffmpeg.org/ffmpeg-filters.html#zoompan>`__\n    \"\"\"\n    return FilterNode(stream, zoompan.__name__, kwargs=kwargs).stream()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmodify the hue and / or saturation of the input.", "response": "def hue(stream, **kwargs):\n    \"\"\"Modify the hue and/or the saturation of the input.\n\n    Args:\n        h: Specify the hue angle as a number of degrees. It accepts an expression, and defaults to \"0\".\n        s: Specify the saturation in the [-10,10] range. It accepts an expression and defaults to \"1\".\n        H: Specify the hue angle as a number of radians. It accepts an expression, and defaults to \"0\".\n        b: Specify the brightness in the [-10,10] range. It accepts an expression and defaults to \"0\".\n\n    Official documentation: `hue <https://ffmpeg.org/ffmpeg-filters.html#hue>`__\n    \"\"\"\n    return FilterNode(stream, hue.__name__, kwargs=kwargs).stream()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef colorchannelmixer(stream, *args, **kwargs):\n    return FilterNode(stream, colorchannelmixer.__name__, kwargs=kwargs).stream()", "response": "Adjust video input frames by re - mixing color channels."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _recursive_repr(item):\n    if isinstance(item, basestring):\n        result = str(item)\n    elif isinstance(item, list):\n        result = '[{}]'.format(', '.join([_recursive_repr(x) for x in item]))\n    elif isinstance(item, dict):\n        kv_pairs = ['{}: {}'.format(_recursive_repr(k), _recursive_repr(item[k])) for k in sorted(item)]\n        result = '{' + ', '.join(kv_pairs) + '}'\n    else:\n        result = repr(item)\n    return result", "response": "Recursively represent a dict or list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef escape_chars(text, chars):\n    text = str(text)\n    chars = list(set(chars))\n    if '\\\\' in chars:\n        chars.remove('\\\\')\n        chars.insert(0, '\\\\')\n    for ch in chars:\n        text = text.replace(ch, '\\\\' + ch)\n    return text", "response": "Helper function to escape uncomfortable characters."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an outgoing stream originating from this node.", "response": "def stream(self, label=None, selector=None):\n        \"\"\"Create an outgoing stream originating from this node.\n\n        More nodes may be attached onto the outgoing stream.\n        \"\"\"\n        return self.__outgoing_stream_type(self, label, upstream_selector=selector)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _base_resize(img, size):\n        '''Helper function that uses TF to resize an image'''\n        img = tf.expand_dims(img, 0)\n        return tf.image.resize_bilinear(img, size)[0,:,:,:]", "response": "Helper function that uses TF to resize an image"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _calc_grad_tiled(self, img, t_grad, tile_size=512):\n        '''Compute the value of tensor t_grad over the image in a tiled way.\n        Random shifts are applied to the image to blur tile boundaries over \n        multiple iterations.'''\n        sz = tile_size\n        h, w = img.shape[:2]\n        sx, sy = np.random.randint(sz, size=2)\n        img_shift = np.roll(np.roll(img, sx, 1), sy, 0)\n        grad = np.zeros_like(img)\n        for y in range(0, max(h-sz//2, sz),sz):\n            for x in range(0, max(w-sz//2, sz),sz):\n                sub = img_shift[y:y+sz,x:x+sz]\n                g = self._session.run(t_grad, {self._t_input:sub})\n                grad[y:y+sz,x:x+sz] = g\n        return np.roll(np.roll(grad, -sx, 1), -sy, 0)", "response": "Compute the value of tensor t_grad over the image in a tiled way."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(SockstatCollector, self).get_default_config()\n        config.update({\n            'path':     'sockets',\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_default_config(self):\n        config = super(XENCollector, self).get_default_config()\n        config.update({\n            'path':     'xen'\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncollects libvirt data and publish it to the log", "response": "def collect(self):\n        \"\"\"\n        Collect libvirt data\n        \"\"\"\n        if libvirt is None:\n            self.log.error('Unable to import either libvirt')\n            return {}\n        # Open a restricted (non-root) connection to the hypervisor\n        conn = libvirt.openReadOnly(None)\n        # Get hardware info\n        conninfo = conn.getInfo()\n        # Initialize variables\n        memallocated = 0\n        coresallocated = 0\n        totalcores = 0\n        results = {}\n        domIds = conn.listDomainsID()\n        if 0 in domIds:\n            # Total cores\n            domU = conn.lookupByID(0)\n            totalcores = domU.info()[3]\n        # Free Space\n        s = os.statvfs('/')\n        freeSpace = (s.f_bavail * s.f_frsize) / 1024\n        # Calculate allocated memory and cores\n        for i in domIds:\n            # Ignore 0\n            if i == 0:\n                continue\n            domU = conn.lookupByID(i)\n            dominfo = domU.info()\n            memallocated += dominfo[2]\n            if i > 0:\n                coresallocated += dominfo[3]\n        results = {\n            'InstalledMem': conninfo[1],\n            'MemAllocated': memallocated / 1024,\n            'MemFree': conninfo[1] - (memallocated / 1024),\n            'AllocatedCores': coresallocated,\n            'DiskFree': freeSpace,\n            'TotalCores': totalcores,\n            'FreeCores': (totalcores - coresallocated)\n        }\n        for k in results.keys():\n            self.publish(k, results[k], 0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_default_config(self):\n        config = super(OssecCollector, self).get_default_config()\n        config.update({\n            'bin':              '/var/ossec/bin/agent_control',\n            'use_sudo':         True,\n            'sudo_cmd':         '/usr/bin/sudo',\n            'path':             'ossec',\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_default_config(self):\n        config = super(VMSDomsCollector, self).get_default_config()\n        config.update({\n            'path':     'vms'\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a UTC datetime to the local timezone", "response": "def utc_to_local(utc_dt):\n    \"\"\"\n    :param utc_dt: datetime in UTC\n    :return: datetime in the local timezone\n    \"\"\"\n    # get integer timestamp to avoid precision lost\n    timestamp = calendar.timegm(utc_dt.timetuple())\n    local_dt = datetime.datetime.fromtimestamp(timestamp)\n    assert utc_dt.resolution >= datetime.timedelta(microseconds=1)\n    return local_dt.replace(microsecond=utc_dt.microsecond)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the availability zones for a given region", "response": "def get_zones(region, auth_kwargs):\n    \"\"\"\n    :param auth_kwargs:\n    :param region: region to get the availability zones for\n    :return: list of availability zones\n    \"\"\"\n    ec2_conn = boto.ec2.connect_to_region(region, **auth_kwargs)\n    return [zone.name for zone in ec2_conn.get_all_zones()]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(ElbCollector, self).get_default_config()\n        config.update({\n            'path': 'elb',\n            'regions': ['us-west-1'],\n            'interval': 60,\n            'format': '$zone.$elb_name.$metric_name',\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npublishes a metric from the given timestamp.", "response": "def publish_delayed_metric(self, name, value, timestamp, raw_value=None,\n                               precision=0, metric_type='GAUGE',\n                               instance=None):\n        \"\"\"\n        Metrics may not be immediately available when querying cloudwatch.\n        Hence, allow the ability to publish a metric from some the past given\n        its timestamp.\n        \"\"\"\n        # Get metric Path\n        path = self.get_metric_path(name, instance)\n\n        # Get metric TTL\n        ttl = float(self.config['interval']) * float(\n            self.config['ttl_multiplier'])\n\n        # Create Metric\n        metric = Metric(path, value, raw_value=raw_value, timestamp=timestamp,\n                        precision=precision, host=self.get_hostname(),\n                        metric_type=metric_type, ttl=ttl)\n\n        # Publish Metric\n        self.publish_metric(metric)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_elb_names(self, region, config):\n        # This function is ripe to be memoized but when ELBs are added/removed\n        # dynamically over time, diamond will have to be restarted to pick\n        # up the changes.\n        region_dict = config.get('regions', {}).get(region, {})\n        if 'elb_names' not in region_dict:\n            elb_conn = boto.ec2.elb.connect_to_region(region,\n                                                      **self.auth_kwargs)\n            full_elb_names = \\\n                [elb.name for elb in elb_conn.get_all_load_balancers()]\n\n            # Regular expressions for ELBs we DO NOT want to get metrics on.\n            matchers = \\\n                [re.compile(regex) for regex in config.get('elbs_ignored', [])]\n\n            # cycle through elbs get the list of elbs that don't match\n            elb_names = []\n            for elb_name in full_elb_names:\n                if matchers and any([m.match(elb_name) for m in matchers]):\n                    continue\n                elb_names.append(elb_name)\n        else:\n            elb_names = region_dict['elb_names']\n        return elb_names", "response": "Get the list of ELB names that are not ignored in the given region."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncollects disk busyness per aggregate object.", "response": "def agr_busy(self):\n        \"\"\" Collector for average disk busyness per aggregate\n\n            As of Nov 22nd 2013 there is no API call for agr busyness.\n            You have to collect all disk busyness and then compute agr\n            busyness. #fml\n\n        \"\"\"\n\n        c1 = {}  # Counters from time a\n        c2 = {}  # Counters from time b\n        disk_results = {}  # Disk busyness results %\n        agr_results = {}  # Aggregate busyness results $\n        names = ['disk_busy', 'base_for_disk_busy', 'raid_name',\n                 'base_for_disk_busy', 'instance_uuid']\n        netapp_api = NaElement('perf-object-get-instances')\n        netapp_api.child_add_string('objectname', 'disk')\n        disk_1 = self.get_netapp_elem(netapp_api, 'instances')\n        time.sleep(1)\n        disk_2 = self.get_netapp_elem(netapp_api, 'instances')\n\n        for instance_data in disk_1:\n            temp = {}\n            for element in instance_data.findall(\".//counters/counter-data\"):\n                if element.find('name').text in names:\n                    temp[element.find('name').text] = element.find(\n                        'value').text\n\n            agr_name = temp['raid_name']\n            agr_name = agr_name[agr_name.find('/', 0):agr_name.find('/', 1)]\n            temp['raid_name'] = agr_name.lstrip('/')\n            c1[temp.pop('instance_uuid')] = temp\n\n        for instance_data in disk_2:\n            temp = {}\n            for element in instance_data.findall(\".//counters/counter-data\"):\n                if element.find('name').text in names:\n                    temp[element.find('name').text] = element.find(\n                        'value').text\n\n            agr_name = temp['raid_name']\n            agr_name = agr_name[agr_name.find('/', 0):agr_name.find('/', 1)]\n            temp['raid_name'] = agr_name.lstrip('/')\n            c2[temp.pop('instance_uuid')] = temp\n\n        for item in c1:\n            t_c1 = int(c1[item]['disk_busy'])  # time_counter_1\n            t_b1 = int(c1[item]['base_for_disk_busy'])  # time_base_1\n            t_c2 = int(c2[item]['disk_busy'])\n            t_b2 = int(c2[item]['base_for_disk_busy'])\n\n            disk_busy = 100 * (t_c2 - t_c1) / (t_b2 - t_b1)\n\n            if c1[item]['raid_name'] in disk_results:\n                disk_results[c1[item]['raid_name']].append(disk_busy)\n            else:\n                disk_results[c1[item]['raid_name']] = [disk_busy]\n\n        for aggregate in disk_results:\n            agr_results[aggregate] = \\\n                sum(disk_results[aggregate]) / len(disk_results[aggregate])\n\n        for aggregate in agr_results:\n            self.push('avg_busy', 'aggregate.' + aggregate,\n                      agr_results[aggregate])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef consistency_point(self):\n\n        cp_delta = {}\n        xml_path = 'instances/instance-data/counters'\n        netapp_api = NaElement('perf-object-get-instances')\n        netapp_api.child_add_string('objectname', 'wafl')\n        instance = NaElement('instances')\n        instance.child_add_string('instance', 'wafl')\n        counter = NaElement('counters')\n        counter.child_add_string('counter', 'cp_count')\n        netapp_api.child_add(counter)\n        netapp_api.child_add(instance)\n\n        cp_1 = self.get_netapp_elem(netapp_api, xml_path)\n        time.sleep(3)\n        cp_2 = self.get_netapp_elem(netapp_api, xml_path)\n\n        for element in cp_1:\n            if element.find('name').text == 'cp_count':\n                cp_1 = element.find('value').text.rsplit(',')\n                break\n        for element in cp_2:\n            if element.find('name').text == 'cp_count':\n                cp_2 = element.find('value').text.rsplit(',')\n                break\n\n        if not type(cp_2) is list or not type(cp_1) is list:\n            log.error(\"consistency point data not available for filer: %s\"\n                      % self.device)\n            return\n\n        cp_1 = {\n            'wafl_timer': cp_1[0],\n            'snapshot': cp_1[1],\n            'wafl_avail_bufs': cp_1[2],\n            'dirty_blk_cnt': cp_1[3],\n            'full_nv_log': cp_1[4],\n            'b2b': cp_1[5],\n            'flush_gen': cp_1[6],\n            'sync_gen': cp_1[7],\n            'def_b2b': cp_1[8],\n            'con_ind_pin': cp_1[9],\n            'low_mbuf_gen': cp_1[10],\n            'low_datavec_gen': cp_1[11]\n        }\n\n        cp_2 = {\n            'wafl_timer': cp_2[0],\n            'snapshot': cp_2[1],\n            'wafl_avail_bufs': cp_2[2],\n            'dirty_blk_cnt': cp_2[3],\n            'full_nv_log': cp_2[4],\n            'b2b': cp_2[5],\n            'flush_gen': cp_2[6],\n            'sync_gen': cp_2[7],\n            'def_b2b': cp_2[8],\n            'con_ind_pin': cp_2[9],\n            'low_mbuf_gen': cp_2[10],\n            'low_datavec_gen': cp_2[11]\n        }\n\n        for item in cp_1:\n            c1 = int(cp_1[item])\n            c2 = int(cp_2[item])\n            cp_delta[item] = c2 - c1\n\n        for item in cp_delta:\n            self.push(item + '_CP', 'system.system', cp_delta[item])", "response": "Get the consistency point information for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef maintenance_center(self, storage_disk_xml=None):\n\n        disk_in_maintenance = 0\n\n        for filer_disk in storage_disk_xml:\n            disk_status = filer_disk.find('disk-raid-info/container-type')\n            if disk_status.text == 'maintenance':\n                disk_in_maintenance += 1\n\n        self.push('maintenance_disk', 'disk', disk_in_maintenance)", "response": "Collect how many disk in maintenance center"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef zero_disk(self, disk_xml=None):\n\n        troubled_disks = 0\n        for filer_disk in disk_xml:\n            raid_state = filer_disk.find('raid-state').text\n            if not raid_state == 'spare':\n                continue\n            is_zeroed = filer_disk.find('is-zeroed').text\n\n            if is_zeroed == 'false':\n                troubled_disks += 1\n        self.push('not_zeroed', 'disk', troubled_disks)", "response": "Collect and publish not zeroed disk metrics\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd the spare disk to the list of unique spare disk types.", "response": "def spare_disk(self, disk_xml=None):\n        \"\"\" Number of spare disk per type.\n\n            For example: storage.ontap.filer201.disk.SATA\n\n        \"\"\"\n\n        spare_disk = {}\n        disk_types = set()\n\n        for filer_disk in disk_xml:\n            disk_types.add(filer_disk.find('effective-disk-type').text)\n            if not filer_disk.find('raid-state').text == 'spare':\n                continue\n\n            disk_type = filer_disk.find('effective-disk-type').text\n            if disk_type in spare_disk:\n                spare_disk[disk_type] += 1\n            else:\n                spare_disk[disk_type] = 1\n\n        for disk_type in disk_types:\n            if disk_type in spare_disk:\n                self.push('spare_' + disk_type, 'disk', spare_disk[disk_type])\n            else:\n                self.push('spare_' + disk_type, 'disk', 0)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve netapp elem from netapp API", "response": "def get_netapp_elem(self, netapp_api=None, sub_element=None):\n        \"\"\" Retrieve netapp elem\n        \"\"\"\n\n        netapp_data = self.server.invoke_elem(netapp_api)\n\n        if netapp_data.results_status() == 'failed':\n            self.log.error(\n                'While using netapp API failed to retrieve '\n                'disk-list-info for netapp filer %s' % self.device)\n            print(netapp_data.sprintf())\n            return\n        netapp_xml = \\\n            ET.fromstring(netapp_data.sprintf()).find(sub_element)\n\n        return netapp_xml"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _netapp_login(self):\n\n        self.server = NaServer(self.ip, 1, 3)\n        self.server.set_transport_type('HTTPS')\n        self.server.set_style('LOGIN')\n        self.server.set_admin_user(self.netapp_user, self.netapp_password)", "response": "Login to our netapp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef collect(self, device, ip, user, password):\n\n        if netappsdk:\n            self.log.error(\n                'Failed to import netappsdk.NaServer or netappsdk.NaElement')\n            return\n\n        if device in self.running:\n            return\n\n        self.running.add(device)\n        parent = (self.config['path_prefix'], self.publish_metric, self.log)\n\n        netappDiskCol(device, ip, user, password, parent)\n        self.running.remove(device)", "response": "Collect our metrics for our netapp filer"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_default_config(self):\n        config = super(OpenLDAPCollector, self).get_default_config()\n        config.update({\n            'path': 'openldap',\n            'host': 'localhost',\n            'port': 389,\n            'username': 'cn=monitor',\n            'password': 'password',\n        })\n        return config", "response": "Returns default config dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(VMSFSCollector, self).get_default_config()\n        config.update({\n            'path':     'vmsfs'\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_default_config(self):\n        config = super(TCPCollector, self).get_default_config()\n        config.update({\n            'path': 'tcp',\n            'allowed_names':\n                'ListenOverflows, ListenDrops, TCPLoss, ' +\n                'TCPTimeouts, TCPFastRetrans, TCPLostRetransmit, ' +\n                'TCPForwardRetrans, TCPSlowStartRetrans, CurrEstab, ' +\n                'TCPAbortOnMemory, TCPBacklogDrop, AttemptFails, ' +\n                'EstabResets, InErrs, ActiveOpens, PassiveOpens',\n            'gauges': 'CurrEstab, MaxConn',\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_default_config(self):\n        config = super(PostfixCollector, self).get_default_config()\n        config.update({\n            'path':             'postfix',\n            'host':             'localhost',\n            'port':             7777,\n            'include_clients':  True,\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate an iterator of pairs where the first value is the joined key names and the second value is the value of the lowest level key.", "response": "def flatten_dictionary(input, sep='.', prefix=None):\n    \"\"\"Produces iterator of pairs where the first value is\n    the joined key names and the second value is the value\n    associated with the lowest level key. For example::\n\n      {'a': {'b': 10},\n       'c': 20,\n       }\n\n    produces::\n\n      [('a.b', 10), ('c', 20)]\n    \"\"\"\n    for name, value in sorted(input.items()):\n        fullname = sep.join(filter(None, [prefix, name]))\n        if isinstance(value, dict):\n            for result in flatten_dictionary(value, sep, fullname):\n                yield result\n        else:\n            yield (fullname, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_default_config(self):\n        config = super(CephCollector, self).get_default_config()\n        config.update({\n            'socket_path': '/var/run/ceph',\n            'socket_prefix': 'ceph-',\n            'socket_ext': 'asok',\n            'ceph_binary': '/usr/bin/ceph',\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_socket_paths(self):\n        socket_pattern = os.path.join(self.config['socket_path'],\n                                      (self.config['socket_prefix'] +\n                                       '*.' + self.config['socket_ext']))\n        return glob.glob(socket_pattern)", "response": "Return a sequence of paths to sockets for communicating\n        with ceph daemons."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_counter_prefix_from_socket_name(self, name):\n        base = os.path.splitext(os.path.basename(name))[0]\n        if base.startswith(self.config['socket_prefix']):\n            base = base[len(self.config['socket_prefix']):]\n        return 'ceph.' + base", "response": "Given the name of a UDS socket return the prefix that is used to generate counters coming from that source."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the parsed JSON data returned when ceph is told to dump the stats from the named socket.", "response": "def _get_stats_from_socket(self, name):\n        \"\"\"Return the parsed JSON data returned when ceph is told to\n        dump the stats from the named socket.\n\n        In the event of an error error, the exception is logged, and\n        an empty result set is returned.\n        \"\"\"\n        try:\n            json_blob = subprocess.check_output(\n                [self.config['ceph_binary'],\n                 '--admin-daemon',\n                 name,\n                 'perf',\n                 'dump',\n                 ])\n        except subprocess.CalledProcessError as err:\n            self.log.info('Could not get stats from %s: %s',\n                          name, err)\n            self.log.exception('Could not get stats from %s' % name)\n            return {}\n\n        try:\n            json_data = json.loads(json_blob)\n        except Exception as err:\n            self.log.info('Could not parse stats from %s: %s',\n                          name, err)\n            self.log.exception('Could not parse stats from %s' % name)\n            return {}\n\n        return json_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _publish_stats(self, counter_prefix, stats):\n        for stat_name, stat_value in flatten_dictionary(\n            stats,\n            prefix=counter_prefix,\n        ):\n            self.publish_gauge(stat_name, stat_value)", "response": "Given a stats dictionary from _get_stats_from_socket publish the individual values."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncollects stats from all sockets and publish them to the server.", "response": "def collect(self):\n        \"\"\"\n        Collect stats\n        \"\"\"\n        for path in self._get_socket_paths():\n            self.log.debug('checking %s', path)\n            counter_prefix = self._get_counter_prefix_from_socket_name(path)\n            stats = self._get_stats_from_socket(path)\n            self._publish_stats(counter_prefix, stats)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_default_config(self):\n        config = super(DseOpsCenterCollector, self).get_default_config()\n        metrics = [\n            'cf-bf-false-positives',\n            'cf-bf-false-ratio',\n            'cf-bf-space-used',\n            'cf-keycache-hit-rate',\n            'cf-keycache-hits',\n            'cf-keycache-requests',\n            'cf-live-disk-used',\n            'cf-live-sstables',\n            'cf-pending-tasks',\n            'cf-read-latency-op',\n            'cf-read-ops',\n            'cf-rowcache-hit-rate',\n            'cf-rowcache-hits',\n            'cf-rowcache-requests',\n            'cf-total-disk-used',\n            'cf-write-latency-op',\n            'cf-write-ops',\n            'cms-collection-count',\n            'cms-collection-time',\n            'data-load',\n            'heap-committed',\n            'heap-max',\n            'heap-used',\n            'key-cache-hit-rate',\n            'key-cache-hits',\n            'key-cache-requests',\n            'nonheap-committed',\n            'nonheap-max',\n            'nonheap-used',\n            'pending-compaction-tasks',\n            'pending-flush-sorter-tasks',\n            'pending-flushes',\n            'pending-gossip-tasks',\n            'pending-hinted-handoff',\n            'pending-internal-responses',\n            'pending-memtable-post-flushers',\n            'pending-migrations',\n            'pending-misc-tasks',\n            'pending-read-ops',\n            'pending-read-repair-tasks',\n            'pending-repair-tasks',\n            'pending-repl-on-write-tasks',\n            'pending-request-responses',\n            'pending-streams',\n            'pending-write-ops',\n            'read-latency-op',\n            'read-ops',\n            'row-cache-hit-rate',\n            'row-cache-hits',\n            'row-cache-requests',\n            'solr-avg-time-per-req',\n            'solr-errors',\n            'solr-requests',\n            'solr-timeouts',\n            'total-bytes-compacted',\n            'total-compactions-completed',\n            'write-latency-op',\n            'write-ops',\n        ]\n        config.update({\n            'host':              '127.0.0.1',\n            'port':              8888,\n            'path':              'cassandra',\n            'node_group':        '*',\n            'metrics':           ','.join(metrics),\n            'default_tail_opts': '&forecast=0&node_aggregation=1',\n\n        })\n        return config", "response": "Returns the default collector settings for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the help text for the configuration options for this handler", "response": "def get_default_config_help(self):\n        \"\"\"\n        Returns the help text for the configuration options for this handler\n        \"\"\"\n        config = super(GraphitePickleHandler, self).get_default_config_help()\n\n        config.update({\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the default config for the handler", "response": "def get_default_config(self):\n        \"\"\"\n        Return the default config for the handler\n        \"\"\"\n        config = super(GraphitePickleHandler, self).get_default_config()\n\n        config.update({\n            'port': 2004,\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npickles the metrics into a form that can be understood by the graphite pickle connector.", "response": "def _pickle_batch(self):\n        \"\"\"\n        Pickle the metrics into a form that can be understood\n        by the graphite pickle connector.\n        \"\"\"\n        # Pickle\n        payload = pickle.dumps(self.batch)\n\n        # Pack Message\n        header = struct.pack(\"!L\", len(payload))\n        message = header + payload\n\n        # Return Message\n        return message"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_default_config(self):\n        config = super(TwemproxyCollector, self).get_default_config()\n        config.update({\n            'path':     'twemproxy',\n            'hosts': ['localhost:22222']\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(OneWireCollector, self).get_default_config()\n        config.update({\n            'path': 'owfs',\n            'owfs': '/mnt/1wire',\n            # 'scan': {'temperature': 't'},\n            # 'id:24.BB000000': {'file_with_value': 'alias'},\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef collect(self):\n\n        metrics = {}\n\n        if 'scan' in self.config:\n            for ld in os.listdir(self.config['owfs']):\n                if '.' in ld:\n                    self.read_values(ld, self.config['scan'], metrics)\n\n        for oid, files in self.config.iteritems():\n            if oid[:3] == 'id:':\n                self.read_values(oid[3:], files, metrics)\n\n        for fn, fv in metrics.iteritems():\n            self.publish(fn, fv, 2)", "response": "Overrides the Collector. collect method\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_values(self, oid, files, metrics):\n\n        oid_path = os.path.join(self.config['owfs'], oid)\n        oid = oid.replace('.', '_')\n\n        for fn, alias in files.iteritems():\n            fv = os.path.join(oid_path, fn)\n            if os.path.isfile(fv):\n                try:\n                    f = open(fv)\n                    v = f.read()\n                    f.close()\n                except:\n                    self.log.error(\"Unable to read %s\", fv)\n                    raise\n\n                try:\n                    v = float(v)\n                except:\n                    self.log.error(\"Unexpected value %s in %s\", v, fv)\n                    raise\n\n                metrics[\"%s.%s\" % (oid, alias)] = v", "response": "Reads values from owfs and updates the metrics dict with format [ oid. alias ] = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(SolrCollector, self).get_default_config()\n        config.update({\n            'host':     'localhost',\n            'port':     8983,\n            'path':     'solr',\n            'core':     None,\n            'stats':    ['jvm', 'core', 'response',\n                         'query', 'update', 'cache'],\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_default_config(self):\n        config = super(IPCollector, self).get_default_config()\n        config.update({\n            'path': 'ip',\n            'allowed_names': 'InAddrErrors, InDelivers, InDiscards, ' +\n            'InHdrErrors, InReceives, InUnknownProtos, OutDiscards, ' +\n            'OutNoRoutes, OutRequests'\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_default_config_help(self):\n        config = super(GmetricHandler, self).get_default_config_help()\n\n        config.update({\n            'host': 'Hostname',\n            'port': 'Port',\n            'protocol': 'udp or tcp',\n        })\n\n        return config", "response": "Returns the help text for the configuration options for this handler\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the default config for the handler AttributeNames", "response": "def get_default_config(self):\n        \"\"\"\n        Return the default config for the handler\n        \"\"\"\n        config = super(GmetricHandler, self).get_default_config()\n\n        config.update({\n            'host': 'localhost',\n            'port': 8651,\n            'protocol': 'udp',\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend data to gmond.", "response": "def _send(self, metric):\n        \"\"\"\n        Send data to gmond.\n        \"\"\"\n        metric_name = self.get_name_from_path(metric.path)\n        tmax = \"60\"\n        dmax = \"0\"\n        slope = \"both\"\n        # FIXME: Badness, shouldn't *assume* double type\n        metric_type = \"double\"\n        units = \"\"\n        group = \"\"\n        self.gmetric.send(metric_name,\n                          metric.value,\n                          metric_type,\n                          units,\n                          slope,\n                          tmax,\n                          dmax,\n                          group)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_default_config(self):\n        config = super(BindCollector, self).get_default_config()\n        config.update({\n            'host': 'localhost',\n            'port': 8080,\n            'path': 'bind',\n            # Available stats:\n            # - resolver (Per-view resolver and cache statistics)\n            # - server (Incoming requests and their answers)\n            # - zonemgmt (Requests/responses related to zone management)\n            # - sockets (Socket statistics)\n            # - memory (Global memory usage)\n            'publish': [\n                'resolver',\n                'server',\n                'zonemgmt',\n                'sockets',\n                'memory',\n            ],\n            # By default we don't publish these special views\n            'publish_view_bind': False,\n            'publish_view_meta': False,\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the help text for the configuration options for this handler", "response": "def get_default_config_help(self):\n        \"\"\"\n        Returns the help text for the configuration options for this handler\n        \"\"\"\n        config = super(MQTTHandler, self).get_default_config_help()\n\n        config.update({\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the default config for the handler", "response": "def get_default_config(self):\n        \"\"\"\n        Return the default config for the handler\n        \"\"\"\n        config = super(MQTTHandler, self).get_default_config()\n\n        config.update({\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process(self, metric):\n\n        if not mosquitto:\n            return\n\n        line = str(metric)\n        topic, value, timestamp = line.split()\n        if len(self.prefix):\n            topic = \"%s/%s\" % (self.prefix, topic)\n        topic = topic.replace('.', '/')\n        topic = topic.replace('#', '&')     # Topic must not contain wildcards\n\n        if self.timestamp == 0:\n            self.mqttc.publish(topic, \"%s\" % (value), self.qos)\n        else:\n            self.mqttc.publish(topic, \"%s %s\" % (value, timestamp), self.qos)", "response": "Process a metric by converting metric name to MQTT topic name ;\n        the payload is metric and timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_default_config_help(self):\n        config = super(TSDBHandler, self).get_default_config_help()\n\n        config.update({\n            'host': '',\n            'port': '',\n            'timeout': '',\n            'tags': '',\n            'prefix': '',\n            'batch': '',\n            'compression': '',\n            'user': '',\n            'password': '',\n            'cleanMetrics': True,\n            'skipAggregates': True,\n        })\n\n        return config", "response": "Returns the help text for the configuration options for this handler\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_default_config(self):\n        config = super(TSDBHandler, self).get_default_config()\n\n        config.update({\n            'host': '127.0.0.1',\n            'port': 4242,\n            'timeout': 5,\n            'tags': '',\n            'prefix': '',\n            'batch': 1,\n            'compression': 0,\n            'user': '',\n            'password': '',\n            'cleanMetrics': True,\n            'skipAggregates': True,\n        })\n\n        return config", "response": "Returns the default config for the handler\nAttributeNames"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process(self, metric):\n        entry = {'timestamp': metric.timestamp, 'value': metric.value,\n                 \"tags\": {}}\n        entry[\"tags\"][\"hostname\"] = metric.host\n\n        if self.cleanMetrics:\n            metric = MetricWrapper(metric, self.log)\n            if self.skipAggregates and metric.isAggregate():\n                return\n            for tagKey in metric.getTags():\n                entry[\"tags\"][tagKey] = metric.getTags()[tagKey]\n\n        entry['metric'] = (self.prefix + metric.getCollectorPath() +\n                           '.' + metric.getMetricPath())\n\n        for [key, value] in self.tags:\n            entry[\"tags\"][key] = value\n\n        self.entrys.append(entry)\n\n        # send data if list is long enough\n        if (len(self.entrys) >= self.batch):\n            # Compress data\n            if self.compression >= 1:\n                data = StringIO.StringIO()\n                with contextlib.closing(gzip.GzipFile(fileobj=data,\n                                        compresslevel=self.compression,\n                                        mode=\"w\")) as f:\n                    f.write(json.dumps(self.entrys))\n                self._send(data.getvalue())\n            else:\n                # no compression\n                data = json.dumps(self.entrys)\n                self._send(data)", "response": "Process a metric by sending it to TSDB"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _send(self, content):\n        retry = 0\n        success = False\n        while retry < 3 and success is False:\n            self.log.debug(content)\n            try:\n                request = urllib2.Request(\"http://\"+self.host+\":\" +\n                                          str(self.port)+\"/api/put\",\n                                          content, self.httpheader)\n                response = urllib2.urlopen(url=request, timeout=self.timeout)\n                if response.getcode() < 301:\n                    self.log.debug(response.read())\n                    # Transaction should be finished\n                    self.log.debug(response.getcode())\n                    success = True\n            except urllib2.HTTPError as e:\n                self.log.error(\"HTTP Error Code: \"+str(e.code))\n                self.log.error(\"Message : \"+str(e.reason))\n            except urllib2.URLError as e:\n                self.log.error(\"Connection Error: \"+str(e.reason))\n            finally:\n                retry += 1\n        self.entrys = []", "response": "Send content to TSDB."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(MySQLPerfCollector, self).get_default_config()\n        config.update({\n            'path':     'mysql',\n            # Connection settings\n            'hosts':    [],\n\n            'slave':    'False',\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, oid, host, port, community):\n        # Initialize return value\n        ret = {}\n\n        # Convert OID to tuple if necessary\n        if not isinstance(oid, tuple):\n            oid = self._convert_to_oid(oid)\n\n        # Convert Host to IP if necessary\n        host = socket.gethostbyname(host)\n\n        # Assemble SNMP Auth Data\n        snmpAuthData = cmdgen.CommunityData(\n            'agent-{}'.format(community),\n            community)\n\n        # Assemble SNMP Transport Data\n        snmpTransportData = cmdgen.UdpTransportTarget(\n            (host, port),\n            int(self.config['timeout']),\n            int(self.config['retries']))\n\n        # Assemble SNMP Next Command\n        result = self.snmpCmdGen.getCmd(snmpAuthData, snmpTransportData, oid)\n        varBind = result[3]\n\n        # TODO: Error check\n\n        for o, v in varBind:\n            ret[str(o)] = v.prettyPrint()\n\n        return ret", "response": "Perform SNMP get for a given OID"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef walk(self, oid, host, port, community):\n        # Initialize return value\n        ret = {}\n\n        # Convert OID to tuple if necessary\n        if not isinstance(oid, tuple):\n            oid = self._convert_to_oid(oid)\n\n        # Convert Host to IP if necessary\n        host = socket.gethostbyname(host)\n\n        # Assemble SNMP Auth Data\n        snmpAuthData = cmdgen.CommunityData(\n            'agent-{}'.format(community),\n            community)\n\n        # Assemble SNMP Transport Data\n        snmpTransportData = cmdgen.UdpTransportTarget(\n            (host, port),\n            int(self.config['timeout']),\n            int(self.config['retries']))\n\n        # Assemble SNMP Next Command\n        resultTable = self.snmpCmdGen.nextCmd(snmpAuthData,\n                                              snmpTransportData,\n                                              oid)\n        varBindTable = resultTable[3]\n\n        # TODO: Error Check\n\n        for varBindTableRow in varBindTable:\n            for o, v in varBindTableRow:\n                ret[str(o)] = v.prettyPrint()\n\n        return ret", "response": "Perform an SNMP walk on a given OID and return a dictionary of the result."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(NfsdCollector, self).get_default_config()\n        config.update({\n            'path':     'nfsd'\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncollects stats from the current cache.", "response": "def collect(self):\n        \"\"\"\n        Collect stats\n        \"\"\"\n        if os.access(self.PROC, os.R_OK):\n\n            results = {}\n            # Open file\n            file = open(self.PROC)\n\n            for line in file:\n                line = line.split()\n\n                if line[0] == 'rc':\n                    results['reply_cache.hits'] = line[1]\n                    results['reply_cache.misses'] = line[2]\n                    results['reply_cache.nocache'] = line[3]\n                elif line[0] == 'fh':\n                    results['filehandle.stale'] = line[1]\n                    results['filehandle.total-lookups'] = line[2]\n                    results['filehandle.anonlookups'] = line[3]\n                    results['filehandle.dir-not-in-cache'] = line[4]\n                    results['filehandle.nodir-not-in-cache'] = line[5]\n                elif line[0] == 'io':\n                    results['input_output.bytes-read'] = line[1]\n                    results['input_output.bytes-written'] = line[2]\n                elif line[0] == 'th':\n                    results['threads.threads'] = line[1]\n                    results['threads.fullcnt'] = line[2]\n                    results['threads.10-20-pct'] = line[3]\n                    results['threads.20-30-pct'] = line[4]\n                    results['threads.30-40-pct'] = line[5]\n                    results['threads.40-50-pct'] = line[6]\n                    results['threads.50-60-pct'] = line[7]\n                    results['threads.60-70-pct'] = line[8]\n                    results['threads.70-80-pct'] = line[9]\n                    results['threads.80-90-pct'] = line[10]\n                    results['threads.90-100-pct'] = line[11]\n                    results['threads.100-pct'] = line[12]\n                elif line[0] == 'ra':\n                    results['read-ahead.cache-size'] = line[1]\n                    results['read-ahead.10-pct'] = line[2]\n                    results['read-ahead.20-pct'] = line[3]\n                    results['read-ahead.30-pct'] = line[4]\n                    results['read-ahead.40-pct'] = line[5]\n                    results['read-ahead.50-pct'] = line[6]\n                    results['read-ahead.60-pct'] = line[7]\n                    results['read-ahead.70-pct'] = line[8]\n                    results['read-ahead.80-pct'] = line[9]\n                    results['read-ahead.90-pct'] = line[10]\n                    results['read-ahead.100-pct'] = line[11]\n                    results['read-ahead.not-found'] = line[12]\n                elif line[0] == 'net':\n                    results['net.cnt'] = line[1]\n                    results['net.udpcnt'] = line[2]\n                    results['net.tcpcnt'] = line[3]\n                    results['net.tcpconn'] = line[4]\n                elif line[0] == 'rpc':\n                    results['rpc.cnt'] = line[1]\n                    results['rpc.badfmt'] = line[2]\n                    results['rpc.badauth'] = line[3]\n                    results['rpc.badclnt'] = line[4]\n                elif line[0] == 'proc2':\n                    results['v2.unknown'] = line[1]\n                    results['v2.null'] = line[2]\n                    results['v2.getattr'] = line[3]\n                    results['v2.setattr'] = line[4]\n                    results['v2.root'] = line[5]\n                    results['v2.lookup'] = line[6]\n                    results['v2.readlink'] = line[7]\n                    results['v2.read'] = line[8]\n                    results['v2.wrcache'] = line[9]\n                    results['v2.write'] = line[10]\n                    results['v2.create'] = line[11]\n                    results['v2.remove'] = line[12]\n                    results['v2.rename'] = line[13]\n                    results['v2.link'] = line[14]\n                    results['v2.symlink'] = line[15]\n                    results['v2.mkdir'] = line[16]\n                    results['v2.rmdir'] = line[17]\n                    results['v2.readdir'] = line[18]\n                    results['v2.fsstat'] = line[19]\n                elif line[0] == 'proc3':\n                    results['v3.unknown'] = line[1]\n                    results['v3.null'] = line[2]\n                    results['v3.getattr'] = line[3]\n                    results['v3.setattr'] = line[4]\n                    results['v3.lookup'] = line[5]\n                    results['v3.access'] = line[6]\n                    results['v3.readlink'] = line[7]\n                    results['v3.read'] = line[8]\n                    results['v3.write'] = line[9]\n                    results['v3.create'] = line[10]\n                    results['v3.mkdir'] = line[11]\n                    results['v3.symlink'] = line[12]\n                    results['v3.mknod'] = line[13]\n                    results['v3.remove'] = line[14]\n                    results['v3.rmdir'] = line[15]\n                    results['v3.rename'] = line[16]\n                    results['v3.link'] = line[17]\n                    results['v3.readdir'] = line[18]\n                    results['v3.readdirplus'] = line[19]\n                    results['v3.fsstat'] = line[20]\n                    results['v3.fsinfo'] = line[21]\n                    results['v3.pathconf'] = line[22]\n                    results['v3.commit'] = line[23]\n                elif line[0] == 'proc4':\n                    results['v4.unknown'] = line[1]\n                    results['v4.null'] = line[2]\n                    results['v4.compound'] = line[3]\n                elif line[0] == 'proc4ops':\n                    results['v4.ops.unknown'] = line[1]\n                    results['v4.ops.op0-unused'] = line[2]\n                    results['v4.ops.op1-unused'] = line[3]\n                    results['v4.ops.op2-future'] = line[4]\n                    results['v4.ops.access'] = line[5]\n                    results['v4.ops.close'] = line[6]\n                    results['v4.ops.commit'] = line[7]\n                    results['v4.ops.create'] = line[8]\n                    results['v4.ops.delegpurge'] = line[9]\n                    results['v4.ops.delegreturn'] = line[10]\n                    results['v4.ops.getattr'] = line[11]\n                    results['v4.ops.getfh'] = line[12]\n                    results['v4.ops.link'] = line[13]\n                    results['v4.ops.lock'] = line[14]\n                    results['v4.ops.lockt'] = line[15]\n                    results['v4.ops.locku'] = line[16]\n                    results['v4.ops.lookup'] = line[17]\n                    results['v4.ops.lookup_root'] = line[18]\n                    results['v4.ops.nverify'] = line[19]\n                    results['v4.ops.open'] = line[20]\n                    results['v4.ops.openattr'] = line[21]\n                    results['v4.ops.open_conf'] = line[22]\n                    results['v4.ops.open_dgrd'] = line[23]\n                    results['v4.ops.putfh'] = line[24]\n                    results['v4.ops.putpubfh'] = line[25]\n                    results['v4.ops.putrootfh'] = line[26]\n                    results['v4.ops.read'] = line[27]\n                    results['v4.ops.readdir'] = line[28]\n                    results['v4.ops.readlink'] = line[29]\n                    results['v4.ops.remove'] = line[30]\n                    results['v4.ops.rename'] = line[31]\n                    results['v4.ops.renew'] = line[32]\n                    results['v4.ops.restorefh'] = line[33]\n                    results['v4.ops.savefh'] = line[34]\n                    results['v4.ops.secinfo'] = line[35]\n                    results['v4.ops.setattr'] = line[36]\n                    results['v4.ops.setcltid'] = line[37]\n                    results['v4.ops.setcltidconf'] = line[38]\n                    results['v4.ops.verify'] = line[39]\n                    results['v4.ops.write'] = line[40]\n                    results['v4.ops.rellockowner'] = line[41]\n\n            # Close File\n            file.close()\n\n            for stat in results.keys():\n                metric_name = '.' + stat\n                metric_value = long(float(results[stat]))\n                metric_value = self.derivative(metric_name, metric_value)\n                self.publish(metric_name, metric_value, precision=3)\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\niterate over connections and count states for specified port and return a dict with stats for each.", "response": "def get_port_stats(port):\n    \"\"\"\n    Iterate over connections and count states for specified port\n    :param port: port for which stats are collected\n    :return: Counter with port states\n    \"\"\"\n    cnts = defaultdict(int)\n    for c in psutil.net_connections():\n        c_port = c.laddr[1]\n        if c_port != port:\n            continue\n        status = c.status.lower()\n        cnts[status] += 1\n    return cnts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\noverride the Collector. collect method to add metrics to the object.", "response": "def collect(self):\n        \"\"\"\n        Overrides the Collector.collect method\n        \"\"\"\n\n        if psutil is None:\n            self.log.error('Unable to import module psutil')\n            return {}\n\n        for port_name, port_cfg in self.ports.iteritems():\n            port = int(port_cfg['number'])\n            stats = get_port_stats(port)\n\n            for stat_name, stat_value in stats.iteritems():\n                metric_name = '%s.%s' % (port_name, stat_name)\n                self.publish(metric_name, stat_value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(VMStatCollector, self).get_default_config()\n        config.update({\n            'path':     'vmstat'\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_default_config(self):\n        config = super(DiskUsageCollector, self).get_default_config()\n        config.update({\n            'path':     'iostat',\n            'devices':  ('PhysicalDrive[0-9]+$' +\n                         '|md[0-9]+$' +\n                         '|sd[a-z]+[0-9]*$' +\n                         '|x?vd[a-z]+[0-9]*$' +\n                         '|disk[0-9]+$' +\n                         '|dm\\-[0-9]+$'),\n            'sector_size': 512,\n            'send_zero': False,\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_disk_statistics(self):\n        result = {}\n\n        if os.access('/proc/diskstats', os.R_OK):\n            self.proc_diskstats = True\n            fp = open('/proc/diskstats')\n\n            try:\n                for line in fp:\n                    try:\n                        columns = line.split()\n                        # On early linux v2.6 versions, partitions have only 4\n                        # output fields not 11. From linux 2.6.25 partitions\n                        # have the full stats set.\n                        if len(columns) < 14:\n                            continue\n                        major = int(columns[0])\n                        minor = int(columns[1])\n                        device = columns[2]\n\n                        if ((device.startswith('ram') or\n                             device.startswith('loop'))):\n                            continue\n\n                        result[(major, minor)] = {\n                            'device': device,\n                            'reads': float(columns[3]),\n                            'reads_merged': float(columns[4]),\n                            'reads_sectors': float(columns[5]),\n                            'reads_milliseconds': float(columns[6]),\n                            'writes': float(columns[7]),\n                            'writes_merged': float(columns[8]),\n                            'writes_sectors': float(columns[9]),\n                            'writes_milliseconds': float(columns[10]),\n                            'io_in_progress': float(columns[11]),\n                            'io_milliseconds': float(columns[12]),\n                            'io_milliseconds_weighted': float(columns[13])\n                        }\n                    except ValueError:\n                        continue\n            finally:\n                fp.close()\n        else:\n            self.proc_diskstats = False\n            if not psutil:\n                self.log.error('Unable to import psutil')\n                return None\n\n            disks = psutil.disk_io_counters(True)\n            sector_size = int(self.config['sector_size'])\n            for disk in disks:\n                result[(0, len(result))] = {\n                    'device': disk,\n                    'reads': disks[disk].read_count,\n                    'reads_sectors': disks[disk].read_bytes / sector_size,\n                    'reads_milliseconds': disks[disk].read_time,\n                    'writes': disks[disk].write_count,\n                    'writes_sectors': disks[disk].write_bytes / sector_size,\n                    'writes_milliseconds': disks[disk].write_time,\n                    'io_milliseconds':\n                        disks[disk].read_time + disks[disk].write_time,\n                    'io_milliseconds_weighted':\n                        disks[disk].read_time + disks[disk].write_time\n                }\n\n        return result", "response": "Get disk statistics for a specific instance of the system."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_default_config_help(self):\n        config = super(RiemannHandler, self).get_default_config_help()\n\n        config.update({\n            'host': '',\n            'port': '',\n            'transport': 'tcp or udp',\n        })\n\n        return config", "response": "Returns the help text for the configuration options for this handler\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the default config for the handler", "response": "def get_default_config(self):\n        \"\"\"\n        Return the default config for the handler\n        \"\"\"\n        config = super(RiemannHandler, self).get_default_config()\n\n        config.update({\n            'host': '',\n            'port': 123,\n            'transport': 'tcp',\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend a metric to Riemann.", "response": "def process(self, metric):\n        \"\"\"\n        Send a metric to Riemann.\n        \"\"\"\n        event = self._metric_to_riemann_event(metric)\n        try:\n            self.client.send_event(event)\n        except Exception as e:\n            self.log.error(\n                \"RiemannHandler: Error sending event to Riemann: %s\", e)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _metric_to_riemann_event(self, metric):\n        # Riemann has a separate \"host\" field, so remove from the path.\n        path = '%s.%s.%s' % (\n            metric.getPathPrefix(),\n            metric.getCollectorPath(),\n            metric.getMetricPath()\n        )\n\n        return self.client.create_event({\n            'host': metric.host,\n            'service': path,\n            'time': metric.timestamp,\n            'metric_f': float(metric.value),\n            'ttl': metric.ttl,\n        })", "response": "Convert a metric to a Riemann event."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(SqsCollector, self).get_default_config()\n        config.update({\n            'path': 'sqs',\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_default_config(self):\n        config = super(MemoryCgroupCollector, self).get_default_config()\n        config.update({\n            'path':     'memory_cgroup',\n            'memory_path': '/sys/fs/cgroup/memory/',\n            'skip': [],\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the help text for the configuration options for this handler.", "response": "def get_default_config_help(self):\n        \"\"\"\n        Returns the help text for the configuration options for this handler\n        \"\"\"\n        config = super(HostedGraphiteHandler, self).get_default_config_help()\n\n        config.update({\n            'apikey': 'Api key to use',\n            'host': 'Hostname',\n            'port': 'Port',\n            'proto': 'udp or tcp',\n            'timeout': '',\n            'batch': 'How many to store before sending to the graphite server',\n            'max_backlog_multiplier': 'how many batches to store before trimming',  # NOQA\n            'trim_backlog_multiplier': 'Trim down how many batches',\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the default config for the handler", "response": "def get_default_config(self):\n        \"\"\"\n        Return the default config for the handler\n        \"\"\"\n        config = super(HostedGraphiteHandler, self).get_default_config()\n\n        config.update({\n            'apikey': '',\n            'host': 'carbon.hostedgraphite.com',\n            'port': 2003,\n            'proto': 'tcp',\n            'timeout': 15,\n            'batch': 1,\n            'max_backlog_multiplier': 5,\n            'trim_backlog_multiplier': 4,\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process(self, metric):\n        metric = self.key + '.' + str(metric)\n        self.graphite.process(metric)", "response": "Process a metric by sending it to graphite"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _process(self, metric):\n        metric = self.key + '.' + str(metric)\n        self.graphite._process(metric)", "response": "Process a metric by sending it to graphite"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(S3BucketCollector, self).get_default_config()\n        config.update({\n            'path':      'aws.s3',\n            'byte_unit': 'byte'\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncollecting s3 bucket stats and publish new stats", "response": "def collect(self):\n        \"\"\"\n        Collect s3 bucket stats\n        \"\"\"\n        if boto is None:\n            self.log.error(\"Unable to import boto python module\")\n            return {}\n        for s3instance in self.config['s3']:\n            self.log.info(\"S3: byte_unit: %s\" % self.config['byte_unit'])\n            aws_access = self.config['s3'][s3instance]['aws_access_key']\n            aws_secret = self.config['s3'][s3instance]['aws_secret_key']\n            for bucket_name in self.config['s3'][s3instance]['buckets']:\n                bucket = self.getBucket(aws_access, aws_secret, bucket_name)\n\n                # collect bucket size\n                total_size = self.getBucketSize(bucket)\n                for byte_unit in self.config['byte_unit']:\n                    new_size = diamond.convertor.binary.convert(\n                        value=total_size,\n                        oldUnit='byte',\n                        newUnit=byte_unit\n                    )\n                    self.publish(\"%s.size.%s\" % (bucket_name, byte_unit),\n                                 new_size)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreplace all non - letter characters with underscores", "response": "def key_to_metric(self, key):\n        \"\"\"Replace all non-letter characters with underscores\"\"\"\n        return ''.join(l if l in string.letters else '_' for l in key)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(IPMISensorCollector, self).get_default_config()\n        config.update({\n            'bin':              '/usr/bin/ipmitool',\n            'use_sudo':         False,\n            'sudo_cmd':         '/usr/bin/sudo',\n            'path':             'ipmi.sensors',\n            'thresholds':       False,\n            'delimiter':        '.'\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_value(self, value):\n        value = value.strip()\n\n        # Skip missing sensors\n        if value == 'na':\n            return None\n\n        # Try just getting the float value\n        try:\n            return float(value)\n        except:\n            pass\n\n        # Next best guess is a hex value\n        try:\n            return float.fromhex(value)\n        except:\n            pass\n\n        # No luck, bail\n        return None", "response": "Parse a value string to float for reporting\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_default_config(self):\n        config = super(SquidCollector, self).get_default_config()\n        config.update({\n            'hosts': ['localhost:3128'],\n            'path': 'squid',\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef collect(self):\n        perfdata_dir = self.config['perfdata_dir']\n\n        try:\n            filenames = os.listdir(perfdata_dir)\n        except OSError:\n            self.log.error(\"Cannot read directory `{dir}'\".format(\n                dir=perfdata_dir))\n            return\n\n        for filename in filenames:\n            self._process_file(os.path.join(perfdata_dir, filename))", "response": "Collect statistics from a Nagios perfdata directory."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _extract_fields(self, line):\n        acc = {}\n        field_tokens = line.split(\"\\t\")\n        for field_token in field_tokens:\n            kv_tokens = field_token.split('::')\n            if len(kv_tokens) == 2:\n                (key, value) = kv_tokens\n                acc[key] = value\n\n        return acc", "response": "Extract the key value fields from a line of performance data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nverify that all necessary fields are present in the perfdata.", "response": "def _fields_valid(self, d):\n        \"\"\"Verify that all necessary fields are present\n\n        Determine whether the fields parsed represent a host or\n        service perfdata. If the perfdata is unknown, return False.\n        If the perfdata does not contain all fields required for that\n        type, return False. Otherwise, return True.\n        \"\"\"\n        if 'DATATYPE' not in d:\n            return False\n\n        datatype = d['DATATYPE']\n        if datatype == 'HOSTPERFDATA':\n            fields = self.GENERIC_FIELDS + self.HOST_FIELDS\n        elif datatype == 'SERVICEPERFDATA':\n            fields = self.GENERIC_FIELDS + self.SERVICE_FIELDS\n        else:\n            return False\n\n        for field in fields:\n            if field not in d:\n                return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nnormalize the value to the given unit.", "response": "def _normalize_to_unit(self, value, unit):\n        \"\"\"Normalize the value to the unit returned.\n\n        We use base-1000 for second-based units, and base-1024 for\n        byte-based units. Sadly, the Nagios-Plugins specification doesn't\n        disambiguate base-1000 (KB) and base-1024 (KiB).\n        \"\"\"\n        if unit == 'ms':\n            return value / 1000.0\n        if unit == 'us':\n            return value / 1000000.0\n        if unit == 'KB':\n            return value * 1024\n        if unit == 'MB':\n            return value * 1024 * 1024\n        if unit == 'GB':\n            return value * 1024 * 1024 * 1024\n        if unit == 'TB':\n            return value * 1024 * 1024 * 1024 * 1024\n\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a string of performance data into a list of tuples.", "response": "def _parse_perfdata(self, s):\n        \"\"\"Parse performance data from a perfdata string\n        \"\"\"\n        metrics = []\n        counters = re.findall(self.TOKENIZER_RE, s)\n        if counters is None:\n            self.log.warning(\"Failed to parse performance data: {s}\".format(\n                s=s))\n            return metrics\n\n        for (key, value, uom, warn, crit, min, max) in counters:\n            try:\n                norm_value = self._normalize_to_unit(float(value), uom)\n                metrics.append((key, norm_value))\n            except ValueError:\n                self.log.warning(\n                    \"Couldn't convert value '{value}' to float\".format(\n                        value=value))\n\n        return metrics"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing and submit the metrics from a file containing a list of line - by - line.", "response": "def _process_file(self, path):\n        \"\"\"Parse and submit the metrics from a file\n        \"\"\"\n        try:\n            f = open(path)\n            for line in f:\n                self._process_line(line)\n\n            os.remove(path)\n        except IOError as ex:\n            self.log.error(\"Could not open file `{path}': {error}\".format(\n                path=path, error=ex.strerror))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _process_line(self, line):\n        fields = self._extract_fields(line)\n        if not self._fields_valid(fields):\n            self.log.warning(\"Missing required fields for line: {line}\".format(\n                line=line))\n\n        metric_path_base = []\n        graphite_prefix = fields.get('GRAPHITEPREFIX')\n        graphite_postfix = fields.get('GRAPHITEPOSTFIX')\n\n        if graphite_prefix:\n            metric_path_base.append(graphite_prefix)\n\n        hostname = fields['HOSTNAME'].lower()\n        metric_path_base.append(hostname)\n\n        datatype = fields['DATATYPE']\n        if datatype == 'HOSTPERFDATA':\n            metric_path_base.append('host')\n        elif datatype == 'SERVICEPERFDATA':\n            service_desc = fields.get('SERVICEDESC')\n            graphite_postfix = fields.get('GRAPHITEPOSTFIX')\n            if graphite_postfix:\n                metric_path_base.append(graphite_postfix)\n            else:\n                metric_path_base.append(service_desc)\n\n        perfdata = fields[datatype]\n        counters = self._parse_perfdata(perfdata)\n\n        for (counter, value) in counters:\n            metric_path = metric_path_base + [counter]\n            metric_path = [self._sanitize(x) for x in metric_path]\n            metric_name = '.'.join(metric_path)\n            self.publish(metric_name, value)", "response": "Parse and submit the metrics from a line of perfdata output"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _bind(self):\n\n        credentials = pika.PlainCredentials(self.user, self.password)\n        params = pika.ConnectionParameters(credentials=credentials,\n                                           host=self.server,\n                                           virtual_host=self.vhost,\n                                           port=self.port)\n\n        self.connection = pika.BlockingConnection(params)\n        self.channel = self.connection.channel()\n\n        # NOTE : PIKA version uses 'exchange_type' instead of 'type'\n\n        self.channel.exchange_declare(exchange=self.topic_exchange,\n                                      exchange_type=\"topic\")", "response": "Create socket and bind"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses a metric and send it to RabbitMQ topic exchange", "response": "def process(self, metric):\n        \"\"\"\n          Process a metric and send it to RabbitMQ topic exchange\n        \"\"\"\n        # Send the data as ......\n        if not pika:\n            return\n\n        routingKeyDic = {\n            'metric': lambda: metric.path,\n            'custom': lambda: self.custom_routing_key,\n\n            # These option and the below are really not needed because\n            #  with Rabbitmq you can use regular expressions to indicate\n            #  what routing_keys to subscribe to. But I figure this is\n            #  a good example of how to allow more routing keys\n\n            'host': lambda: metric.host,\n            'metric.path': metric.getMetricPath,\n            'path.prefix': metric.getPathPrefix,\n            'collector.path': metric.getCollectorPath,\n        }\n\n        try:\n            self.channel.basic_publish(\n                exchange=self.topic_exchange,\n                routing_key=routingKeyDic[self.routing_key](),\n                body=\"%s\" % metric)\n\n        except Exception:  # Rough connection re-try logic.\n            self.log.info(\n                \"Failed publishing to rabbitMQ. Attempting reconnect\")\n            self._bind()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(OpenvzCollector, self).get_default_config()\n        config.update({\n            'path': 'openvz',\n            'bin': '/usr/sbin/vzlist',\n            'keyname': 'hostname'\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\noverriding the default get_default_config method to provide the default config for the SNMPInterfaceCollector", "response": "def get_default_config(self):\n        \"\"\"\n        Override SNMPCollector.get_default_config method to provide\n        default_config for the SNMPInterfaceCollector\n        \"\"\"\n        default_config = super(SNMPInterfaceCollector,\n                               self).get_default_config()\n        default_config['path'] = 'interface'\n        default_config['byte_unit'] = ['bit', 'byte']\n        return default_config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncollect SNMP interface statistics from a device.", "response": "def collect_snmp(self, device, host, port, community):\n        \"\"\"\n        Collect SNMP interface data from device\n        \"\"\"\n        # Log\n        self.log.info(\"Collecting SNMP interface statistics from: %s\", device)\n\n        # Define a list of interface indexes\n        ifIndexes = []\n\n        # Get Interface Indexes\n        ifIndexOid = '.'.join([self.IF_MIB_INDEX_OID])\n        ifIndexData = self.walk(ifIndexOid, host, port, community)\n        ifIndexes = [v for v in ifIndexData.values()]\n\n        for ifIndex in ifIndexes:\n            # Get Interface Type\n            ifTypeOid = '.'.join([self.IF_MIB_TYPE_OID, ifIndex])\n            ifTypeData = self.get(ifTypeOid, host, port, community)\n            if ifTypeData[ifTypeOid] not in self.IF_TYPES:\n                # Skip Interface\n                continue\n            # Get Interface Name\n            ifNameOid = '.'.join([self.IF_MIB_NAME_OID, ifIndex])\n            ifNameData = self.get(ifNameOid, host, port, community)\n            ifName = ifNameData[ifNameOid]\n            # Remove quotes from string\n            ifName = re.sub(r'(\\\"|\\')', '', ifName)\n\n            # Get Gauges\n            for gaugeName, gaugeOid in self.IF_MIB_GAUGE_OID_TABLE.items():\n                ifGaugeOid = '.'.join([self.IF_MIB_GAUGE_OID_TABLE[gaugeName],\n                                       ifIndex])\n                ifGaugeData = self.get(ifGaugeOid, host, port, community)\n                ifGaugeValue = ifGaugeData[ifGaugeOid]\n                if not ifGaugeValue:\n                    continue\n\n                # Get Metric Name and Value\n                metricIfDescr = re.sub(r'\\W', '_', ifName)\n                metricName = '.'.join([metricIfDescr, gaugeName])\n                metricValue = int(ifGaugeValue)\n                # Get Metric Path\n                metricPath = '.'.join(['devices',\n                                       device,\n                                       self.config['path'],\n                                       metricName])\n                # Publish Metric\n                self.publish_gauge(metricPath, metricValue)\n\n            # Get counters (64bit)\n            counterItems = self.IF_MIB_COUNTER_OID_TABLE.items()\n            for counterName, counterOid in counterItems:\n                ifCounterOid = '.'.join(\n                    [self.IF_MIB_COUNTER_OID_TABLE[counterName], ifIndex])\n                ifCounterData = self.get(ifCounterOid, host, port, community)\n                ifCounterValue = ifCounterData[ifCounterOid]\n                if not ifCounterValue:\n                    continue\n\n                # Get Metric Name and Value\n                metricIfDescr = re.sub(r'\\W', '_', ifName)\n\n                if counterName in ['ifHCInOctets', 'ifHCOutOctets']:\n                    for unit in self.config['byte_unit']:\n                        # Convert Metric\n                        metricName = '.'.join([metricIfDescr,\n                                               counterName.replace('Octets',\n                                                                   unit)])\n                        metricValue = diamond.convertor.binary.convert(\n                            value=ifCounterValue,\n                            oldUnit='byte',\n                            newUnit=unit)\n\n                        # Get Metric Path\n                        metricPath = '.'.join(['devices',\n                                               device,\n                                               self.config['path'],\n                                               metricName])\n                        # Publish Metric\n                        self.publish_counter(metricPath,\n                                             metricValue,\n                                             max_value=18446744073709600000,\n                                             )\n                else:\n                    metricName = '.'.join([metricIfDescr, counterName])\n                    metricValue = int(ifCounterValue)\n\n                    # Get Metric Path\n                    metricPath = '.'.join(['devices',\n                                           device,\n                                           self.config['path'],\n                                           metricName])\n                    # Publish Metric\n                    self.publish_counter(metricPath,\n                                         metricValue,\n                                         max_value=18446744073709600000,\n                                         )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_default_config(self):\n        config = super(HBaseCollector, self).get_default_config()\n        config.update({\n            'path':     'hbase',\n            'metrics':  ['/var/log/hbase/*.metrics'],\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(KVMCollector, self).get_default_config()\n        config.update({\n            'path': 'kvm',\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_default_config(self):\n        config = super(DarnerCollector, self).get_default_config()\n        config.update({\n            'path':     'darner',\n\n            # Which rows of 'status' you would like to publish.\n            # 'telnet host port' and type stats and hit enter to see the list\n            # of possibilities.\n            # Leave unset to publish all\n            # 'publish': ''\n            'publish_queues': True,\n\n            # Connection settings\n            'hosts': ['localhost:22133']\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(CPUCollector, self).get_default_config()\n        config.update({\n            'path':     'cpu',\n            'percore':  'True',\n            'xenfix':   None,\n            'simple':   'False',\n            'normalize': 'False',\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef collect(self):\n\n        def cpu_time_list():\n            \"\"\"\n            get cpu time list\n            \"\"\"\n\n            statFile = open(self.PROC, \"r\")\n            timeList = statFile.readline().split(\" \")[2:6]\n            for i in range(len(timeList)):\n                timeList[i] = int(timeList[i])\n            statFile.close()\n            return timeList\n\n        def cpu_delta_time(interval):\n            \"\"\"\n            Get before and after cpu times for usage calc\n            \"\"\"\n            pre_check = cpu_time_list()\n            time.sleep(interval)\n            post_check = cpu_time_list()\n            for i in range(len(pre_check)):\n                post_check[i] -= pre_check[i]\n            return post_check\n\n        if os.access(self.PROC, os.R_OK):\n\n            # If simple only return aggregate CPU% metric\n            if str_to_bool(self.config['simple']):\n                dt = cpu_delta_time(self.INTERVAL)\n                cpuPct = 100 - (dt[len(dt) - 1] * 100.00 / sum(dt))\n                self.publish('percent', str('%.4f' % cpuPct))\n                return True\n\n            results = {}\n            # Open file\n            file = open(self.PROC)\n\n            ncpus = -1  # dont want to count the 'cpu'(total) cpu.\n            for line in file:\n                if not line.startswith('cpu'):\n                    continue\n\n                ncpus += 1\n                elements = line.split()\n\n                cpu = elements[0]\n\n                if cpu == 'cpu':\n                    cpu = 'total'\n                elif not str_to_bool(self.config['percore']):\n                    continue\n\n                results[cpu] = {}\n\n                if len(elements) >= 2:\n                    results[cpu]['user'] = elements[1]\n                if len(elements) >= 3:\n                    results[cpu]['nice'] = elements[2]\n                if len(elements) >= 4:\n                    results[cpu]['system'] = elements[3]\n                if len(elements) >= 5:\n                    results[cpu]['idle'] = elements[4]\n                if len(elements) >= 6:\n                    results[cpu]['iowait'] = elements[5]\n                if len(elements) >= 7:\n                    results[cpu]['irq'] = elements[6]\n                if len(elements) >= 8:\n                    results[cpu]['softirq'] = elements[7]\n                if len(elements) >= 9:\n                    results[cpu]['steal'] = elements[8]\n                if len(elements) >= 10:\n                    results[cpu]['guest'] = elements[9]\n                if len(elements) >= 11:\n                    results[cpu]['guest_nice'] = elements[10]\n\n            # Close File\n            file.close()\n\n            metrics = {'cpu_count': ncpus}\n\n            for cpu in results.keys():\n                stats = results[cpu]\n                for s in stats.keys():\n                    # Get Metric Name\n                    metric_name = '.'.join([cpu, s])\n                    # Get actual data\n                    if ((str_to_bool(self.config['normalize']) and\n                         cpu == 'total' and\n                         ncpus > 0)):\n                        metrics[metric_name] = self.derivative(\n                            metric_name,\n                            long(stats[s]),\n                            self.MAX_VALUES[s]) / ncpus\n                    else:\n                        metrics[metric_name] = self.derivative(\n                            metric_name,\n                            long(stats[s]),\n                            self.MAX_VALUES[s])\n\n            # Check for a bug in xen where the idle time is doubled for guest\n            # See https://bugzilla.redhat.com/show_bug.cgi?id=624756\n            if self.config['xenfix'] is None or self.config['xenfix'] is True:\n                if os.path.isdir('/proc/xen'):\n                    total = 0\n                    for metric_name in metrics.keys():\n                        if 'cpu0.' in metric_name:\n                            total += int(metrics[metric_name])\n                    if total > 110:\n                        self.config['xenfix'] = True\n                        for mname in metrics.keys():\n                            if '.idle' in mname:\n                                metrics[mname] = float(metrics[mname]) / 2\n                    elif total > 0:\n                        self.config['xenfix'] = False\n                else:\n                    self.config['xenfix'] = False\n\n            # Publish Metric Derivative\n            for metric_name in metrics.keys():\n                self.publish(metric_name,\n                             metrics[metric_name],\n                             precision=2)\n            return True\n\n        else:\n            if not psutil:\n                self.log.error('Unable to import psutil')\n                self.log.error('No cpu metrics retrieved')\n                return None\n\n            cpu_time = psutil.cpu_times(True)\n            cpu_count = len(cpu_time)\n            total_time = psutil.cpu_times()\n            for i in range(0, len(cpu_time)):\n                metric_name = 'cpu' + str(i)\n                self.publish(\n                    metric_name + '.user',\n                    self.derivative(metric_name + '.user',\n                                    cpu_time[i].user,\n                                    self.MAX_VALUES['user']),\n                    precision=2)\n\n                if hasattr(cpu_time[i], 'nice'):\n                    self.publish(\n                        metric_name + '.nice',\n                        self.derivative(metric_name + '.nice',\n                                        cpu_time[i].nice,\n                                        self.MAX_VALUES['nice']),\n                        precision=2)\n\n                self.publish(\n                    metric_name + '.system',\n                    self.derivative(metric_name + '.system',\n                                    cpu_time[i].system,\n                                    self.MAX_VALUES['system']),\n                    precision=2)\n\n                self.publish(\n                    metric_name + '.idle',\n                    self.derivative(metric_name + '.idle',\n                                    cpu_time[i].idle,\n                                    self.MAX_VALUES['idle']),\n                    precision=2)\n\n            metric_name = 'total'\n            self.publish(\n                metric_name + '.user',\n                self.derivative(metric_name + '.user',\n                                total_time.user,\n                                self.MAX_VALUES['user']) / cpu_count,\n                precision=2)\n\n            if hasattr(total_time, 'nice'):\n                self.publish(\n                    metric_name + '.nice',\n                    self.derivative(metric_name + '.nice',\n                                    total_time.nice,\n                                    self.MAX_VALUES['nice']) / cpu_count,\n                    precision=2)\n\n            self.publish(\n                metric_name + '.system',\n                self.derivative(metric_name + '.system',\n                                total_time.system,\n                                self.MAX_VALUES['system']) / cpu_count,\n                precision=2)\n\n            self.publish(\n                metric_name + '.idle',\n                self.derivative(metric_name + '.idle',\n                                total_time.idle,\n                                self.MAX_VALUES['idle']) / cpu_count,\n                precision=2)\n\n            self.publish('cpu_count', psutil.cpu_count())\n\n            return True\n\n        return None", "response": "Collect all the available CPU stats and CPU stats for the current process."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _collect_for_instance(self, instance, connection):\n        with connection.cursor() as cursor:\n            for queue, metrics in self.get_queue_info(instance, cursor):\n                for name, metric in metrics.items():\n                    self.publish('.'.join((instance, queue, name)), metric)\n\n        with connection.cursor() as cursor:\n            consumers = self.get_consumer_info(instance, cursor)\n            for queue, consumer, metrics in consumers:\n                for name, metric in metrics.items():\n                    key_parts = (instance, queue, 'consumers', consumer, name)\n                    self.publish('.'.join(key_parts), metric)", "response": "Collects metrics for a named connection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nyield a generator that yields the queue name ticker_lag ev_per_sec and the queue name.", "response": "def get_queue_info(self, instance, cursor):\n        \"\"\"Collects metrics for all queues on the connected database.\"\"\"\n        cursor.execute(self.QUEUE_INFO_STATEMENT)\n        for queue_name, ticker_lag, ev_per_sec in cursor:\n            yield queue_name, {\n                'ticker_lag': ticker_lag,\n                'ev_per_sec': ev_per_sec,\n            }"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the consumer info for all consumers on the connected database.", "response": "def get_consumer_info(self, instance, cursor):\n        \"\"\"Collects metrics for all consumers on the connected database.\"\"\"\n        cursor.execute(self.CONSUMER_INFO_STATEMENT)\n        for queue_name, consumer_name, lag, pending_events, last_seen in cursor:\n            yield queue_name, consumer_name, {\n                'lag': lag,\n                'pending_events': pending_events,\n                'last_seen': last_seen,\n            }"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npublishes all mdstat metrics.", "response": "def collect(self):\n        \"\"\"Publish all mdstat metrics.\"\"\"\n        def traverse(d, metric_name=''):\n            \"\"\"\n            Traverse the given nested dict using depth-first search.\n\n            If a value is reached it will be published with a metric name\n            consisting of the hierarchically concatenated keys\n            of its branch.\n            \"\"\"\n            for key, value in d.iteritems():\n                if isinstance(value, dict):\n                    if metric_name == '':\n                        metric_name_next = key\n                    else:\n                        metric_name_next = metric_name + '.' + key\n                    traverse(value, metric_name_next)\n                else:\n                    metric_name_finished = metric_name + '.' + key\n                    self.publish_gauge(\n                        name=metric_name_finished,\n                        value=value,\n                        precision=1\n                    )\n\n        md_state = self._parse_mdstat()\n\n        traverse(md_state, '')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_mdstat(self):\n\n        arrays = {}\n        mdstat_array_blocks = ''\n\n        try:\n            with open(self.MDSTAT_PATH, 'r') as f:\n                lines = f.readlines()\n        except IOError as err:\n            self.log.exception(\n                'Error opening {mdstat_path} for reading: {err}'.format(\n                    mdstat_path=self.MDSTAT_PATH,\n                    err=err\n                )\n            )\n            return arrays\n\n        # concatenate all lines except the first and last one\n        for line in lines[1:-1]:\n            mdstat_array_blocks += line\n\n        if mdstat_array_blocks == '':\n            # no md arrays found\n            return arrays\n        for block in mdstat_array_blocks.split('\\n\\n'):\n            md_device_name = self._parse_device_name(block)\n            if md_device_name:\n                # this block begins with a md device name\n\n                # 'member_count' and 'status' are mandatory keys\n                arrays[md_device_name] = {\n                    'member_count': self._parse_array_member_state(block),\n                    'status': self._parse_array_status(block),\n                }\n\n                # 'bitmap' and 'recovery' are optional keys\n                bitmap_status = self._parse_array_bitmap(block)\n                recovery_status = self._parse_array_recovery(block)\n                if bitmap_status:\n                    arrays[md_device_name].update(\n                        {'bitmap': bitmap_status}\n                    )\n                if recovery_status:\n                    arrays[md_device_name].update(\n                        {'recovery': recovery_status}\n                    )\n\n        return arrays", "response": "Parse the mdstat file and return a dict of the keys."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_array_member_state(self, block):\n        members = block.split('\\n')[0].split(' : ')[1].split(' ')[2:]\n\n        device_regexp = re.compile(\n            '^(?P<member_name>.*)'\n            '\\[(?P<member_role_number>\\d*)\\]'\n            '\\(?(?P<member_state>[FS])?\\)?$'\n        )\n\n        ret = {\n            'active': 0,\n            'faulty': 0,\n            'spare': 0\n        }\n        for member in members:\n            member_dict = device_regexp.match(member).groupdict()\n\n            if member_dict['member_state'] == 'S':\n                ret['spare'] += 1\n            elif member_dict['member_state'] == 'F':\n                ret['faulty'] += 1\n            else:\n                ret['active'] += 1\n\n        return ret", "response": "Parse the state of the md array members."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the status of the md array.", "response": "def _parse_array_status(self, block):\n        \"\"\"\n        Parse the status of the md array.\n\n        >>> block = 'md0 : active raid1 sdd2[0] sdb2[2](S) sdc2[1]\\n'\n        >>>         '      100171776 blocks super 1.2 [2/2] [UU]\\n'\n        >>>         '      bitmap: 1/1 pages [4KB], 65536KB chunk\\n\\n'\n        >>> print _parse_array_status(block)\n        {\n            'total_members': '2',\n            'actual_members': '2',\n            'superblock_version': '1.2',\n            'blocks': '100171776'\n        }\n\n        :return: dictionary of status information\n        :rtype: dict\n        \"\"\"\n        array_status_regexp = re.compile(\n            '^ *(?P<blocks>\\d*) blocks '\n            '(?:super (?P<superblock_version>\\d\\.\\d) )?'\n            '(?:level (?P<raid_level>\\d), '\n            '(?P<chunk_size>\\d*)k chunk, '\n            'algorithm (?P<algorithm>\\d) )?'\n            '(?:\\[(?P<total_members>\\d*)/(?P<actual_members>\\d*)\\])?'\n            '(?:(?P<rounding_factor>\\d*)k rounding)?.*$'\n        )\n\n        array_status_dict = \\\n            array_status_regexp.match(block.split('\\n')[1]).groupdict()\n\n        array_status_dict_sanitizied = {}\n\n        # convert all non None values to float\n        for key, value in array_status_dict.iteritems():\n            if not value:\n                continue\n            if key == 'superblock_version':\n                array_status_dict_sanitizied[key] = float(value)\n            else:\n                array_status_dict_sanitizied[key] = int(value)\n\n        if 'chunk_size' in array_status_dict_sanitizied:\n            # convert chunk size from kBytes to Bytes\n            array_status_dict_sanitizied['chunk_size'] *= 1024\n\n        if 'rounding_factor' in array_status_dict_sanitizied:\n            # convert rounding_factor from kBytes to Bytes\n            array_status_dict_sanitizied['rounding_factor'] *= 1024\n\n        return array_status_dict_sanitizied"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the bitmap status of the md array.", "response": "def _parse_array_bitmap(self, block):\n        \"\"\"\n        Parse the bitmap status of the md array.\n\n        >>> block = 'md0 : active raid1 sdd2[0] sdb2[2](S) sdc2[1]\\n'\n        >>>         '      100171776 blocks super 1.2 [2/2] [UU]\\n'\n        >>>         '      bitmap: 1/1 pages [4KB], 65536KB chunk\\n\\n'\n        >>> print _parse_array_bitmap(block)\n        {\n            'total_pages': '1',\n            'allocated_pages': '1',\n            'page_size': 4096,\n            'chunk_size': 67108864\n        }\n\n        :return: dictionary of bitmap status information\n        :rtype: dict\n        \"\"\"\n        array_bitmap_regexp = re.compile(\n            '^ *bitmap: (?P<allocated_pages>[0-9]*)/'\n            '(?P<total_pages>[0-9]*) pages '\n            '\\[(?P<page_size>[0-9]*)KB\\], '\n            '(?P<chunk_size>[0-9]*)KB chunk.*$',\n            re.MULTILINE\n        )\n\n        regexp_res = array_bitmap_regexp.search(block)\n\n        # bitmap is optionally in mdstat\n        if not regexp_res:\n            return None\n\n        array_bitmap_dict = regexp_res.groupdict()\n        array_bitmap_dict_sanitizied = {}\n\n        # convert all values to int\n        for key, value in array_bitmap_dict.iteritems():\n                if not value:\n                    continue\n                array_bitmap_dict_sanitizied[key] = int(value)\n\n        # convert page_size to bytes\n        array_bitmap_dict_sanitizied['page_size'] *= 1024\n\n        # convert chunk_size to bytes\n        array_bitmap_dict_sanitizied['chunk_size'] *= 1024\n\n        return array_bitmap_dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the recovery progress of the md array.", "response": "def _parse_array_recovery(self, block):\n        \"\"\"\n        Parse the recovery progress of the md array.\n\n        >>> block = 'md0 : active raid1 sdd2[0] sdb2[2](S) sdc2[1]\\n'\n        >>>         '      100171776 blocks super 1.2 [2/2] [UU]\\n'\n        >>>         '      [===================>.]  recovery = 99.5% '\n        >>>         '(102272/102272) finish=13.37min speed=102272K/sec\\n'\n        >>>         '\\n'\n        >>> print _parse_array_recovery(block)\n        {\n            'percent': '99.5',\n            'speed': 104726528,\n            'remaining_time': 802199\n        }\n\n        :return: dictionary of recovery progress status information\n        :rtype: dict\n        \"\"\"\n        array_recovery_regexp = re.compile(\n            '^ *\\[.*\\] *recovery = (?P<percent>\\d*\\.?\\d*)%'\n            ' \\(\\d*/\\d*\\) finish=(?P<remaining_time>\\d*\\.?\\d*)min '\n            'speed=(?P<speed>\\d*)K/sec$',\n            re.MULTILINE\n        )\n\n        regexp_res = array_recovery_regexp.search(block)\n\n        # recovery is optionally in mdstat\n        if not regexp_res:\n            return None\n\n        array_recovery_dict = regexp_res.groupdict()\n\n        array_recovery_dict['percent'] = \\\n            float(array_recovery_dict['percent'])\n\n        # convert speed to bits\n        array_recovery_dict['speed'] = \\\n            int(array_recovery_dict['speed']) * 1024\n\n        # convert minutes to milliseconds\n        array_recovery_dict['remaining_time'] = \\\n            int(float(array_recovery_dict['remaining_time'])*60*1000)\n\n        return array_recovery_dict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the help text for the configuration options for this handler.", "response": "def get_default_config_help(self):\n        \"\"\"\n        Returns the help text for the configuration options for this handler\n        \"\"\"\n        config = super(MultiGraphitePickleHandler,\n                       self).get_default_config_help()\n\n        config.update({\n            'host': 'Hostname, Hostname, Hostname',\n            'port': 'Port',\n            'proto': 'udp or tcp',\n            'timeout': '',\n            'batch': 'How many to store before sending to the graphite server',\n            'max_backlog_multiplier': 'how many batches to store before trimming',  # NOQA\n            'trim_backlog_multiplier': 'Trim down how many batches',\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the default config for the handler", "response": "def get_default_config(self):\n        \"\"\"\n        Return the default config for the handler\n        \"\"\"\n        config = super(MultiGraphitePickleHandler, self).get_default_config()\n\n        config.update({\n            'host': ['localhost'],\n            'port': 2003,\n            'proto': 'tcp',\n            'timeout': 15,\n            'batch': 1,\n            'max_backlog_multiplier': 5,\n            'trim_backlog_multiplier': 4,\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(MemoryCollector, self).get_default_config()\n        config.update({\n            'path': 'memory',\n            'method': 'Threaded',\n            'force_psutil': 'False'\n            # Collect all the nodes or just a few standard ones?\n            # Uncomment to enable\n            # 'detailed': 'True'\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncollects memory stats from the file and publish them to the appropriate events.", "response": "def collect(self):\n        \"\"\"\n        Collect memory stats\n        \"\"\"\n        if ((os.access(self.PROC, os.R_OK) and\n             self.config.get('force_psutil') != 'True')):\n            file = open(self.PROC)\n            data = file.read()\n            file.close()\n\n            memory_total = None\n            memory_available = None\n            for line in data.splitlines():\n                try:\n                    name, value, units = line.split()\n                    name = name.rstrip(':')\n                    value = int(value)\n\n                    if ((name not in _KEY_MAPPING and\n                         'detailed' not in self.config)):\n                        continue\n\n                    if name in 'MemTotal':\n                        memory_total = value\n                    elif name in 'MemAvailable':\n                        memory_available = value\n\n                    for unit in self.config['byte_unit']:\n                        value = diamond.convertor.binary.convert(value=value,\n                                                                 oldUnit=units,\n                                                                 newUnit=unit)\n                        self.publish(name, value, metric_type='GAUGE')\n\n                        # TODO: We only support one unit node here. Fix it!\n                        break\n\n                except ValueError:\n                    continue\n\n            if memory_total is not None and memory_available is not None:\n                memory_used = memory_total - memory_available\n                memory_used_percent = Decimal(str(100.0 *\n                                              memory_used /\n                                              memory_total))\n                self.publish('MemUsedPercentage',\n                             round(memory_used_percent, 2),\n                             metric_type='GAUGE')\n            return True\n        else:\n            if not psutil:\n                self.log.error('Unable to import psutil')\n                self.log.error('No memory metrics retrieved')\n                return None\n\n            # psutil.phymem_usage() and psutil.virtmem_usage() are deprecated.\n            if hasattr(psutil, \"phymem_usage\"):\n                phymem_usage = psutil.phymem_usage()\n                virtmem_usage = psutil.virtmem_usage()\n            else:\n                phymem_usage = psutil.virtual_memory()\n                virtmem_usage = psutil.swap_memory()\n\n            units = 'B'\n\n            for unit in self.config['byte_unit']:\n                memory_total = value = diamond.convertor.binary.convert(\n                    value=phymem_usage.total, oldUnit=units, newUnit=unit)\n                self.publish('MemTotal', value, metric_type='GAUGE')\n\n                memory_available = value = diamond.convertor.binary.convert(\n                    value=phymem_usage.available, oldUnit=units, newUnit=unit)\n                self.publish('MemAvailable', value, metric_type='GAUGE')\n\n                memory_used = memory_total - memory_available\n\n                memory_used_percent = Decimal(str(100.0 *\n                                              memory_used /\n                                              memory_total))\n                self.publish('MemUsedPercentage',\n                             round(memory_used_percent, 2),\n                             metric_type='GAUGE')\n\n                value = diamond.convertor.binary.convert(\n                    value=phymem_usage.free, oldUnit=units, newUnit=unit)\n                self.publish('MemFree', value, metric_type='GAUGE')\n\n                value = diamond.convertor.binary.convert(\n                    value=virtmem_usage.total, oldUnit=units, newUnit=unit)\n                self.publish('SwapTotal', value, metric_type='GAUGE')\n\n                value = diamond.convertor.binary.convert(\n                    value=virtmem_usage.free, oldUnit=units, newUnit=unit)\n                self.publish('SwapFree', value, metric_type='GAUGE')\n\n                # TODO: We only support one unit node here. Fix it!\n                break\n\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(VarnishCollector, self).get_default_config()\n        config.update({\n            'path':             'varnish',\n            'bin':             '/usr/bin/varnishstat',\n            'use_sudo':         False,\n            'sudo_cmd':         '/usr/bin/sudo',\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning help text for the passenger collector", "response": "def get_default_config_help(self):\n        \"\"\"\n        Return help text\n        \"\"\"\n        config_help = super(PassengerCollector, self).get_default_config_help()\n        config_help.update({\n            \"bin\":         \"The path to the binary\",\n            \"use_sudo\":    \"Use sudo?\",\n            \"sudo_cmd\":    \"Path to sudo\",\n            \"passenger_status_bin\":\n                           \"The path to the binary passenger-status\",\n            \"passenger_memory_stats_bin\":\n                           \"The path to the binary passenger-memory-stats\",\n        })\n        return config_help"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(PassengerCollector, self).get_default_config()\n        config.update({\n            \"path\":         \"passenger_stats\",\n            \"bin\":          \"/usr/lib/ruby-flo/bin/passenger-memory-stats\",\n            \"use_sudo\":     False,\n            \"sudo_cmd\":     \"/usr/bin/sudo\",\n            \"passenger_status_bin\": \"/usr/bin/passenger-status\",\n            \"passenger_memory_stats_bin\": \"/usr/bin/passenger-memory-stats\",\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting passenger - memory - stats and parse its output return dictionary with dict with keys apache_procs nginx_procs and passenger_mem_total", "response": "def get_passenger_memory_stats(self):\n        \"\"\"\n        Execute passenger-memory-stats, parse its output, return dictionary with\n        stats.\n        \"\"\"\n        command = [self.config[\"passenger_memory_stats_bin\"]]\n        if str_to_bool(self.config[\"use_sudo\"]):\n            command.insert(0, self.config[\"sudo_cmd\"])\n\n        try:\n            proc1 = subprocess.Popen(command, stdout=subprocess.PIPE)\n            (std_out, std_err) = proc1.communicate()\n        except OSError:\n            return {}\n\n        if std_out is None:\n            return {}\n\n        dict_stats = {\n            \"apache_procs\": [],\n            \"nginx_procs\": [],\n            \"passenger_procs\": [],\n            \"apache_mem_total\": 0.0,\n            \"nginx_mem_total\": 0.0,\n            \"passenger_mem_total\": 0.0,\n        }\n        #\n        re_colour = re.compile(\"\\x1B\\[([0-9]{1,3}((;[0-9]{1,3})*)?)?[m|K]\")\n        re_digit = re.compile(\"^\\d\")\n        #\n        apache_flag = 0\n        nginx_flag = 0\n        passenger_flag = 0\n        for raw_line in std_out.splitlines():\n            line = re_colour.sub(\"\", raw_line)\n            if \"Apache processes\" in line:\n                apache_flag = 1\n            elif \"Nginx processes\" in line:\n                nginx_flag = 1\n            elif \"Passenger processes\" in line:\n                passenger_flag = 1\n            elif re_digit.match(line):\n                # If line starts with digit, then store PID and memory consumed\n                line_splitted = line.split()\n                if apache_flag == 1:\n                    dict_stats[\"apache_procs\"].append(line_splitted[0])\n                    dict_stats[\"apache_mem_total\"] += float(line_splitted[4])\n                elif nginx_flag == 1:\n                    dict_stats[\"nginx_procs\"].append(line_splitted[0])\n                    dict_stats[\"nginx_mem_total\"] += float(line_splitted[4])\n                elif passenger_flag == 1:\n                    dict_stats[\"passenger_procs\"].append(line_splitted[0])\n                    dict_stats[\"passenger_mem_total\"] += float(line_splitted[3])\n\n            elif \"Processes:\" in line:\n                passenger_flag = 0\n                apache_flag = 0\n                nginx_flag = 0\n\n        return dict_stats"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_passenger_cpu_usage(self, dict_stats):\n        try:\n            proc1 = subprocess.Popen(\n                [\"top\", \"-b\", \"-n\", \"2\"],\n                stdout=subprocess.PIPE)\n            (std_out, std_err) = proc1.communicate()\n        except OSError:\n            return (-1)\n\n        re_lspaces = re.compile(\"^\\s*\")\n        re_digit = re.compile(\"^\\d\")\n        overall_cpu = 0\n        for raw_line in std_out.splitlines():\n            line = re_lspaces.sub(\"\", raw_line)\n            if not re_digit.match(line):\n                continue\n\n            line_splitted = line.split()\n            if line_splitted[0] in dict_stats[\"apache_procs\"]:\n                overall_cpu += float(line_splitted[8])\n            elif line_splitted[0] in dict_stats[\"nginx_procs\"]:\n                overall_cpu += float(line_splitted[8])\n            elif line_splitted[0] in dict_stats[\"passenger_procs\"]:\n                overall_cpu += float(line_splitted[8])\n\n        return overall_cpu", "response": "Execute % top and return STDOUT."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexecute passenger - stats and parse its output return and requests in queue", "response": "def get_passenger_queue_stats(self):\n        \"\"\"\n        Execute passenger-stats, parse its output, returnand requests in queue\n        \"\"\"\n        queue_stats = {\n            \"top_level_queue_size\": 0.0,\n            \"passenger_queue_size\": 0.0,\n        }\n\n        command = [self.config[\"passenger_status_bin\"]]\n        if str_to_bool(self.config[\"use_sudo\"]):\n            command.insert(0, self.config[\"sudo_cmd\"])\n\n        try:\n            proc1 = subprocess.Popen(command, stdout=subprocess.PIPE)\n            (std_out, std_err) = proc1.communicate()\n\n        except OSError:\n            return {}\n\n        if std_out is None:\n            return {}\n\n        re_colour = re.compile(\"\\x1B\\[([0-9]{1,3}((;[0-9]{1,3})*)?)?[m|K]\")\n        re_requests = re.compile(r\"Requests\")\n        re_topqueue = re.compile(r\"^top-level\")\n\n        gen_info_flag = 0\n        app_groups_flag = 0\n        for raw_line in std_out.splitlines():\n            line = re_colour.sub(\"\", raw_line)\n            if \"General information\" in line:\n                gen_info_flag = 1\n            if \"Application groups\" in line:\n                app_groups_flag = 1\n            elif re_requests.match(line) and re_topqueue.search(line):\n                # If line starts with Requests and line has top-level queue then\n                # store queue size\n                line_splitted = line.split()\n                if gen_info_flag == 1 and line_splitted:\n                    queue_stats[\"top_level_queue_size\"] = float(\n                        line_splitted[5])\n            elif re_requests.search(line) and not re_topqueue.search(line):\n                # If line has Requests and nothing else special\n                line_splitted = line.split()\n                if app_groups_flag == 1 and line_splitted:\n                    queue_stats[\"passenger_queue_size\"] = float(\n                        line_splitted[3])\n\n        return queue_stats"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_default_config(self):\n        config = super(KafkaConsumerLagCollector, self).get_default_config()\n        config.update({\n            'path': 'kafka.ConsumerLag',\n            'bin': '/opt/kafka/bin/kafka-run-class.sh',\n            'zookeeper': 'localhost:2181'\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(HAProxyCollector, self).get_default_config()\n        config.update({\n            'method':           'http',\n            'path':             'haproxy',\n            'url':              'http://localhost/haproxy?stats;csv',\n            'user':             'admin',\n            'pass':             'password',\n            'sock':             '/var/run/haproxy.sock',\n            'ignore_servers':   False,\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the HTTP stats from the HAProxy server.", "response": "def http_get_csv_data(self, section=None):\n        \"\"\"\n        Request stats from HAProxy Server\n        \"\"\"\n        metrics = []\n        req = urllib2.Request(self._get_config_value(section, 'url'))\n        try:\n            handle = urllib2.urlopen(req)\n            return handle.readlines()\n        except Exception as e:\n            if not hasattr(e, 'code') or e.code != 401:\n                self.log.error(\"Error retrieving HAProxy stats. %s\", e)\n                return metrics\n\n        # get the www-authenticate line from the headers\n        # which has the authentication scheme and realm in it\n        authline = e.headers['www-authenticate']\n\n        # this regular expression is used to extract scheme and realm\n        authre = (r'''(?:\\s*www-authenticate\\s*:)?\\s*''' +\n                  '''(\\w*)\\s+realm=['\"]([^'\"]+)['\"]''')\n        authobj = re.compile(authre, re.IGNORECASE)\n        matchobj = authobj.match(authline)\n        if not matchobj:\n            # if the authline isn't matched by the regular expression\n            # then something is wrong\n            self.log.error('The authentication header is malformed.')\n            return metrics\n\n        scheme = matchobj.group(1)\n        # here we've extracted the scheme\n        # and the realm from the header\n        if scheme.lower() != 'basic':\n            self.log.error('Invalid authentication scheme.')\n            return metrics\n\n        base64string = base64.encodestring(\n            '%s:%s' % (self._get_config_value(section, 'user'),\n                       self._get_config_value(section, 'pass')))[:-1]\n        authheader = 'Basic %s' % base64string\n        req.add_header(\"Authorization\", authheader)\n        try:\n            handle = urllib2.urlopen(req)\n            metrics = handle.readlines()\n            return metrics\n        except IOError as e:\n            # here we shouldn't fail if the USER/PASS is right\n            self.log.error(\"Error retrieving HAProxy stats. \" +\n                           \"(Invalid username or password?) %s\", e)\n            return metrics"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef match_process(pid, name, cmdline, exe, cfg):\n    if cfg['selfmon'] and pid == os.getpid():\n        return True\n    for exe_re in cfg['exe']:\n        if exe_re.search(exe):\n            return True\n    for name_re in cfg['name']:\n        if name_re.search(name):\n            return True\n    for cmdline_re in cfg['cmdline']:\n        if cmdline_re.search(' '.join(cmdline)):\n            return True\n    return False", "response": "Determines if a process matches with a given process descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprepares self. processes and self. processes_info", "response": "def process_config(self):\n        super(ProcessResourcesCollector, self).process_config()\n        \"\"\"\n        prepare self.processes, which is a descriptor dictionary in\n        pg_name: {\n            exe: [regex],\n            name: [regex],\n            cmdline: [regex],\n            selfmon: [boolean],\n            procs: [psutil.Process],\n            count_workers: [boolean]\n        }\n        \"\"\"\n        self.processes = {}\n        self.processes_info = {}\n        for pg_name, cfg in self.config['process'].items():\n            pg_cfg = {}\n            for key in ('exe', 'name', 'cmdline'):\n                pg_cfg[key] = cfg.get(key, [])\n                if not isinstance(pg_cfg[key], list):\n                    pg_cfg[key] = [pg_cfg[key]]\n                pg_cfg[key] = [re.compile(e) for e in pg_cfg[key]]\n            pg_cfg['selfmon'] = cfg.get('selfmon', '').lower() == 'true'\n            pg_cfg['count_workers'] = cfg.get(\n                'count_workers', '').lower() == 'true'\n            self.processes[pg_name] = pg_cfg\n            self.processes_info[pg_name] = {}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_default_config(self):\n        config = super(ProcessResourcesCollector, self).get_default_config()\n        config.update({\n            'info_keys': ['num_ctx_switches', 'cpu_percent', 'cpu_times',\n                          'io_counters', 'num_threads', 'num_fds',\n                          'memory_percent', 'memory_info_ex', ],\n            'path': 'process',\n            'unit': 'B',\n            'process': {},\n        })\n        return config", "response": "Returns the default configuration for the process resources collector."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncollects resources usage of each process defined under the process subsection of the config file.", "response": "def collect(self):\n        \"\"\"\n        Collects resources usage of each process defined under the\n        `process` subsection of the config file\n        \"\"\"\n        if not psutil:\n            self.log.error('Unable to import psutil')\n            self.log.error('No process resource metrics retrieved')\n            return None\n\n        for process in psutil.process_iter():\n            self.collect_process_info(process)\n\n        # publish results\n        for pg_name, counters in self.processes_info.iteritems():\n            if counters:\n                metrics = (\n                    (\"%s.%s\" % (pg_name, key), value)\n                    for key, value in counters.iteritems())\n            else:\n                if self.processes[pg_name]['count_workers']:\n                    metrics = (('%s.workers_count' % pg_name, 0), )\n                else:\n                    metrics = ()\n\n            [self.publish(*metric) for metric in metrics]\n            # reinitialize process info\n            self.processes_info[pg_name] = {}"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a string and create a metric object.", "response": "def parse(cls, string):\n        \"\"\"\n        Parse a string and create a metric\n        \"\"\"\n        match = re.match(r'^(?P<name>[A-Za-z0-9\\.\\-_]+)\\s+' +\n                         '(?P<value>[0-9\\.]+)\\s+' +\n                         '(?P<timestamp>[0-9\\.]+)(\\n?)$',\n                         string)\n        try:\n            groups = match.groupdict()\n            # TODO: get precision from value string\n            return Metric(groups['name'],\n                          groups['value'],\n                          float(groups['timestamp']))\n        except:\n            raise DiamondException(\n                \"Metric could not be parsed from string: %s.\" % string)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the path prefix for the current resource.", "response": "def getPathPrefix(self):\n        \"\"\"\n            Returns the path prefix path\n            servers.host.cpu.total.idle\n            return \"servers\"\n        \"\"\"\n        # If we don't have a host name, assume it's just the first part of the\n        # metric path\n        if self.host is None:\n            return self.path.split('.')[0]\n\n        offset = self.path.index(self.host) - 1\n        return self.path[0:offset]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the collector path of the current server.", "response": "def getCollectorPath(self):\n        \"\"\"\n            Returns collector path\n            servers.host.cpu.total.idle\n            return \"cpu\"\n        \"\"\"\n        # If we don't have a host name, assume it's just the third part of the\n        # metric path\n        if self.host is None:\n            return self.path.split('.')[2]\n\n        offset = self.path.index(self.host)\n        offset += len(self.host) + 1\n        endoffset = self.path.index('.', offset)\n        return self.path[offset:endoffset]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getMetricPath(self):\n        # If we don't have a host name, assume it's just the fourth+ part of the\n        # metric path\n        if self.host is None:\n            path = self.path.split('.')[3:]\n            return '.'.join(path)\n\n        prefix = '.'.join([self.getPathPrefix(), self.host,\n                           self.getCollectorPath()])\n\n        offset = len(prefix) + 1\n        return self.path[offset:]", "response": "Returns the metric path after the collector name"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _throttle_error(self, msg, *args, **kwargs):\n        now = time.time()\n        if msg in self._errors:\n            if ((now - self._errors[msg]) >=\n                    self.server_error_interval):\n                fn = self.log.error\n                self._errors[msg] = now\n            else:\n                fn = self.log.debug\n        else:\n            self._errors[msg] = now\n            fn = self.log.error\n\n        return fn(msg, *args, **kwargs)", "response": "Throttles sending an error message to the error logging facility."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _reset_errors(self, msg=None):\n        if msg is not None and msg in self._errors:\n            del self._errors[msg]\n        else:\n            self._errors = {}", "response": "Resets the logging throttle cache so the next error is emitted."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nraises an exception if the signal is not handled by the system.", "response": "def signal_to_exception(signum, frame):\n    \"\"\"\n    Called by the timeout alarm during the collector run time\n    \"\"\"\n    if signum == signal.SIGALRM:\n        raise SIGALRMException()\n    if signum == signal.SIGHUP:\n        raise SIGHUPException()\n    if signum == signal.SIGUSR1:\n        raise SIGUSR1Exception()\n    if signum == signal.SIGUSR2:\n        raise SIGUSR2Exception()\n    raise SignalException(signum)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncollecting statistics from the current directory and return the statvers value.", "response": "def collect(self):\n        \"\"\"Collect statistics from /proc/self/mountstats.\n\n        Currently, we do fairly naive parsing and do not actually check\n        the statvers value returned by mountstats.\n        \"\"\"\n\n        if str_to_bool(self.config['use_sudo']):\n            if not os.access(self.config['sudo_cmd'], os.X_OK):\n                self.log.error(\"Cannot find or exec %s\"\n                               % self.config['sudo_cmd'])\n                return None\n\n            command = [self.config['sudo_cmd'], '/bin/cat', self.MOUNTSTATS]\n            p = subprocess.Popen(command,\n                                 stdout=subprocess.PIPE).communicate()[0][:-1]\n            lines = p.split(\"\\n\")\n\n        else:\n            if not os.access(self.MOUNTSTATS, os.R_OK):\n                self.log.error(\"Cannot read path %s\" % self.MOUNTSTATS)\n                return None\n\n            f = open(self.MOUNTSTATS)\n            lines = f.readlines()\n            f.close()\n\n        path = None\n        for line in lines:\n            tokens = line.split()\n            if len(tokens) == 0:\n                continue\n\n            if tokens[0] == 'device':\n                path = tokens[4]\n\n                skip = False\n                if self.exclude_reg:\n                    skip = self.exclude_reg.match(path)\n                if self.include_reg:\n                    skip = not self.include_reg.match(path)\n\n                if skip:\n                    self.log.debug(\"Ignoring %s\", path)\n                else:\n                    self.log.debug(\"Keeping %s\", path)\n\n                path = path.replace('.', '_')\n                path = path.replace('/', '_')\n            elif skip:\n                # If we are in a skip state, don't pay any attention to\n                # anything that isn't the next device line\n                continue\n            elif tokens[0] == 'events:':\n                for i in range(0, len(self.EVENTS_MAP)):\n                    metric_name = \"%s.events.%s\" % (path, self.EVENTS_MAP[i])\n                    metric_value = long(tokens[i + 1])\n                    self.publish_counter(metric_name, metric_value)\n            elif tokens[0] == 'bytes:':\n                for i in range(0, len(self.BYTES_MAP)):\n                    metric_name = \"%s.bytes.%s\" % (path, self.BYTES_MAP[i])\n                    metric_value = long(tokens[i + 1])\n                    self.publish_counter(metric_name, metric_value)\n            elif tokens[0] == 'xprt:':\n                proto = tokens[1]\n                if not self.XPRT_MAP[proto]:\n                    self.log.error(\"Unknown protocol %s\", proto)\n                    continue\n\n                for i in range(0, len(self.XPRT_MAP[proto])):\n                    metric_name = \"%s.xprt.%s.%s\" % (path, proto,\n                                                     self.XPRT_MAP[proto][i])\n                    metric_value = long(tokens[i + 2])\n                    self.publish_counter(metric_name, metric_value)\n            elif tokens[0][:-1] in self.RPCS_MAP:\n                rpc = tokens[0][:-1]\n                ops = long(tokens[1])\n                rtt = long(tokens[7])\n                exe = long(tokens[8])\n\n                metric_fmt = \"%s.rpc.%s.%s\"\n                ops_name = metric_fmt % (path, rpc.lower(), 'ops')\n                rtt_name = metric_fmt % (path, rpc.lower(), 'rtt')\n                exe_name = metric_fmt % (path, rpc.lower(), 'exe')\n\n                self.publish_counter(ops_name, ops)\n                self.publish_counter(rtt_name, rtt)\n                self.publish_counter(exe_name, exe)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(ServerTechPDUCollector, self).get_default_config()\n        config.update({\n            'path':     'pdu',\n            'timeout': 15,\n            'retries': 3,\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef collect_snmp(self, device, host, port, community):\n        # Log\n        self.log.info(\"Collecting ServerTech PDU statistics from: %s\" % device)\n\n        # Set timestamp\n        timestamp = time.time()\n\n        inputFeeds = {}\n\n        # Collect PDU input gauge values\n        for gaugeName, gaugeOid in self.PDU_SYSTEM_GAUGES.items():\n            systemGauges = self.walk(gaugeOid, host, port, community)\n            for o, gaugeValue in systemGauges.items():\n                # Get Metric Name\n                metricName = gaugeName\n                # Get Metric Value\n                metricValue = float(gaugeValue)\n                # Get Metric Path\n                metricPath = '.'.join(\n                    ['devices', device, 'system', metricName])\n                # Create Metric\n                metric = Metric(metricPath, metricValue, timestamp, 2)\n                # Publish Metric\n                self.publish_metric(metric)\n\n        # Collect PDU input feed names\n        inputFeedNames = self.walk(\n            self.PDU_INFEED_NAMES, host, port, community)\n        for o, inputFeedName in inputFeedNames.items():\n            # Extract input feed name\n            inputFeed = \".\".join(o.split(\".\")[-2:])\n            inputFeeds[inputFeed] = inputFeedName\n\n        # Collect PDU input gauge values\n        for gaugeName, gaugeOid in self.PDU_INFEED_GAUGES.items():\n            inputFeedGauges = self.walk(gaugeOid, host, port, community)\n            for o, gaugeValue in inputFeedGauges.items():\n                # Extract input feed name\n                inputFeed = \".\".join(o.split(\".\")[-2:])\n\n                # Get Metric Name\n                metricName = '.'.join([re.sub(r'\\.|\\\\', '_',\n                                              inputFeeds[inputFeed]),\n                                       gaugeName])\n\n                # Get Metric Value\n                if gaugeName == \"infeedVolts\":\n                    # Note: Voltage is in \"tenth volts\", so divide by 10\n                    metricValue = float(gaugeValue) / 10.0\n                elif gaugeName == \"infeedAmps\":\n                    # Note: Amps is in \"hundredth amps\", so divide by 100\n                    metricValue = float(gaugeValue) / 100.0\n                else:\n                    metricValue = float(gaugeValue)\n\n                # Get Metric Path\n                metricPath = '.'.join(['devices', device, 'input', metricName])\n                # Create Metric\n                metric = Metric(metricPath, metricValue, timestamp, 2)\n                # Publish Metric\n                self.publish_metric(metric)", "response": "Collect SNMP statistics from a serverTech device."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the default collector help text", "response": "def get_default_config_help(self):\n        \"\"\"\n        Returns the default collector help text\n        \"\"\"\n        config_help = super(UsersCollector, self).get_default_config_help()\n        config_help.update({\n        })\n        return config_help"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_default_config(self):\n        config = super(UsersCollector, self).get_default_config()\n        config.update({\n            'path':     'users',\n            'utmp':     None,\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(NtpCollector, self).get_default_config()\n        config.update({\n            'bin':      self.find_binary('/usr/sbin/ntpdate'),\n            'ntp_pool': 'pool.ntp.org',\n            'path':     'ntp',\n            'precision': 0,\n            'time_scale': 'milliseconds',\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(CpuAcctCgroupCollector, self).get_default_config()\n        config.update({\n            'path':     '/sys/fs/cgroup/cpuacct/'\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_default_config_help(self):\n        config = super(StatsdHandler, self).get_default_config_help()\n\n        config.update({\n            'host': '',\n            'port': '',\n            'batch': '',\n        })\n\n        return config", "response": "Returns the help text for the configuration options for this handler\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the default config for the handler", "response": "def get_default_config(self):\n        \"\"\"\n        Return the default config for the handler\n        \"\"\"\n        config = super(StatsdHandler, self).get_default_config()\n\n        config.update({\n            'host': '',\n            'port': 1234,\n            'batch': 1,\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _send(self):\n        if not statsd:\n            return\n        for metric in self.metrics:\n\n            # Split the path into a prefix and a name\n            # to work with the statsd module's view of the world.\n            # It will get re-joined by the python-statsd module.\n            #\n            # For the statsd module, you specify prefix in the constructor\n            # so we just use the full metric path.\n            (prefix, name) = metric.path.rsplit(\".\", 1)\n            logging.debug(\"Sending %s %s|g\", name, metric.value)\n\n            if metric.metric_type == 'GAUGE':\n                if hasattr(statsd, 'StatsClient'):\n                    self.connection.gauge(metric.path, metric.value)\n                else:\n                    statsd.Gauge(prefix, self.connection).send(\n                        name, metric.value)\n            else:\n                # To send a counter, we need to just send the delta\n                # but without any time delta changes\n                value = metric.raw_value\n                if metric.path in self.old_values:\n                    value = value - self.old_values[metric.path]\n                self.old_values[metric.path] = metric.raw_value\n\n                if hasattr(statsd, 'StatsClient'):\n                    self.connection.incr(metric.path, value)\n                else:\n                    statsd.Counter(prefix, self.connection).increment(\n                        name, value)\n\n        if hasattr(statsd, 'StatsClient'):\n            self.connection.send()\n        self.metrics = []", "response": "Send data to statsd."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconnecting to the statsd server and create a socket connection", "response": "def _connect(self):\n        \"\"\"\n        Connect to the statsd server\n        \"\"\"\n        if not statsd:\n            return\n\n        if hasattr(statsd, 'StatsClient'):\n            self.connection = statsd.StatsClient(\n                host=self.host,\n                port=self.port\n            ).pipeline()\n        else:\n            # Create socket\n            self.connection = statsd.Connection(\n                host=self.host,\n                port=self.port,\n                sample_rate=1.0\n            )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if the metric path matches the metric path False otherwise.", "response": "def _match_metric(self, metric):\n        \"\"\"\n        matches the metric path, if the metrics are empty, it shorts to True\n        \"\"\"\n        if len(self._compiled_filters) == 0:\n            return True\n        for (collector, filter_regex) in self._compiled_filters:\n            if collector != metric.getCollectorPath():\n                continue\n            if filter_regex.match(metric.getMetricPath()):\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_default_config_help(self):\n        config = super(SignalfxHandler, self).get_default_config_help()\n\n        config.update({\n            'url': 'Where to send metrics',\n            'batch': 'How many to store before sending',\n            'filter_metrics_regex': 'Comma separated collector:regex filters',\n            'auth_token': 'Org API token to use when sending metrics',\n        })\n\n        return config", "response": "Returns the help text for the configuration options for this handler\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_default_config(self):\n        config = super(SignalfxHandler, self).get_default_config()\n\n        config.update({\n            'url': 'https://ingest.signalfx.com/v2/datapoint',\n            'batch': 300,\n            'filter_metrics_regex': '',\n            # Don't wait more than 30 sec between pushes\n            'batch_max_interval': 30,\n            'auth_token': '',\n        })\n\n        return config", "response": "Return the default config for the handler\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nqueuing a metric. Flushing queue if batch size reached", "response": "def process(self, metric):\n        \"\"\"\n        Queue a metric.  Flushing queue if batch size reached\n        \"\"\"\n        if self._match_metric(metric):\n            self.metrics.append(metric)\n        if self.should_flush():\n            self._send()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef into_signalfx_point(self, metric):\n        dims = {\n            \"collector\": metric.getCollectorPath(),\n            \"prefix\": metric.getPathPrefix(),\n        }\n        if metric.host is not None and metric.host != \"\":\n            dims[\"host\"] = metric.host\n\n        return {\n            \"metric\": metric.getMetricPath(),\n            \"value\": metric.value,\n            \"dimensions\": dims,\n            # We expect ms timestamps\n            \"timestamp\": metric.timestamp * 1000,\n        }", "response": "Convert diamond metric into something signalfx can understand\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting a metric to the server.", "response": "def gmetric_write(NAME, VAL, TYPE, UNITS, SLOPE, TMAX, DMAX, GROUP):\n    \"\"\"\n    Arguments are in all upper-case to match XML\n    \"\"\"\n    packer = Packer()\n    HOSTNAME = \"test\"\n    SPOOF = 0\n    # Meta data about a metric\n    packer.pack_int(128)\n    packer.pack_string(HOSTNAME)\n    packer.pack_string(NAME)\n    packer.pack_int(SPOOF)\n    packer.pack_string(TYPE)\n    packer.pack_string(NAME)\n    packer.pack_string(UNITS)\n    packer.pack_int(slope_str2int[SLOPE])  # map slope string to int\n    packer.pack_uint(int(TMAX))\n    packer.pack_uint(int(DMAX))\n    # Magic number. Indicates number of entries to follow. Put in 1 for GROUP\n    if GROUP == \"\":\n        packer.pack_int(0)\n    else:\n        packer.pack_int(1)\n        packer.pack_string(\"GROUP\")\n        packer.pack_string(GROUP)\n\n    # Actual data sent in a separate packet\n    data = Packer()\n    data.pack_int(128 + 5)\n    data.pack_string(HOSTNAME)\n    data.pack_string(NAME)\n    data.pack_int(SPOOF)\n    data.pack_string(\"%s\")\n    data.pack_string(str(VAL))\n\n    return packer.get_buffer(),  data.get_buffer()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_default_config(self):\n        config = super(CelerymonCollector, self).get_default_config()\n        config.update({\n            'host':     'localhost',\n            'port':     '8989'\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef collect(self):\n\n        # Handle collection time intervals correctly\n        CollectTime = int(time.time())\n        time_delta = float(self.config['interval'])\n        if not self.LastCollectTime:\n            self.LastCollectTime = CollectTime - time_delta\n\n        host = self.config['host']\n        port = self.config['port']\n\n        celerymon_url = \"http://%s:%s/api/task/?since=%i\" % (\n            host, port, self.LastCollectTime)\n        response = urllib2.urlopen(celerymon_url)\n        body = response.read()\n        celery_data = json.loads(body)\n\n        results = dict()\n        total_messages = 0\n        for data in celery_data:\n            name = str(data[1]['name'])\n            if name not in results:\n                results[name] = dict()\n            state = str(data[1]['state'])\n            if state not in results[name]:\n                results[name][state] = 1\n            else:\n                results[name][state] += 1\n            total_messages += 1\n\n        # Publish Metric\n        self.publish('total_messages', total_messages)\n        for result in results:\n            for state in results[result]:\n                metric_value = results[result][state]\n                metric_name = \"%s.%s\" % (result, state)\n                self.publish(metric_name, metric_value)\n\n        self.LastCollectTime = CollectTime", "response": "Collect the available Celery modules and publish metrics."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning default configuration options.", "response": "def get_default_config(self):\n        \"\"\"\n        Returns default configuration options.\n        \"\"\"\n        config = super(DiskTemperatureCollector, self).get_default_config()\n        config.update({\n            'path': 'disktemp',\n            'bin': 'hddtemp',\n            'use_sudo': False,\n            'sudo_cmd': '/usr/bin/sudo',\n            'devices': '^disk[0-9]$|^sd[a-z]$|^hd[a-z]$'\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncollect and publish disk temperatures", "response": "def collect(self):\n        \"\"\"\n        Collect and publish disk temperatures\n        \"\"\"\n        instances = {}\n\n        # Support disks such as /dev/(sd.*)\n        for device in os.listdir('/dev/'):\n            instances.update(self.match_device(device, '/dev/'))\n\n        # Support disk by id such as /dev/disk/by-id/wwn-(.*)\n        for device_id in os.listdir('/dev/disk/by-id/'):\n            instances.update(self.match_device(device, '/dev/disk/by-id/'))\n\n        metrics = {}\n        for device, p in instances.items():\n            output = p.communicate()[0].strip()\n\n            try:\n                metrics[device + \".Temperature\"] = float(output)\n            except:\n                self.log.warn('Disk temperature retrieval failed on ' + device)\n\n        for metric in metrics.keys():\n            self.publish(metric, metrics[metric])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_default_config(self):\n        config = super(ElasticSearchCollector, self).get_default_config()\n        config.update({\n            'host':           '127.0.0.1',\n            'port':           9200,\n            'user':           '',\n            'password':       '',\n            'instances':      [],\n            'scheme':         'http',\n            'path':           'elasticsearch',\n            'stats':          ['jvm', 'thread_pool', 'indices'],\n            'logstash_mode': False,\n            'cluster':       False,\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute a ES API call. Convert response into JSON and assert its structure.", "response": "def _get(self, scheme, host, port, path, assert_key=None):\n        \"\"\"\n        Execute a ES API call. Convert response into JSON and\n        optionally assert its structure.\n        \"\"\"\n        url = '%s://%s:%i/%s' % (scheme, host, port, path)\n        try:\n            request = urllib2.Request(url)\n            if self.config['user'] and self.config['password']:\n                base64string = base64.standard_b64encode(\n                    '%s:%s' % (self.config['user'], self.config['password']))\n                request.add_header(\"Authorization\", \"Basic %s\" % base64string)\n            response = urllib2.urlopen(request)\n        except Exception as err:\n            self.log.error(\"%s: %s\" % (url, err))\n            return False\n\n        try:\n            doc = json.load(response)\n        except (TypeError, ValueError):\n            self.log.error(\"Unable to parse response from elasticsearch as a\" +\n                           \" json object\")\n            return False\n\n        if assert_key and assert_key not in doc:\n            self.log.error(\"Bad response from elasticsearch, expected key \"\n                           \"'%s' was missing for %s\" % (assert_key, url))\n            return False\n        return doc"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a metric to the list of metrics.", "response": "def _add_metric(self, metrics, metric_path, data, data_path):\n        \"\"\"If the path specified by data_path (a list) exists in data,\n        add to metrics.  Use when the data path may not be present\"\"\"\n        current_item = data\n        for path_element in data_path:\n            current_item = current_item.get(path_element)\n            if current_item is None:\n                return\n\n        self._set_or_sum_metric(metrics, metric_path, current_item)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets or sum the value of a metric.", "response": "def _set_or_sum_metric(self, metrics, metric_path, value):\n        \"\"\"If we already have a datapoint for this metric, lets add\n        the value. This is used when the logstash mode is enabled.\"\"\"\n        if metric_path in metrics:\n            metrics[metric_path] += value\n        else:\n            metrics[metric_path] = value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_default_config(self):\n        config = super(NagiosStatsCollector, self).get_default_config()\n        config.update({\n            'bin':              '/usr/sbin/nagios3stats',\n            'vars':            ['AVGACTHSTLAT',\n                                'AVGACTSVCLAT',\n                                'AVGACTHSTEXT',\n                                'AVGACTSVCEXT',\n                                'NUMHSTUP',\n                                'NUMHSTDOWN',\n                                'NUMHSTUNR',\n                                'NUMSVCOK',\n                                'NUMSVCWARN',\n                                'NUMSVCUNKN',\n                                'NUMSVCCRIT',\n                                'NUMHSTACTCHK5M',\n                                'NUMHSTPSVCHK5M',\n                                'NUMSVCACTCHK5M',\n                                'NUMSVCPSVCHK5M',\n                                'NUMACTHSTCHECKS5M',\n                                'NUMOACTHSTCHECKS5M',\n                                'NUMCACHEDHSTCHECKS5M',\n                                'NUMSACTHSTCHECKS5M',\n                                'NUMPARHSTCHECKS5M',\n                                'NUMSERHSTCHECKS5M',\n                                'NUMPSVHSTCHECKS5M',\n                                'NUMACTSVCCHECKS5M',\n                                'NUMOACTSVCCHECKS5M',\n                                'NUMCACHEDSVCCHECKS5M',\n                                'NUMSACTSVCCHECKS5M',\n                                'NUMPSVSVCCHECKS5M'],\n            'use_sudo':         True,\n            'sudo_cmd':         '/usr/bin/sudo',\n            'path':             'nagiosstats'\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(UnboundCollector, self).get_default_config()\n        config.update({\n            'path':         'unbound',\n            'bin':          self.find_binary('/usr/sbin/unbound-control'),\n            'histogram':    True,\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_default_config(self):\n        config = super(SlabInfoCollector, self).get_default_config()\n        config.update({\n            'path':     'slabinfo'\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef collect(self):\n        if not os.access(self.PROC, os.R_OK):\n            return False\n\n        # Open PROC file\n        file = open(self.PROC, 'r')\n\n        # Get data\n        for line in file:\n            if line.startswith('slabinfo'):\n                continue\n\n            if line.startswith('#'):\n                keys = line.split()[1:]\n                continue\n\n            data = line.split()\n\n            for key in ['<active_objs>', '<num_objs>', '<objsize>',\n                        '<objperslab>', '<pagesperslab>']:\n                i = keys.index(key)\n                metric_name = data[0] + '.' + key.replace(\n                    '<', '').replace('>', '')\n                metric_value = int(data[i])\n                self.publish(metric_name, metric_value)\n\n            for key in ['<limit>', '<batchcount>', '<sharedfactor>']:\n                i = keys.index(key)\n                metric_name = data[0] + '.tunables.' + key.replace(\n                    '<', '').replace('>', '')\n                metric_value = int(data[i])\n                self.publish(metric_name, metric_value)\n\n            for key in ['<active_slabs>', '<num_slabs>', '<sharedavail>']:\n                i = keys.index(key)\n                metric_name = data[0] + '.slabdata.' + key.replace(\n                    '<', '').replace('>', '')\n                metric_value = int(data[i])\n                self.publish(metric_name, metric_value)\n\n        # Close file\n        file.close()", "response": "Collect process stat data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_default_config(self):\n        config = super(MySQLCollector, self).get_default_config()\n        config.update({\n            'path':     'mysql',\n            # Connection settings\n            'hosts':    [],\n\n            # Which rows of 'SHOW GLOBAL STATUS' you would like to publish.\n            # http://dev.mysql.com/doc/refman/5.1/en/show-status.html\n            # Leave unset to publish all\n            # 'publish': '',\n\n            'slave':    False,\n            'master':   False,\n            'innodb':   False,\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_default_config(self):\n        config = super(PostqueueCollector, self).get_default_config()\n        config.update({\n            'path':             'postqueue',\n            'bin':              '/usr/bin/postqueue',\n            'use_sudo':         False,\n            'sudo_cmd':         '/usr/bin/sudo',\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_default_config(self):\n        config = super(RedisCollector, self).get_default_config()\n        config.update({\n            'host': self._DEFAULT_HOST,\n            'port': self._DEFAULT_PORT,\n            'timeout': self._DEFAULT_SOCK_TIMEOUT,\n            'db': self._DEFAULT_DB,\n            'auth': None,\n            'databases': self._DATABASE_COUNT,\n            'path': 'redis',\n            'instances': [],\n        })\n        return config", "response": "Returns default config dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a redis client for the configuration.", "response": "def _client(self, host, port, unix_socket, auth):\n        \"\"\"Return a redis client for the configuration.\n\n:param str host: redis host\n:param int port: redis port\n:rtype: redis.Redis\n\n        \"\"\"\n        db = int(self.config['db'])\n        timeout = int(self.config['timeout'])\n        try:\n            cli = redis.Redis(host=host, port=port,\n                              db=db, socket_timeout=timeout, password=auth,\n                              unix_socket_path=unix_socket)\n            cli.ping()\n            return cli\n        except Exception as ex:\n            self.log.error(\"RedisCollector: failed to connect to %s:%i. %s.\",\n                           unix_socket or host, port, ex)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _precision(self, value):\n        value = str(value)\n        decimal = value.rfind('.')\n        if decimal == -1:\n            return 0\n        return len(value) - decimal - 1", "response": "Return the precision of the number\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns info dict from specified Redis instance", "response": "def _get_info(self, host, port, unix_socket, auth):\n        \"\"\"Return info dict from specified Redis instance\n\n:param str host: redis host\n:param int port: redis port\n:rtype: dict\n\n        \"\"\"\n\n        client = self._client(host, port, unix_socket, auth)\n        if client is None:\n            return None\n\n        info = client.info()\n        del client\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting config string from specified Redis instance and config key", "response": "def _get_config(self, host, port, unix_socket, auth, config_key):\n        \"\"\"Return config string from specified Redis instance and config key\n\n:param str host: redis host\n:param int port: redis port\n:param str host: redis config_key\n:rtype: str\n\n        \"\"\"\n\n        client = self._client(host, port, unix_socket, auth)\n        if client is None:\n            return None\n\n        config_value = client.config_get(config_key)\n        del client\n        return config_value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncollect metrics from a single Redis instance.", "response": "def collect_instance(self, nick, host, port, unix_socket, auth):\n        \"\"\"Collect metrics from a single Redis instance\n\n:param str nick: nickname of redis instance\n:param str host: redis host\n:param int port: redis port\n:param str unix_socket: unix socket, if applicable\n:param str auth: authentication password\n\n        \"\"\"\n\n        # Connect to redis and get the info\n        info = self._get_info(host, port, unix_socket, auth)\n        if info is None:\n            return\n\n        # The structure should include the port for multiple instances per\n        # server\n        data = dict()\n\n        # Role needs to be handled outside the the _KEYS dict\n        # since the value is a string, not a int / float\n        # Also, master_sync_in_progress is only available if the\n        # redis instance is a slave, so default it here so that\n        # the metric is cleared if the instance flips from slave\n        # to master\n        if 'role' in info:\n            if info['role'] == \"master\":\n                data['replication.master'] = 1\n                data['replication.master_sync_in_progress'] = 0\n            else:\n                data['replication.master'] = 0\n\n        # Connect to redis and get the maxmemory config value\n        # Then calculate the % maxmemory of memory used\n        maxmemory_config = self._get_config(host, port, unix_socket, auth,\n                                            'maxmemory')\n        if maxmemory_config and 'maxmemory' in maxmemory_config.keys():\n            maxmemory = float(maxmemory_config['maxmemory'])\n\n            # Only report % used if maxmemory is a non zero value\n            if maxmemory == 0:\n                maxmemory_percent = 0.0\n            else:\n                maxmemory_percent = info['used_memory'] / maxmemory * 100\n                maxmemory_percent = round(maxmemory_percent, 2)\n            data['memory.used_percent'] = float(\"%.2f\" % maxmemory_percent)\n\n        # Iterate over the top level keys\n        for key in self._KEYS:\n            if self._KEYS[key] in info:\n                data[key] = info[self._KEYS[key]]\n\n        # Iterate over renamed keys for 2.6 support\n        for key in self._RENAMED_KEYS:\n            if self._RENAMED_KEYS[key] in info:\n                data[key] = info[self._RENAMED_KEYS[key]]\n\n        # Look for databaase speific stats\n        for dbnum in range(0, int(self.config.get('databases',\n                                                  self._DATABASE_COUNT))):\n            db = 'db%i' % dbnum\n            if db in info:\n                for key in info[db]:\n                    data['%s.%s' % (db, key)] = info[db][key]\n\n        # Time since last save\n        for key in ['last_save_time', 'rdb_last_save_time']:\n            if key in info:\n                data['last_save.time_since'] = int(time.time()) - info[key]\n\n        # Publish the data to graphite\n        for key in data:\n            self.publish(self._publish_key(nick, key),\n                         data[key],\n                         precision=self._precision(data[key]),\n                         metric_type='GAUGE')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef collect(self):\n        if redis is None:\n            self.log.error('Unable to import module redis')\n            return {}\n\n        for nick in self.instances.keys():\n            (host, port, unix_socket, auth) = self.instances[nick]\n            self.collect_instance(nick, host, int(port), unix_socket, auth)", "response": "Collect the stats from the redis instance and publish them."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns default collector settings.", "response": "def get_default_config(self):\n        \"\"\"\n        Returns default collector settings.\n        \"\"\"\n        config = super(FilesCollector, self).get_default_config()\n        config.update({\n            'path': '.',\n            'dir': '/tmp/diamond',\n            'delete': False,\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the default configuration for the XFS collector", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the xfs collector settings\n        \"\"\"\n        config = super(XFSCollector, self).get_default_config()\n        config.update({\n            'path': 'xfs'\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncollects the stats for the current extended object.", "response": "def collect(self):\n        \"\"\"\n        Collect xfs stats.\n\n        For an explanation of the following metrics visit\n        http://xfs.org/index.php/Runtime_Stats\n        https://github.com/torvalds/linux/blob/master/fs/xfs/xfs_stats.h\n        \"\"\"\n        data_structure = {\n            'extent_alloc': (\n                'alloc_extent',\n                'alloc_block',\n                'free_extent',\n                'free_block'\n            ),\n            'abt': (\n                'lookup',\n                'compare',\n                'insrec',\n                'delrec'\n            ),\n            'blk_map': (\n                'read_ops',\n                'write_ops',\n                'unmap',\n                'add_exlist',\n                'del_exlist',\n                'look_exlist',\n                'cmp_exlist'\n            ),\n            'bmbt': (\n                'lookup',\n                'compare',\n                'insrec',\n                'delrec'\n            ),\n            'dir': (\n                'lookup',\n                'create',\n                'remove',\n                'getdents'\n            ),\n            'trans': (\n                'sync',\n                'async',\n                'empty'\n            ),\n            'ig': (\n                'ig_attempts',\n                'ig_found',\n                'ig_frecycle',\n                'ig_missed',\n                'ig_dup',\n                'ig_reclaims',\n                'ig_attrchg'\n            ),\n            'log': (\n                'writes',\n                'blocks',\n                'noiclogs',\n                'force',\n                'force_sleep'\n            ),\n            'push_ail': (\n                'try_logspace',\n                'sleep_logspace',\n                'pushes',\n                'success',\n                'pushbuf',\n                'pinned',\n                'locked',\n                'flushing',\n                'restarts',\n                'flush'\n            ),\n            'xstrat': (\n                'quick',\n                'split'\n            ),\n            'rw': (\n                'write_calls',\n                'read_calls'\n            ),\n            'attr': (\n                'get',\n                'set',\n                'remove',\n                'list'\n            ),\n            'icluster': (\n                'iflush_count',\n                'icluster_flushcnt',\n                'icluster_flushinode'\n            ),\n            'vnodes': (\n                'vn_active',\n                'vn_alloc',\n                'vn_get',\n                'vn_hold',\n                'vn_rele',\n                'vn_reclaim',\n                'vn_remove',\n                'vn_free'\n            ),\n            'buf': (\n                'xb_get',\n                'xb_create',\n                'xb_get_locked',\n                'xb_get_locked_waited',\n                'xb_busy_locked',\n                'xb_miss_locked',\n                'xb_page_retries',\n                'xb_page_found',\n                'xb_get_read'\n            ),\n            'abtb2': (\n                'xs_abtb_2_lookup',\n                'xs_abtb_2_compare',\n                'xs_abtb_2_insrec',\n                'xs_abtb_2_delrec',\n                'xs_abtb_2_newroot',\n                'xs_abtb_2_killroot',\n                'xs_abtb_2_increment',\n                'xs_abtb_2_decrement',\n                'xs_abtb_2_lshift',\n                'xs_abtb_2_rshift',\n                'xs_abtb_2_split',\n                'xs_abtb_2_join',\n                'xs_abtb_2_alloc',\n                'xs_abtb_2_free',\n                'xs_abtb_2_moves'\n            ),\n            'abtc2': (\n                'xs_abtc_2_lookup',\n                'xs_abtc_2_compare',\n                'xs_abtc_2_insrec',\n                'xs_abtc_2_delrec',\n                'xs_abtc_2_newroot',\n                'xs_abtc_2_killroot',\n                'xs_abtc_2_increment',\n                'xs_abtc_2_decrement',\n                'xs_abtc_2_lshift',\n                'xs_abtc_2_rshift',\n                'xs_abtc_2_split',\n                'xs_abtc_2_join',\n                'xs_abtc_2_alloc',\n                'xs_abtc_2_free',\n                'xs_abtc_2_moves'\n            ),\n            'bmbt2': (\n                'xs_bmbt_2_lookup',\n                'xs_bmbt_2_compare',\n                'xs_bmbt_2_insrec',\n                'xs_bmbt_2_delrec',\n                'xs_bmbt_2_newroot',\n                'xs_bmbt_2_killroot',\n                'xs_bmbt_2_increment',\n                'xs_bmbt_2_decrement',\n                'xs_bmbt_2_lshift',\n                'xs_bmbt_2_rshift',\n                'xs_bmbt_2_split',\n                'xs_bmbt_2_join',\n                'xs_bmbt_2_alloc',\n                'xs_bmbt_2_free',\n                'xs_bmbt_2_moves'\n            ),\n            'ibt2': (\n                'lookup',\n                'compare',\n                'insrec',\n                'delrec',\n                'newroot',\n                'killroot',\n                'increment',\n                'decrement',\n                'lshift',\n                'rshift',\n                'split',\n                'join',\n                'alloc',\n                'free',\n                'moves'\n            ),\n            'fibt2': (\n                'lookup',\n                'compare',\n                'insrec',\n                'delrec',\n                'newroot',\n                'killroot',\n                'increment',\n                'decrement',\n                'lshift',\n                'rshift',\n                'split',\n                'join',\n                'alloc',\n                'free',\n                'moves'\n            ),\n            'qm': (\n                'xs_qm_dquot',\n                'xs_qm_dquot_unused'\n            ),\n\n            'xpc': (\n                'xs_xstrat_bytes',\n                'xs_write_bytes',\n                'xs_read_bytes'\n            ),\n            'debug': (\n                'debug',\n            )\n        }\n\n        f = open(self.PROC)\n        new_stats = f.readlines()\n        f.close()\n\n        stats = {}\n        for line in new_stats:\n            items = line.rstrip().split()\n            stats[items[0]] = [int(a) for a in items[1:]]\n\n        for key in stats.keys():\n            for item in enumerate(data_structure[key]):\n                metric_name = '.'.join([key, item[1]])\n                value = stats[key][item[0]]\n                self.publish_counter(metric_name, value)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(EntropyStatCollector, self).get_default_config()\n        config.update({\n            'path':     'entropy'\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_default_config_help(self):\n        config = super(zmqHandler, self).get_default_config_help()\n\n        config.update({\n            'port': '',\n        })\n\n        return config", "response": "Returns the help text for the configuration options for this handler\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the default config for the handler", "response": "def get_default_config(self):\n        \"\"\"\n        Return the default config for the handler\n        \"\"\"\n        config = super(zmqHandler, self).get_default_config()\n\n        config.update({\n            'port': 1234,\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate PUB socket and bind it to the local port", "response": "def _bind(self):\n        \"\"\"\n           Create PUB socket and bind\n        \"\"\"\n        if not zmq:\n            return\n        self.context = zmq.Context()\n        self.socket = self.context.socket(zmq.PUB)\n        self.socket.bind(\"tcp://*:%i\" % self.port)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses a single entry in the queue.", "response": "def _process(self, metric):\n        \"\"\"\n        We skip any locking code due to the fact that this is now a single\n        process per collector\n        \"\"\"\n        try:\n            self.queue.put(metric, block=False)\n        except Queue.Full:\n            self._throttle_error('Queue full, check handlers for delays')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nflush the queue to the queue if the queue is full.", "response": "def _flush(self):\n        \"\"\"\n        We skip any locking code due to the fact that this is now a single\n        process per collector\n        \"\"\"\n        # Send a None down the queue to indicate a flush\n        try:\n            self.queue.put(None, block=False)\n        except Queue.Full:\n            self._throttle_error('Queue full, check handlers for delays')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the help text for the configuration options for this handler.", "response": "def get_default_config_help(self):\n        \"\"\"\n        Returns the help text for the configuration options for this handler\n        \"\"\"\n        config = super(cloudwatchHandler, self).get_default_config_help()\n\n        config.update({\n            'region': 'AWS region',\n            'metric': 'Diamond metric name',\n            'namespace': 'CloudWatch metric namespace',\n            'name': 'CloudWatch metric name',\n            'unit': 'CloudWatch metric unit',\n            'collector': 'Diamond collector name',\n            'collect_by_instance': 'Collect metrics for instances separately',\n            'collect_without_dimension': 'Collect metrics without dimension'\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_default_config(self):\n        config = super(cloudwatchHandler, self).get_default_config()\n\n        config.update({\n            'region': 'us-east-1',\n            'collector': 'loadavg',\n            'metric': '01',\n            'namespace': 'MachineLoad',\n            'name': 'Avg01',\n            'unit': 'None',\n            'collect_by_instance': True,\n            'collect_without_dimension': False\n        })\n\n        return config", "response": "Returns the default config for the handler\nAttributeNames"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing a metric and send it to CloudWatch", "response": "def process(self, metric):\n        \"\"\"\n          Process a metric and send it to CloudWatch\n        \"\"\"\n        if not boto:\n            return\n\n        collector = str(metric.getCollectorPath())\n        metricname = str(metric.getMetricPath())\n\n        # Send the data as ......\n\n        for rule in self.rules:\n            self.log.debug(\n                \"Comparing Collector: [%s] with (%s) \"\n                \"and Metric: [%s] with (%s)\",\n                str(rule['collector']),\n                collector,\n                str(rule['metric']),\n                metricname\n            )\n\n            if ((str(rule['collector']) == collector and\n                 str(rule['metric']) == metricname)):\n\n                if rule['collect_by_instance'] and self.instance_id:\n                    self.send_metrics_to_cloudwatch(\n                        rule,\n                        metric,\n                        {'InstanceId': self.instance_id})\n\n                if rule['collect_without_dimension']:\n                    self.send_metrics_to_cloudwatch(\n                        rule,\n                        metric,\n                        {})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending metrics to CloudWatch for the given rule and dimensions.", "response": "def send_metrics_to_cloudwatch(self, rule, metric, dimensions):\n        \"\"\"\n          Send metrics to CloudWatch for the given dimensions\n        \"\"\"\n\n        timestamp = datetime.datetime.utcfromtimestamp(metric.timestamp)\n\n        self.log.debug(\n            \"CloudWatch: Attempting to publish metric: %s to %s \"\n            \"with value (%s) for dimensions %s @%s\",\n            rule['name'],\n            rule['namespace'],\n            str(metric.value),\n            str(dimensions),\n            str(metric.timestamp)\n        )\n\n        try:\n            self.connection.put_metric_data(\n                str(rule['namespace']),\n                str(rule['name']),\n                str(metric.value),\n                timestamp, str(rule['unit']),\n                dimensions)\n            self.log.debug(\n                \"CloudWatch: Successfully published metric: %s to\"\n                \" %s with value (%s) for dimensions %s\",\n                rule['name'],\n                rule['namespace'],\n                str(metric.value),\n                str(dimensions))\n        except AttributeError as e:\n            self.log.error(\n                \"CloudWatch: Failed publishing - %s \", str(e))\n        except Exception as e:  # Rough connection re-try logic.\n            self.log.error(\n                \"CloudWatch: Failed publishing - %s\\n%s \",\n                str(e),\n                str(sys.exc_info()[0]))\n            self._bind()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(TokuMXCollector, self).get_default_config()\n        config.update({\n            'path':      'mongo',\n            'hosts':     ['localhost'],\n            'user':      None,\n            'passwd':      None,\n            'databases': '.*',\n            'ignore_collections': '^tmp\\.mr\\.',\n            'network_timeout': None,\n            'simple': 'False',\n            'translate_collections': 'False'\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef collect(self):\n\n        if pymongo is None:\n            self.log.error('Unable to import pymongo')\n            return\n\n        # we need this for backwards compatibility\n        if 'host' in self.config:\n            self.config['hosts'] = [self.config['host']]\n\n        # convert network_timeout to integer\n        if self.config['network_timeout']:\n            self.config['network_timeout'] = int(\n                self.config['network_timeout'])\n\n        # use auth if given\n        if 'user' in self.config:\n            user = self.config['user']\n        else:\n            user = None\n\n        if 'passwd' in self.config:\n            passwd = self.config['passwd']\n        else:\n            passwd = None\n\n        for host in self.config['hosts']:\n            if len(self.config['hosts']) == 1:\n                # one host only, no need to have a prefix\n                base_prefix = []\n            else:\n                matches = re.search('((.+)\\@)?(.+)?', host)\n                alias = matches.group(2)\n                host = matches.group(3)\n\n                if alias is None:\n                    base_prefix = [re.sub('[:\\.]', '_', host)]\n                else:\n                    base_prefix = [alias]\n\n            try:\n                if ReadPreference is None:\n                    conn = pymongo.Connection(\n                        host,\n                        network_timeout=self.config['network_timeout'],\n                        slave_okay=True\n                    )\n                else:\n                    conn = pymongo.Connection(\n                        host,\n                        network_timeout=self.config['network_timeout'],\n                        read_preference=ReadPreference.SECONDARY,\n                    )\n            except Exception as e:\n                self.log.error('Couldnt connect to mongodb: %s', e)\n                continue\n\n            # try auth\n            if user:\n                try:\n                    conn.admin.authenticate(user, passwd)\n                except Exception as e:\n                    self.log.error(\n                        'User auth given, but could not autheticate' +\n                        ' with host: %s, err: %s' % (host, e))\n                    return{}\n\n            serverStatus = conn.db.command('serverStatus')\n            engineStatus = conn.db.command('engineStatus')\n            data = dict(serverStatus.items() + engineStatus.items())\n\n            self._publish_transformed(data, base_prefix)\n            if str_to_bool(self.config['simple']):\n                data = self._extract_simple_data(data)\n\n            self._publish_dict_with_prefix(data, base_prefix)\n            db_name_filter = re.compile(self.config['databases'])\n            ignored_collections = re.compile(self.config['ignore_collections'])\n            for db_name in conn.database_names():\n                if not db_name_filter.search(db_name):\n                    continue\n                db_stats = conn[db_name].command('dbStats')\n                db_prefix = base_prefix + ['databases', db_name]\n                self._publish_dict_with_prefix(db_stats, db_prefix)\n                for collection_name in conn[db_name].collection_names():\n                    if ignored_collections.search(collection_name):\n                        continue\n                    collection_stats = conn[db_name].command('collstats',\n                                                             collection_name)\n                    if str_to_bool(self.config['translate_collections']):\n                        collection_name = collection_name.replace('.', '_')\n                    collection_prefix = db_prefix + [collection_name]\n                    self._publish_dict_with_prefix(collection_stats,\n                                                   collection_prefix)", "response": "Collect number values from db. serverStatus and db. engineStatus"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _publish_transformed(self, data, base_prefix):\n        self._publish_dict_with_prefix(data.get('opcounters', {}),\n                                       base_prefix + ['opcounters_per_sec'],\n                                       self.publish_counter)\n        self._publish_dict_with_prefix(data.get('opcountersRepl', {}),\n                                       base_prefix +\n                                       ['opcountersRepl_per_sec'],\n                                       self.publish_counter)\n        self._publish_dict_with_prefix(data.get('network', {}),\n                                       base_prefix + ['network_per_sec'],\n                                       self.publish_counter)\n        self._publish_metrics(base_prefix + ['extra_info_per_sec'],\n                              'page_faults',\n                              data.get('extra_info', {}),\n                              self.publish_counter)\n\n        def get_dotted_value(data, key_name):\n            key_name = key_name.split('.')\n            for i in key_name:\n                data = data.get(i, {})\n                if not data:\n                    return 0\n            return data\n\n        def compute_interval(data, total_name):\n            current_total = get_dotted_value(data, total_name)\n            total_key = '.'.join(base_prefix + [total_name])\n            last_total = self.__totals.get(total_key, current_total)\n            interval = current_total - last_total\n            self.__totals[total_key] = current_total\n            return interval\n\n        def publish_percent(value_name, total_name, data):\n            value = float(get_dotted_value(data, value_name) * 100)\n            interval = compute_interval(data, total_name)\n            key = '.'.join(base_prefix + ['percent', value_name])\n            self.publish_counter(key, value, time_delta=bool(interval),\n                                 interval=interval)\n\n        publish_percent('globalLock.lockTime', 'globalLock.totalTime', data)\n\n        locks = data.get('locks')\n        if locks:\n            if '.' in locks:\n                locks['_global_'] = locks['.']\n                del (locks['.'])\n            key_prefix = '.'.join(base_prefix + ['percent'])\n            db_name_filter = re.compile(self.config['databases'])\n            interval = compute_interval(data, 'uptimeMillis')\n            for db_name in locks:\n                if not db_name_filter.search(db_name):\n                    continue\n                r = get_dotted_value(\n                    locks,\n                    '%s.timeLockedMicros.r' % db_name)\n                R = get_dotted_value(\n                    locks,\n                    '.%s.timeLockedMicros.R' % db_name)\n                value = float(r + R) / 10\n                if value:\n                    self.publish_counter(\n                        key_prefix + '.locks.%s.read' % db_name,\n                        value, time_delta=bool(interval),\n                        interval=interval)\n                w = get_dotted_value(\n                    locks,\n                    '%s.timeLockedMicros.w' % db_name)\n                W = get_dotted_value(\n                    locks,\n                    '%s.timeLockedMicros.W' % db_name)\n                value = float(w + W) / 10\n                if value:\n                    self.publish_counter(\n                        key_prefix + '.locks.%s.write' % db_name,\n                        value, time_delta=bool(interval), interval=interval)", "response": "Publishes the values of type counter or percent"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_default_config(self):\n        config = super(EndecaDgraphCollector, self).get_default_config()\n        config.update({\n            'path': 'endeca.dgraph',\n            'host': 'localhost',\n            'port': 8080,\n            'timeout': 1,\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_default_config_help(self):\n        config = super(ArchiveHandler, self).get_default_config_help()\n\n        config.update({\n            'log_file': 'Path to the logfile',\n            'when': 'type of interval; S, M, H, D, Weekday, midnight',\n            'days': 'How many days to store',\n            'rollover_interval': 'rollover interval length',\n            'encoding': '',\n            'propagate': 'Pass handled metrics to configured root logger',\n        })\n\n        return config", "response": "Returns the help text for the configuration options for this handler\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_default_config(self):\n        config = super(ArchiveHandler, self).get_default_config()\n\n        config.update({\n            'log_file': '',\n            'when': 'midnight',\n            'days': 7,\n            'rollover_interval': 1,\n            'encoding': None,\n            'propagate': False,\n        })\n\n        return config", "response": "Returns the default config for the handler\nAttributeNames"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_default_config(self):\n        config = super(MongoDBCollector, self).get_default_config()\n        config.update({\n            'path': 'mongo',\n            'hosts': ['localhost'],\n            'user': None,\n            'passwd': None,\n            'databases': '.*',\n            'ignore_collections': '^tmp\\.mr\\.',\n            'network_timeout': None,\n            'simple': 'False',\n            'translate_collections': 'False',\n            'collection_sample_rate': 1,\n            'ssl': False,\n            'replica': False,\n            'replset_node_name': '_id'\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncollect number values from db. serverStatus", "response": "def collect(self):\n        \"\"\"Collect number values from db.serverStatus()\"\"\"\n\n        if pymongo is None:\n            self.log.error('Unable to import pymongo')\n            return\n\n        hosts = self.config.get('hosts')\n\n        # Convert a string config value to be an array\n        if isinstance(hosts, basestring):\n            hosts = [hosts]\n\n        # we need this for backwards compatibility\n        if 'host' in self.config:\n            hosts = [self.config['host']]\n\n        # convert network_timeout to integer\n        if self.config['network_timeout']:\n            self.config['network_timeout'] = int(\n                self.config['network_timeout'])\n\n        # convert collection_sample_rate to float\n        if self.config['collection_sample_rate']:\n            self.config['collection_sample_rate'] = float(\n                self.config['collection_sample_rate'])\n\n        # use auth if given\n        if 'user' in self.config:\n            user = self.config['user']\n        else:\n            user = None\n\n        if 'passwd' in self.config:\n            passwd = self.config['passwd']\n        else:\n            passwd = None\n\n        for host in hosts:\n            matches = re.search('((.+)\\@)?(.+)?', host)\n            alias = matches.group(2)\n            host = matches.group(3)\n\n            if alias is None:\n                if len(hosts) == 1:\n                    # one host only, no need to have a prefix\n                    base_prefix = []\n                else:\n                    base_prefix = [re.sub('[:\\.]', '_', host)]\n            else:\n                base_prefix = [alias]\n\n            try:\n                # Ensure that the SSL option is a boolean.\n                if type(self.config['ssl']) is str:\n                    self.config['ssl'] = str_to_bool(self.config['ssl'])\n\n                if ReadPreference is None:\n                    conn = pymongo.MongoClient(\n                        host,\n                        socketTimeoutMS=self.config['network_timeout'],\n                        ssl=self.config['ssl'],\n                    )\n                else:\n                    conn = pymongo.MongoClient(\n                        host,\n                        socketTimeoutMS=self.config['network_timeout'],\n                        ssl=self.config['ssl'],\n                        read_preference=ReadPreference.SECONDARY,\n                    )\n            except Exception as e:\n                self.log.error('Couldnt connect to mongodb: %s', e)\n                continue\n\n            # try auth\n            if user:\n                try:\n                    conn.admin.authenticate(user, passwd)\n                except Exception as e:\n                    self.log.error(\n                        'User auth given, but could not autheticate' +\n                        ' with host: %s, err: %s' % (host, e))\n                    return{}\n\n            data = conn.db.command('serverStatus')\n            self._publish_transformed(data, base_prefix)\n            if str_to_bool(self.config['simple']):\n                data = self._extract_simple_data(data)\n            if str_to_bool(self.config['replica']):\n                try:\n                    replset_data = conn.admin.command('replSetGetStatus')\n                    self._publish_replset(replset_data, base_prefix)\n                except pymongo.errors.OperationFailure as e:\n                    self.log.error('error getting replica set status', e)\n\n            self._publish_dict_with_prefix(data, base_prefix)\n            db_name_filter = re.compile(self.config['databases'])\n            ignored_collections = re.compile(self.config['ignore_collections'])\n            sample_threshold = self.MAX_CRC32 * self.config[\n                'collection_sample_rate']\n            for db_name in conn.database_names():\n                if not db_name_filter.search(db_name):\n                    continue\n                db_stats = conn[db_name].command('dbStats')\n                db_prefix = base_prefix + ['databases', db_name]\n                self._publish_dict_with_prefix(db_stats, db_prefix)\n                for collection_name in conn[db_name].collection_names():\n                    if ignored_collections.search(collection_name):\n                        continue\n                    if (self.config['collection_sample_rate'] < 1 and (\n                            zlib.crc32(collection_name) & 0xffffffff\n                    ) > sample_threshold):\n                        continue\n\n                    collection_stats = conn[db_name].command('collstats',\n                                                             collection_name)\n                    if str_to_bool(self.config['translate_collections']):\n                        collection_name = collection_name.replace('.', '_')\n                    collection_prefix = db_prefix + [collection_name]\n                    self._publish_dict_with_prefix(collection_stats,\n                                                   collection_prefix)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _publish_replset(self, data, base_prefix):\n        prefix = base_prefix + ['replset']\n        self._publish_dict_with_prefix(data, prefix)\n        total_nodes = len(data['members'])\n        healthy_nodes = reduce(lambda value, node: value + node['health'],\n                               data['members'], 0)\n\n        self._publish_dict_with_prefix({\n            'healthy_nodes': healthy_nodes,\n            'total_nodes': total_nodes\n        }, prefix)\n        for node in data['members']:\n            replset_node_name = node[self.config['replset_node_name']]\n            node_name = str(replset_node_name.split('.')[0])\n            self._publish_dict_with_prefix(node, prefix + ['node', node_name])", "response": "Given a response to replSetGetStatus this method publishes all numeric values\n            of the instance and all observed statuses of all nodes in the replica set."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(PuppetAgentCollector, self).get_default_config()\n        config.update({\n            'yaml_path': '/var/lib/puppet/state/last_run_summary.yaml',\n            'path':     'puppetagent',\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the default collector settings for the AUCUPSD collector.", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(ApcupsdCollector, self).get_default_config()\n        config.update({\n            'path':     'apcupsd',\n            'hostname': 'localhost',\n            'port': 3551,\n            'metrics': ['LINEV', 'LOADPCT', 'BCHARGE', 'TIMELEFT', 'BATTV',\n                        'NUMXFERS', 'TONBATT', 'MAXLINEV', 'MINLINEV',\n                        'OUTPUTV', 'ITEMP', 'LINEFREQ', 'CUMONBATT', ],\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_default_config_help(self):\n        config = super(InfluxdbHandler, self).get_default_config_help()\n\n        config.update({\n            'hostname': 'Hostname',\n            'port': 'Port',\n            'ssl': 'set to True to use HTTPS instead of http',\n            'batch_size': 'How many metrics to store before sending to the'\n            ' influxdb server',\n            'cache_size': 'How many values to store in cache in case of'\n            ' influxdb failure',\n            'username': 'Username for connection',\n            'password': 'Password for connection',\n            'database': 'Database name',\n            'time_precision': 'time precision in second(s), milisecond(ms) or '\n            'microsecond (u)',\n        })\n\n        return config", "response": "Returns the help text for the configuration options for this handler\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the default config for the handler AttributeNames", "response": "def get_default_config(self):\n        \"\"\"\n        Return the default config for the handler\n        \"\"\"\n        config = super(InfluxdbHandler, self).get_default_config()\n\n        config.update({\n            'hostname': 'localhost',\n            'port': 8086,\n            'ssl': False,\n            'username': 'root',\n            'password': 'root',\n            'database': 'graphite',\n            'batch_size': 1,\n            'cache_size': 20000,\n            'time_precision': 's',\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _send(self):\n        # Check to see if we have a valid socket. If not, try to connect.\n        try:\n            if self.influx is None:\n                self.log.debug(\"InfluxdbHandler: Socket is not connected. \"\n                               \"Reconnecting.\")\n                self._connect()\n            if self.influx is None:\n                self.log.debug(\"InfluxdbHandler: Reconnect failed.\")\n            else:\n                # build metrics data\n                metrics = []\n                for path in self.batch:\n                    metrics.append({\n                        \"points\": self.batch[path],\n                        \"name\": path,\n                        \"columns\": [\"time\", \"value\"]})\n                # Send data to influxdb\n                self.log.debug(\"InfluxdbHandler: writing %d series of data\",\n                               len(metrics))\n                self.influx.write_points(metrics,\n                                         time_precision=self.time_precision)\n\n                # empty batch buffer\n                self.batch = {}\n                self.batch_count = 0\n                self.time_multiplier = 1\n\n        except Exception:\n            self._close()\n            if self.time_multiplier < 5:\n                self.time_multiplier += 1\n            self._throttle_error(\n                \"InfluxdbHandler: Error sending metrics, waiting for %ds.\",\n                2**self.time_multiplier)\n            raise", "response": "Send data to Influxdb."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconnecting to the influxdb server and return the ID.", "response": "def _connect(self):\n        \"\"\"\n        Connect to the influxdb server\n        \"\"\"\n\n        try:\n            # Open Connection\n            self.influx = InfluxDBClient(self.hostname, self.port,\n                                         self.username, self.password,\n                                         self.database, self.ssl)\n            # Log\n            self.log.debug(\"InfluxdbHandler: Established connection to \"\n                           \"%s:%d/%s.\",\n                           self.hostname, self.port, self.database)\n        except Exception as ex:\n            # Log Error\n            self._throttle_error(\"InfluxdbHandler: Failed to connect to \"\n                                 \"%s:%d/%s. %s\",\n                                 self.hostname, self.port, self.database, ex)\n            # Close Socket\n            self._close()\n            return"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_default_config(self):\n        config = super(KafkaCollector, self).get_default_config()\n        config.update({\n            'host': '127.0.0.1',\n            'port': 8082,\n            'path': 'kafka',\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(LibvirtKVMCollector, self).get_default_config()\n        config.update({\n            'path':     'libvirt-kvm',\n            'sort_by_uuid': False,\n            'uri':      'qemu:///system',\n            'cpu_absolute': False\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_default_config(self):\n        default_config = super(WebsiteMonitorCollector,\n                               self).get_default_config()\n        default_config['URL'] = ''\n        default_config['path'] = 'websitemonitor'\n        return default_config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_config(self):\n        if 'rmq_port' in self.config:\n            self.rmq_port = int(self.config['rmq_port'])\n\n        if 'rmq_user' in self.config:\n            self.rmq_user = self.config['rmq_user']\n\n        if 'rmq_password' in self.config:\n            self.rmq_password = self.config['rmq_password']\n\n        if 'rmq_vhost' in self.config:\n            self.rmq_vhost = self.config['rmq_vhost']\n\n        if 'rmq_exchange_type' in self.config:\n            self.rmq_exchange_type = self.config['rmq_exchange_type']\n\n        if 'rmq_durable' in self.config:\n            self.rmq_durable = bool(self.config['rmq_durable'])\n\n        if 'rmq_heartbeat_interval' in self.config:\n            self.rmq_heartbeat_interval = int(\n                self.config['rmq_heartbeat_interval'])", "response": "Get and set config options from config file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the help text for the configuration options for this handler", "response": "def get_default_config_help(self):\n        \"\"\"\n        Returns the help text for the configuration options for this handler\n        \"\"\"\n        config = super(rmqHandler, self).get_default_config_help()\n\n        config.update({\n            'server': '',\n            'rmq_exchange': '',\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_default_config(self):\n        config = super(rmqHandler, self).get_default_config()\n\n        config.update({\n            'server': '127.0.0.1',\n            'rmq_exchange': 'diamond',\n        })\n\n        return config", "response": "Returns the default config for the handler\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _bind(self, rmq_server):\n        if ((rmq_server in self.connections.keys() and\n             self.connections[rmq_server] is not None and\n             self.connections[rmq_server].is_open)):\n            # It seems we already have this server, so let's try _unbind just\n            # to be safe.\n            self._unbind(rmq_server)\n\n        credentials = None\n        if self.rmq_user and self.rmq_password:\n            credentials = pika.PlainCredentials(\n                self.rmq_user,\n                self.rmq_password)\n\n        parameters = pika.ConnectionParameters(\n            host=rmq_server,\n            port=self.rmq_port,\n            virtual_host=self.rmq_vhost,\n            credentials=credentials,\n            heartbeat_interval=self.rmq_heartbeat_interval,\n            retry_delay=5,\n            connection_attempts=3)\n\n        self.connections[rmq_server] = None\n        while (self.connections[rmq_server] is None or\n               self.connections[rmq_server].is_open is False):\n            try:\n                self.connections[rmq_server] = pika.BlockingConnection(\n                    parameters)\n                self.channels[rmq_server] = self.connections[\n                    rmq_server].channel()\n                self.channels[rmq_server].exchange_declare(\n                    exchange=self.rmq_exchange,\n                    type=self.rmq_exchange_type,\n                    durable=self.rmq_durable)\n                # Reset reconnect_interval after a successful connection\n                self.reconnect_interval = 1\n            except Exception as exception:\n                self.log.debug(\"Caught exception in _bind: %s\", exception)\n                if rmq_server in self.connections.keys():\n                    self._unbind(rmq_server)\n\n                if self.reconnect_interval >= 16:\n                    break\n\n                if self.reconnect_interval < 16:\n                    self.reconnect_interval = self.reconnect_interval * 2\n\n                time.sleep(self.reconnect_interval)", "response": "Bind to an existing PUB socket and create a new channel and bind it to the server."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nunbind a connection and unsets the channel", "response": "def _unbind(self, rmq_server=None):\n        \"\"\" Close AMQP connection and unset channel \"\"\"\n        try:\n            self.connections[rmq_server].close()\n        except AttributeError:\n            pass\n\n        self.connections[rmq_server] = None\n        self.channels[rmq_server] = None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprocess a metric and send it to RMQ pub socket", "response": "def process(self, metric):\n        \"\"\"\n          Process a metric and send it to RMQ pub socket\n        \"\"\"\n        for rmq_server in self.connections.keys():\n            try:\n                if ((self.connections[rmq_server] is None or\n                     self.connections[rmq_server].is_open is False)):\n                    self._bind(rmq_server)\n\n                channel = self.channels[rmq_server]\n                channel.basic_publish(exchange=self.rmq_exchange,\n                                      routing_key='', body=\"%s\" % metric)\n            except Exception as exception:\n                self.log.error(\n                    \"Failed publishing to %s, attempting reconnect\",\n                    rmq_server)\n                self.log.debug(\"Caught exception: %s\", exception)\n                self._unbind(rmq_server)\n                self._bind(rmq_server)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_default_config(self):\n        config = super(ChronydCollector, self).get_default_config()\n        config.update({\n            'path':             'chrony',\n            'bin':              '/usr/bin/chronyc',\n            'use_sudo':         False,\n            'sudo_cmd':         '/usr/bin/sudo',\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_default_config(self):\n        config = super(NetstatCollector, self).get_default_config()\n        config.update({\n            'path':         'netstat',\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\noverrides the Collector. collect method AttributeNames", "response": "def collect(self):\n        \"\"\"\n        Overrides the Collector.collect method\n        \"\"\"\n\n        content = self._load()\n        result = dict((self.STATE[num], 0) for num in self.STATE)\n\n        for line in content:\n            line_array = self._remove_empty(line.split(' '))\n            state = self.STATE[line_array[3]]\n\n            result[state] += 1\n\n        for state in result:\n            self.publish(state, result[state])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _load():\n        with open(NetstatCollector.PROC_TCP, 'r') as f:\n            content = f.readlines()\n            content.pop(0)\n        return content", "response": "Read the table of TCP connections and remove header"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns default collector settings.", "response": "def get_default_config(self):\n        \"\"\"\n        Returns default collector settings.\n        \"\"\"\n        config = super(LMSensorsCollector, self).get_default_config()\n        config.update({\n            'path': 'sensors',\n            'send_zero': False\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(DRBDCollector, self).get_default_config()\n        config.update({\n            'path': 'drbd'\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef collect(self):\n        performance_indicators = {\n            'ns': 'network_send',\n            'nr': 'network_receive',\n            'dw': 'disk_write',\n            'dr': 'disk_read',\n            'al': 'activity_log',\n            'bm': 'bit_map',\n            'lo': 'local_count',\n            'pe': 'pending',\n            'ua': 'unacknowledged',\n            'ap': 'application_pending',\n            'ep': 'epochs',\n            'wo': 'write_order',\n            'oos': 'out_of_sync',\n            'cs': 'connection_state',\n            'ro': 'roles',\n            'ds': 'disk_states'\n        }\n\n        results = dict()\n        try:\n            statusfile = open('/proc/drbd', 'r')\n            current_resource = ''\n            for line in statusfile:\n                if re.search('version', line) is None:\n                    if re.search(r' \\d: cs', line):\n                        matches = re.match(r' (\\d): (cs:\\w+) (ro:\\w+/\\w+) '\n                                           '(ds:\\w+/\\w+) (\\w{1}) .*', line)\n                        current_resource = matches.group(1)\n                        results[current_resource] = dict()\n                    elif re.search(r'\\sns:', line):\n                        metrics = line.strip().split(\" \")\n                        for metric in metrics:\n                            item, value = metric.split(\":\")\n                            results[current_resource][\n                                performance_indicators[item]] = value\n\n                else:\n                    continue\n            statusfile.close()\n        except IOError as errormsg:\n            self.log.error(\"Can't read DRBD status file: {}\".format(errormsg))\n            return\n\n        for resource in results.keys():\n            for metric_name, metric_value in results[resource].items():\n                if metric_value.isdigit():\n                    self.publish(resource + \".\" + metric_name, metric_value)\n                else:\n                    continue", "response": "Collect the current state of the current system."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_default_config(self):\n\n        config = super(UPSCollector, self).get_default_config()\n        config.update({\n            'path':             'ups',\n            'ups_name':         'cyberpower',\n            'bin':              '/bin/upsc',\n            'use_sudo':         False,\n            'sudo_cmd':         '/usr/bin/sudo',\n        })\n        return config", "response": "Returns default collector settings."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(ResqueWebCollector, self).get_default_config()\n        config.update({\n            'host': 'localhost',\n            'port': 5678,\n            'path': 'resqueweb',\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_default_config_help(self):\n        config_help = super(MemoryLxcCollector, self).get_default_config_help()\n        config_help.update({\n            \"sys_path\": \"Defaults to '/sys/fs/cgroup/lxc'\",\n        })\n        return config_help", "response": "Return help text for collector configuration."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn default settings for collector.", "response": "def get_default_config(self):\n        \"\"\"\n        Returns default settings for collector.\n        \"\"\"\n        config = super(MemoryLxcCollector, self).get_default_config()\n        config.update({\n            \"path\":     \"lxc\",\n            \"sys_path\": \"/sys/fs/cgroup/lxc\",\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncollect memory stats of LXCs.", "response": "def collect(self):\n        \"\"\"\n        Collect memory stats of LXCs.\n        \"\"\"\n        lxc_metrics = [\"memory.usage_in_bytes\", \"memory.limit_in_bytes\"]\n        if os.path.isdir(self.config[\"sys_path\"]) is False:\n            self.log.debug(\"sys_path '%s' isn't directory.\",\n                           self.config[\"sys_path\"])\n            return {}\n\n        collected = {}\n        for item in os.listdir(self.config[\"sys_path\"]):\n            fpath = \"%s/%s\" % (self.config[\"sys_path\"], item)\n            if os.path.isdir(fpath) is False:\n                continue\n\n            for lxc_metric in lxc_metrics:\n                filename = \"%s/%s\" % (fpath, lxc_metric)\n                metric_name = \"%s.%s\" % (\n                    item.replace(\".\", \"_\"),\n                    lxc_metric.replace(\"_in_bytes\", \"\"))\n                self.log.debug(\"Trying to collect from %s\", filename)\n                collected[metric_name] = self._read_file(filename)\n\n        for key in collected.keys():\n            if collected[key] is None:\n                continue\n\n            for unit in self.config[\"byte_unit\"]:\n                value = diamond.convertor.binary.convert(\n                    collected[key],\n                    oldUnit=\"B\",\n                    newUnit=unit)\n                new_key = \"%s_in_%ss\" % (key, unit)\n                self.log.debug(\"Publishing '%s %s'\", new_key, value)\n                self.publish(new_key, value, metric_type=\"GAUGE\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _read_file(self, filename):\n        try:\n            with open(filename, \"r\") as fhandle:\n                stats = float(fhandle.readline().rstrip(\"\\n\"))\n        except Exception:\n            stats = None\n\n        return stats", "response": "Read contents of given file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(EximCollector, self).get_default_config()\n        config.update({\n            'path':            'exim',\n            'bin':              '/usr/sbin/exim',\n            'use_sudo':         False,\n            'sudo_cmd':         '/usr/bin/sudo',\n            'sudo_user':        'root',\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nimports all modules from the given path", "response": "def load_modules_from_path(path):\n    \"\"\"\n    Import all modules from the given directory\n    \"\"\"\n    # Check and fix the path\n    if path[-1:] != '/':\n        path += '/'\n\n    # Get a list of files in the directory, if the directory exists\n    if not os.path.exists(path):\n        raise OSError(\"Directory does not exist: %s\" % path)\n\n    # Add path to the system path\n    sys.path.append(path)\n    # Load all the files in path\n    for f in os.listdir(path):\n        # Ignore anything that isn't a .py file\n        if len(f) > 3 and f[-3:] == '.py':\n            modname = f[:-3]\n            # Import the module\n            __import__(modname, globals(), locals(), ['*'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhelping text for logentries.", "response": "def get_default_config_help(self):\n        \"\"\"\n        Help text\n        \"\"\"\n        config = super(LogentriesDiamondHandler,\n                       self).get_default_config_help()\n\n        config.update({\n            'log_token':\n                '[Your log token](https://logentries.com/doc/input-token/)',\n            'queue_size': ''\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns default config for the handler", "response": "def get_default_config(self):\n        \"\"\"\n        Return default config for the handler\n        \"\"\"\n        config = super(LogentriesDiamondHandler, self).get_default_config()\n\n        config.update({\n            'log_token': '',\n            'queue_size': 100\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprocessing a metric by sending it to datadog api", "response": "def process(self, metric):\n        \"\"\"\n        Process metric by sending it to datadog api\n        \"\"\"\n\n        self.queue.append(metric)\n        if len(self.queue) >= self.queue_size:\n            logging.debug(\"Queue is full, sending logs to Logentries\")\n            self._send()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _send(self):\n        while len(self.queue) > 0:\n            metric = self.queue.popleft()\n            topic, value, timestamp = str(metric).split()\n            msg = json.dumps({\"event\": {topic: value}})\n            req = urllib2.Request(\"https://js.logentries.com/v1/logs/\" +\n                                  self.log_token, msg)\n            try:\n                urllib2.urlopen(req)\n            except urllib2.URLError as e:\n                logging.error(\"Can't send log message to Logentries %s\", e)", "response": "Send a message to Lognetries"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_default_config(self):\n        config = super(SmartCollector, self).get_default_config()\n        config.update({\n            'path': 'smart',\n            'bin': 'smartctl',\n            'use_sudo':         False,\n            'sudo_cmd':         '/usr/bin/sudo',\n            'devices': '^disk[0-9]$|^sd[a-z]$|^hd[a-z]$',\n        })\n        return config", "response": "Returns default configuration options."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncollect and publish S. M. A. R. T. attributes.", "response": "def collect(self):\n        \"\"\"\n        Collect and publish S.M.A.R.T. attributes\n        \"\"\"\n        devices = re.compile(self.config['devices'])\n\n        for device in os.listdir('/dev'):\n            if devices.match(device):\n\n                command = [self.config['bin'], \"-A\", os.path.join('/dev',\n                                                                  device)]\n\n                if str_to_bool(self.config['use_sudo']):\n                    command.insert(0, self.config['sudo_cmd'])\n\n                attributes = subprocess.Popen(\n                    command,\n                    stdout=subprocess.PIPE\n                ).communicate()[0].strip().splitlines()\n\n                metrics = {}\n\n                start_line = self.find_attr_start_line(attributes)\n                for attr in attributes[start_line:]:\n                    attribute = attr.split()\n                    if attribute[1] != \"Unknown_Attribute\":\n                        metric = \"%s.%s\" % (device, attribute[1])\n                    else:\n                        metric = \"%s.%s\" % (device, attribute[0])\n\n                    # 234 Thermal_Throttle (...)  0/0\n                    if '/' in attribute[9]:\n                        expanded = attribute[9].split('/')\n                        for i, subattribute in enumerate(expanded):\n                            submetric = '%s_%d' % (metric, i)\n                            if submetric not in metrics:\n                                metrics[submetric] = subattribute\n                            elif metrics[submetric] == 0 and subattribute > 0:\n                                metrics[submetric] = subattribute\n                    else:\n                        # New metric? Store it\n                        if metric not in metrics:\n                            metrics[metric] = attribute[9]\n                        # Duplicate metric? Only store if it has a larger value\n                        # This happens semi-often with the Temperature_Celsius\n                        # attribute You will have a PASS/FAIL after the real\n                        # temp, so only overwrite if The earlier one was a\n                        # PASS/FAIL (0/1)\n                        elif metrics[metric] == 0 and attribute[9] > 0:\n                            metrics[metric] = attribute[9]\n                        else:\n                            continue\n\n                for metric in metrics.keys():\n                    self.publish(metric, metrics[metric])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_attr_start_line(self, lines, min_line=4, max_line=9):\n        for idx, line in enumerate(lines[min_line:max_line]):\n            col = line.split()\n            if len(col) > 1 and col[1] == 'ATTRIBUTE_NAME':\n                return idx + min_line + 1\n\n        self.log.warn('ATTRIBUTE_NAME not found in second column of'\n                      ' smartctl output between lines %d and %d.'\n                      % (min_line, max_line))\n\n        return max_line + 1", "response": "Find the first line of the first real attribute and value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_default_config(self):\n        config = super(RabbitMQCollector, self).get_default_config()\n        config.update({\n            'path': 'rabbitmq',\n            'host': 'localhost:55672',\n            'user': 'guest',\n            'password': 'guest',\n            'replace_dot': False,\n            'replace_slash': False,\n            'queues_ignored': '',\n            'cluster': False,\n            'scheme': 'http',\n            'query_individual_queues': False,\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\noverrides the default get_default_config method to provide the default config for the SNMPRawCollector object.", "response": "def get_default_config(self):\n        \"\"\"\n        Override SNMPCollector.get_default_config method to provide\n        default_config for the SNMPInterfaceCollector\n        \"\"\"\n        default_config = super(SNMPRawCollector,\n                               self).get_default_config()\n        default_config.update({\n            'oids': {},\n            'path_prefix': 'servers',\n            'path_suffix': 'snmp',\n        })\n        return default_config"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncollecting SNMP interface data from a device.", "response": "def collect_snmp(self, device, host, port, community):\n        \"\"\"\n        Collect SNMP interface data from device\n        \"\"\"\n        self.log.debug(\n            'Collecting raw SNMP statistics from device \\'{}\\''.format(device))\n\n        dev_config = self.config['devices'][device]\n        if 'oids' in dev_config:\n            for oid, metricName in dev_config['oids'].items():\n\n                if (device, oid) in self.skip_list:\n                    self.log.debug(\n                        'Skipping OID \\'{}\\' ({}) on device \\'{}\\''.format(\n                            oid, metricName, device))\n                    continue\n\n                timestamp = time.time()\n                value = self._get_value(device, oid, host, port, community)\n                if value is None:\n                    continue\n\n                self.log.debug(\n                    '\\'{}\\' ({}) on device \\'{}\\' - value=[{}]'.format(\n                        oid, metricName, device, value))\n\n                path = '.'.join([self.config['path_prefix'], device,\n                                 self.config['path_suffix'], metricName])\n                metric = Metric(path=path, value=value, timestamp=timestamp,\n                                precision=self._precision(value),\n                                metric_type='GAUGE')\n                self.publish_metric(metric)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_default_config(self):\n        config = super(DiskSpaceCollector, self).get_default_config()\n        config.update({\n            'path': 'diskspace',\n            # filesystems to examine\n            'filesystems': 'ext2, ext3, ext4, xfs, glusterfs, nfs, nfs4, ' +\n                           ' ntfs, hfs, fat32, fat16, btrfs',\n\n            # exclude_filters\n            #   A list of regex patterns\n            #   A filesystem matching any of these patterns will be excluded\n            #   from disk space metrics collection.\n            #\n            # Examples:\n            #       exclude_filters =,\n            # no exclude filters at all\n            #       exclude_filters = ^/boot, ^/mnt\n            # exclude everything that begins /boot or /mnt\n            #       exclude_filters = m,\n            # exclude everything that includes the letter \"m\"\n            'exclude_filters': ['^/export/home'],\n\n            # Default numeric output\n            'byte_unit': ['byte']\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_disk_labels(self):\n        path = '/dev/disk/by-label/'\n        labels = {}\n        if not os.path.isdir(path):\n            return labels\n\n        for label in os.listdir(path):\n            label = label.replace('\\\\x2f', '/')\n            device = os.path.realpath(path + '/' + label)\n            labels[device] = label\n\n        return labels", "response": "Returns a mapping of device nodes to filesystem labels."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_file_systems(self):\n        result = {}\n        if os.access('/proc/mounts', os.R_OK):\n            file = open('/proc/mounts')\n            for line in file:\n                try:\n                    mount = line.split()\n                    device = mount[0]\n                    mount_point = mount[1]\n                    fs_type = mount[2]\n                except (IndexError, ValueError):\n                    continue\n\n                # Skip the filesystem if it is not in the list of valid\n                # filesystems\n                if fs_type not in self.filesystems:\n                    self.log.debug(\"Ignoring %s since it is of type %s \" +\n                                   \" which is not in the list of filesystems.\",\n                                   mount_point, fs_type)\n                    continue\n\n                # Process the filters\n                if self.exclude_reg.search(mount_point):\n                    self.log.debug(\"Ignoring %s since it is in the \" +\n                                   \"exclude_filter list.\", mount_point)\n                    continue\n\n                if ((('/' in device or device == 'tmpfs') and\n                     mount_point.startswith('/'))):\n                    try:\n                        stat = os.stat(mount_point)\n                    except OSError:\n                        self.log.debug(\"Path %s is not mounted - skipping.\",\n                                       mount_point)\n                        continue\n\n                    if stat.st_dev in result:\n                        continue\n\n                    result[stat.st_dev] = {\n                        'device': os.path.realpath(device),\n                        'mount_point': mount_point,\n                        'fs_type': fs_type\n                    }\n\n            file.close()\n\n        else:\n            if not psutil:\n                self.log.error('Unable to import psutil')\n                return None\n\n            partitions = psutil.disk_partitions(False)\n            for partition in partitions:\n                result[len(result)] = {\n                    'device': os.path.realpath(partition.device),\n                    'mount_point': partition.mountpoint,\n                    'fs_type': partition.fstype\n                }\n            pass\n\n        return result", "response": "Returns a dictionary of mounted filesystems on the machine."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_default_config_help(self):\n        config = super(LibratoHandler, self).get_default_config_help()\n\n        config.update({\n            'user': 'Librato username',\n            'apikey': 'Librato API key',\n            'apply_metric_prefix': 'Allow diamond to apply metric prefix',\n            'queue_max_size': 'Max measurements to queue before submitting',\n            'queue_max_interval':\n                'Max seconds to wait before submitting. For best behavior, '\n                'be sure your highest collector poll interval is lower than '\n                'or equal to the queue_max_interval setting.',\n            'include_filters':\n                'A list of regex patterns. Only measurements whose path '\n                'matches a filter will be submitted. Useful for limiting '\n                'usage to *only* desired measurements, e.g. '\n                '`\"^diskspace\\..*\\.byte_avail$\", \"^loadavg\\.01\"` or '\n                '`\"^sockets\\.\",` (note trailing comma to indicate a list)',\n        })\n\n        return config", "response": "Returns the help text for the configuration options for this handler."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the default config for the handler", "response": "def get_default_config(self):\n        \"\"\"\n        Return the default config for the handler\n        \"\"\"\n        config = super(LibratoHandler, self).get_default_config()\n\n        config.update({\n            'user': '',\n            'apikey': '',\n            'apply_metric_prefix': False,\n            'queue_max_size': 300,\n            'queue_max_interval': 60,\n            'include_filters': ['^.*'],\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprocesses a metric by sending it to Librato.", "response": "def process(self, metric):\n        \"\"\"\n        Process a metric by sending it to Librato\n        \"\"\"\n        path = metric.getCollectorPath()\n        path += '.'\n        path += metric.getMetricPath()\n        if self.config['apply_metric_prefix']:\n            path = metric.getPathPrefix() + '.' + path\n\n        if self.include_reg.match(path):\n            if metric.metric_type == 'GAUGE':\n                m_type = 'gauge'\n            else:\n                m_type = 'counter'\n            self.queue.add(path,                # name\n                           float(metric.value),  # value\n                           type=m_type,\n                           source=metric.host,\n                           measure_time=metric.timestamp)\n            self.current_n_measurements += 1\n        else:\n            self.log.debug(\"LibratoHandler: Skip %s, no include_filters match\",\n                           path)\n\n        if (self.current_n_measurements >= self.queue_max_size or\n                time.time() >= self.queue_max_timestamp):\n            self.log.debug(\"LibratoHandler: Sending batch size: %d\",\n                           self.current_n_measurements)\n            self._send()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending data to Librato.", "response": "def _send(self):\n        \"\"\"\n        Send data to Librato.\n        \"\"\"\n        self.queue.submit()\n        self.queue_max_timestamp = int(time.time() + self.queue_max_interval)\n        self.current_n_measurements = 0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef collect(self):\n        stats = self.parse_stats_file(self.config[\"status_path\"])\n        if len(stats) == 0:\n            return {}\n        elif \"info\" not in stats.keys():\n            return {}\n        elif \"programstatus\" not in stats.keys():\n            return {}\n\n        metrics = self.get_icinga_stats(stats[\"programstatus\"])\n        if \"hoststatus\" in stats.keys():\n            metrics = dict(\n                metrics.items() + self.get_host_stats(\n                    stats[\"hoststatus\"]).items())\n\n        if \"servicestatus\" in stats.keys():\n            metrics = dict(\n                metrics.items() + self.get_svc_stats(\n                    stats[\"servicestatus\"]).items())\n\n        for metric in metrics.keys():\n            self.log.debug(\"Publishing '%s %s'.\", metric, metrics[metric])\n            self.publish(metric, metrics[metric])", "response": "Collect and publish metrics\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_default_config_help(self):\n        config_help = super(IcingaStatsCollector,\n                            self).get_default_config_help()\n        config_help.update({\n            \"status_path\": \"Path to Icinga status.dat file\"\n        })\n        return config_help", "response": "Return help text for Icinga Stats Collector"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_default_config(self):\n        config = super(IcingaStatsCollector, self).get_default_config()\n        config.update({\n            \"path\": \"icinga_stats\",\n            \"status_path\": \"/var/lib/icinga/status.dat\",\n        })\n        return config", "response": "Returns default settings for collector\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting metrics from icinga", "response": "def get_icinga_stats(self, app_stats):\n        \"\"\" Extract metrics from 'programstatus' \"\"\"\n        stats = {}\n        stats = dict(stats.items() + self._get_active_stats(app_stats).items())\n        stats = dict(stats.items() + self._get_cached_stats(app_stats).items())\n        stats = dict(\n            stats.items() + self._get_command_execution(app_stats).items())\n        stats = dict(\n            stats.items() + self._get_externalcmd_stats(app_stats).items())\n        stats[\"uptime\"] = self._get_uptime(app_stats)\n        return stats"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_stats_file(self, file_name):\n        stats = {}\n        try:\n            with open(file_name, \"r\") as fhandle:\n                fbuffer = []\n                save_buffer = False\n                for line in fhandle:\n                    line = line.rstrip(\"\\n\")\n                    line = self._trim(line)\n                    if line == \"\" or line.startswith(\"#\"):\n                        continue\n                    elif line.endswith(\"{\"):\n                        save_buffer = True\n                        fbuffer.append(line)\n                        continue\n                    elif line.endswith(\"}\"):\n                        tmp_dict = self._parse_config_buffer(fbuffer)\n                        fbuffer = None\n                        fbuffer = list()\n                        if len(tmp_dict) < 1:\n                            continue\n\n                        if tmp_dict[\"_type\"] == \"info\":\n                            stats[\"info\"] = tmp_dict\n                        elif tmp_dict[\"_type\"] == \"programstatus\":\n                            stats[\"programstatus\"] = tmp_dict\n                        else:\n                            entity_type = tmp_dict[\"_type\"]\n                            if entity_type not in stats.keys():\n                                stats[entity_type] = []\n\n                            stats[entity_type].append(tmp_dict)\n\n                        continue\n                    elif save_buffer is True:\n                        fbuffer.append(line)\n\n        except Exception as exception:\n            self.log.info(\"Caught exception: %s\", exception)\n\n        return stats", "response": "Read and parse given file_name return a dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_host_stats(self, hosts):\n        stats = {\n            \"hosts.total\": 0,\n            \"hosts.ok\": 0,\n            \"hosts.down\": 0,\n            \"hosts.unreachable\": 0,\n            \"hosts.flapping\": 0,\n            \"hosts.in_downtime\": 0,\n            \"hosts.checked\": 0,\n            \"hosts.scheduled\": 0,\n            \"hosts.active_checks\": 0,\n            \"hosts.passive_checks\": 0,\n        }\n        for host in list(hosts):\n            if type(host) is not dict:\n                continue\n\n            sane = self._sanitize_entity(host)\n            stats[\"hosts.total\"] += 1\n            stats[\"hosts.flapping\"] += self._trans_binary(sane[\"flapping\"])\n            stats[\n                \"hosts.in_downtime\"] += self._trans_dtime(sane[\"in_downtime\"])\n            stats[\"hosts.checked\"] += self._trans_binary(sane[\"checked\"])\n            stats[\"hosts.scheduled\"] += self._trans_binary(sane[\"scheduled\"])\n            stats[\"hosts.active_checks\"] += sane[\"active_checks\"]\n            stats[\"hosts.passive_checks\"] += sane[\"passive_checks\"]\n            state_key = self._trans_host_state(sane[\"state\"])\n            stats[\"hosts.%s\" % (state_key)] += 1\n\n        return stats", "response": "Get statistics for hosts resp. Host entities"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget statistics for Services resp. Service entities", "response": "def get_svc_stats(self, svcs):\n        \"\"\" Get statistics for Services, resp. Service entities \"\"\"\n        stats = {\n            \"services.total\": 0,\n            \"services.ok\": 0,\n            \"services.warning\": 0,\n            \"services.critical\": 0,\n            \"services.unknown\": 0,\n            \"services.flapping\": 0,\n            \"services.in_downtime\": 0,\n            \"services.checked\": 0,\n            \"services.scheduled\": 0,\n            \"services.active_checks\": 0,\n            \"services.passive_checks\": 0,\n        }\n        for svc in svcs:\n            if type(svc) is not dict:\n                continue\n\n            sane = self._sanitize_entity(svc)\n            stats[\"services.total\"] += 1\n            stats[\"services.flapping\"] += self._trans_binary(sane[\"flapping\"])\n            stats[\"services.in_downtime\"] += self._trans_dtime(\n                sane[\"in_downtime\"])\n            stats[\"services.checked\"] += self._trans_binary(sane[\"checked\"])\n            stats[\n                \"services.scheduled\"] += self._trans_binary(sane[\"scheduled\"])\n            stats[\"services.active_checks\"] += sane[\"active_checks\"]\n            stats[\"services.passive_checks\"] += sane[\"passive_checks\"]\n            state_key = self._trans_svc_state(sane[\"state\"])\n            stats[\"services.%s\" % (state_key)] += 1\n\n        return stats"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nturning a string like 10 178 528 into tuple of integers", "response": "def _convert_tripplet(self, tripplet):\n        \"\"\" Turn '10,178,528' into tuple of integers \"\"\"\n        splitted = tripplet.split(\",\")\n        if len(splitted) != 3:\n            self.log.debug(\"Got %i chunks, expected 3.\", len(splitted))\n            return (0, 0, 0)\n\n        try:\n            x01 = int(splitted[0])\n            x05 = int(splitted[1])\n            x15 = int(splitted[2])\n        except Exception as exception:\n            self.log.warning(\"Caught exception: %s\", exception)\n            x01 = 0\n            x05 = 0\n            x15 = 0\n\n        return (x01, x05, x15)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the active stats from the app stats.", "response": "def _get_active_stats(self, app_stats):\n        \"\"\"\n        Process:\n          * active_scheduled_host_check_stats\n          * active_scheduled_service_check_stats\n          * active_ondemand_host_check_stats\n          * active_ondemand_service_check_stats\n        \"\"\"\n        stats = {}\n        app_keys = [\n            \"active_scheduled_host_check_stats\",\n            \"active_scheduled_service_check_stats\",\n            \"active_ondemand_host_check_stats\",\n            \"active_ondemand_service_check_stats\",\n        ]\n        for app_key in app_keys:\n            if app_key not in app_stats.keys():\n                continue\n\n            splitted = app_key.split(\"_\")\n            metric = \"%ss.%s_%s\" % (splitted[2], splitted[0], splitted[1])\n            (x01, x05, x15) = self._convert_tripplet(app_stats[app_key])\n            stats[\"%s.01\" % (metric)] = x01\n            stats[\"%s.05\" % (metric)] = x05\n            stats[\"%s.15\" % (metric)] = x15\n\n        return stats"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_cached_stats(self, app_stats):\n        stats = {}\n        app_keys = [\n            \"cached_host_check_stats\",\n            \"cached_service_check_stats\",\n        ]\n        for app_key in app_keys:\n            if app_key not in app_stats.keys():\n                continue\n\n            (x01, x05, x15) = self._convert_tripplet(app_stats[app_key])\n            scratch = app_key.split(\"_\")[1]\n            stats[\"%ss.cached.01\" % (scratch)] = x01\n            stats[\"%ss.cached.05\" % (scratch)] = x05\n            stats[\"%ss.cached.15\" % (scratch)] = x15\n\n        return stats", "response": "Get the cached stats from the app stats."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_externalcmd_stats(self, app_stats):\n        khigh = \"high_external_command_buffer_slots\"\n        ktotal = \"total_external_command_buffer_slots\"\n        kused = \"used_external_command_buffer_slots\"\n        kstats = \"external_command_stats\"\n        aliases = {\n            khigh: \"external_command.buffer_high\",\n            ktotal: \"external_command.buffer_total\",\n            kused: \"external_command.buffer_used\",\n            \"x01\": \"external_command.01\",\n            \"x05\": \"external_command.05\",\n            \"x15\": \"external_command.15\",\n        }\n        stats = {}\n        if khigh in app_stats.keys() and str(app_stats[khigh]).isdigit():\n            key = aliases[khigh]\n            stats[key] = int(app_stats[khigh])\n\n        if ktotal in app_stats.keys() and str(app_stats[ktotal].isdigit()):\n            key = aliases[ktotal]\n            stats[key] = int(app_stats[ktotal])\n\n        if kused in app_stats.keys() and str(app_stats[kused].isdigit()):\n            key = aliases[kused]\n            stats[key] = int(app_stats[ktotal])\n\n        if kstats in app_stats.keys():\n            (x01, x05, x15) = self._convert_tripplet(app_stats[kstats])\n            stats[aliases[\"x01\"]] = x01\n            stats[aliases[\"x05\"]] = x05\n            stats[aliases[\"x01\"]] = x15\n\n        return stats", "response": "Get external command stats from app stats."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_uptime(self, app_stats):\n        if \"program_start\" not in app_stats.keys():\n            return 0\n\n        if not app_stats[\"program_start\"].isdigit():\n            return 0\n\n        uptime = int(time.time()) - int(app_stats[\"program_start\"])\n        if uptime < 0:\n            return 0\n\n        return uptime", "response": "Return Icinga s uptime"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse buffered config into dict", "response": "def _parse_config_buffer(self, fbuffer):\n        \"\"\" Parse buffered chunk of config into dict \"\"\"\n        if len(fbuffer) < 1 or not fbuffer[0].endswith(\"{\"):\n            # Invalid input\n            return {}\n\n        entity = {}\n        entity_type = fbuffer.pop(0)\n        entity_type = entity_type.rstrip(\"{\")\n        entity[\"_type\"] = self._trim(entity_type)\n        for chunk in fbuffer:\n            splitted = chunk.split(\"=\")\n            if len(splitted) < 2:\n                # If there is no '=', then it's an invalid line\n                continue\n\n            key = self._trim(splitted[0])\n            value = self._trim(\"=\".join(splitted[1:]))\n            entity[key] = value\n\n        return entity"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _sanitize_entity(self, entity):\n        aliases = {\n            \"current_state\": \"state\",\n            \"is_flapping\": \"flapping\",\n            \"scheduled_downtime_depth\": \"in_downtime\",\n            \"has_been_checked\": \"checked\",\n            \"should_be_scheduled\": \"scheduled\",\n            \"active_checks_enabled\": \"active_checks\",\n            \"passive_checks_enabled\": \"passive_checks\",\n        }\n        sane = {}\n        for akey in aliases.keys():\n            sane[aliases[akey]] = None\n\n        aliases_keys = aliases.keys()\n        for key in entity.keys():\n            if key not in aliases_keys:\n                continue\n\n            alias = aliases[key]\n            try:\n                sane[alias] = int(entity[key])\n            except Exception:\n                sane[alias] = None\n\n        if sane[\"active_checks\"] not in [0, 1]:\n            sane[\"active_checks\"] = 0\n        elif sane[\"active_checks\"] == 1:\n            sane[\"passive_checks\"] = 0\n\n        if sane[\"passive_checks\"] not in [0, 1]:\n            sane[\"passive_checks\"] = 0\n\n        return sane", "response": "Make given entity sane for further use."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntrimming left - right given string", "response": "def _trim(self, somestr):\n        \"\"\" Trim left-right given string \"\"\"\n        tmp = RE_LSPACES.sub(\"\", somestr)\n        tmp = RE_TSPACES.sub(\"\", tmp)\n        return str(tmp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _list_request(self):\n        try:\n            # https://jolokia.org/reference/html/protocol.html\n            #\n            # A maxDepth of 1 restricts the return value to a map with the JMX\n            # domains as keys. The values of the maps don't have any meaning\n            # and are dummy values.\n            #\n            # maxCollectionSize=0 means \"unlimited\". This works around an issue\n            # prior to Jolokia 1.3 where results were truncated at 1000\n            #\n            url = \"http://%s:%s/%s%s?maxDepth=1&maxCollectionSize=0\" % (\n                self.config['host'],\n                self.config['port'],\n                self.jolokia_path,\n                self.LIST_URL)\n            # need some time to process the downloaded metrics, so that's why\n            # timeout is lower than the interval.\n            timeout = max(2, float(self.config['interval']) * 2 / 3)\n            with closing(urllib2.urlopen(self._create_request(url),\n                                         timeout=timeout)) as response:\n                return self._read_json(response)\n        except (urllib2.HTTPError, ValueError) as e:\n            self.log.error('Unable to read JSON response: %s', str(e))\n            return {}", "response": "Returns a dictionary with JMX domain names as keys"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(MonitCollector, self).get_default_config()\n        config.update({\n            'host':         '127.0.0.1',\n            'port':         2812,\n            'user':         'monit',\n            'passwd':       'monit',\n            'path':         'monit',\n            'byte_unit':    ['byte'],\n            'send_totals':  False,\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_default_config(self):\n        config = super(OpenstackSwiftCollector, self).get_default_config()\n        config.update({\n            'path': 'openstackswift',\n            'enable_dispersion_report': False,\n            'enable_container_metrics': True,\n            # don't use the threaded model with this one.\n            # for some reason it crashes.\n            'interval': 1200,  # by default, every 20 minutes\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(SoftInterruptCollector, self).get_default_config()\n        config.update({\n            'path':     'softirq'\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncollect interrupt data from the PROC file and publish the interrupt data.", "response": "def collect(self):\n        \"\"\"\n        Collect interrupt data\n        \"\"\"\n        if not os.access(self.PROC, os.R_OK):\n            return False\n\n        # Open PROC file\n        file = open(self.PROC, 'r')\n\n        # Get data\n        for line in file:\n\n            if not line.startswith('softirq'):\n                continue\n\n            data = line.split()\n\n            metric_name = 'total'\n            metric_value = int(data[1])\n            metric_value = int(self.derivative(\n                metric_name,\n                long(metric_value), counter))\n            self.publish(metric_name, metric_value)\n\n            for i in range(2, len(data)):\n                metric_name = str(i - 2)\n                metric_value = int(data[i])\n                metric_value = int(self.derivative(\n                    metric_name,\n                    long(metric_value), counter))\n                self.publish(metric_name, metric_value)\n\n        # Close file\n        file.close()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_default_config(self):\n        config = super(FilestatCollector, self).get_default_config()\n        config.update({\n            'path':     'files',\n            'user_include': None,\n            'user_exclude': None,\n            'group_include': None,\n            'group_exclude': None,\n            'uid_min': 0,\n            'uid_max': 65536,\n            'type_include': None,\n            'type_exclude': None,\n            'collect_user_data': False\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_userlist(self):\n    # convert user/group  lists to arrays if strings\n        if isinstance(self.config['user_include'], basestring):\n            self.config['user_include'] = self.config['user_include'].split()\n        if isinstance(self.config['user_exclude'], basestring):\n            self.config['user_exclude'] = self.config['user_exclude'].split()\n        if isinstance(self.config['group_include'], basestring):\n            self.config['group_include'] = self.config['group_include'].split()\n        if isinstance(self.config['group_exclude'], basestring):\n            self.config['group_exclude'] = self.config['group_exclude'].split()\n\n        rawusers = os.popen(\"lsof | awk '{ print $3 }' | sort | uniq -d\"\n                            ).read().split()\n        userlist = []\n\n        # remove any not on the user include list\n        if ((self.config['user_include'] is None or\n             len(self.config['user_include']) == 0)):\n            userlist = rawusers\n        else:\n            # only work with specified include list, which is added at the end\n            userlist = []\n\n        # add any user in the group include list\n        addedByGroup = []\n        if ((self.config['group_include'] is not None and\n             len(self.config['group_include']) > 0)):\n            for u in rawusers:\n                self.log.info(u)\n                # get list of groups of user\n                user_groups = os.popen(\"id -Gn %s\" % (u)).read().split()\n                for gi in self.config['group_include']:\n                    if gi in user_groups and u not in userlist:\n                        userlist.append(u)\n                        addedByGroup.append(u)\n                        break\n\n        # remove any user in the exclude group list\n        if ((self.config['group_exclude'] is not None and\n             len(self.config['group_exclude']) > 0)):\n            # create tmp list to iterate over while editing userlist\n            tmplist = userlist[:]\n            for u in tmplist:\n                # get list of groups of user\n                groups = os.popen(\"id -Gn %s\" % (u)).read().split()\n                for gi in self.config['group_exclude']:\n                    if gi in groups:\n                        userlist.remove(u)\n                        break\n\n        # remove any that aren't within the uid limits\n        # make sure uid_min/max are ints\n        self.config['uid_min'] = int(self.config['uid_min'])\n        self.config['uid_max'] = int(self.config['uid_max'])\n        tmplist = userlist[:]\n        for u in tmplist:\n            if ((self.config['user_include'] is None or\n                 u not in self.config['user_include'])):\n                if u not in addedByGroup:\n                    uid = int(os.popen(\"id -u %s\" % (u)).read())\n                    if ((uid < self.config['uid_min'] and\n                         self.config['uid_min'] is not None and\n                         u in userlist)):\n                        userlist.remove(u)\n                    if ((uid > self.config['uid_max'] and\n                         self.config['uid_max'] is not None and\n                         u in userlist)):\n                        userlist.remove(u)\n\n        # add users that are in the users include list\n        if ((self.config['user_include'] is not None and\n             len(self.config['user_include']) > 0)):\n            for u in self.config['user_include']:\n                if u in rawusers and u not in userlist:\n                    userlist.append(u)\n\n        # remove any that is on the user exclude list\n        if ((self.config['user_exclude'] is not None and\n             len(self.config['user_exclude']) > 0)):\n            for u in self.config['user_exclude']:\n                if u in userlist:\n                    userlist.remove(u)\n\n        return userlist", "response": "This function returns a list of all the users with open files on the system and filters them based on the variables user_include and user_exclude."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_typelist(self):\n        typelist = []\n\n        # convert type list into arrays if strings\n        if isinstance(self.config['type_include'], basestring):\n            self.config['type_include'] = self.config['type_include'].split()\n        if isinstance(self.config['type_exclude'], basestring):\n            self.config['type_exclude'] = self.config['type_exclude'].split()\n\n        # remove any not in include list\n        if self.config['type_include'] is None or len(\n                self.config['type_include']) == 0:\n            typelist = os.popen(\"lsof | awk '{ print $5 }' | sort | uniq -d\"\n                                ).read().split()\n        else:\n            typelist = self.config['type_include']\n\n        # remove any in the exclude list\n        if self.config['type_exclude'] is not None and len(\n                self.config['type_include']) > 0:\n            for t in self.config['type_exclude']:\n                if t in typelist:\n                    typelist.remove(t)\n\n        return typelist", "response": "This function returns a list of all avaliable types and applies include and exclude filters"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_lsof(self, users, types):\n        d = {}\n        for u in users:\n            d[u] = {}\n            tmp = os.popen(\"lsof -wbu %s | awk '{ print $5 }'\" % (\n                u)).read().split()\n            for t in types:\n                d[u][t] = tmp.count(t)\n        return d", "response": "Get the list of users and file types to collect for and collect the\n        data from lsof"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn default configuration options.", "response": "def get_default_config(self):\n        \"\"\"\n        Returns default configuration options.\n        \"\"\"\n        config = super(NetfilterAccountingCollector, self).get_default_config()\n        config.update({\n            'path': 'nfacct',\n            'bin': 'nfacct',\n            'use_sudo': False,\n            'reset': True,\n            'sudo_cmd': '/usr/bin/sudo',\n            'method': 'Threaded'\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef collect(self):\n        cmd = [self.config['bin'], \"list\"]\n\n        if str_to_bool(self.config['reset']):\n            cmd.append(\"reset\")\n\n        if str_to_bool(self.config['use_sudo']):\n            cmd.insert(0, self.config['sudo_cmd'])\n\n        # We avoid use of the XML format to mtaintain compatbility with older\n        # versions of nfacct and also to avoid the bug where pkts and bytes were\n        # flipped\n\n        # Each line is of the format:\n        # { pkts = 00000000000001121700, bytes = 00000000000587037355 } = ipv4;\n        matcher = re.compile(\"{ pkts = (.*), bytes = (.*) } = (.*);\")\n        lines = Popen(cmd, stdout=PIPE).communicate()[0].strip().splitlines()\n\n        for line in lines:\n            matches = re.match(matcher, line)\n            if matches:\n                num_packets = int(matches.group(1))\n                num_bytes = int(matches.group(2))\n                name = matches.group(3)\n                self.publish(name + \".pkts\", num_packets)\n                self.publish(name + \".bytes\", num_bytes)", "response": "Collect and publish netfilter counters\n           "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_default_config(self):\n        config = super(SidekiqWebCollector, self).get_default_config()\n        config.update({\n            'host': 'localhost',\n            'port': 9999,\n            'byte_unit': ['byte'],\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(ZookeeperCollector, self).get_default_config()\n        config.update({\n            'path':     'zookeeper',\n\n            # Which rows of 'status' you would like to publish.\n            # 'telnet host port' and type mntr and hit enter to see the list of\n            # possibilities.\n            # Leave unset to publish all\n            # 'publish': ''\n\n            # Connection settings\n            'hosts': ['localhost:2181']\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_default_config(self):\n        default_config = super(FlumeCollector, self).get_default_config()\n        default_config['path'] = 'flume'\n        default_config['req_host'] = 'localhost'\n        default_config['req_port'] = 41414\n        default_config['req_path'] = '/metrics'\n        return default_config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the help text for the configuration options for this handler.", "response": "def get_default_config_help(self):\n        \"\"\"\n        Returns the help text for the configuration options for this handler\n        \"\"\"\n        config = super(MultiGraphiteHandler, self).get_default_config_help()\n\n        config.update({\n            'host': 'Hostname, Hostname, Hostname',\n            'port': 'Port',\n            'proto': 'udp or tcp',\n            'timeout': '',\n            'batch': 'How many to store before sending to the graphite server',\n            'max_backlog_multiplier': 'how many batches to store before trimming',  # NOQA\n            'trim_backlog_multiplier': 'Trim down how many batches',\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the default config for the handler", "response": "def get_default_config(self):\n        \"\"\"\n        Return the default config for the handler\n        \"\"\"\n        config = super(MultiGraphiteHandler, self).get_default_config()\n\n        config.update({\n            'host': ['localhost'],\n            'port': 2003,\n            'proto': 'tcp',\n            'timeout': 15,\n            'batch': 1,\n            'max_backlog_multiplier': 5,\n            'trim_backlog_multiplier': 4,\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(self):\n\n        #######################################################################\n        # Config\n        #######################################################################\n        self.config = load_config(self.configfile)\n\n        collectors = load_collectors(self.config['server']['collectors_path'])\n        metric_queue_size = int(self.config['server'].get('metric_queue_size',\n                                                          16384))\n        self.metric_queue = self.manager.Queue(maxsize=metric_queue_size)\n        self.log.debug('metric_queue_size: %d', metric_queue_size)\n\n        #######################################################################\n        # Handlers\n        #\n        # TODO: Eventually move each handler to it's own process space?\n        #######################################################################\n\n        if 'handlers_path' in self.config['server']:\n            handlers_path = self.config['server']['handlers_path']\n\n            # Make an list if not one\n            if isinstance(handlers_path, basestring):\n                handlers_path = handlers_path.split(',')\n                handlers_path = map(str.strip, handlers_path)\n                self.config['server']['handlers_path'] = handlers_path\n\n            load_include_path(handlers_path)\n\n        if 'handlers' not in self.config['server']:\n            self.log.critical('handlers missing from server section in config')\n            sys.exit(1)\n\n        handlers = self.config['server'].get('handlers')\n        if isinstance(handlers, basestring):\n            handlers = [handlers]\n\n        # Prevent the Queue Handler from being a normal handler\n        if 'diamond.handler.queue.QueueHandler' in handlers:\n            handlers.remove('diamond.handler.queue.QueueHandler')\n\n        self.handlers = load_handlers(self.config, handlers)\n\n        QueueHandler = load_dynamic_class(\n            'diamond.handler.queue.QueueHandler',\n            Handler\n        )\n\n        self.handler_queue = QueueHandler(\n            config=self.config, queue=self.metric_queue, log=self.log)\n\n        handlers_process = multiprocessing.Process(\n            name=\"Handlers\",\n            target=handler_process,\n            args=(self.handlers, self.metric_queue, self.log),\n        )\n\n        handlers_process.daemon = True\n        handlers_process.start()\n\n        #######################################################################\n        # Signals\n        #######################################################################\n\n        if hasattr(signal, 'SIGHUP'):\n            signal.signal(signal.SIGHUP, signal_to_exception)\n\n        #######################################################################\n\n        while True:\n            try:\n                active_children = multiprocessing.active_children()\n                running_processes = []\n                for process in active_children:\n                    running_processes.append(process.name)\n                running_processes = set(running_processes)\n\n                ##############################################################\n                # Collectors\n                ##############################################################\n\n                running_collectors = []\n                for collector, config in self.config['collectors'].iteritems():\n                    if config.get('enabled', False) is not True:\n                        continue\n                    running_collectors.append(collector)\n                running_collectors = set(running_collectors)\n\n                # Collectors that are running but shouldn't be\n                for process_name in running_processes - running_collectors:\n                    if 'Collector' not in process_name:\n                        continue\n                    for process in active_children:\n                        if process.name == process_name:\n                            process.terminate()\n\n                collector_classes = dict(\n                    (cls.__name__.split('.')[-1], cls)\n                    for cls in collectors.values()\n                )\n\n                load_delay = self.config['server'].get('collectors_load_delay',\n                                                       1.0)\n                for process_name in running_collectors - running_processes:\n                    # To handle running multiple collectors concurrently, we\n                    # split on white space and use the first word as the\n                    # collector name to spin\n                    collector_name = process_name.split()[0]\n\n                    if 'Collector' not in collector_name:\n                        continue\n\n                    if collector_name not in collector_classes:\n                        self.log.error('Can not find collector %s',\n                                       collector_name)\n                        continue\n\n                    collector = initialize_collector(\n                        collector_classes[collector_name],\n                        name=process_name,\n                        configfile=self.configfile,\n                        handlers=[self.handler_queue])\n\n                    if collector is None:\n                        self.log.error('Failed to load collector %s',\n                                       process_name)\n                        continue\n\n                    # Splay the loads\n                    time.sleep(float(load_delay))\n\n                    process = multiprocessing.Process(\n                        name=process_name,\n                        target=collector_process,\n                        args=(collector, self.metric_queue, self.log)\n                    )\n                    process.daemon = True\n                    process.start()\n\n                if not handlers_process.is_alive():\n                    self.log.error('Handlers process exited')\n                    if (str_to_bool(self.config['server'].get(\n                            'abort_on_handlers_process_exit', 'False'))):\n                        raise Exception('Handlers process exited')\n\n                ##############################################################\n\n                time.sleep(1)\n\n            except SIGHUPException:\n                # ignore further SIGHUPs for now\n                original_sighup_handler = signal.getsignal(signal.SIGHUP)\n                signal.signal(signal.SIGHUP, signal.SIG_IGN)\n\n                self.log.info('Reloading state due to HUP')\n                self.config = load_config(self.configfile)\n                collectors = load_collectors(\n                    self.config['server']['collectors_path'])\n                # restore SIGHUP handler\n                signal.signal(signal.SIGHUP, original_sighup_handler)", "response": "Load handler and collector classes and start collectors\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_default_config(self):\n        config = super(PuppetDashboardCollector, self).get_default_config()\n        config.update({\n            'host': 'localhost',\n            'port': 5678,\n            'path': 'puppetdashboard',\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_default_config(self):\n        config = super(OpenVPNCollector, self).get_default_config()\n        config.update({\n            'path':      'openvpn',\n            'instances': 'file:///var/log/openvpn/status.log',\n            'timeout':   '10',\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_url(self, uri):\n        parsed = urlparse.urlparse(uri)\n        if 'scheme' not in parsed:\n            class Object(object):\n                pass\n            newparsed = Object()\n            newparsed.scheme = parsed[0]\n            newparsed.netloc = parsed[1]\n            newparsed.path = parsed[2]\n            newparsed.params = parsed[3]\n            newparsed.query = parsed[4]\n            newparsed.fragment = parsed[5]\n            newparsed.username = ''\n            newparsed.password = ''\n            newparsed.hostname = ''\n            newparsed.port = ''\n            parsed = newparsed\n        return parsed", "response": "Convert urlparse from a python 2. 4 layout to a python 2. 7 layout"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(ExampleCollector, self).get_default_config()\n        config.update({\n            'path':     'example'\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(IPVSCollector, self).get_default_config()\n        config.update({\n            'bin':              '/usr/sbin/ipvsadm',\n            'use_sudo':         True,\n            'sudo_cmd':         '/usr/bin/sudo',\n            'path':             'ipvs'\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sanitize_word(s):\n    s = re.sub('[^\\w-]+', '_', s)\n    s = re.sub('__+', '_', s)\n    return s.strip('_')", "response": "Remove non - alphanumerical characters from metric word.\n    And trim excessive underscores."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_default_config(self):\n        config = super(JCollectdCollector, self).get_default_config()\n        config.update({\n            'path':     'jvm',\n            'listener_host': '127.0.0.1',\n            'listener_port': 25826,\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_default_config(self):\n        config = super(OpenstackSwiftReconCollector, self).get_default_config()\n        config.update({\n            'path': 'swiftrecon',\n            'recon_account_cache': '/var/cache/swift/account.recon',\n            'recon_container_cache': '/var/cache/swift/container.recon',\n            'recon_object_cache': '/var/cache/swift/object.recon',\n            'interval': 300,\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _process_cache(self, d, path=()):\n        for k, v in d.iteritems():\n            if not isinstance(v, dict):\n                self.metrics.append((path + (k,), v))\n            else:\n                self._process_cache(v, path + (k,))", "response": "Recursively walk a nested recon cache dict to obtain path and values"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef str_to_bool(value):\n    if isinstance(value, basestring):\n        value = value.strip().lower()\n        if value in ['true', 't', 'yes', 'y']:\n            return True\n        elif value in ['false', 'f', 'no', 'n', '']:\n            return False\n        else:\n            raise NotImplementedError(\"Unknown bool %s\" % value)\n\n    return value", "response": "Converts string truthy or falsey strings to a bool\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload the full config file and merge splitted configs if not already loaded", "response": "def load_config(configfile):\n    \"\"\"\n    Load the full config / merge splitted configs if configured\n    \"\"\"\n\n    configfile = os.path.abspath(configfile)\n    config = configobj.ConfigObj(configfile)\n\n    config_extension = '.conf'\n\n    #########################################################################\n    # Load up other config files\n    #########################################################################\n\n    if 'configs' in config:\n        config_extension = config['configs'].get('extension', config_extension)\n\n        # Load other configs\n        if 'path' in config['configs']:\n            for cfgfile in os.listdir(config['configs']['path']):\n                cfgfile = os.path.join(config['configs']['path'],\n                                       cfgfile)\n                cfgfile = os.path.abspath(cfgfile)\n                if not cfgfile.endswith(config_extension):\n                    continue\n                newconfig = configobj.ConfigObj(cfgfile)\n                config.merge(newconfig)\n\n    #########################################################################\n\n    if 'server' not in config:\n        raise Exception('Failed to load config file %s!' % configfile)\n\n    #########################################################################\n    # Load up handler specific configs\n    #########################################################################\n\n    if 'handlers' not in config:\n        config['handlers'] = configobj.ConfigObj()\n\n    if 'handlers_config_path' in config['server']:\n        handlers_config_path = config['server']['handlers_config_path']\n        if os.path.exists(handlers_config_path):\n            for cfgfile in os.listdir(handlers_config_path):\n                cfgfile = os.path.join(handlers_config_path, cfgfile)\n                cfgfile = os.path.abspath(cfgfile)\n                if not cfgfile.endswith(config_extension):\n                    continue\n                filename = os.path.basename(cfgfile)\n                handler = os.path.splitext(filename)[0]\n\n                if handler not in config['handlers']:\n                    config['handlers'][handler] = configobj.ConfigObj()\n\n                newconfig = configobj.ConfigObj(cfgfile)\n                config['handlers'][handler].merge(newconfig)\n\n    #########################################################################\n    # Load up Collector specific configs\n    #########################################################################\n\n    if 'collectors' not in config:\n        config['collectors'] = configobj.ConfigObj()\n\n    if 'collectors_config_path' in config['server']:\n        collectors_config_path = config['server']['collectors_config_path']\n        if os.path.exists(collectors_config_path):\n            for cfgfile in os.listdir(collectors_config_path):\n                cfgfile = os.path.join(collectors_config_path, cfgfile)\n                cfgfile = os.path.abspath(cfgfile)\n                if not cfgfile.endswith(config_extension):\n                    continue\n                filename = os.path.basename(cfgfile)\n                collector = os.path.splitext(filename)[0]\n\n                if collector not in config['collectors']:\n                    config['collectors'][collector] = configobj.ConfigObj()\n\n                try:\n                    newconfig = configobj.ConfigObj(cfgfile)\n                except Exception as e:\n                    raise Exception(\"Failed to load config file %s due to %s\" %\n                                    (cfgfile, e))\n\n                config['collectors'][collector].merge(newconfig)\n\n    # Convert enabled to a bool\n    for collector in config['collectors']:\n        if 'enabled' in config['collectors'][collector]:\n            config['collectors'][collector]['enabled'] = str_to_bool(\n                config['collectors'][collector]['enabled']\n            )\n\n    #########################################################################\n\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_default_config(self):\n        config = super(MesosCollector, self).get_default_config()\n        config.update({\n            'host': 'localhost',\n            'port': 5050,\n            'path': 'mesos',\n            'master': True\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes cpu usage based on cpu time spent compared to elapsed time", "response": "def _add_cpu_usage(self, cur_read):\n        \"\"\"Compute cpu usage based on cpu time spent compared to elapsed time\n        \"\"\"\n\n        for executor_id, cur_data in cur_read.items():\n            if executor_id in self.executors_prev_read:\n                prev_data = self.executors_prev_read[executor_id]\n                prev_stats = prev_data['statistics']\n                cur_stats = cur_data['statistics']\n                # from sum of current cpus time subtract previous sum\n                cpus_time_diff_s = cur_stats['cpus_user_time_secs']\n                cpus_time_diff_s += cur_stats['cpus_system_time_secs']\n                cpus_time_diff_s -= prev_stats['cpus_user_time_secs']\n                cpus_time_diff_s -= prev_stats['cpus_system_time_secs']\n                ts_diff = cur_stats['timestamp'] - prev_stats['timestamp']\n                if ts_diff != 0:\n                    cur_stats['cpus_utilisation'] = cpus_time_diff_s / ts_diff\n\n            self.executors_prev_read[executor_id] = cur_read[executor_id]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing cpu percent based on the provided utilisation", "response": "def _add_cpu_percent(self, cur_read):\n        \"\"\"Compute cpu percent basing on the provided utilisation\n        \"\"\"\n        for executor_id, cur_data in cur_read.items():\n            stats = cur_data['statistics']\n            cpus_limit = stats.get('cpus_limit')\n            cpus_utilisation = stats.get('cpus_utilisation')\n            if cpus_utilisation and cpus_limit != 0:\n                stats['cpus_percent'] = cpus_utilisation / cpus_limit"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the memory percent utilisation based on the mem_rss_bytes and mem_limit_bytes.", "response": "def _add_mem_percent(self, cur_read):\n        \"\"\"Compute memory percent utilisation based on the\n        mem_rss_bytes and mem_limit_bytes\n        \"\"\"\n        for executor_id, cur_data in cur_read.items():\n            stats = cur_data['statistics']\n            mem_rss_bytes = stats.get('mem_rss_bytes')\n            mem_limit_bytes = stats.get('mem_limit_bytes')\n            if mem_rss_bytes and mem_limit_bytes != 0:\n                stats['mem_percent'] = mem_rss_bytes / float(mem_limit_bytes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _group_and_publish_tasks_statistics(self, result):\n        for i in result:\n            executor_id = i['executor_id']\n            i['executor_id'] = executor_id[:executor_id.rfind('.')]\n            i['statistics']['instances_count'] = 1\n\n        r = {}\n        for i in result:\n            executor_id = i['executor_id']\n            r[executor_id] = r.get(executor_id, {})\n            r[executor_id]['framework_id'] = i['framework_id']\n            r[executor_id]['statistics'] = r[executor_id].get('statistics', {})\n            r[executor_id]['statistics'] = self._sum_statistics(\n                i['statistics'], r[executor_id]['statistics'])\n\n        self._add_cpu_usage(r)\n        self._add_cpu_percent(r)\n        self._add_mem_percent(r)\n        self._publish(r)", "response": "This function group statistics of same tasks by adding them. It also adds statistics about the tasks that are running on the server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get(self, path):\n        url = self._get_url(path)\n        try:\n            response = urllib2.urlopen(url)\n        except Exception as err:\n            self.log.error(\"%s: %s\", url, err)\n            return False\n\n        try:\n            doc = json.load(response)\n        except (TypeError, ValueError):\n            self.log.error(\"Unable to parse response from Mesos as a\"\n                           \" json object\")\n            return False\n\n        return doc", "response": "Execute a Mesos API call."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(BeanstalkdCollector, self).get_default_config()\n        config.update({\n            'path':     'beanstalkd',\n            'host':     'localhost',\n            'port':     11300,\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_default_config(self):\n        config = super(UDPCollector, self).get_default_config()\n        config.update({\n            'path':          'udp',\n            'allowed_names': 'InDatagrams, NoPorts, InErrors, ' +\n                             'OutDatagrams, RcvbufErrors, SndbufErrors'\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(InterruptCollector, self).get_default_config()\n        config.update({\n            'path':     'interrupts'\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncollecting interrupt data from the PROC file and publish the interrupt data.", "response": "def collect(self):\n        \"\"\"\n        Collect interrupt data\n        \"\"\"\n        if not os.access(self.PROC, os.R_OK):\n            return False\n\n        # Open PROC file\n        file = open(self.PROC, 'r')\n        # Get data\n        cpuCount = None\n        for line in file:\n            if not cpuCount:\n                cpuCount = len(line.split())\n            else:\n                data = line.strip().split(None, cpuCount + 2)\n                data[0] = data[0].replace(':', '')\n\n                if len(data) == 2:\n                    metric_name = data[0]\n                    metric_value = data[1]\n                    self.publish(metric_name,\n                                 self.derivative(metric_name,\n                                                 long(metric_value),\n                                                 counter))\n                else:\n                    if len(data[0]) == cpuCount + 1:\n                        metric_name = data[0] + '.'\n                    elif len(data[0]) == 3:\n                        metric_name = (\n                            ((data[-2] + ' ' +\n                              data[-1]).replace(' ', '_')) + '.')\n                    else:\n                        metric_name = (\n                            ((data[-2]).replace(' ', '_')) +\n                            '.' +\n                            ((data[-1]).replace(', ', '-').replace(' ', '_')) +\n                            '.' + data[0] + '.')\n                    total = 0\n                    for index, value in enumerate(data):\n                        if index == 0 or index >= cpuCount + 1:\n                            continue\n\n                        metric_name_node = metric_name + 'CPU' + str(index - 1)\n                        value = int(self.derivative(metric_name_node,\n                                                    long(value), counter))\n                        total += value\n                        self.publish(metric_name_node, value)\n\n                    # Roll up value\n                    metric_name_node = metric_name + 'total'\n                    self.publish(metric_name_node, total)\n\n        # Close file\n        file.close()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_default_config_help(self):\n        config_help = super(PostgresqlCollector,\n                            self).get_default_config_help()\n        config_help.update({\n            'host': 'Hostname',\n            'dbname': 'DB to connect to in order to get list of DBs in PgSQL',\n            'user': 'Username',\n            'password': 'Password',\n            'port': 'Port number',\n            'password_provider': \"Whether to auth with supplied password or\"\n            \" .pgpass file  <password|pgpass>\",\n            'sslmode': 'Whether to use SSL - <disable|allow|require|...>',\n            'underscore': 'Convert _ to .',\n            'extended': 'Enable collection of extended database stats.',\n            'metrics': 'List of enabled metrics to collect',\n            'pg_version': \"The version of postgres that you'll be monitoring\"\n            \" eg. in format 9.2\",\n            'has_admin': 'Admin privileges are required to execute some'\n            ' queries.',\n        })\n        return config_help", "response": "Return help text for collector\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncollects all known databases and publish them", "response": "def collect(self):\n        \"\"\"\n        Do pre-flight checks, get list of db names, collect metrics, publish\n        \"\"\"\n        if psycopg2 is None:\n            self.log.error('Unable to import module psycopg2')\n            return {}\n\n        # Get list of databases\n        dbs = self._get_db_names()\n        if len(dbs) == 0:\n            self.log.error(\"I have 0 databases!\")\n            return {}\n\n        if self.config['metrics']:\n            metrics = self.config['metrics']\n        elif str_to_bool(self.config['extended']):\n            metrics = registry['extended']\n            if str_to_bool(self.config['has_admin']) \\\n                    and 'WalSegmentStats' not in metrics:\n                metrics.append('WalSegmentStats')\n\n        else:\n            metrics = registry['basic']\n\n        # Iterate every QueryStats class\n        for metric_name in set(metrics):\n            if metric_name not in metrics_registry:\n                self.log.error(\n                    'metric_name %s not found in metric registry' % metric_name)\n                continue\n\n            for dbase in dbs:\n                conn = self._connect(database=dbase)\n                try:\n                    klass = metrics_registry[metric_name]\n                    stat = klass(dbase, conn,\n                                 underscore=self.config['underscore'])\n                    stat.fetch(self.config['pg_version'])\n                    for metric, value in stat:\n                        if value is not None:\n                            self.publish(metric, value)\n\n                    # Setting multi_db to True will run this query on all known\n                    # databases. This is bad for queries that hit views like\n                    # pg_database, which are shared across databases.\n                    #\n                    # If multi_db is False, bail early after the first query\n                    # iteration. Otherwise, continue to remaining databases.\n                    if stat.multi_db is False:\n                        break\n                finally:\n                    conn.close()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_db_names(self):\n        query = \"\"\"\n            SELECT datname FROM pg_database\n            WHERE datallowconn AND NOT datistemplate\n            AND NOT datname='postgres' AND NOT datname='rdsadmin' ORDER BY 1\n        \"\"\"\n        conn = self._connect(self.config['dbname'])\n        cursor = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)\n        cursor.execute(query)\n        datnames = [d['datname'] for d in cursor.fetchall()]\n        conn.close()\n\n        # Exclude `postgres` database list, unless it is the\n        # only database available (required for querying pg_stat_database)\n        if not datnames:\n            datnames = ['postgres']\n\n        return datnames", "response": "Try to get a list of db names from pg_stat_database"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconnecting to given database and return connection object.", "response": "def _connect(self, database=None):\n        \"\"\"\n        Connect to given database\n        \"\"\"\n        conn_args = {\n            'host': self.config['host'],\n            'user': self.config['user'],\n            'password': self.config['password'],\n            'port': self.config['port'],\n            'sslmode': self.config['sslmode'],\n        }\n\n        if database:\n            conn_args['database'] = database\n        else:\n            conn_args['database'] = 'postgres'\n\n        # libpq will use ~/.pgpass only if no password supplied\n        if self.config['password_provider'] == 'pgpass':\n            del conn_args['password']\n\n        try:\n            conn = psycopg2.connect(**conn_args)\n        except Exception as e:\n            self.log.error(e)\n            raise e\n\n        # Avoid using transactions, set isolation level to autocommit\n        conn.set_isolation_level(0)\n        return conn"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(PingCollector, self).get_default_config()\n        config.update({\n            'path':             'ping',\n            'bin':              '/bin/ping',\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_default_config(self):\n        config = super(IODriveSNMPCollector, self).get_default_config()\n        config.update({\n            'path':     'iodrive',\n            'timeout':  15,\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_string_index_oid(self, s):\n        return (len(self.get_bytes(s)), ) + self.get_bytes(s)", "response": "Turns a string into an oid format is length of name followed by name chars in ascii"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef collect_snmp(self, device, host, port, community):\n\n        # Set timestamp\n        timestamp = time.time()\n\n        for k, v in self.IODRIVE_STATS.items():\n            # Get Metric Name and Value\n            metricName = '.'.join([k])\n            metricValue = int(self.get(v, host, port, community)[v])\n\n            # Get Metric Path\n            metricPath = '.'.join(['servers', host, device, metricName])\n\n            # Create Metric\n            metric = Metric(metricPath, metricValue, timestamp, 0)\n\n            # Publish Metric\n            self.publish_metric(metric)\n\n        for k, v in self.IODRIVE_BYTE_STATS.items():\n            # Get Metric Name and Value\n            metricName = '.'.join([k])\n            metricValue = int(self.get(v, host, port, community)[v])\n\n            # Get Metric Path\n            metricPath = '.'.join(['servers', host, device, metricName])\n\n            # Create Metric\n            metric = Metric(metricPath, metricValue, timestamp, 0)\n\n            # Publish Metric\n            self.publish_metric(metric)", "response": "Collect Fusion IO Drive SNMP stats from device\n            and publish them to the metric store."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(DropwizardCollector, self).get_default_config()\n        config.update({\n            'host':     '127.0.0.1',\n            'port':     8081,\n            'path':     'dropwizard',\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_default_config_help(self):\n        config = super(GraphiteHandler, self).get_default_config_help()\n\n        config.update({\n            'host': 'Hostname',\n            'port': 'Port',\n            'proto': 'udp, udp4, udp6, tcp, tcp4, or tcp6',\n            'timeout': '',\n            'batch': 'How many to store before sending to the graphite server',\n            'max_backlog_multiplier': 'how many batches to store before trimming',  # NOQA\n            'trim_backlog_multiplier': 'Trim down how many batches',\n            'keepalive': 'Enable keepalives for tcp streams',\n            'keepaliveinterval': 'How frequently to send keepalives',\n            'flow_info': 'IPv6 Flow Info',\n            'scope_id': 'IPv6 Scope ID',\n            'reconnect_interval': 'How often (seconds) to reconnect to '\n                                  'graphite. Default (0) is never',\n        })\n\n        return config", "response": "Returns the help text for the configuration options for this handler."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the default config for the handler", "response": "def get_default_config(self):\n        \"\"\"\n        Return the default config for the handler\n        \"\"\"\n        config = super(GraphiteHandler, self).get_default_config()\n\n        config.update({\n            'host': 'localhost',\n            'port': 2003,\n            'proto': 'tcp',\n            'timeout': 15,\n            'batch': 1,\n            'max_backlog_multiplier': 5,\n            'trim_backlog_multiplier': 4,\n            'keepalive': 0,\n            'keepaliveinterval': 10,\n            'flow_info': 0,\n            'scope_id': 0,\n            'reconnect_interval': 0,\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _send_data(self, data):\n        try:\n            self.socket.sendall(data)\n            self._reset_errors()\n        except:\n            self._close()\n            self._throttle_error(\"GraphiteHandler: Socket error, \"\n                                 \"trying reconnect.\")\n            self._connect()\n            try:\n                self.socket.sendall(data)\n            except:\n                return\n            self._reset_errors()", "response": "Send data to the socket."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _send(self):\n        # Check to see if we have a valid socket. If not, try to connect.\n        try:\n            try:\n                if self.socket is None:\n                    self.log.debug(\"GraphiteHandler: Socket is not connected. \"\n                                   \"Reconnecting.\")\n                    self._connect()\n                if self.socket is None:\n                    self.log.debug(\"GraphiteHandler: Reconnect failed.\")\n                else:\n                    # Send data to socket\n                    self._send_data(''.join(self.metrics))\n                    self.metrics = []\n                    if self._time_to_reconnect():\n                        self._close()\n            except Exception:\n                self._close()\n                self._throttle_error(\"GraphiteHandler: Error sending metrics.\")\n                raise\n        finally:\n            if len(self.metrics) >= (\n                    self.batch_size * self.max_backlog_multiplier):\n                trim_offset = (self.batch_size *\n                               self.trim_backlog_multiplier * -1)\n                self.log.warn('GraphiteHandler: Trimming backlog. Removing' +\n                              ' oldest %d and keeping newest %d metrics',\n                              len(self.metrics) - abs(trim_offset),\n                              abs(trim_offset))\n                self.metrics = self.metrics[trim_offset:]", "response": "Send data to graphite."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _connect(self):\n        if (self.proto == 'udp'):\n            stream = socket.SOCK_DGRAM\n        else:\n            stream = socket.SOCK_STREAM\n\n        if (self.proto[-1] == '4'):\n            family = socket.AF_INET\n            connection_struct = (self.host, self.port)\n        elif (self.proto[-1] == '6'):\n            family = socket.AF_INET6\n            connection_struct = (self.host, self.port,\n                                 self.flow_info, self.scope_id)\n        else:\n            connection_struct = (self.host, self.port)\n            try:\n                addrinfo = socket.getaddrinfo(self.host, self.port, 0, stream)\n            except socket.gaierror as ex:\n                self.log.error(\"GraphiteHandler: Error looking up graphite host\"\n                               \" '%s' - %s\",\n                               self.host, ex)\n                return\n            if (len(addrinfo) > 0):\n                family = addrinfo[0][0]\n                if (family == socket.AF_INET6):\n                    connection_struct = (self.host, self.port,\n                                         self.flow_info, self.scope_id)\n            else:\n                family = socket.AF_INET\n\n        # Create socket\n        self.socket = socket.socket(family, stream)\n        if self.socket is None:\n            # Log Error\n            self.log.error(\"GraphiteHandler: Unable to create socket.\")\n            # Close Socket\n            self._close()\n            return\n        # Enable keepalives?\n        if self.proto != 'udp' and self.keepalive:\n            self.log.error(\"GraphiteHandler: Setting socket keepalives...\")\n            self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)\n            self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPIDLE,\n                                   self.keepaliveinterval)\n            self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPINTVL,\n                                   self.keepaliveinterval)\n            self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPCNT, 3)\n        # Set socket timeout\n        self.socket.settimeout(self.timeout)\n        # Connect to graphite server\n        try:\n            self.socket.connect(connection_struct)\n            # Log\n            self.log.debug(\"GraphiteHandler: Established connection to \"\n                           \"graphite server %s:%d.\",\n                           self.host, self.port)\n            self.last_connect_timestamp = time.time()\n        except Exception as ex:\n            # Log Error\n            self._throttle_error(\"GraphiteHandler: Failed to connect to \"\n                                 \"%s:%i. %s.\", self.host, self.port, ex)\n            # Close Socket\n            self._close()\n            return", "response": "Connect to the graphite server and return the unique ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_default_config(self):\n        config = super(KSMCollector, self).get_default_config()\n        config.update({\n            'path': 'ksm',\n            'ksm_path': '/sys/kernel/mm/ksm'})\n        return config", "response": "Return default config.\n\n        path: Graphite path output\n        ksm_path: location where KSM kernel data can be found"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses a metric by doing nothing", "response": "def process(self, metric):\n        \"\"\"\n        Process a metric by doing nothing\n        \"\"\"\n        self.log.debug(\"Metric: %s\", str(metric).rstrip().replace(' ', '\\t'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_default_config_help(self):\n        config = super(NullHandler, self).get_default_config_help()\n\n        config.update({\n        })\n\n        return config", "response": "Returns the help text for the configuration options for this handler\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_default_config(self):\n        config = super(NullHandler, self).get_default_config()\n\n        config.update({\n        })\n\n        return config", "response": "Returns the default config for the handler\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(SidekiqCollector, self).get_default_config()\n        config.update({\n            'path': 'sidekiq',\n            'host': 'localhost',\n            'ports': '6379',\n            'password': None,\n            'databases': 16,\n            'sentinel_ports': None,\n            'sentinel_name': None,\n            'cluster_prefix': None\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_master(self, host, port, sentinel_port, sentinel_name):\n        if sentinel_port and sentinel_name:\n            master = Sentinel([(host, sentinel_port)], socket_timeout=1)\\\n                .discover_master(sentinel_name)\n            return master\n        return host, port", "response": "Get master ip and port from Sentinel"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a Redis client for the current set of cache entries.", "response": "def get_redis_client(self):\n        \"\"\"\n        :return: Redis client\n        \"\"\"\n        host = self.config['host']\n        ports = self.config['ports']\n        sentinel_ports = self.config['sentinel_ports']\n        sentinel_name = self.config['sentinel_name']\n        password = self.config['password']\n        databases = self.config['databases']\n\n        if not isinstance(ports, list):\n            ports = [ports]\n\n        if not isinstance(sentinel_ports, list):\n            sentinel_ports = [sentinel_ports]\n\n        if sentinel_ports:\n            assert len(sentinel_ports) == len(ports)\n        else:\n            sentinel_ports = [None for _ in xrange(len(ports))]\n\n        for port, sentinel_port in izip(ports, sentinel_ports):\n            for db in xrange(0, int(databases)):\n                master = self.get_master(\n                    host, port, sentinel_port, sentinel_name\n                )\n                pool = redis.ConnectionPool(\n                    host=master[0], port=int(master[1]),\n                    password=password, db=db\n                )\n                yield redis.Redis(connection_pool=pool), port, db"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef collect(self):\n        if redis is None:\n            self.log.error('Unable to import module redis')\n            return {}\n\n        try:\n            for redis_client, port, db in self.get_redis_client():\n                try:\n                    self.publish_queue_length(redis_client, port, db)\n                    self.publish_schedule_length(redis_client, port, db)\n                    self.publish_retry_length(redis_client, port, db)\n                except Exception as execption:\n                    self.log.error(execption)\n        except Exception as execption:\n            self.log.error(execption)", "response": "Collect the Sidekiq metrics from Redis"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef publish_schedule_length(self, redis_client, port, db):\n        schedule_length = redis_client.zcard('schedule')\n        self.__publish(port, db, 'schedule', schedule_length)", "response": "Publish the schedule length of the current node"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef publish_retry_length(self, redis_client, port, db):\n        retry_length = redis_client.zcard('retry')\n        self.__publish(port, db, 'retry', retry_length)", "response": ":param redis_client: Redis client\n        :param db: Redis Database index\n        :param port: Redis port\n        :return: Redis schedule length"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef publish_queue_length(self, redis_client, port, db):\n        for queue in redis_client.smembers('queues'):\n            queue_length = redis_client.llen('queue:%s' % queue)\n            self.__publish(port, db, queue, queue_length)", "response": "Publish the length of all the elements in the Redis queues in the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npublish the number of messages in the queue.", "response": "def __publish(self, port, db, queue, queue_length):\n        \"\"\"\n        :param port: Redis port\n        :param db: Redis db index to report\n        :param queue: Queue name to report\n        :param queue_length: Queue length to report\n        :return:\n        \"\"\"\n        metric_name_segaments = ['queue']\n        cluster = self.config['cluster_prefix']\n        if cluster:\n            metric_name_segaments.append(cluster)\n        metric_name_segaments.append(port)\n        metric_name_segaments.append(str(db))\n        metric_name_segaments.append(queue)\n        self.publish_gauge(\n            name='.'.join(metric_name_segaments), value=queue_length\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_default_config(self):\n        config = super(MemcachedCollector, self).get_default_config()\n        config.update({\n            'path':     'memcached',\n\n            # Which rows of 'status' you would like to publish.\n            # 'telnet host port' and type stats and hit enter to see the list of\n            # possibilities.\n            # Leave unset to publish all\n            # 'publish': ''\n\n            # Connection settings\n            'hosts': ['localhost:11211']\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_hostname(config, method=None):\n    method = method or config.get('hostname_method', 'smart')\n\n    # case insensitive method\n    method = method.lower()\n\n    if 'hostname' in config and method != 'shell':\n        return config['hostname']\n\n    if method in get_hostname.cached_results:\n        return get_hostname.cached_results[method]\n\n    if method == 'shell':\n        if 'hostname' not in config:\n            raise DiamondException(\n                \"hostname must be set to a shell command for\"\n                \" hostname_method=shell\")\n        else:\n            proc = subprocess.Popen(config['hostname'],\n                                    shell=True,\n                                    stdout=subprocess.PIPE)\n            hostname = proc.communicate()[0].strip()\n            if proc.returncode != 0:\n                raise subprocess.CalledProcessError(proc.returncode,\n                                                    config['hostname'])\n            get_hostname.cached_results[method] = hostname\n            return hostname\n\n    if method == 'smart':\n        hostname = get_hostname(config, 'fqdn_short')\n        if hostname != 'localhost':\n            get_hostname.cached_results[method] = hostname\n            return hostname\n        hostname = get_hostname(config, 'hostname_short')\n        get_hostname.cached_results[method] = hostname\n        return hostname\n\n    if method == 'fqdn_short':\n        hostname = socket.getfqdn().split('.')[0]\n        get_hostname.cached_results[method] = hostname\n        if hostname == '':\n            raise DiamondException('Hostname is empty?!')\n        return hostname\n\n    if method == 'fqdn':\n        hostname = socket.getfqdn().replace('.', '_')\n        get_hostname.cached_results[method] = hostname\n        if hostname == '':\n            raise DiamondException('Hostname is empty?!')\n        return hostname\n\n    if method == 'fqdn_rev':\n        hostname = socket.getfqdn().split('.')\n        hostname.reverse()\n        hostname = '.'.join(hostname)\n        get_hostname.cached_results[method] = hostname\n        if hostname == '':\n            raise DiamondException('Hostname is empty?!')\n        return hostname\n\n    if method == 'uname_short':\n        hostname = os.uname()[1].split('.')[0]\n        get_hostname.cached_results[method] = hostname\n        if hostname == '':\n            raise DiamondException('Hostname is empty?!')\n        return hostname\n\n    if method == 'uname_rev':\n        hostname = os.uname()[1].split('.')\n        hostname.reverse()\n        hostname = '.'.join(hostname)\n        get_hostname.cached_results[method] = hostname\n        if hostname == '':\n            raise DiamondException('Hostname is empty?!')\n        return hostname\n\n    if method == 'hostname':\n        hostname = socket.gethostname()\n        get_hostname.cached_results[method] = hostname\n        if hostname == '':\n            raise DiamondException('Hostname is empty?!')\n        return hostname\n\n    if method == 'hostname_short':\n        hostname = socket.gethostname().split('.')[0]\n        get_hostname.cached_results[method] = hostname\n        if hostname == '':\n            raise DiamondException('Hostname is empty?!')\n        return hostname\n\n    if method == 'hostname_rev':\n        hostname = socket.gethostname().split('.')\n        hostname.reverse()\n        hostname = '.'.join(hostname)\n        get_hostname.cached_results[method] = hostname\n        if hostname == '':\n            raise DiamondException('Hostname is empty?!')\n        return hostname\n\n    if method == 'none':\n        get_hostname.cached_results[method] = None\n        return None\n\n    raise NotImplementedError(config['hostname_method'])", "response": "Returns a hostname as configured by the user"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads a configfile or reload if previously given one.", "response": "def load_config(self, configfile=None, override_config=None):\n        \"\"\"\n        Process a configfile, or reload if previously given one.\n        \"\"\"\n        self.config = configobj.ConfigObj()\n\n        # Load in the collector's defaults\n        if self.get_default_config() is not None:\n            self.config.merge(self.get_default_config())\n\n        if configfile is not None:\n            self.configfile = os.path.abspath(configfile)\n\n        if self.configfile is not None:\n            config = load_config(self.configfile)\n\n            if 'collectors' in config:\n                if 'default' in config['collectors']:\n                    self.config.merge(config['collectors']['default'])\n\n                if self.name in config['collectors']:\n                    self.config.merge(config['collectors'][self.name])\n\n        if override_config is not None:\n            if 'collectors' in override_config:\n                if 'default' in override_config['collectors']:\n                    self.config.merge(override_config['collectors']['default'])\n\n                if self.name in override_config['collectors']:\n                    self.config.merge(override_config['collectors'][self.name])\n\n        self.process_config()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprocess the configuration file and update the internal state of the object.", "response": "def process_config(self):\n        \"\"\"\n        Intended to put any code that should be run after any config reload\n        event\n        \"\"\"\n        if 'byte_unit' in self.config:\n            if isinstance(self.config['byte_unit'], basestring):\n                self.config['byte_unit'] = self.config['byte_unit'].split()\n\n        if 'enabled' in self.config:\n            self.config['enabled'] = str_to_bool(self.config['enabled'])\n\n        if 'measure_collector_time' in self.config:\n            self.config['measure_collector_time'] = str_to_bool(\n                self.config['measure_collector_time'])\n\n        # Raise an error if both whitelist and blacklist are specified\n        if ((self.config.get('metrics_whitelist', None) and\n             self.config.get('metrics_blacklist', None))):\n            raise DiamondException(\n                'Both metrics_whitelist and metrics_blacklist specified ' +\n                'in file %s' % self.configfile)\n\n        if self.config.get('metrics_whitelist', None):\n            self.config['metrics_whitelist'] = re.compile(\n                self.config['metrics_whitelist'])\n        elif self.config.get('metrics_blacklist', None):\n            self.config['metrics_blacklist'] = re.compile(\n                self.config['metrics_blacklist'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_metric_path(self, name, instance=None):\n        if 'path' in self.config:\n            path = self.config['path']\n        else:\n            path = self.__class__.__name__\n\n        if instance is not None:\n            if 'instance_prefix' in self.config:\n                prefix = self.config['instance_prefix']\n            else:\n                prefix = 'instances'\n            if path == '.':\n                return '.'.join([prefix, instance, name])\n            else:\n                return '.'.join([prefix, instance, path, name])\n\n        if 'path_prefix' in self.config:\n            prefix = self.config['path_prefix']\n        else:\n            prefix = 'systems'\n\n        if 'path_suffix' in self.config:\n            suffix = self.config['path_suffix']\n        else:\n            suffix = None\n\n        hostname = get_hostname(self.config)\n        if hostname is not None:\n            if prefix:\n                prefix = \".\".join((prefix, hostname))\n            else:\n                prefix = hostname\n\n        # if there is a suffix, add after the hostname\n        if suffix:\n            prefix = '.'.join((prefix, suffix))\n\n        is_path_invalid = path == '.' or not path\n\n        if is_path_invalid and prefix:\n            return '.'.join([prefix, name])\n        elif prefix:\n            return '.'.join([prefix, path, name])\n        elif is_path_invalid:\n            return name\n        else:\n            return '.'.join([path, name])", "response": "Get the metric path."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npublish a metric with the given name and value.", "response": "def publish(self, name, value, raw_value=None, precision=0,\n                metric_type='GAUGE', instance=None):\n        \"\"\"\n        Publish a metric with the given name\n        \"\"\"\n        # Check whitelist/blacklist\n        if self.config['metrics_whitelist']:\n            if not self.config['metrics_whitelist'].match(name):\n                return\n        elif self.config['metrics_blacklist']:\n            if self.config['metrics_blacklist'].match(name):\n                return\n\n        # Get metric Path\n        path = self.get_metric_path(name, instance=instance)\n\n        # Get metric TTL\n        ttl = float(self.config['interval']) * float(\n            self.config['ttl_multiplier'])\n\n        # Create Metric\n        try:\n            metric = Metric(path, value, raw_value=raw_value, timestamp=None,\n                            precision=precision, host=self.get_hostname(),\n                            metric_type=metric_type, ttl=ttl)\n        except DiamondException:\n            self.log.error(('Error when creating new Metric: path=%r, '\n                            'value=%r'), path, value)\n            raise\n\n        # Publish Metric\n        self.publish_metric(metric)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef derivative(self, name, new, max_value=0,\n                   time_delta=True, interval=None,\n                   allow_negative=False, instance=None):\n        \"\"\"\n        Calculate the derivative of the metric.\n        \"\"\"\n        # Format Metric Path\n        path = self.get_metric_path(name, instance=instance)\n\n        if path in self.last_values:\n            old = self.last_values[path]\n            # Check for rollover\n            if new < old:\n                old = old - max_value\n            # Get Change in X (value)\n            derivative_x = new - old\n\n            # If we pass in a interval, use it rather then the configured one\n            if interval is None:\n                interval = float(self.config['interval'])\n\n            # Get Change in Y (time)\n            if time_delta:\n                derivative_y = interval\n            else:\n                derivative_y = 1\n\n            result = float(derivative_x) / float(derivative_y)\n            if result < 0 and not allow_negative:\n                result = 0\n        else:\n            result = 0\n\n        # Store Old Value\n        self.last_values[path] = new\n\n        # Return result\n        return result", "response": "Calculate the derivative of a metric."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _run(self):\n        try:\n            start_time = time.time()\n\n            # Collect Data\n            self.collect()\n\n            end_time = time.time()\n            collector_time = int((end_time - start_time) * 1000)\n\n            self.log.debug('Collection took %s ms', collector_time)\n\n            if 'measure_collector_time' in self.config:\n                if self.config['measure_collector_time']:\n                    metric_name = 'collector_time_ms'\n                    metric_value = collector_time\n                    self.publish(metric_name, metric_value)\n        finally:\n            # After collector run, invoke a flush\n            # method on each handler.\n            for handler in self.handlers:\n                handler._flush()", "response": "Run the collector if it s already running"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_binary(self, binary):\n        if os.path.exists(binary):\n            return binary\n\n        # Extract out the filename if we were given a full path\n        binary_name = os.path.basename(binary)\n\n        # Gather $PATH\n        search_paths = os.environ['PATH'].split(':')\n\n        # Extra paths to scan...\n        default_paths = [\n            '/usr/bin',\n            '/bin'\n            '/usr/local/bin',\n            '/usr/sbin',\n            '/sbin'\n            '/usr/local/sbin',\n        ]\n\n        for path in default_paths:\n            if path not in search_paths:\n                search_paths.append(path)\n\n        for path in search_paths:\n            if os.path.isdir(path):\n                filename = os.path.join(path, binary_name)\n                if os.path.exists(filename):\n                    return filename\n\n        return binary", "response": "Scan and return the first path to a binary that we can find."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_default_config(self):\n        config = super(ProcessCollector, self).get_default_config()\n        config.update({\n            'use_sudo':     False,\n            'sudo_cmd':     self.find_binary('/usr/bin/sudo'),\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_default_config(self):\n        config = super(HadoopCollector, self).get_default_config()\n        config.update({\n            'path':      'hadoop',\n            'metrics':   ['/var/log/hadoop/*-metrics.out'],\n            'truncate':  False,\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert output from memcached s stats slabs into a Python dict.", "response": "def parse_slab_stats(slab_stats):\n    \"\"\"Convert output from memcached's `stats slabs` into a Python dict.\n\n    Newlines are returned by memcached along with carriage returns\n    (i.e. '\\r\\n').\n\n    >>> parse_slab_stats(\n            \"STAT 1:chunk_size 96\\r\\nSTAT 1:chunks_per_page 10922\\r\\nSTAT \"\n            \"active_slabs 1\\r\\nSTAT total_malloced 1048512\\r\\nEND\\r\\n\")\n    {\n        'slabs': {\n            1: {\n                'chunk_size': 96,\n                'chunks_per_page': 10922,\n                # ...\n            },\n        },\n        'active_slabs': 1,\n        'total_malloced': 1048512,\n    }\n    \"\"\"\n    stats_dict = {'slabs': defaultdict(lambda: {})}\n\n    for line in slab_stats.splitlines():\n        if line == 'END':\n            break\n        # e.g.: \"STAT 1:chunks_per_page 10922\"\n        cmd, key, value = line.split(' ')\n        if cmd != 'STAT':\n            continue\n        # e.g.: \"STAT active_slabs 1\"\n        if \":\" not in key:\n            stats_dict[key] = int(value)\n            continue\n        slab, key = key.split(':')\n        stats_dict['slabs'][int(slab)][key] = int(value)\n\n    return stats_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dict_to_paths(dict_):\n    metrics = {}\n    for k, v in dict_.iteritems():\n        if isinstance(v, dict):\n            submetrics = dict_to_paths(v)\n            for subk, subv in submetrics.iteritems():\n                metrics['.'.join([str(k), str(subk)])] = subv\n        else:\n            metrics[k] = v\n    return metrics", "response": "Convert a dict to metric paths."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve slab stats from memcached.", "response": "def get_slab_stats(self):\n        \"\"\"Retrieve slab stats from memcached.\"\"\"\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        s.connect((self.host, self.port))\n        s.send(\"stats slabs\\n\")\n        try:\n            data = \"\"\n            while True:\n                data += s.recv(4096)\n                if data.endswith('END\\r\\n'):\n                    break\n            return data\n        finally:\n            s.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef push(self, metric_name=None, metric_value=None, volume=None):\n\n        graphite_path = self.path_prefix\n        graphite_path += '.' + self.device + '.' + 'volume'\n        graphite_path += '.' + volume + '.' + metric_name\n\n        metric = Metric(graphite_path, metric_value, precision=4,\n                        host=self.device)\n\n        self.publish_metric(metric)", "response": "Pushes a metric to graphite"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_netapp_data(self):\n\n        netapp_data = self.server.invoke('volume-list-info')\n\n        if netapp_data.results_status() == 'failed':\n            self.log.error(\n                'While using netapp API failed to retrieve '\n                'volume-list-info for netapp filer %s' % self.device)\n            return\n\n        netapp_xml = ET.fromstring(netapp_data.sprintf()).find('volumes')\n\n        return netapp_xml", "response": "Retrieve netapp volume information and return ElementTree of netapp volume information"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef collect(self, device, ip, user, password):\n\n        if netappsdk is None:\n            self.log.error(\n                'Failed to import netappsdk.NaServer or netappsdk.NaElement')\n            return\n\n        if device in self.running:\n            return\n\n        self.running.add(device)\n        prefix = self.config['path_prefix']\n        pm = self.publish_metric\n\n        netapp_inodeCol(device, ip, user, password, prefix, pm)\n        self.running.remove(device)", "response": "Collect metrics for our netapp filer"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_default_config(self):\n        config = super(NtpdCollector, self).get_default_config()\n        config.update({\n            'path':         'ntpd',\n            'ntpq_bin':     self.find_binary('/usr/bin/ntpq'),\n            'ntpdc_bin':    self.find_binary('/usr/bin/ntpdc'),\n            'use_sudo':     False,\n            'sudo_cmd':     self.find_binary('/usr/bin/sudo'),\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncollect memory stats from a single AVAISD - E.", "response": "def collect(self):\n        \"\"\"\n        Collect memory stats\n        \"\"\"\n        try:\n            if str_to_bool(self.config['use_sudo']):\n                # Use -u instead of --user as the former is more portable. Not\n                # all versions of sudo support the long form --user.\n                cmdline = [\n                    self.config['sudo_exe'], '-u', self.config['sudo_user'],\n                    '--', self.config['amavisd_exe'], '-c', '1'\n                ]\n            else:\n                cmdline = [self.config['amavisd_exe'], '-c', '1']\n            agent = subprocess.Popen(cmdline, stdout=subprocess.PIPE)\n            agent_out = agent.communicate()[0]\n            lines = agent_out.strip().split(os.linesep)\n            for line in lines:\n                for rex in self.matchers:\n                    res = rex.match(line)\n                    if res:\n                        groups = res.groupdict()\n                        name = groups['name']\n                        for metric, value in groups.items():\n                            if metric == 'name':\n                                continue\n                            mtype = 'GAUGE'\n                            precision = 2\n                            if metric in ('count', 'time'):\n                                mtype = 'COUNTER'\n                                precision = 0\n                            self.publish(\"{}.{}\".format(name, metric),\n                                         value, metric_type=mtype,\n                                         precision=precision)\n\n        except OSError as err:\n            self.log.error(\"Could not run %s: %s\",\n                           self.config['amavisd_exe'],\n                           err)\n            return None\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns more complete message", "response": "def verbose_message(self):\n        \"\"\"return more complete message\"\"\"\n        if self.threshold is None:\n            return 'No threshold'\n        return '%.1f is %s than %.1f' % (self.value,\n                                         self.adjective,\n                                         self.threshold)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprocess a single diamond metric and return a new object with the count of errors and pass.", "response": "def process(self, metric, handler):\n        \"\"\"\n        process a single diamond metric\n        @type metric: diamond.metric.Metric\n        @param metric: metric to process\n        @type handler: diamond.handler.sentry.SentryHandler\n        @param handler: configured Sentry graphite handler\n        @rtype None\n        \"\"\"\n        match = self.regexp.match(metric.path)\n        if match:\n            minimum = Minimum(metric.value, self.min)\n            maximum = Maximum(metric.value, self.max)\n\n            if minimum.is_error or maximum.is_error:\n                self.counter_errors += 1\n                message = \"%s Warning on %s: %.1f\" % (self.name,\n                                                      handler.hostname,\n                                                      metric.value)\n                culprit = \"%s %s\" % (handler.hostname, match.group('path'))\n                handler.raven_logger.error(message, extra={\n                    'culprit': culprit,\n                    'data': {\n                        'metric prefix': match.group('prefix'),\n                        'metric path': match.group('path'),\n                        'minimum check': minimum.verbose_message,\n                        'maximum check': maximum.verbose_message,\n                        'metric original path': metric.path,\n                        'metric value': metric.value,\n                        'metric precision': metric.precision,\n                        'metric timestamp': metric.timestamp,\n                        'minimum threshold': self.min,\n                        'maximum threshold': self.max,\n                        'path regular expression': self.regexp.pattern,\n                        'total errors': self.counter_errors,\n                        'total pass': self.counter_pass,\n                        'hostname': handler.hostname\n                    }\n                }\n                )\n            else:\n                self.counter_pass += 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the help text for the configuration options for this handler", "response": "def get_default_config_help(self):\n        \"\"\"\n        Returns the help text for the configuration options for this handler\n        \"\"\"\n        config = super(SentryHandler, self).get_default_config_help()\n\n        config.update({\n            'dsn': '',\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the default config for the handler", "response": "def get_default_config(self):\n        \"\"\"\n        Return the default config for the handler\n        \"\"\"\n        config = super(SentryHandler, self).get_default_config()\n\n        config.update({\n            'dsn': '',\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compile_rules(self):\n        output = []\n        # validate configuration, skip invalid section\n        for key_name, section in self.config.items():\n            rule = self.compile_section(section)\n            if rule is not None:\n                output.append(rule)\n        return output", "response": "Compile alert rules\n        @rtype list of Rules"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compile_section(self, section):\n        if section.__class__ != Section:\n            # not a section, just skip\n            return\n\n        # name and path are mandatory\n        keys = section.keys()\n        for key in ('name', 'path'):\n            if key not in keys:\n                self.log.warning(\"section %s miss key '%s' ignore\", key,\n                                 section.name)\n                return\n\n        # just warn if invalid key in section\n        for key in keys:\n            if key not in self.VALID_RULES_KEYS:\n                self.log.warning(\"invalid key %s in section %s\",\n                                 key, section.name)\n\n        # need at least a min or a max\n        if 'min' not in keys and 'max' not in keys:\n            self.log.warning(\"either 'min' or 'max' is defined in %s\",\n                             section.name)\n            return\n\n        # add rule to the list\n        kwargs = {\n            'name': section['name'],\n            'path': section['path']\n        }\n        for argument in ('min', 'max'):\n            try:\n                kwargs[argument] = section[argument]\n            except KeyError:\n                pass\n\n        # init rule\n        try:\n            return Rule(**kwargs)\n        except InvalidRule as err:\n            self.log.error(str(err))", "response": "Validate if a section is a valid rule or None if invalid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef configure_sentry_errors(self):\n        sentry_errors_logger = logging.getLogger('sentry.errors')\n        root_logger = logging.getLogger()\n        for handler in root_logger.handlers:\n            sentry_errors_logger.addHandler(handler)", "response": "Configure sentry. errors to use the same loggers as the root handler"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses a single metric", "response": "def process(self, metric):\n        \"\"\"\n        process a single metric\n        @type metric: diamond.metric.Metric\n        @param metric: metric to process\n        @rtype None\n        \"\"\"\n        for rule in self.rules:\n            rule.process(metric, self)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_default_config(self):\n        config = super(NetworkCollector, self).get_default_config()\n        config.update({\n            'path':         'network',\n            'interfaces':   ['eth', 'bond', 'em', 'p1p', 'eno', 'enp', 'ens',\n                             'enx'],\n            'byte_unit':    ['bit', 'byte'],\n            'greedy':       'true',\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncollects network interface stats.", "response": "def collect(self):\n        \"\"\"\n        Collect network interface stats.\n        \"\"\"\n\n        # Initialize results\n        results = {}\n\n        if os.access(self.PROC, os.R_OK):\n\n            # Open File\n            file = open(self.PROC)\n            # Build Regular Expression\n            greed = ''\n            if str_to_bool(self.config['greedy']):\n                greed = '\\S*'\n\n            exp = (('^(?:\\s*)((?:%s)%s):(?:\\s*)' +\n                    '(?P<rx_bytes>\\d+)(?:\\s*)' +\n                    '(?P<rx_packets>\\w+)(?:\\s*)' +\n                    '(?P<rx_errors>\\d+)(?:\\s*)' +\n                    '(?P<rx_drop>\\d+)(?:\\s*)' +\n                    '(?P<rx_fifo>\\d+)(?:\\s*)' +\n                    '(?P<rx_frame>\\d+)(?:\\s*)' +\n                    '(?P<rx_compressed>\\d+)(?:\\s*)' +\n                    '(?P<rx_multicast>\\d+)(?:\\s*)' +\n                    '(?P<tx_bytes>\\d+)(?:\\s*)' +\n                    '(?P<tx_packets>\\w+)(?:\\s*)' +\n                    '(?P<tx_errors>\\d+)(?:\\s*)' +\n                    '(?P<tx_drop>\\d+)(?:\\s*)' +\n                    '(?P<tx_fifo>\\d+)(?:\\s*)' +\n                    '(?P<tx_colls>\\d+)(?:\\s*)' +\n                    '(?P<tx_carrier>\\d+)(?:\\s*)' +\n                    '(?P<tx_compressed>\\d+)(?:.*)$') %\n                   (('|'.join(self.config['interfaces'])), greed))\n            reg = re.compile(exp)\n            # Match Interfaces\n            for line in file:\n                match = reg.match(line)\n                if match:\n                    device = match.group(1)\n                    results[device] = match.groupdict()\n            # Close File\n            file.close()\n        else:\n            if not psutil:\n                self.log.error('Unable to import psutil')\n                self.log.error('No network metrics retrieved')\n                return None\n\n            network_stats = psutil.network_io_counters(True)\n            for device in network_stats.keys():\n                network_stat = network_stats[device]\n                results[device] = {}\n                results[device]['rx_bytes'] = network_stat.bytes_recv\n                results[device]['tx_bytes'] = network_stat.bytes_sent\n                results[device]['rx_packets'] = network_stat.packets_recv\n                results[device]['tx_packets'] = network_stat.packets_sent\n\n        for device in results:\n            stats = results[device]\n            for s, v in stats.items():\n                # Get Metric Name\n                metric_name = '.'.join([device, s])\n                # Get Metric Value\n                metric_value = self.derivative(metric_name,\n                                               long(v),\n                                               diamond.collector.MAX_COUNTER)\n\n                # Convert rx_bytes and tx_bytes\n                if s == 'rx_bytes' or s == 'tx_bytes':\n                    convertor = diamond.convertor.binary(value=metric_value,\n                                                         unit='byte')\n\n                    for u in self.config['byte_unit']:\n                        # Public Converted Metric\n                        self.publish(metric_name.replace('bytes', u),\n                                     convertor.get(unit=u), 2)\n                else:\n                    # Publish Metric Derivative\n                    self.publish(metric_name, metric_value)\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_default_config_help(self):\n        config_help = super(ConnTrackCollector, self).get_default_config_help()\n        config_help.update({\n            \"dir\":      \"Directories with files of interest, comma seperated\",\n            \"files\":    \"List of files to collect statistics from\",\n        })\n        return config_help", "response": "Return help text for collector configuration\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_default_config(self):\n        config = super(ConnTrackCollector, self).get_default_config()\n        config.update({\n            \"path\":  \"conntrack\",\n            \"dir\":   \"/proc/sys/net/ipv4/netfilter,/proc/sys/net/netfilter\",\n            \"files\": \"ip_conntrack_count,ip_conntrack_max,\"\n                     \"nf_conntrack_count,nf_conntrack_max\",\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncollects metrics from conntrack kernel module", "response": "def collect(self):\n        \"\"\"\n        Collect metrics\n        \"\"\"\n        collected = {}\n        files = []\n\n        if isinstance(self.config['dir'], basestring):\n            dirs = [d.strip() for d in self.config['dir'].split(',')]\n        elif isinstance(self.config['dir'], list):\n            dirs = self.config['dir']\n\n        if isinstance(self.config['files'], basestring):\n            files = [f.strip() for f in self.config['files'].split(',')]\n        elif isinstance(self.config['files'], list):\n            files = self.config['files']\n\n        for sdir in dirs:\n            for sfile in files:\n                if sfile.endswith('conntrack_count'):\n                    metric_name = 'ip_conntrack_count'\n                elif sfile.endswith('conntrack_max'):\n                    metric_name = 'ip_conntrack_max'\n                else:\n                    self.log.error('Unknown file for collection: %s', sfile)\n                    continue\n                fpath = os.path.join(sdir, sfile)\n                if not os.path.exists(fpath):\n                    continue\n                try:\n                    with open(fpath, \"r\") as fhandle:\n                        metric = float(fhandle.readline().rstrip(\"\\n\"))\n                        collected[metric_name] = metric\n                except Exception as exception:\n                    self.log.error(\"Failed to collect from '%s': %s\",\n                                   fpath,\n                                   exception)\n        if not collected:\n            self.log.error('No metric was collected, looks like '\n                           'nf_conntrack/ip_conntrack kernel module was '\n                           'not loaded')\n        else:\n            for key in collected.keys():\n                self.publish(key, collected[key])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(NumaCollector, self).get_default_config()\n        config.update({\n            'path':     'numa',\n            'bin':      self.find_binary('numactl'),\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreplaces the metric name and publish.", "response": "def _replace_and_publish(self, path, prettyname, value, device):\n        \"\"\"\n        Inputs a complete path for a metric and a value.\n        Replace the metric name and publish.\n        \"\"\"\n        if value is None:\n            return\n        newpath = path\n        # Change metric name before publish if needed.\n        newpath = \".\".join([\".\".join(path.split(\".\")[:-1]), prettyname])\n        metric = Metric(newpath, value, precision=4, host=device)\n        self.publish_metric(metric)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating delta for a metric based on the delta of another metric.", "response": "def _gen_delta_depend(self, path, derivative, multiplier, prettyname,\n                          device):\n        \"\"\"\n        For some metrics we need to divide the delta for one metric\n        with the delta of another.\n        Publishes a metric if the convertion goes well.\n        \"\"\"\n        primary_delta = derivative[path]\n        shortpath = \".\".join(path.split(\".\")[:-1])\n        basename = path.split(\".\")[-1]\n        secondary_delta = None\n        if basename in self.DIVIDERS.keys():\n            mateKey = \".\".join([shortpath, self.DIVIDERS[basename]])\n        else:\n            return\n        if mateKey in derivative.keys():\n            secondary_delta = derivative[mateKey]\n        else:\n            return\n\n        # If we find a corresponding secondary_delta, publish a metric\n        if primary_delta > 0 and secondary_delta > 0:\n            value = (float(primary_delta) / secondary_delta) * multiplier\n            self._replace_and_publish(path, prettyname, value, device)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _gen_delta_per_sec(self, path, value_delta, time_delta, multiplier,\n                           prettyname, device):\n        \"\"\"\n        Calulates the difference between to point, and scales is to per second.\n        \"\"\"\n        if time_delta < 0:\n            return\n        value = (value_delta / time_delta) * multiplier\n        # Only publish if there is any data.\n        # This helps keep unused metrics out of Graphite\n        if value > 0.0:\n            self._replace_and_publish(path, prettyname, value, device)", "response": "Generate the difference between to point and scales is to per second."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(HttpdCollector, self).get_default_config()\n        config.update({\n            'path':     'httpd',\n            'urls':     ['localhost http://localhost:8080/server-status?auto']\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_default_config(self):\n        config = super(PuppetDBCollector, self).get_default_config()\n        config.update({\n            'host': 'localhost',\n            'port': 8080,\n            'path': 'PuppetDB',\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning default config for the handler", "response": "def get_default_config(self):\n        \"\"\"\n        Return default config for the handler\n        \"\"\"\n        config = super(DatadogHandler, self).get_default_config()\n\n        config.update({\n            'api_key': '',\n            'queue_size': '',\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _send(self):\n\n        while len(self.queue) > 0:\n            metric = self.queue.popleft()\n\n            path = '%s.%s.%s' % (\n                metric.getPathPrefix(),\n                metric.getCollectorPath(),\n                metric.getMetricPath()\n            )\n\n            topic, value, timestamp = str(metric).split()\n            logging.debug(\n                \"Sending.. topic[%s], value[%s], timestamp[%s]\",\n                path,\n                value,\n                timestamp\n            )\n\n            self.api.metric(path, (timestamp, value), host=metric.host)", "response": "Send metrics to Datadog API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(ProcessStatCollector, self).get_default_config()\n        config.update({\n            'path':     'proc'\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncollecting process stat data", "response": "def collect(self):\n        \"\"\"\n        Collect process stat data\n        \"\"\"\n        if not os.access(self.PROC, os.R_OK):\n            return False\n\n        # Open PROC file\n        file = open(self.PROC, 'r')\n\n        # Get data\n        for line in file:\n\n            if line.startswith('ctxt') or line.startswith('processes'):\n                data = line.split()\n                metric_name = data[0]\n                metric_value = int(data[1])\n                metric_value = int(self.derivative(metric_name,\n                                                   long(metric_value),\n                                                   counter))\n                self.publish(metric_name, metric_value)\n\n            if line.startswith('procs_') or line.startswith('btime'):\n                data = line.split()\n                metric_name = data[0]\n                metric_value = int(data[1])\n                self.publish(metric_name, metric_value)\n\n        # Close file\n        file.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(PhpFpmCollector, self).get_default_config()\n        config.update({\n            'host': 'localhost',\n            'port': 80,\n            'uri': 'fpm-status',\n            'byte_unit': ['byte'],\n            'path': 'phpfpm',\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_version():\n    try:\n        f = open('version.txt')\n    except IOError:\n        os.system(\"./version.sh > version.txt\")\n        f = open('version.txt')\n    version = ''.join(f.readlines()).rstrip()\n    f.close()\n    return version", "response": "Read the version. txt file to get the new version string"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pkgPath(root, path, rpath=\"/\"):\n    global data_files\n    if not os.path.exists(path):\n        return\n    files = []\n    for spath in os.listdir(path):\n        # Ignore test directories\n        if spath == 'test':\n            continue\n        subpath = os.path.join(path, spath)\n        spath = os.path.join(rpath, spath)\n        if os.path.isfile(subpath):\n            files.append(subpath)\n        if os.path.isdir(subpath):\n            pkgPath(root, subpath, spath)\n    data_files.append((root + rpath, files))", "response": "Package up a path recursively"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_default_config(self):\n        config = super(NetscalerSNMPCollector, self).get_default_config()\n        config.update({\n            'path':     'netscaler',\n            'timeout':  15,\n            'exclude_service_type': [],\n            'exclude_vserver_type': [],\n            'exclude_service_state': [],\n            'exclude_vserver_state': []\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncollecting SNMP stats from a device.", "response": "def collect_snmp(self, device, host, port, community):\n        \"\"\"\n        Collect Netscaler SNMP stats from device\n        \"\"\"\n        # Log\n        self.log.info(\"Collecting Netscaler statistics from: %s\", device)\n\n        # Set timestamp\n        timestamp = time.time()\n\n        # Collect Netscaler System OIDs\n        for k, v in self.NETSCALER_SYSTEM_GUAGES.items():\n            # Get Metric Name and Value\n            metricName = '.'.join([k])\n            metricValue = int(self.get(v, host, port, community)[v])\n            # Get Metric Path\n            metricPath = '.'.join(['devices', device, 'system', metricName])\n            # Create Metric\n            metric = Metric(metricPath, metricValue, timestamp, 0)\n            # Publish Metric\n            self.publish_metric(metric)\n\n        # Collect Netscaler System Counter OIDs\n        for k, v in self.NETSCALER_SYSTEM_COUNTERS.items():\n            # Get Metric Name and Value\n            metricName = '.'.join([k])\n            # Get Metric Path\n            metricPath = '.'.join(['devices', device, 'system', metricName])\n            # Get Metric Value\n            metricValue = self.derivative(metricPath, long(\n                self.get(v, host, port, community)[v]), self.MAX_VALUE)\n            # Create Metric\n            metric = Metric(metricPath, metricValue, timestamp, 0)\n            # Publish Metric\n            self.publish_metric(metric)\n\n        # Collect Netscaler Services\n        serviceNames = [v.strip(\"\\'\") for v in self.walk(\n            self.NETSCALER_SERVICE_NAMES, host, port, community).values()]\n\n        for serviceName in serviceNames:\n            # Get Service Name in OID form\n            serviceNameOid = self.get_string_index_oid(serviceName)\n\n            # Get Service Type\n            serviceTypeOid = \".\".join([self.NETSCALER_SERVICE_TYPE,\n                                       self._convert_from_oid(serviceNameOid)])\n            serviceType = int(self.get(serviceTypeOid,\n                                       host,\n                                       port,\n                                       community)[serviceTypeOid].strip(\"\\'\"))\n\n            # Filter excluded service types\n            if serviceType in map(lambda v: int(v),\n                                  self.config.get('exclude_service_type')):\n                continue\n\n            # Get Service State\n            serviceStateOid = \".\".join([self.NETSCALER_SERVICE_STATE,\n                                        self._convert_from_oid(serviceNameOid)])\n            serviceState = int(self.get(serviceStateOid,\n                                        host,\n                                        port,\n                                        community)[serviceStateOid].strip(\"\\'\"))\n\n            # Filter excluded service states\n            if serviceState in map(lambda v: int(v),\n                                   self.config.get('exclude_service_state')):\n                continue\n\n            for k, v in self.NETSCALER_SERVICE_GUAGES.items():\n                serviceGuageOid = \".\".join(\n                    [v, self._convert_from_oid(serviceNameOid)])\n                # Get Metric Name\n                metricName = '.'.join([re.sub(r'\\.|\\\\', '_', serviceName), k])\n                # Get Metric Value\n                metricValue = int(self.get(serviceGuageOid,\n                                           host,\n                                           port,\n                                           community\n                                           )[serviceGuageOid].strip(\"\\'\"))\n                # Get Metric Path\n                metricPath = '.'.join(['devices',\n                                       device,\n                                       'service',\n                                       metricName])\n                # Create Metric\n                metric = Metric(metricPath, metricValue, timestamp, 0)\n                # Publish Metric\n                self.publish_metric(metric)\n\n        # Collect Netscaler Vservers\n        vserverNames = [v.strip(\"\\'\") for v in self.walk(\n            self.NETSCALER_VSERVER_NAMES, host, port, community).values()]\n\n        for vserverName in vserverNames:\n            # Get Vserver Name in OID form\n            vserverNameOid = self.get_string_index_oid(vserverName)\n\n            # Get Vserver Type\n            vserverTypeOid = \".\".join([self.NETSCALER_VSERVER_TYPE,\n                                       self._convert_from_oid(vserverNameOid)])\n            vserverType = int(self.get(vserverTypeOid,\n                                       host,\n                                       port,\n                                       community)[vserverTypeOid].strip(\"\\'\"))\n\n            # filter excluded vserver types\n            if vserverType in map(lambda v: int(v),\n                                  self.config.get('exclude_vserver_type')):\n                continue\n\n            # Get Service State\n            vserverStateOid = \".\".join([self.NETSCALER_VSERVER_STATE,\n                                        self._convert_from_oid(vserverNameOid)])\n            vserverState = int(self.get(vserverStateOid,\n                                        host,\n                                        port,\n                                        community)[vserverStateOid].strip(\"\\'\"))\n\n            # Filter excluded vserver state\n            if vserverState in map(lambda v: int(v),\n                                   self.config.get('exclude_vserver_state')):\n                continue\n\n            for k, v in self.NETSCALER_VSERVER_GUAGES.items():\n                vserverGuageOid = \".\".join(\n                    [v, self._convert_from_oid(vserverNameOid)])\n                # Get Metric Name\n                metricName = '.'.join([re.sub(r'\\.|\\\\', '_', vserverName), k])\n                # Get Metric Value\n                metricValue = int(self.get(vserverGuageOid,\n                                           host,\n                                           port,\n                                           community\n                                           )[vserverGuageOid].strip(\"\\'\"))\n                # Get Metric Path\n                metricPath = '.'.join(['devices',\n                                       device,\n                                       'vserver',\n                                       metricName])\n                # Create Metric\n                metric = Metric(metricPath, metricValue, timestamp, 0)\n                # Publish Metric\n                self.publish_metric(metric)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting the calendar year to a number of years.", "response": "def year(self, value=None):\n        \"\"\"\n        We do *NOT* know for what year we are converting so lets assume the\n        year has 365 days.\n        \"\"\"\n        if value is None:\n            return self.day() / 365\n        else:\n            self.millisecond(self.day(value * 365))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_default_config(self):\n        config = super(UserScriptsCollector, self).get_default_config()\n        config.update({\n            'path':         '.',\n            'scripts_path': '/etc/diamond/user_scripts/',\n            'floatprecision': 4,\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_default_config(self):\n        config = super(NfsCollector, self).get_default_config()\n        config.update({\n            'path':     'nfs'\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(PowerDNSCollector, self).get_default_config()\n        config.update({\n            'bin': '/usr/bin/pdns_control',\n            'path': 'powerdns',\n            'use_sudo':         False,\n            'sudo_cmd':         '/usr/bin/sudo',\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the help text for the configuration options for this handler", "response": "def get_default_config_help(self):\n        \"\"\"\n        Returns the help text for the configuration options for this handler\n        \"\"\"\n        config = super(StatsiteHandler, self).get_default_config_help()\n\n        config.update({\n            'host': '',\n            'tcpport': '',\n            'udpport': '',\n            'timeout': '',\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the default config for the handler", "response": "def get_default_config(self):\n        \"\"\"\n        Return the default config for the handler\n        \"\"\"\n        config = super(StatsiteHandler, self).get_default_config()\n\n        config.update({\n            'host': '',\n            'tcpport': 1234,\n            'udpport': 1234,\n            'timeout': 5,\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _send(self, data):\n        retry = self.RETRY\n        # Attempt to send any data in the queue\n        while retry > 0:\n            # Check socket\n            if not self.socket:\n                # Log Error\n                self.log.error(\"StatsiteHandler: Socket unavailable.\")\n                # Attempt to restablish connection\n                self._connect()\n                # Decrement retry\n                retry -= 1\n                # Try again\n                continue\n            try:\n                # Send data to socket\n                data = data.split()\n                data = data[0] + \":\" + data[1] + \"|kv\\n\"\n                self.socket.sendall(data)\n                # Done\n                break\n            except socket.error as e:\n                # Log Error\n                self.log.error(\"StatsiteHandler: Failed sending data. %s.\", e)\n                # Attempt to restablish connection\n                self._close()\n                # Decrement retry\n                retry -= 1\n                # try again\n                continue", "response": "Send data to statsite."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _connect(self):\n        # Create socket\n        if self.udpport > 0:\n            self.socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n            self.port = self.udpport\n        elif self.tcpport > 0:\n            self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            self.port = self.tcpport\n        if socket is None:\n            # Log Error\n            self.log.error(\"StatsiteHandler: Unable to create socket.\")\n            # Close Socket\n            self._close()\n            return\n        # Set socket timeout\n        self.socket.settimeout(self.timeout)\n        # Connect to statsite server\n        try:\n            self.socket.connect((self.host, self.port))\n            # Log\n            self.log.debug(\"Established connection to statsite server %s:%d\",\n                           self.host, self.port)\n        except Exception as ex:\n            # Log Error\n            self.log.error(\"StatsiteHandler: Failed to connect to %s:%i. %s\",\n                           self.host, self.port, ex)\n            # Close Socket\n            self._close()\n            return", "response": "Connect to the statsite server and return the unique ID."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nscanning for and add paths to the include path", "response": "def load_include_path(paths):\n    \"\"\"\n    Scan for and add paths to the include path\n    \"\"\"\n    for path in paths:\n        # Verify the path is valid\n        if not os.path.isdir(path):\n            continue\n        # Add path to the system path, to avoid name clashes\n        # with mysql-connector for example ...\n        if path not in sys.path:\n            sys.path.insert(1, path)\n        # Load all the files in path\n        for f in os.listdir(path):\n            # Are we a directory? If so process down the tree\n            fpath = os.path.join(path, f)\n            if os.path.isdir(fpath):\n                load_include_path([fpath])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_handlers(config, handler_names):\n\n    handlers = []\n\n    if isinstance(handler_names, basestring):\n        handler_names = [handler_names]\n\n    for handler in handler_names:\n        logger.debug('Loading Handler %s', handler)\n        try:\n            # Load Handler Class\n            cls = load_dynamic_class(handler, Handler)\n            cls_name = cls.__name__\n\n            # Initialize Handler config\n            handler_config = configobj.ConfigObj()\n            # Merge default Handler default config\n            handler_config.merge(config['handlers']['default'])\n            # Check if Handler config exists\n            if cls_name in config['handlers']:\n                # Merge Handler config section\n                handler_config.merge(config['handlers'][cls_name])\n\n            # Check for config file in config directory\n            if 'handlers_config_path' in config['server']:\n                configfile = os.path.join(\n                    config['server']['handlers_config_path'],\n                    cls_name) + '.conf'\n                if os.path.exists(configfile):\n                    # Merge Collector config file\n                    handler_config.merge(configobj.ConfigObj(configfile))\n\n            # Initialize Handler class\n            h = cls(handler_config)\n            handlers.append(h)\n\n        except (ImportError, SyntaxError):\n            # Log Error\n            logger.warning(\"Failed to load handler %s. %s\",\n                           handler,\n                           traceback.format_exc())\n            continue\n\n    return handlers", "response": "Load handlers from a list of handler names."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_collectors_from_paths(paths):\n    # Initialize return value\n    collectors = {}\n\n    if paths is None:\n        return\n\n    if isinstance(paths, basestring):\n        paths = paths.split(',')\n        paths = map(str.strip, paths)\n\n    load_include_path(paths)\n\n    for path in paths:\n        # Get a list of files in the directory, if the directory exists\n        if not os.path.exists(path):\n            raise OSError(\"Directory does not exist: %s\" % path)\n\n        if path.endswith('tests') or path.endswith('fixtures'):\n            return collectors\n\n        # Load all the files in path\n        for f in os.listdir(path):\n\n            # Are we a directory? If so process down the tree\n            fpath = os.path.join(path, f)\n            if os.path.isdir(fpath):\n                subcollectors = load_collectors_from_paths([fpath])\n                for key in subcollectors:\n                    collectors[key] = subcollectors[key]\n\n            # Ignore anything that isn't a .py file\n            elif (os.path.isfile(fpath) and\n                  len(f) > 3 and\n                  f[-3:] == '.py' and\n                  f[0:4] != 'test' and\n                  f[0] != '.'):\n\n                modname = f[:-3]\n\n                fp, pathname, description = imp.find_module(modname, [path])\n\n                try:\n                    # Import the module\n                    mod = imp.load_module(modname, fp, pathname, description)\n                except (KeyboardInterrupt, SystemExit) as err:\n                    logger.error(\n                        \"System or keyboard interrupt \"\n                        \"while loading module %s\"\n                        % modname)\n                    if isinstance(err, SystemExit):\n                        sys.exit(err.code)\n                    raise KeyboardInterrupt\n                except Exception:\n                    # Log error\n                    logger.error(\"Failed to import module: %s. %s\",\n                                 modname,\n                                 traceback.format_exc())\n                else:\n                    for name, cls in get_collectors_from_module(mod):\n                        collectors[name] = cls\n                finally:\n                    if fp:\n                        fp.close()\n\n    # Return Collector classes\n    return collectors", "response": "Load all the collectors from a list of paths."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload collectors that were installed into an entry_point.", "response": "def load_collectors_from_entry_point(path):\n    \"\"\"\n    Load collectors that were installed into an entry_point.\n    \"\"\"\n    collectors = {}\n    for ep in pkg_resources.iter_entry_points(path):\n        try:\n            mod = ep.load()\n        except Exception:\n            logger.error('Failed to import entry_point: %s. %s',\n                         ep.name,\n                         traceback.format_exc())\n        else:\n            collectors.update(get_collectors_from_module(mod))\n    return collectors"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlocates all of the collector classes within a module", "response": "def get_collectors_from_module(mod):\n    \"\"\"\n    Locate all of the collector classes within a given module\n    \"\"\"\n    for attrname in dir(mod):\n        attr = getattr(mod, attrname)\n        # Only attempt to load classes that are infact classes\n        # are Collectors but are not the base Collector class\n        if ((inspect.isclass(attr) and\n             issubclass(attr, Collector) and\n             attr != Collector)):\n            if attrname.startswith('parent_'):\n                continue\n            # Get class name\n            fqcn = '.'.join([mod.__name__, attrname])\n            try:\n                # Load Collector class\n                cls = load_dynamic_class(fqcn, Collector)\n                # Add Collector class\n                yield cls.__name__, cls\n            except Exception:\n                # Log error\n                logger.error(\n                    \"Failed to load Collector: %s. %s\",\n                    fqcn, traceback.format_exc())\n                continue"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing collector with given name configfile and handlers", "response": "def initialize_collector(cls, name=None, configfile=None, handlers=[]):\n    \"\"\"\n    Initialize collector\n    \"\"\"\n    collector = None\n\n    try:\n        # Initialize Collector\n        collector = cls(name=name, configfile=configfile, handlers=handlers)\n    except Exception:\n        # Log error\n        logger.error(\"Failed to initialize Collector: %s. %s\",\n                     cls.__name__, traceback.format_exc())\n\n    # Return collector\n    return collector"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the help text for the configuration options for this handler", "response": "def get_default_config_help(self):\n        \"\"\"\n        Returns the help text for the configuration options for this handler\n        \"\"\"\n        config = super(HttpPostHandler, self).get_default_config_help()\n\n        config.update({\n            'url': 'Fully qualified url to send metrics to',\n            'batch': 'How many to store before sending to the graphite server',\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_default_config(self):\n        config = super(HttpPostHandler, self).get_default_config()\n\n        config.update({\n            'url': 'http://localhost/blah/blah/blah',\n            'batch': 100,\n        })\n\n        return config", "response": "Return the default config for the handler\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the help text for the configuration options for this handler", "response": "def get_default_config_help(self):\n        \"\"\"\n        Returns the help text for the configuration options for this handler\n        \"\"\"\n        config = super(MySQLHandler, self).get_default_config_help()\n\n        config.update({\n        })\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_default_config(self):\n        config = super(MySQLHandler, self).get_default_config()\n\n        config.update({\n        })\n\n        return config", "response": "Returns the default config for the handler\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _send(self, data):\n        data = data.strip().split(' ')\n        try:\n            cursor = self.conn.cursor()\n            cursor.execute(\"INSERT INTO %s (%s, %s, %s) VALUES(%%s, %%s, %%s)\"\n                           % (self.table, self.col_metric,\n                              self.col_time, self.col_value),\n                           (data[0], data[2], data[1]))\n            cursor.close()\n            self.conn.commit()\n        except BaseException as e:\n            # Log Error\n            self.log.error(\"MySQLHandler: Failed sending data. %s.\", e)\n            # Attempt to restablish connection\n            self._connect()", "response": "Send data to the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconnecting to the MySQL server and create a new instance of the class.", "response": "def _connect(self):\n        \"\"\"\n        Connect to the MySQL server\n        \"\"\"\n        self._close()\n        self.conn = MySQLdb.Connect(host=self.hostname,\n                                    port=self.port,\n                                    user=self.username,\n                                    passwd=self.password,\n                                    db=self.database)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the default collector settings", "response": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(NvidiaGPUCollector, self).get_default_config()\n        config.update({\n            'path': 'nvidia',\n            'bin': '/usr/bin/nvidia-smi',\n            'stats': [\n                'index',\n                'memory.total',\n                'memory.used',\n                'memory.free',\n                'utilization.gpu',\n                'utilization.memory',\n                'temperature.gpu'\n            ]\n        })\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef collect_via_nvidia_smi(self, stats_config):\n        raw_output = self.run_command([\n            '--query-gpu={query_gpu}'.format(query_gpu=','.join(stats_config)),\n            '--format=csv,nounits,noheader'\n        ])\n\n        if raw_output is None:\n            return\n\n        results = raw_output[0].strip().split(\"\\n\")\n        for result in results:\n            stats = result.strip().split(',')\n            assert len(stats) == len(stats_config)\n            index = stats[0]\n            for stat_name, metric in izip(stats_config[1:], stats[1:]):\n                metric_name = 'gpu_{index}.{stat_name}'.format(\n                    index=str(index),\n                    stat_name=stat_name\n                )\n                self.publish(metric_name, metric)", "response": "Use nvidia smi command line tool to collect metrics from nvidia smi stats"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncollects the metrics from the NVML device.", "response": "def collect_via_pynvml(self, stats_config):\n        \"\"\"\n        Use pynvml python binding to collect metrics\n        :param stats_config:\n        :return:\n        \"\"\"\n        try:\n            NVML_TEMPERATURE_GPU = 0\n            pynvml.nvmlInit()\n            device_count = pynvml.nvmlDeviceGetCount()\n\n            for device_index in xrange(device_count):\n                handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)\n                memoryInfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\n                utilizationRates = pynvml.nvmlDeviceGetUtilizationRates(handle)\n\n                metrics = {\n                    'memory.total': memoryInfo.total / 1024 / 1024,\n                    'memory.used': memoryInfo.total / 1024 / 1024,\n                    'memory.free': memoryInfo.free / 1024 / 1024,\n                    'utilization.gpu': utilizationRates.gpu,\n                    'utilization.memory': utilizationRates.memory,\n                    'temperature.gpu':\n                        pynvml.nvmlDeviceGetTemperature(handle,\n                                                        NVML_TEMPERATURE_GPU)\n                }\n\n                for stat_name in stats_config[1:]:\n                    metric = metrics.get(stat_name)\n                    if metric:\n                        metric_name = 'gpu_{index}.{stat_name}'.format(\n                            index=str(device_index),\n                            stat_name=stat_name\n                        )\n                        self.publish(metric_name, metric)\n        finally:\n            pynvml.nvmlShutdown()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncollect all metrics for the current node.", "response": "def collect(self):\n        \"\"\"\n        Collector GPU stats\n        \"\"\"\n        stats_config = self.config['stats']\n        if USE_PYTHON_BINDING:\n            collect_metrics = self.collect_via_pynvml\n        else:\n            collect_metrics = self.collect_via_nvidia_smi\n\n        collect_metrics(stats_config)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decode_network_values(ptype, plen, buf):\n    nvalues = short.unpack_from(buf, header.size)[0]\n    off = header.size + short.size + nvalues\n    valskip = double.size\n\n    # Check whether our expected packet size is the reported one\n    assert ((valskip + 1) * nvalues + short.size + header.size) == plen\n    assert double.size == number.size\n\n    result = []\n    for dstype in [ord(x) for x in buf[header.size + short.size:off]]:\n        if dstype == DS_TYPE_COUNTER:\n            result.append((dstype, number.unpack_from(buf, off)[0]))\n            off += valskip\n        elif dstype == DS_TYPE_GAUGE:\n            result.append((dstype, double.unpack_from(buf, off)[0]))\n            off += valskip\n        elif dstype == DS_TYPE_DERIVE:\n            result.append((dstype, number.unpack_from(buf, off)[0]))\n            off += valskip\n        elif dstype == DS_TYPE_ABSOLUTE:\n            result.append((dstype, number.unpack_from(buf, off)[0]))\n            off += valskip\n        else:\n            raise ValueError(\"DS type %i unsupported\" % dstype)\n\n    return result", "response": "Decodes a list of DS values in collectd network format\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decode_network_packet(buf):\n    off = 0\n    blen = len(buf)\n\n    while off < blen:\n        ptype, plen = header.unpack_from(buf, off)\n\n        if plen > blen - off:\n            raise ValueError(\"Packet longer than amount of data in buffer\")\n\n        if ptype not in _decoders:\n            raise ValueError(\"Message type %i not recognized\" % ptype)\n\n        yield ptype, _decoders[ptype](ptype, plen, buf[off:])\n        off += plen", "response": "Decodes a network packet in collectd format."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef receive(self, poll_interval):\n        readable, writeable, errored = select.select(self._readlist, [], [],\n                                                     poll_interval)\n        for s in readable:\n            data, addr = s.recvfrom(self.BUFFER_SIZE)\n            if data:\n                return data\n\n        return None", "response": "Receives a single raw collect network packet."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef decode(self, poll_interval, buf=None):\n        if buf is None:\n            buf = self.receive(poll_interval)\n        if buf is None:\n            return None\n        return decode_network_packet(buf)", "response": "Decodes a given buffer or the next received packet."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_default_config(self):\n        config = super(LoadAverageCollector, self).get_default_config()\n        config.update({\n            'path':     'loadavg',\n            'simple':   'False'\n        })\n        return config", "response": "Returns the default collector settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrequests running configuration and device state information.", "response": "def request(self, filter=None):\n        \"\"\"Retrieve running configuration and device state information.\n\n        *filter* specifies the portion of the configuration to retrieve (by default entire configuration is retrieved)\n\n        :seealso: :ref:`filter_params`\n        \"\"\"\n        node = new_ele(\"get-bulk\")\n        if filter is not None:\n            node.append(util.build_filter(filter))\n        return self._request(node)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrequest command text view", "response": "def request(self, command=None):\n        \"\"\"command text\n        view: Execution user view exec\n              Configuration system view exec\n        \"\"\"\n        node = new_ele(\"CLI\")\n        node.append(validated_element(command))\n        # sub_ele(node, view).text = command\n        return self._request(node)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a device handler object that provides device specific parameters and functions.", "response": "def make_device_handler(device_params):\n    \"\"\"\n    Create a device handler object that provides device specific parameters and\n    functions, which are called in various places throughout our code.\n\n    If no device_params are defined or the \"name\" in the parameter dict is not\n    known then a default handler will be returned.\n\n    \"\"\"\n    if device_params is None:\n        device_params = {}\n\n    handler = device_params.get('handler', None)\n    if handler:\n        return handler(device_params)\n\n    device_name = device_params.get(\"name\", \"default\")\n    # Attempt to import device handler class. All device handlers are\n    # in a module called \"ncclient.devices.<devicename>\" and in a class named\n    # \"<devicename>DeviceHandler\", with the first letter capitalized.\n    class_name          = \"%sDeviceHandler\" % device_name.capitalize()\n    devices_module_name = \"ncclient.devices.%s\" % device_name\n    dev_module_obj      = __import__(devices_module_name)\n    handler_module_obj  = getattr(getattr(dev_module_obj, \"devices\"), device_name)\n    class_obj           = getattr(handler_module_obj, class_name)\n    handler_obj         = class_obj(device_params)\n    return handler_obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef connect_ssh(*args, **kwds):\n    # Extract device parameter dict, if it was passed into this function. Need to\n    # remove it from kwds, since the session.connect() doesn't like extra stuff in\n    # there.\n    if \"device_params\" in kwds:\n        device_params = kwds[\"device_params\"]\n        del kwds[\"device_params\"]\n    else:\n        device_params = None\n\n    device_handler = make_device_handler(device_params)\n    device_handler.add_additional_ssh_connect_params(kwds)\n    global VENDOR_OPERATIONS\n    VENDOR_OPERATIONS.update(device_handler.add_additional_operations())\n    session = transport.SSHSession(device_handler)\n    if \"hostkey_verify\" not in kwds or kwds[\"hostkey_verify\"]:\n        session.load_known_hosts()\n\n    try:\n       session.connect(*args, **kwds)\n    except Exception as ex:\n        if session.transport:\n            session.close()\n        raise\n    return Manager(session, device_handler, **kwds)", "response": "Initialize a new Manager over the SSH transport."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a context manager for a lock on a datastore where the target is the name of the configuration datastore to lock.", "response": "def locked(self, target):\n        \"\"\"Returns a context manager for a lock on a datastore, where\n        *target* is the name of the configuration datastore to lock, e.g.::\n\n            with m.locked(\"running\"):\n                # do your stuff\n\n        ... instead of::\n\n            m.lock(\"running\")\n            try:\n                # do your stuff\n            finally:\n                m.unlock(\"running\")\n        \"\"\"\n        return operations.LockContext(self._session, self._device_handler, target)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nattempts to retrieve one notification from the queue of received notifications.", "response": "def take_notification(self, block=True, timeout=None):\n        \"\"\"Attempt to retrieve one notification from the queue of received\n        notifications.\n\n        If block is True, the call will wait until a notification is\n        received.\n\n        If timeout is a number greater than 0, the call will wait that\n        many seconds to receive a notification before timing out.\n\n        If there is no notification available when block is False or\n        when the timeout has elapse, None will be returned.\n\n        Otherwise a :class:`~ncclient.operations.notify.Notification`\n        object will be returned.\n        \"\"\"\n        return self._session.take_notification(block, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nforce the termination of a NETCONF session", "response": "def request(self, session_id):\n        \"\"\"Force the termination of a NETCONF session (not the current one!)\n\n        *session_id* is the session identifier of the NETCONF session to be terminated as a string\n        \"\"\"\n        node = new_ele(\"kill-session\")\n        sub_ele(node, \"session-id\").text = session_id\n        return self._request(node)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrequesting a new entry for the specified command.", "response": "def request(self, cmds):\n        \"\"\"\n        Single Execution element is permitted.\n        cmds can be a list or single command\n        \"\"\"\n        if isinstance(cmds, list):\n            cmd = '\\n'.join(cmds)\n        elif isinstance(cmds, str) or isinstance(cmds, unicode):\n            cmd = cmds\n\n        node = etree.Element(qualify('CLI', BASE_NS_1_0))\n\n        etree.SubElement(node, qualify('Execution',\n                                       BASE_NS_1_0)).text = cmd\n\n        return self._request(node)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncommits the candidate configuration as the device s new current configuration.", "response": "def request(self, confirmed=False, timeout=None, comment=None, synchronize=False, at_time=None, check=False):\n        \"\"\"Commit the candidate configuration as the device's new current configuration. Depends on the `:candidate` capability.\n\n        A confirmed commit (i.e. if *confirmed* is `True`) is reverted if there is no followup commit within the *timeout* interval. If no timeout is specified the confirm timeout defaults to 600 seconds (10 minutes). A confirming commit may have the *confirmed* parameter but this is not required. Depends on the `:confirmed-commit` capability.\n\n        *confirmed* whether this is a confirmed commit. Mutually exclusive with at_time.\n\n        *timeout* specifies the confirm timeout in seconds\n\n        *comment* a string to comment the commit with. Review on the device using 'show system commit'\n\n        *synchronize* Whether we should synch this commit across both Routing Engines\n\n        *at_time* Mutually exclusive with confirmed. The time at which the commit should happen. Junos expects either of these two formats:\n            A time value of the form hh:mm[:ss] (hours, minutes, and, optionally, seconds)\n            A date and time value of the form yyyy-mm-dd hh:mm[:ss] (year, month, date, hours, minutes, and, optionally, seconds)\n\n        *check* Verify the syntactic correctness of the candidate configuration\"\"\"\n        # NOTE: non netconf standard, Junos specific commit-configuration element, see\n        # https://www.juniper.net/documentation/en_US/junos/topics/reference/tag-summary/junos-xml-protocol-commit-configuration.html\n        node = new_ele_ns(\"commit-configuration\", \"\")\n        if confirmed and at_time is not None:\n            raise NCClientError(\"'Commit confirmed' and 'commit at' are mutually exclusive.\")\n        if confirmed:\n            self._assert(\":confirmed-commit\")\n            sub_ele(node, \"confirmed\")\n            if timeout is not None:\n                # NOTE: For Junos, confirm-timeout has to be given in minutes:\n                # https://www.juniper.net/documentation/en_US/junos/topics/reference/tag-summary/junos-xml-protocol-commit-configuration.html\n                # but the netconf standard and ncclient library uses seconds:\n                # https://tools.ietf.org/html/rfc6241#section-8.4.5\n                # http://ncclient.readthedocs.io/en/latest/manager.html#ncclient.manager.Manager.commit\n                # so need to convert the value in seconds to minutes.\n                timeout_int = int(timeout) if isinstance(timeout, str) else timeout\n                sub_ele(node, \"confirm-timeout\").text = str(int(math.ceil(timeout_int/60.0)))\n        elif at_time is not None:\n            sub_ele(node, \"at-time\").text = at_time\n        if comment is not None:\n            sub_ele(node, \"log\").text = comment\n        if synchronize:\n            sub_ele(node, \"synchronize\")\n        if check:\n            sub_ele(node, \"check\")\n        return self._request(node)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting and return the XML for an *ele* with specified encoding.", "response": "def to_xml(ele, encoding=\"UTF-8\", pretty_print=False):\n    \"Convert and return the XML for an *ele* (:class:`~xml.etree.ElementTree.Element`) with specified *encoding*.\"\n    xml = etree.tostring(ele, encoding=encoding, pretty_print=pretty_print)\n    if sys.version < '3':\n        return xml if xml.startswith('<?xml') else '<?xml version=\"1.0\" encoding=\"%s\"?>%s' % (encoding, xml)\n    else:\n        return xml.decode('UTF-8') if xml.startswith(b'<?xml') \\\n            else '<?xml version=\"1.0\" encoding=\"%s\"?>%s' % (encoding, xml.decode('UTF-8'))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_ele(x):\n    \"Convert and return the :class:`~xml.etree.ElementTree.Element` for the XML document *x*. If *x* is already an :class:`~xml.etree.ElementTree.Element` simply returns that.\"\n    if sys.version < '3':\n        return x if etree.iselement(x) else etree.fromstring(x, parser=parser)\n    else:\n        return x if etree.iselement(x) else etree.fromstring(x.encode('UTF-8'), parser=parser)", "response": "Convert and return the : class : ~xml. etree. ElementTree. Element for the XML document *x*. If *x* is already an : class : ~xml. etree. Element simply returns that."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the root element of an XML document or Element meets the supplied criteria.", "response": "def validated_element(x, tags=None, attrs=None):\n    \"\"\"Checks if the root element of an XML document or Element meets the supplied criteria.\n\n    *tags* if specified is either a single allowable tag name or sequence of allowable alternatives\n\n    *attrs* if specified is a sequence of required attributes, each of which may be a sequence of several allowable alternatives\n\n    Raises :exc:`XMLError` if the requirements are not met.\n    \"\"\"\n    ele = to_ele(x)\n    if tags:\n        if isinstance(tags, (str, bytes)):\n            tags = [tags]\n        if ele.tag not in tags:\n            raise XMLError(\"Element [%s] does not meet requirement\" % ele.tag)\n    if attrs:\n        for req in attrs:\n            if isinstance(req, (str, bytes)): req = [req]\n            for alt in req:\n                if alt in ele.attrib:\n                    break\n            else:\n                raise XMLError(\"Element [%s] does not have required attributes\" % ele.tag)\n    return ele"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef xpath(self, expression):\n        self.__expression = expression\n        self.__namespaces = XPATH_NAMESPACES\n        return self.__doc.xpath(self.__expression, namespaces=self.__namespaces)", "response": "xpath - returns the result of a call to lxml xpath"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns result for a call to lxml ElementPath find", "response": "def find(self, expression):\n        \"\"\"return result for a call to lxml ElementPath find()\"\"\"\n        self.__expression = expression\n        return self.__doc.find(self.__expression)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef findtext(self, expression):\n        self.__expression = expression\n        return self.__doc.findtext(self.__expression)", "response": "return the text of the element that matches the given expression"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a pretty - printed string output for rpc reply", "response": "def tostring(self):\n        \"\"\"return a pretty-printed string output for rpc reply\"\"\"\n        parser = etree.XMLParser(remove_blank_text=True)\n        outputtree = etree.XML(etree.tostring(self.__doc), parser)\n        return etree.tostring(outputtree, pretty_print=True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving xmlns attributes from rpc reply", "response": "def remove_namespaces(self, rpc_reply):\n        \"\"\"remove xmlns attributes from rpc reply\"\"\"\n        self.__xslt=self.__transform_reply\n        self.__parser = etree.XMLParser(remove_blank_text=True)\n        self.__xslt_doc = etree.parse(io.BytesIO(self.__xslt), self.__parser)\n        self.__transform = etree.XSLT(self.__xslt_doc)\n        self.__root = etree.fromstring(str(self.__transform(etree.parse(StringIO(str(rpc_reply))))))\n        return self.__root"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the *rpc - reply*.", "response": "def parse(self):\n        \"Parses the *rpc-reply*.\"\n        if self._parsed: return\n        root = self._root = to_ele(self._raw) # The <rpc-reply> element\n        # Per RFC 4741 an <ok/> tag is sent when there are no errors or warnings\n        ok = root.find(qualify(\"ok\"))\n        if ok is None:\n            # Create RPCError objects from <rpc-error> elements\n            error = root.find('.//'+qualify('rpc-error'))\n            if error is not None:\n                for err in root.getiterator(error.tag):\n                    # Process a particular <rpc-error>\n                    self._errors.append(self.ERROR_CLS(err))\n        self._parsing_hook(root)\n        self._parsed = True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending the request and process the reply.", "response": "def _request(self, op):\n        \"\"\"Implementations of :meth:`request` call this method to send the request and process the reply.\n\n        In synchronous mode, blocks until the reply is received and returns :class:`RPCReply`. Depending on the :attr:`raise_mode` a `rpc-error` element in the reply may lead to an :exc:`RPCError` exception.\n\n        In asynchronous mode, returns immediately, returning `self`. The :attr:`event` attribute will be set when the reply has been received (see :attr:`reply`) or an error occured (see :attr:`error`).\n\n        *op* is the operation to be requested as an :class:`~xml.etree.ElementTree.Element`\n        \"\"\"\n        self.logger.info('Requesting %r', self.__class__.__name__)\n        req = self._wrap(op)\n        self._session.send(req)\n        if self._async:\n            self.logger.debug('Async request, returning %r', self)\n            return self\n        else:\n            self.logger.debug('Sync request, will wait for timeout=%r', self._timeout)\n            self._event.wait(self._timeout)\n            if self._event.isSet():\n                if self._error:\n                    # Error that prevented reply delivery\n                    raise self._error\n                self._reply.parse()\n                if self._reply.error is not None and not self._device_handler.is_rpc_error_exempt(self._reply.error.message):\n                    # <rpc-error>'s [ RPCError ]\n\n                    if self._raise_mode == RaiseMode.ALL or (self._raise_mode == RaiseMode.ERRORS and self._reply.error.severity == \"error\"):\n                        errlist = []\n                        errors = self._reply.errors\n                        if len(errors) > 1:\n                            raise RPCError(to_ele(self._reply._raw), errs=errors)\n                        else:\n                            raise self._reply.error\n                if self._device_handler.transform_reply():\n                    return NCElement(self._reply, self._device_handler.transform_reply())\n                else:\n                    return self._reply\n            else:\n                raise TimeoutExpiredError('ncclient timed out while waiting for an rpc reply.')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_valid_with_defaults_modes(capabilities):\n    capability = capabilities[\":with-defaults\"]\n\n    try:\n        valid_modes = [capability.parameters[\"basic-mode\"]]\n    except KeyError:\n        raise WithDefaultsError(\n            \"Invalid 'with-defaults' capability URI advertised by the server; \"\n            \"missing 'basic-mode' parameter\"\n        )\n\n    try:\n        also_supported = capability.parameters[\"also-supported\"]\n    except KeyError:\n        return valid_modes\n\n    valid_modes.extend(also_supported.split(\",\"))\n\n    return valid_modes", "response": "Return a list of valid with - defaults modes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve running configuration and device state information.", "response": "def request(self, filter=None, with_defaults=None):\n        \"\"\"Retrieve running configuration and device state information.\n\n        *filter* specifies the portion of the configuration to retrieve (by default entire configuration is retrieved)\n\n        *with_defaults* defines an explicit method of retrieving default values from the configuration (see RFC 6243)\n\n        :seealso: :ref:`filter_params`\n        \"\"\"\n        node = new_ele(\"get\")\n        if filter is not None:\n            node.append(util.build_filter(filter))\n        if with_defaults is not None:\n            self._assert(\":with-defaults\")\n            _append_with_defaults_mode(\n                node,\n                with_defaults,\n                self._session.server_capabilities,\n            )\n        return self._request(node)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef request(self, identifier, version=None, format=None):\n        node = etree.Element(qualify(\"get-schema\",NETCONF_MONITORING_NS))\n        if identifier is not None:\n            elem = etree.Element(qualify(\"identifier\",NETCONF_MONITORING_NS))\n            elem.text = identifier\n            node.append(elem)\n        if version is not None:\n            elem = etree.Element(qualify(\"version\",NETCONF_MONITORING_NS))\n            elem.text = version\n            node.append(elem)\n        if format is not None:\n            elem = etree.Element(qualify(\"format\",NETCONF_MONITORING_NS))\n            elem.text = format\n            node.append(elem)\n        return self._request(node)", "response": "Get a named schema with optional revision and type."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndispatches an rpc command to retrieve the configuration of the current configuration.", "response": "def request(self, rpc_command, source=None, filter=None):\n        \"\"\"\n        *rpc_command* specifies rpc command to be dispatched either in plain text or in xml element format (depending on command)\n\n        *source* name of the configuration datastore being queried\n\n        *filter* specifies the portion of the configuration to retrieve (by default entire configuration is retrieved)\n\n        :seealso: :ref:`filter_params`\n\n        Examples of usage::\n\n            dispatch('clear-arp-table')\n\n        or dispatch element like ::\n\n            xsd_fetch = new_ele('get-xnm-information')\n            sub_ele(xsd_fetch, 'type').text=\"xml-schema\"\n            sub_ele(xsd_fetch, 'namespace').text=\"junos-configuration\"\n            dispatch(xsd_fetch)\n        \"\"\"\n\n\n        if etree.iselement(rpc_command):\n            node = rpc_command\n        else:\n            node = new_ele(rpc_command)\n        if source is not None:\n            node.append(util.datastore_or_url(\"source\", source, self._assert))\n        if filter is not None:\n            node.append(util.build_filter(filter))\n\n        return self._request(node)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_rpc_error_exempt(self, error_text):\n        if error_text is not None:\n            error_text = error_text.lower().strip()\n        else:\n            error_text = 'no error given'\n\n        # Compare the error text against all the exempt errors.\n        for ex in self._exempt_errors_exact_match:\n            if error_text == ex:\n                return True\n\n        for ex in self._exempt_errors_startwith_wildcard_match:\n            if error_text.endswith(ex):\n                return True\n\n        for ex in self._exempt_errors_endwith_wildcard_match:\n            if error_text.startswith(ex):\n                return True\n\n        for ex in self._exempt_errors_full_wildcard_match:\n            if ex in error_text:\n                return True\n\n        return False", "response": "Check whether an RPC error message is excempt."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of possible SSH subsystem names.", "response": "def get_ssh_subsystem_names(self):\n        \"\"\"\n        Return a list of possible SSH subsystem names.\n\n        Different NXOS versions use different SSH subsystem names for netconf.\n        Therefore, we return a list so that several can be tried, if necessary.\n\n        The Nexus device handler also accepts\n\n        \"\"\"\n        preferred_ssh_subsystem = self.device_params.get(\"ssh_subsystem_name\")\n        name_list = [ \"netconf\", \"xmlagent\" ]\n        if preferred_ssh_subsystem:\n            return [ preferred_ssh_subsystem ] + \\\n                        [ n for n in name_list if n != preferred_ssh_subsystem ]\n        else:\n            return name_list"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_listener(self, listener):\n        self.logger.debug('installing listener %r', listener)\n        if not isinstance(listener, SessionListener):\n            raise SessionError(\"Listener must be a SessionListener type\")\n        with self._lock:\n            self._listeners.add(listener)", "response": "Register a listener that will be notified of incoming messages and the session errors."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_listener(self, listener):\n        self.logger.debug('discarding listener %r', listener)\n        with self._lock:\n            self._listeners.discard(listener)", "response": "Unregister some listener ; ignore it."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_listener_instance(self, cls):\n        with self._lock:\n            for listener in self._listeners:\n                if isinstance(listener, cls):\n                    return listener", "response": "Returns the first instance of the specified class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend the supplied message to the NETCONF server.", "response": "def send(self, message):\n        \"\"\"Send the supplied *message* (xml string) to NETCONF server.\"\"\"\n        if not self.connected:\n            raise TransportError('Not connected to NETCONF server')\n        self.logger.debug('queueing %s', message)\n        self._q.put(message)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a list of capability URI s returns <hello > message XML string", "response": "def build(capabilities, device_handler):\n        \"Given a list of capability URI's returns <hello> message XML string\"\n        if device_handler:\n            # This is used as kwargs dictionary for lxml's Element() function.\n            # Therefore the arg-name (\"nsmap\") is used as key here.\n            xml_namespace_kwargs = { \"nsmap\" : device_handler.get_xml_base_namespace_dict() }\n        else:\n            xml_namespace_kwargs = {}\n        hello = new_ele(\"hello\", **xml_namespace_kwargs)\n        caps = sub_ele(hello, \"capabilities\")\n        def fun(uri): sub_ele(caps, \"capability\").text = uri\n        #python3 changes\n        if sys.version < '3':\n            map(fun, capabilities)\n        else:\n            list(map(fun, capabilities))\n        return to_xml(hello)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns tuple of ( session - id ( str ) capabilities ( Capabilities )", "response": "def parse(raw):\n        \"Returns tuple of (session-id (str), capabilities (Capabilities)\"\n        sid, capabilities = 0, []\n        root = to_ele(raw)\n        for child in root.getchildren():\n            if child.tag == qualify(\"session-id\") or child.tag == \"session-id\":\n                sid = child.text\n            elif child.tag == qualify(\"capabilities\") or child.tag == \"capabilities\" :\n                for cap in child.getchildren():\n                    if cap.tag == qualify(\"capability\") or cap.tag == \"capability\":\n                        capabilities.append(cap.text)\n        return sid, Capabilities(capabilities)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef request(self, filter=None, stream_name=None, start_time=None, stop_time=None):\n        node = new_ele_ns(\"create-subscription\", NETCONF_NOTIFICATION_NS)\n        if filter is not None:\n            node.append(util.build_filter(filter))\n        if stream_name is not None:\n            sub_ele(node, \"stream\").text = stream_name\n\n        if start_time is not None:\n            sub_ele(node, \"startTime\").text = start_time\n\n        if stop_time is not None:\n            if start_time is None:\n                raise ValueError(\"You must provide start_time if you provide stop_time\")\n            sub_ele(node, \"stopTime\").text = stop_time\n\n        return self._request(node)", "response": "Creates a new subscription for notifications from the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading all or part of the specified config to the specified target configuration datastore.", "response": "def request(self, config, format='xml', target='candidate', default_operation=None,\n            test_option=None, error_option=None):\n        \"\"\"Loads all or part of the specified *config* to the *target* configuration datastore.\n\n        *target* is the name of the configuration datastore being edited\n\n        *config* is the configuration, which must be rooted in the `config` element. It can be specified either as a string or an :class:`~xml.etree.ElementTree.Element`.\n\n        *default_operation* if specified must be one of { `\"merge\"`, `\"replace\"`, or `\"none\"` }\n\n        *test_option* if specified must be one of { `\"test_then_set\"`, `\"set\"` }\n\n        *error_option* if specified must be one of { `\"stop-on-error\"`, `\"continue-on-error\"`, `\"rollback-on-error\"` }\n\n        The `\"rollback-on-error\"` *error_option* depends on the `:rollback-on-error` capability.\n        \"\"\"\n        node = new_ele(\"edit-config\")\n        node.append(util.datastore_or_url(\"target\", target, self._assert))\n        if error_option is not None:\n            if error_option == \"rollback-on-error\":\n                self._assert(\":rollback-on-error\")\n            sub_ele(node, \"error-option\").text = error_option\n        if test_option is not None:\n            self._assert(':validate')\n            sub_ele(node, \"test-option\").text = test_option\n        if default_operation is not None:\n        # TODO: check if it is a valid default-operation\n            sub_ele(node, \"default-operation\").text = default_operation\n# <<<<<<< HEAD\n#         node.append(validated_element(config, (\"config\", qualify(\"config\"))))\n# =======\n        if format == 'xml':\n            node.append(validated_element(config, (\"config\", qualify(\"config\"))))\n        if format == 'text':\n            config_text = sub_ele(node, \"config-text\")\n            sub_ele(config_text, \"configuration-text\").text = config\n# >>>>>>> juniper\n        return self._request(node)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes a configuration datastore.", "response": "def request(self, target):\n        \"\"\"Delete a configuration datastore.\n\n        *target* specifies the  name or URL of configuration datastore to delete\n\n        :seealso: :ref:`srctarget_params`\"\"\"\n        node = new_ele(\"delete-config\")\n        node.append(util.datastore_or_url(\"target\", target, self._assert))\n        return self._request(node)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef request(self, source, target):\n        node = new_ele(\"copy-config\")\n        node.append(util.datastore_or_url(\"target\", target, self._assert))\n\n        try:\n            # datastore name or URL\n            node.append(util.datastore_or_url(\"source\", source, self._assert))\n        except Exception:\n            # `source` with `config` element containing the configuration subtree to copy\n            node.append(validated_element(source, (\"source\", qualify(\"source\"))))\n\n        return self._request(node)", "response": "Create or replace an entire configuration datastore with the contents of another complete\n        configuration datastore."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating the contents of the specified configuration.", "response": "def request(self, source=\"candidate\"):\n        \"\"\"Validate the contents of the specified configuration.\n\n        *source* is the name of the configuration datastore being validated or `config` element containing the configuration subtree to be validated\n\n        :seealso: :ref:`srctarget_params`\"\"\"\n        node = new_ele(\"validate\")\n        if type(source) is str:\n            src = util.datastore_or_url(\"source\", source, self._assert)\n        else:\n            validated_element(source, (\"config\", qualify(\"config\")))\n            src = new_ele(\"source\")\n            src.append(source)\n        node.append(src)\n        return self._request(node)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncommitting the candidate configuration as the device s new current configuration.", "response": "def request(self, confirmed=False, timeout=None, persist=None):\n        \"\"\"Commit the candidate configuration as the device's new current configuration. Depends on the `:candidate` capability.\n\n        A confirmed commit (i.e. if *confirmed* is `True`) is reverted if there is no followup commit within the *timeout* interval. If no timeout is specified the confirm timeout defaults to 600 seconds (10 minutes). A confirming commit may have the *confirmed* parameter but this is not required. Depends on the `:confirmed-commit` capability.\n\n        *confirmed* whether this is a confirmed commit\n\n        *timeout* specifies the confirm timeout in seconds\n\n        *persist* make the confirmed commit survive a session termination, and set a token on the ongoing confirmed commit\n        \"\"\"\n        node = new_ele(\"commit\")\n        if confirmed:\n            self._assert(\":confirmed-commit\")\n            sub_ele(node, \"confirmed\")\n            if timeout is not None:\n                sub_ele(node, \"confirm-timeout\").text = timeout\n            if persist is not None:\n                sub_ele(node, \"persist\").text = persist\n\n        return self._request(node)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncancel an ongoing confirmed commit.", "response": "def request(self, persist_id=None):\n        \"\"\"Cancel an ongoing confirmed commit. Depends on the `:candidate` and `:confirmed-commit` capabilities.\n\n        *persist-id* value must be equal to the value given in the <persist> parameter to the previous <commit> operation.\n        \"\"\"\n        node = new_ele(\"cancel-commit\")\n        if persist_id is not None:\n            sub_ele(node, \"persist-id\").text = persist_id\n\n        return self._request(node)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nallow the client to lock the configuration system of a device.", "response": "def request(self, target=\"candidate\"):\n        \"\"\"Allows the client to lock the configuration system of a device.\n\n        *target* is the name of the configuration datastore to lock\n        \"\"\"\n        node = new_ele(\"lock\")\n        sub_ele(sub_ele(node, \"target\"), target)\n        return self._request(node)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef request(self, command=None):\n        node = new_ele('get')\n        filter = sub_ele(node, 'filter')\n        block = sub_ele(filter, 'oper-data-format-cli-block')\n        sub_ele(block, 'cli-show').text = command\n\n        return self._request(node)", "response": "Run CLI - show commands\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef request(self, content='xml', filter=None, detail=False):\n\n        node = new_ele('get-config')\n        node.append(util.datastore_or_url('source', 'running', self._assert))\n\n        if filter is not None:\n\n            if content == 'xml':\n                node.append(util.build_filter(('subtree', filter)))\n\n            elif content == 'cli':\n                rep = new_ele('filter')\n                sub_filter = sub_ele(rep, 'config-format-cli-block')\n\n                if filter is not None:\n                    for item in filter:\n                        if detail:\n                            sub_ele(sub_filter, 'cli-info-detail').text = item\n                        else:\n                            sub_ele(sub_filter, 'cli-info').text = item\n                else:\n                    if detail:\n                        sub_ele(sub_filter, 'cli-info-detail')\n                    else:\n                        sub_ele(sub_filter, 'cli-info')\n\n                node.append(validated_element(rep))\n\n        return self._request(node)", "response": "Request config from Alu router."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the next 10 bytes of the buffer.", "response": "def _parse10(self):\n\n        \"\"\"Messages are delimited by MSG_DELIM. The buffer could have grown by\n        a maximum of BUF_SIZE bytes everytime this method is called. Retains\n        state across method calls and if a chunk has been read it will not be\n        considered again.\"\"\"\n\n        self.logger.debug(\"parsing netconf v1.0\")\n        buf = self._buffer\n        buf.seek(self._parsing_pos10)\n        if MSG_DELIM in buf.read().decode('UTF-8'):\n            buf.seek(0)\n            msg, _, remaining = buf.read().decode('UTF-8').partition(MSG_DELIM)\n            msg = msg.strip()\n            if sys.version < '3':\n                self._dispatch_message(msg.encode())\n            else:\n                self._dispatch_message(msg)\n            # create new buffer which contains remaining of old buffer\n            self._buffer = StringIO()\n            self._buffer.write(remaining.encode())\n            self._parsing_pos10 = 0\n            if len(remaining) > 0:\n                # There could be another entire message in the\n                # buffer, so we should try to parse again.\n                self.logger.debug('Trying another round of parsing since there is still data')\n                self._parse10()\n        else:\n            # handle case that MSG_DELIM is split over two chunks\n            self._parsing_pos10 = buf.tell() - MSG_DELIM_LEN\n            if self._parsing_pos10 < 0:\n                self._parsing_pos10 = 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the 11 - bit message in the file and return a dict of the items.", "response": "def _parse11(self):\n\n        \"\"\"Messages are split into chunks. Chunks and messages are delimited\n        by the regex #RE_NC11_DELIM defined earlier in this file. Each\n        time we get called here either a chunk delimiter or an\n        end-of-message delimiter should be found iff there is enough\n        data. If there is not enough data, we will wait for more. If a\n        delimiter is found in the wrong place, a #NetconfFramingError\n        will be raised.\"\"\"\n\n        self.logger.debug(\"_parse11: starting\")\n\n        # suck in whole string that we have (this is what we will work on in\n        # this function) and initialize a couple of useful values\n        self._buffer.seek(0, os.SEEK_SET)\n        data = self._buffer.getvalue()\n        data_len = len(data)\n        start = 0\n        self.logger.debug('_parse11: working with buffer of %d bytes', data_len)\n        while True and start < data_len:\n            # match to see if we found at least some kind of delimiter\n            self.logger.debug('_parse11: matching from %d bytes from start of buffer', start)\n            re_result = RE_NC11_DELIM.match(data[start:])\n            if not re_result:\n\n                # not found any kind of delimiter just break; this should only\n                # ever happen if we just have the first few characters of a\n                # message such that we don't yet have a full delimiter\n                self.logger.debug('_parse11: no delimiter found, buffer=\"%s\"', data[start:].decode())\n                break\n\n            # save useful variables for reuse\n            re_start = re_result.start()\n            re_end = re_result.end()\n            self.logger.debug('_parse11: regular expression start=%d, end=%d', re_start, re_end)\n\n            # If the regex doesn't start at the beginning of the buffer,\n            # we're in trouble, so throw an error\n            if re_start != 0:\n                raise NetconfFramingError('_parse11: delimiter not at start of match buffer', data[start:])\n\n            if re_result.group(2):\n                # we've found the end of the message, need to form up\n                # whole message, save back remainder (if any) to buffer\n                # and dispatch the message\n                start += re_end\n                message = ''.join(self._message_list)\n                self._message_list = []\n                self.logger.debug('_parse11: found end of message delimiter')\n                self._dispatch_message(message)\n                break\n\n            elif re_result.group(1):\n                # we've found a chunk delimiter, and group(2) is the digit\n                # string that will tell us how many bytes past the end of\n                # where it was found that we need to have available to\n                # save the next chunk off\n                self.logger.debug('_parse11: found chunk delimiter')\n                digits = int(re_result.group(1))\n                self.logger.debug('_parse11: chunk size %d bytes', digits)\n                if (data_len-start) >= (re_end + digits):\n                    # we have enough data for the chunk\n                    fragment = textify(data[start+re_end:start+re_end+digits])\n                    self._message_list.append(fragment)\n                    start += re_end + digits\n                    self.logger.debug('_parse11: appending %d bytes', digits)\n                    self.logger.debug('_parse11: fragment = \"%s\"', fragment)\n                else:\n                    # we don't have enough bytes, just break out for now\n                    # after updating start pointer to start of new chunk\n                    start += re_start\n                    self.logger.debug('_parse11: not enough data for chunk yet')\n                    self.logger.debug('_parse11: setting start to %d', start)\n                    break\n\n        # Now out of the loop, need to see if we need to save back any content\n        if start > 0:\n            self.logger.debug(\n                '_parse11: saving back rest of message after %d bytes, original size %d',\n                start, data_len)\n            self._buffer = StringIO(data[start:])\n            if start < data_len:\n                self.logger.debug('_parse11: still have data, may have another full message!')\n                self._parse11()\n        self.logger.debug('_parse11: ending')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload host keys from an openssh known_hosts - style file. Can be called multiple times.", "response": "def load_known_hosts(self, filename=None):\n\n        \"\"\"Load host keys from an openssh :file:`known_hosts`-style file. Can\n        be called multiple times.\n\n        If *filename* is not specified, looks in the default locations i.e. :file:`~/.ssh/known_hosts` and :file:`~/ssh/known_hosts` for Windows.\n        \"\"\"\n\n        if filename is None:\n            filename = os.path.expanduser('~/.ssh/known_hosts')\n            try:\n                self._host_keys.load(filename)\n            except IOError:\n                # for windows\n                filename = os.path.expanduser('~/ssh/known_hosts')\n                try:\n                    self._host_keys.load(filename)\n                except IOError:\n                    pass\n        else:\n            self._host_keys.load(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconnects to a NETCONF server.", "response": "def connect(\n            self,\n            host,\n            port                = PORT_NETCONF_DEFAULT,\n            timeout             = None,\n            unknown_host_cb     = default_unknown_host_cb,\n            username            = None,\n            password            = None,\n            key_filename        = None,\n            allow_agent         = True,\n            hostkey_verify      = True,\n            hostkey_b64         = None,\n            look_for_keys       = True,\n            ssh_config          = None,\n            sock_fd             = None):\n\n        \"\"\"Connect via SSH and initialize the NETCONF session. First attempts the publickey authentication method and then password authentication.\n\n        To disable attempting publickey authentication altogether, call with *allow_agent* and *look_for_keys* as `False`.\n\n        *host* is the hostname or IP address to connect to\n\n        *port* is by default 830 (PORT_NETCONF_DEFAULT), but some devices use the default SSH port of 22 (PORT_SSH_DEFAULT) so this may need to be specified\n\n        *timeout* is an optional timeout for socket connect\n\n        *unknown_host_cb* is called when the server host key is not recognized. It takes two arguments, the hostname and the fingerprint (see the signature of :func:`default_unknown_host_cb`)\n\n        *username* is the username to use for SSH authentication\n\n        *password* is the password used if using password authentication, or the passphrase to use for unlocking keys that require it\n\n        *key_filename* is a filename where a the private key to be used can be found\n\n        *allow_agent* enables querying SSH agent (if found) for keys\n\n        *hostkey_verify* enables hostkey verification from ~/.ssh/known_hosts\n\n        *hostkey_b64* only connect when server presents a public hostkey matching this (obtain from server /etc/ssh/ssh_host_*pub or ssh-keyscan)\n\n        *look_for_keys* enables looking in the usual locations for ssh keys (e.g. :file:`~/.ssh/id_*`)\n\n        *ssh_config* enables parsing of an OpenSSH configuration file, if set to its path, e.g. :file:`~/.ssh/config` or to True (in this case, use :file:`~/.ssh/config`).\n\n        *sock_fd* is an already open socket which shall be used for this connection. Useful for NETCONF outbound ssh. Use host=None together with a valid sock_fd number\n        \"\"\"\n        if not (host or sock_fd):\n            raise SSHError(\"Missing host or socket fd\")\n\n        self._host = host\n\n        # Optionally, parse .ssh/config\n        config = {}\n        if ssh_config is True:\n            ssh_config = \"~/.ssh/config\" if sys.platform != \"win32\" else \"~/ssh/config\"\n        if ssh_config is not None:\n            config = paramiko.SSHConfig()\n            config.parse(open(os.path.expanduser(ssh_config)))\n            \n            # Save default Paramiko SSH port so it can be reverted\n            paramiko_default_ssh_port = paramiko.config.SSH_PORT\n            \n            # Change the default SSH port to the port specified by the user so expand_variables \n            # replaces %p with the passed in port rather than 22 (the defauld paramiko.config.SSH_PORT)\n\n            paramiko.config.SSH_PORT = port\n\n            config = config.lookup(host)\n\n            # paramiko.config.SSHconfig::expand_variables is called by lookup so we can set the SSH port\n            # back to the default\n            paramiko.config.SSH_PORT = paramiko_default_ssh_port\n\n            host = config.get(\"hostname\", host)\n            if username is None:\n                username = config.get(\"user\")\n            if key_filename is None:\n                key_filename = config.get(\"identityfile\")\n            if hostkey_verify:\n                userknownhostsfile = config.get(\"userknownhostsfile\")\n                if userknownhostsfile:\n                    self.load_known_hosts(os.path.expanduser(userknownhostsfile))\n            if timeout is None:\n                timeout = config.get(\"connecttimeout\")\n                if timeout:\n                    timeout = int(timeout)\n\n        if username is None:\n            username = getpass.getuser()\n\n        if sock_fd is None:\n            proxycommand = config.get(\"proxycommand\")\n            if proxycommand:\n                self.logger.debug(\"Configuring Proxy. %s\", proxycommand)\n                if not isinstance(proxycommand, six.string_types):\n                  proxycommand = [os.path.expanduser(elem) for elem in proxycommand]\n                else:\n                  proxycommand = os.path.expanduser(proxycommand)\n                sock = paramiko.proxy.ProxyCommand(proxycommand)\n            else:\n                for res in socket.getaddrinfo(host, port, socket.AF_UNSPEC, socket.SOCK_STREAM):\n                    af, socktype, proto, canonname, sa = res\n                    try:\n                        sock = socket.socket(af, socktype, proto)\n                        sock.settimeout(timeout)\n                    except socket.error:\n                        continue\n                    try:\n                        sock.connect(sa)\n                    except socket.error:\n                        sock.close()\n                        continue\n                    break\n                else:\n                    raise SSHError(\"Could not open socket to %s:%s\" % (host, port))\n        else:\n            if sys.version_info[0] < 3:\n                s = socket.fromfd(int(sock_fd), socket.AF_INET, socket.SOCK_STREAM)\n                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM, _sock=s)\n            else:\n                sock = socket.fromfd(int(sock_fd), socket.AF_INET, socket.SOCK_STREAM)\n            sock.settimeout(timeout)\n\n        self._transport = paramiko.Transport(sock)\n        self._transport.set_log_channel(logger.name)\n        if config.get(\"compression\") == 'yes':\n            self._transport.use_compression()\n\n        if hostkey_b64:\n            # If we need to connect with a specific hostkey, negotiate for only its type\n            hostkey_obj = None\n            for key_cls in [paramiko.DSSKey, paramiko.Ed25519Key, paramiko.RSAKey, paramiko.ECDSAKey]:\n                try:\n                    hostkey_obj = key_cls(data=base64.b64decode(hostkey_b64))\n                except paramiko.SSHException:\n                    # Not a key of this type - try the next\n                    pass\n            if not hostkey_obj:\n                # We've tried all known host key types and haven't found a suitable one to use - bail\n                raise SSHError(\"Couldn't find suitable paramiko key class for host key %s\" % hostkey_b64)\n            self._transport._preferred_keys = [hostkey_obj.get_name()]\n        elif self._host_keys:\n            # Else set preferred host keys to those we possess for the host\n            # (avoids situation where known_hosts contains a valid key for the host, but that key type is not selected during negotiation)\n            if port == PORT_SSH_DEFAULT:\n                known_hosts_lookup = host\n            else:\n                known_hosts_lookup = '[%s]:%s' % (host, port)\n            known_host_keys_for_this_host = self._host_keys.lookup(known_hosts_lookup)\n            if known_host_keys_for_this_host:\n                self._transport._preferred_keys = [x.key.get_name() for x in known_host_keys_for_this_host._entries]\n\n        # Connect\n        try:\n            self._transport.start_client()\n        except paramiko.SSHException as e:\n            raise SSHError('Negotiation failed: %s' % e)\n\n        server_key_obj = self._transport.get_remote_server_key()\n        fingerprint = _colonify(hexlify(server_key_obj.get_fingerprint()))\n\n        if hostkey_verify:\n            is_known_host = False\n\n            # For looking up entries for nonstandard (22) ssh ports in known_hosts\n            # we enclose host in brackets and append port number\n            if port == PORT_SSH_DEFAULT:\n                known_hosts_lookup = host\n            else:\n                known_hosts_lookup = '[%s]:%s' % (host, port)\n\n            if hostkey_b64:\n                # If hostkey specified, remote host /must/ use that hostkey\n                if(hostkey_obj.get_name() == server_key_obj.get_name() and hostkey_obj.asbytes() == server_key_obj.asbytes()):\n                    is_known_host = True\n            else:\n                # Check known_hosts\n                is_known_host = self._host_keys.check(known_hosts_lookup, server_key_obj)\n\n            if not is_known_host and not unknown_host_cb(host, fingerprint):\n                raise SSHUnknownHostError(known_hosts_lookup, fingerprint)\n\n        # Authenticating with our private key/identity\n        if key_filename is None:\n            key_filenames = []\n        elif isinstance(key_filename, (str, bytes)):\n            key_filenames = [key_filename]\n        else:\n            key_filenames = key_filename\n\n        self._auth(username, password, key_filenames, allow_agent, look_for_keys)\n\n        self._connected = True      # there was no error authenticating\n        self._closing.clear()\n\n        # TODO: leopoul: Review, test, and if needed rewrite this part\n        subsystem_names = self._device_handler.get_ssh_subsystem_names()\n        for subname in subsystem_names:\n            self._channel = self._transport.open_session()\n            self._channel_id = self._channel.get_id()\n            channel_name = \"%s-subsystem-%s\" % (subname, str(self._channel_id))\n            self._channel.set_name(channel_name)\n            try:\n                self._channel.invoke_subsystem(subname)\n            except paramiko.SSHException as e:\n                self.logger.info(\"%s (subsystem request rejected)\", e)\n                handle_exception = self._device_handler.handle_connection_exceptions(self)\n                # Ignore the exception, since we continue to try the different\n                # subsystem names until we find one that can connect.\n                # have to handle exception for each vendor here\n                if not handle_exception:\n                    continue\n            self._channel_name = self._channel.get_name()\n            self._post_connect()\n            return\n        raise SSHError(\"Could not open connection, possibly due to unacceptable\"\n                       \" SSH subsystem name.\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef one_of(*args):\n    \"Verifies that only one of the arguments is not None\"\n    for i, arg in enumerate(args):\n        if arg is not None:\n            for argh in args[i+1:]:\n                if argh is not None:\n                    raise OperationError(\"Too many parameters\")\n            else:\n                return\n    raise OperationError(\"Insufficient parameters\")", "response": "Verifies that only one of the arguments is not None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns model scale shape and skewness latent variables.", "response": "def _get_scale_and_shape(self, transformed_lvs):\n        \"\"\" Obtains model scale, shape and skewness latent variables\n\n        Parameters\n        ----------\n        transformed_lvs : np.array\n            Transformed latent variable vector\n\n        Returns\n        ----------\n        - Tuple of model scale, model shape, model skewness\n        \"\"\"\n\n        if self.scale is True:\n            if self.shape is True:\n                model_shape = transformed_lvs[-1]  \n                model_scale = transformed_lvs[-2]\n            else:\n                model_shape = 0\n                model_scale = transformed_lvs[-1]\n        else:\n            model_scale = 0\n            model_shape = 0 \n\n        if self.skewness is True:\n            model_skewness = transformed_lvs[-3]\n        else:\n            model_skewness = 0\n\n        return model_scale, model_shape, model_skewness"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets model scale shape skewness latent variables for a given set of simulations.", "response": "def _get_scale_and_shape_sim(self, transformed_lvs):\n        \"\"\" Obtains model scale, shape, skewness latent variables for\n        a 2d array of simulations.\n\n        Parameters\n        ----------\n        transformed_lvs : np.array\n            Transformed latent variable vector (2d - with draws of each variable)\n\n        Returns\n        ----------\n        - Tuple of np.arrays (each being scale, shape and skewness draws)\n        \"\"\"\n\n        if self.scale is True:\n            if self.shape is True:\n                model_shape = self.latent_variables.z_list[-1].prior.transform(transformed_lvs[-1, :]) \n                model_scale = self.latent_variables.z_list[-2].prior.transform(transformed_lvs[-2, :])\n            else:\n                model_shape = np.zeros(transformed_lvs.shape[1])\n                model_scale = self.latent_variables.z_list[-1].prior.transform(transformed_lvs[-1, :])\n        else:\n            model_scale = np.zeros(transformed_lvs.shape[1])\n            model_shape = np.zeros(transformed_lvs.shape[1])\n\n        if self.skewness is True:\n            model_skewness = self.latent_variables.z_list[-3].prior.transform(transformed_lvs[-3, :])\n        else:\n            model_skewness = np.zeros(transformed_lvs.shape[1])\n\n        return model_scale, model_shape, model_skewness"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate the structure of the ARIMAX model for the given latent variables.", "response": "def _normal_model(self, beta):\n        \"\"\" Creates the structure of the model (model matrices, etc) for\n        a Normal family ARIMAX model.\n\n        Parameters\n        ----------\n        beta : np.ndarray\n            Contains untransformed starting values for the latent variables\n\n        Returns\n        ----------\n        mu : np.ndarray\n            Contains the predicted values (location) for the time series\n\n        Y : np.ndarray\n            Contains the length-adjusted time series (accounting for lags)\n        \"\"\"     \n\n        Y = self.y[self.max_lag:]\n\n        # Transform latent variables\n        z = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n\n        # Constant and AR terms\n        if self.ar == 0:\n            mu = np.transpose(self.ar_matrix)\n        elif self.ar == 1:\n            mu = np.transpose(self.ar_matrix)*z[:-self.family_z_no-self.ma-len(self.X_names)][0]\n        else:\n            mu = np.matmul(np.transpose(self.ar_matrix),z[:-self.family_z_no-self.ma-len(self.X_names)])\n\n        # X terms\n        mu = mu + np.matmul(self.X[self.integ+self.max_lag:],z[self.ma+self.ar:(self.ma+self.ar+len(self.X_names))])\n\n        # MA terms\n        if self.ma != 0:\n            mu = arimax_recursion(z, mu, Y, self.max_lag, Y.shape[0], self.ar, self.ma)\n\n        return mu, Y"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate the structure of the model (model matrices, etc) for a mini-batch Normal family ARIMAX model. Here the structure is the same as for _normal_model() but we are going to sample a random choice of data points (of length mini_batch). Parameters ---------- beta : np.ndarray Contains untransformed starting values for the latent variables mini_batch : int Mini batch size for the data sampling Returns ---------- mu : np.ndarray Contains the predicted values (location) for the time series Y : np.ndarray Contains the length-adjusted time series (accounting for lags)", "response": "def _mb_normal_model(self, beta, mini_batch):\n        \"\"\" Creates the structure of the model (model matrices, etc) for\n        a mini-batch Normal family ARIMAX model.\n\n        Here the structure is the same as for _normal_model() but we are going to\n        sample a random choice of data points (of length mini_batch).\n\n        Parameters\n        ----------\n        beta : np.ndarray\n            Contains untransformed starting values for the latent variables\n\n        mini_batch : int\n            Mini batch size for the data sampling\n\n        Returns\n        ----------\n        mu : np.ndarray\n            Contains the predicted values (location) for the time series\n\n        Y : np.ndarray\n            Contains the length-adjusted time series (accounting for lags)\n        \"\"\"     \n\n        rand_int =  np.random.randint(low=0, high=self.data_length-mini_batch-self.max_lag+1)\n        sample = np.arange(start=rand_int, stop=rand_int+mini_batch)\n\n        data = self.y[sample]\n        X = self.X[sample, :]\n        Y = data[self.max_lag:]\n\n        if self.ar != 0:\n            ar_matrix = data[(self.max_lag-1):-1]\n            for i in range(1, self.ar):\n                ar_matrix = np.vstack((ar_matrix, data[(self.max_lag-i-1):-i-1]))\n        else:\n            ar_matrix = np.zeros(data.shape[0]-self.max_lag)\n\n        # Transform latent variables\n        z = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n\n        # Constant and AR terms\n        if self.ar == 0:\n            mu = np.transpose(ar_matrix)\n        elif self.ar == 1:\n            mu = np.transpose(ar_matrix)*z[:-self.family_z_no-self.ma-len(self.X_names)][0]\n        else:\n            mu = np.matmul(np.transpose(ar_matrix),z[:-self.family_z_no-self.ma-len(self.X_names)])\n\n        # X terms\n        mu = mu + np.matmul(X[self.integ+self.max_lag:],z[self.ma+self.ar:(self.ma+self.ar+len(self.X_names))])\n\n        # MA terms\n        if self.ma != 0:\n            mu = arimax_recursion(z, mu, Y, self.max_lag, Y.shape[0], self.ar, self.ma)\n\n        return mu, Y"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _summarize_simulations(self, mean_values, sim_vector, date_index, h, past_values):\n\n        error_bars = []\n        for pre in range(5,100,5):\n            error_bars.append(np.insert([np.percentile(i,pre) for i in sim_vector], 0, mean_values[-h-1]))\n        if self.latent_variables.estimation_method in ['M-H']:\n            forecasted_values = np.insert([np.mean(i) for i in sim_vector], 0, mean_values[-h-1])\n        else:\n            forecasted_values = mean_values[-h-1:]\n        plot_values = mean_values[-h-past_values:]\n        plot_index = date_index[-h-past_values:]\n        return error_bars, forecasted_values, plot_values, plot_index", "response": "Produces forecasted values to plot along with prediction intervals"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef normal_neg_loglik(self, beta):\n        mu, Y = self._model(beta)\n        return -np.sum(ss.norm.logpdf(Y, loc=mu, scale=self.latent_variables.z_list[-1].prior.transform(beta[-1])))", "response": "Calculates the negative log - likelihood of the model for Normal family\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the negative log - likelihood of the Normal model for a minibatch.", "response": "def normal_mb_neg_loglik(self, beta, mini_batch):\n        \"\"\" Calculates the negative log-likelihood of the Normal model for a minibatch\n\n        Parameters\n        ----------\n        beta : np.ndarray\n            Contains untransformed starting values for latent variables\n\n        mini_batch : int\n            Size of each mini batch of data\n\n        Returns\n        ----------\n        The negative logliklihood of the model\n        \"\"\"     \n\n        mu, Y = self._mb_model(beta, mini_batch)\n        return -np.sum(ss.norm.logpdf(Y, loc=mu, scale=self.latent_variables.z_list[-1].prior.transform(beta[-1])))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot_predict(self, h=5, past_values=20, intervals=True, oos_data=None, **kwargs):\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n\n            dep_var = self.formula.split(\"~\")[0]\n            oos_data[dep_var] = oos_data[dep_var].replace(np.nan, 0)\n\n            _, X_oos = dmatrices(self.formula, oos_data)\n            X_oos = np.array([X_oos])[0]\n            X_pred = X_oos[:h]\n\n            # Retrieve data, dates and (transformed) latent variables\n            mu, Y = self._model(self.latent_variables.get_z_values())         \n            date_index = self.shift_dates(h)\n\n            if self.latent_variables.estimation_method in ['M-H']:\n                sim_vector = self._sim_prediction_bayes(h, X_pred, 15000)\n                error_bars = []\n\n                for pre in range(5,100,5):\n                    error_bars.append(np.insert([np.percentile(i,pre) for i in sim_vector], 0, Y[-1]))\n\n                forecasted_values = np.insert([np.mean(i) for i in sim_vector], 0, Y[-1])\n                plot_values = np.append(Y[-1-past_values:-2], forecasted_values)\n                plot_index = date_index[-h-past_values:]\n\n            else:\n                t_z = self.transform_z()\n                mean_values = self._mean_prediction(mu, Y, h, t_z, X_pred)\n\n                if self.model_name2 == \"Skewt\":\n                    model_scale, model_shape, model_skewness = self._get_scale_and_shape(t_z)\n                    m1 = (np.sqrt(model_shape)*sp.gamma((model_shape-1.0)/2.0))/(np.sqrt(np.pi)*sp.gamma(model_shape/2.0))\n                    forecasted_values = mean_values[-h:] + (model_skewness - (1.0/model_skewness))*model_scale*m1 \n                else:\n                    forecasted_values = mean_values[-h:] \n\n                if intervals is True:\n                    sim_values = self._sim_prediction(mu, Y, h, t_z, X_pred, 15000)\n                else:\n                    sim_values = self._sim_prediction(mu, Y, h, t_z, X_pred, 2)\n\n                error_bars, forecasted_values, plot_values, plot_index = self._summarize_simulations(mean_values, sim_values, date_index, h, past_values)\n\n            plt.figure(figsize=figsize)\n            if intervals == True:\n                alpha =[0.15*i/float(100) for i in range(50,12,-2)]\n                for count, pre in enumerate(error_bars):\n                    plt.fill_between(date_index[-h-1:], error_bars[count], error_bars[-count-1],alpha=alpha[count])             \n            plt.plot(plot_index,plot_values)\n            plt.title(\"Forecast for \" + self.data_name)\n            plt.xlabel(\"Time\")\n            plt.ylabel(self.data_name)\n            plt.show()", "response": "Plots the predictive model of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking a forecast with the estimated model and predicts the values of the latent variables.", "response": "def predict(self, h=5, oos_data=None, intervals=False):\n        \"\"\" Makes forecast with the estimated model\n\n        Parameters\n        ----------\n        h : int (default : 5)\n            How many steps ahead would you like to forecast?\n\n        oos_data : pd.DataFrame\n            Data for the variables to be used out of sample (ys can be NaNs)\n\n        intervals : boolean (default: False)\n            Whether to return prediction intervals\n\n        Returns\n        ----------\n        - pd.DataFrame with predicted values\n        \"\"\" \n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n\n            dep_var = self.formula.split(\"~\")[0]\n            oos_data[dep_var] = oos_data[dep_var].replace(np.nan, 0)\n\n            _, X_oos = dmatrices(self.formula, oos_data)\n            X_oos = np.array([X_oos])[0]\n            X_pred = X_oos[:h]\n\n            # Retrieve data, dates and (transformed) latent variables\n            mu, Y = self._model(self.latent_variables.get_z_values())         \n            date_index = self.shift_dates(h)\n\n            if self.latent_variables.estimation_method in ['M-H']:\n                sim_vector = self._sim_prediction_bayes(h, X_pred, 15000)\n\n                forecasted_values = np.array([np.mean(i) for i in sim_vector])\n                prediction_01 = np.array([np.percentile(i, 1) for i in sim_vector])\n                prediction_05 = np.array([np.percentile(i, 5) for i in sim_vector])\n                prediction_95 = np.array([np.percentile(i, 95) for i in sim_vector])\n                prediction_99 = np.array([np.percentile(i, 99) for i in sim_vector])\n\n            else:\n                t_z = self.transform_z()\n                mean_values = self._mean_prediction(mu, Y, h, t_z, X_pred)\n\n                if self.model_name2 == \"Skewt\":\n                    model_scale, model_shape, model_skewness = self._get_scale_and_shape(t_z)\n                    m1 = (np.sqrt(model_shape)*sp.gamma((model_shape-1.0)/2.0))/(np.sqrt(np.pi)*sp.gamma(model_shape/2.0))\n                    forecasted_values = mean_values[-h:] + (model_skewness - (1.0/model_skewness))*model_scale*m1 \n                else:\n                    forecasted_values = mean_values[-h:] \n\n                if intervals is True:\n                    sim_values = self._sim_prediction(mu, Y, h, t_z, X_pred, 15000)\n                else:\n                    sim_values = self._sim_prediction(mu, Y, h, t_z, X_pred, 2)\n\n            if intervals is False:\n                result = pd.DataFrame(forecasted_values)\n                result.rename(columns={0:self.data_name}, inplace=True)\n            else:\n                # Get mean prediction and simulations (for errors)\n                if self.latent_variables.estimation_method not in ['M-H']:\n                    sim_values = self._sim_prediction(mu, Y, h, t_z, X_pred, 15000)\n                    prediction_01 = np.array([np.percentile(i, 1) for i in sim_values])\n                    prediction_05 = np.array([np.percentile(i, 5) for i in sim_values])\n                    prediction_95 = np.array([np.percentile(i, 95) for i in sim_values])\n                    prediction_99 = np.array([np.percentile(i, 99) for i in sim_values])\n\n                result = pd.DataFrame([forecasted_values, prediction_01, prediction_05, \n                    prediction_95, prediction_99]).T\n                result.rename(columns={0:self.data_name, 1: \"1% Prediction Interval\", \n                    2: \"5% Prediction Interval\", 3: \"95% Prediction Interval\", 4: \"99% Prediction Interval\"}, \n                    inplace=True)\n \n            result.index = date_index[-h:]\n\n            return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sample(self, nsims=1000):\n        if self.latent_variables.estimation_method not in ['BBVI', 'M-H']:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            lv_draws = self.draw_latent_variables(nsims=nsims)\n            mus = [self._model(lv_draws[:,i])[0] for i in range(nsims)]\n            model_scale, model_shape, model_skewness = self._get_scale_and_shape_sim(lv_draws)\n            data_draws = np.array([self.family.draw_variable(self.link(mus[i]), \n                np.repeat(model_scale[i], mus[i].shape[0]), np.repeat(model_shape[i], mus[i].shape[0]), \n                np.repeat(model_skewness[i], mus[i].shape[0]), mus[i].shape[0]) for i in range(nsims)])\n            return data_draws", "response": "Returns a numpy array of draws from the posterior predictive distribution\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ppc(self, nsims=1000, T=np.mean):\n        if self.latent_variables.estimation_method not in ['BBVI', 'M-H']:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            lv_draws = self.draw_latent_variables(nsims=nsims)\n            mus = [self._model(lv_draws[:,i])[0] for i in range(nsims)]\n            model_scale, model_shape, model_skewness = self._get_scale_and_shape_sim(lv_draws)\n            data_draws = np.array([self.family.draw_variable(self.link(mus[i]), \n                np.repeat(model_scale[i], mus[i].shape[0]), np.repeat(model_shape[i], mus[i].shape[0]), \n                np.repeat(model_skewness[i], mus[i].shape[0]), mus[i].shape[0]) for i in range(nsims)])\n            T_sims = T(self.sample(nsims=nsims), axis=1)\n            T_actual = T(self.data)\n            return len(T_sims[T_sims>T_actual])/nsims", "response": "Computes the posterior predictive p - value for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_ppc(self, nsims=1000, T=np.mean, **kwargs):\n        if self.latent_variables.estimation_method not in ['BBVI', 'M-H']:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            import matplotlib.pyplot as plt\n            import seaborn as sns\n\n            figsize = kwargs.get('figsize',(10,7))\n\n            lv_draws = self.draw_latent_variables(nsims=nsims)\n            mus = [self._model(lv_draws[:,i])[0] for i in range(nsims)]\n            model_scale, model_shape, model_skewness = self._get_scale_and_shape_sim(lv_draws)\n            data_draws = np.array([self.family.draw_variable(self.link(mus[i]), \n                np.repeat(model_scale[i], mus[i].shape[0]), np.repeat(model_shape[i], mus[i].shape[0]), \n                np.repeat(model_skewness[i], mus[i].shape[0]), mus[i].shape[0]) for i in range(nsims)])\n            T_sim = T(self.sample(nsims=nsims), axis=1)\n            T_actual = T(self.data)\n\n            if T == np.mean:\n                description = \" of the mean\"\n            elif T == np.max:\n                description = \" of the maximum\"\n            elif T == np.min:\n                description = \" of the minimum\"\n            elif T == np.median:\n                description = \" of the median\"\n            else:\n                description = \"\"\n\n            plt.figure(figsize=figsize)\n            ax = plt.subplot()\n            ax.axvline(T_actual)\n            sns.distplot(T_sim, kde=False, ax=ax)\n            ax.set(title='Posterior predictive' + description, xlabel='T(x)', ylabel='Frequency');\n            plt.show()", "response": "Plots histogram of the posterior with the discrepancy of the posterior with the given number of draws of the posterior and the given time period."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef approximating_model_reg(self, beta, T, Z, R, Q, h_approx, data, X, state_no):\n\n        H = np.ones(data.shape[0])*h_approx\n        mu = np.zeros(data.shape[0])\n\n        return H, mu", "response": "Creates approximating Gaussian state space model for Skewt measurement density"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef draw_variable(loc, scale, shape, skewness, nsims):\n        return loc + scale*Skewt.rvs(shape, skewness, nsims)", "response": "Draw random variables from Skew t distribution\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef first_order_score(y, mean, scale, shape, skewness):\n        m1 = (np.sqrt(shape)*sp.gamma((shape-1.0)/2.0))/(np.sqrt(np.pi)*sp.gamma(shape/2.0))\n        mean = mean + (skewness - (1.0/skewness))*scale*m1\n        if (y-mean)>=0:\n            return ((shape+1)/shape)*(y-mean)/(np.power(skewness*scale,2) + (np.power(y-mean,2)/shape))\n        else:\n            return ((shape+1)/shape)*(y-mean)/(np.power(scale,2) + (np.power(skewness*(y-mean),2)/shape))", "response": "GAS Skew t Update term using gradient only - native Python function\n           UpdateTerm"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rvs(df, gamma, n):\n\n        if type(n) == list:\n            u = np.random.uniform(size=n[0]*n[1])\n            result = Skewt.ppf(q=u, df=df, gamma=gamma)\n            result = np.split(result,n[0])\n            return np.array(result)\n        else:\n            u = np.random.uniform(size=n)\n            if isinstance(df, np.ndarray) or isinstance(gamma, np.ndarray):\n                return np.array([Skewt.ppf(q=np.array([u[i]]), df=df[i], gamma=gamma[i])[0] for i in range(n)])\n            else:\n                return Skewt.ppf(q=u, df=df, gamma=gamma)", "response": "Generates random variables from a Skew t distribution\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef logpdf(self, mu):\n        if self.transform is not None:\n            mu = self.transform(mu)    \n        return self.logpdf_internal_prior(mu, df=self.df0, loc=self.loc0, scale=self.scale0, gamma=self.gamma0)", "response": "Log PDF for Skew t prior\n        mu"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef markov_blanket(y, mean, scale, shape, skewness):\n        m1 = (np.sqrt(shape)*sp.gamma((shape-1.0)/2.0))/(np.sqrt(np.pi)*sp.gamma(shape/2.0))\n        mean = mean + (skewness - (1.0/skewness))*scale*m1\n        return Skewt.logpdf_internal(x=y, df=shape, loc=mean, gamma=skewness, scale=scale)", "response": "Markov blanket for each likelihood term in the univariate time series y with the given mean scale and shape and skewness."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pdf_internal(x, df, loc=0.0, scale=1.0, gamma = 1.0):\n        result = np.zeros(x.shape[0])\n        result[x<0] = 2.0/(gamma + 1.0/gamma)*stats.t.pdf(x=gamma*x[(x-loc) < 0], loc=loc[(x-loc) < 0]*gamma,df=df, scale=scale)\n        result[x>=0] = 2.0/(gamma + 1.0/gamma)*stats.t.pdf(x=x[(x-loc) >= 0]/gamma, loc=loc[(x-loc) >= 0]/gamma,df=df, scale=scale)\n        return result", "response": "Raw PDF function for the Skew t distribution"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pdf(self, mu):\n        if self.transform is not None:\n            mu = self.transform(mu)    \n        return self.pdf_internal(mu, df=self.df0, loc=self.loc0, scale=self.scale0, gamma=self.gamma0)", "response": "PDF for Skew t prior\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _model(self, beta):\n\n        Y = np.array(self.data[self.max_lag:self.data.shape[0]])\n        X = np.ones(Y.shape[0])\n        scores = np.zeros(Y.shape[0])\n\n        # Transform latent variables\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n        lmda = np.ones(Y.shape[0])*parm[0]\n\n        # Loop over time series\n        for t in range(0, Y.shape[0]):\n\n            if t < self.max_lag:\n\n                lmda[t] = parm[0]/(1-np.sum(parm[1:(self.p+1)]))\n\n            else:\n\n                # Loop over GARCH terms\n                for p_term in range(0, self.p):\n                    lmda[t] += parm[1+p_term]*lmda[t-p_term-1]\n\n                # Loop over Score terms\n                for q_term in range(0, self.q):\n                    lmda[t] += parm[1+self.p+q_term]*scores[t-q_term-1]\n\n                if self.leverage is True:\n                    lmda[t] += parm[-3]*np.sign(-(Y[t-1]-parm[-1]))*(scores[t-1]+1)\n\n            scores[t] = (((parm[-2]+1.0)*np.power(Y[t]-parm[-1],2))/float(parm[-2]*np.exp(lmda[t]) + np.power(Y[t]-parm[-1],2))) - 1.0\n        return lmda, Y, scores", "response": "Creates the structure of the model for the current state of the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates the model matrix for the mini batch model.", "response": "def _mb_model(self, beta, mini_batch):\n        \"\"\" Creates the structure of the model (model matrices etc) for mini batch model.\n        \n        Here the structure is the same as for _normal_model() but we are going to\n        sample a random choice of data points (of length mini_batch).\n\n        Parameters\n        ----------\n        beta : np.ndarray\n            Contains untransformed starting values for the latent variables\n\n        mini_batch : int\n            Mini batch size for the data sampling\n\n        Returns\n        ----------\n        mu : np.ndarray\n            Contains the predicted values (location) for the time series\n\n        Y : np.ndarray\n            Contains the length-adjusted time series (accounting for lags)\n        \"\"\"     \n\n        # Transform latent variables\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n\n        rand_int =  np.random.randint(low=0, high=self.data_length-mini_batch+1)\n        sample = np.arange(start=rand_int, stop=rand_int+mini_batch)\n        sampled_data = self.data[sample]\n\n        Y = np.array(sampled_data[self.max_lag:])\n        X = np.ones(Y.shape[0])\n        scores = np.zeros(Y.shape[0])\n        lmda = np.ones(Y.shape[0])*parm[0]\n\n        # Loop over time series\n        for t in range(0, Y.shape[0]):\n\n            if t < self.max_lag:\n\n                lmda[t] = parm[0]/(1-np.sum(parm[1:(self.p+1)]))\n\n            else:\n\n                # Loop over GARCH terms\n                for p_term in range(0, self.p):\n                    lmda[t] += parm[1+p_term]*lmda[t-p_term-1]\n\n                # Loop over Score terms\n                for q_term in range(0, self.q):\n                    lmda[t] += parm[1+self.p+q_term]*scores[t-q_term-1]\n\n                if self.leverage is True:\n                    lmda[t] += parm[-3]*np.sign(-(Y[t-1]-parm[-1]))*(scores[t-1]+1)\n\n            scores[t] = (((parm[-2]+1.0)*np.power(Y[t]-parm[-1],2))/float(parm[-2]*np.exp(lmda[t]) + np.power(Y[t]-parm[-1],2))) - 1.0\n        \n        return lmda, Y, scores"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _mean_prediction(self, lmda, Y, scores, h, t_params):\n\n        # Create arrays to iteratre over\n        lmda_exp = lmda.copy()\n        scores_exp = scores.copy()\n        Y_exp = Y.copy()\n\n        # Loop over h time periods          \n        for t in range(0, h):\n            new_value = t_params[0]\n\n            if self.p != 0:\n                for j in range(1, self.p+1):\n                    new_value += t_params[j]*lmda_exp[-j]\n\n            if self.q != 0:\n                for k in range(1, self.q+1):\n                    new_value += t_params[k+self.p]*scores_exp[-k]\n\n            if self.leverage is True:\n                new_value += t_params[1+self.p+self.q]*np.sign(-(Y_exp[-1]-t_params[-1]))*(scores_exp[-1]+1)\n\n            lmda_exp = np.append(lmda_exp, [new_value]) # For indexing consistency\n            scores_exp = np.append(scores_exp, [0]) # expectation of score is zero\n            Y_exp = np.append(Y_exp, [t_params[-1]])\n\n        return lmda_exp", "response": "This function creates an h - step ahead mean prediction for the given set of latent variables."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef predict(self, h=5, intervals=False):\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n\n            lmda, Y, scores = self._model(self.latent_variables.get_z_values())         \n            date_index = self.shift_dates(h)\n\n            if self.latent_variables.estimation_method in ['M-H']:\n                sim_vector = self._sim_prediction_bayes(h, 15000)\n                error_bars = []\n\n                for pre in range(5,100,5):\n                    error_bars.append(np.insert([np.percentile(i,pre) for i in sim_vector], 0, lmda[-1]))\n\n                forecasted_values = np.array([np.mean(i) for i in sim_vector])\n                prediction_01 = np.array([np.percentile(i, 1) for i in sim_vector])\n                prediction_05 = np.array([np.percentile(i, 5) for i in sim_vector])\n                prediction_95 = np.array([np.percentile(i, 95) for i in sim_vector])\n                prediction_99 = np.array([np.percentile(i, 99) for i in sim_vector])\n\n            else:\n                t_z = self.transform_z()\n\n                if intervals is True:\n                    sim_values = self._sim_prediction(lmda, Y, scores, h, t_z, 15000)\n                else:\n                    sim_values = self._sim_prediction(lmda, Y, scores, h, t_z, 2)\n\n                mean_values = self._sim_predicted_mean(lmda, Y, scores, h, t_z, 15000)\n                forecasted_values = mean_values[-h:]\n\n            if intervals is False:\n                result = pd.DataFrame(np.exp(forecasted_values/2.0))\n                result.rename(columns={0:self.data_name}, inplace=True)\n            else:\n                if self.latent_variables.estimation_method not in ['M-H']:\n                    sim_values = self._sim_prediction(lmda, Y, scores, h, t_z, 15000)\n                    prediction_01 = np.array([np.percentile(i, 1) for i in sim_values])\n                    prediction_05 = np.array([np.percentile(i, 5) for i in sim_values])\n                    prediction_95 = np.array([np.percentile(i, 95) for i in sim_values])\n                    prediction_99 = np.array([np.percentile(i, 99) for i in sim_values])\n\n                result = np.exp(pd.DataFrame([forecasted_values, prediction_01, prediction_05, \n                    prediction_95, prediction_99]).T/2.0)\n                result.rename(columns={0:self.data_name, 1: \"1% Prediction Interval\", \n                    2: \"5% Prediction Interval\", 3: \"95% Prediction Interval\", 4: \"99% Prediction Interval\"}, \n                    inplace=True)\n \n            result.index = date_index[-h:]\n\n            return result", "response": "Makes forecast with the estimated model and returns predicted values."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsample from the posterior predictive distribution", "response": "def sample(self, nsims=1000):\n        \"\"\" Samples from the posterior predictive distribution\n\n        Parameters\n        ----------\n        nsims : int (default : 1000)\n            How many draws from the posterior predictive distribution\n\n        Returns\n        ----------\n        - np.ndarray of draws from the data\n        \"\"\"     \n        if self.latent_variables.estimation_method not in ['BBVI', 'M-H']:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            lv_draws = self.draw_latent_variables(nsims=nsims)\n            sigmas = [self._model(lv_draws[:,i])[0] for i in range(nsims)]\n            data_draws = np.array([ss.t.rvs(loc=self.latent_variables.z_list[-1].prior.transform(lv_draws[-1,i]),\n                df=self.latent_variables.z_list[-2].prior.transform(lv_draws[-2,i]), scale=np.exp(sigmas[i]/2.0)) for i in range(nsims)])\n            return data_draws"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ppc(self, nsims=1000, T=np.mean):\n        if self.latent_variables.estimation_method not in ['BBVI', 'M-H']:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            lv_draws = self.draw_latent_variables(nsims=nsims)\n            sigmas = [self._model(lv_draws[:,i])[0] for i in range(nsims)]\n            data_draws = np.array([ss.t.rvs(loc=self.latent_variables.z_list[-1].prior.transform(lv_draws[-1,i]),\n                df=self.latent_variables.z_list[-2].prior.transform(lv_draws[-2,i]), scale=np.exp(sigmas[i]/2.0)) for i in range(nsims)])\n            T_sims = T(self.sample(nsims=nsims), axis=1)\n            T_actual = T(self.data)\n            return len(T_sims[T_sims>T_actual])/nsims", "response": "Computes posterior predictive p - value for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef itransform_define(transform):\n        if transform == 'tanh':\n            return np.arctanh\n        elif transform == 'exp':\n            return np.log\n        elif transform == 'logit':\n            return Family.logit\n        elif transform is None:\n            return np.array\n        else:\n            return None", "response": "This function links the user s choice of transformation with its inverse\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_latent_variables(self):\n\n        for p_term in range(self.p):\n            self.latent_variables.add_z('p(' + str(p_term+1) + ')', fam.Normal(0,0.5,transform='logit'), fam.Normal(0,3))\n            if p_term == 0:\n                self.latent_variables.z_list[-1].start = 3.00\n            else:\n                self.latent_variables.z_list[-1].start = -4.00\n\n        for q_term in range(self.q):\n            self.latent_variables.add_z('q(' + str(q_term+1) + ')', fam.Normal(0,0.5,transform='logit'), fam.Normal(0,3))\n            if q_term == 0:\n                self.latent_variables.z_list[-1].start = -1.50  \n            else: \n                self.latent_variables.z_list[-1].start = -4.00  \n\n        self.latent_variables.add_z('v', fam.Flat(transform='exp'), fam.Normal(0,3))\n        self.latent_variables.add_z('GARCH-M', fam.Normal(0, 3, transform=None),fam.Normal(0,3))\n\n        for parm in range(len(self.X_names)):\n            self.latent_variables.add_z('Vol Beta ' + self.X_names[parm], fam.Normal(0,10,transform=None), fam.Normal(0,3))\n\n        for parm in range(len(self.X_names)):\n            self.latent_variables.add_z('Returns Beta ' + self.X_names[parm], fam.Normal(0,10,transform=None), fam.Normal(0,3))\n\n        # Starting values        \n\n        for i in range(self.p+self.q, self.z_no):\n            self.latent_variables.z_list[i].start = 0.0\n\n        self.latent_variables.z_list[self.p+self.q].start = 2.0", "response": "Creates latent variables for the current class."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _model(self, beta):\n\n        Y = np.array(self.data[self.max_lag:self.data.shape[0]])\n        X = np.ones(Y.shape[0])\n        scores = np.zeros(Y.shape[0])\n\n        # Transform latent variables\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n\n        lmda = np.zeros(Y.shape[0])\n        theta = np.zeros(Y.shape[0])\n\n        # Loop over time series\n        for t in range(0,Y.shape[0]):\n            if t < self.max_lag:\n                lmda[t] = parm[-len(self.X_names)*2]/(1-np.sum(parm[:self.p]))\n                theta[t] = np.dot(self.X[t],parm[-len(self.X_names):])\n            else:\n                # Loop over GARCH terms\n                for p_term in range(0,self.p):\n                    lmda[t] += parm[p_term]*lmda[t-p_term-1]\n\n                # Loop over Score terms\n                for q_term in range(0,self.q):\n                    lmda[t] += parm[self.p+q_term]*scores[t-q_term-1]\n\n                if self.leverage is True:\n                    lmda[t] += parm[-(len(self.X_names)*2)-3]*np.sign(-(Y[t-1]-theta[t-1]))*(scores[t-1]+1)\n\n                lmda[t] += np.dot(self.X[t],parm[-len(self.X_names)*2:-len(self.X_names)])\n\n                theta[t] = np.dot(self.X[t],parm[-len(self.X_names):]) + parm[-(len(self.X_names)*2)-1]*np.exp(lmda[t]/2.0)\n            \n            scores[t] = (((parm[self.p+self.q]+1.0)*np.power(Y[t]-theta[t],2))/float(parm[self.p+self.q]*np.exp(lmda[t]) + np.power(Y[t]-theta[t],2))) - 1.0        \n\n        return lmda, Y, scores, theta", "response": "Creates the structure of the model for the current state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _mb_model(self, beta, mini_batch):\n\n        # Transform latent variables\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n\n        rand_int =  np.random.randint(low=0, high=self.data_length-mini_batch+1)\n        sample = np.arange(start=rand_int, stop=rand_int+mini_batch)\n\n        data = self.y[sample]\n        X = self.X[sample, :]\n        Y = data[self.max_lag:]\n\n        scores = np.zeros(Y.shape[0])\n        lmda = np.ones(Y.shape[0])\n        theta = np.ones(Y.shape[0])\n\n        # Loop over time series\n        for t in range(0,Y.shape[0]):\n            if t < self.max_lag:\n                lmda[t] = parm[-len(self.X_names)*2]/(1-np.sum(parm[:self.p]))\n                theta[t] = np.dot(self.X[t],parm[-len(self.X_names):])\n            else:\n                # Loop over GARCH terms\n                for p_term in range(0,self.p):\n                    lmda[t] += parm[p_term]*lmda[t-p_term-1]\n\n                # Loop over Score terms\n                for q_term in range(0,self.q):\n                    lmda[t] += parm[self.p+q_term]*scores[t-q_term-1]\n\n                if self.leverage is True:\n                    lmda[t] += parm[-(len(self.X_names)*2)-3]*np.sign(-(Y[t-1]-theta[t-1]))*(scores[t-1]+1)\n\n                lmda[t] += np.dot(self.X[t],parm[-len(self.X_names)*2:-len(self.X_names)])\n\n                theta[t] = np.dot(self.X[t],parm[-len(self.X_names):]) + parm[-(len(self.X_names)*2)-1]*np.exp(lmda[t]/2.0)\n            \n            scores[t] = (((parm[self.p+self.q]+1.0)*np.power(Y[t]-theta[t],2))/float(parm[self.p+self.q]*np.exp(lmda[t]) + np.power(Y[t]-theta[t],2))) - 1.0        \n\n        return lmda, Y, scores, theta", "response": "Creates the structure of the model for the current state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a h-step ahead mean prediction Parameters ---------- lmda : np.array The past predicted values Y : np.array The past data scores : np.array The past scores h : int How many steps ahead for the prediction t_params : np.array A vector of (transformed) latent variables X_oos : np.array Out of sample predictors Returns ---------- h-length vector of mean predictions", "response": "def _mean_prediction(self, lmda, Y, scores, h, t_params, X_oos):\n        \"\"\" Creates a h-step ahead mean prediction\n\n        Parameters\n        ----------\n        lmda : np.array\n            The past predicted values\n\n        Y : np.array\n            The past data\n\n        scores : np.array\n            The past scores\n\n        h : int\n            How many steps ahead for the prediction\n\n        t_params : np.array\n            A vector of (transformed) latent variables\n\n        X_oos : np.array\n            Out of sample predictors\n\n        Returns\n        ----------\n        h-length vector of mean predictions\n        \"\"\"     \n\n        # Create arrays to iteratre over\n        lmda_exp = lmda.copy()\n        scores_exp = scores.copy()\n        Y_exp = Y.copy()\n\n        # Loop over h time periods          \n        for t in range(0,h):\n            new_lambda_value = 0\n\n            if self.p != 0:\n                for j in range(self.p):\n                    new_lambda_value += t_params[j]*lmda_exp[-j-1]\n\n            if self.q != 0:\n                for k in range(self.q):\n                    new_lambda_value += t_params[k+self.p]*scores_exp[-k-1]\n\n            # No leverage term for mean (should be zero in expectation?)\n\n            new_lambda_value += np.dot(X_oos[t],t_params[-len(self.X_names)*2:-len(self.X_names)]) \n            new_theta_value = np.dot(X_oos[t],t_params[-len(self.X_names):]) + t_params[-(len(self.X_names)*2)-1]*np.exp(new_lambda_value/2.0)\n            lmda_exp = np.append(lmda_exp,[new_lambda_value]) # For indexing consistency\n            scores_exp = np.append(scores_exp,[0]) # expectation of score is zero\n            Y_exp = np.append(Y_exp,new_theta_value)\n\n        return lmda_exp"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_leverage(self):\n\n        if self.leverage is True:\n            pass\n        else:\n            self.leverage = True\n            self.z_no += 1\n            \n            for i in range(len(self.X_names)*2+3):\n                self.latent_variables.z_list.pop()            \n\n            for parm in range(len(self.X_names)):\n                self.latent_variables.add_z('Vol Beta ' + self.X_names[parm], fam.Normal(0,10,transform=None), fam.Normal(0,3))\n\n            for parm in range(len(self.X_names)):\n                self.latent_variables.add_z('Returns Beta ' + self.X_names[parm], fam.Normal(0,10,transform=None), fam.Normal(0,3))\n\n            self.latent_variables.add_z('Leverage Term', fam.Flat(transform=None), fam.Normal(0,3))\n            self.latent_variables.add_z('v', fam.Flat(transform='exp'), fam.Normal(0,3))\n            self.latent_variables.add_z('Returns Constant', fam.Normal(0,3,transform=None), fam.Normal(0,3))\n            self.latent_variables.add_z('GARCH-M', fam.Normal(0,3,transform=None), fam.Normal(0,3))\n            self.latent_variables.z_list[-3].start = 2.0", "response": "Adds leverage term to the model"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef neg_loglik(self, beta):\n\n        lmda, Y, _, theta = self._model(beta)\n        return -np.sum(ss.t.logpdf(x=Y,\n            df=self.latent_variables.z_list[-(len(self.X_names)*2)-2].prior.transform(beta[-(len(self.X_names)*2)-2]),\n            loc=theta,scale=np.exp(lmda/2.0)))", "response": "Creates the negative log - likelihood of the model for the given set of latent variables."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _alpha(self, L):\n        return la.cho_solve((L.T, True), la.cho_solve((L, True), np.transpose(self.data)))", "response": "Covariance - derived term to construct expectations. See Rasmussen & Williams.\n        \n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _construct_predict(self, beta, h):    \n\n        # Refactor this entire code in future\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n        Xstart = self.X().copy()\n        Xstart = [i for i in Xstart]\n        predictions = np.zeros(h)\n        variances = np.zeros(h)\n\n        for step in range(0,h):\n            Xstar = []\n\n            for lag in range(0,self.max_lag):\n                if lag == 0:\n                    if step == 0:\n                        Xstar.append([self.data[-1]])\n                        Xstart[0] = np.append(Xstart[0],self.data[-1])\n                    else:\n                        Xstar.append([predictions[step-1]])\n                        Xstart[0] = np.append(Xstart[0],predictions[step-1])\n                else:\n                    Xstar.append([Xstart[lag-1][-2]])\n                    Xstart[lag] = np.append(Xstart[lag],Xstart[lag-1][-2])\n\n            Kstar = self.kernel.Kstar(parm, np.transpose(np.array(Xstar)))\n\n            L = self._L(parm)\n            alpha = self._alpha(L)   \n\n            predictions[step] = np.dot(np.transpose(Kstar), alpha)\n            v = la.cho_solve((L, True), Kstar)\n            variances[step] = self.kernel.Kstarstar(parm, np.transpose(np.array(Xstar))) - np.dot(v.T, v)\n\n        return predictions, variances, predictions - 1.98*np.power(variances,0.5), predictions + 1.98*np.power(variances,0.5)", "response": "Constructs the predict and variance for the Gaussian process."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_latent_variables(self):\n\n        # Create latent variables\n        for no, i in enumerate(self.kernel.build_latent_variables()):\n            self.latent_variables.add_z(i[0],i[1],i[2])\n            self.latent_variables.z_list[no].start = i[3]\n\n        self.z_no = len(self.kernel.build_latent_variables())\n\n        # Use an ARIMA model to find starting point for the initial noise latent variable\n        arma_start = arma.ARIMA(self.data, ar=self.ar, ma=0, integ=self.integ)\n        x = arma_start.fit()\n        arma_starting_values = arma_start.latent_variables.get_z_values()\n        self.latent_variables.z_list[0].start = np.log(np.exp(np.power(arma_starting_values[-1],2)))", "response": "Creates latent variables for the current class"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate cholesky decomposition of covariance matrix of latent variables and returns the matrix L.", "response": "def _L(self, parm):\n        \"\"\" Creates cholesky decomposition of covariance matrix\n\n        Parameters\n        ----------\n        parm : np.array\n            Contains transformed latent variables\n\n        Returns\n        ----------\n        The cholesky decomposition (L) of K\n        \"\"\" \n\n        return np.linalg.cholesky(self.kernel.K(parm) + np.identity(self.X().shape[1])*parm[0])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating the design matrix of variables to use in GP regression Returns ---------- X The design matrix of variables to use in GP regression", "response": "def X(self):\n        \"\"\" Creates design matrix of variables to use in GP regression\n        \n        Returns\n        ----------\n        The design matrix\n        \"\"\"     \n        if self.ar == 1:\n            return np.array([self.data_full[(self.max_lag-1):-1]])\n        else:\n            for i in range(0,self.ar):\n                datapoint = self.data_full[(self.max_lag-i-1):-i-1]         \n                if i == 0:\n                    X = datapoint\n                else:\n                    X = np.vstack((X,datapoint))\n        return X"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the expected values of the function given the covariance matrix and hyperparameters.", "response": "def expected_values(self, beta):\n        \"\"\" Expected values of the function given the covariance matrix and hyperparameters\n        \n        Parameters\n        ----------\n        beta : np.ndarray\n            Contains untransformed values for latent variables\n        \n        Returns\n        ----------\n        The expected values of the function\n        \"\"\"     \n\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n        L = self._L(parm)\n        alpha = self._alpha(L)\n        return np.dot(np.transpose(self.kernel.K(parm)), alpha)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef variance_values(self, beta):\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n        L = self._L(parm)\n        v = la.cho_solve((L, True), self.kernel.K(parm))\n        return self.kernel.K(parm) - np.dot(v.T, v)", "response": "Returns the variance of the estimated function in the given latent variables."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating the negative log marginal likelihood of the model with respect to the model s latent variables.", "response": "def full_neg_loglik(self, beta):\n        \"\"\" Creates the negative log marginal likelihood of the model\n        \n        Parameters\n        ----------\n        beta : np.array\n            Contains untransformed starting values for latent variables\n        \n        Returns\n        ----------\n        The negative log marginal logliklihood of the model\n        \"\"\"             \n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n        L = self._L(parm)\n        return -(-0.5*(np.dot(np.transpose(self.data),self._alpha(L))) - np.log(np.diag(L)).sum() - (self.data.shape[0]/2.0)*np.log(2.0*np.pi))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nplotting the fit of the Gaussian process model to the data.", "response": "def plot_fit(self, intervals=True, **kwargs):\n        \"\"\" Plots the fit of the Gaussian process model to the data\n\n        Parameters\n        ----------\n        beta : np.array\n            Contains untransformed starting values for latent variables\n        \n        intervals : Boolean\n            Whether to plot uncertainty intervals or not\n        \n        Returns\n        ----------\n        None (plots the fit of the function)\n        \"\"\"\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n\n        date_index = self.index[self.max_lag:]\n        expectation = self.expected_values(self.latent_variables.get_z_values())\n        variance = self.variance_values(self.latent_variables.get_z_values())\n        upper = expectation + 1.98*np.power(np.diag(variance),0.5)\n        lower = expectation - 1.98*np.power(np.diag(variance),0.5)\n\n        plt.figure(figsize=figsize) \n\n        plt.subplot(2, 2, 1)\n        plt.title(self.data_name + \" Raw\")  \n        plt.plot(date_index,self.data*self._norm_std + self._norm_mean,'k')\n\n        plt.subplot(2, 2, 2)\n        plt.title(self.data_name + \" Raw and Expected\") \n        plt.plot(date_index,self.data*self._norm_std + self._norm_mean,'k',alpha=0.2)\n        plt.plot(date_index,self.expected_values(self.latent_variables.get_z_values())*self._norm_std + self._norm_mean,'b')\n\n        plt.subplot(2, 2, 3)\n        plt.title(self.data_name + \" Raw and Expected (with intervals)\")    \n\n        if intervals == True:\n            plt.fill_between(date_index, lower*self._norm_std + self._norm_mean, upper*self._norm_std + self._norm_mean, alpha=0.2)          \n            \n        plt.plot(date_index,self.data*self._norm_std + self._norm_mean,'k',alpha=0.2)\n        plt.plot(date_index,self.expected_values(self.latent_variables.get_z_values())*self._norm_std + self._norm_mean,'b')\n\n        plt.subplot(2, 2, 4)\n\n        plt.title(\"Expected \" + self.data_name + \" (with intervals)\")   \n\n        if intervals == True:\n            plt.fill_between(date_index, lower*self._norm_std + self._norm_mean, upper*self._norm_std + self._norm_mean, alpha=0.2)          \n            \n        plt.plot(date_index,self.expected_values(self.latent_variables.get_z_values())*self._norm_std + self._norm_mean,'b')\n\n        plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot the predicted model for the current object.", "response": "def plot_predict(self, h=5, past_values=20, intervals=True,**kwargs):\n        \"\"\" Plots forecast with the estimated model\n        \n        Parameters\n        ----------\n        h : int (default : 5)\n            How many steps ahead would you like to forecast?\n        \n        past_values : int (default : 20)\n            How many past observations to show on the forecast graph?\n        \n        intervals : Boolean\n            Would you like to show 95% prediction intervals for the forecast?\n        \n        Returns\n        ----------\n        - Plot of the forecast\n        - Error bars, forecasted_values, plot_values, plot_index\n        \"\"\"\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n\n            predictions, variance, lower, upper = self._construct_predict(self.latent_variables.get_z_values(),h) \n            full_predictions = np.append(self.data,predictions)\n            full_lower = np.append(self.data,lower)\n            full_upper = np.append(self.data,upper)\n            date_index = self.shift_dates(h)\n\n            # Plot values (how far to look back)\n            plot_values = full_predictions[-h-past_values:]*self._norm_std + self._norm_mean\n            plot_index = date_index[-h-past_values:]\n\n            # Lower and upper intervals\n            lower = np.append(full_predictions[-h-1],lower)\n            upper = np.append(full_predictions[-h-1],upper)\n\n            plt.figure(figsize=figsize)\n            if intervals == True:\n                plt.fill_between(date_index[-h-1:], \n                    lower*self._norm_std + self._norm_mean, \n                    upper*self._norm_std + self._norm_mean,\n                    alpha=0.2)          \n            \n            plt.plot(plot_index,plot_values)\n            plt.title(\"Forecast for \" + self.data_name)\n            plt.xlabel(\"Time\")\n            plt.ylabel(self.data_name)\n            plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmake forecast with the estimated model and returns a pd. DataFrame with predicted values.", "response": "def predict(self, h=5):\n        \"\"\" Makes forecast with the estimated model\n        \n        Parameters\n        ----------\n        h : int (default : 5)\n            How many steps ahead would you like to forecast?\n        \n        Returns\n        ----------\n        - pd.DataFrame with predicted values\n        \"\"\"     \n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            predictions, _, _, _ = self._construct_predict(self.latent_variables.get_z_values(),h)    \n            predictions = predictions*self._norm_std + self._norm_mean  \n            date_index = self.shift_dates(h)\n            result = pd.DataFrame(predictions)\n            result.rename(columns={0:self.data_name}, inplace=True)\n            result.index = date_index[-h:]\n\n            return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_model_matrices(self):\n\n        self.model_Y = np.array(self.data[self.max_lag:self.data.shape[0]])\n        self.model_scores = np.zeros(self.model_Y.shape[0])", "response": "Creates model matrices and vectors for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_scale_and_shape(self,parm):\n\n        if self.scale is True:\n            if self.shape is True:\n                model_shape = parm[-1]  \n                model_scale = parm[-2]\n            else:\n                model_shape = 0\n                model_scale = parm[-1]\n        else:\n            model_scale = 0\n            model_shape = 0 \n\n        if self.skewness is True:\n            model_skewness = parm[-3]\n        else:\n            model_skewness = 0\n\n        return model_scale, model_shape, model_skewness", "response": "Returns model scale model shape and skewness of the latent variable."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot the sample of the posterior predictive density against the data.", "response": "def plot_sample(self, nsims=10, plot_data=True, **kwargs):\n        \"\"\"\n        Plots draws from the posterior predictive density against the data\n\n        Parameters\n        ----------\n        nsims : int (default : 1000)\n            How many draws from the posterior predictive distribution\n\n        plot_data boolean\n            Whether to plot the data or not\n        \"\"\"\n\n        if self.latent_variables.estimation_method not in ['BBVI', 'M-H']:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            import matplotlib.pyplot as plt\n            import seaborn as sns\n\n            figsize = kwargs.get('figsize',(10,7))\n            plt.figure(figsize=figsize)\n            date_index = self.index[max(self.ar, self.sc):self.data_length]\n            mu, Y, scores = self._model(self.latent_variables.get_z_values())\n            draws = self.sample(nsims).T\n            plt.plot(date_index, draws, label='Posterior Draws', alpha=1.0)\n            if plot_data is True:\n                plt.plot(date_index, Y, label='Data', c='black', alpha=0.5, linestyle='', marker='s')\n            plt.title(self.data_name)\n            plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ar_matrix(self):\n\n        X = np.ones(self.data_length-self.max_lag)\n\n        if self.ar != 0:\n            for i in range(0, self.ar):\n                X = np.vstack((X,self.data[(self.max_lag-i-1):-i-1]))\n\n        return X", "response": "Creates the Autoregressive matrix for the model."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_latent_variables(self):\n\n        self.latent_variables.add_z('Constant', fam.Normal(0,3,transform=None), fam.Normal(0, 3))\n\n        for ar_term in range(self.ar):\n            self.latent_variables.add_z('AR(' + str(ar_term+1) + ')', fam.Normal(0,0.5,transform=None), fam.Normal(0, 3))\n\n        for ma_term in range(self.ma):\n            self.latent_variables.add_z('MA(' + str(ma_term+1) + ')', fam.Normal(0,0.5,transform=None), fam.Normal(0, 3))", "response": "Creates the model s latent variables."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating the structure of the model for the normal family ARIMA model.", "response": "def _normal_model(self, beta):\n        \"\"\" Creates the structure of the model (model matrices etc) for\n        a Normal family ARIMA model.\n\n        Parameters\n        ----------\n        beta : np.ndarray\n            Contains untransformed starting values for the latent variables\n\n        Returns\n        ----------\n        mu : np.ndarray\n            Contains the predicted values (location) for the time series\n\n        Y : np.ndarray\n            Contains the length-adjusted time series (accounting for lags)\n        \"\"\"     \n\n        Y = np.array(self.data[self.max_lag:])\n\n        # Transform latent variables\n        z = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n\n        # Constant and AR terms\n        if self.ar != 0:\n            mu = np.matmul(np.transpose(self.X),z[:-self.family_z_no-self.ma])\n        else:\n            mu = np.ones(Y.shape[0])*z[0]\n            \n        # MA terms\n        if self.ma != 0:\n            mu = arima_recursion_normal(z, mu, Y, self.max_lag, Y.shape[0], self.ar, self.ma)\n\n        return mu, Y"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _mb_non_normal_model(self, beta, mini_batch):\n\n        rand_int =  np.random.randint(low=0, high=self.data_length-mini_batch-self.max_lag+1)\n        sample = np.arange(start=rand_int, stop=rand_int+mini_batch)\n\n        Y = self.data[self.max_lag:][sample]\n        X = self.X[:, sample]\n\n        # Transform latent variables\n        z = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n\n        # Constant and AR terms\n        if self.ar != 0:\n            mu = np.matmul(np.transpose(X),z[:-self.family_z_no-self.ma])\n        else:\n            mu = np.ones(Y.shape[0])*z[0]\n            \n        # MA terms\n        if self.ma != 0:\n            mu = arima_recursion(z, mu, self.link(mu), Y, self.max_lag, Y.shape[0], self.ar, self.ma)\n\n        return mu, Y", "response": "Creates the structure of the model for the mini batch model."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _mean_prediction(self, mu, Y, h, t_z):\n\n        # Create arrays to iteratre over\n        Y_exp = Y.copy()\n        mu_exp = mu.copy()\n\n        # Loop over h time periods          \n        for t in range(0,h):\n            new_value = t_z[0]\n\n            if self.ar != 0:\n                for j in range(1, self.ar+1):\n                    new_value += t_z[j]*Y_exp[-j]\n\n            if self.ma != 0:\n                for k in range(1, self.ma+1):\n                    if (k-1) >= t:\n                        new_value += t_z[k+self.ar]*(Y_exp[-k]-self.link(mu_exp[-k]))\n\n            if self.model_name2 == \"Exponential\":\n                Y_exp = np.append(Y_exp, [1.0/self.link(new_value)])\n            else:\n                Y_exp = np.append(Y_exp, [self.link(new_value)])\n\n            mu_exp = np.append(mu_exp,[0]) # For indexing consistency\n        del mu_exp\n        return Y_exp", "response": "This function creates a h - step ahead mean prediction for the current ARIMA object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsimulates a h - step ahead mean prediction and shock the family with random draws from the family.", "response": "def _sim_prediction(self, mu, Y, h, t_z, simulations):\n        \"\"\" Simulates a h-step ahead mean prediction\n\n        Same as _mean_prediction() but now we repeat the process \n        by a number of times (simulations) and shock the process\n        with random draws from the family, e.g. Normal shocks.\n\n        Parameters\n        ----------\n        mu : np.ndarray\n            The past predicted values\n\n        Y : np.ndarray\n            The past data\n\n        h : int\n            How many steps ahead for the prediction\n\n        t_params : np.ndarray\n            A vector of (transformed) latent variables\n\n        simulations : int\n            How many simulations to perform\n\n        Returns\n        ----------\n        Matrix of simulations\n        \"\"\"     \n\n        model_scale, model_shape, model_skewness = self._get_scale_and_shape(t_z)\n\n        sim_vector = np.zeros([simulations,h])\n\n        for n in range(0, simulations):\n            # Create arrays to iteratre over        \n            Y_exp = Y.copy()\n            mu_exp = mu.copy()\n\n            # Loop over h time periods          \n            for t in range(0,h):\n\n                new_value = t_z[0]\n\n                if self.ar != 0:\n                    for j in range(1, self.ar+1):\n                        new_value += t_z[j]*Y_exp[-j]\n\n                if self.ma != 0:\n                    for k in range(1, self.ma+1):\n                        if (k-1) >= t:\n                            new_value += t_z[k+self.ar]*(Y_exp[-k]-mu_exp[-k])\n\n                if self.model_name2 == \"Exponential\":\n                    rnd_value = self.family.draw_variable(1.0/self.link(new_value), model_scale, model_shape, model_skewness, 1)[0]\n                else:\n                    rnd_value = self.family.draw_variable(self.link(new_value), model_scale, model_shape, model_skewness, 1)[0]\n\n                Y_exp = np.append(Y_exp, [rnd_value])\n                mu_exp = np.append(mu_exp, [0]) # For indexing consistency\n\n                sim_vector[n] = Y_exp[-h:]\n            del Y_exp\n            del mu_exp\n        return np.transpose(sim_vector)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _sim_prediction_bayes(self, h, simulations):\n\n        sim_vector = np.zeros([simulations,h])\n\n        for n in range(0, simulations):\n\n            t_z = self.draw_latent_variables(nsims=1).T[0]\n            mu, Y = self._model(t_z)  \n            t_z = np.array([self.latent_variables.z_list[k].prior.transform(t_z[k]) for k in range(t_z.shape[0])])\n\n            model_scale, model_shape, model_skewness = self._get_scale_and_shape(t_z)\n\n            # Create arrays to iteratre over        \n            Y_exp = Y.copy()\n            mu_exp = mu.copy()\n\n            # Loop over h time periods          \n            for t in range(0,h):\n\n                new_value = t_z[0]\n\n                if self.ar != 0:\n                    for j in range(1, self.ar+1):\n                        new_value += t_z[j]*Y_exp[-j]\n\n                if self.ma != 0:\n                    for k in range(1, self.ma+1):\n                        if (k-1) >= t:\n                            new_value += t_z[k+self.ar]*(Y_exp[-k]-mu_exp[-k])\n\n                if self.model_name2 == \"Exponential\":\n                    rnd_value = self.family.draw_variable(1.0/self.link(new_value), model_scale, model_shape, model_skewness, 1)[0]\n                else:\n                    rnd_value = self.family.draw_variable(self.link(new_value), model_scale, model_shape, model_skewness, 1)[0]\n\n                Y_exp = np.append(Y_exp, [rnd_value])\n                mu_exp = np.append(mu_exp, [0]) # For indexing consistency\n\n                sim_vector[n] = Y_exp[-h:]\n            del Y_exp\n            del mu_exp\n        return np.transpose(sim_vector)", "response": "Simulates a h - step ahead mean prediction with random draws from the family."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlog PDF for Inverse Wishart prior", "response": "def logpdf(self, X):\n        \"\"\"\n        Log PDF for Inverse Wishart prior\n\n        Parameters\n        ----------\n        X : float\n            Covariance matrix for which the prior is being formed over\n\n        Returns\n        ----------\n        - log(p(X))\n        \"\"\"\n        return invwishart.logpdf(X, df=self.v, scale=self.Psi)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pdf(self, X):\n        return invwishart.pdf(X, df=self.v, scale=self.Psi)", "response": "PDF for Inverse Wishart prior"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates IDs for both players and teams.", "response": "def _create_ids(self, home_teams, away_teams):\n        \"\"\"\n        Creates IDs for both players/teams\n        \"\"\"\n        categories = pd.Categorical(np.append(home_teams,away_teams))\n        home_id, away_id = categories.codes[0:int(len(categories)/2)], categories.codes[int(len(categories)/2):len(categories)+1]\n        return home_id, away_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_latent_variables(self):\n\n        self.latent_variables.add_z('Constant', fam.Normal(0,10,transform=None), fam.Normal(0,3))\n        self.latent_variables.add_z('Ability Scale', fam.Normal(0,1,transform=None), fam.Normal(0,3))", "response": "Creates latent variables for the current object"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate the structure of the model for one latent variable.", "response": "def _model_one_components(self,beta):\n        \"\"\" Creates the structure of the model\n\n        Parameters\n        ----------\n        beta : np.array\n            Contains untransformed starting values for latent variables\n\n        Returns\n        ----------\n        theta : np.array\n            Contains the predicted values for the time series\n\n        Y : np.array\n            Contains the length-adjusted time series (accounting for lags)\n\n        scores : np.array\n            Contains the scores for the time series\n        \"\"\"\n\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n        scale, shape, skewness = self._get_scale_and_shape(parm)\n        state_vectors = np.zeros(shape=(self.max_team+1))\n        theta = np.zeros(shape=(self.data.shape[0]))\n\n        for t in range(0,self.data.shape[0]):\n            theta[t] = parm[0] + state_vectors[self.home_id[t]] - state_vectors[self.away_id[t]]\n\n            state_vectors[self.home_id[t]] += parm[1]*self.family.score_function(self.data[t], self.link(theta[t]), scale, shape, skewness)\n            state_vectors[self.away_id[t]] += -parm[1]*self.family.score_function(self.data[t], self.link(theta[t]), scale, shape, skewness)\n\n        return theta, self.data, state_vectors"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _model_two_components(self,beta):\n\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n        scale, shape, skewness = self._get_scale_and_shape(parm)\n        state_vectors_1 = np.zeros(shape=(self.max_team+1))\n        state_vectors_2 = np.zeros(shape=(self.max_team_2+1))\n        theta = np.zeros(shape=(self.data.shape[0]))\n\n        for t in range(0,self.data.shape[0]):\n            theta[t] = parm[0] + state_vectors_2[self.home_2_id[t]] - state_vectors_2[self.away_2_id[t]] + state_vectors_1[self.home_id[t]] - state_vectors_1[self.away_id[t]]\n\n            state_vectors_1[self.home_id[t]] += parm[1]*self.family.score_function(self.data[t], self.link(theta[t]), scale, shape, skewness)\n            state_vectors_1[self.away_id[t]] += -parm[1]*self.family.score_function(self.data[t], self.link(theta[t]), scale, shape, skewness)\n            state_vectors_2[self.home_2_id[t]] += parm[2]*self.family.score_function(self.data[t], self.link(theta[t]), scale, shape, skewness)\n            state_vectors_2[self.away_2_id[t]] += -parm[2]*self.family.score_function(self.data[t], self.link(theta[t]), scale, shape, skewness)\n\n        return theta, self.data, state_vectors_1", "response": "Creates the structure of the model for two components of the time series."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates the structure of the model - store abilities for one component of the time series.", "response": "def _model_abilities_one_components(self,beta):\n        \"\"\" Creates the structure of the model - store abilities\n\n        Parameters\n        ----------\n        beta : np.array\n            Contains untransformed starting values for latent variables\n\n        Returns\n        ----------\n        theta : np.array\n            Contains the predicted values for the time series\n\n        Y : np.array\n            Contains the length-adjusted time series (accounting for lags)\n\n        scores : np.array\n            Contains the scores for the time series\n        \"\"\"\n\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n        scale, shape, skewness = self._get_scale_and_shape(parm)\n        state_vectors = np.zeros(shape=(self.max_team+1))\n        state_vectors_store = np.zeros(shape=(int(np.max(self.home_count)+50),int(self.max_team+1)))\n        theta = np.zeros(shape=(self.data.shape[0]))\n\n        for t in range(0,self.data.shape[0]):\n            theta[t] = parm[0] + state_vectors[self.home_id[t]] - state_vectors[self.away_id[t]]\n\n            state_vectors[self.home_id[t]] += parm[1]*self.family.score_function(self.data[t], self.link(theta[t]), scale, shape, skewness)\n            state_vectors[self.away_id[t]] += -parm[1]*self.family.score_function(self.data[t], self.link(theta[t]), scale, shape, skewness)\n            state_vectors_store[int(self.home_count[t]), self.home_id[t]] = state_vectors_store[max(0,int(self.home_count[t])-1), self.home_id[t]] + parm[1]*self.family.score_function(self.data[t], self.link(theta[t]), scale, shape, skewness)\n            state_vectors_store[int(self.away_count[t]), self.away_id[t]] = state_vectors_store[max(0,int(self.away_count[t])-1), self.away_id[t]] -parm[1]*self.family.score_function(self.data[t], self.link(theta[t]), scale, shape, skewness)\n\n        return state_vectors_store"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates the structure of the model - store abilities for two - component states.", "response": "def _model_abilities_two_components(self,beta):\n        \"\"\" Creates the structure of the model - store abilities\n\n        Parameters\n        ----------\n        beta : np.array\n            Contains untransformed starting values for latent variables\n\n        Returns\n        ----------\n        theta : np.array\n            Contains the predicted values for the time series\n\n        Y : np.array\n            Contains the length-adjusted time series (accounting for lags)\n\n        scores : np.array\n            Contains the scores for the time series\n        \"\"\"\n\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n        scale, shape, skewness = self._get_scale_and_shape(parm)\n        state_vectors = np.zeros(shape=(self.max_team+1))\n        state_vectors_2 = np.zeros(shape=(self.max_team_2+1))\n        state_vectors_store_1 = np.zeros(shape=(int(np.max(self.home_count)+50),int(self.max_team+1)))\n        state_vectors_store_2 = np.zeros(shape=(int(np.max(self.home_2_count)+50),int(self.max_team_2+1)))\n        theta = np.zeros(shape=(self.data.shape[0]))\n\n        for t in range(0,self.data.shape[0]):\n            theta[t] = parm[0] + state_vectors_2[self.home_2_id[t]] - state_vectors_2[self.away_2_id[t]] + state_vectors[self.home_id[t]] - state_vectors[self.away_id[t]]\n\n            state_vectors[self.home_id[t]] += parm[1]*self.family.score_function(self.data[t], self.link(theta[t]), scale, shape, skewness)\n            state_vectors[self.away_id[t]] += -parm[1]*self.family.score_function(self.data[t], self.link(theta[t]), scale, shape, skewness)\n            state_vectors_2[self.home_2_id[t]] += parm[2]*self.family.score_function(self.data[t], self.link(theta[t]), scale, shape, skewness)\n            state_vectors_2[self.away_2_id[t]] += -parm[2]*self.family.score_function(self.data[t], self.link(theta[t]), scale, shape, skewness)\n\n            state_vectors_store_1[int(self.home_count[t]), self.home_id[t]] = state_vectors_store_1[max(0,int(self.home_count[t])-1), self.home_id[t]] + parm[1]*self.family.score_function(self.data[t], self.link(theta[t]), scale, shape, skewness)\n            state_vectors_store_1[int(self.away_count[t]), self.away_id[t]] = state_vectors_store_1[max(0,int(self.away_count[t])-1), self.away_id[t]] -parm[1]*self.family.score_function(self.data[t], self.link(theta[t]), scale, shape, skewness)\n            state_vectors_store_2[int(self.home_2_count[t]), self.home_2_id[t]] = state_vectors_store_2[max(0,int(self.home_2_count[t])-1), self.home_2_id[t]] + parm[1]*self.family.score_function(self.data[t], self.link(theta[t]), scale, shape, skewness)\n            state_vectors_store_2[int(self.away_2_count[t]), self.away_2_id[t]] = state_vectors_store_2[max(0,int(self.away_2_count[t])-1), self.away_2_id[t]] -parm[1]*self.family.score_function(self.data[t], self.link(theta[t]), scale, shape, skewness)\n\n        return state_vectors_store_1, state_vectors_store_2"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npredict the probability of winning a specific one component of a class.", "response": "def predict_one_component(self, team_1, team_2, neutral=False):\n        \"\"\"\n        Returns team 1's probability of winning\n        \"\"\"\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            if type(team_1) == str:\n                team_1_ability = np.trim_zeros(self._model_abilities(self.latent_variables.get_z_values()).T[self.team_dict[team_1]], trim='b')[-1]\n                team_2_ability = np.trim_zeros(self._model_abilities(self.latent_variables.get_z_values()).T[self.team_dict[team_2]], trim='b')[-1]\n \n            else:\n                team_1_ability = np.trim_zeros(self._model_abilities(self.latent_variables.get_z_values()).T[team_1], trim='b')[-1]\n                team_2_ability = np.trim_zeros(self._model_abilities(self.latent_variables.get_z_values()).T[team_2], trim='b')[-1]\n\n        t_z = self.transform_z()\n\n        if neutral is False:\n            return self.link(t_z[0] + team_1_ability - team_2_ability)\n        else:\n            return self.link(team_1_ability - team_2_ability)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npredicting the probability of winning two components of a class.", "response": "def predict_two_components(self, team_1, team_2, team_1b, team_2b, neutral=False):\n        \"\"\"\n        Returns team 1's probability of winning\n        \"\"\"\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            if type(team_1) == str:\n                team_1_ability = np.trim_zeros(self._model_abilities(self.latent_variables.get_z_values())[0].T[self.team_dict[team_1]], trim='b')[-1]\n                team_2_ability = np.trim_zeros(self._model_abilities(self.latent_variables.get_z_values())[0].T[self.team_dict[team_2]], trim='b')[-1]\n                team_1_b_ability = np.trim_zeros(self._model_abilities(self.latent_variables.get_z_values())[1].T[self.team_dict[team_1]], trim='b')[-1]\n                team_2_b_ability = np.trim_zeros(self._model_abilities(self.latent_variables.get_z_values())[1].T[self.team_dict[team_2]], trim='b')[-1]\n  \n            else:\n                team_1_ability = np.trim_zeros(self._model_abilities(self.latent_variables.get_z_values())[0].T[team_1], trim='b')[-1]\n                team_2_ability = np.trim_zeros(self._model_abilities(self.latent_variables.get_z_values())[0].T[team_2], trim='b')[-1]\n                team_1_b_ability = np.trim_zeros(self._model_abilities(self.latent_variables.get_z_values())[1].T[team_1_b], trim='b')[-1]\n                team_2_b_ability = np.trim_zeros(self._model_abilities(self.latent_variables.get_z_values())[1].T[team_2_b], trim='b')[-1]\n\n        t_z = self.transform_z()\n\n        if neutral is False:\n            return self.link(t_z[0] + team_1_ability - team_2_ability + team_1_b_ability - team_2_b_ability)\n        else:\n            return self.link(team_1_ability - team_2_ability + team_1_b_ability - team_2_b_ability)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning negative loglikelihood of the model", "response": "def neg_loglik(self, beta):\n        \"\"\" Creates negative loglikelihood of the model\n\n        Parameters\n        ----------\n        beta : np.array\n            Contains untransformed starting values for latent variables\n\n        Returns\n        ----------\n        - Negative loglikelihood\n        \"\"\"     \n        Z = np.zeros(2)\n        Z[0] = 1          \n        states = np.zeros([self.state_no, self.data.shape[0]])\n        states[0,:] = beta[self.z_no:self.z_no+self.data.shape[0]] \n        states[1,:] = beta[self.z_no+self.data.shape[0]:] \n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(self.z_no)]) # transformed distribution parameters\n        scale, shape, skewness = self._get_scale_and_shape(parm)\n        return self.state_likelihood(beta, states) + self.family.neg_loglikelihood(self.data, self.link(np.dot(Z, states)), scale, shape, skewness)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef state_likelihood(self, beta, alpha):\n        _, _, _, Q = self._ss_matrices(beta)\n        residuals_1 = alpha[0][1:alpha[0].shape[0]]-alpha[0][0:alpha[0].shape[0]-1]\n        residuals_2 = alpha[1][1:alpha[1].shape[0]]-alpha[1][0:alpha[1].shape[0]-1]\n        return np.sum(ss.norm.logpdf(residuals_1,loc=0,scale=np.power(Q[0][0],0.5))) + np.sum(ss.norm.logpdf(residuals_2,loc=0,scale=np.power(Q[1][1],0.5)))", "response": "Returns likelihood of the states given the variance latent variables beta and alpha."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef state_likelihood_markov_blanket(self, beta, alpha, col_no):\n        _, _, _, Q = self._ss_matrices(beta)\n        blanket = np.append(0,ss.norm.logpdf(alpha[col_no][1:]-alpha[col_no][:-1],loc=0,scale=np.sqrt(Q[col_no][col_no])))\n        blanket[:-1] = blanket[:-1] + blanket[1:]\n        return blanket", "response": "Returns the Markov blanket of the states given the variance latent variables beta and alpha."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef neg_logposterior(self, beta):\n        post = self.neg_loglik(beta)\n        for k in range(0,self.z_no):\n            post += -self.latent_variables.z_list[k].prior.logpdf(beta[k])\n        return post", "response": "Returns negative log posterior of posterior at state alpha."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate the Markov blanket for the states in the log likelihood.", "response": "def markov_blanket(self, beta, alpha):\n        \"\"\" Creates total Markov blanket for states\n\n        Parameters\n        ----------\n        beta : np.array\n            Contains untransformed starting values for latent variables\n\n        alpha : np.array\n            A vector of states\n\n        Returns\n        ----------\n        Markov blanket for states\n        \"\"\"             \n        likelihood_blanket = self.likelihood_markov_blanket(beta)\n        state_blanket = self.state_likelihood_markov_blanket(beta,alpha,0)\n        for i in range(self.state_no-1):\n            likelihood_blanket = np.append(likelihood_blanket,self.likelihood_markov_blanket(beta))\n            state_blanket = np.append(state_blanket,self.state_likelihood_markov_blanket(beta,alpha,i+1))\n        return likelihood_blanket + state_blanket"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates the structure of the model for the given time series and untransformed starting values for latent variables.", "response": "def _model(self, data, beta):\n        \"\"\" Creates the structure of the model\n\n        Parameters\n        ----------\n        data : np.array\n            Contains the time series\n\n        beta : np.array\n            Contains untransformed starting values for latent variables\n\n        Returns\n        ----------\n        a,P,K,F,v : np.array\n            Filted states, filtered variances, Kalman gains, F matrix, residuals\n        \"\"\"     \n\n        T, Z, R, Q, H = self._ss_matrices(beta)\n\n        return univariate_kalman(data,Z,H,T,Q,R,0.0)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _preoptimize_model(self):\n        gaussian_model = LLT(self.data, integ=self.integ, target=self.target)\n        gaussian_model.fit()\n        self.latent_variables.z_list[0].start = gaussian_model.latent_variables.get_z_values()[1]\n        self.latent_variables.z_list[1].start = gaussian_model.latent_variables.get_z_values()[2]\n\n        if self.model_name2 == 't':\n\n            def temp_function(params):\n                return -np.sum(ss.t.logpdf(x=self.data, df=np.exp(params[0]), \n                    loc=np.ones(self.data.shape[0])*params[1], scale=np.exp(params[2])))\n\n            p = optimize.minimize(temp_function,np.array([2.0, 0.0, -1.0]),method='L-BFGS-B')\n            self.latent_variables.z_list[2].start = p.x[2]\n            self.latent_variables.z_list[3].start = p.x[0]\n\n        elif self.model_name2 == 'Skewt':\n\n            def temp_function(params):\n                return -np.sum(fam.Skewt.logpdf_internal(x=self.data,df=np.exp(params[0]),\n                    loc=np.ones(self.data.shape[0])*params[1], scale=np.exp(params[2]),gamma=np.exp(params[3])))\n\n            p = optimize.minimize(temp_function,np.array([2.0, 0.0, -1.0, 0.0]),method='L-BFGS-B')\n            self.latent_variables.z_list[2].start = p.x[3]\n            self.latent_variables.z_list[3].start = p.x[2]\n            self.latent_variables.z_list[4].start = p.x[0]\n\n        return gaussian_model.latent_variables", "response": "Pre - optimizes the model by estimating a Gaussian state space models\n        \n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _general_approximating_model(self, beta, T, Z, R, Q, h_approx):\n\n        H = np.ones(self.data_length)*h_approx\n        mu = np.zeros(self.data_length)\n\n        return H, mu", "response": "Creates a simplest kind of approximating Gaussian model for the current set of latent variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfits the model with the specified optimizer and returns the BBVI object.", "response": "def fit(self, optimizer='RMSProp', iterations=1000, print_progress=True, start_diffuse=False, **kwargs):\n        \"\"\" Fits the model\n\n        Parameters\n        ----------\n        optimizer : string\n            Stochastic optimizer: either RMSProp or ADAM.\n\n        iterations: int\n            How many iterations to run\n\n        print_progress : bool\n            Whether tp print the ELBO progress or not\n        \n        start_diffuse : bool\n            Whether to start from diffuse values (if not: use approx Gaussian)\n        \n        Returns\n        ----------\n        BBVI fit object\n        \"\"\"     \n\n        return self._bbvi_fit(optimizer=optimizer, print_progress=print_progress,\n            start_diffuse=start_diffuse, iterations=iterations, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot_predict(self,h=5,past_values=20,intervals=True,**kwargs):\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            # Retrieve data, dates and (transformed) latent variables\n            scale, shape, skewness = scale, shape, skewness = self._get_scale_and_shape(self.latent_variables.get_z_values(transformed=True))\n\n            # Get expected values\n            forecasted_values = np.zeros(h)\n\n            for value in range(0,h):\n                if value == 0:\n                    forecasted_values[value] = self.states[0][-1] + self.states[1][-1]\n                else:\n                    forecasted_values[value] = forecasted_values[value-1] + self.states[1][-1]\n\n            previous_value = self.data[-1]  \n            date_index = self.shift_dates(h)\n            simulations = 10000\n            sim_vector = np.zeros([simulations,h])\n\n            for n in range(0,simulations):  \n                rnd_q = np.random.normal(0,np.sqrt(self.latent_variables.get_z_values(transformed=True)[0]),h)\n                rnd_q2 = np.random.normal(0,np.sqrt(self.latent_variables.get_z_values(transformed=True)[1]),h)\n                exp_0 = np.zeros(h)\n                exp_1 = np.zeros(h)\n\n                for value in range(0,h):\n                    if value == 0:\n                        exp_0[value] = self.states[1][-1] + self.states[0][-1] + rnd_q[value]\n                        exp_1[value] = self.states[1][-1] + rnd_q2[value]\n                    else:\n                        exp_0[value] = exp_0[value-1] + exp_1[value-1] + rnd_q[value]\n                        exp_1[value] = exp_1[value-1] + rnd_q2[value]\n\n                sim_vector[n] = self.family.draw_variable(loc=self.link(exp_0),shape=shape,scale=scale,skewness=skewness,nsims=exp_0.shape[0])\n\n            sim_vector = np.transpose(sim_vector)\n            forecasted_values = self.link(forecasted_values)\n\n            plt.figure(figsize=figsize) \n\n            if intervals == True:\n                plt.fill_between(date_index[-h-1:], np.insert([np.percentile(i,5) for i in sim_vector],0,previous_value), \n                    np.insert([np.percentile(i,95) for i in sim_vector],0,previous_value), alpha=0.2,label=\"95 C.I.\")   \n\n            plot_values = np.append(self.data[-past_values:],forecasted_values)\n            plot_index = date_index[-h-past_values:]\n\n            plt.plot(plot_index,plot_values,label=self.data_name)\n            plt.title(\"Forecast for \" + self.data_name)\n            plt.xlabel(\"Time\")\n            plt.ylabel(self.data_name)\n            plt.show()", "response": "Plots the predict method for the current model and returns the plot of the forecasted model."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_fit(self,intervals=True,**kwargs):\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            date_index = copy.deepcopy(self.index)\n            date_index = date_index[self.integ:self.data_original.shape[0]+1]\n\n            states_0_upper_95 = self.states[0] + 1.98*np.sqrt(self.states_var[0])\n            states_0_lower_95 = self.states[0] - 1.98*np.sqrt(self.states_var[0])\n            states_1_upper_95 = self.states[1] + 1.98*np.sqrt(self.states_var[1])\n            states_1_lower_95 = self.states[1] - 1.98*np.sqrt(self.states_var[1])\n\n            plt.figure(figsize=figsize) \n            \n            plt.subplot(2, 2, 1)\n            plt.title(self.data_name + \" Raw and Smoothed\") \n\n            if intervals == True:\n                alpha =[0.15*i/float(100) for i in range(50,12,-2)]\n                plt.fill_between(date_index, self.link(states_0_lower_95), self.link(states_0_upper_95), alpha=0.15,label='95% C.I.')   \n\n            plt.plot(date_index,self.data,label='Data')\n            plt.plot(date_index,self.link(self.states[0]),label=\"Smoothed\",c='black')\n            plt.legend(loc=2)\n            \n            plt.subplot(2, 2, 2)\n            plt.title(self.data_name + \" Local Level\")  \n\n            if intervals == True:\n                alpha =[0.15*i/float(100) for i in range(50,12,-2)]\n                plt.fill_between(date_index, self.link(states_0_lower_95), self.link(states_0_upper_95), alpha=0.15,label='95% C.I.')   \n\n            plt.plot(date_index,self.link(self.states[0]),label='Smoothed State')\n            plt.legend(loc=2)\n            \n            plt.subplot(2, 2, 3)\n            plt.title(self.data_name + \" Trend\")    \n\n            if intervals == True:\n                alpha =[0.15*i/float(100) for i in range(50,12,-2)]\n                plt.fill_between(date_index, states_1_lower_95, states_1_upper_95, alpha=0.15,label='95% C.I.') \n\n            plt.plot(date_index,self.states[1],label='Smoothed State')\n            plt.legend(loc=2)\n            \n            plt.show()", "response": "Plots the fit of the model and returns the data and the fit"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake a forecast with the estimated model and returns a pd. DataFrame with predictions", "response": "def predict(self, h=5):      \n        \"\"\" Makes forecast with the estimated model\n\n        Parameters\n        ----------\n        h : int (default : 5)\n            How many steps ahead would you like to forecast?\n\n        Returns\n        ----------\n        - pd.DataFrame with predictions\n        \"\"\"     \n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            # Retrieve data, dates and (transformed) latent variables         \n            date_index = self.shift_dates(h)\n\n            # Get expected values\n            forecasted_values = np.zeros(h)\n\n            for value in range(0,h):\n                if value == 0:\n                    forecasted_values[value] = self.states[0][-1] + self.states[1][-1]\n                else:\n                    forecasted_values[value] = forecasted_values[value-1] + self.states[1][-1]\n\n            result = pd.DataFrame(self.link(forecasted_values))\n            result.rename(columns={0:self.data_name}, inplace=True)\n            result.index = date_index[-h:]\n\n            return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a design matrix for Python 2. 7.", "response": "def create_design_matrix_2(Z, data, Y_len, lag_no):\n    \"\"\"\n    For Python 2.7 - cythonized version only works for 3.5\n    \"\"\"\n    row_count = 1\n\n    for lag in range(1, lag_no+1):\n        for reg in range(Y_len):\n            Z[row_count, :] = data[reg][(lag_no-lag):-lag]\n            row_count += 1\n\n    return Z"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_B(self,Y):\n\n        Z = self._create_Z(Y)\n        return np.dot(np.dot(Y,np.transpose(Z)),np.linalg.inv(np.dot(Z,np.transpose(Z))))", "response": "Creates the coefficient matrix B for the set of dependent variables Y."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_B_direct(self):\n\n        Y = np.array([reg[self.lags:reg.shape[0]] for reg in self.data])        \n        Z = self._create_Z(Y)\n        return np.dot(np.dot(Y,np.transpose(Z)),np.linalg.inv(np.dot(Z,np.transpose(Z))))", "response": "Creates the coefficient matrix for the OLS model."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates latent variables for the current model and returns the related model attributes.", "response": "def _create_latent_variables(self):\n        \"\"\" Creates model latent variables\n\n        Returns\n        ----------\n        None (changes model attributes)\n        \"\"\"\n\n        # TODO: There must be a cleaner way to do this below\n\n        # Create VAR latent variables\n        for variable in range(self.ylen):\n            self.latent_variables.add_z(self.data_name[variable] + ' Constant', fam.Normal(0,3,transform=None), fam.Normal(0,3))\n            other_variables = np.delete(range(self.ylen), [variable])\n            for lag_no in range(self.lags):\n                self.latent_variables.add_z(str(self.data_name[variable]) + ' AR(' + str(lag_no+1) + ')', fam.Normal(0,0.5,transform=None), fam.Normal(0,3))\n                for other in other_variables:\n                    self.latent_variables.add_z(str(self.data_name[other]) + ' to ' + str(self.data_name[variable]) + ' AR(' + str(lag_no+1) + ')', fam.Normal(0,0.5,transform=None), fam.Normal(0,3))\n\n        starting_params_temp = self._create_B_direct().flatten()\n\n        # Variance latent variables\n        for i in range(self.ylen):\n            for k in range(self.ylen):\n                if i == k:\n                    self.latent_variables.add_z('Cholesky Diagonal ' + str(i), fam.Flat(transform='exp'), fam.Normal(0,3))\n                elif i > k:\n                    self.latent_variables.add_z('Cholesky Off-Diagonal (' + str(i) + ',' + str(k) + ')', fam.Flat(transform=None), fam.Normal(0,3))\n\n        for i in range(0,self.ylen):\n            for k in range(0,self.ylen):\n                if i == k:\n                    starting_params_temp = np.append(starting_params_temp,np.array([0.5]))\n                elif i > k:\n                    starting_params_temp = np.append(starting_params_temp,np.array([0.0]))\n\n        self.latent_variables.set_z_starting_values(starting_params_temp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates the design matrix holding the lagged variables", "response": "def _create_Z(self,Y):\n        \"\"\" Creates design matrix holding the lagged variables\n\n        Parameters\n        ----------\n        Y : np.array\n            The dependent variables Y\n\n        Returns\n        ----------\n        The design matrix Z\n        \"\"\"\n\n        Z = np.ones(((self.ylen*self.lags +1),Y[0].shape[0]))\n        return self.create_design_matrix(Z, self.data, Y.shape[0], self.lags)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfunctions allows for mean prediction of the data for the forecasted data", "response": "def _forecast_mean(self,h,t_params,Y,shock_type=None,shock_index=0,shock_value=None,shock_dir='positive',irf_intervals=False):\n        \"\"\" Function allows for mean prediction; also allows shock specification for simulations or impulse response effects\n\n        Parameters\n        ----------\n        h : int\n            How many steps ahead to forecast\n\n        t_params : np.array\n            Transformed latent variables vector\n\n        Y : np.array\n            Data for series that is being forecast\n\n        shock_type : None or str\n            Type of shock; options include None, 'Cov' (simulate from covariance matrix), 'IRF' (impulse response shock)\n\n        shock_index : int\n            Which latent variable to apply the shock to if using an IRF.\n\n        shock_value : None or float\n            If specified, applies a custom-sized impulse response shock.\n\n        shock_dir : str\n            Direction of the IRF shock. One of 'positive' or 'negative'.\n\n        irf_intervals : Boolean\n            Whether to have intervals for the IRF plot or not\n\n        Returns\n        ----------\n        A vector of forecasted data\n        \"\"\"         \n\n        random = self._shock_create(h, shock_type, shock_index, shock_value, shock_dir,irf_intervals)\n        exp = [Y[variable] for variable in range(0,self.ylen)]\n        \n        # Each forward projection\n        for t in range(0,h):\n            new_values = np.zeros(self.ylen)\n\n            # Each variable\n            for variable in range(0,self.ylen):\n                index_ref = variable*(1+self.ylen*self.lags)\n                new_values[variable] = t_params[index_ref] # constant\n\n                # VAR(p) terms\n                for lag in range(0,self.lags):\n                    for lagged_var in range(0,self.ylen):\n                        new_values[variable] += t_params[index_ref+lagged_var+(lag*self.ylen)+1]*exp[lagged_var][-1-lag]\n                \n                # Random shock\n                new_values[variable] += random[t][variable]\n\n            # Add new values\n            for variable in range(0,self.ylen):\n                exp[variable] = np.append(exp[variable],new_values[variable])\n\n        return np.array(exp)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate the structure of the model for the time series", "response": "def _model(self,beta):\n        \"\"\" Creates the structure of the model\n\n        Parameters\n        ----------\n        beta : np.array\n            Contains untransformed starting values for latent variables\n\n        Returns\n        ----------\n        mu : np.array\n            Contains the predicted values for the time series\n\n        Y : np.array\n            Contains the length-adjusted time series (accounting for lags)\n        \"\"\"     \n\n        Y = np.array([reg[self.lags:reg.shape[0]] for reg in self.data])\n\n        # Transform latent variables\n        beta = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n\n        params = []\n        col_length = 1 + self.ylen*self.lags\n        for i in range(0,self.ylen):\n            params.append(beta[(col_length*i): (col_length*(i+1))])\n\n        mu = np.dot(np.array(params),self._create_Z(Y))\n        return mu, Y"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfunction creates the shocks based on the desired specification", "response": "def _shock_create(self, h, shock_type, shock_index, shock_value, shock_dir, irf_intervals):\n        \"\"\" Function creates shocks based on desired specification\n\n        Parameters\n        ----------\n        h : int\n            How many steps ahead to forecast\n\n        shock_type : None or str\n            Type of shock; options include None, 'Cov' (simulate from covariance matrix), 'IRF' (impulse response shock)\n\n        shock_index : int\n            Which latent variables to apply the shock to if using an IRF.\n\n        shock_value : None or float\n            If specified, applies a custom-sized impulse response shock.\n\n        shock_dir : str\n            Direction of the IRF shock. One of 'positive' or 'negative'.\n\n        irf_intervals : Boolean\n            Whether to have intervals for the IRF plot or not\n\n        Returns\n        ----------\n        A h-length list which contains np.arrays containing shocks for each variable\n        \"\"\"     \n\n        if shock_type is None:\n\n            random = [np.zeros(self.ylen) for i in range(0,h)]\n\n        elif shock_type == 'IRF':\n\n            if self.use_ols_covariance is False:\n                cov = self.custom_covariance(self.latent_variables.get_z_values())\n            else:\n                cov = self.ols_covariance()\n\n            post = ss.multivariate_normal(np.zeros(self.ylen),cov)\n            \n            if irf_intervals is False:\n                random = [np.zeros(self.ylen) for i in range(0,h)]\n            else:\n                random = [post.rvs() for i in range(0,h)]\n                random[0] = np.zeros(self.ylen)\n\n            if shock_value is None:\n                if shock_dir=='positive':\n                    random[0][shock_index] = cov[shock_index,shock_index]**0.5\n                elif shock_dir=='negative':\n                    random[0][shock_index] = -cov[shock_index,shock_index]**0.5\n                else:\n                    raise ValueError(\"Unknown shock direction!\")    \n            else:\n                random[0][shock_index] = shock_value        \n\n        elif shock_type == 'Cov':\n            \n            if self.use_ols_covariance is False:\n                cov = self.custom_covariance(self.latent_variables.get_z_values())\n            else:\n                cov = self.ols_covariance()\n\n            post = ss.multivariate_normal(np.zeros(self.ylen),cov)\n            random = [post.rvs() for i in range(0,h)]\n\n        return random"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a custom covariance matrix for a given Beta Vector", "response": "def custom_covariance(self,beta):\n        \"\"\" Creates Covariance Matrix for a given Beta Vector\n        (Not necessarily the OLS covariance)\n\n        Parameters\n        ----------\n        beta : np.array\n            Contains untransformed starting values for latent variables\n\n        Returns\n        ----------\n        A Covariance Matrix\n        \"\"\"         \n\n        cov_matrix = np.zeros((self.ylen,self.ylen))\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n        return custom_covariance_matrix(cov_matrix, self.ylen, self.lags, parm)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef estimator_cov(self,method):\n        \n        Y = np.array([reg[self.lags:] for reg in self.data])    \n        Z = self._create_Z(Y)\n        if method == 'OLS':\n            sigma = self.ols_covariance()\n        else:           \n            sigma = self.custom_covariance(self.latent_variables.get_z_values())\n        return np.kron(np.linalg.inv(np.dot(Z,np.transpose(Z))), sigma)", "response": "Creates a covariance matrix for the estimators"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef neg_loglik(self,beta):\n\n        mu, Y = self._model(beta)\n\n        if self.use_ols_covariance is False:\n            cm = self.custom_covariance(beta)\n        else:\n            cm = self.ols_covariance()\n\n        diff = Y.T - mu.T\n        ll1 =  -(mu.T.shape[0]*mu.T.shape[1]/2.0)*np.log(2.0*np.pi) - (mu.T.shape[0]/2.0)*np.linalg.slogdet(cm)[1]\n        inverse = np.linalg.pinv(cm)\n\n        return var_likelihood(ll1, mu.T.shape[0], diff, inverse)", "response": "Creates the negative log - likelihood of the model and returns it"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ols_covariance(self):\n\n        Y = np.array([reg[self.lags:reg.shape[0]] for reg in self.data])        \n        return (1.0/(Y[0].shape[0]))*np.dot(self.residuals(Y),np.transpose(self.residuals(Y)))", "response": "Creates OLS estimate of the covariance matrix"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot the fit of the model", "response": "def plot_fit(self,**kwargs):\n        \"\"\" Plots the fit of the model\n\n        Returns\n        ----------\n        None (plots data and the fit)\n        \"\"\"\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            date_index = self.index[self.lags:self.data[0].shape[0]]\n            mu, Y = self._model(self.latent_variables.get_z_values())\n            for series in range(0,Y.shape[0]):\n                plt.figure(figsize=figsize)\n                plt.plot(date_index,Y[series],label='Data ' + str(series))\n                plt.plot(date_index,mu[series],label='Filter' + str(series),c='black')  \n                plt.title(self.data_name[series])\n                plt.legend(loc=2)   \n            plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot the predictive model of the current object.", "response": "def plot_predict(self,h=5,past_values=20,intervals=True,**kwargs):\n\n        \"\"\" Makes forecast with the estimated model\n\n        Parameters\n        ----------\n        h : int (default : 5)\n            How many steps ahead would you like to forecast?\n\n        past_values : int (default : 20)\n            How many past observations to show on the forecast graph?\n\n        intervals : Boolean\n            Would you like to show prediction intervals for the forecast?\n\n        Returns\n        ----------\n        - Plot of the forecast\n        \"\"\"\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent varaibles estimated!\")\n        else:\n\n            # Retrieve data, dates and (transformed) latent variables\n            mu, Y = self._model(self.latent_variables.get_z_values()) \n            date_index = self.shift_dates(h)\n            t_params = self.transform_z()\n\n            # Expectation\n            exps = self._forecast_mean(h,t_params,Y,None,None)\n\n            # Simulation\n            sim_vector = np.array([np.zeros([15000,h]) for i in range(self.ylen)])\n            for it in range(0,15000):\n                exps_sim = self._forecast_mean(h,t_params,Y,\"Cov\",None)\n                for variable in range(self.ylen):\n                    sim_vector[variable][it,:] = exps_sim[variable][-h:]\n\n            for variable in range(0,exps.shape[0]):\n                test = np.transpose(sim_vector[variable])\n                error_bars, forecasted_values, plot_values, plot_index = self._summarize_simulations(exps[variable],test,date_index,h,past_values)\n                plt.figure(figsize=figsize)\n                if intervals == True:\n                    alpha = [0.15*i/float(100) for i in range(50,12,-2)]\n                    for count, pre in enumerate(error_bars):\n                        plt.fill_between(date_index[-h-1:], forecasted_values-pre, forecasted_values+pre,alpha=alpha[count])            \n                plt.plot(plot_index,plot_values)\n                plt.title(\"Forecast for \" + self.data_name[variable])\n                plt.xlabel(\"Time\")\n                plt.ylabel(self.data_name[variable])\n                plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef predict_is(self, h=5, fit_once=False, fit_method='OLS', **kwargs):\n\n        iterations = kwargs.get('iterations', 1000)\n\n        predictions = []\n\n        for t in range(0,h):\n            new_data = self.data_original.iloc[:-h+t]\n            x = VAR(lags=self.lags, integ=self.integ, data=new_data)\n            \n            if fit_once is False:\n                if fit_method == 'BBVI':\n                    x.fit(fit_method='BBVI', iterations=iterations)\n                else:\n                    x.fit(fit_method=fit_method)\n\n            if t == 0:\n\n                if fit_once is True:\n                    if fit_method == 'BBVI':\n                        x.fit(fit_method='BBVI', iterations=iterations)\n                    else:\n                        x.fit(fit_method=fit_method)\n                    saved_lvs = x.latent_variables\n\n                predictions = x.predict(1)\n            else:\n                if fit_once is True:\n                    x.latent_variables = saved_lvs\n                    \n                predictions = pd.concat([predictions,x.predict(1)])\n        \n        #predictions.rename(columns={0:self.data_name}, inplace=True)\n        predictions.index = self.index[-h:]\n\n        return predictions", "response": "Predicts the model for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef predict(self,h=5):\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n\n            # Retrieve data, dates and (transformed) latent variables\n            mu, Y = self._model(self.latent_variables.get_z_values()) \n            date_index = self.shift_dates(h)\n            t_params = self.transform_z()\n\n            # Expectation\n            exps = self._forecast_mean(h,t_params,Y,None,None)\n\n            for variable in range(0,exps.shape[0]):\n                forecasted_values = exps[variable][-h:]\n                if variable == 0:\n                    result = pd.DataFrame(forecasted_values)\n                    result.rename(columns={0:self.data_name[variable]}, inplace=True)\n                    result.index = date_index[-h:]          \n                else:\n                    result[self.data_name[variable]] = forecasted_values    \n\n            return result", "response": "Makes a forecast with the estimated model and returns a pd. DataFrame with predicted values."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates the model residuals for the set of dependent variables Y.", "response": "def residuals(self,Y):\n        \"\"\" Creates the model residuals\n\n        Parameters\n        ----------\n        Y : np.array\n            The dependent variables Y\n\n        Returns\n        ----------\n        The model residuals\n        \"\"\"         \n\n        return (Y-np.dot(self._create_B(Y),self._create_Z(Y)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef construct_wishart(self,v,X):\n        self.adjust_prior(list(range(int((len(self.latent_variables.z_list)-self.ylen-(self.ylen**2-self.ylen)/2)),\n            int(len(self.latent_variables.z_list)))), fam.InverseWishart(v,X))", "response": "Construct a Wishart prior for the covariance matrix v and X."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _forecast_model(self,beta,h):\n\n        T, Z, R, Q, H = self._ss_matrices(beta)\n        return univariate_kalman_fcst(self.data,Z,H,T,Q,R,0.0,h)", "response": "Creates the forecasted states and variances of the current object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef neg_loglik(self,beta):\n        _, _, _, F, v = self._model(self.data,beta)\n        loglik = 0.0\n        for i in range(0,self.data.shape[0]):\n            loglik += np.linalg.slogdet(F[:,:,i])[1] + np.dot(v[i],np.dot(np.linalg.pinv(F[:,:,i]),v[i]))\n        return -(-((self.data.shape[0]/2)*np.log(2*np.pi))-0.5*loglik.T[0].sum())", "response": "Returns the negative log likelihood of the model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating the negative log likelihood of the model.", "response": "def mb_neg_loglik(self, beta, mini_batch):\n        \"\"\" Creates the negative log likelihood of the model\n\n        Parameters\n        ----------\n        beta : np.array\n            Contains untransformed starting values for latent variables\n\n        mini_batch : int\n            Size of each mini batch of data\n\n        Returns\n        ----------\n        The negative log logliklihood of the model\n        \"\"\"    \n\n        rand_int =  np.random.randint(low=0, high=self.data.shape[0]-mini_batch-self.max_lag+1)\n        sample = np.arange(start=rand_int, stop=rand_int+mini_batch)\n\n        _, _, _, F, v = self._model(self.data[sample],beta)\n        loglik = 0.0\n        for i in range(0,len(sample)):\n            loglik += np.linalg.slogdet(F[:,:,i])[1] + np.dot(v[i],np.dot(np.linalg.pinv(F[:,:,i]),v[i]))\n        return -(-((len(sample)/2)*np.log(2*np.pi))-0.5*loglik.T[0].sum())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot_predict(self, h=5, past_values=20, intervals=True, **kwargs):      \n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n        nsims = kwargs.get('nsims', 200)\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            # Retrieve data, dates and (transformed) latent variables   \n            if self.latent_variables.estimation_method in ['M-H']:\n                lower_final = 0\n                upper_final = 0\n                plot_values_final = 0\n                date_index = self.shift_dates(h)\n                plot_index = date_index[-h-past_values:]\n\n                for i in range(nsims):\n\n                    t_params = self.draw_latent_variables(nsims=1).T[0]\n                    a, P = self._forecast_model(t_params, h)\n\n                    plot_values = a[0][-h-past_values:]\n                    forecasted_values = a[0][-h:]\n\n                    lower = forecasted_values - 1.96*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(t_params[0]),0.5)\n                    upper = forecasted_values + 1.96*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(t_params[0]),0.5)\n                    lower_final += np.append(plot_values[-h-1], lower)\n                    upper_final += np.append(plot_values[-h-1], upper)\n                    plot_values_final += plot_values\n\n                plot_values_final = plot_values_final / nsims\n                lower_final = lower_final / nsims\n                upper_final = upper_final / nsims\n\n                plt.figure(figsize=figsize)\n                if intervals == True:\n                    plt.fill_between(date_index[-h-1:], lower_final, upper_final, alpha=0.2)            \n\n                plt.plot(plot_index, plot_values_final)\n                plt.title(\"Forecast for \" + self.data_name)\n                plt.xlabel(\"Time\")\n                plt.ylabel(self.data_name)\n                plt.show()\n            else:\n                a, P = self._forecast_model(self.latent_variables.get_z_values(),h)\n                date_index = self.shift_dates(h)\n                plot_values = a[0][-h-past_values:]\n                forecasted_values = a[0][-h:]\n\n                lower = forecasted_values - 1.96*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(self.latent_variables.get_z_values()[0]),0.5)\n                upper = forecasted_values + 1.96*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(self.latent_variables.get_z_values()[0]),0.5)\n                lower = np.append(plot_values[-h-1],lower)\n                upper = np.append(plot_values[-h-1],upper)\n\n                plot_index = date_index[-h-past_values:]\n\n                plt.figure(figsize=figsize)\n                if intervals == True:\n                    plt.fill_between(date_index[-h-1:], lower, upper, alpha=0.2)            \n\n                plt.plot(plot_index,plot_values)\n                plt.title(\"Forecast for \" + self.data_name)\n                plt.xlabel(\"Time\")\n                plt.ylabel(self.data_name)\n                plt.show()", "response": "Plots the predict for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_fit(self, intervals=True, **kwargs):\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n        series_type = kwargs.get('series_type','Smoothed')\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            date_index = copy.deepcopy(self.index)\n            date_index = date_index[self.integ:self.data_original.shape[0]+1]\n\n            if series_type == 'Smoothed':\n                mu, V = self.smoothed_state(self.data,self.latent_variables.get_z_values())\n            elif series_type == 'Filtered':\n                mu, V, _, _, _ = self._model(self.data,self.latent_variables.get_z_values())\n            else:\n                mu, V = self.smoothed_state(self.data,self.latent_variables.get_z_values())\n\n            mu = mu[0][:-1]\n            V = V.ravel()\n\n            plt.figure(figsize=figsize) \n            \n            plt.subplot(3, 1, 1)\n            plt.title(self.data_name + \" Raw and \" + series_type)   \n\n            if intervals == True:\n                alpha =[0.15*i/float(100) for i in range(50,12,-2)]\n                plt.fill_between(date_index[2:], mu[2:] + 1.98*np.sqrt(V[:-1][2:]), mu[2:] - 1.98*np.sqrt(V[:-1][2:]), alpha=0.15,label='95% C.I.') \n\n            plt.plot(date_index,self.data,label='Data')\n            plt.plot(date_index,mu,label=series_type,c='black')\n            plt.legend(loc=2)\n            \n            plt.subplot(3, 1, 2)\n            plt.title(self.data_name + \" Local Level\")  \n\n            if intervals == True:\n                alpha =[0.15*i/float(100) for i in range(50,12,-2)]\n                plt.fill_between(date_index[2:], mu[2:] + 1.98*np.sqrt(V[:-1][2:]), mu[2:] - 1.98*np.sqrt(V[:-1][2:]), alpha=0.15,label='95% C.I.') \n\n            plt.plot(date_index,mu,label='Local Level')\n            plt.legend(loc=2)\n            plt.subplot(3, 1, 3)\n            plt.title(\"Measurement Noise\")  \n            plt.plot(date_index,self.data-mu)\n            plt.show()", "response": "Plots the fit of the model of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake a forecast with the estimated model and returns predictions for the current set of latent variables.", "response": "def predict(self, h=5, intervals=False, **kwargs):      \n        \"\"\" Makes forecast with the estimated model\n\n        Parameters\n        ----------\n        h : int (default : 5)\n            How many steps ahead would you like to forecast?\n\n        intervals : boolean (default: False)\n            Whether to return prediction intervals\n\n        Returns\n        ----------\n        - pd.DataFrame with predictions\n        \"\"\"     \n\n        nsims = kwargs.get('nsims', 200)\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            # Retrieve data, dates and (transformed) latent variables   \n            if self.latent_variables.estimation_method in ['M-H']:\n                lower_1_final = 0\n                upper_99_final = 0\n                lower_5_final = 0\n                upper_95_final = 0\n                forecasted_values_final = 0\n                date_index = self.shift_dates(h)\n\n                for i in range(nsims):\n                    t_params = self.draw_latent_variables(nsims=1).T[0]\n                    a, P = self._forecast_model(t_params, h)\n\n                    forecasted_values = a[0][-h:]\n                    lower_5 = forecasted_values - 1.96*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(t_params[0]),0.5)\n                    upper_95 = forecasted_values + 1.96*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(t_params[0]),0.5)\n                    lower_5_final += lower_5\n                    upper_95_final += upper_95\n                    lower_1 = forecasted_values - 2.575*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(t_params[0]),0.5)\n                    upper_99 = forecasted_values + 2.575*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(t_params[0]),0.5)\n                    lower_1_final += lower_1\n                    upper_99_final += upper_99\n                    forecasted_values_final += forecasted_values\n\n                forecasted_values_final = forecasted_values_final / nsims\n                lower_1_final = lower_1_final / nsims\n                lower_5_final = lower_5_final / nsims\n                upper_95_final = upper_95_final / nsims\n                upper_99_final = upper_99_final / nsims\n\n                if intervals is False:\n                    result = pd.DataFrame(forecasted_values_final)\n                    result.rename(columns={0:self.data_name}, inplace=True)\n                else:\n                    prediction_05 = lower_5_final\n                    prediction_95 = upper_95_final\n                    prediction_01 = lower_1_final\n                    prediction_99 = upper_99_final\n\n                    result = pd.DataFrame([forecasted_values_final, prediction_01, prediction_05, \n                        prediction_95, prediction_99]).T\n                    result.rename(columns={0:self.data_name, 1: \"1% Prediction Interval\", \n                        2: \"5% Prediction Interval\", 3: \"95% Prediction Interval\", 4: \"99% Prediction Interval\"}, \n                        inplace=True)\n\n                result.index = date_index[-h:]\n\n                return result\n     \n            else:\n                # Retrieve data, dates and (transformed) latent variables         \n                a, P = self._forecast_model(self.latent_variables.get_z_values(),h)\n                date_index = self.shift_dates(h)\n                forecasted_values = a[0][-h:]\n\n                if intervals is False:\n                    result = pd.DataFrame(forecasted_values)\n                    result.rename(columns={0:self.data_name}, inplace=True)\n                else:\n                    prediction_05 = forecasted_values - 1.96*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(self.latent_variables.get_z_values()[0]),0.5)\n                    prediction_95 = forecasted_values + 1.96*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(self.latent_variables.get_z_values()[0]),0.5)\n                    prediction_01 = forecasted_values - 2.575*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(self.latent_variables.get_z_values()[0]),0.5)\n                    prediction_99 = forecasted_values + 2.575*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(self.latent_variables.get_z_values()[0]),0.5)\n\n                    result = pd.DataFrame([forecasted_values, prediction_01, prediction_05, \n                        prediction_95, prediction_99]).T\n                    result.rename(columns={0:self.data_name, 1: \"1% Prediction Interval\", \n                        2: \"5% Prediction Interval\", 3: \"95% Prediction Interval\", 4: \"99% Prediction Interval\"}, \n                        inplace=True)\n     \n                result.index = date_index[-h:]\n\n                return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot the forecasts with the estimated model against data", "response": "def plot_predict_is(self, h=5, fit_once=True, fit_method='MLE', **kwargs):\n        \"\"\" Plots forecasts with the estimated model against data\n            (Simulated prediction with data)\n\n        Parameters\n        ----------\n        h : int (default : 5)\n            How many steps to forecast\n\n        fit_once : boolean\n            (default: True) Fits only once before the in-sample prediction; if False, fits after every new datapoint\n\n        fit_method : string\n            Which method to fit the model with\n\n        Returns\n        ----------\n        - Plot of the forecast against data \n        \"\"\"\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n\n        plt.figure(figsize=figsize)\n        predictions = self.predict_is(h, fit_once=fit_once, fit_method=fit_method)\n        data = self.data[-h:]\n        plt.plot(predictions.index,data,label='Data')\n        plt.plot(predictions.index,predictions,label='Predictions',c='black')\n        plt.title(self.data_name)\n        plt.legend(loc=2)   \n        plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef simulation_smoother(self,beta):\n\n        T, Z, R, Q, H = self._ss_matrices(beta)\n\n        # Generate e_t+ and n_t+\n        rnd_h = np.random.normal(0,np.sqrt(H),self.data.shape[0]+1)\n        q_dist = ss.multivariate_normal([0.0], Q)\n        rnd_q = q_dist.rvs(self.data.shape[0]+1)\n\n        # Generate a_t+ and y_t+\n        a_plus = np.zeros((T.shape[0],self.data.shape[0]+1)) \n        a_plus[0,0] = np.mean(self.data[0:5])\n        y_plus = np.zeros(self.data.shape[0])\n\n        for t in range(0,self.data.shape[0]+1):\n            if t == 0:\n                a_plus[:,t] = np.dot(T,a_plus[:,t]) + rnd_q[t]\n                y_plus[t] = np.dot(Z,a_plus[:,t]) + rnd_h[t]\n            else:\n                if t != self.data.shape[0]:\n                    a_plus[:,t] = np.dot(T,a_plus[:,t-1]) + rnd_q[t]\n                    y_plus[t] = np.dot(Z,a_plus[:,t]) + rnd_h[t]\n\n        alpha_hat,_ = self.smoothed_state(self.data,beta)\n        alpha_hat_plus,_ = self.smoothed_state(y_plus,beta)\n        alpha_tilde = alpha_hat - alpha_hat_plus + a_plus\n    \n        return alpha_tilde", "response": "Simulates from states given by latent variables and observations."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates the negative log marginal likelihood of the model with the given data and beta.", "response": "def smoothed_state(self,data,beta):\n        \"\"\" Creates the negative log marginal likelihood of the model\n\n        Parameters\n        ----------\n\n        data : np.array\n            Data to be smoothed\n\n        beta : np.array\n            Contains untransformed starting values for latent variables\n\n        Returns\n        ----------\n        - Smoothed states\n        \"\"\"         \n\n        T, Z, R, Q, H = self._ss_matrices(beta)\n        alpha, V = univariate_KFS(data,Z,H,T,Q,R,0.0)\n        return alpha, V"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sample(self, nsims=1000):\n        if self.latent_variables.estimation_method not in ['BBVI', 'M-H']:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            lv_draws = self.draw_latent_variables(nsims=nsims)\n            mus = [self.smoothed_state(self.data, lv_draws[:,i])[0][0][:-1] for i in range(nsims)]\n            data_draws = np.array([np.random.normal(mus[i], np.sqrt(self.latent_variables.z_list[0].prior.transform(lv_draws[0,i])), mus[i].shape[0]) for i in range(nsims)])\n            return data_draws", "response": "Samples from the posterior predictive distribution\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ppc(self, nsims=1000, T=np.mean):\n        if self.latent_variables.estimation_method not in ['BBVI', 'M-H']:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            lv_draws = self.draw_latent_variables(nsims=nsims)\n            mus = [self.smoothed_state(self.data, lv_draws[:,i])[0][0][:-1] for i in range(nsims)]\n            data_draws = np.array([np.random.normal(mus[i], np.sqrt(self.latent_variables.z_list[0].prior.transform(lv_draws[0,i])), mus[i].shape[0]) for i in range(nsims)])\n            T_sims = T(self.sample(nsims=nsims), axis=1)\n            T_actual = T(self.data)\n            return len(T_sims[T_sims>T_actual])/nsims", "response": "Computes the posterior predictive p - value for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _bbvi_fit(self, posterior, optimizer='RMSProp', iterations=1000, \n        map_start=True, batch_size=12, mini_batch=None, learning_rate=0.001, \n        record_elbo=False, quiet_progress=False, **kwargs):\n        \"\"\" Performs Black Box Variational Inference\n\n        Parameters\n        ----------\n        posterior : method\n            Hands bbvi_fit a posterior object\n\n        optimizer : string\n            Stochastic optimizer: one of RMSProp or ADAM.\n\n        iterations: int\n            How many iterations for BBVI\n\n        map_start : boolean\n            Whether to start values from a MAP estimate (if False, uses default starting values)\n\n        Returns\n        ----------\n        BBVIResults object\n        \"\"\"\n\n        # Starting values\n        phi = self.latent_variables.get_z_starting_values()\n        phi = kwargs.get('start',phi).copy() # If user supplied\n\n        if self.model_type not in ['GPNARX','GPR','GP','GASRank'] and map_start is True and mini_batch is None:\n            p = optimize.minimize(posterior, phi, method='L-BFGS-B') # PML starting values\n            start_loc = 0.8*p.x + 0.2*phi\n        else:\n            start_loc = phi\n        start_ses = None\n\n        # Starting values for approximate distribution\n        for i in range(len(self.latent_variables.z_list)):\n            approx_dist = self.latent_variables.z_list[i].q\n            if isinstance(approx_dist, Normal):\n                if start_ses is None:\n                    self.latent_variables.z_list[i].q.mu0 = start_loc[i]\n                    self.latent_variables.z_list[i].q.sigma0 = np.exp(-3.0)\n                else:\n                    self.latent_variables.z_list[i].q.mu0 = start_loc[i]\n                    self.latent_variables.z_list[i].q.sigma0 = start_ses[i]\n\n        q_list = [k.q for k in self.latent_variables.z_list]\n\n        if mini_batch is None:\n            bbvi_obj = BBVI(posterior, q_list, batch_size, optimizer, iterations, learning_rate, record_elbo, quiet_progress)\n        else:\n            bbvi_obj = BBVIM(posterior, self.neg_logposterior, q_list, mini_batch, optimizer, iterations, learning_rate, mini_batch, record_elbo, quiet_progress)\n        \n        q, q_z, q_ses, elbo_records = bbvi_obj.run()\n        self.latent_variables.set_z_values(q_z,'BBVI',np.exp(q_ses),None)\n\n        for k in range(len(self.latent_variables.z_list)):\n            self.latent_variables.z_list[k].q = q[k]\n\n        self.latent_variables.estimation_method = 'BBVI'\n\n        theta, Y, scores, states, states_var, X_names = self._categorize_model_output(q_z)\n\n        # Change this in future\n        try:\n            latent_variables_store = self.latent_variables.copy()\n        except:\n            latent_variables_store = self.latent_variables\n\n        return BBVIResults(data_name=self.data_name, X_names=X_names, model_name=self.model_name,\n            model_type=self.model_type, latent_variables=latent_variables_store, data=Y, index=self.index,\n            multivariate_model=self.multivariate_model, objective_object=self.neg_logposterior, \n            method='BBVI', ses=q_ses, signal=theta, scores=scores, elbo_records=elbo_records,\n            z_hide=self._z_hide, max_lag=self.max_lag, states=states, states_var=states_var)", "response": "This method is used to fit a single object in the BBVI model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming a Laplace approximation to the posterior object.", "response": "def _laplace_fit(self,obj_type):\n        \"\"\" Performs a Laplace approximation to the posterior\n\n        Parameters\n        ----------\n        obj_type : method\n            Whether a likelihood or a posterior\n\n        Returns\n        ----------\n        None (plots posterior)\n        \"\"\"\n\n        # Get Mode and Inverse Hessian information\n        y = self.fit(method='PML',printer=False)\n\n        if y.ihessian is None:\n            raise Exception(\"No Hessian information - Laplace approximation cannot be performed\")\n        else:\n\n            self.latent_variables.estimation_method = 'Laplace'\n\n            theta, Y, scores, states, states_var, X_names = self._categorize_model_output(self.latent_variables.get_z_values())\n\n            # Change this in future\n            try:\n                latent_variables_store = self.latent_variables.copy()\n            except:\n                latent_variables_store = self.latent_variables\n\n            return LaplaceResults(data_name=self.data_name,X_names=X_names,model_name=self.model_name,\n                model_type=self.model_type, latent_variables=latent_variables_store,data=Y,index=self.index,\n                multivariate_model=self.multivariate_model,objective_object=obj_type, \n                method='Laplace',ihessian=y.ihessian,signal=theta,scores=scores,\n                z_hide=self._z_hide,max_lag=self.max_lag,states=states,states_var=states_var)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _mcmc_fit(self, scale=1.0, nsims=10000, printer=True, method=\"M-H\", \n        cov_matrix=None, map_start=True, quiet_progress=False, **kwargs):\n        \"\"\" Performs random walk Metropolis-Hastings\n\n        Parameters\n        ----------\n        scale : float\n            Default starting scale\n\n        nsims : int\n            Number of simulations\n\n        printer : Boolean\n            Whether to print results or not\n\n        method : str\n            What type of MCMC\n\n        cov_matrix: None or np.array\n            Can optionally provide a covariance matrix for M-H.\n        \"\"\"\n        scale = 2.38/np.sqrt(self.z_no)\n\n        # Get Mode and Inverse Hessian information\n        if self.model_type in ['GPNARX', 'GPR', 'GP'] or map_start is True:\n            y = self.fit(method='PML', printer=False)\n            starting_values = y.z.get_z_values()\n\n            # TODO: Bad use of a try/except - remove in future\n            try:\n                ses = np.abs(np.diag(y.ihessian))\n                if len(ses[np.isnan(ses)]) != 0:\n                    ses = np.ones(ses.shape[0])\n                cov_matrix = np.zeros((len(ses), len(ses)))\n                np.fill_diagonal(cov_matrix, ses)\n            except:\n                pass\n        else:\n            starting_values = self.latent_variables.get_z_starting_values()\n\n        if method == \"M-H\":\n            sampler = MetropolisHastings(self.neg_logposterior, scale, nsims, starting_values, \n                cov_matrix=cov_matrix, model_object=None, quiet_progress=quiet_progress)\n            chain, mean_est, median_est, upper_95_est, lower_95_est = sampler.sample()\n        else:\n            raise Exception(\"Method not recognized!\")\n\n        if len(self.latent_variables.z_list) == 1:\n            self.latent_variables.set_z_values(mean_est,'M-H',None,chain)\n            mean_est = self.latent_variables.z_list[0].prior.transform(mean_est)\n            median_est = self.latent_variables.z_list[0].prior.transform(median_est)\n            upper_95_est = self.latent_variables.z_list[0].prior.transform(upper_95_est)\n            lower_95_est = self.latent_variables.z_list[0].prior.transform(lower_95_est)        \n\n        else:\n            self.latent_variables.set_z_values(mean_est, 'M-H', None, chain)\n\n            for k in range(len(chain)):\n                mean_est[k] = self.latent_variables.z_list[k].prior.transform(mean_est[k])\n                median_est[k] = self.latent_variables.z_list[k].prior.transform(median_est[k])\n                upper_95_est[k] = self.latent_variables.z_list[k].prior.transform(upper_95_est[k])\n                lower_95_est[k] = self.latent_variables.z_list[k].prior.transform(lower_95_est[k])        \n\n        self.latent_variables.estimation_method = 'M-H'\n\n        theta, Y, scores, states, states_var, X_names = self._categorize_model_output(mean_est)\n    \n        # Change this in future\n        try:\n            latent_variables_store = self.latent_variables.copy()\n        except:\n            latent_variables_store = self.latent_variables\n\n        return MCMCResults(data_name=self.data_name,X_names=X_names,model_name=self.model_name,\n            model_type=self.model_type, latent_variables=latent_variables_store,data=Y,index=self.index,\n            multivariate_model=self.multivariate_model,objective_object=self.neg_logposterior, \n            method='Metropolis Hastings',samples=chain,mean_est=mean_est,median_est=median_est,lower_95_est=lower_95_est,\n            upper_95_est=upper_95_est,signal=theta,scores=scores, z_hide=self._z_hide,max_lag=self.max_lag,\n            states=states,states_var=states_var)", "response": "Fits the model and returns the MCMC object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ols_fit(self):\n\n        # TO DO - A lot of things are VAR specific here; might need to refactor in future, or just move to VAR script\n\n        method = 'OLS'\n        self.use_ols_covariance = True\n        \n        res_z = self._create_B_direct().flatten()\n        z = res_z.copy()\n        cov = self.ols_covariance()\n\n        # Inelegant - needs refactoring\n        for i in range(self.ylen):\n            for k in range(self.ylen):\n                if i == k or i > k:\n                    z = np.append(z,self.latent_variables.z_list[-1].prior.itransform(cov[i,k]))\n\n        ihessian = self.estimator_cov('OLS')\n        res_ses = np.power(np.abs(np.diag(ihessian)),0.5)\n        ses = np.append(res_ses,np.ones([z.shape[0]-res_z.shape[0]]))\n        self.latent_variables.set_z_values(z,method,ses,None)\n\n        self.latent_variables.estimation_method = 'OLS'\n\n        theta, Y, scores, states, states_var, X_names = self._categorize_model_output(z)\n\n        # Change this in future\n        try:\n            latent_variables_store = self.latent_variables.copy()\n        except:\n            latent_variables_store = self.latent_variables\n\n        return MLEResults(data_name=self.data_name,X_names=X_names,model_name=self.model_name,\n            model_type=self.model_type, latent_variables=latent_variables_store,results=None,data=Y, index=self.index,\n            multivariate_model=self.multivariate_model,objective_object=self.neg_loglik, \n            method=method,ihessian=ihessian,signal=theta,scores=scores,\n            z_hide=self._z_hide,max_lag=self.max_lag,states=states,states_var=states_var)", "response": "Performs OLS fit on the latent variables and returns MLEResults"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfits a model to obtain a set of related objects.", "response": "def fit(self, method=None, **kwargs):\n        \"\"\" Fits a model\n\n        Parameters\n        ----------\n        method : str\n            A fitting method (e.g 'MLE'). Defaults to model specific default method.\n\n        Returns\n        ----------\n        None (stores fit information)\n        \"\"\"\n\n        cov_matrix = kwargs.get('cov_matrix', None)\n        iterations = kwargs.get('iterations', 1000)\n        nsims = kwargs.get('nsims', 10000)\n        optimizer = kwargs.get('optimizer', 'RMSProp')\n        batch_size = kwargs.get('batch_size', 12)\n        mini_batch = kwargs.get('mini_batch', None)\n        map_start = kwargs.get('map_start', True)\n        learning_rate = kwargs.get('learning_rate', 0.001)\n        record_elbo = kwargs.get('record_elbo', None)\n        quiet_progress = kwargs.get('quiet_progress', False)\n\n        if method is None:\n            method = self.default_method\n        elif method not in self.supported_methods:\n            raise ValueError(\"Method not supported!\")\n\n        if method == 'MLE':\n            return self._optimize_fit(self.neg_loglik, **kwargs)\n        elif method == 'PML':\n            return self._optimize_fit(self.neg_logposterior, **kwargs)   \n        elif method == 'M-H':\n            return self._mcmc_fit(nsims=nsims, method=method, cov_matrix=cov_matrix,\n                map_start=map_start, quiet_progress=quiet_progress)\n        elif method == \"Laplace\":\n            return self._laplace_fit(self.neg_logposterior) \n        elif method == \"BBVI\":\n            if mini_batch is None:\n                posterior = self.neg_logposterior\n            else:\n                posterior = self.mb_neg_logposterior\n            return self._bbvi_fit(posterior, optimizer=optimizer, iterations=iterations,\n                batch_size=batch_size, mini_batch=mini_batch, map_start=map_start, \n                learning_rate=learning_rate, record_elbo=record_elbo, quiet_progress=quiet_progress)\n        elif method == \"OLS\":\n            return self._ols_fit()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mb_neg_logposterior(self, beta, mini_batch):\n\n        post = (self.data.shape[0]/mini_batch)*self.mb_neg_loglik(beta, mini_batch)\n        for k in range(0,self.z_no):\n            post += -self.latent_variables.z_list[k].prior.logpdf(beta[k])\n        return post", "response": "Returns negative log posterior of the posterior for the latent variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef multivariate_neg_logposterior(self,beta):\n\n        post = self.neg_loglik(beta)\n        for k in range(0,self.z_no):\n            if self.latent_variables.z_list[k].prior.covariance_prior is True:\n                post += -self.latent_variables.z_list[k].prior.logpdf(self.custom_covariance(beta))\n                break\n            else:\n                post += -self.latent_variables.z_list[k].prior.logpdf(beta[k])\n        return post", "response": "Returns negative log posterior for a model with a covariance matrix \n"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef shift_dates(self,h):\n\n        date_index = copy.deepcopy(self.index)\n        date_index = date_index[self.max_lag:len(date_index)]\n\n        if self.is_pandas is True:\n\n            if isinstance(date_index, pd.core.indexes.datetimes.DatetimeIndex):\n\n                if pd.infer_freq(date_index) in ['H', 'M', 'S']:\n\n                    for t in range(h):\n                        date_index += pd.DateOffset((date_index[len(date_index)-1] - date_index[len(date_index)-2]).seconds)\n\n                else: # Assume higher frequency (configured for days)\n\n                    for t in range(h):\n                        date_index += pd.DateOffset((date_index[len(date_index)-1] - date_index[len(date_index)-2]).days)\n\n            elif isinstance(date_index, pd.core.indexes.numeric.Int64Index):\n\n                for i in range(h):\n                    new_value = date_index.values[len(date_index.values)-1] + (date_index.values[len(date_index.values)-1] - date_index.values[len(date_index.values)-2])\n                    date_index = pd.Int64Index(np.append(date_index.values,new_value))\n\n        else:\n\n            for t in range(h):\n                date_index.append(date_index[len(date_index)-1]+1)\n\n        return date_index", "response": "Auxiliary function for creating dates for forecasts based on the number of steps to forecast."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots the latent variables by calling latent parameters object Returns ---------- Plot Pretty", "response": "def plot_z(self, indices=None,figsize=(15,5),**kwargs):\n        \"\"\" Plots latent variables by calling latent parameters object\n\n        Returns\n        ----------\n        Pretty plot \n        \"\"\"\n        self.latent_variables.plot_z(indices=indices,figsize=figsize,**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_parameters(self, indices=None,figsize=(15,5),**kwargs):\n        self.plot_z(indices,figsize,**kwargs)", "response": "Plot the parameters of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef adjust_prior(self, index, prior):\n        self.latent_variables.adjust_prior(index=index, prior=prior)", "response": "Adjusts priors for the latent variables in the current object to match the prior distribution."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndraws latent variables from the model", "response": "def draw_latent_variables(self, nsims=5000):\n        \"\"\" Draws latent variables from the model (for Bayesian inference)\n\n        Parameters\n        ----------\n        nsims : int\n            How many draws to take\n\n        Returns\n        ----------\n        - np.ndarray of draws\n        \"\"\"\n        if self.latent_variables.estimation_method is None:\n            raise Exception(\"No latent variables estimated!\")\n        elif self.latent_variables.estimation_method == 'BBVI':\n            return np.array([i.q.draw_variable_local(size=nsims) for i in self.latent_variables.z_list])\n        elif self.latent_variables.estimation_method == \"M-H\":\n            chain = np.array([self.latent_variables.z_list[i].sample for i in range(len(self.latent_variables.z_list))])\n            return chain[:,np.random.choice(chain.shape[1], nsims)]\n        else:\n            raise Exception(\"No latent variables estimated through Bayesian inference\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating latent variables for the current object.", "response": "def _create_latent_variables(self):\n        \"\"\" Creates model latent variables\n\n        Returns\n        ----------\n        None (changes model attributes)\n        \"\"\"\n\n        self.latent_variables.add_z('Sigma^2 irregular', fam.Flat(transform='exp'), fam.Normal(0,3))\n\n        for parm in range(self.z_no-1):\n            self.latent_variables.add_z('Sigma^2 ' + self.X_names[parm], fam.Flat(transform='exp'), fam.Normal(0,3))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates the forecasted states and variances of the kalman model", "response": "def _forecast_model(self,beta,Z,h):\n        \"\"\" Creates forecasted states and variances\n\n        Parameters\n        ----------\n        beta : np.ndarray\n            Contains untransformed starting values for latent variables\n\n        Returns\n        ----------\n        a : np.ndarray\n            Forecasted states\n\n        P : np.ndarray\n            Variance of forecasted states\n        \"\"\"     \n\n        T, _, R, Q, H = self._ss_matrices(beta)\n        return dl_univariate_kalman_fcst(self.data,Z,H,T,Q,R,0.0,h)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ss_matrices(self, beta):\n\n        T = np.identity(self.z_no-1)\n        H = np.identity(1)*self.latent_variables.z_list[0].prior.transform(beta[0])       \n        Z = self.X\n        R = np.identity(self.z_no-1)\n        \n        Q = np.identity(self.z_no-1)\n        for i in range(0,self.z_no-1):\n            Q[i][i] = self.latent_variables.z_list[i+1].prior.transform(beta[i+1])\n\n        return T, Z, R, Q, H", "response": "Creates the state space matrices required for the KFS algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplot the predictive model of the current object.", "response": "def plot_predict(self, h=5, past_values=20, intervals=True, oos_data=None, **kwargs):        \n        \"\"\" Makes forecast with the estimated model\n\n        Parameters\n        ----------\n        h : int (default : 5)\n            How many steps ahead would you like to forecast?\n\n        past_values : int (default : 20)\n            How many past observations to show on the forecast graph?\n\n        intervals : Boolean\n            Would you like to show 95% prediction intervals for the forecast?\n\n        oos_data : pd.DataFrame\n            Data for the variables to be used out of sample (ys can be NaNs)\n\n        Returns\n        ----------\n        - Plot of the forecast\n        \"\"\"\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n        nsims = kwargs.get('nsims', 200)\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n\n            _, X_oos = dmatrices(self.formula, oos_data)\n            X_oos = np.array([X_oos])[0]\n            full_X = self.X.copy()\n            full_X = np.append(full_X,X_oos,axis=0)\n            Z = full_X\n            date_index = self.shift_dates(h)\n\n            # Retrieve data, dates and (transformed) latent variables   \n            if self.latent_variables.estimation_method in ['M-H']:\n                lower_final = 0\n                upper_final = 0\n                plot_values_final = 0\n                plot_index = date_index[-h-past_values:]\n\n                for i in range(nsims):\n\n                    t_params = self.draw_latent_variables(nsims=1).T[0]\n                    a, P = self._forecast_model(t_params, Z, h)\n\n                    smoothed_series = np.zeros(self.y.shape[0]+h)\n                    series_variance = np.zeros(self.y.shape[0]+h)\n\n                    for t in range(self.y.shape[0]+h):\n                        smoothed_series[t] = np.dot(Z[t],a[:,t])\n                        series_variance[t] = np.dot(np.dot(Z[t],P[:,:,t]),Z[t].T)\n\n                    plot_values = smoothed_series[-h-past_values:]\n                    lower = smoothed_series[-h:] - 1.96*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(t_params[0]),0.5)\n                    upper = smoothed_series[-h:] + 1.96*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(t_params[0]),0.5)\n                    lower_final += np.append(plot_values[-h-1], lower)\n                    upper_final += np.append(plot_values[-h-1], upper)\n                    plot_values_final += plot_values\n\n                plot_values_final = plot_values_final / nsims\n                lower_final = lower_final / nsims\n                upper_final = upper_final / nsims\n\n                plt.figure(figsize=figsize)\n                if intervals == True:\n                    plt.fill_between(date_index[-h-1:], lower_final, upper_final, alpha=0.2)            \n\n                plt.plot(plot_index, plot_values_final)\n                plt.title(\"Forecast for \" + self.data_name)\n                plt.xlabel(\"Time\")\n                plt.ylabel(self.data_name)\n                plt.show()\n            else:\n                a, P = self._forecast_model(self.latent_variables.get_z_values(), h)\n                plot_values = a[0][-h-past_values:]\n                forecasted_values = a[0][-h:]\n\n                smoothed_series = np.zeros(self.y.shape[0]+h)\n                series_variance = np.zeros(self.y.shape[0]+h)\n\n                for t in range(self.y.shape[0]+h):\n                    smoothed_series[t] = np.dot(Z[t],a[:,t])\n                    series_variance[t] = np.dot(np.dot(Z[t],P[:,:,t]),Z[t].T)\n\n                lower = forecasted_values - 1.96*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(self.latent_variables.get_z_values()[0]),0.5)\n                upper = forecasted_values + 1.96*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(self.latent_variables.get_z_values()[0]),0.5)\n                lower = np.append(plot_values[-h-1],lower)\n                upper = np.append(plot_values[-h-1],upper)\n\n                plot_index = date_index[-h-past_values:]\n\n                plt.figure(figsize=figsize)\n                if intervals == True:\n                    plt.fill_between(date_index[-h-1:], lower, upper, alpha=0.2)            \n\n                plt.plot(plot_index,plot_values)\n                plt.title(\"Forecast for \" + self.data_name)\n                plt.xlabel(\"Time\")\n                plt.ylabel(self.data_name)\n                plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nplots the fit of the model of the current object.", "response": "def plot_fit(self,intervals=False,**kwargs):\n        \"\"\" Plots the fit of the model\n\n        Parameters\n        ----------\n        intervals : Boolean\n            Whether to plot 95% confidence interval of states\n\n        Returns\n        ----------\n        None (plots data and the fit)\n        \"\"\"\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n        series_type = kwargs.get('series_type','Smoothed')\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            date_index = copy.deepcopy(self.index)\n            date_index = date_index[:self.y.shape[0]+1]\n\n            if series_type == 'Smoothed':\n                mu, V = self.smoothed_state(self.data,self.latent_variables.get_z_values())\n            elif series_type == 'Filtered':\n                mu, V, _, _, _ = self._model(self.data,self.latent_variables.get_z_values())\n            else:\n                mu, V = self.smoothed_state(self.data,self.latent_variables.get_z_values())\n\n            # Create smoothed/filtered aggregate series\n            _, Z, _, _, _ = self._ss_matrices(self.latent_variables.get_z_values())\n            smoothed_series = np.zeros(self.y.shape[0])\n\n            for t in range(0,self.y.shape[0]):\n                smoothed_series[t] = np.dot(Z[t],mu[:,t])\n\n            plt.figure(figsize=figsize) \n            \n            plt.subplot(self.z_no+1, 1, 1)\n            plt.title(self.y_name + \" Raw and \" + series_type)  \n            plt.plot(date_index,self.data,label='Data')\n            plt.plot(date_index,smoothed_series,label=series_type,c='black')\n            plt.legend(loc=2)\n\n            for coef in range(0,self.z_no-1):\n                V_coef = V[0][coef][:-1]    \n                plt.subplot(self.z_no+1, 1, 2+coef)\n                plt.title(\"Beta \" + self.X_names[coef]) \n\n                if intervals == True:\n                    alpha =[0.15*i/float(100) for i in range(50,12,-2)]\n                    plt.fill_between(date_index[5:], mu[coef,0:mu.shape[1]-1][5:] + 1.98*np.sqrt(V_coef[5:]), mu[coef,0:mu.shape[1]-1][5:] - 1.98*np.sqrt(V_coef[5:]), alpha=0.15,label='95% C.I.') \n                plt.plot(date_index,mu[coef,0:mu.shape[1]-1],label='Data')\n                plt.legend(loc=2)               \n            \n            plt.subplot(self.z_no+1, 1, self.z_no+1)\n            plt.title(\"Measurement Error\")\n            plt.plot(date_index,self.data-smoothed_series,label='Irregular')\n            plt.legend(loc=2)   \n\n            plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef predict(self, h=5, intervals=False, oos_data=None, **kwargs):        \n\n        nsims = kwargs.get('nsims', 200)\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n\n            _, X_oos = dmatrices(self.formula, oos_data)\n            X_oos = np.array([X_oos])[0]\n            full_X = self.X.copy()\n            full_X = np.append(full_X,X_oos,axis=0)\n            Z = full_X\n            date_index = self.shift_dates(h)\n\n            # Retrieve data, dates and (transformed) latent variables   \n            if self.latent_variables.estimation_method in ['M-H']:\n                lower_1_final = 0\n                upper_99_final = 0\n                lower_5_final = 0\n                upper_95_final = 0\n                forecasted_values_final = 0\n\n                for i in range(nsims):\n                    t_params = self.draw_latent_variables(nsims=1).T[0]\n                    a, P = self._forecast_model(t_params, Z, h)\n\n                    smoothed_series = np.zeros(h)\n                    series_variance = np.zeros(h)\n\n                    for t in range(h):\n                        smoothed_series[t] = np.dot(Z[self.y.shape[0]+t],a[:,self.y.shape[0]+t])\n                        series_variance[t] = np.dot(np.dot(Z[self.y.shape[0]+t],P[:,:,self.y.shape[0]+t]),Z[self.y.shape[0]+t].T)\n\n                    forecasted_values = smoothed_series\n                    lower_5 = smoothed_series - 1.96*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(t_params[0]),0.5)\n                    upper_95 = smoothed_series + 1.96*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(t_params[0]),0.5)\n                    lower_5_final += lower_5\n                    upper_95_final += upper_95\n                    lower_1 = smoothed_series - 2.575*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(t_params[0]),0.5)\n                    upper_99 = smoothed_series + 2.575*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(t_params[0]),0.5)\n                    lower_1_final += lower_1\n                    upper_99_final += upper_99\n                    forecasted_values_final += forecasted_values\n\n                forecasted_values_final = forecasted_values_final / nsims\n                lower_1_final = lower_1_final / nsims\n                lower_5_final = lower_5_final / nsims\n                upper_95_final = upper_95_final / nsims\n                upper_99_final = upper_99_final / nsims\n\n                if intervals is False:\n                    result = pd.DataFrame(forecasted_values_final)\n                    result.rename(columns={0:self.data_name}, inplace=True)\n                else:\n                    prediction_05 = lower_5_final\n                    prediction_95 = upper_95_final\n                    prediction_01 = lower_1_final\n                    prediction_99 = upper_99_final\n\n                    result = pd.DataFrame([forecasted_values_final, prediction_01, prediction_05, \n                        prediction_95, prediction_99]).T\n                    result.rename(columns={0:self.data_name, 1: \"1% Prediction Interval\", \n                        2: \"5% Prediction Interval\", 3: \"95% Prediction Interval\", 4: \"99% Prediction Interval\"}, \n                        inplace=True)\n\n                result.index = date_index[-h:]\n\n                return result\n     \n            else:\n                t_params = self.latent_variables.get_z_values()\n                a, P = self._forecast_model(t_params, Z, h)\n                smoothed_series = np.zeros(h)\n\n                for t in range(h):\n                    smoothed_series[t] = np.dot(Z[self.y.shape[0]+t],a[:,self.y.shape[0]+t])\n\n                # Retrieve data, dates and (transformed) latent variables         \n                forecasted_values = smoothed_series\n\n                if intervals is False:\n                    result = pd.DataFrame(forecasted_values)\n                    result.rename(columns={0:self.data_name}, inplace=True)\n                else:\n\n                    series_variance = np.zeros(h)\n\n                    for t in range(h):\n                        series_variance[t] = np.dot(np.dot(Z[self.y.shape[0]+t],P[:,:,self.y.shape[0]+t]),Z[self.y.shape[0]+t].T)\n\n                    prediction_05 = forecasted_values - 1.96*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(self.latent_variables.get_z_values()[0]),0.5)\n                    prediction_95 = forecasted_values + 1.96*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(self.latent_variables.get_z_values()[0]),0.5)\n                    prediction_01 = forecasted_values - 2.575*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(self.latent_variables.get_z_values()[0]),0.5)\n                    prediction_99 = forecasted_values + 2.575*np.power(P[0][0][-h:] + self.latent_variables.z_list[0].prior.transform(self.latent_variables.get_z_values()[0]),0.5)\n\n                    result = pd.DataFrame([forecasted_values, prediction_01, prediction_05, \n                        prediction_95, prediction_99]).T\n                    result.rename(columns={0:self.data_name, 1: \"1% Prediction Interval\", \n                        2: \"5% Prediction Interval\", 3: \"95% Prediction Interval\", 4: \"99% Prediction Interval\"}, \n                        inplace=True)\n     \n                result.index = date_index[-h:]\n\n                return result", "response": "Makes a forecast with the estimated model and returns the predictions for the current set of latent variables."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef predict_is(self, h=5, fit_once=True, fit_method='MLE', intervals=False):\n\n        predictions = []\n\n        for t in range(0,h):\n            data1 = self.data_original.iloc[:-h+t,:]\n            data2 = self.data_original.iloc[-h+t:,:]\n            x = DynReg(formula=self.formula, data=data1)\n\n            if fit_once is False:\n                x.fit(printer=False, fit_method=fit_method)\n            if t == 0:\n                if fit_once is True:\n                    x.fit(printer=False, fit_method=fit_method)\n                    saved_lvs = x.latent_variables\n                predictions = x.predict(1, oos_data=data2, intervals=intervals)\n            else:\n                if fit_once is True:\n                    x.latent_variables = saved_lvs\n                predictions = pd.concat([predictions,x.predict(h=1, oos_data=data2, intervals=intervals)])\n        \n        predictions.rename(columns={0:self.y_name}, inplace=True)\n        predictions.index = self.index[-h:]\n\n        return predictions", "response": "Makes dynamic in - sample predictions with the estimated model."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sample(self, nsims=1000):\n        if self.latent_variables.estimation_method not in ['BBVI', 'M-H']:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            lv_draws = self.draw_latent_variables(nsims=nsims)\n\n            mus = []\n            for i in range(nsims):\n                mu, V = self.smoothed_state(self.data, lv_draws[:,i])\n                _, Z, _, _, _ = self._ss_matrices(lv_draws[:,i])\n                smoothed_series = np.zeros(self.y.shape[0])\n                for t in range(0,self.y.shape[0]):\n                    smoothed_series[t] = np.dot(Z[t],mu[:,t])\n                mus.append(smoothed_series)\n\n            data_draws = np.array([np.random.normal(mus[i], np.sqrt(self.latent_variables.z_list[0].prior.transform(lv_draws[0,i])), mus[i].shape[0]) for i in range(nsims)])\n            return data_draws", "response": "Samples from the posterior predictive distribution and returns the data for the next sample."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef version(package, encoding='utf-8'):\n    path = os.path.join(os.path.dirname(__file__), package, '__init__.py')\n    with io.open(path, encoding=encoding) as fp:\n        version_info = fp.read()\n    version_match = re.search(r\"\"\"^__version__ = ['\"]([^'\"]*)['\"]\"\"\",\n                              version_info, re.M)\n    if not version_match:\n        raise RuntimeError(\"Unable to find version string.\")\n    return version_match.group(1)", "response": "Obtain the packge version from a python file e. g. pkg / __init__. py"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _uncythonized_model(self, beta):\n\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n        theta = np.zeros(self.model_Y.shape[0])\n        model_scale, model_shape, model_skewness = self._get_scale_and_shape(parm)\n\n         # Loop over time series\n        theta, self.model_scores = gas_llev_recursion(parm, theta, self.model_scores, self.model_Y, self.model_Y.shape[0], \n            self.family.score_function, self.link, model_scale, model_shape, model_skewness, self.max_lag)\n\n        return theta, self.model_Y, self.model_scores", "response": "Creates the structure of the model for the uncythonized time series."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _mean_prediction(self,theta,Y,scores,h,t_params):\n\n        Y_exp = Y.copy()\n        theta_exp = theta.copy()\n        scores_exp = scores.copy()\n\n        #(TODO: vectorize the inner construction here)      \n        for t in range(0,h):\n            new_value = theta_exp[-1] + t_params[0]*scores_exp[-1]\n            if self.model_name2 == \"Exponential\":\n                Y_exp = np.append(Y_exp, [1.0/self.link(new_value)])\n            else:\n                Y_exp = np.append(Y_exp, [self.link(new_value)])\n            theta_exp = np.append(theta_exp, [new_value]) # For indexing consistency\n            scores_exp = np.append(scores_exp, [0]) # expectation of score is zero\n        return Y_exp", "response": "Creates a h - step ahead mean prediction for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the likelihood of the states given the variance latent variables beta and alpha.", "response": "def state_likelihood(self, beta, alpha):\n        \"\"\" Returns likelihood of the states given the variance latent variables\n\n        Parameters\n        ----------\n        beta : np.array\n            Contains untransformed starting values for latent variables\n        alpha : np.array\n            State matrix\n        \n        Returns\n        ----------\n        State likelihood\n        \"\"\"\n        _, _, _, Q = self._ss_matrices(beta)\n        state_lik = 0\n        for i in range(alpha.shape[0]):\n            state_lik += np.sum(ss.norm.logpdf(alpha[i][1:]-alpha[i][:-1],loc=0,scale=np.power(Q[i][i],0.5))) \n        return state_lik"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef evo_blanket(self, beta, alpha):\n\n        # Markov blanket for each state\n        evo_blanket = np.zeros(self.state_no)\n        for i in range(evo_blanket.shape[0]):\n            evo_blanket[i] = self.state_likelihood_markov_blanket(beta, alpha, i).sum()\n\n        # If the family has additional parameters, add their markov blankets\n        if self.family_z_no > 0:\n            evo_blanket = np.append([self.likelihood_markov_blanket(beta).sum()]*(self.family_z_no),evo_blanket)\n\n        return evo_blanket", "response": "Creates the Markov blanket for the variance latent variables and returns the new Markov blanket for each state."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef log_p_blanket(self, beta):\n        states = np.zeros([self.state_no, self.data.shape[0]])\n        for state_i in range(self.state_no):\n            states[state_i,:] = beta[(self.z_no + (self.data.shape[0]*state_i)):(self.z_no + (self.data.shape[0]*(state_i+1)))]     \n        \n        return np.append(self.evo_blanket(beta,states),self.markov_blanket(beta,states))", "response": "Creates complete Markov blanket for latent variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _create_latent_variables(self):\n        for parm in range(self.z_no):\n            self.latent_variables.add_z('Sigma^2 ' + self.X_names[parm], fam.Flat(transform='exp'), fam.Normal(0,3))", "response": "Creates latent variables for the current class"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _preoptimize_model(self):\n        gaussian_model = DynReg(formula=self.formula, data=self.data_original)\n        gaussian_model.fit()\n\n        for i in range(self.z_no-self.family_z_no):\n            self.latent_variables.z_list[i].start = gaussian_model.latent_variables.get_z_values()[i+1]\n\n        if self.model_name2 == 't':\n\n            def temp_function(params):\n                return -np.sum(ss.t.logpdf(x=self.data, df=np.exp(params[0]), \n                    loc=np.ones(self.data.shape[0])*params[1], scale=np.exp(params[2])))\n\n            p = optimize.minimize(temp_function,np.array([2.0,0.0,-1.0]),method='L-BFGS-B')\n            self.latent_variables.z_list[-2].start = p.x[2]\n            self.latent_variables.z_list[-1].start = p.x[0]\n\n        elif self.model_name2 == 'Skewt':\n\n            def temp_function(params):\n                return -np.sum(fam.Skewt.logpdf_internal(x=self.data,df=np.exp(params[0]),\n                    loc=np.ones(self.data.shape[0])*params[1], scale=np.exp(params[2]),gamma=np.exp(params[3])))\n\n            p = optimize.minimize(temp_function,np.array([2.0,0.0,-1.0,0.0]),method='L-BFGS-B')\n            self.latent_variables.z_list[-3].start = p.x[3]\n            self.latent_variables.z_list[-2].start = p.x[2]\n            self.latent_variables.z_list[-1].start = p.x[0]\n\n        return gaussian_model.latent_variables", "response": "Pre - optimizes the model by estimating a Gaussian state space models\n         - Current state space latent variable object\n 69"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _ss_matrices(self,beta):\n\n\n        T = np.identity(self.state_no)\n        Z = self.X\n        R = np.identity(self.state_no)\n        \n        Q = np.identity(self.state_no)\n        for i in range(0,self.state_no):\n            Q[i][i] = self.latent_variables.z_list[i].prior.transform(beta[i])\n\n        return T, Z, R, Q", "response": "Creates the state space matrices required for the KFS algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_predict(self, h=5, past_values=20, intervals=True, oos_data=None, **kwargs):        \n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            # Retrieve data, dates and (transformed) latent variables\n            scale, shape, skewness = self._get_scale_and_shape(self.latent_variables.get_z_values(transformed=True))\n\n            # Retrieve data, dates and (transformed) latent variables\n            date_index = self.shift_dates(h)\n            simulations = 10000\n            sim_vector = np.zeros([simulations,h])\n\n            _, X_oos = dmatrices(self.formula, oos_data)\n            X_oos = np.array([X_oos])[0]\n            full_X = self.X.copy()\n            full_X = np.append(full_X,X_oos,axis=0)\n            Z = full_X\n            a = self.states\n\n            # Retrieve data, dates and (transformed) latent variables         \n            smoothed_series = np.zeros(h)\n            for t in range(h):\n                smoothed_series[t] = self.link(np.dot(Z[self.y.shape[0]+t],a[:,-1]))\n\n            for n in range(0,simulations):  \n                rnd_q = np.zeros((self.state_no,h))\n                coeff_sim = np.zeros((self.state_no,h))\n\n                # TO DO: vectorize this (easy)\n                for state in range(self.state_no):\n                    rnd_q[state] = np.random.normal(0,np.sqrt(self.latent_variables.get_z_values(transformed=True)[state]),h)\n\n                for t in range(0,h):\n                    if t == 0:\n                        for state in range(self.state_no):\n                            coeff_sim[state][t] = a[state][-1] + rnd_q[state][t]\n                    else:\n                        for state in range(self.state_no):\n                            coeff_sim[state][t] = coeff_sim[state][t-1] + rnd_q[state][t]\n\n                sim_vector[n] = self.family.draw_variable(loc=self.link(np.sum(coeff_sim.T*Z[self.y.shape[0]:self.y.shape[0]+h,:],axis=1)),shape=shape,scale=scale,skewness=skewness,nsims=h)\n\n            sim_vector = np.transpose(sim_vector)\n            forecasted_values = smoothed_series\n            previous_value = self.data[-1]\n\n            plt.figure(figsize=figsize) \n\n            if intervals == True:\n                plt.fill_between(date_index[-h-1:], np.insert([np.percentile(i,5) for i in sim_vector],0,previous_value), \n                    np.insert([np.percentile(i,95) for i in sim_vector],0,previous_value), alpha=0.2,label=\"95 C.I.\")   \n\n            plot_values = np.append(self.data[-past_values:],forecasted_values)\n            plot_index = date_index[-h-past_values:]\n\n            plt.plot(plot_index,plot_values,label=self.data_name)\n            plt.title(\"Forecast for \" + self.data_name)\n            plt.xlabel(\"Time\")\n            plt.ylabel(self.data_name)\n            plt.show()", "response": "Plots the predictive model of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_fit(self, intervals=True, **kwargs):\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            date_index = copy.deepcopy(self.index)\n            date_index = date_index[:self.y.shape[0]+1]\n\n            mu = self.states\n            var = self.states_var\n            # Create smoothed/filtered aggregate series\n            _, Z, _, _ = self._ss_matrices(self.latent_variables.get_z_values())\n            smoothed_series = np.zeros(self.y.shape[0])\n\n            for t in range(0,self.y.shape[0]):\n                smoothed_series[t] = np.dot(Z[t],mu[:,t])\n\n            plt.figure(figsize=figsize) \n            \n            plt.subplot(self.state_no+1, 1, 1)\n            plt.title(self.y_name + \" Raw and Smoothed\")    \n            plt.plot(date_index,self.data,label='Data')\n            plt.plot(date_index,self.link(smoothed_series),label='Smoothed Series',c='black')\n            plt.legend(loc=2)\n            \n            for coef in range(0,self.state_no):\n                V_coef = self.states_var[coef]\n                plt.subplot(self.state_no+1, 1, 2+coef)\n                plt.title(\"Beta \" + self.X_names[coef]) \n                states_upper_95 = self.states[coef] + 1.98*np.sqrt(V_coef)\n                states_lower_95 = self.states[coef] - 1.98*np.sqrt(V_coef)\n\n                if intervals == True:\n                    alpha =[0.15*i/float(100) for i in range(50,12,-2)]\n                    plt.fill_between(date_index, states_lower_95, states_upper_95, alpha=0.15,label='95% C.I.') \n\n                plt.plot(date_index,mu[coef,:],label='Coefficient')\n                plt.legend(loc=2)               \n            \n            plt.show()", "response": "Plots the fit of the model and returns the plotted data and the fit."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake a forecast with the estimated model and returns a pd. DataFrame with predictions for the current model.", "response": "def predict(self,h=5,oos_data=None):        \n        \"\"\" Makes forecast with the estimated model\n\n        Parameters\n        ----------\n        h : int (default : 5)\n            How many steps ahead would you like to forecast?\n\n        oos_data : pd.DataFrame\n            Data for the variables to be used out of sample (ys can be NaNs)\n\n        Returns\n        ----------\n        - pd.DataFrame with predictions\n        \"\"\"     \n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            # Sort/manipulate the out-of-sample data\n            _, X_oos = dmatrices(self.formula, oos_data)\n            X_oos = np.array([X_oos])[0]\n            full_X = self.X.copy()\n            full_X = np.append(full_X,X_oos,axis=0)\n            Z = full_X\n            a = self.states\n\n            # Retrieve data, dates and (transformed) latent variables         \n            smoothed_series = np.zeros(h)\n            for t in range(h):\n                smoothed_series[t] = self.link(np.dot(Z[self.y.shape[0]+t],a[:,-1]))\n\n            date_index = self.shift_dates(h)\n\n            result = pd.DataFrame(smoothed_series)\n            result.rename(columns={0:self.y_name}, inplace=True)\n            result.index = date_index[-h:]\n\n            return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes dynamic in - sample predictions with the estimated model", "response": "def predict_is(self,h=5):\n        \"\"\" Makes dynamic in-sample predictions with the estimated model\n\n        Parameters\n        ----------\n        h : int (default : 5)\n            How many steps would you like to forecast?\n\n        Returns\n        ----------\n        - pd.DataFrame with predicted values\n        \"\"\"     \n\n        predictions = []\n\n        for t in range(0,h):\n            data1 = self.data_original.iloc[:-h+t,:]\n            data2 = self.data_original.iloc[-h+t:,:] \n            x = NDynReg(formula=self.formula, data=data1, family=self.family)                                       \n            x.fit(print_progress=False)\n            if t == 0:\n                predictions = x.predict(1,oos_data=data2)\n            else:\n                predictions = pd.concat([predictions,x.predict(1,oos_data=data2)])\n        \n        predictions.rename(columns={0:self.data_name}, inplace=True)\n        predictions.index = self.index[-h:]\n\n        return predictions"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlogs PDF for Exponential prior at a given Latent variable mu.", "response": "def logpdf(self, mu):\n        \"\"\"\n        Log PDF for Exponential prior\n\n        Parameters\n        ----------\n        mu : float\n            Latent variable for which the prior is being formed over\n\n        Returns\n        ----------\n        - log(p(mu))\n        \"\"\"\n        if self.transform is not None:\n            mu = self.transform(mu)    \n        return ss.expon.logpdf(mu, self.lmd0)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the Markov blanket of the Exponential distribution with the given univariate time series y and mean.", "response": "def markov_blanket(y, mean, scale, shape, skewness):\n        \"\"\" Markov blanket for the Exponential distribution\n\n        Parameters\n        ----------\n        y : np.ndarray\n            univariate time series\n\n        mean : np.ndarray\n            array of location parameters for the Exponential distribution\n\n        scale : float\n            scale parameter for the Exponential distribution\n\n        shape : float\n            tail thickness parameter for the Exponential distribution\n\n        skewness : float\n            skewness parameter for the Exponential distribution\n\n        Returns\n        ----------\n        - Markov blanket of the Exponential family\n        \"\"\"\n        return ss.expon.logpdf(x=y, scale=1/mean)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nnegatives loglikelihood of a univariate time series", "response": "def neg_loglikelihood(y, mean, scale, shape, skewness):\n        \"\"\" Negative loglikelihood function\n\n        Parameters\n        ----------\n        y : np.ndarray\n            univariate time series\n\n        mean : np.ndarray\n            array of location parameters for the Exponential distribution\n\n        scale : float\n            scale parameter for the Exponential distribution\n\n        shape : float\n            tail thickness parameter for the Exponential distribution\n\n        skewness : float\n            skewness parameter for the Exponential distribution\n\n        Returns\n        ----------\n        - Negative loglikelihood of the Exponential family\n        \"\"\"\n        return -np.sum(ss.expon.logpdf(x=y, scale=1/mean))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_latent_variables(self):\n\n        for ar_term in range(self.ar):\n            self.latent_variables.add_z('AR(' + str(ar_term+1) + ')', fam.Normal(0,0.5,transform=None), fam.Normal(0,3))\n\n        for sc_term in range(self.sc):\n            self.latent_variables.add_z('SC(' + str(sc_term+1) + ')', fam.Normal(0,0.5,transform=None), fam.Normal(0,3))\n\n        for parm in range(len(self.X_names)):\n            self.latent_variables.add_z('Beta ' + self.X_names[parm], fam.Normal(0,3,transform=None), fam.Normal(0,3))", "response": "Creates latent variables for the current class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating the structure of the model for the given latent variables.", "response": "def _cythonized_model(self, beta):\n        \"\"\" Creates the structure of the model\n\n        Parameters\n        ----------\n        beta : np.array\n            Contains untransformed starting values for latent variables\n\n        Returns\n        ----------\n        theta : np.array\n            Contains the predicted values for the time series\n\n        Y : np.array\n            Contains the length-adjusted time series (accounting for lags)\n\n        scores : np.array\n            Contains the scores for the time series\n        \"\"\"\n\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n        model_scale, model_shape, model_skewness = self._get_scale_and_shape(parm)\n        theta = np.matmul(self.X[self.integ+self.max_lag:],parm[self.sc+self.ar:(self.sc+self.ar+len(self.X_names))])\n\n        # Loop over time series\n        theta, self.model_scores = self.recursion(parm, theta, self.model_scores, self.model_Y, self.ar, self.sc, self.model_Y.shape[0], model_scale, model_shape, model_skewness, self.max_lag)\n\n        return theta, self.model_Y, self.model_scores"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _uncythonized_model(self, beta):\n\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n        model_scale, model_shape, model_skewness = self._get_scale_and_shape(parm)\n        theta = np.matmul(self.X[self.integ+self.max_lag:],parm[self.sc+self.ar:(self.sc+self.ar+len(self.X_names))])\n\n        # Loop over time series\n        theta, self.model_scores = gasx_recursion(parm, theta, self.model_scores, self.model_Y, self.ar, self.sc, self.model_Y.shape[0], self.family.score_function, \n            self.link, model_scale, model_shape, model_skewness, self.max_lag)\n\n        return np.array(theta), self.model_Y, self.model_scores", "response": "Creates the structure of the model for the untransformed starting values for latent variables and returns the model for the latent variables."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate the structure of the model for the unythonized MB model.", "response": "def _uncythonized_mb_model(self, beta, mini_batch):\n        \"\"\" Creates the structure of the model\n\n        Parameters\n        ----------\n        beta : np.array\n            Contains untransformed starting values for latent variables\n\n        mini_batch : int\n            Size of each mini batch of data\n\n        Returns\n        ----------\n        theta : np.array\n            Contains the predicted values for the time series\n\n        Y : np.array\n            Contains the length-adjusted time series (accounting for lags)\n\n        scores : np.array\n            Contains the scores for the time series\n        \"\"\"\n\n        rand_int =  np.random.randint(low=0, high=self.data.shape[0]-mini_batch-self.max_lag+1)\n        sample = np.arange(start=rand_int, stop=rand_int+mini_batch)\n\n        data = self.data[sample]\n        X = self.X[sample, :]\n        Y = data[self.max_lag:]\n\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n        model_scale, model_shape, model_skewness = self._get_scale_and_shape(parm)\n        theta = np.matmul(X[self.integ+self.max_lag:],parm[self.sc+self.ar:(self.sc+self.ar+len(self.X_names))])\n\n        # Loop over time series\n        theta, self.model_scores = gasx_recursion(parm, theta, self.model_scores, Y, self.ar, self.sc, Y.shape[0], self.family.score_function, \n            self.link, model_scale, model_shape, model_skewness, self.max_lag)\n\n        return np.array(theta), Y, self.model_scores"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _mean_prediction(self, theta, Y, scores, h, t_params, X_oos):\n\n        Y_exp = Y.copy()\n        theta_exp = theta.copy()\n        scores_exp = scores.copy()\n\n        #(TODO: vectorize the inner construction here)      \n        for t in range(0,h):\n            new_value = t_params[0]\n\n            if self.ar != 0:\n                for j in range(1,self.ar+1):\n                    new_value += t_params[j]*theta_exp[-j]\n            if self.sc != 0:\n                for k in range(1,self.sc+1):\n                    new_value += t_params[k+self.ar]*scores_exp[-k]\n\n            new_value += np.matmul(X_oos[t,:],t_params[self.sc+self.ar:(self.sc+self.ar+len(self.X_names))])            \n\n            if self.model_name2 == \"Exponential GAS\":\n                Y_exp = np.append(Y_exp,[1.0/self.link(new_value)])\n            else:\n                Y_exp = np.append(Y_exp,[self.link(new_value)])\n            theta_exp = np.append(theta_exp,[new_value]) # For indexing consistency\n            scores_exp = np.append(scores_exp,[0]) # expectation of score is zero\n\n        return Y_exp", "response": "Creates a h - step ahead mean prediction for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _preoptimize_model(self, initials, method):\n        if not (self.ar==0 and self.sc == 0):\n            toy_model = GASX(formula=self.formula, ar=0, sc=0, integ=self.integ, family=self.family, data=self.data_original)\n            toy_model.fit(method)\n            self.latent_variables.z_list[0].start = toy_model.latent_variables.get_z_values(transformed=False)[0]\n            \n            for extra_z in range(len(self.family.build_latent_variables())):\n                self.latent_variables.z_list[self.ar+self.sc+extra_z].start = toy_model.latent_variables.get_z_values(transformed=False)[extra_z]\n            \n            # Random search for good AR/SC starting values\n            random_starts = np.random.normal(0.3, 0.3, [self.ar+self.sc+len(self.X_names), 1000])\n\n            best_start = self.latent_variables.get_z_starting_values()\n            best_lik = self.neg_loglik(self.latent_variables.get_z_starting_values())\n            proposal_start = best_start.copy()\n\n            for start in range(random_starts.shape[1]):\n                proposal_start[:self.ar+self.sc+len(self.X_names)] = random_starts[:,start]\n                proposal_start[0] = proposal_start[0]*(1.0-np.sum(random_starts[:self.ar,start]))\n                proposal_likelihood = self.neg_loglik(proposal_start)\n                if proposal_likelihood < best_lik:\n                    best_lik = proposal_likelihood\n                    best_start = proposal_start.copy()\n\n            return best_start\n\n        else:\n            return initials", "response": "Pre - optimizes the model by estimating a static model then quick search of good dynamic parameters"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_latent_variables(self):\n\n        self.latent_variables.add_z('Sigma^2 irregular', fam.Flat(transform='exp'), fam.Normal(0,3))\n\n        self.latent_variables.add_z('Constant', fam.Flat(transform=None), fam.Normal(0,3))\n\n        for parm in range(1,self.ar+1):\n            self.latent_variables.add_z('Sigma^2 AR(' + str(parm) + ')', fam.Flat(transform='exp'), fam.Normal(0,3))", "response": "Creates latent variables for the current class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot the predict method for the forecast with the estimated model.", "response": "def plot_predict(self, h=5, past_values=20, intervals=True, **kwargs):        \n        \"\"\" Makes forecast with the estimated model\n\n        Parameters\n        ----------\n        h : int (default : 5)\n            How many steps ahead would you like to forecast?\n\n        past_values : int (default : 20)\n            How many past observations to show on the forecast graph?\n\n        intervals : Boolean\n            Would you like to show 95% prediction intervals for the forecast?\n\n        Returns\n        ----------\n        - Plot of the forecast\n        \"\"\"\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            y_holder = self.y.copy() # holds past data and predicted data to create AR matrix\n            full_X = self.X.copy()\n            full_X = np.append(full_X,np.array([np.append(1.0, y_holder[-self.ar:][::-1])]), axis=0)\n            Z = full_X\n\n            # Construct Z matrix\n            for step in range(h):\n                a, P = self._forecast_model(self.latent_variables.get_z_values(),Z,step)\n                new_value = np.dot(Z[-1,:],a[:,self.y.shape[0]+step])\n                y_holder = np.append(y_holder, new_value)\n                Z = np.append(Z, np.array([np.append(1.0, y_holder[-self.ar:][::-1])]), axis=0)\n\n            # Retrieve data, dates and (transformed) latent variables         \n            a, P = self._forecast_model(self.latent_variables.get_z_values(),Z,h)\n            smoothed_series = np.zeros(self.y.shape[0]+h)\n            series_variance = np.zeros(self.y.shape[0]+h)\n            for t in range(self.y.shape[0]+h):\n                smoothed_series[t] = np.dot(Z[t],a[:,t])\n                series_variance[t] = np.dot(np.dot(Z[t],P[:,:,t]),Z[t].T) + self.latent_variables.z_list[0].prior.transform(self.latent_variables.get_z_values()[0])    \n\n            date_index = self.shift_dates(h)\n            plot_values = smoothed_series[-h-past_values:]\n            forecasted_values = smoothed_series[-h:]\n            lower = forecasted_values - 1.98*np.power(series_variance[-h:],0.5)\n            upper = forecasted_values + 1.98*np.power(series_variance[-h:],0.5)\n            lower = np.append(plot_values[-h-1],lower)\n            upper = np.append(plot_values[-h-1],upper)\n\n            plot_index = date_index[-h-past_values:]\n\n            plt.figure(figsize=figsize)\n            if intervals == True:\n                plt.fill_between(date_index[-h-1:], lower, upper, alpha=0.2)            \n\n            plt.plot(plot_index,plot_values)\n            plt.title(\"Forecast for \" + self.y_name)\n            plt.xlabel(\"Time\")\n            plt.ylabel(self.y_name)\n            plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake forecast with the estimated model", "response": "def predict(self, h=5):        \n        \"\"\" Makes forecast with the estimated model\n\n        Parameters\n        ----------\n        h : int (default : 5)\n            How many steps ahead would you like to forecast?\n\n        Returns\n        ----------\n        - pd.DataFrame with predictions\n        \"\"\"     \n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            y_holder = self.y.copy() # holds past data and predicted data to create AR matrix\n            full_X = self.X.copy()\n            full_X = np.append(full_X,np.array([np.append(1.0, y_holder[-self.ar:][::-1])]), axis=0)\n            Z = full_X\n\n            for step in range(h):\n                a, P = self._forecast_model(self.latent_variables.get_z_values(),Z,step)\n                new_value = np.dot(Z[-1,:],a[:,self.y.shape[0]+step])\n                y_holder = np.append(y_holder, new_value)\n                Z = np.append(Z, np.array([np.append(1.0, y_holder[-self.ar:][::-1])]), axis=0)\n\n            date_index = self.shift_dates(h)\n\n            result = pd.DataFrame(y_holder[-h:])\n            result.rename(columns={0:self.y_name}, inplace=True)\n            result.index = date_index[-h:]\n\n            return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef predict_is(self, h=5, fit_once=True):\n\n        predictions = []\n\n        for t in range(0,h):\n            data1 = self.data_original_nondf[:-h+t]\n            x = DAR(data=data1, ar=self.ar, integ=self.integ)\n            if fit_once is False:\n                x.fit(printer=False)\n            if t == 0:\n                if fit_once is True:\n                    x.fit(printer=False)\n                    saved_lvs = x.latent_variables\n                predictions = x.predict(1)\n            else:\n                if fit_once is True:\n                    x.latent_variables = saved_lvs\n                predictions = pd.concat([predictions,x.predict(1)])\n\n        predictions.rename(columns={0:self.y_name}, inplace=True)\n        predictions.index = self.index[-h:]\n\n        return predictions", "response": "Makes dynamic in - sample predictions with the estimated model."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef change_parameters(self,params):\n        no_of_params = 0\n        for core_param in range(len(self.q)):\n            for approx_param in range(self.q[core_param].param_no):\n                self.q[core_param].vi_change_param(approx_param, params[no_of_params])\n                no_of_params += 1", "response": "Utility function for changing the approximate distribution parameters"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating logq components for mean - field normal family", "response": "def create_normal_logq(self,z):\n        \"\"\"\n        Create logq components for mean-field normal family (the entropy estimate)\n        \"\"\"\n        means, scale = self.get_means_and_scales()\n        return ss.norm.logpdf(z,loc=means,scale=scale).sum()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef current_parameters(self):\n        current = []\n        for core_param in range(len(self.q)):\n            for approx_param in range(self.q[core_param].param_no):\n                current.append(self.q[core_param].vi_return_param(approx_param))\n        return np.array(current)", "response": "Gets an array with the current parameters"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndrawing parameters from a mean - field normal family", "response": "def draw_normal(self):\n        \"\"\"\n        Draw parameters from a mean-field normal family\n        \"\"\"\n        means, scale = self.get_means_and_scales()\n        return np.random.normal(means,scale,size=[self.sims,means.shape[0]]).T"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef draw_normal_initial(self):\n        means, scale = self.get_means_and_scales_from_q()\n        return np.random.normal(means,scale,size=[self.sims,means.shape[0]]).T", "response": "Draw parameters from a mean - field normal family"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndrawing parameters from the approximating distributions", "response": "def draw_variables(self):\n        \"\"\"\n        Draw parameters from the approximating distributions\n        \"\"\"        \n        z = self.q[0].draw_variable_local(self.sims)\n        for i in range(1,len(self.q)):\n            z = np.vstack((z,self.q[i].draw_variable_local(self.sims)))\n        return z"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_means_and_scales_from_q(self):\n        means = np.zeros(len(self.q))\n        scale = np.zeros(len(self.q))\n        for i in range(len(self.q)):\n            means[i] = self.q[i].mu0\n            scale[i] = self.q[i].sigma0 \n        return means, scale", "response": "Gets the mean and scale for normal approximating parameters\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_means_and_scales(self):\n        return self.optim.parameters[::2], np.exp(self.optim.parameters[1::2])", "response": "Gets the mean and scale for normal approximating parameters\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the gradients of the approximating distributions", "response": "def grad_log_q(self,z):\n        \"\"\"\n        The gradients of the approximating distributions\n        \"\"\"        \n        param_count = 0\n        grad = np.zeros((np.sum(self.approx_param_no),self.sims))\n        for core_param in range(len(self.q)):\n            for approx_param in range(self.q[core_param].param_no):\n                grad[param_count] = self.q[core_param].vi_score(z[core_param],approx_param)        \n                param_count += 1\n        return grad"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the log posterior components of the log posterior of the log posterior of the specified z.", "response": "def log_p(self,z):\n        \"\"\"\n        The unnormalized log posterior components (the quantity we want to approximate)\n        RAO-BLACKWELLIZED!\n        \"\"\"      \n        return np.array([self.log_p_blanket(i) for i in z])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the unnormalized log posterior components for the mean - field normal family.", "response": "def normal_log_q(self,z):\n        \"\"\"\n        The unnormalized log posterior components for mean-field normal family (the quantity we want to approximate)\n        RAO-BLACKWELLIZED!\n        \"\"\"             \n        means, scale = self.get_means_and_scales()\n        return ss.norm.logpdf(z,loc=means,scale=scale)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef normal_log_q_initial(self,z):\n        means, scale = self.get_means_and_scales_from_q()\n        return ss.norm.logpdf(z,loc=means,scale=scale)", "response": "Returns the initial log - posterior of the mean - field normal family at the given z."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_progress(self, i, current_params):\n        for split in range(1,11):\n            if i == (round(self.iterations/10*split)-1):\n                post = -self.full_neg_posterior(current_params)\n                approx = self.create_normal_logq(current_params)\n                diff = post - approx\n                if not self.quiet_progress:\n                    print(str(split) + \"0% done : ELBO is \" + str(diff) + \", p(y,z) is \" + str(post) + \", q(z) is \" + str(approx))", "response": "Print the current ELBO at every decile of total iterations"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate the structure of the model for the current time series and returns the model for the current time series and the model for the current latent variable.", "response": "def _model(self, beta):\n        \"\"\" Creates the structure of the model\n\n        Parameters\n        ----------\n        beta : np.array\n            Contains untransformed starting values for latent variables\n\n        Returns\n        ----------\n        sigma2 : np.array\n            Contains the values for the conditional volatility series\n\n        Y : np.array\n            Contains the length-adjusted time series (accounting for lags)\n\n        eps : np.array\n            Contains the squared residuals (ARCH terms) for the time series\n        \"\"\"\n\n        # Transform latent variables\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n\n        xeps = np.power(self.data-parm[-1],2)\n        Y = np.array(self.data[self.max_lag:])\n        eps = np.power(Y-parm[-1],2)\n        X = np.ones(Y.shape[0])\n\n        # ARCH terms\n        if self.q != 0:\n            for i in range(0,self.q):   \n                X = np.vstack((X,xeps[(self.max_lag-i-1):-i-1]))\n            sigma2 = np.matmul(np.transpose(X),parm[0:-self.p-1])\n        else:\n            sigma2 = np.transpose(X*parm[0])\n\n        sigma2 = garch_recursion(parm, sigma2, self.q, self.p, Y.shape[0], self.max_lag)\n\n        return np.array(sigma2), Y, eps"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates the structure of the model for mini batch model.", "response": "def _mb_model(self, beta, mini_batch):\n        \"\"\" Creates the structure of the model (model matrices etc) for mini batch model.\n        \n        Here the structure is the same as for _normal_model() but we are going to\n        sample a random choice of data points (of length mini_batch).\n\n        Parameters\n        ----------\n        beta : np.ndarray\n            Contains untransformed starting values for the latent variables\n\n        mini_batch : int\n            Mini batch size for the data sampling\n\n        Returns\n        ----------\n        mu : np.ndarray\n            Contains the predicted values (location) for the time series\n\n        Y : np.ndarray\n            Contains the length-adjusted time series (accounting for lags)\n        \"\"\"     \n\n        # Transform latent variables\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n\n        rand_int =  np.random.randint(low=0, high=self.data_length-mini_batch+1)\n        sample = np.arange(start=rand_int, stop=rand_int+mini_batch)\n        sampled_data = self.data[sample]\n\n        xeps = np.power(sampled_data-parm[-1],2)\n        Y = np.array(sampled_data[self.max_lag:])\n        eps = np.power(Y-parm[-1],2)\n        X = np.ones(Y.shape[0])\n\n        # ARCH terms\n        if self.q != 0:\n            for i in range(0,self.q):   \n                X = np.vstack((X,xeps[(self.max_lag-i-1):-i-1]))\n            sigma2 = np.matmul(np.transpose(X),parm[0:-self.p-1])\n        else:\n            sigma2 = np.transpose(X*parm[0])\n\n        sigma2 = garch_recursion(parm, sigma2, self.q, self.p, Y.shape[0], self.max_lag)\n\n        return np.array(sigma2), Y, eps"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _mean_prediction(self, sigma2, Y, scores, h, t_params):\n\n        # Create arrays to iteratre over\n        sigma2_exp = sigma2.copy()\n        scores_exp = scores.copy()\n\n        # Loop over h time periods          \n        for t in range(0,h):\n            new_value = t_params[0]\n\n            # ARCH\n            if self.q != 0:\n                for j in range(1,self.q+1):\n                    new_value += t_params[j]*scores_exp[-j]\n\n            # GARCH\n            if self.p != 0:\n                for k in range(1,self.p+1):\n                    new_value += t_params[k+self.q]*sigma2_exp[-k]                  \n\n            sigma2_exp = np.append(sigma2_exp,[new_value]) # For indexing consistency\n            scores_exp = np.append(scores_exp,[0]) # expectation of score is zero\n\n        return sigma2_exp", "response": "This function creates a h - step ahead mean prediction for the current set of latent variables and returns the sigma2 and scores arrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _sim_prediction(self, sigma2, Y, scores, h, t_params, simulations):\n\n        sim_vector = np.zeros([simulations,h])\n\n        for n in range(0,simulations):\n            # Create arrays to iteratre over        \n            sigma2_exp = sigma2.copy()\n            scores_exp = scores.copy()\n\n            # Loop over h time periods          \n            for t in range(0,h):\n                new_value = t_params[0]\n\n                if self.q != 0:\n                    for j in range(1,self.q+1):\n                        new_value += t_params[j]*scores_exp[-j]\n\n                if self.p != 0:\n                    for k in range(1,self.p+1):\n                        new_value += t_params[k+self.q]*sigma2_exp[-k]  \n\n                sigma2_exp = np.append(sigma2_exp,[new_value]) # For indexing consistency\n                scores_exp = np.append(scores_exp,scores[np.random.randint(scores.shape[0])]) # expectation of score is zero\n\n            sim_vector[n] = sigma2_exp[-h:]\n\n        return np.transpose(sim_vector)", "response": "Simulates a h - step ahead mean prediction of the current state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _sim_prediction_bayes(self, h, simulations):\n\n        sim_vector = np.zeros([simulations,h])\n\n        for n in range(0,simulations):\n\n            t_z = self.draw_latent_variables(nsims=1).T[0]\n            sigma2, Y, scores = self._model(t_z)\n            t_z = np.array([self.latent_variables.z_list[k].prior.transform(t_z[k]) for k in range(t_z.shape[0])])\n\n            # Create arrays to iteratre over        \n            sigma2_exp = sigma2.copy()\n            scores_exp = scores.copy()\n\n            # Loop over h time periods          \n            for t in range(0,h):\n                new_value = t_z[0]\n\n                if self.q != 0:\n                    for j in range(1,self.q+1):\n                        new_value += t_z[j]*scores_exp[-j]\n\n                if self.p != 0:\n                    for k in range(1,self.p+1):\n                        new_value += t_z[k+self.q]*sigma2_exp[-k]  \n\n                sigma2_exp = np.append(sigma2_exp,[new_value]) # For indexing consistency\n                scores_exp = np.append(scores_exp,scores[np.random.randint(scores.shape[0])]) # expectation of score is zero\n\n            sim_vector[n] = sigma2_exp[-h:]\n\n        return np.transpose(sim_vector)", "response": "Simulates a h - step ahead mean prediction for a set of simulations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate the negative log - likelihood of the model with respect to the model s latent variables.", "response": "def neg_loglik(self, beta):\n        \"\"\" Creates the negative log-likelihood of the model\n\n        Parameters\n        ----------\n        beta : np.array\n            Contains untransformed starting values for latent variables\n\n        Returns\n        ----------\n        The negative logliklihood of the model\n        \"\"\"     \n\n        sigma2, Y, __ = self._model(beta)\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n        return -np.sum(ss.norm.logpdf(Y, loc=parm[-1]*np.ones(sigma2.shape[0]), scale=np.sqrt(sigma2)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nplots the fit of the model and returns None", "response": "def plot_fit(self, **kwargs):\n        \"\"\" Plots the fit of the model\n\n        Returns\n        ----------\n        None (plots data and the fit)\n        \"\"\"\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            plt.figure(figsize=figsize)\n            date_index = self.index[max(self.p, self.q):]\n            t_params = self.transform_z()\n            sigma2, Y, ___ = self._model(self.latent_variables.get_z_values())\n            plt.plot(date_index, np.abs(Y-t_params[-1]), label=self.data_name + ' Absolute Demeaned Values')\n            plt.plot(date_index, np.power(sigma2,0.5), label='GARCH(' + str(self.p) + ',' + str(self.q) + ') Conditional Volatility',c='black')\n            plt.title(self.data_name + \" Volatility Plot\")  \n            plt.legend(loc=2)   \n            plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot the predict method of the current model and returns a matplotlib. pyplot. pyplot.", "response": "def plot_predict(self, h=5, past_values=20, intervals=True, **kwargs):      \n        \"\"\" Makes forecast with the estimated model\n\n        Parameters\n        ----------\n        h : int (default : 5)\n            How many steps ahead would you like to forecast?\n\n        past_values : int (default : 20)\n            How many past observations to show on the forecast graph?\n\n        intervals : Boolean\n            Would you like to show prediction intervals for the forecast?\n\n        Returns\n        ----------\n        - Plot of the forecast\n        \"\"\"\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n\n            # Retrieve data, dates and (transformed) latent variables\n            sigma2, Y, scores = self._model(self.latent_variables.get_z_values())         \n            date_index = self.shift_dates(h)\n\n            if self.latent_variables.estimation_method in ['M-H']:\n                sim_vector = self._sim_prediction_bayes(h, 15000)\n                error_bars = []\n\n                for pre in range(5,100,5):\n                    error_bars.append(np.insert([np.percentile(i,pre) for i in sim_vector], 0, sigma2[-1]))\n\n                forecasted_values = np.insert([np.mean(i) for i in sim_vector], 0, sigma2[-1])\n                plot_values = np.append(sigma2[-1-past_values:-2], forecasted_values)\n                plot_index = date_index[-h-past_values:]\n\n            else:\n                t_z = self.transform_z()\n                sim_values = self._sim_prediction(sigma2, Y, scores, h, t_z, 15000)\n                error_bars, forecasted_values, plot_values, plot_index = self._summarize_simulations(sigma2, sim_values, date_index, h, past_values)\n\n            plt.figure(figsize=figsize)\n            if intervals == True:\n                alpha =[0.15*i/float(100) for i in range(50,12,-2)]\n                for count, pre in enumerate(error_bars):\n                    plt.fill_between(date_index[-h-1:], error_bars[count], error_bars[-count-1], alpha=alpha[count])   \n\n            plt.plot(plot_index, plot_values)\n            plt.title(\"Forecast for \" + self.data_name + \" Conditional Volatility\")\n            plt.xlabel(\"Time\")\n            plt.ylabel(self.data_name + \" Conditional Volatility\")\n            plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef predict_is(self, h=5, fit_once=True, fit_method='MLE', intervals=False, **kwargs):\n\n        predictions = []\n\n        for t in range(0,h):\n            x = GARCH(p=self.p, q=self.q, data=self.data[0:-h+t])\n\n            if fit_once is False:\n                x.fit(method=fit_method, printer=False)\n\n            if t == 0:\n                if fit_once is True:\n                    x.fit(method=fit_method, printer=False)\n                    saved_lvs = x.latent_variables\n                predictions = x.predict(1, intervals=intervals)\n            else:\n                if fit_once is True:\n                    x.latent_variables = saved_lvs\n                predictions = pd.concat([predictions,x.predict(1, intervals=intervals)])\n        \n        if intervals is True:\n            predictions.rename(columns={0:self.data_name, 1: \"1% Prediction Interval\", \n                2: \"5% Prediction Interval\", 3: \"95% Prediction Interval\", 4: \"99% Prediction Interval\"}, inplace=True)\n        else:\n            predictions.rename(columns={0:self.data_name}, inplace=True)\n\n        predictions.index = self.index[-h:]\n\n        return predictions", "response": "Predicts the in - sample data for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlogging PDF for the Skew - t distribution.", "response": "def logpdf(x, shape, loc=0.0, scale=1.0, skewness=1.0):\n    \"\"\"\n    Log PDF for the Skew-t distribution\n\n    Parameters\n    ----------\n    x : np.array\n        random variables\n\n    shape : float\n        The degrees of freedom for the skew-t distribution\n\n    loc : np.array\n        The location parameter for the skew-t distribution\n\n    scale : float\n        The scale of the distribution\n\n    skewness : float\n        Skewness parameter (if 1, no skewness, if > 1, +ve skew, if < 1, -ve skew)\n\n    \"\"\"\n    m1 = (np.sqrt(shape)*sp.gamma((shape-1.0)/2.0))/(np.sqrt(np.pi)*sp.gamma(shape/2.0))\n    loc = loc + (skewness - (1.0/skewness))*scale*m1\n    result = np.zeros(x.shape[0])\n    result[x-loc<0] = np.log(2.0) - np.log(skewness + 1.0/skewness) + ss.t.logpdf(x=skewness*x[(x-loc) < 0], loc=loc[(x-loc) < 0]*skewness,df=shape, scale=scale[(x-loc) < 0])\n    result[x-loc>=0] = np.log(2.0) - np.log(skewness + 1.0/skewness) + ss.t.logpdf(x=x[(x-loc) >= 0]/skewness, loc=loc[(x-loc) >= 0]/skewness,df=shape, scale=scale[(x-loc) >= 0])\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding leverage term to the model Returns None if the model is updated", "response": "def add_leverage(self):\n        \"\"\" Adds leverage term to the model\n\n        Returns\n        ----------\n        None (changes instance attributes)\n        \"\"\"             \n\n        if self.leverage is True:\n            pass\n        else:\n            self.leverage = True\n            self.z_no += 1\n            self.latent_variables.z_list.pop()\n            self.latent_variables.z_list.pop()\n            self.latent_variables.z_list.pop()\n            self.latent_variables.z_list.pop()\n            self.latent_variables.add_z('Leverage Term', fam.Flat(transform=None), fam.Normal(0, 3))\n            self.latent_variables.add_z('Skewness', fam.Flat(transform='exp'), fam.Normal(0, 3))\n            self.latent_variables.add_z('v', fam.Flat(transform='exp'), fam.Normal(0, 3))\n            self.latent_variables.add_z('Returns Constant', fam.Normal(0,3,transform=None), fam.Normal(0, 3))\n            self.latent_variables.add_z('GARCH-M', fam.Normal(0,3,transform=None), fam.Normal(0, 3))\n            self.latent_variables.z_list[-3].start = 2.0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef neg_loglik(self, beta):\n\n        lmda, Y, ___, theta = self._model(beta)\n        return -np.sum(logpdf(Y, self.latent_variables.z_list[-3].prior.transform(beta[-3]), \n            loc=theta, scale=np.exp(lmda/2.0), \n            skewness = self.latent_variables.z_list[-4].prior.transform(beta[-4])))", "response": "Creates the negative log - likelihood of the model for the given set of latent variables."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsample from the posterior predictive distribution", "response": "def sample(self, nsims=1000):\n        \"\"\" Samples from the posterior predictive distribution\n\n        Parameters\n        ----------\n        nsims : int (default : 1000)\n            How many draws from the posterior predictive distribution\n\n        Returns\n        ----------\n        - np.ndarray of draws from the data\n        \"\"\"     \n        if self.latent_variables.estimation_method not in ['BBVI', 'M-H']:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            lv_draws = self.draw_latent_variables(nsims=nsims)\n            sigmas = [self._model(lv_draws[:,i])[0] for i in range(nsims)]\n            data_draws = np.array([fam.Skewt.draw_variable(loc=self.latent_variables.z_list[-2].prior.transform(lv_draws[-2,i]),\n                shape=self.latent_variables.z_list[-3].prior.transform(lv_draws[-3,i]), \n                skewness=self.latent_variables.z_list[-4].prior.transform(lv_draws[-4,i]), nsims=len(sigmas[i]), scale=np.exp(sigmas[i]/2.0)) for i in range(nsims)])\n            return data_draws"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the posterior predictive p - value for the current object.", "response": "def ppc(self, nsims=1000, T=np.mean):\n        \"\"\" Computes posterior predictive p-value\n\n        Parameters\n        ----------\n        nsims : int (default : 1000)\n            How many draws for the PPC\n\n        T : function\n            A discrepancy measure - e.g. np.mean, np.std, np.max\n\n        Returns\n        ----------\n        - float (posterior predictive p-value)\n        \"\"\"     \n        if self.latent_variables.estimation_method not in ['BBVI', 'M-H']:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            lv_draws = self.draw_latent_variables(nsims=nsims)\n            sigmas = [self._model(lv_draws[:,i])[0] for i in range(nsims)]\n            data_draws = np.array([fam.Skewt.draw_variable(loc=self.latent_variables.z_list[-2].prior.transform(lv_draws[-2,i]),\n                shape=self.latent_variables.z_list[-3].prior.transform(lv_draws[-3,i]), \n                skewness=self.latent_variables.z_list[-4].prior.transform(lv_draws[-4,i]), nsims=len(sigmas[i]), scale=np.exp(sigmas[i]/2.0)) for i in range(nsims)])\n            T_sims = T(self.sample(nsims=nsims), axis=1)\n            T_actual = T(self.data)\n            return len(T_sims[T_sims>T_actual])/nsims"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplotting histogram of the discrepancy from draws of the posterior and the posterior.", "response": "def plot_ppc(self, nsims=1000, T=np.mean, **kwargs):\n        \"\"\" Plots histogram of the discrepancy from draws of the posterior\n\n        Parameters\n        ----------\n        nsims : int (default : 1000)\n            How many draws for the PPC\n\n        T : function\n            A discrepancy measure - e.g. np.mean, np.std, np.max\n        \"\"\"     \n        if self.latent_variables.estimation_method not in ['BBVI', 'M-H']:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            import matplotlib.pyplot as plt\n            import seaborn as sns\n\n            figsize = kwargs.get('figsize',(10,7))\n\n            lv_draws = self.draw_latent_variables(nsims=nsims)\n            sigmas = [self._model(lv_draws[:,i])[0] for i in range(nsims)]\n            data_draws = np.array([fam.Skewt.draw_variable(loc=self.latent_variables.z_list[-2].prior.transform(lv_draws[-2,i]),\n                shape=self.latent_variables.z_list[-3].prior.transform(lv_draws[-3,i]), \n                skewness=self.latent_variables.z_list[-4].prior.transform(lv_draws[-4,i]), nsims=len(sigmas[i]), scale=np.exp(sigmas[i]/2.0)) for i in range(nsims)])\n            T_sim = T(self.sample(nsims=nsims), axis=1)\n            T_actual = T(self.data)\n\n            if T == np.mean:\n                description = \" of the mean\"\n            elif T == np.max:\n                description = \" of the maximum\"\n            elif T == np.min:\n                description = \" of the minimum\"\n            elif T == np.median:\n                description = \" of the median\"\n            else:\n                description = \"\"\n\n            plt.figure(figsize=figsize)\n            ax = plt.subplot()\n            ax.axvline(T_actual)\n            sns.distplot(T_sim, kde=False, ax=ax)\n            ax.set(title='Posterior predictive' + description, xlabel='T(x)', ylabel='Frequency');\n            plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_latent_variables(self):\n\n        self.latent_variables.add_z('Sigma^2 irregular', fam.Flat(transform='exp'), fam.Normal(0,3))\n        self.latent_variables.add_z('Sigma^2 level', fam.Flat(transform='exp'), fam.Normal(0,3))\n        self.latent_variables.add_z('Sigma^2 trend', fam.Flat(transform='exp'), fam.Normal(0,3))", "response": "Creates latent variables for the current object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate the state space matrices required for the KFS algorithm.", "response": "def _ss_matrices(self,beta):\n        \"\"\" Creates the state space matrices required\n\n        Parameters\n        ----------\n        beta : np.array\n            Contains untransformed starting values for latent variables\n\n        Returns\n        ----------\n        T, Z, R, Q, H : np.array\n            State space matrices used in KFS algorithm\n        \"\"\"     \n\n        T = np.identity(2)\n        T[0][1] = 1\n        \n        Z = np.zeros(2)\n        Z[0] = 1\n\n        R = np.identity(2)\n        Q = np.identity(2)\n        H = np.identity(1)*self.latent_variables.z_list[0].prior.transform(beta[0])\n        Q[0][0] = self.latent_variables.z_list[1].prior.transform(beta[1])\n        Q[1][1] = self.latent_variables.z_list[2].prior.transform(beta[2])\n\n        return T, Z, R, Q, H"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nplotting the fit of the model of the current object.", "response": "def plot_fit(self,intervals=True,**kwargs):\n        \"\"\" Plots the fit of the model\n\n        Parameters\n        ----------\n        intervals : Boolean\n            Whether to plot 95% confidence interval of states\n\n        Returns\n        ----------\n        None (plots data and the fit)\n        \"\"\"\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n        series_type = kwargs.get('series_type','Smoothed')\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            date_index = copy.deepcopy(self.index)\n            date_index = date_index[self.integ:self.data_original.shape[0]+1]\n\n            if series_type == 'Smoothed':\n                mu, V= self.smoothed_state(self.data,self.latent_variables.get_z_values())\n            elif series_type == 'Filtered':\n                mu, V, _, _, _ = self._model(self.data,self.latent_variables.get_z_values())\n            else:\n                mu, V = self.smoothed_state(self.data,self.latent_variables.get_z_values())\n\n            mu0 = mu[0][:-1]\n            mu1 = mu[1][:-1]\n            Vlev = V[0][0][:-1]\n            Vtrend = V[0][1][:-1]   \n\n            plt.figure(figsize=figsize) \n            \n            plt.subplot(2, 2, 1)\n            plt.title(self.data_name + \" Raw and \" + series_type)   \n\n            if intervals == True:\n                alpha =[0.15*i/float(100) for i in range(50,12,-2)]\n                plt.fill_between(date_index[2:], mu0[2:] + 1.98*np.sqrt(Vlev[2:]), mu0[2:] - 1.98*np.sqrt(Vlev[2:]), alpha=0.15,label='95% C.I.')   \n\n            plt.plot(date_index,self.data,label='Data')\n            plt.plot(date_index,mu0,label=series_type,c='black')\n            plt.legend(loc=2)\n            \n            plt.subplot(2, 2, 2)\n\n            if intervals == True:\n                alpha =[0.15*i/float(100) for i in range(50,12,-2)]\n                plt.fill_between(date_index[2:], mu0[2:] + 1.98*np.sqrt(Vlev[2:]), mu0[2:] - 1.98*np.sqrt(Vlev[2:]), alpha=0.15,label='95% C.I.')   \n\n            plt.title(self.data_name + \" Local Level\")  \n            plt.plot(date_index,mu0,label='Local Level')\n            plt.legend(loc=2)\n            \n            plt.subplot(2, 2, 3)\n\n            if intervals == True:\n                alpha =[0.15*i/float(100) for i in range(50,12,-2)]\n                plt.fill_between(date_index[2:], mu1[2:] + 1.98*np.sqrt(Vtrend[2:]), mu1[2:] - 1.98*np.sqrt(Vtrend[2:]), alpha=0.15,label='95% C.I.')   \n\n            plt.title(self.data_name + \" Trend\")    \n            plt.plot(date_index,mu1,label='Stochastic Trend')\n            plt.legend(loc=2)\n\n            plt.subplot(2, 2, 4)\n            plt.title(\"Measurement Noise\")  \n            plt.plot(date_index[1:self.data.shape[0]],self.data[1:self.data.shape[0]]-mu0[1:self.data.shape[0]],label='Irregular term')\n            plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_latent_variables(self):\n\n        self.latent_variables.add_z('Vol Constant', fam.Normal(0,3,transform=None), fam.Normal(0,3))\n\n        for component in range(2):\n            increment = 0.05\n            for p_term in range(self.p):\n                self.latent_variables.add_z(\"Component \" + str(component+1) + ' p(' + str(p_term+1) + ')', fam.Normal(0,0.5,transform='logit'), fam.Normal(0,3))\n                if p_term == 0:\n                    self.latent_variables.z_list[1+p_term+component*(self.p+self.q)].start = 3.00\n                else:\n                    self.latent_variables.z_list[1+p_term+component*(self.p+self.q)].start = 2.00\n\n            for q_term in range(self.q):\n                self.latent_variables.add_z(\"Component \" + str(component+1) + ' q(' + str(q_term+1) + ')', fam.Normal(0,0.5,transform='logit'), fam.Normal(0,3))\n                if p_term == 0 and component == 0:\n                    self.latent_variables.z_list[1+self.p+q_term+component*(self.p+self.q)].start = -4.00  \n                elif p_term == 0 and component == 1:\n                    self.latent_variables.z_list[1+self.p+q_term+component*(self.p+self.q)].start = -3.00  \n\n        self.latent_variables.add_z('v', fam.Flat(transform='exp'), fam.Normal(0,3))\n        self.latent_variables.add_z('Returns Constant', fam.Normal(0, 3, transform=None), fam.Normal(0,3))\n        self.latent_variables.z_list[-2].start = 2.0", "response": "Creates latent variables for the current class"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _mean_prediction(self, lmda, lmda_c, Y, scores, h, t_params):\n\n        # Create arrays to iteratre over\n        lmda_exp = lmda.copy()\n        lmda_c_exp = [lmda_c.T[0],lmda_c.T[1]] \n        scores_exp = scores.copy()\n        Y_exp = Y.copy()\n\n        # Loop over h time periods          \n        for t in range(0,h):\n            new_value_comp = np.zeros(2)\n            new_value = 0\n\n            for comp in range(2):\n\n                if self.p != 0:\n                    for j in range(self.p):\n                        new_value_comp[comp] += t_params[1+j+(comp*(self.q+self.p))]*lmda_c_exp[comp][-j-1]\n\n                if self.q != 0:\n                    for k in range(self.q):\n                        new_value_comp[comp] += t_params[1+k+self.p+(comp*(self.q+self.p))]*scores_exp[-k-1]\n\n            if self.leverage is True:\n                new_value_comp[1] += t_params[-3]*np.sign(-(Y_exp[-1]-t_params[-1]))*(scores_exp[t-1]+1)\n\n            lmda_exp = np.append(lmda_exp,[new_value_comp.sum()+t_params[0]]) # For indexing consistency\n            lmda_c_exp[0] = np.append(lmda_c_exp[0],new_value_comp[0])\n            lmda_c_exp[1] = np.append(lmda_c_exp[1],new_value_comp[1])\n            scores_exp = np.append(scores_exp,[0]) # expectation of score is zero\n            Y_exp = np.append(Y_exp,[t_params[-1]])\n\n        return lmda_exp", "response": "This function creates a h - step ahead mean prediction for the current set of latent variables."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsimulate a h - step ahead mean prediction of a set of latent variables.", "response": "def _sim_prediction(self, lmda, lmda_c, Y, scores, h, t_params, simulations):\n        \"\"\" Simulates a h-step ahead mean prediction\n\n        Parameters\n        ----------\n        lmda : np.array\n            The combined volatility component\n\n        lmda_c : np.array\n            The two volatility components\n\n        Y : np.array\n            The past data\n\n        scores : np.array\n            The past scores\n\n        h : int\n            How many steps ahead for the prediction\n\n        t_params : np.array\n            A vector of (transformed) latent variables\n\n        simulations : int\n            How many simulations to perform\n\n        Returns\n        ----------\n        Matrix of simulations\n        \"\"\"     \n\n        sim_vector = np.zeros([simulations,h])\n\n        for n in range(0,simulations):\n            # Create arrays to iteratre over\n            lmda_exp = lmda.copy()\n            lmda_c_exp = [lmda_c.T[0],lmda_c.T[1]] \n            scores_exp = scores.copy()\n            Y_exp = Y.copy()\n\n            # Loop over h time periods          \n            for t in range(0,h):\n                new_value_comp = np.zeros(2)\n                new_value = 0\n\n                for comp in range(2):\n\n                    if self.p != 0:\n                        for j in range(self.p):\n                            new_value_comp[comp] += t_params[1+j+(comp*(self.q+self.p))]*lmda_c_exp[comp][-j-1]\n\n                    if self.q != 0:\n                        for k in range(self.q):\n                            new_value_comp[comp] += t_params[1+k+self.p+(comp*(self.q+self.p))]*scores_exp[-k-1]\n\n                if self.leverage is True:\n                    new_value_comp[1] += t_params[-3]*np.sign(-(Y_exp[-1]-t_params[-1]))*(scores_exp[t-1]+1)\n\n                lmda_exp = np.append(lmda_exp,[new_value_comp.sum()+t_params[0]]) # For indexing consistency\n                lmda_c_exp[0] = np.append(lmda_c_exp[0],new_value_comp[0])\n                lmda_c_exp[1] = np.append(lmda_c_exp[1],new_value_comp[1])\n                rnd_no = np.random.randint(scores.shape[0])\n                scores_exp = np.append(scores_exp,scores[rnd_no]) # expectation of score is zero\n                Y_exp = np.append(Y_exp,Y[rnd_no])\n            sim_vector[n] = lmda_exp[-h:]\n\n        return np.transpose(sim_vector)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _sim_prediction_bayes(self, h, simulations):\n\n        sim_vector = np.zeros([simulations,h])\n\n        for n in range(0,simulations):\n\n            t_z = self.draw_latent_variables(nsims=1).T[0]\n            lmda, lmda_c, Y, scores = self._model(self.latent_variables.get_z_values())\n            t_z = np.array([self.latent_variables.z_list[k].prior.transform(t_z[k]) for k in range(t_z.shape[0])])\n\n            # Create arrays to iteratre over\n            lmda_exp = lmda.copy()\n            lmda_c_exp = [lmda_c.T[0],lmda_c.T[1]] \n            scores_exp = scores.copy()\n            Y_exp = Y.copy()\n\n            # Loop over h time periods          \n            for t in range(0,h):\n                new_value_comp = np.zeros(2)\n                new_value = 0\n\n                for comp in range(2):\n\n                    if self.p != 0:\n                        for j in range(self.p):\n                            new_value_comp[comp] += t_z[1+j+(comp*(self.q+self.p))]*lmda_c_exp[comp][-j-1]\n\n                    if self.q != 0:\n                        for k in range(self.q):\n                            new_value_comp[comp] += t_z[1+k+self.p+(comp*(self.q+self.p))]*scores_exp[-k-1]\n\n                if self.leverage is True:\n                    new_value_comp[1] += t_z[-3]*np.sign(-(Y_exp[-1]-t_z[-1]))*(scores_exp[t-1]+1)\n\n                lmda_exp = np.append(lmda_exp,[new_value_comp.sum()+t_z[0]]) # For indexing consistency\n                lmda_c_exp[0] = np.append(lmda_c_exp[0],new_value_comp[0])\n                lmda_c_exp[1] = np.append(lmda_c_exp[1],new_value_comp[1])\n                rnd_no = np.random.randint(scores.shape[0])\n                scores_exp = np.append(scores_exp,scores[rnd_no]) # expectation of score is zero\n                Y_exp = np.append(Y_exp,Y[rnd_no])\n            sim_vector[n] = lmda_exp[-h:]\n\n        return np.transpose(sim_vector)", "response": "Simulates a h - step ahead mean prediction for a set of simulations."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _uncythonized_mb_model(self, beta, mini_batch):\n\n        rand_int =  np.random.randint(low=0, high=self.data.shape[0]-mini_batch-self.max_lag+1)\n        sample = np.arange(start=rand_int, stop=rand_int+mini_batch)\n\n        Y = self.model_Y[sample]\n\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n        theta = np.zeros(Y.shape[0])\n        theta_t = np.zeros(Y.shape[0])\n        model_scale, model_shape, model_skewness = self._get_scale_and_shape(parm)\n\n        # Loop over time series\n        theta, self.model_scores = gas_llt_recursion(parm, theta, theta_t, self.model_scores, Y, Y.shape[0], \n            self.family.score_function, self.link, model_scale, model_shape, model_skewness, self.max_lag)\n\n        return theta, theta_t, Y, self.model_scores", "response": "Creates the structure of the model for the unythonized M - Bayesian Model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _mean_prediction(self, theta, theta_t, Y, scores, h, t_params):\n\n        Y_exp = Y.copy()\n        theta_exp = theta.copy()\n        theta_t_exp = theta_t.copy()\n        scores_exp = scores.copy()\n\n        #(TODO: vectorize the inner construction here)   \n        for t in range(0,h):\n            new_value1 = theta_t_exp[-1] + theta_exp[-1] + t_params[0]*scores_exp[-1]\n            new_value2 = theta_t_exp[-1] + t_params[1]*scores_exp[-1]\n            if self.model_name2 == \"Exponential\":\n                Y_exp = np.append(Y_exp, [1.0/self.link(new_value1)])\n            else:\n                Y_exp = np.append(Y_exp, [self.link(new_value1)])\n            theta_exp = np.append(theta_exp, [new_value1]) # For indexing consistency\n            theta_t_exp = np.append(theta_t_exp, [new_value2])\n            scores_exp = np.append(scores_exp, [0]) # expectation of score is zero\n        return Y_exp", "response": "Creates a h - step ahead mean prediction for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _sim_prediction(self, theta, theta_t, Y, scores, h, t_params, simulations):\n\n        model_scale, model_shape, model_skewness = self._get_scale_and_shape(t_params)\n\n        sim_vector = np.zeros([simulations,h])\n\n        for n in range(0,simulations):\n\n            Y_exp = Y.copy()\n            theta_exp = theta.copy()\n            theta_t_exp = theta_t.copy()\n            scores_exp = scores.copy()\n\n            #(TODO: vectorize the inner construction here)      \n            for t in range(0,h):\n                new_value1 = theta_t_exp[-1] + theta_exp[-1] + t_params[0]*scores_exp[-1]\n                new_value2 = theta_t_exp[-1] + t_params[1]*scores_exp[-1]\n\n                if self.model_name2 == \"Exponential\":\n                    rnd_value = self.family.draw_variable(1.0/self.link(new_value1),model_scale,model_shape,model_skewness,1)[0]\n                else:\n                    rnd_value = self.family.draw_variable(self.link(new_value1),model_scale,model_shape,model_skewness,1)[0]\n\n                Y_exp = np.append(Y_exp,[rnd_value])\n                theta_exp = np.append(theta_exp,[new_value1]) # For indexing consistency\n                theta_t_exp = np.append(theta_t_exp,[new_value2])\n                scores_exp = np.append(scores_exp,scores[np.random.randint(scores.shape[0])]) # expectation of score is zero\n\n            sim_vector[n] = Y_exp[-h:]\n\n        return np.transpose(sim_vector)", "response": "Simulates a h - step ahead mean prediction of a set of latent variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _sim_prediction_bayes(self, h, simulations):\n\n        sim_vector = np.zeros([simulations,h])\n\n        for n in range(0,simulations):\n\n            t_z = self.draw_latent_variables(nsims=1).T[0]\n            theta, theta_t, Y, scores = self._model(t_z)\n            t_z = np.array([self.latent_variables.z_list[k].prior.transform(t_z[k]) for k in range(t_z.shape[0])])\n\n            model_scale, model_shape, model_skewness = self._get_scale_and_shape(t_z)\n\n            Y_exp = Y.copy()\n            theta_exp = theta.copy()\n            theta_t_exp = theta_t.copy()\n            scores_exp = scores.copy()\n\n            #(TODO: vectorize the inner construction here)      \n            for t in range(0,h):\n                new_value1 = theta_t_exp[-1] + theta_exp[-1] + t_z[0]*scores_exp[-1]\n                new_value2 = theta_t_exp[-1] + t_z[1]*scores_exp[-1]\n\n                if self.model_name2 == \"Exponential\":\n                    rnd_value = self.family.draw_variable(1.0/self.link(new_value1),model_scale,model_shape,model_skewness,1)[0]\n                else:\n                    rnd_value = self.family.draw_variable(self.link(new_value1),model_scale,model_shape,model_skewness,1)[0]\n\n                Y_exp = np.append(Y_exp,[rnd_value])\n                theta_exp = np.append(theta_exp,[new_value1]) # For indexing consistency\n                theta_t_exp = np.append(theta_t_exp,[new_value2])\n                scores_exp = np.append(scores_exp,scores[np.random.randint(scores.shape[0])]) # expectation of score is zero\n\n            sim_vector[n] = Y_exp[-h:]\n\n        return np.transpose(sim_vector)", "response": "Simulates a h - step ahead mean prediction for a set of simulations."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef approximating_model(self, beta, T, Z, R, Q, h_approx, data):\n\n        H = np.ones(data.shape[0])\n        mu = np.zeros(data.shape[0])\n\n        alpha = np.array([np.zeros(data.shape[0])])\n        tol = 100.0\n        it = 0\n        while tol > 10**-7 and it < 5:\n            old_alpha = alpha[0]\n            alpha, V = nl_univariate_KFS(data,Z,H,T,Q,R,mu)\n            H = np.exp(-alpha[0])\n            mu = data - alpha[0] - np.exp(-alpha[0])*(data - np.exp(alpha[0]))\n            tol = np.mean(np.abs(alpha[0]-old_alpha))\n            it += 1\n\n        return H, mu", "response": "Creates approximating Gaussian state space model for Poisson measurement density."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef approximating_model_reg(self, beta, T, Z, R, Q, h_approx, data, X, state_no):\n        \n        H = np.ones(data.shape[0])\n        mu = np.zeros(data.shape[0])\n\n        alpha = np.zeros([state_no, data.shape[0]])\n        tol = 100.0\n        it = 0\n        while tol > 10**-7 and it < 5:\n            old_alpha = np.sum(X*alpha.T,axis=1)\n            alpha, V = nld_univariate_KFS(data,Z,H,T,Q,R,mu)\n            H = np.exp(-np.sum(X*alpha.T,axis=1))\n            mu = data - np.sum(X*alpha.T,axis=1) - np.exp(-np.sum(X*alpha.T,axis=1))*(data - np.exp(np.sum(X*alpha.T,axis=1)))\n            tol = np.mean(np.abs(np.sum(X*alpha.T,axis=1)-old_alpha))\n            it += 1\n\n        return H, mu", "response": "Creates approximating Gaussian model for Poisson measurement density"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlogs PDF for Poisson prior at a given Latent variable mu.", "response": "def logpdf(self, mu):\n        \"\"\"\n        Log PDF for Poisson prior\n\n        Parameters\n        ----------\n        mu : float\n            Latent variable for which the prior is being formed over\n\n        Returns\n        ----------\n        - log(p(mu))\n        \"\"\"\n        if self.transform is not None:\n            mu = self.transform(mu)    \n        return ss.poisson.logpmf(mu, self.lmd0)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef markov_blanket(y, mean, scale, shape, skewness):\n        return ss.poisson.logpmf(y, mean)", "response": "Returns the Markov blanket of the Poisson distribution with the given univariate time series y and mean."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the attributes of this family and all of its children.", "response": "def setup():\n        \"\"\" Returns the attributes of this family\n\n        Notes\n        ----------\n        - scale notes whether family has a variance parameter (sigma)\n        - shape notes whether family has a tail thickness parameter (nu)\n        - skewness notes whether family has a skewness parameter (gamma)\n        - mean_transform is a function which transforms the location parameter\n        - cythonized notes whether the family has cythonized routines\n        \n        Returns\n        ----------\n        - model name, link function, scale, shape, skewness, mean_transform, cythonized\n        \"\"\"\n        name = \"Poisson\"\n        link = np.exp\n        scale = False\n        shape = False\n        skewness = False\n        mean_transform = np.log\n        cythonized = True\n        return name, link, scale, shape, skewness, mean_transform, cythonized"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nnegatives loglikelihood function for the Poisson distribution", "response": "def neg_loglikelihood(y, mean, scale, shape, skewness):\n        \"\"\" Negative loglikelihood function\n\n        Parameters\n        ----------\n        y : np.ndarray\n            univariate time series\n\n        mean : np.ndarray\n            array of location parameters for the Poisson distribution\n\n        scale : float\n            scale parameter for the Poisson distribution\n\n        shape : float\n            tail thickness parameter for the Poisson distribution\n\n        skewness : float\n            skewness parameter for the Poisson distribution\n\n        Returns\n        ----------\n        - Negative loglikelihood of the Poisson family\n        \"\"\"\n        return -np.sum(-mean + np.log(mean)*y - sp.gammaln(y + 1))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pdf(self, mu):\n        if self.transform is not None:\n            mu = self.transform(mu)                \n        return ss.poisson.pmf(mu, self.lmd0)", "response": "PDF for Poisson prior returning the prior in the logarithm of the prior."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngas Poisson Regression Update term using gradient only native Python function", "response": "def reg_score_function(X, y, mean, scale, shape, skewness):\n        \"\"\" GAS Poisson Regression Update term using gradient only - native Python function\n\n        Parameters\n        ----------\n        X : float\n            datapoint for the right hand side variable\n    \n        y : float\n            datapoint for the time series\n\n        mean : float\n            location parameter for the Poisson distribution\n\n        scale : float\n            scale parameter for the Poisson distribution\n\n        shape : float\n            tail thickness parameter for the Poisson distribution\n\n        skewness : float\n            skewness parameter for the Poisson distribution\n\n        Returns\n        ----------\n        - Score of the Poisson family\n        \"\"\"\n        return X*(y-mean)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngas Poisson Update term potentially using second - order information.", "response": "def second_order_score(y, mean, scale, shape, skewness):\n        \"\"\" GAS Poisson Update term potentially using second-order information - native Python function\n\n        Parameters\n        ----------\n        y : float\n            datapoint for the time series\n\n        mean : float\n            location parameter for the Poisson distribution\n\n        scale : float\n            scale parameter for the Poisson distribution\n\n        shape : float\n            tail thickness parameter for the Poisson distribution\n\n        skewness : float\n            skewness parameter for the Poisson distribution\n\n        Returns\n        ----------\n        - Adjusted score of the Poisson family\n        \"\"\"\n        return (y-mean)/float(mean)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef logpdf(self, x):\n        if self.transform is not None:\n            x = self.transform(x)       \n        return (-self.alpha-1)*np.log(x) - (self.beta/float(x))", "response": "Log PDF for Inverse Gamma prior."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pdf(self, x):\n        if self.transform is not None:\n            x = self.transform(x)               \n        return (x**(-self.alpha-1))*np.exp(-(self.beta/float(x)))", "response": "PDF for Inverse Gamma prior\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef draw_variable(loc, scale, shape, skewness, nsims):\n        return ss.cauchy.rvs(loc, scale, nsims)", "response": "Draw random variables from this distribution and returns the result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlogs PDF for Cauchy prior at a given Latent variable mu.", "response": "def logpdf(self, mu):\n        \"\"\"\n        Log PDF for Cauchy prior\n\n        Parameters\n        ----------\n        mu : float\n            Latent variable for which the prior is being formed over\n\n        Returns\n        ----------\n        - log(p(mu))\n        \"\"\"\n        if self.transform is not None:\n            mu = self.transform(mu)     \n        return ss.cauchy.logpdf(mu, self.loc0, self.scale0)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pdf(self, mu):\n        return ss.cauchy.pdf(mu, self.loc0, self.scale0)", "response": "PDF for Cauchy prior\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the Markov blanket of each likelihood term in the state space.", "response": "def markov_blanket(y, mean, scale, shape, skewness):\n        \"\"\" Markov blanket for each likelihood term - used for state space models\n\n        Parameters\n        ----------\n        y : np.ndarray\n            univariate time series\n\n        mean : np.ndarray\n            array of location parameters for the Cauchy distribution\n\n        scale : float\n            scale parameter for the Cauchy distribution\n\n        shape : float\n            tail thickness parameter for the Cauchy distribution\n\n        skewness : float\n            skewness parameter for the Cauchy distribution\n\n        Returns\n        ----------\n        - Markov blanket of the Cauchy family\n        \"\"\"\n        return ss.cauchy.logpdf(y, loc=mean, scale=scale)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets up the attributes of this family if using in a probabilistic model.", "response": "def setup():\n        \"\"\" Returns the attributes of this family if using in a probabilistic model\n\n        Notes\n        ----------\n        - scale notes whether family has a variance parameter (sigma)\n        - shape notes whether family has a tail thickness parameter (nu)\n        - skewness notes whether family has a skewness parameter (gamma)\n        - mean_transform is a function which transforms the location parameter\n        - cythonized notes whether the family has cythonized routines\n        \n        Returns\n        ----------\n        - model name, link function, scale, shape, skewness, mean_transform, cythonized\n        \"\"\"\n        name = \"Cauchy\"\n        link = np.array\n        scale = True\n        shape = False\n        skewness = False\n        mean_transform = np.array\n        cythonized = True # used for GAS models\n        return name, link, scale, shape, skewness, mean_transform, cythonized"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef neg_loglikelihood(y, mean, scale, shape, skewness):\n        return -np.sum(ss.cauchy.logpdf(y, loc=mean, scale=scale))", "response": "Negative loglikelihood function for this distribution"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngas Cauchy Regression Update term using native Python function", "response": "def reg_score_function(X, y, mean, scale, shape, skewness):\n        \"\"\" GAS Cauchy Regression Update term using gradient only - native Python function\n\n        Parameters\n        ----------\n        X : float\n            datapoint for the right hand side variable\n    \n        y : float\n            datapoint for the time series\n\n        mean : float\n            location parameter for the Cauchy distribution\n\n        scale : float\n            scale parameter for the Cauchy distribution\n\n        shape : float\n            tail thickness parameter for the Cauchy distribution\n\n        skewness : float\n            skewness parameter for the Cauchy distribution\n\n        Returns\n        ----------\n        - Score of the Cauchy family\n        \"\"\"\n        return 2.0*((y-mean)*X)/(np.power(scale,2)+np.power((y-mean),2))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_latent_variables(self):\n\n        self.latent_variables.add_z('Vol Constant', fam.Normal(0,3,transform=None), fam.Normal(0,3))\n\n        for p_term in range(self.p):\n            self.latent_variables.add_z('p(' + str(p_term+1) + ')', fam.Normal(0,0.5,transform='logit'), fam.Normal(0,3))\n            if p_term == 0:\n                self.latent_variables.z_list[-1].start = 3.00\n            else:\n                self.latent_variables.z_list[-1].start = -4.00\n\n        for q_term in range(self.q):\n            self.latent_variables.add_z('q(' + str(q_term+1) + ')', fam.Normal(0,0.5,transform='logit'), fam.Normal(0,3))\n            if q_term == 0:\n                self.latent_variables.z_list[-1].start = -1.50  \n            else: \n                self.latent_variables.z_list[-1].start = -4.00  \n\n        self.latent_variables.add_z('v', fam.Flat(transform='exp'), fam.Normal(0,3))\n        self.latent_variables.add_z('Returns Constant', fam.Normal(0,3,transform=None), fam.Normal(0,3))\n        self.latent_variables.add_z('GARCH-M', fam.Normal(0,3,transform=None), fam.Normal(0,3))\n\n        # Starting values\n        self.latent_variables.z_list[-3].start = 2.0", "response": "Creates latent variables for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _sim_prediction(self, lmda, Y, scores, h, t_params, simulations):\n\n        sim_vector = np.zeros([simulations,h])\n\n        for n in range(0,simulations):\n            # Create arrays to iteratre over        \n            lmda_exp = lmda.copy()\n            scores_exp = scores.copy()\n            Y_exp = Y.copy()\n\n            # Loop over h time periods          \n            for t in range(0,h):\n                new_value = t_params[0]\n\n                if self.p != 0:\n                    for j in range(1,self.p+1):\n                        new_value += t_params[j]*lmda_exp[-j]\n\n                if self.q != 0:\n                    for k in range(1,self.q+1):\n                        new_value += t_params[k+self.p]*scores_exp[-k]\n\n                if self.leverage is True:\n                    new_value += t_params[1+self.p+self.q]*np.sign(-(Y_exp[-1]-t_params[-2]-t_params[-1]*np.exp(lmda_exp[-1]/2.0)))*(scores_exp[-1]+1)\n\n                lmda_exp = np.append(lmda_exp,[new_value]) # For indexing consistency\n                scores_exp = np.append(scores_exp,scores[np.random.randint(scores.shape[0])]) # expectation of score is zero\n                Y_exp = np.append(Y_exp,Y[np.random.randint(Y.shape[0])]) # bootstrap returns\n\n            sim_vector[n] = lmda_exp[-h:]\n\n        return np.transpose(sim_vector)", "response": "Simulates a h - step ahead mean prediction of a set of latent variables."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsimulate a h - step ahead mean prediction for a set of simulations.", "response": "def _sim_prediction_bayes(self, h, simulations):\n        \"\"\" Simulates a h-step ahead mean prediction\n\n        Parameters\n        ----------\n        h : int\n            How many steps ahead for the prediction\n\n        simulations : int\n            How many simulations to perform\n\n        Returns\n        ----------\n        Matrix of simulations\n        \"\"\"     \n\n        sim_vector = np.zeros([simulations,h])\n\n        for n in range(0,simulations):\n\n            t_z = self.draw_latent_variables(nsims=1).T[0]\n            lmda, Y, scores = self._model(t_z)\n            t_z = np.array([self.latent_variables.z_list[k].prior.transform(t_z[k]) for k in range(t_z.shape[0])])\n\n            # Create arrays to iteratre over        \n            lmda_exp = lmda.copy()\n            scores_exp = scores.copy()\n            Y_exp = Y.copy()\n\n            # Loop over h time periods          \n            for t in range(0,h):\n                new_value = t_z[0]\n\n                if self.p != 0:\n                    for j in range(1,self.p+1):\n                        new_value += t_z[j]*lmda_exp[-j]\n\n                if self.q != 0:\n                    for k in range(1,self.q+1):\n                        new_value += t_z[k+self.p]*scores_exp[-k]\n\n                if self.leverage is True:\n                    new_value += t_z[1+self.p+self.q]*np.sign(-(Y_exp[-1]-t_z[-2]-t_z[-1]*np.exp(lmda_exp[-1]/2.0)))*(scores_exp[-1]+1)\n\n                lmda_exp = np.append(lmda_exp,[new_value]) # For indexing consistency\n                scores_exp = np.append(scores_exp,scores[np.random.randint(scores.shape[0])]) # expectation of score is zero\n                Y_exp = np.append(Y_exp,Y[np.random.randint(Y.shape[0])]) # bootstrap returns\n\n            sim_vector[n] = lmda_exp[-h:]\n\n        return np.transpose(sim_vector)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates the negative log - likelihood of the model for the given set of latent variables.", "response": "def neg_loglik(self, beta):\n        \"\"\" Creates the negative log-likelihood of the model\n\n        Parameters\n        ----------\n        beta : np.array\n            Contains untransformed starting values for latent variables\n\n        Returns\n        ----------\n        The negative logliklihood of the model\n        \"\"\"     \n\n        lmda, Y, ___ = self._model(beta)\n        loc = np.ones(lmda.shape[0])*self.latent_variables.z_list[-2].prior.transform(beta[-2]) + self.latent_variables.z_list[-1].prior.transform(beta[-1])*np.exp(lmda/2.0)\n        return -np.sum(ss.t.logpdf(x=Y,\n            df=self.latent_variables.z_list[-3].prior.transform(beta[-3]),\n            loc=loc,scale=np.exp(lmda/2.0)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating the model s latent variables. Returns None if the model attributes are not changed.", "response": "def _create_latent_variables(self):\n        \"\"\" Creates the model's latent variables\n\n        Returns\n        ----------\n        None (changes model attributes)\n        \"\"\"\n\n        # Input layer\n        for unit in range(self.units):\n            self.latent_variables.add_z('Constant | Layer ' + str(1) + ' | Unit ' + str(unit+1), fam.Cauchy(0,1,transform=None), fam.Normal(0, 3))\n\n            for ar_term in range(self.ar):\n                self.latent_variables.add_z('AR' + str(ar_term+1) + ' | Layer ' + str(1) + ' | Unit ' + str(unit+1), fam.Cauchy(0,1,transform=None), fam.Normal(0, 3))\n\n            for z in range(len(self.X_names)):\n                self.latent_variables.add_z('Weight ' + self.X_names[z], fam.Cauchy(0, 1, transform=None), fam.Normal(0, 3))\n\n        # Hidden layers\n        for layer in range(1, self.layers):\n            for unit in range(self.units):\n                for weight in range(self.units):\n                    self.latent_variables.add_z('Weight ' + str(weight+1) + ' | Layer ' + str(layer+1) + ' | Unit ' + str(unit+1), fam.Cauchy(0,1,transform=None), fam.Normal(0, 3))\n\n        # Output layer\n        for weight in range(self.units):\n            self.latent_variables.add_z('Output Weight ' + str(weight+1), fam.Cauchy(0,1,transform=None), fam.Normal(0, 3))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate the structure of the model", "response": "def _model(self, beta):\n        \"\"\" Creates the structure of the model (model matrices etc)\n\n        Parameters\n        ----------\n        beta : np.ndarray\n            Contains untransformed starting values for the latent variables\n\n        Returns\n        ----------\n        mu : np.ndarray\n            Contains the predicted values (location) for the time series\n\n        Y : np.ndarray\n            Contains the length-adjusted time series (accounting for lags)\n        \"\"\"     \n\n\n        Y = np.array(self.data[self.max_lag:])\n\n        # Transform latent variables\n        z = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n\n        return neural_network_tanh(Y, self.X, z, self.units, self.layers, self.ar+len(self.X_names)), Y"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _mb_model(self, beta, mini_batch):\n\n        Y = np.array(self.data[self.max_lag:])\n\n        sample = np.random.choice(len(Y), mini_batch, replace=False)\n\n        Y = Y[sample]\n        X = self.X[:, sample]\n\n        # Transform latent variables\n        z = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n\n        return neural_network_tanh_mb(Y, X, z, self.units, self.layers, self.ar+len(self.X_names)), Y", "response": "Creates the structure of the model for mini batch model"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _mean_prediction(self, mu, Y, h, t_z):\n\n        # Create arrays to iteratre over\n        Y_exp = Y.copy()\n\n        # Loop over h time periods          \n        for t in range(0,h):\n\n            if self.ar != 0:\n                Y_exp_normalized = (Y_exp[-self.ar:][::-1] - self._norm_mean) / self._norm_std\n                new_value = self.predict_new(np.append(1.0, Y_exp_normalized), self.latent_variables.get_z_values())\n\n            else:  \n                new_value = self.predict_new(np.array([1.0]), self.latent_variables.get_z_values())\n\n            Y_exp = np.append(Y_exp, [self.link(new_value)])\n\n        return Y_exp", "response": "Creates a h - step ahead mean prediction for the current set of latent variables."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsimulate a h - step ahead mean prediction of the current state of the object.", "response": "def _sim_prediction(self, mu, Y, h, t_z, simulations):\n        \"\"\" Simulates a h-step ahead mean prediction\n\n        Parameters\n        ----------\n        mu : np.ndarray\n            The past predicted values\n\n        Y : np.ndarray\n            The past data\n\n        h : int\n            How many steps ahead for the prediction\n\n        t_z : np.ndarray\n            A vector of (transformed) latent variables\n\n        simulations : int\n            How many simulations to perform\n\n        Returns\n        ----------\n        Matrix of simulations\n        \"\"\"     \n\n        model_scale, model_shape, model_skewness = self._get_scale_and_shape(t_z)\n        sim_vector = np.zeros([simulations,h])\n\n        for n in range(0, simulations):\n            # Create arrays to iteratre over        \n            Y_exp = Y.copy()\n\n            # Loop over h time periods          \n            for t in range(0,h):\n\n                if self.ar != 0:\n                    Y_exp_normalized = (Y_exp[-self.ar:][::-1] - self._norm_mean) / self._norm_std\n                    new_value = self.predict_new(np.append(1.0, Y_exp_normalized), self.latent_variables.get_z_values())\n\n                else:\n                    new_value = self.predict_new(np.array([1.0]), self.latent_variables.get_z_values())\n\n                new_value += np.random.randn(1)*t_z[-1]\n\n                if self.model_name2 == \"Exponential\":\n                    rnd_value = self.family.draw_variable(1.0/self.link(new_value), model_scale, model_shape, model_skewness, 1)[0]\n                else:\n                    rnd_value = self.family.draw_variable(self.link(new_value), model_scale, model_shape, model_skewness, 1)[0]\n\n                Y_exp = np.append(Y_exp, [rnd_value])\n\n            sim_vector[n] = Y_exp[-h:]\n\n        return np.transpose(sim_vector)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef general_neg_loglik(self, beta):\n\n        mu, Y = self._model(beta)\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n        #TODO: Replace above with transformation that only acts on scale, shape, skewness in future (speed-up)\n        model_scale, model_shape, model_skewness = self._get_scale_and_shape(parm)\n        return self.family.neg_loglikelihood(Y, self.link(mu), model_scale, model_shape, model_skewness)", "response": "Calculates the negative log - likelihood of the model with respect to the latent variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots the fit of the model against the data", "response": "def plot_fit(self, **kwargs):\n        \"\"\" \n        Plots the fit of the model against the data\n        \"\"\"\n\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n        plt.figure(figsize=figsize)\n        date_index = self.index[self.ar:self.data.shape[0]]\n        mu, Y = self._model(self.latent_variables.get_z_values())\n        plt.plot(date_index,Y,label='Data')\n        plt.plot(date_index,mu,label='Filter',c='black')\n        plt.title(self.data_name)\n        plt.legend(loc=2)   \n        plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nplotting the forecasts with the estimated model and the estimated model.", "response": "def plot_predict(self, h=5, past_values=20, intervals=True, **kwargs):\n        \"\"\" Plots forecasts with the estimated model\n\n        Parameters\n        ----------\n        h : int (default : 5)\n            How many steps ahead would you like to forecast?\n\n        past_values : int (default : 20)\n            How many past observations to show on the forecast graph?\n\n        intervals : boolean\n            Would you like to show prediction intervals for the forecast?\n\n        Returns\n        ----------\n        - Plot of the forecast\n        \"\"\"     \n\n        figsize = kwargs.get('figsize',(10,7))\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n\n            import matplotlib.pyplot as plt\n            import seaborn as sns\n\n            # Retrieve data, dates and (transformed) latent variables\n            mu, Y = self._model(self.latent_variables.get_z_values())         \n            date_index = self.shift_dates(h)\n            t_z = self.transform_z()\n\n            # Get mean prediction and simulations (for errors)\n            mean_values = self._mean_prediction(mu, Y, h, t_z)\n            if intervals is True:\n                sim_values = self._sim_prediction(mu, Y, h, t_z, 15000)\n            else:\n                sim_values = self._sim_prediction(mu, Y, h, t_z, 2)\n            error_bars, forecasted_values, plot_values, plot_index = self._summarize_simulations(mean_values, sim_values, date_index, h, past_values)\n\n            plt.figure(figsize=figsize)\n            if intervals is True:\n                alpha =[0.15*i/float(100) for i in range(50,12,-2)]\n                for count, pre in enumerate(error_bars):\n                    plt.fill_between(date_index[-h-1:], forecasted_values-pre, forecasted_values+pre,alpha=alpha[count])            \n            plt.plot(plot_index,plot_values)\n            plt.title(\"Forecast for \" + self.data_name)\n            plt.xlabel(\"Time\")\n            plt.ylabel(self.data_name)\n            plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef predict_is(self, h=5, fit_once=True, fit_method='MLE', intervals=False, **kwargs):\n        predictions = []\n\n        for t in range(0,h):\n            x = NNAR(ar=self.ar, units=self.units, \n                layers=self.layers, data=self.data_original[:-h+t], family=self.family)\n            if fit_once is False:\n                x.fit(method=fit_method, printer=False)\n            if t == 0:\n                if fit_once is True:\n                    x.fit(method=fit_method, printer=False)\n                    saved_lvs = x.latent_variables\n                predictions = x.predict(1, intervals=intervals)\n            else:\n                if fit_once is True:\n                    x.latent_variables = saved_lvs\n                predictions = pd.concat([predictions,x.predict(1, intervals=intervals)])\n        \n        if intervals is True:\n            predictions.rename(columns={0:self.data_name, 1: \"1% Prediction Interval\", \n                2: \"5% Prediction Interval\", 3: \"95% Prediction Interval\", 4: \"99% Prediction Interval\"}, inplace=True)\n        else:\n            predictions.rename(columns={0:self.data_name}, inplace=True)\n\n        predictions.index = self.index[-h:]\n\n        return predictions", "response": "Predicts the in - sample data for the in - sample data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplot the ELBO progress", "response": "def plot_elbo(self, figsize=(15,7)):\n        \"\"\"\n        Plots the ELBO progress (if present)\n        \"\"\"\n        import matplotlib.pyplot as plt\n\n        plt.figure(figsize=figsize)\n        plt.plot(self.elbo_records)\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"ELBO\")\n        plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate likelihood markov blanket of the model", "response": "def likelihood_markov_blanket(self, beta):\n        \"\"\" Creates likelihood markov blanket of the model\n\n        Parameters\n        ----------\n        beta : np.array\n            Contains untransformed starting values for latent variables\n\n        Returns\n        ----------\n        - Negative loglikelihood\n        \"\"\"     \n        states = beta[self.z_no:self.z_no+self.data_length] # the local level (untransformed)\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(self.z_no)]) # transformed distribution parameters\n        scale, shape, skewness = self._get_scale_and_shape(parm)\n        return self.family.markov_blanket(self.data, self.link(states), scale, shape, skewness)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning likelihood of the states given the variance latent variables beta and alpha.", "response": "def state_likelihood(self, beta, alpha):\n        \"\"\" Returns likelihood of the states given the variance latent variables\n\n        Parameters\n        ----------\n        beta : np.array\n            Contains untransformed starting values for latent variables\n        alpha : np.array\n            State matrix\n        \n        Returns\n        ----------\n        State likelihood\n        \"\"\"\n        _, _, _, Q = self._ss_matrices(beta)\n        residuals = alpha[0][1:]-alpha[0][:-1]\n        return np.sum(ss.norm.logpdf(residuals, loc=0, scale=np.power(Q.ravel(),0.5)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nproduce animated plot of BBVI optimization", "response": "def _animate_bbvi(self,stored_latent_variables,stored_predictive_likelihood):\n        \"\"\" Produces animated plot of BBVI optimization\n\n        Returns\n        ----------\n        None (changes model attributes)\n        \"\"\"\n\n        from matplotlib.animation import FuncAnimation, writers\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n        \n        fig = plt.figure()\n        ax = fig.add_subplot(1, 1, 1)\n        ud = BBVINLLMAnimate(ax,self.data,stored_latent_variables,self.index,self.z_no,self.link)\n        anim = FuncAnimation(fig, ud, frames=np.arange(stored_latent_variables.shape[0]), init_func=ud.init,\n                interval=10, blit=True)\n        plt.plot(self.data)\n        plt.xlabel(\"Time\")\n        plt.ylabel(self.data_name)\n        plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_latent_variables(self):\n\n        self.latent_variables.add_z('Sigma^2 level', fam.Flat(transform='exp'), fam.Normal(0,3))", "response": "Creates latent variables for the current object"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninitialize the appoximate distibution for the model.", "response": "def initialize_approx_dist(self, phi, start_diffuse, gaussian_latents):\n        \"\"\" Initializes the appoximate distibution for the model\n\n        Parameters\n        ----------\n        phi : np.ndarray\n            Latent variables\n\n        start_diffuse: boolean\n            Whether to start from diffuse values or not\n        \n        gaussian_latents: LatentVariables object\n            Latent variables for the Gaussian approximation\n\n        Returns\n        ----------\n        BBVI fit object\n        \"\"\"     \n\n        # Starting values for approximate distribution\n        for i in range(len(self.latent_variables.z_list)):\n            approx_dist = self.latent_variables.z_list[i].q\n            if isinstance(approx_dist, fam.Normal):\n                self.latent_variables.z_list[i].q.mu0 = phi[i]\n                self.latent_variables.z_list[i].q.sigma0 = np.exp(-3.0)\n\n        q_list = [k.q for k in self.latent_variables.z_list]\n\n        # Get starting values for states\n        T, Z, R, Q = self._ss_matrices(phi)\n        H, mu = self.family.approximating_model(phi, T, Z, R, Q, gaussian_latents.get_z_values(transformed=True)[0], self.data)\n        a, V = self.smoothed_state(self.data, phi, H, mu)\n        V[0][0][0] = V[0][0][-1] \n\n        for item in range(self.data_length):\n            if start_diffuse is False:\n                q_list.append(fam.Normal(a[0][item], np.sqrt(np.abs(V[0][0][item]))))\n            else:\n                q_list.append(fam.Normal(self.family.itransform(np.mean(self.data)), np.sqrt(np.abs(V[0][0][item]))))\n\n        return q_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms a single Black Box Variational Inference of a single object.", "response": "def _bbvi_fit(self, optimizer='RMSProp', iterations=1000, print_progress=True,\n        start_diffuse=False, **kwargs):\n        \"\"\" Performs Black Box Variational Inference\n\n        Parameters\n        ----------\n        posterior : method\n            Hands bbvi_fit a posterior object\n\n        optimizer : string\n            Stochastic optimizer: either RMSProp or ADAM.\n\n        iterations: int\n            How many iterations to run\n\n        print_progress : bool\n            Whether tp print the ELBO progress or not\n        \n        start_diffuse : bool\n            Whether to start from diffuse values (if not: use approx Gaussian)\n\n        Returns\n        ----------\n        BBVIResults object\n        \"\"\"\n        if self.model_name2 in [\"t\", \"Skewt\"]:\n            default_learning_rate = 0.0001\n        else:\n            default_learning_rate = 0.001\n\n        animate = kwargs.get('animate', False)\n        batch_size = kwargs.get('batch_size', 24) \n        learning_rate = kwargs.get('learning_rate', default_learning_rate) \n        record_elbo = kwargs.get('record_elbo', False) \n\n        # Starting values\n        gaussian_latents = self._preoptimize_model() # find parameters for Gaussian model\n        phi = self.latent_variables.get_z_starting_values()\n        q_list = self.initialize_approx_dist(phi, start_diffuse, gaussian_latents)\n\n        # PERFORM BBVI\n        bbvi_obj = ifr.CBBVI(self.neg_logposterior, self.log_p_blanket, q_list, batch_size, \n            optimizer, iterations, learning_rate, record_elbo)\n\n        if print_progress is False:\n            bbvi_obj.printer = False\n\n        if animate is True:\n            q, q_params, q_ses, stored_z, stored_predictive_likelihood = bbvi_obj.run_and_store()\n            self._animate_bbvi(stored_z,stored_predictive_likelihood)\n        else:\n            q, q_params, q_ses, elbo_records = bbvi_obj.run()\n\n        self.latent_variables.set_z_values(q_params[:self.z_no],'BBVI',np.exp(q_ses[:self.z_no]),None)    \n\n        # STORE RESULTS\n        for k in range(len(self.latent_variables.z_list)):\n            self.latent_variables.z_list[k].q = q[k]\n\n        theta = q_params[self.z_no:]\n        Y = self.data\n        scores = None\n        states = q_params[self.z_no:]\n        X_names = None\n        states_ses = np.exp(q_ses[self.z_no:])\n\n        self.states = states\n        self.states_ses = states_ses\n\n        return res.BBVISSResults(data_name=self.data_name,X_names=X_names,model_name=self.model_name,\n            model_type=self.model_type, latent_variables=self.latent_variables,data=Y,index=self.index,\n            multivariate_model=self.multivariate_model,objective=self.neg_logposterior(q_params), \n            method='BBVI',ses=q_ses[:self.z_no],signal=theta,scores=scores,elbo_records=elbo_records,\n            z_hide=self._z_hide,max_lag=self.max_lag,states=states,states_var=np.power(states_ses,2))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_predict(self,h=5,past_values=20,intervals=True,**kwargs):      \n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            # Retrieve data, dates and (transformed) latent variables\n            scale, shape, skewness = self._get_scale_and_shape(self.latent_variables.get_z_values(transformed=True))\n            previous_value = self.data[-1]  \n            forecasted_values = np.ones(h)*self.states[-1]  \n            date_index = self.shift_dates(h)\n            simulations = 10000\n            sim_vector = np.zeros([simulations,h])\n            t_params = self.transform_z()\n\n            for n in range(0,simulations):  \n                rnd_q = np.random.normal(0,np.sqrt(self.latent_variables.get_z_values(transformed=True)[0]),h) \n                exp = forecasted_values.copy()\n\n                for t in range(0,h):\n                    if t == 0:\n                        exp[t] = forecasted_values[t] + rnd_q[t]\n                    else:\n                        exp[t] = exp[t-1] + rnd_q[t]\n\n                sim_vector[n] = self.family.draw_variable(loc=self.link(exp),shape=shape,scale=scale,skewness=skewness,nsims=exp.shape[0])\n\n            sim_vector = np.transpose(sim_vector)\n\n            forecasted_values = self.link(forecasted_values)\n\n            if self.model_name2 == 'Skewt':\n                forecasted_values = forecasted_values + ((t_params[-3] - (1.0/t_params[-3]))*t_params[-2]*gas.SkewtScore.tv_variate_exp(t_params[-1]))\n\n            plt.figure(figsize=figsize) \n\n            if intervals == True:\n                plt.fill_between(date_index[-h-1:], np.insert([np.percentile(i,5) for i in sim_vector],0,previous_value), \n                    np.insert([np.percentile(i,95) for i in sim_vector],0,previous_value), alpha=0.2,label=\"95 C.I.\")   \n\n            plot_values = np.append(self.data[-past_values:],forecasted_values)\n            plot_index = date_index[-h-past_values:]\n\n            plt.plot(plot_index,plot_values,label=self.data_name)\n            plt.title(\"Forecast for \" + self.data_name)\n            plt.xlabel(\"Time\")\n            plt.ylabel(self.data_name)\n            plt.show()", "response": "Plots the predict method of the current model and returns a matplotlib. pyplot. pyplot."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot_fit(self,intervals=True,**kwargs):\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            date_index = copy.deepcopy(self.index)\n            date_index = date_index[self.integ:self.data_original.shape[0]+1]\n            t_params = self.transform_z()\n\n            if self.model_name2 == 'Skewt':\n                states_upper_95 = (self.states + 1.98*np.sqrt(self.states_ses)) + ((t_params[-3] - (1.0/t_params[-3]))*t_params[-2]*gas.SkewtScore.tv_variate_exp(t_params[-1]))\n                states_lower_95 = (self.states - 1.98*np.sqrt(self.states_ses)) + ((t_params[-3] - (1.0/t_params[-3]))*t_params[-2]*gas.SkewtScore.tv_variate_exp(t_params[-1])) \n                mean_states = self.states + ((t_params[-3] - (1.0/t_params[-3]))*t_params[-2]*gas.SkewtScore.tv_variate_exp(t_params[-1]))\n            else:\n                states_upper_95 = (self.states + 1.98*np.sqrt(self.states_ses)) \n                states_lower_95 = (self.states - 1.98*np.sqrt(self.states_ses))\n                mean_states = self.states\n\n            plt.figure(figsize=figsize) \n            \n            plt.subplot(2, 1, 1)\n            plt.title(self.data_name + \" Raw and Smoothed\") \n\n            if intervals == True:\n                alpha =[0.15*i/float(100) for i in range(50,12,-2)]\n                plt.fill_between(date_index, self.link(states_lower_95), self.link(states_upper_95), alpha=0.15,label='95% C.I.')   \n\n            plt.plot(date_index, self.data, label='Data')\n            plt.plot(date_index, self.link(mean_states), label='Smoothed', c='black')          \n            plt.legend(loc=2)\n            \n            plt.subplot(2, 1, 2)\n            plt.title(self.data_name + \" Local Level\")  \n\n            if intervals == True:\n                alpha =[0.15*i/float(100) for i in range(50,12,-2)]\n                plt.fill_between(date_index, self.link(states_lower_95), self.link(states_upper_95), alpha=0.15,label='95% C.I.')   \n\n            plt.plot(date_index, self.link(mean_states), label='Smoothed State')\n            plt.legend(loc=2)\n            plt.show()", "response": "Plots the fit of the model and the data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake a forecast with the estimated model and returns a pd. DataFrame with predictions", "response": "def predict(self, h=5):      \n        \"\"\" Makes forecast with the estimated model\n\n        Parameters\n        ----------\n        h : int (default : 5)\n            How many steps ahead would you like to forecast?\n\n        Returns\n        ----------\n        - pd.DataFrame with predictions\n        \"\"\"     \n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            # Retrieve data, dates and (transformed) latent variables        \n            date_index = self.shift_dates(h)\n            forecasted_values = np.ones(h)*self.states[-1]\n            t_params = self.transform_z()\n\n            if self.model_name2 == 'Skewt':\n                forecasted_values = forecasted_values + ((t_params[-3] - (1.0/t_params[-3]))*t_params[-2]*gas.SkewtScore.tv_variate_exp(t_params[-1]))\n\n            result = pd.DataFrame(self.link(forecasted_values))\n            result.rename(columns={0:self.data_name}, inplace=True)\n            result.index = date_index[-h:]\n\n            return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes dynamic in - sample predictions with the estimated model", "response": "def predict_is(self, h=5, fit_once=True):\n        \"\"\" Makes dynamic in-sample predictions with the estimated model\n\n        Parameters\n        ----------\n        h : int (default : 5)\n            How many steps would you like to forecast?\n\n        fit_once : boolean\n            (default: True) Fits only once before the in-sample prediction; if False, fits after every new datapoint\n            This method is not functional currently for this model\n\n        Returns\n        ----------\n        - pd.DataFrame with predicted values\n        \"\"\"     \n\n        predictions = []\n\n        for t in range(0,h):\n            x = NLLEV(family=self.family, integ=self.integ, data=self.data_original[:(-h+t)])\n            x.fit(print_progress=False)\n            if t == 0:\n                predictions = x.predict(h=1)\n            else:\n                predictions = pd.concat([predictions,x.predict(h=1)])\n\n        predictions.rename(columns={0:self.data_name}, inplace=True)\n        predictions.index = self.index[-h:]\n\n        return predictions"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlogs PDF for t prior mu", "response": "def logpdf(self, mu):\n        \"\"\"\n        Log PDF for t prior\n\n        Parameters\n        ----------\n        mu : float\n            Latent variable for which the prior is being formed over\n\n        Returns\n        ----------\n        - log(p(mu))\n        \"\"\"\n        if self.transform is not None:\n            mu = self.transform(mu)    \n        return ss.t.logpdf(mu, df=self.df0, loc=self.loc0, scale=self.scale0)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef markov_blanket(y, mean, scale, shape, skewness):\n        return ss.t.logpdf(x=y, df=shape, loc=mean, scale=scale)", "response": "Returns the Markov blanket of the Poisson distribution with the specified univariate time series y and mean."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef neg_loglikelihood(y, mean, scale, shape, skewness):\n        return -np.sum(ss.t.logpdf(x=y, df=shape, loc=mean, scale=scale))", "response": "Negative loglikelihood of a univariate time series with the specified mean and scale and shape."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef second_order_score(y, mean, scale, shape, skewness):\n        return ((shape+1)/shape)*(y-mean)/(np.power(scale,2) + (np.power(y-mean,2)/shape))/((shape+1)*((np.power(scale,2)*shape) - np.power(y-mean,2))/np.power((np.power(scale,2)*shape) + np.power(y-mean,2),2))", "response": "GAS t Update term potentially using second - order information - native Python function"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload the hashes dict from the hashfile", "response": "def load_hashes(filename):\n    \"\"\"Load the hashes dict from the hashfile\"\"\"\n    # { filename : (sha1 of header if available or 'NA',\n    #               sha1 of input,\n    #               sha1 of output) }\n\n    hashes = {}\n    try:\n        with open(filename, 'r') as cython_hash_file:\n            for hash_record in cython_hash_file:\n                (filename, header_hash,\n                 cython_hash, gen_file_hash) = hash_record.split()\n                hashes[filename] = (header_hash, cython_hash, gen_file_hash)\n    except (KeyError, ValueError, AttributeError, IOError):\n        hashes = {}\n    return hashes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving the hashes dict to the hashfile", "response": "def save_hashes(hashes, filename):\n    \"\"\"Save the hashes dict to the hashfile\"\"\"\n    with open(filename, 'w') as cython_hash_file:\n        for key, value in hashes.items():\n            cython_hash_file.write(\"%s %s %s %s\\n\"\n                                   % (key, value[0], value[1], value[2]))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_hash_tuple(header_path, cython_path, gen_file_path):\n\n    header_hash = (sha1_of_file(header_path)\n                   if os.path.exists(header_path) else 'NA')\n    from_hash = sha1_of_file(cython_path)\n    to_hash = (sha1_of_file(gen_file_path)\n               if os.path.exists(gen_file_path) else 'NA')\n\n    return header_hash, from_hash, to_hash", "response": "Get the hashes from the given files"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlogs PDF for Truncated Normal prior mu - Latent variable mu in the log distribution", "response": "def logpdf(self, mu):\n        \"\"\"\n        Log PDF for Truncated Normal prior\n\n        Parameters\n        ----------\n        mu : float\n            Latent variable for which the prior is being formed over\n\n        Returns\n        ----------\n        - log(p(mu))\n        \"\"\"\n        if self.transform is not None:\n            mu = self.transform(mu)     \n        if mu < self.lower and self.lower is not None:\n            return -10.0**6\n        elif mu > self.upper and self.upper is not None:\n            return -10.0**6\n        else:\n            return -np.log(float(self.sigma0)) - (0.5*(mu-self.mu0)**2)/float(self.sigma0**2)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tune_scale(acceptance, scale):\n\n        if acceptance > 0.8:\n            scale *= 2.0\n        elif acceptance <= 0.8 and acceptance > 0.4:\n            scale *= 1.3            \n        elif acceptance < 0.234 and acceptance > 0.1:\n            scale *= (1/1.3)\n        elif acceptance <= 0.1 and acceptance > 0.05:\n            scale *= 0.4\n        elif acceptance <= 0.05 and acceptance > 0.01:\n            scale *= 0.2\n        elif acceptance <= 0.01:\n            scale *= 0.1\n        return scale", "response": "Tunes scale for M - H algorithm."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsample from M - H algorithm and return the sample of the metropolis - hastings.", "response": "def sample(self):\n        \"\"\" Sample from M-H algorithm\n\n        Returns\n        ----------\n        chain : np.array\n            Chains for each parameter\n\n        mean_est : np.array\n            Mean values for each parameter\n\n        median_est : np.array\n            Median values for each parameter\n\n        upper_95_est : np.array\n            Upper 95% credibility interval for each parameter\n\n        lower_95_est : np.array\n            Lower 95% credibility interval for each parameter           \n        \"\"\"     \n\n        acceptance = 1\n        finish = 0\n\n        while (acceptance < 0.234 or acceptance > 0.4) or finish == 0:\n\n            # If acceptance is in range, proceed to sample, else continue tuning\n            if not (acceptance < 0.234 or acceptance > 0.4):\n                finish = 1\n                if not self.quiet_progress:\n                    print(\"\")\n                    print(\"Tuning complete! Now sampling.\")\n                sims_to_do = self.nsims\n            else:\n                sims_to_do = int(self.nsims/2) # For acceptance rate tuning\n\n            # Holds data on acceptance rates and uniform random numbers\n            a_rate = np.zeros([sims_to_do,1])\n            crit = np.random.rand(sims_to_do,1)\n            post = multivariate_normal(np.zeros(self.param_no), self.cov_matrix)\n            rnums = post.rvs()*self.scale\n            \n            for k in range(1,sims_to_do): \n                rnums = np.vstack((rnums,post.rvs()*self.scale))\n\n            self.phi, a_rate = metropolis_sampler(sims_to_do, self.phi, self.posterior, \n                a_rate, rnums, crit)\n\n            acceptance = a_rate.sum()/a_rate.shape[0]\n            self.scale = self.tune_scale(acceptance,self.scale)\n            if not self.quiet_progress:\n                print(\"Acceptance rate of Metropolis-Hastings is \" + str(acceptance))\n\n        # Remove warm-up and thin\n        self.phi = self.phi[int(self.nsims/2):,:][::self.thinning,:]\n\n        chain = np.array([self.phi[i][0] for i in range(0, self.phi.shape[0])])\n\n        for m in range(1, self.param_no):\n            chain = np.vstack((chain, [self.phi[i][m] for i in range(0,self.phi.shape[0])]))\n\n        if self.param_no == 1:\n            chain = np.array([chain])\n\n        mean_est = np.array([np.mean(np.array([self.phi[i][j] for i in range(0,self.phi.shape[0])])) for j in range(self.param_no)])\n        median_est = np.array([np.median(np.array([self.phi[i][j] for i in range(0,self.phi.shape[0])])) for j in range(self.param_no)])\n        upper_95_est = np.array([np.percentile(np.array([self.phi[i][j] for i in range(0,self.phi.shape[0])]), 95) for j in range(self.param_no)])\n        lower_95_est = np.array([np.percentile(np.array([self.phi[i][j] for i in range(0,self.phi.shape[0])]), 5) for j in range(self.param_no)])\n\n        return chain, mean_est, median_est, upper_95_est, lower_95_est"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef draw_variable_local(self, size): \n        return ss.norm.rvs(loc=self.mu0, scale=self.sigma0, size=size)", "response": "Simulate from the Normal distribution using instance values\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngas Normal Update term using gradient only - native Python function .", "response": "def first_order_score(y, mean, scale, shape, skewness):\n        \"\"\" GAS Normal Update term using gradient only - native Python function\n\n        Parameters\n        ----------\n        y : float\n            datapoint for the time series\n\n        mean : float\n            location parameter for the Normal distribution\n\n        scale : float\n            scale parameter for the Normal distribution\n\n        shape : float\n            tail thickness parameter for the Normal distribution\n\n        skewness : float\n            skewness parameter for the Normal distribution\n\n        Returns\n        ----------\n        - Score of the Normal family\n        \"\"\"\n        return (y-mean)/np.power(scale,2)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlog PDF for Normal prior mu", "response": "def logpdf(self, mu):\n        \"\"\"\n        Log PDF for Normal prior\n\n        Parameters\n        ----------\n        mu : float\n            Latent variable for which the prior is being formed over\n\n        Returns\n        ----------\n        - log(p(mu))\n        \"\"\"\n        if self.transform is not None:\n            mu = self.transform(mu)     \n        return -np.log(float(self.sigma0)) - (0.5*(mu-self.mu0)**2)/float(self.sigma0**2)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pdf(self, mu):\n        if self.transform is not None:\n            mu = self.transform(mu)             \n        return (1.0/float(self.sigma0))*np.exp(-(0.5*(mu-self.mu0)**2)/float(self.sigma0**2))", "response": "PDF for Normal prior\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwrapping function for changing latent variables - variational inference", "response": "def vi_change_param(self, index, value):\n        \"\"\" Wrapper function for changing latent variables - variational inference\n\n        Parameters\n        ----------\n        index : int\n            0 or 1 depending on which latent variable\n\n        value : float\n            What to change the latent variable to\n        \"\"\"\n        if index == 0:\n            self.mu0 = value\n        elif index == 1:\n            self.sigma0 = np.exp(value)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrap function for selecting appropriate latent variable for variational inference", "response": "def vi_return_param(self, index):\n        \"\"\" Wrapper function for selecting appropriate latent variable for variational inference\n\n        Parameters\n        ----------\n        index : int\n            0 or 1 depending on which latent variable\n\n        Returns\n        ----------\n        The appropriate indexed parameter\n        \"\"\"\n        if index == 0:\n            return self.mu0\n        elif index == 1:\n            return np.log(self.sigma0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef vi_scale_score(self,x):\n        return np.exp(-2.0*np.log(self.sigma0))*(x-self.mu0)**2 - 1.0", "response": "Returns the score of the variational inference of the latent variable at x"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef vi_score(self, x, index):\n        if index == 0:\n            return self.vi_loc_score(x)\n        elif index == 1:\n            return self.vi_scale_score(x)", "response": "Wrapper function for selecting appropriate score for a random variable at x"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef draw_variable(loc, scale, shape, skewness, nsims):\n        return np.random.laplace(loc, scale, nsims)", "response": "Draw random variables from this distribution returning a numpy array"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngases Laplace Update term using gradient only - native Python function .", "response": "def first_order_score(y, mean, scale, shape, skewness):\n        \"\"\" GAS Laplace Update term using gradient only - native Python function\n\n        Parameters\n        ----------\n        y : float\n            datapoint for the time series\n\n        mean : float\n            location parameter for the Laplace distribution\n\n        scale : float\n            scale parameter for the Laplace distribution\n\n        shape : float\n            tail thickness parameter for the Laplace distribution\n\n        skewness : float\n            skewness parameter for the Laplace distribution\n\n        Returns\n        ----------\n        - Score of the Laplace family\n        \"\"\"\n        return (y-mean)/float(scale*np.abs(y-mean))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef logpdf(self, mu):\n        if self.transform is not None:\n            mu = self.transform(mu)    \n        return ss.laplace.logpdf(mu, loc=self.loc0, scale=self.scale0)", "response": "Log PDF for Laplace prior\n        mu"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pdf(self, mu):\n        if self.transform is not None:\n            mu = self.transform(mu)                \n        return ss.laplace.pdf(mu, self.loc0, self.scale0)", "response": "PDF for Laplace prior\n        mu"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngases Laplace Update term potentially using second - order information - native Python function .", "response": "def second_order_score(y, mean, scale, shape, skewness):\n        \"\"\" GAS Laplace Update term potentially using second-order information - native Python function\n\n        Parameters\n        ----------\n        y : float\n            datapoint for the time series\n\n        mean : float\n            location parameter for the Laplace distribution\n\n        scale : float\n            scale parameter for the Laplace distribution\n\n        shape : float\n            tail thickness parameter for the Laplace distribution\n\n        skewness : float\n            skewness parameter for the Laplace distribution\n\n        Returns\n        ----------\n        - Adjusted score of the Laplace family\n        \"\"\"\n        return ((y-mean)/float(scale*np.abs(y-mean))) / (-(np.power(y-mean,2) - np.power(np.abs(mean-y),2))/(scale*np.power(np.abs(mean-y),3)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck data type of the object and returns the transformed data data_name is_pandas is True if pandas data is used else numpy data is used", "response": "def data_check(data,target):\n    \"\"\" Checks data type\n\n    Parameters\n    ----------\n    data : pd.DataFrame or np.array\n        Field to specify the time series data that will be used.\n    \n    target : int or str\n        Target column\n        \n    Returns\n    ----------\n    transformed_data : np.array\n        Raw data array for use in the model\n        \n    data_name : str\n        Name of the data\n    \n    is_pandas : Boolean\n        True if pandas data, else numpy\n    \n    data_index : np.array\n        The time indices for the data\n    \"\"\"\n\n    # Check pandas or numpy\n    if isinstance(data, pd.DataFrame) or isinstance(data, pd.core.frame.DataFrame):\n        data_index = data.index         \n        if target is None:\n            transformed_data = data.ix[:,0].values\n            data_name = str(data.columns.values[0])\n        else:\n            transformed_data = data[target].values          \n            data_name = str(target)                 \n        is_pandas = True\n        \n    elif isinstance(data, np.ndarray):\n        data_name = \"Series\"        \n        is_pandas = False\n        if any(isinstance(i, np.ndarray) for i in data):\n            if target is None:\n                transformed_data = data[0]          \n                data_index = list(range(len(data[0])))\n            else:\n                transformed_data = data[target]         \n                data_index = list(range(len(data[target])))\n        else:\n            transformed_data = data                 \n            data_index = list(range(len(data)))\n    else:\n        raise Exception(\"The data input is not pandas or numpy compatible!\")\n    \n    return transformed_data, data_name, is_pandas, data_index"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_z(self, name, prior, q, index=True):\n\n        self.z_list.append(LatentVariable(name,len(self.z_list),prior,q))\n        if index is True:\n            self.z_indices[name] = {'start': len(self.z_list)-1, 'end': len(self.z_list)-1}", "response": "Adds a latent variable to the set of latent variables."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create(self, name, dim, prior, q):\n\n        def rec(dim, prev=[]):\n           if len(dim) > 0:\n               return [rec(dim[1:], prev + [i]) for i in range(dim[0])]\n           else:\n               return \"(\" + \",\".join([str(j) for j in prev]) + \")\" \n\n        indices = rec(dim)\n\n        for f_dim in range(1, len(dim)):\n            indices = sum(indices, [])\n\n        if self.z_list is None:\n            starting_index = 0\n        else:\n            starting_index = len(self.z_list)\n\n        self.z_indices[name] = {'start': starting_index, 'end': starting_index+len(indices)-1, 'dim': len(dim)}\n\n        for index in indices:\n            self.add_z(name + \" \" + index, prior, q, index=False)", "response": "Creates multiple latent variables in the same order as the original."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadjust priors for the latent variables in the list.", "response": "def adjust_prior(self,index,prior):\n        \"\"\" Adjusts priors for the latent variables\n\n        Parameters\n        ----------\n        index : int or list[int]\n            Which latent variable index/indices to be altered\n\n        prior : Prior object\n            Which prior distribution? E.g. Normal(0,1)\n\n        Returns\n        ----------\n        None (changes priors in Parameters object)\n        \"\"\"\n\n        if isinstance(index, list):\n            for item in index:\n                if item < 0 or item > (len(self.z_list)-1) or not isinstance(item, int):\n                    raise ValueError(\"Oops - the latent variable index \" + str(item) + \" you have entered is invalid!\")\n                else:\n                    self.z_list[item].prior = prior \n                    if hasattr(self.z_list[item].prior, 'mu0'):\n                        self.z_list[item].start = self.z_list[item].prior.mu0    \n                    elif hasattr(self.z_list[item].prior, 'loc0'):\n                        self.z_list[item].start = self.z_list[item].prior.loc0\n        else:\n            if index < 0 or index > (len(self.z_list)-1) or not isinstance(index, int):\n                raise ValueError(\"Oops - the latent variable index \" + str(index) + \" you have entered is invalid!\")\n            else:\n                self.z_list[index].prior = prior\n                if hasattr(self.z_list[index].prior, 'mu0'):\n                    self.z_list[index].start = self.z_list[index].prior.mu0  \n                elif hasattr(self.z_list[index].prior, 'loc0'):\n                    self.z_list[index].start = self.z_list[index].prior.loc0"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef K(self, parm):\n        return OU_K_matrix(self.X, parm) + np.identity(self.X.shape[0])*(10**-10)", "response": "Returns the Gram Matrix"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_latent_variables(self):\n        lvs_to_build = []\n        lvs_to_build.append(['Noise Sigma^2', fam.Flat(transform='exp'), fam.Normal(0,3), -1.0])\n        for lag in range(self.X.shape[1]):\n            lvs_to_build.append(['l lag' + str(lag+1), fam.FLat(transform='exp'), fam.Normal(0,3), -1.0])\n        lvs_to_build.append(['tau', fam.Flat(transform='exp'), fam.Normal(0,3), -1.0])\n        return lvs_to_build", "response": "Builds latent variables for this kernel"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the Gram Matrix", "response": "def K(self, parm):\n        \"\"\" Returns the Gram Matrix\n\n        Parameters\n        ----------\n        parm : np.ndarray\n            Parameters for the Gram Matrix\n\n        Returns\n        ----------\n        - Gram Matrix (np.ndarray)\n        \"\"\"\n        return ARD_K_matrix(self.X, parm) + np.identity(self.X.shape[0])*(10**-10)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the Gram Matrix K", "response": "def K(self, parm):\n        \"\"\" Returns the Gram Matrix\n\n        Parameters\n        ----------\n        parm : np.ndarray\n            Parameters for the Gram Matrix\n\n        Returns\n        ----------\n        - Gram Matrix (np.ndarray)\n        \"\"\"\n        return RQ_K_matrix(self.X, parm) + np.identity(self.X.shape[0])*(10**-10)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef K(self, parm):\n        return Periodic_K_matrix(self.X, parm) + np.identity(self.X.shape[0])*(10**-10)", "response": "Returns the Gram Matrix K"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _model(self, beta):\n\n        Y = np.array(self.data[self.max_lag:self.data.shape[0]])\n        X = np.ones(Y.shape[0])\n        scores = np.zeros(Y.shape[0])\n\n        # Transform latent variables\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n\n        lmda = np.ones(Y.shape[0])*parm[0]\n        theta = np.ones(Y.shape[0])*parm[-1]\n\n        # Loop over time series\n        for t in range(0,Y.shape[0]):\n\n            if t < self.max_lag:\n                lmda[t] = parm[0]/(1-np.sum(parm[1:(self.p+1)]))\n                theta[t] += (parm[-3] - (1.0/parm[-3]))*np.exp(lmda[t])*(np.sqrt(parm[-2])*sp.gamma((parm[-2]-1.0)/2.0))/(np.sqrt(np.pi)*sp.gamma(parm[-2]/2.0))\n            else:\n\n                # Loop over GARCH terms\n                for p_term in range(0,self.p):\n                    lmda[t] += parm[1+p_term]*lmda[t-p_term-1]\n\n                # Loop over Score terms\n                for q_term in range(0,self.q):\n                    lmda[t] += parm[1+self.p+q_term]*scores[t-q_term-1]\n\n                if self.leverage is True:\n                    lmda[t] += parm[-4]*np.sign(-(Y[t-1]-theta[t-1]))*(scores[t-1]+1)\n\n                theta[t] += (parm[-3] - (1.0/parm[-3]))*np.exp(lmda[t]/2.0)*(np.sqrt(parm[-2])*sp.gamma((parm[-2]-1.0)/2.0))/(np.sqrt(np.pi)*sp.gamma(parm[-2]/2.0))\n            \n            if (Y[t]-theta[t])>=0:\n                scores[t] = (((parm[-2]+1.0)*np.power(Y[t]-theta[t],2))/float(np.power(parm[-3], 2)*parm[-2]*np.exp(lmda[t]) + np.power(Y[t]-theta[t],2))) - 1.0\n            else:\n                scores[t] = (((parm[-2]+1.0)*np.power(Y[t]-theta[t],2))/float(np.power(parm[-3],-2)*parm[-2]*np.exp(lmda[t]) + np.power(Y[t]-theta[t],2))) - 1.0    \n\n        return lmda, Y, scores, theta", "response": "Creates the structure of the model for the current state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating the structure of the model for the current state of the object.", "response": "def _mb_model(self, beta, mini_batch):\n        \"\"\" Creates the structure of the model\n\n        Parameters\n        ----------\n        beta : np.array\n            Contains untransformed starting values for latent variables\n\n        mini_batch : int\n            Mini batch size for the data sampling\n\n        Returns\n        ----------\n        lambda : np.array\n            Contains the values for the conditional volatility series\n\n        Y : np.array\n            Contains the length-adjusted time series (accounting for lags)\n\n        scores : np.array\n            Contains the score terms for the time series\n        \"\"\"\n\n        # Transform latent variables\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n\n        rand_int =  np.random.randint(low=0, high=self.data_length-mini_batch+1)\n        sample = np.arange(start=rand_int, stop=rand_int+mini_batch)\n        sampled_data = self.data[sample]\n\n        Y = np.array(sampled_data[self.max_lag:])\n        X = np.ones(Y.shape[0])\n        scores = np.zeros(Y.shape[0])\n        lmda = np.ones(Y.shape[0])*parm[0]\n        theta = np.ones(Y.shape[0])*parm[-1]\n\n        # Loop over time series\n        for t in range(0,Y.shape[0]):\n\n            if t < self.max_lag:\n                lmda[t] = parm[0]/(1-np.sum(parm[1:(self.p+1)]))\n                theta[t] += (parm[-3] - (1.0/parm[-3]))*np.exp(lmda[t])*(np.sqrt(parm[-2])*sp.gamma((parm[-2]-1.0)/2.0))/(np.sqrt(np.pi)*sp.gamma(parm[-2]/2.0))\n            else:\n\n                # Loop over GARCH terms\n                for p_term in range(0,self.p):\n                    lmda[t] += parm[1+p_term]*lmda[t-p_term-1]\n\n                # Loop over Score terms\n                for q_term in range(0,self.q):\n                    lmda[t] += parm[1+self.p+q_term]*scores[t-q_term-1]\n\n                if self.leverage is True:\n                    lmda[t] += parm[-4]*np.sign(-(Y[t-1]-theta[t-1]))*(scores[t-1]+1)\n\n                theta[t] += (parm[-3] - (1.0/parm[-3]))*np.exp(lmda[t]/2.0)*(np.sqrt(parm[-2])*sp.gamma((parm[-2]-1.0)/2.0))/(np.sqrt(np.pi)*sp.gamma(parm[-2]/2.0))\n            \n            if (Y[t]-theta[t])>=0:\n                scores[t] = (((parm[-2]+1.0)*np.power(Y[t]-theta[t],2))/float(np.power(parm[-3], 2)*parm[-2]*np.exp(lmda[t]) + np.power(Y[t]-theta[t],2))) - 1.0\n            else:\n                scores[t] = (((parm[-2]+1.0)*np.power(Y[t]-theta[t],2))/float(np.power(parm[-3],-2)*parm[-2]*np.exp(lmda[t]) + np.power(Y[t]-theta[t],2))) - 1.0    \n\n        return lmda, Y, scores, theta"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a h-step ahead mean prediction Parameters ---------- lmda : np.array The past predicted values Y : np.array The past data scores : np.array The past scores h : int How many steps ahead for the prediction t_params : np.array A vector of (transformed) latent variables Returns ---------- h-length vector of mean predictions", "response": "def _mean_prediction(self, lmda, Y, scores, h, t_params):\n        \"\"\" Creates a h-step ahead mean prediction\n\n        Parameters\n        ----------\n        lmda : np.array\n            The past predicted values\n\n        Y : np.array\n            The past data\n\n        scores : np.array\n            The past scores\n\n        h : int\n            How many steps ahead for the prediction\n\n        t_params : np.array\n            A vector of (transformed) latent variables\n\n        Returns\n        ----------\n        h-length vector of mean predictions\n        \"\"\"     \n\n        # Create arrays to iteratre over\n        lmda_exp = lmda.copy()\n        scores_exp = scores.copy()\n        Y_exp = Y.copy()\n        m1 = (np.sqrt(t_params[-2])*sp.gamma((t_params[-2]-1.0)/2.0))/(np.sqrt(np.pi)*sp.gamma(t_params[-2]/2.0))\n        temp_theta = t_params[-1] + (t_params[-3] - (1.0/t_params[-3]))*np.exp(lmda_exp[-1]/2.0)*m1\n        # Loop over h time periods          \n        for t in range(0,h):\n            new_value = t_params[0]\n\n            if self.p != 0:\n                for j in range(1,self.p+1):\n                    new_value += t_params[j]*lmda_exp[-j]\n\n            if self.q != 0:\n                for k in range(1,self.q+1):\n                    new_value += t_params[k+self.p]*scores_exp[-k]\n\n            if self.leverage is True:\n                m1 = (np.sqrt(t_params[-2])*sp.gamma((t_params[-2]-1.0)/2.0))/(np.sqrt(np.pi)*sp.gamma(t_params[-2]/2.0))\n                new_value += t_params[1+self.p+self.q]*np.sign(-(Y_exp[-1]-temp_theta))*(scores_exp[-1]+1)\n\n            temp_theta = t_params[-1] + (t_params[-3] - (1.0/t_params[-3]))*np.exp(new_value/2.0)*m1\n\n            lmda_exp = np.append(lmda_exp,[new_value]) # For indexing consistency\n            scores_exp = np.append(scores_exp,[0]) # expectation of score is zero\n            Y_exp = np.append(Y_exp,[temp_theta])\n\n        return lmda_exp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating the negative log - likelihood of the model for the given set of latent variables.", "response": "def mb_neg_loglik(self, beta, mini_batch):\n        \"\"\" Creates the negative log-likelihood of the model\n\n        Parameters\n        ----------\n        beta : np.array\n            Contains untransformed starting values for latent variables\n\n        mini_batch : int\n            Size of each mini batch of data\n\n        Returns\n        ----------\n        The negative logliklihood of the model\n        \"\"\"     \n\n        lmda, Y, ___, theta = self._mb_model(beta, mini_batch)\n        return -np.sum(logpdf(Y, self.latent_variables.z_list[-2].prior.transform(beta[-2]), \n            loc=theta, scale=np.exp(lmda/2.0), \n            skewness = self.latent_variables.z_list[-3].prior.transform(beta[-3])))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate Autoregressive matrix X = np. array Y = self. data", "response": "def _ar_matrix(self):\n        \"\"\" Creates Autoregressive matrix\n\n        Returns\n        ----------\n        X : np.ndarray\n            Autoregressive Matrix\n\n        \"\"\"\n        Y = np.array(self.data[self.max_lag:self.data.shape[0]])\n        X = self.data[(self.max_lag-1):-1]\n\n        if self.ar != 0:\n            for i in range(1, self.ar):\n                X = np.vstack((X,self.data[(self.max_lag-i-1):-i-1]))\n\n        return X"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _create_latent_variables(self):\n\n        no_of_features = self.ar\n\n        self.latent_variables.create(name='Bias', dim=[self.layers, self.units], prior=fam.Cauchy(0, 1, transform=None), q=fam.Normal(0, 3))\n\n        self.latent_variables.create(name='Output bias', dim=[1], prior=fam.Cauchy(0, 1, transform=None), q=fam.Normal(0, 3))\n\n        self.latent_variables.create(name='Input weight', dim=[self.units, self.ar], prior=fam.Cauchy(0, 1, transform=None), q=fam.Normal(0, 3))\n\n        self.latent_variables.create(name='Hidden weight', dim=[self.layers-1, self.units, self.units], prior=fam.Cauchy(0, 1, transform=None), q=fam.Normal(0, 3))\n\n        self.latent_variables.create(name='Output weight', dim=[self.units], prior=fam.Cauchy(0, 1, transform=None), q=fam.Normal(0, 3))", "response": "Creates the latent variables for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating the structure of the model.", "response": "def _model(self, beta):\n        \"\"\" Creates the structure of the model (model matrices etc)\n\n        Parameters\n        ----------\n        beta : np.ndarray\n            Contains untransformed starting values for the latent variables\n\n        Returns\n        ----------\n        mu : np.ndarray\n            Contains the predicted values (location) for the time series\n\n        Y : np.ndarray\n            Contains the length-adjusted time series (accounting for lags)\n        \"\"\"     \n\n        Y = np.array(self.data[self.max_lag:])\n\n        # Transform latent variables\n        z = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n        bias = z[self.latent_variables.z_indices['Bias']['start']:self.latent_variables.z_indices['Bias']['end']+1]\n        bias = np.reshape(bias, (-1, self.units))\n        output_bias = z[self.latent_variables.z_indices['Output bias']['start']]\n        input_weights = z[self.latent_variables.z_indices['Input weight']['start']:self.latent_variables.z_indices['Input weight']['end']+1]\n        input_weights = np.reshape(input_weights, (-1, self.units))\n        output_weights = z[self.latent_variables.z_indices['Output weight']['start']:self.latent_variables.z_indices['Output weight']['end']+1]\n\n        # Construct neural network\n        h = self.activation(self.X.T.dot(input_weights) + bias[0])\n\n        if self.layers > 1:\n            hidden_weights = z[self.latent_variables.z_indices['Hidden weight']['start']:self.latent_variables.z_indices['Hidden weight']['end']+1]\n            hidden_weights = np.reshape(hidden_weights, (self.layers-1, self.units, -1))\n\n            for k in range(0, self.layers-1):\n                h = self.activation(h.dot(hidden_weights[k]) + bias[1+k])\n\n        return h.dot(output_weights) + output_bias, Y"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating the structure of the model for mini batch model.", "response": "def _mb_model(self, beta, mini_batch):\n        \"\"\" Creates the structure of the model (model matrices etc) for mini batch model\n\n        Parameters\n        ----------\n        beta : np.ndarray\n            Contains untransformed starting values for the latent variables\n\n        mini_batch : int\n            Mini batch size for the data sampling\n\n        Returns\n        ----------\n        mu : np.ndarray\n            Contains the predicted values (location) for the time series\n\n        Y : np.ndarray\n            Contains the length-adjusted time series (accounting for lags)\n        \"\"\"     \n\n        Y = np.array(self.data[self.max_lag:])\n\n        sample = np.random.choice(len(Y), mini_batch, replace=False)\n\n        Y = Y[sample]\n        X = self.X[:, sample]\n\n        # Transform latent variables\n        z = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n        bias = z[self.latent_variables.z_indices['Bias']['start']:self.latent_variables.z_indices['Bias']['end']+1]\n        bias = np.reshape(bias, (-1, self.units))\n        output_bias = z[self.latent_variables.z_indices['Output bias']['start']]\n        input_weights = z[self.latent_variables.z_indices['Input weight']['start']:self.latent_variables.z_indices['Input weight']['end']+1]\n        input_weights = np.reshape(input_weights, (-1, self.units))\n        output_weights = z[self.latent_variables.z_indices['Output weight']['start']:self.latent_variables.z_indices['Output weight']['end']+1]\n\n        # Construct neural network\n        h = self.activation(X.T.dot(input_weights) + bias[0])\n\n        if self.layers > 1:\n            hidden_weights = z[self.latent_variables.z_indices['Hidden weight']['start']:self.latent_variables.z_indices['Hidden weight']['end']+1]\n            hidden_weights = np.reshape(hidden_weights, (self.layers-1, self.units, -1))\n\n            for k in range(0, self.layers-1):\n                h = self.activation(h.dot(hidden_weights[k]) + bias[1+k])\n\n        return h.dot(output_weights) + output_bias, Y"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _mean_prediction(self, mu, Y, h, t_z):\n\n        # Create arrays to iteratre over\n        Y_exp = Y.copy()\n\n        # Loop over h time periods          \n        for t in range(0,h):\n            new_value = self.predict_new(Y_exp[-self.ar:][::-1], self.latent_variables.get_z_values())\n            Y_exp = np.append(Y_exp, [self.link(new_value)])\n\n        return Y_exp", "response": "Create a h - step ahead mean prediction for the current set of latent variables."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_model(self, model):\n\n        if model.model_type not in self.supported_models:\n            raise ValueError('Model type not supported for Aggregate! Apologies')\n\n        if not self.model_list:\n            self.model_list.append(model)\n            if model.model_type in ['EGARCH', 'EGARCHM', 'EGARCHMReg', 'GARCH', 'LMEGARCH', 'LMSEGARCH', 'SEGARCH', 'SEGARCHM']:\n                self.data = np.abs(model.data)\n            else:\n                self.data = model.data\n            self.index = model.index\n        else:\n            if model.model_type in ['EGARCH', 'EGARCHM', 'EGARCHMReg', 'GARCH', 'LMEGARCH', 'LMSEGARCH', 'SEGARCH', 'SEGARCHM']:\n                if np.isclose(np.abs(np.abs(model.data[-self.match_window:])-self.data[-self.match_window:]).sum(),0.0) or model.model_type=='GPNARX':\n                    self.model_list.append(model)\n                else:\n                    raise ValueError('Data entered is deemed different based on %s last values!' % (s))\n            else:\n                if np.isclose(np.abs(model.data[-self.match_window:]-self.data[-self.match_window:]).sum(),0.0) or model.model_type=='GPNARX':\n                    self.model_list.append(model)\n                else:\n                    raise ValueError('Data entered is deemed different based on %s last values!' % (s))\n\n        self.model_names = [i.model_name for i in self.model_list]", "response": "Adds a PyFlux univariate model to the aggregating algorithm\n        "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\noutputting ensemble model predictions for out - of - sample data.", "response": "def _model_predict(self, h, recalculate=False, fit_once=True):\n        \"\"\" Outputs ensemble model predictions for out-of-sample data\n        \n        Parameters\n        ----------\n        h : int\n            How many steps at the end of the series to run the ensemble on\n\n        recalculate: boolean\n            Whether to recalculate the predictions or not\n\n        fit_once : boolean\n            Whether to fit the model once at the beginning, or with every iteration\n        \n        Returns\n        ----------\n        - pd.DataFrame of the model predictions, index of dates\n        \"\"\"\n\n        if len(self.model_predictions) == 0 or h != self.h or recalculate is True:\n\n            for no, model in enumerate(self.model_list):\n                if no == 0:\n                    model.fit()\n                    result = model.predict(h)\n                    self.predict_index = result.index\n                    result.columns = [model.model_name]\n                else:\n                    model.fit()\n                    new_frame = model.predict(h)\n                    new_frame.columns = [model.model_name]\n                    result = pd.concat([result,new_frame], axis=1)\n\n            self.model_predictions = result\n            self.h = h\n            return result, self.predict_index\n\n        else:\n            return self.model_predictions, self.predict_index"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\noutputs ensemble model predictions for the end - of - period data.", "response": "def _model_predict_is(self, h, recalculate=False, fit_once=True):\n        \"\"\" Outputs ensemble model predictions for the end-of-period data\n        \n        Parameters\n        ----------\n        h : int\n            How many steps at the end of the series to run the ensemble on\n\n        recalculate: boolean\n            Whether to recalculate the predictions or not\n\n        fit_once : boolean\n            Whether to fit the model once at the beginning, or with every iteration\n        \n        Returns\n        ----------\n        - pd.DataFrame of the model predictions, index of dates\n        \"\"\"\n\n        if len(self.model_predictions_is) == 0 or h != self.h or recalculate is True:\n\n            for no, model in enumerate(self.model_list):\n                if no == 0:\n                    result = model.predict_is(h, fit_once=fit_once)\n                    result.columns = [model.model_name]\n                else:\n                    new_frame = model.predict_is(h, fit_once=fit_once)\n                    new_frame.columns = [model.model_name]\n                    result = pd.concat([result,new_frame], axis=1)\n\n            self.model_predictions_is = result\n            self.h = h\n            return result\n\n        else:\n            return self.model_predictions_is"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconstructs the losses for the ensemble and each constitute model", "response": "def _construct_losses(self, data, predictions, ensemble_prediction):\n        \"\"\" Construct losses for the ensemble and each constitute model\n        \n        Parameters\n        ----------\n        data: np.ndarray\n            The univariate time series\n\n        predictions : np.ndarray\n            The predictions of each constitute model\n\n        ensemble_prediction : np.ndarray\n            The prediction of the ensemble model\n\n        Returns\n        ----------\n        - np.ndarray of the losses for each model\n        \"\"\"\n\n        losses = []\n        losses.append(self.loss_type(data, ensemble_prediction).sum()/data.shape[0])\n        for model in range(len(self.model_list)):\n            losses.append(self.loss_type(data, predictions[:,model]).sum()/data.shape[0])\n        return losses"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(self, h, recalculate=False):\n\n        data = self.data[-h:] \n        predictions = self._model_predict_is(h, recalculate=recalculate).values\n        weights = np.zeros((h, len(self.model_list)))\n        normalized_weights = np.zeros((h, len(self.model_list)))\n        ensemble_prediction = np.zeros(h)\n        \n        for t in range(h):\n            if t == 0:\n                weights[t,:] = 100000\n                ensemble_prediction[t] = np.dot(weights[t,:]/weights[t,:].sum(), predictions[t,:])\n                weights[t,:] = weights[t,:]*np.exp(-self.learning_rate*self.loss_type(data[t], predictions[t,:]))\n                normalized_weights[t,:] = weights[t,:]/weights[t,:].sum()\n            else:\n                ensemble_prediction[t] = np.dot(weights[t-1,:]/weights[t-1,:].sum(), predictions[t,:])\n                weights[t,:] = weights[t-1,:]*np.exp(-self.learning_rate*self.loss_type(data[t], predictions[t,:]))\n                normalized_weights[t,:] = weights[t,:]/weights[t,:].sum()\n\n        return normalized_weights, self._construct_losses(data, predictions, ensemble_prediction), ensemble_prediction", "response": "Run the aggregating algorithm on \n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_weights(self, h, **kwargs):\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n\n        weights, _, _ = self.run(h=h)\n        plt.figure(figsize=figsize)\n        plt.plot(self.index[-h:],weights)\n        plt.legend(self.model_names)\n        plt.show()", "response": "Plot the weights from the aggregating algorithm on \n\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun out - of - sample predicitons for Aggregate algorithm", "response": "def predict(self, h, h_train=40):\n        \"\"\" Run out-of-sample predicitons for Aggregate algorithm\n        (This only works for non-exogenous variable models currently)\n        \n        Parameters\n        ----------\n        h : int\n            How many out-of-sample steps to run the aggregating algorithm on \n\n        h_train : int\n            How many in-sample steps to warm-up the ensemble weights on\n\n        Returns\n        ----------\n        - pd.DataFrame of Aggregate out-of-sample predictions\n        \"\"\"\n\n        predictions, index = self._model_predict(h)\n        normalized_weights = self.run(h=h_train)[0][-1, :]\n        ensemble_prediction = np.zeros(h)\n        \n        for t in range(h):\n            ensemble_prediction[t] = np.dot(normalized_weights, predictions.values[t,:])\n\n        result = pd.DataFrame(ensemble_prediction)\n        result.index = index\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef predict_is(self, h):\n        result = pd.DataFrame([self.run(h=h)[2]]).T  \n        result.index = self.index[-h:]\n        return result", "response": "Outputs predictions for the Aggregate algorithm on the in - sample data set"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsummarize the results for each model for h steps of the algorithm on Returns a pd. DataFrame of the results for each model for h steps of the algorithm on", "response": "def summary(self, h):\n        \"\"\" \n        Summarize the results for each model for h steps of the algorithm\n\n       Parameters\n        ----------\n        h : int\n            How many steps to run the aggregating algorithm on \n\n        Returns\n        ----------\n        - pd.DataFrame of losses for each model       \n        \"\"\"\n        _, losses, _ = self.run(h=h)\n        df = pd.DataFrame(losses)\n        df.index = ['Ensemble'] + self.model_names\n        df.columns = [self.loss_name]\n        return df"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_model_matrices(self):\n\n        self.model_Y = self.data\n        self.model_scores = np.zeros((self.X.shape[1], self.model_Y.shape[0]+1))", "response": "Creates model matrices and vectors for the current version of the base class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _create_latent_variables(self):\n\n        for parm in range(self.z_no):\n            self.latent_variables.add_z('Scale ' + self.X_names[parm], fam.Flat(transform='exp'), fam.Normal(0, 3))\n            self.latent_variables.z_list[parm].start = -5.0\n        self.z_no = len(self.latent_variables.z_list)", "response": "Creates latent variables for the current class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _uncythonized_model(self, beta):\n\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n        coefficients = np.zeros((self.X.shape[1],self.model_Y.shape[0]+1))\n        coefficients[:,0] = self.initial_values\n        theta = np.zeros(self.model_Y.shape[0]+1)\n        model_scale, model_shape, model_skewness = self._get_scale_and_shape(parm)\n\n        # Loop over time series\n        theta, self.model_scores, coefficients = gas_reg_recursion(parm, theta, self.X, coefficients, self.model_scores, self.model_Y, self.model_Y.shape[0], \n            self.family.reg_score_function, self.link, model_scale, model_shape, model_skewness, self.max_lag)\n\n        return theta[:-1], self.model_Y, self.model_scores, coefficients", "response": "Creates the structure of the model for the uncythonized time series."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_fit(self, **kwargs):\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n\n            date_index = self.index.copy()\n            mu, Y, scores, coefficients = self._model(self.latent_variables.get_z_values())\n\n            if self.model_name2 == \"Exponential\":\n                values_to_plot = 1.0/self.link(mu)\n            elif self.model_name2 == \"Skewt\":\n                t_params = self.transform_z()\n                model_scale, model_shape, model_skewness = self._get_scale_and_shape(t_params)\n                m1 = (np.sqrt(model_shape)*sp.gamma((model_shape-1.0)/2.0))/(np.sqrt(np.pi)*sp.gamma(model_shape/2.0))\n                additional_loc = (model_skewness - (1.0/model_skewness))*model_scale*m1\n                values_to_plot = mu + additional_loc\n            else:\n                values_to_plot = self.link(mu)\n\n            plt.figure(figsize=figsize) \n            \n            plt.subplot(len(self.X_names)+1, 1, 1)\n            plt.title(self.y_name + \" Filtered\")\n            plt.plot(date_index,Y,label='Data')\n            plt.plot(date_index,values_to_plot,label='GAS Filter',c='black')\n            plt.legend(loc=2)\n\n            for coef in range(0,len(self.X_names)):\n                plt.subplot(len(self.X_names)+1, 1, 2+coef)\n                plt.title(\"Beta \" + self.X_names[coef]) \n                plt.plot(date_index,coefficients[coef,0:-1],label='Coefficient')\n                plt.legend(loc=2)               \n\n            plt.show()", "response": "Plots the fit of the model of the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots the predict function for the current object.", "response": "def plot_predict(self, h=5, past_values=20, intervals=True, oos_data=None, **kwargs):\n        \"\"\" Makes forecast with the estimated model\n\n        Parameters\n        ----------\n        h : int (default : 5)\n            How many steps ahead would you like to forecast?\n\n        past_values : int (default : 20)\n            How many past observations to show on the forecast graph?\n\n        intervals : Boolean\n            Would you like to show prediction intervals for the forecast?\n\n        oos_data : pd.DataFrame\n            Data for the variables to be used out of sample (ys can be NaNs)\n\n        Returns\n        ----------\n        - Plot of the forecast\n        \"\"\"\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        figsize = kwargs.get('figsize',(10,7))\n\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            # Sort/manipulate the out-of-sample data\n            _, X_oos = dmatrices(self.formula, oos_data)\n            X_oos = np.array([X_oos])[0]\n            X_pred = X_oos[:h]\n\n            date_index = self.shift_dates(h)\n\n            if self.latent_variables.estimation_method in ['M-H']:\n                \n                sim_vector = np.zeros([15000,h])\n\n                for n in range(0, 15000):\n                    t_z = self.draw_latent_variables(nsims=1).T[0]\n                    _, Y, _, coefficients = self._model(t_z)\n                    coefficients_star = coefficients.T[-1]\n                    theta_pred = np.dot(np.array([coefficients_star]), X_pred.T)[0]\n                    t_z = np.array([self.latent_variables.z_list[k].prior.transform(t_z[k]) for k in range(t_z.shape[0])])\n                    model_scale, model_shape, model_skewness = self._get_scale_and_shape(t_z)\n                    sim_vector[n,:] = self.family.draw_variable(self.link(theta_pred), model_scale, model_shape, model_skewness, theta_pred.shape[0])\n                mean_values = np.append(Y, self.link(np.array([np.mean(i) for i in sim_vector.T])))\n            else:\n\n                # Retrieve data, dates and (transformed) latent variables\n                _, Y, _, coefficients = self._model(self.latent_variables.get_z_values()) \n                coefficients_star = coefficients.T[-1] \n                theta_pred = np.dot(np.array([coefficients_star]), X_pred.T)[0]  \n                t_z = self.transform_z()\n                sim_vector = np.zeros([15000,h])\n                mean_values = np.append(Y, self.link(theta_pred))\n                model_scale, model_shape, model_skewness = self._get_scale_and_shape(t_z)\n\n                if self.model_name2 == \"Skewt\":\n                    m1 = (np.sqrt(model_shape)*sp.gamma((model_shape-1.0)/2.0))/(np.sqrt(np.pi)*sp.gamma(model_shape/2.0))\n                    mean_values += (model_skewness - (1.0/model_skewness))*model_scale*m1 \n\n                for n in range(0,15000):\n                    sim_vector[n,:] = self.family.draw_variable(self.link(theta_pred),model_scale,model_shape,model_skewness,theta_pred.shape[0])\n\n            sim_vector = sim_vector.T\n            error_bars = []\n            for pre in range(5,100,5):\n                error_bars.append(np.insert([np.percentile(i,pre) for i in sim_vector], 0, mean_values[-h-1]))\n            forecasted_values = mean_values[-h-1:]\n            plot_values = mean_values[-h-past_values:]\n            plot_index = date_index[-h-past_values:]\n\n            plt.figure(figsize=figsize)\n            if intervals == True:\n                alpha =[0.15*i/float(100) for i in range(50,12,-2)]\n                for count in range(9):\n                    plt.fill_between(date_index[-h-1:], error_bars[count], error_bars[-count],\n                        alpha=alpha[count])     \n            plt.plot(plot_index,plot_values)\n            plt.title(\"Forecast for \" + self.data_name)\n            plt.xlabel(\"Time\")\n            plt.ylabel(self.data_name)\n            plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef predict(self, h=5, oos_data=None, intervals=False, **kwargs):\n        if self.latent_variables.estimated is False:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            # Sort/manipulate the out-of-sample data\n            _, X_oos = dmatrices(self.formula, oos_data)\n            X_oos = np.array([X_oos])[0]\n            X_pred = X_oos[:h]\n\n            date_index = self.shift_dates(h)\n\n            if self.latent_variables.estimation_method in ['M-H']:\n                \n                sim_vector = np.zeros([15000,h])\n\n                for n in range(0, 15000):\n                    t_z = self.draw_latent_variables(nsims=1).T[0]\n                    _, Y, _, coefficients = self._model(t_z)\n                    coefficients_star = coefficients.T[-1]\n                    theta_pred = np.dot(np.array([coefficients_star]), X_pred.T)[0]\n                    t_z = np.array([self.latent_variables.z_list[k].prior.transform(t_z[k]) for k in range(t_z.shape[0])])\n                    model_scale, model_shape, model_skewness = self._get_scale_and_shape(t_z)\n                    sim_vector[n,:] = self.family.draw_variable(self.link(theta_pred), model_scale, model_shape, model_skewness, theta_pred.shape[0])\n\n                sim_vector = sim_vector.T\n\n                forecasted_values = np.array([np.mean(i) for i in sim_vector])\n                prediction_01 = np.array([np.percentile(i, 1) for i in sim_vector])\n                prediction_05 = np.array([np.percentile(i, 5) for i in sim_vector])\n                prediction_95 = np.array([np.percentile(i, 95) for i in sim_vector])\n                prediction_99 = np.array([np.percentile(i, 99) for i in sim_vector])\n\n            else:\n\n                # Retrieve data, dates and (transformed) latent variables\n                _, Y, _, coefficients = self._model(self.latent_variables.get_z_values()) \n                coefficients_star = coefficients.T[-1] \n                theta_pred = np.dot(np.array([coefficients_star]), X_pred.T)[0]  \n                t_z = self.transform_z()\n                mean_values = np.append(Y, self.link(theta_pred))\n                model_scale, model_shape, model_skewness = self._get_scale_and_shape(t_z)\n\n                if self.model_name2 == \"Skewt\":\n                    m1 = (np.sqrt(model_shape)*sp.gamma((model_shape-1.0)/2.0))/(np.sqrt(np.pi)*sp.gamma(model_shape/2.0))\n                    forecasted_values = mean_values[-h:] + (model_skewness - (1.0/model_skewness))*model_scale*m1 \n                else:\n                    forecasted_values = mean_values[-h:] \n\n            if intervals is False:\n                result = pd.DataFrame(forecasted_values)\n                result.rename(columns={0:self.data_name}, inplace=True)\n            else:\n                # Get mean prediction and simulations (for errors)\n                if self.latent_variables.estimation_method not in ['M-H']:\n                    sim_values = np.zeros([15000,h])\n\n                    if intervals is True:\n                        for n in range(0,15000):\n                            sim_values[n,:] = self.family.draw_variable(self.link(theta_pred),model_scale,model_shape,model_skewness,theta_pred.shape[0])\n\n                    sim_values = sim_values.T\n\n                    prediction_01 = np.array([np.percentile(i, 1) for i in sim_values])\n                    prediction_05 = np.array([np.percentile(i, 5) for i in sim_values])\n                    prediction_95 = np.array([np.percentile(i, 95) for i in sim_values])\n                    prediction_99 = np.array([np.percentile(i, 99) for i in sim_values])\n\n                result = pd.DataFrame([forecasted_values, prediction_01, prediction_05, \n                    prediction_95, prediction_99]).T\n                result.rename(columns={0:self.data_name, 1: \"1% Prediction Interval\", \n                    2: \"5% Prediction Interval\", 3: \"95% Prediction Interval\", 4: \"99% Prediction Interval\"}, \n                    inplace=True)\n \n            result.index = date_index[-h:]\n\n            return result", "response": "Makes a forecast with the estimated model and returns the predicted values."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_geometry(self):\n        gf = vtk.vtkCompositeDataGeometryFilter()\n        gf.SetInputData(self)\n        gf.Update()\n        return wrap(gf.GetOutputDataObject(0))", "response": "Combines the geomertry of all blocks into a single polydata object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef combine(self, merge_points=False):\n        alg = vtk.vtkAppendFilter()\n        for block in self:\n            alg.AddInputData(block)\n        alg.SetMergePoints(merge_points)\n        alg.Update()\n        return wrap(alg.GetOutputDataObject(0))", "response": "Appends all blocks into a single unstructured grid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload a vtkMultiBlockDataSet from a file.", "response": "def _load_file(self, filename):\n        \"\"\"Load a vtkMultiBlockDataSet from a file (extension ``.vtm`` or\n        ``.vtmb``)\n        \"\"\"\n        filename = os.path.abspath(os.path.expanduser(filename))\n        # test if file exists\n        if not os.path.isfile(filename):\n            raise Exception('File %s does not exist' % filename)\n\n        # Get extension\n        ext = vtki.get_ext(filename)\n        # Extensions: .vtm and .vtmb\n\n        # Select reader\n        if ext in ['.vtm', '.vtmb']:\n            reader = vtk.vtkXMLMultiBlockDataReader()\n        else:\n            raise IOError('File extension must be either \"vtm\" or \"vtmb\"')\n\n        # Load file\n        reader.SetFileName(filename)\n        reader.Update()\n        self.ShallowCopy(reader.GetOutput())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save(self, filename, binary=True):\n        filename = os.path.abspath(os.path.expanduser(filename))\n        ext = vtki.get_ext(filename)\n        if ext in ['.vtm', '.vtmb']:\n            writer = vtk.vtkXMLMultiBlockDataWriter()\n        else:\n            raise Exception('File extension must be either \"vtm\" or \"vtmb\"')\n\n        writer.SetFileName(filename)\n        writer.SetInputDataObject(self)\n        if binary:\n            writer.SetDataModeToBinary()\n        else:\n            writer.SetDataModeToAscii()\n        writer.Write()\n        return", "response": "Writes a MultiBlock dataset to disk."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bounds(self):\n        bounds = [np.inf,-np.inf, np.inf,-np.inf, np.inf,-np.inf]\n\n        def update_bounds(ax, nb, bounds):\n            \"\"\"internal helper to update bounds while keeping track\"\"\"\n            if nb[2*ax] < bounds[2*ax]:\n                bounds[2*ax] = nb[2*ax]\n            if nb[2*ax+1] > bounds[2*ax+1]:\n                bounds[2*ax+1] = nb[2*ax+1]\n            return bounds\n\n        # get bounds for each block and update\n        for i in range(self.n_blocks):\n            try:\n                bnds = self[i].GetBounds()\n                for a in range(3):\n                    bounds = update_bounds(a, bnds, bounds)\n            except AttributeError:\n                # Data object doesn't have bounds or is None\n                pass\n\n        return bounds", "response": "Finds min max for each block and returns the tuple of min max along each axis"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the min max of a scalar given its name across all blocks", "response": "def get_data_range(self, name):\n        \"\"\"Gets the min/max of a scalar given its name across all blocks\"\"\"\n        mini, maxi = np.inf, -np.inf\n        for i in range(self.n_blocks):\n            data = self[i]\n            if data is None:\n                continue\n            # get the scalar if availble\n            arr = get_scalar(data, name)\n            if arr is None:\n                continue\n            tmi, tma = np.nanmin(arr), np.nanmax(arr)\n            if tmi < mini:\n                mini = tmi\n            if tma > maxi:\n                maxi = tma\n        return mini, maxi"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind the index number by name", "response": "def get_index_by_name(self, name):\n        \"\"\"Find the index number by block name\"\"\"\n        for i in range(self.n_blocks):\n            if self.get_block_name(i) == name:\n                return i\n        raise KeyError('Block name ({}) not found'.format(name))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef append(self, data):\n        index = self.n_blocks # note off by one so use as index\n        self[index] = data\n        self.refs.append(data)", "response": "Add a data set to the next block index"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_block_name(self, index, name):\n        if name is None:\n            return\n        self.GetMetaData(index).Set(vtk.vtkCompositeDataSet.NAME(), name)\n        self.Modified()", "response": "Set a block s string name at the specified index"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the string name of the block at the given index", "response": "def get_block_name(self, index):\n        \"\"\"Returns the string name of the block at the given index\"\"\"\n        meta = self.GetMetaData(index)\n        if meta is not None:\n            return meta.Get(vtk.vtkCompositeDataSet.NAME())\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget all the block names in the dataset", "response": "def keys(self):\n        \"\"\"Get all the block names in the dataset\"\"\"\n        names = []\n        for i in range(self.n_blocks):\n            names.append(self.get_block_name(i))\n        return names"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef next(self):\n        if self._iter_n < self.n_blocks:\n            result = self[self._iter_n]\n            self._iter_n += 1\n            return result\n        else:\n            raise StopIteration", "response": "Get the next block from the iterator"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_attrs(self):\n        attrs = []\n        attrs.append((\"N Blocks\", self.n_blocks, \"{}\"))\n        bds = self.bounds\n        attrs.append((\"X Bounds\", (bds[0], bds[1]), \"{:.3f}, {:.3f}\"))\n        attrs.append((\"Y Bounds\", (bds[2], bds[3]), \"{:.3f}, {:.3f}\"))\n        attrs.append((\"Z Bounds\", (bds[4], bds[5]), \"{:.3f}, {:.3f}\"))\n        return attrs", "response": "An internal helper for the representation methods"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntranslating a mesh centered at the origin and oriented at the x direction.", "response": "def translate(surf, center, direction):\n    \"\"\"\n    Translates and orientates a mesh centered at the origin and\n    facing in the x direction to a new center and direction\n    \"\"\"\n    normx = np.array(direction)/np.linalg.norm(direction)\n    normz = np.cross(normx, [0, 1.0, 0.0000001])\n    normz /= np.linalg.norm(normz)\n    normy = np.cross(normz, normx)\n\n    trans = np.zeros((4, 4))\n    trans[:3, 0] = normx\n    trans[:3, 1] = normy\n    trans[:3, 2] = normz\n    trans[3, 3] = 1\n\n    surf.transform(trans)\n    surf.points += np.array(center)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new cylinder surface.", "response": "def Cylinder(center=(0.,0.,0.), direction=(1.,0.,0.), radius=0.5, height=1.0,\n             resolution=100, **kwargs):\n\n    \"\"\"\n    Create the surface of a cylinder.\n\n    Parameters\n    ----------\n    center : list or np.ndarray\n        Location of the centroid in [x, y, z]\n\n    direction : list or np.ndarray\n        Direction cylinder points to  in [x, y, z]\n\n    radius : float\n        Radius of the cylinder.\n\n    height : float\n        Height of the cylinder.\n\n    resolution : int\n        Number of points on the circular face of the cylinder.\n\n    capping : bool, optional\n        Cap cylinder ends with polygons.  Default True\n\n    Returns\n    -------\n    cylinder : vtki.PolyData\n        Cylinder surface.\n\n    Examples\n    --------\n    >>> import vtki\n    >>> import numpy as np\n    >>> cylinder = vtki.Cylinder(np.array([1, 2, 3]), np.array([1, 1, 1]), 1, 1)\n    >>> cylinder.plot() # doctest:+SKIP\n\n    \"\"\"\n    capping = kwargs.get('capping', kwargs.get('cap_ends', True))\n    cylinderSource = vtk.vtkCylinderSource()\n    cylinderSource.SetRadius(radius)\n    cylinderSource.SetHeight(height)\n    cylinderSource.SetCapping(capping)\n    cylinderSource.SetResolution(resolution)\n    cylinderSource.Update()\n    surf = PolyData(cylinderSource.GetOutput())\n    surf.rotate_z(-90)\n    translate(surf, center, direction)\n    return surf"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Arrow(start=(0.,0.,0.), direction=(1.,0.,0.), tip_length=0.25,\n          tip_radius=0.1, shaft_radius=0.05, shaft_resolution=20):\n    \"\"\"\n    Create a vtk Arrow\n\n    Parameters\n    ----------\n    start : np.ndarray\n        Start location in [x, y, z]\n\n    direction : list or np.ndarray\n        Direction the arrow points to in [x, y, z]\n\n    tip_length : float, optional\n        Length of the tip.\n\n    tip_radius : float, optional\n        Radius of the tip.\n\n    shaft_radius : float, optional\n        Radius of the shaft.\n\n    shaft_resolution : int, optional\n        Number of faces around the shaft\n\n    Returns\n    -------\n    arrow : vtki.PolyData\n        Arrow surface.\n    \"\"\"\n    # Create arrow object\n    arrow = vtk.vtkArrowSource()\n    arrow.SetTipLength(tip_length)\n    arrow.SetTipRadius(tip_radius)\n    arrow.SetShaftRadius(shaft_radius)\n    arrow.SetShaftResolution(shaft_resolution)\n    arrow.Update()\n    surf = PolyData(arrow.GetOutput())\n    translate(surf, start, direction)\n    return surf", "response": "Create an Arrow object for a single resource."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new sphere with the specified radius center and direction.", "response": "def Sphere(radius=0.5, center=(0, 0, 0), direction=(0, 0, 1), theta_resolution=30,\n           phi_resolution=30, start_theta=0, end_theta=360, start_phi=0, end_phi=180):\n    \"\"\"\n    Create a vtk Sphere\n\n    Parameters\n    ----------\n    radius : float, optional\n        Sphere radius\n\n    center : np.ndarray or list, optional\n        Center in [x, y, z]\n\n    direction : list or np.ndarray\n        Direction the top of the sphere points to in [x, y, z]\n\n    theta_resolution: int , optional\n        Set the number of points in the longitude direction (ranging from\n        start_theta to end theta).\n\n    phi_resolution : int, optional\n        Set the number of points in the latitude direction (ranging from\n        start_phi to end_phi).\n\n    start_theta : float, optional\n        Starting longitude angle.\n\n    end_theta : float, optional\n        Ending longitude angle.\n\n    start_phi : float, optional\n        Starting latitude angle.\n\n    end_phi : float, optional\n        Ending latitude angle.\n\n    Returns\n    -------\n    sphere : vtki.PolyData\n        Sphere mesh.\n    \"\"\"\n    sphere = vtk.vtkSphereSource()\n    sphere.SetRadius(radius)\n    sphere.SetThetaResolution(theta_resolution)\n    sphere.SetPhiResolution(phi_resolution)\n    sphere.SetStartTheta(start_theta)\n    sphere.SetEndTheta(end_theta)\n    sphere.SetStartPhi(start_phi)\n    sphere.SetEndPhi(end_phi)\n    sphere.Update()\n    surf = PolyData(sphere.GetOutput())\n    surf.rotate_y(-90)\n    translate(surf, center, direction)\n    return surf"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a plane in a bunch of cylinders.", "response": "def Plane(center=(0, 0, 0), direction=(0, 0, 1), i_size=1, j_size=1,\n          i_resolution=10, j_resolution=10):\n    \"\"\"\n    Create a plane\n\n    Parameters\n    ----------\n    center : list or np.ndarray\n        Location of the centroid in [x, y, z]\n\n    direction : list or np.ndarray\n        Direction cylinder points to  in [x, y, z]\n\n    i_size : float\n        Size of the plane in the i direction.\n\n    j_size : float\n        Size of the plane in the i direction.\n\n    i_resolution : int\n        Number of points on the plane in the i direction.\n\n    j_resolution : int\n        Number of points on the plane in the j direction.\n\n    Returns\n    -------\n    plane : vtki.PolyData\n        Plane mesh\n\n    \"\"\"\n    planeSource = vtk.vtkPlaneSource()\n    planeSource.SetXResolution(i_resolution)\n    planeSource.SetYResolution(j_resolution)\n    planeSource.Update()\n\n    surf = PolyData(planeSource.GetOutput())\n\n    surf.points[:, 0] *= i_size\n    surf.points[:, 1] *= j_size\n    surf.rotate_y(-90)\n    translate(surf, center, direction)\n    return surf"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Line(pointa=(-0.5, 0., 0.), pointb=(0.5, 0., 0.), resolution=1):\n    if np.array(pointa).size != 3:\n        raise TypeError('Point A must be a length three tuple of floats.')\n    if np.array(pointb).size != 3:\n        raise TypeError('Point B must be a length three tuple of floats.')\n    src = vtk.vtkLineSource()\n    src.SetPoint1(*pointa)\n    src.SetPoint2(*pointb)\n    src.SetResolution(resolution)\n    src.Update()\n    return vtki.wrap(src.GetOutput())", "response": "Create a line in a\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new cube.", "response": "def Cube(center=(0., 0., 0.), x_length=1.0, y_length=1.0, z_length=1.0, bounds=None):\n    \"\"\"Create a cube by either specifying the center and side lengths or just\n    the bounds of the cube. If ``bounds`` are given, all other arguments are\n    ignored.\n\n    Parameters\n    ----------\n    center : np.ndarray or list\n        Center in [x, y, z].\n\n    x_length : float\n        length of the cube in the x-direction.\n\n    y_length : float\n        length of the cube in the y-direction.\n\n    z_length : float\n        length of the cube in the z-direction.\n\n    bounds : np.ndarray or list\n        Specify the bounding box of the cube. If given, all other arguments are\n        ignored. ``(xMin,xMax, yMin,yMax, zMin,zMax)``\n    \"\"\"\n    src = vtk.vtkCubeSource()\n    if bounds is not None:\n        if np.array(bounds).size != 6:\n            raise TypeError('Bounds must be given as length 6 tuple: (xMin,xMax, yMin,yMax, zMin,zMax)')\n        src.SetBounds(bounds)\n    else:\n        src.SetCenter(center)\n        src.SetXLength(x_length)\n        src.SetYLength(y_length)\n        src.SetZLength(z_length)\n    src.Update()\n    return vtki.wrap(src.GetOutput())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new cone in a single axis.", "response": "def Cone(center=(0., 0., 0.), direction=(1., 0., 0.), height=1.0, radius=0.5,\n         capping=True, angle=26.6, resolution=6):\n    \"\"\"Create a cone\n\n    Parameters\n    ----------\n    center : np.ndarray or list\n        Center in [x, y, z]. middle of the axis of the cone.\n\n    direction : np.ndarray or list\n        direction vector in [x, y, z]. orientation vector of the cone.\n\n    height : float\n        height along the cone in its specified direction.\n\n    radius : float\n        base radius of the cone\n\n    capping : bool\n        Turn on/off whether to cap the base of the cone with a polygon.\n\n    angle : float\n        The angle degrees between the axis of the cone and a generatrix.\n\n    resolution : int\n        number of facets used to represent the cone\n    \"\"\"\n    src = vtk.vtkConeSource()\n    src.SetAngle(angle)\n    src.SetCapping(capping)\n    src.SetCenter(center)\n    src.SetHeight(height)\n    src.SetRadius(radius)\n    src.SetResolution(resolution)\n    src.Update()\n    return vtki.wrap(src.GetOutput())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Polygon(center=(0.,0.,0.), radius=1, normal=(0,0,1), n_sides=6):\n    src = vtk.vtkRegularPolygonSource()\n    src.SetCenter(center)\n    src.SetNumberOfSides(n_sides)\n    src.SetRadius(radius)\n    src.SetNormal(normal)\n    src.Update()\n    return vtki.wrap(src.GetOutput())", "response": "Create a polygonal disk with a hole."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a polygonal disk with a hole in the center.", "response": "def Disc(center=(0.,0.,0.), inner=0.25, outer=0.5, normal=(0,0,1), r_res=1,\n         c_res=6):\n    \"\"\"\n    Createa a polygonal disk with a hole in the center. The disk has zero\n    height. The user can specify the inner and outer radius of the disk, and\n    the radial and circumferential resolution of the polygonal representation.\n\n    Parameters\n    ----------\n    center : np.ndarray or list\n        Center in [x, y, z]. middle of the axis of the disc.\n\n    inner : flaot\n        The inner radius\n\n    outer : float\n        The outer radius\n\n    normal : np.ndarray or list\n        direction vector in [x, y, z]. orientation vector of the cone.\n\n    r_res: int\n        number of points in radius direction.\n\n    r_res: int\n        number of points in circumferential direction.\n    \"\"\"\n    src = vtk.vtkDiskSource()\n    src.SetInnerRadius(inner)\n    src.SetOuterRadius(outer)\n    src.SetRadialResolution(r_res)\n    src.SetCircumferentialResolution(c_res)\n    src.Update()\n    return vtki.wrap(src.GetOutput())"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nuses a given reader from the READERS mapping in the common VTK reading tree.", "response": "def standard_reader_routine(reader, filename, attrs=None):\n    \"\"\"Use a given reader from the ``READERS`` mapping in the common VTK reading\n    pipeline routine.\n\n    Parameters\n    ----------\n    reader : vtkReader\n        Any instantiated VTK reader class\n\n    filename : str\n        The string filename to the data file to read.\n\n    attrs : dict, optional\n        A dictionary of attributes to call on the reader. Keys of dictionary are\n        the attribute/method names and values are the arguments passed to those\n        calls. If you do not have any attributes to call, pass ``None`` as the\n        value.\n    \"\"\"\n    if attrs is None:\n        attrs = {}\n    if not isinstance(attrs, dict):\n        raise TypeError('Attributes must be a dictionary of name and arguments.')\n    reader.SetFileName(filename)\n    # Apply any attributes listed\n    for name, args in attrs.items():\n        attr = getattr(reader, name)\n        if args is not None:\n            if not isinstance(args, (list, tuple)):\n                args = [args]\n            attr(*args)\n        else:\n            attr()\n    # Perform the read\n    reader.Update()\n    return vtki.wrap(reader.GetOutputDataObject(0))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nuses VTK s legacy reader to read a file", "response": "def read_legacy(filename):\n    \"\"\"Use VTK's legacy reader to read a file\"\"\"\n    reader = vtk.vtkDataSetReader()\n    reader.SetFileName(filename)\n    # Ensure all data is fetched with poorly formated legacy files\n    reader.ReadAllScalarsOn()\n    reader.ReadAllColorScalarsOn()\n    reader.ReadAllNormalsOn()\n    reader.ReadAllTCoordsOn()\n    reader.ReadAllVectorsOn()\n    # Perform the read\n    reader.Update()\n    output = reader.GetOutputDataObject(0)\n    if output is None:\n        raise AssertionError('No output when using VTKs legacy reader')\n    return vtki.wrap(output)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_texture(filename, attrs=None):\n    filename = os.path.abspath(os.path.expanduser(filename))\n    try:\n        # intitialize the reader using the extnesion to find it\n        reader = get_reader(filename)\n        image = standard_reader_routine(reader, filename, attrs=attrs)\n        return vtki.image_to_texture(image)\n    except KeyError:\n        # Otherwise, use the imageio reader\n        pass\n    return vtki.numpy_to_texture(imageio.imread(filename))", "response": "Loads a vtkTexture from an image file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads an ExodusII file and returns a vtki. vtkObject.", "response": "def read_exodus(filename,\n                animate_mode_shapes=True,\n                apply_displacements=True,\n                displacement_magnitude=1.0,\n                enabled_sidesets=None):\n    \"\"\"Read an ExodusII file (``'.e'`` or ``'.exo'``)\"\"\"\n    reader = vtk.vtkExodusIIReader()\n    reader.SetFileName(filename)\n    reader.UpdateInformation()\n    reader.SetAnimateModeShapes(animate_mode_shapes)\n    reader.SetApplyDisplacements(apply_displacements)\n    reader.SetDisplacementMagnitude(displacement_magnitude)\n\n    if enabled_sidesets is None:\n        enabled_sidesets = list(range(reader.GetNumberOfSideSetArrays()))\n\n    for sideset in enabled_sidesets:\n        if isinstance(sideset, int):\n            name = reader.GetSideSetArrayName(sideset)\n        elif isinstance(sideset, str):\n            name = sideset\n        else:\n            raise ValueError('Could not parse sideset ID/name: {}'.format(sideset))\n\n        reader.SetSideSetArrayStatus(name, 1)\n\n    reader.Update()\n    return vtki.wrap(reader.GetOutput())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_downloads():\n    shutil.rmtree(vtki.EXAMPLES_PATH)\n    os.makedirs(vtki.EXAMPLES_PATH)\n    return True", "response": "Delete all downloaded examples to free space or update the files"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndownloads the blood vessels and returns a vtki. Mesh", "response": "def download_blood_vessels():\n    \"\"\"data representing the bifurcation of blood vessels.\"\"\"\n    local_path, _ = _download_file('pvtu_blood_vessels/blood_vessels.zip')\n    filename = os.path.join(local_path, 'T0000000500.pvtu')\n    mesh = vtki.read(filename)\n    mesh.set_active_vectors('velocity')\n    return mesh"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef download_kitchen(split=False):\n    mesh =  _download_and_read('kitchen.vtk')\n    if not split:\n        return mesh\n    extents = {\n        'door' : (27, 27, 14, 18, 0, 11),\n        'window1' : (0, 0, 9, 18, 6, 12),\n        'window2' : (5, 12, 23, 23, 6, 12),\n        'klower1' : (17, 17, 0, 11, 0, 6),\n        'klower2' : (19, 19, 0, 11, 0, 6),\n        'klower3' : (17, 19, 0, 0, 0, 6),\n        'klower4' : (17, 19, 11, 11, 0, 6),\n        'klower5' : (17, 19, 0, 11, 0, 0),\n        'klower6' : (17, 19, 0, 7, 6, 6),\n        'klower7' : (17, 19, 9, 11, 6, 6),\n        'hood1' : (17, 17, 0, 11, 11, 16),\n        'hood2' : (19, 19, 0, 11, 11, 16),\n        'hood3' : (17, 19, 0, 0, 11, 16),\n        'hood4' : (17, 19, 11, 11, 11, 16),\n        'hood5' : (17, 19, 0, 11, 16, 16),\n        'cookingPlate' : (17, 19, 7, 9, 6, 6),\n        'furniture' : (17, 19, 7, 9, 11, 11),\n    }\n    kitchen = vtki.MultiBlock()\n    for key, extent in extents.items():\n        alg = vtk.vtkStructuredGridGeometryFilter()\n        alg.SetInputDataObject(mesh)\n        alg.SetExtent(extent)\n        alg.Update()\n        result = vtki.filters._get_output(alg)\n        kitchen[key] = result\n    return kitchen", "response": "Download a structured grid of the kitchen with velocity field."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_scalar_names(self, limit=None):\n        names = []\n        if limit == 'point':\n            inpnames = list(self.input_dataset.point_arrays.keys())\n        elif limit == 'cell':\n            inpnames = list(self.input_dataset.cell_arrays.keys())\n        else:\n            inpnames = self.input_dataset.scalar_names\n        for name in inpnames:\n            arr = self.input_dataset.get_scalar(name)\n            rng = self.input_dataset.get_data_range(name)\n            if arr is not None and arr.size > 0 and (rng[1]-rng[0] > 0.0):\n                names.append(name)\n        try:\n            self._last_scalars = names[0]\n        except IndexError:\n            pass\n        return names", "response": "Only give scalar options that have a varying range"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _initialize(self, show_bounds, reset_camera, outline):\n        self.plotter.subplot(*self.loc)\n        if outline is None:\n            self.plotter.add_mesh(self.input_dataset.outline_corners(),\n                    reset_camera=False, color=vtki.rcParams['outline_color'],\n                    loc=self.loc)\n        elif outline:\n            self.plotter.add_mesh(self.input_dataset.outline(),\n                    reset_camera=False, color=vtki.rcParams['outline_color'],\n                    loc=self.loc)\n        # add the axis labels\n        if show_bounds:\n            self.plotter.show_bounds(reset_camera=False, loc=loc)\n        if reset_camera:\n            cpos = self.plotter.get_default_cam_pos()\n            self.plotter.camera_position = cpos\n            self.plotter.reset_camera()\n            self.plotter.camera_set = False", "response": "Initializes the plotter and sets up the scene"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the plotting parameters of the current object.", "response": "def _update_plotting_params(self, **kwargs):\n        \"\"\"Some plotting parameters can be changed through the tool; this\n        updataes those plotting parameters.\n        \"\"\"\n        scalars = kwargs.get('scalars', None)\n        if scalars is not None:\n            old = self.display_params['scalars']\n            self.display_params['scalars'] = scalars\n            if old != scalars:\n                self.plotter.subplot(*self.loc)\n                self.plotter.remove_actor(self._data_to_update, reset_camera=False)\n                self._need_to_update = True\n                self.valid_range = self.input_dataset.get_data_range(scalars)\n                # self.display_params['rng'] = self.valid_range\n        cmap = kwargs.get('cmap', None)\n        if cmap is not None:\n            self.display_params['cmap'] = cmap"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _remove_mapper_from_plotter(plotter, actor, reset_camera):\n    try:\n        mapper = actor.GetMapper()\n    except AttributeError:\n        return\n    for name in list(plotter._scalar_bar_mappers.keys()):\n        try:\n            plotter._scalar_bar_mappers[name].remove(mapper)\n        except ValueError:\n            pass\n        if len(plotter._scalar_bar_mappers[name]) < 1:\n            slot = plotter._scalar_bar_slot_lookup.pop(name)\n            plotter._scalar_bar_mappers.pop(name)\n            plotter._scalar_bar_ranges.pop(name)\n            plotter.remove_actor(plotter._scalar_bar_actors.pop(name), reset_camera=reset_camera)\n            plotter._scalar_bar_slots.add(slot)\n    return", "response": "removes this actor s mapper from the given plotter s _scalar_bar_mappers and _scalar_bar_slot_lookup"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd an actor to the render window.", "response": "def add_actor(self, uinput, reset_camera=False, name=None, loc=None,\n                  culling=False):\n        \"\"\"\n        Adds an actor to render window.  Creates an actor if input is\n        a mapper.\n\n        Parameters\n        ----------\n        uinput : vtk.vtkMapper or vtk.vtkActor\n            vtk mapper or vtk actor to be added.\n\n        reset_camera : bool, optional\n            Resets the camera when true.\n\n        loc : int, tuple, or list\n            Index of the renderer to add the actor to.  For example,\n            ``loc=2`` or ``loc=(1, 1)``.\n\n        culling : bool optional\n            Does not render faces that should not be visible to the\n            plotter.  This can be helpful for dense surface meshes,\n            especially when edges are visible, but can cause flat\n            meshes to be partially displayed.  Default False.\n\n        Returns\n        -------\n        actor : vtk.vtkActor\n            The actor.\n\n        actor_properties : vtk.Properties\n            Actor properties.\n\n        \"\"\"\n        # Remove actor by that name if present\n        rv = self.remove_actor(name, reset_camera=False)\n\n        if isinstance(uinput, vtk.vtkMapper):\n            actor = vtk.vtkActor()\n            actor.SetMapper(uinput)\n        else:\n            actor = uinput\n\n        self.AddActor(actor)\n        actor.renderer = proxy(self)\n\n        if name is None:\n            name = str(hex(id(actor)))\n\n        self._actors[name] = actor\n\n        if reset_camera:\n            self.reset_camera()\n        elif not self.camera_set and reset_camera is None and not rv:\n            self.reset_camera()\n        else:\n            self.parent._render()\n\n        self.update_bounds_axes()\n\n        if culling:\n            try:\n                actor.GetProperty().BackfaceCullingOn()\n            except AttributeError:  # pragma: no cover\n                pass\n\n        return actor, actor.GetProperty()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_axes_at_origin(self):\n        self.marker_actor = vtk.vtkAxesActor()\n        # renderer = self.renderers[self.loc_to_index(loc)]\n        self.AddActor(self.marker_actor)\n        self.parent._actors[str(hex(id(self.marker_actor)))] = self.marker_actor\n        return self.marker_actor", "response": "Add axes actor at origin"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef show_bounds(self, mesh=None, bounds=None, show_xaxis=True,\n                    show_yaxis=True, show_zaxis=True, show_xlabels=True,\n                    show_ylabels=True, show_zlabels=True, italic=False,\n                    bold=True, shadow=False, font_size=None,\n                    font_family=None, color=None,\n                    xlabel='X Axis', ylabel='Y Axis', zlabel='Z Axis',\n                    use_2d=False, grid=None, location='closest', ticks=None,\n                    all_edges=False, corner_factor=0.5, loc=None, fmt=None,\n                    minor_ticks=False, padding=0.0):\n        \"\"\"\n        Adds bounds axes.  Shows the bounds of the most recent input\n        mesh unless mesh is specified.\n\n        Parameters\n        ----------\n        mesh : vtkPolydata or unstructured grid, optional\n            Input mesh to draw bounds axes around\n\n        bounds : list or tuple, optional\n            Bounds to override mesh bounds.\n            [xmin, xmax, ymin, ymax, zmin, zmax]\n\n        show_xaxis : bool, optional\n            Makes x axis visible.  Default True.\n\n        show_yaxis : bool, optional\n            Makes y axis visible.  Default True.\n\n        show_zaxis : bool, optional\n            Makes z axis visible.  Default True.\n\n        show_xlabels : bool, optional\n            Shows x labels.  Default True.\n\n        show_ylabels : bool, optional\n            Shows y labels.  Default True.\n\n        show_zlabels : bool, optional\n            Shows z labels.  Default True.\n\n        italic : bool, optional\n            Italicises axis labels and numbers.  Default False.\n\n        bold : bool, optional\n            Bolds axis labels and numbers.  Default True.\n\n        shadow : bool, optional\n            Adds a black shadow to the text.  Default False.\n\n        font_size : float, optional\n            Sets the size of the label font.  Defaults to 16.\n\n        font_family : string, optional\n            Font family.  Must be either courier, times, or arial.\n\n        color : string or 3 item list, optional\n            Color of all labels and axis titles.  Default white.\n            Either a string, rgb list, or hex color string.  For example:\n\n                color='white'\n                color='w'\n                color=[1, 1, 1]\n                color='#FFFFFF'\n\n        xlabel : string, optional\n            Title of the x axis.  Default \"X Axis\"\n\n        ylabel : string, optional\n            Title of the y axis.  Default \"Y Axis\"\n\n        zlabel : string, optional\n            Title of the z axis.  Default \"Z Axis\"\n\n        use_2d : bool, optional\n            A bug with vtk 6.3 in Windows seems to cause this function\n            to crash this can be enabled for smoother plotting for\n            other enviornments.\n\n        grid : bool or str, optional\n            Add grid lines to the backface (``True``, ``'back'``, or\n            ``'backface'``) or to the frontface (``'front'``,\n            ``'frontface'``) of the axes actor.\n\n        location : str, optional\n            Set how the axes are drawn: either static (``'all'``),\n            closest triad (``front``), furthest triad (``'back'``),\n            static closest to the origin (``'origin'``), or outer\n            edges (``'outer'``) in relation to the camera\n            position. Options include: ``'all', 'front', 'back',\n            'origin', 'outer'``\n\n        ticks : str, optional\n            Set how the ticks are drawn on the axes grid. Options include:\n            ``'inside', 'outside', 'both'``\n\n        all_edges : bool, optional\n            Adds an unlabeled and unticked box at the boundaries of\n            plot. Useful for when wanting to plot outer grids while\n            still retaining all edges of the boundary.\n\n        corner_factor : float, optional\n            If ``all_edges````, this is the factor along each axis to\n            draw the default box. Dafuault is 0.5 to show the full box.\n\n        loc : int, tuple, or list\n            Index of the renderer to add the actor to.  For example,\n            ``loc=2`` or ``loc=(1, 1)``.  If None, selects the last\n            active Renderer.\n\n        padding : float, optional\n            An optional percent padding along each axial direction to cushion\n            the datasets in the scene from the axes annotations. Defaults to\n            have no padding\n\n        Returns\n        -------\n        cube_axes_actor : vtk.vtkCubeAxesActor\n            Bounds actor\n\n        Examples\n        --------\n        >>> import vtki\n        >>> from vtki import examples\n        >>> mesh = vtki.Sphere()\n        >>> plotter = vtki.Plotter()\n        >>> _ = plotter.add_mesh(mesh)\n        >>> _ = plotter.show_bounds(grid='front', location='outer', all_edges=True)\n        >>> plotter.show() # doctest:+SKIP\n        \"\"\"\n        self.remove_bounds_axes()\n\n        if font_family is None:\n            font_family = rcParams['font']['family']\n        if font_size is None:\n            font_size = rcParams['font']['size']\n        if color is None:\n            color = rcParams['font']['color']\n        if fmt is None:\n            fmt = rcParams['font']['fmt']\n\n        color = parse_color(color)\n\n        # Use the bounds of all data in the rendering window\n        if not mesh and not bounds:\n            bounds = self.bounds\n\n        # create actor\n        cube_axes_actor = vtk.vtkCubeAxesActor()\n        if use_2d or not np.allclose(self.scale, [1.0, 1.0, 1.0]):\n            cube_axes_actor.SetUse2DMode(True)\n        else:\n            cube_axes_actor.SetUse2DMode(False)\n\n        if grid:\n            if isinstance(grid, str) and grid.lower() in ('front', 'frontface'):\n                cube_axes_actor.SetGridLineLocation(cube_axes_actor.VTK_GRID_LINES_CLOSEST)\n            if isinstance(grid, str) and grid.lower() in ('both', 'all'):\n                cube_axes_actor.SetGridLineLocation(cube_axes_actor.VTK_GRID_LINES_ALL)\n            else:\n                cube_axes_actor.SetGridLineLocation(cube_axes_actor.VTK_GRID_LINES_FURTHEST)\n            cube_axes_actor.DrawXGridlinesOn()\n            cube_axes_actor.DrawYGridlinesOn()\n            cube_axes_actor.DrawZGridlinesOn()\n            # Set the colors\n            cube_axes_actor.GetXAxesGridlinesProperty().SetColor(color)\n            cube_axes_actor.GetYAxesGridlinesProperty().SetColor(color)\n            cube_axes_actor.GetZAxesGridlinesProperty().SetColor(color)\n\n        if isinstance(ticks, str):\n            ticks = ticks.lower()\n            if ticks in ('inside'):\n                cube_axes_actor.SetTickLocationToInside()\n            elif ticks in ('outside'):\n                cube_axes_actor.SetTickLocationToOutside()\n            elif ticks in ('both'):\n                cube_axes_actor.SetTickLocationToBoth()\n            else:\n                raise ValueError('Value of ticks ({}) not understood.'.format(ticks))\n\n        if isinstance(location, str):\n            location = location.lower()\n            if location in ('all'):\n                cube_axes_actor.SetFlyModeToStaticEdges()\n            elif location in ('origin'):\n                cube_axes_actor.SetFlyModeToStaticTriad()\n            elif location in ('outer'):\n                cube_axes_actor.SetFlyModeToOuterEdges()\n            elif location in ('default', 'closest', 'front'):\n                cube_axes_actor.SetFlyModeToClosestTriad()\n            elif location in ('furthest', 'back'):\n                cube_axes_actor.SetFlyModeToFurthestTriad()\n            else:\n                raise ValueError('Value of location ({}) not understood.'.format(location))\n\n        # set bounds\n        if not bounds:\n            bounds = np.array(mesh.GetBounds())\n        if isinstance(padding, (int, float)) and 0.0 <= padding < 1.0:\n            if not np.any(np.abs(bounds) == np.inf):\n                cushion = np.array([np.abs(bounds[1] - bounds[0]),\n                                    np.abs(bounds[3] - bounds[2]),\n                                    np.abs(bounds[5] - bounds[4])]) * padding\n                bounds[::2] -= cushion\n                bounds[1::2] += cushion\n        else:\n            raise ValueError('padding ({}) not understood. Must be float between 0 and 1'.format(padding))\n        cube_axes_actor.SetBounds(bounds)\n\n        # show or hide axes\n        cube_axes_actor.SetXAxisVisibility(show_xaxis)\n        cube_axes_actor.SetYAxisVisibility(show_yaxis)\n        cube_axes_actor.SetZAxisVisibility(show_zaxis)\n\n        # disable minor ticks\n        if not minor_ticks:\n            cube_axes_actor.XAxisMinorTickVisibilityOff()\n            cube_axes_actor.YAxisMinorTickVisibilityOff()\n            cube_axes_actor.ZAxisMinorTickVisibilityOff()\n\n        cube_axes_actor.SetCamera(self.camera)\n\n        # set color\n        cube_axes_actor.GetXAxesLinesProperty().SetColor(color)\n        cube_axes_actor.GetYAxesLinesProperty().SetColor(color)\n        cube_axes_actor.GetZAxesLinesProperty().SetColor(color)\n\n        # empty arr\n        empty_str = vtk.vtkStringArray()\n        empty_str.InsertNextValue('')\n\n        # show lines\n        if show_xaxis:\n            cube_axes_actor.SetXTitle(xlabel)\n        else:\n            cube_axes_actor.SetXTitle('')\n            cube_axes_actor.SetAxisLabels(0, empty_str)\n\n        if show_yaxis:\n            cube_axes_actor.SetYTitle(ylabel)\n        else:\n            cube_axes_actor.SetYTitle('')\n            cube_axes_actor.SetAxisLabels(1, empty_str)\n\n        if show_zaxis:\n            cube_axes_actor.SetZTitle(zlabel)\n        else:\n            cube_axes_actor.SetZTitle('')\n            cube_axes_actor.SetAxisLabels(2, empty_str)\n\n        # show labels\n        if not show_xlabels:\n            cube_axes_actor.SetAxisLabels(0, empty_str)\n\n        if not show_ylabels:\n            cube_axes_actor.SetAxisLabels(1, empty_str)\n\n        if not show_zlabels:\n            cube_axes_actor.SetAxisLabels(2, empty_str)\n\n        # set font\n        font_family = parse_font_family(font_family)\n        for i in range(3):\n            cube_axes_actor.GetTitleTextProperty(i).SetFontSize(font_size)\n            cube_axes_actor.GetTitleTextProperty(i).SetColor(color)\n            cube_axes_actor.GetTitleTextProperty(i).SetFontFamily(font_family)\n            cube_axes_actor.GetTitleTextProperty(i).SetBold(bold)\n\n            cube_axes_actor.GetLabelTextProperty(i).SetFontSize(font_size)\n            cube_axes_actor.GetLabelTextProperty(i).SetColor(color)\n            cube_axes_actor.GetLabelTextProperty(i).SetFontFamily(font_family)\n            cube_axes_actor.GetLabelTextProperty(i).SetBold(bold)\n\n        self.add_actor(cube_axes_actor, reset_camera=False)\n        self.cube_axes_actor = cube_axes_actor\n\n        if all_edges:\n            self.add_bounding_box(color=color, corner_factor=corner_factor)\n\n        if fmt is not None:\n            cube_axes_actor.SetXLabelFormat(fmt)\n            cube_axes_actor.SetYLabelFormat(fmt)\n            cube_axes_actor.SetZLabelFormat(fmt)\n\n        return cube_axes_actor", "response": "Displays the bounds of the most recent input object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_bounds_axes(self, *args, **kwargs):\n        logging.warning('`add_bounds_axes` is deprecated. Use `show_bounds` or `show_grid`.')\n        return self.show_bounds(*args, **kwargs)", "response": "Deprecated. Use show_bounds and show_grid."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving the bounding box from the actor list.", "response": "def remove_bounding_box(self):\n        \"\"\" Removes bounding box \"\"\"\n        if hasattr(self, '_box_object'):\n            actor = self.bounding_box_actor\n            self.bounding_box_actor = None\n            del self._box_object\n            self.remove_actor(actor, reset_camera=False)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_bounding_box(self, color=None, corner_factor=0.5, line_width=None,\n                         opacity=1.0, render_lines_as_tubes=False, lighting=None,\n                         reset_camera=None):\n        \"\"\"\n        Adds an unlabeled and unticked box at the boundaries of\n        plot.  Useful for when wanting to plot outer grids while\n        still retaining all edges of the boundary.\n\n        Parameters\n        ----------\n        corner_factor : float, optional\n            If ``all_edges``, this is the factor along each axis to\n            draw the default box. Dafuault is 0.5 to show the full\n            box.\n        \"\"\"\n        if lighting is None:\n            lighting = rcParams['lighting']\n\n        self.remove_bounding_box()\n        if color is None:\n            color = rcParams['font']['color']\n        rgb_color = parse_color(color)\n        self._bounding_box = vtk.vtkOutlineCornerSource()\n        self._bounding_box.SetBounds(self.bounds)\n        self._bounding_box.SetCornerFactor(corner_factor)\n        self._bounding_box.Update()\n        self._box_object = wrap(self._bounding_box.GetOutput())\n        name = 'BoundingBox({})'.format(hex(id(self._box_object)))\n\n        mapper = vtk.vtkDataSetMapper()\n        mapper.SetInputData(self._box_object)\n        self.bounding_box_actor, prop = self.add_actor(mapper,\n                                                       reset_camera=reset_camera,\n                                                       name=name)\n\n        prop.SetColor(rgb_color)\n        prop.SetOpacity(opacity)\n        if render_lines_as_tubes:\n            prop.SetRenderLinesAsTubes(render_lines_as_tubes)\n\n        # lighting display style\n        if lighting is False:\n            prop.LightingOff()\n\n        # set line thickness\n        if line_width:\n            prop.SetLineWidth(line_width)\n\n        prop.SetRepresentationToSurface()\n\n        return self.bounding_box_actor", "response": "Adds an unlabeled and unticked box at the boundaries of the plot."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the camera position of the active render window", "response": "def camera_position(self):\n        \"\"\" Returns camera position of active render window \"\"\"\n        return [self.camera.GetPosition(),\n                self.camera.GetFocalPoint(),\n                self.camera.GetViewUp()]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset camera position of all active render windows", "response": "def camera_position(self, camera_location):\n        \"\"\" Set camera position of all active render windows \"\"\"\n        if camera_location is None:\n            return\n\n        if isinstance(camera_location, str):\n            camera_location = camera_location.lower()\n            if camera_location == 'xy':\n                self.view_xy()\n            elif camera_location == 'xz':\n                self.view_xz()\n            elif camera_location == 'yz':\n                self.view_yz()\n            elif camera_location == 'yx':\n                self.view_xy(True)\n            elif camera_location == 'zx':\n                self.view_xz(True)\n            elif camera_location == 'zy':\n                self.view_yz(True)\n            return\n\n        if isinstance(camera_location[0], (int, float)):\n            return self.view_vector(camera_location)\n\n        # everything is set explicitly\n        self.camera.SetPosition(camera_location[0])\n        self.camera.SetFocalPoint(camera_location[1])\n        self.camera.SetViewUp(camera_location[2])\n\n        # reset clipping range\n        self.ResetCameraClippingRange()\n        self.camera_set = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves an actor from the Renderer.", "response": "def remove_actor(self, actor, reset_camera=False):\n        \"\"\"\n        Removes an actor from the Renderer.\n\n        Parameters\n        ----------\n        actor : vtk.vtkActor\n            Actor that has previously added to the Renderer.\n\n        reset_camera : bool, optional\n            Resets camera so all actors can be seen.\n\n        Returns\n        -------\n        success : bool\n            True when actor removed.  False when actor has not been\n            removed.\n        \"\"\"\n        name = None\n        if isinstance(actor, str):\n            name = actor\n            keys = list(self._actors.keys())\n            names = []\n            for k in keys:\n                if k.startswith('{}-'.format(name)):\n                    names.append(k)\n            if len(names) > 0:\n                self.remove_actor(names, reset_camera=reset_camera)\n            try:\n                actor = self._actors[name]\n            except KeyError:\n                # If actor of that name is not present then return success\n                return False\n        if isinstance(actor, collections.Iterable):\n            success = False\n            for a in actor:\n                rv = self.remove_actor(a, reset_camera=reset_camera)\n                if rv or success:\n                    success = True\n            return success\n        if actor is None:\n            return False\n\n        # First remove this actor's mapper from _scalar_bar_mappers\n        _remove_mapper_from_plotter(self.parent, actor, False)\n        self.RemoveActor(actor)\n\n        if name is None:\n            for k, v in self._actors.items():\n                if v == actor:\n                    name = k\n        self._actors.pop(name, None)\n        self.update_bounds_axes()\n        if reset_camera:\n            self.reset_camera()\n        elif not self.camera_set and reset_camera is None:\n            self.reset_camera()\n        else:\n            self.parent._render()\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_scale(self, xscale=None, yscale=None, zscale=None, reset_camera=True):\n        if xscale is None:\n            xscale = self.scale[0]\n        if yscale is None:\n            yscale = self.scale[1]\n        if zscale is None:\n            zscale = self.scale[2]\n        self.scale = [xscale, yscale, zscale]\n\n        # Update the camera's coordinate system\n        transform = vtk.vtkTransform()\n        transform.Scale(xscale, yscale, zscale)\n        self.camera.SetModelTransformMatrix(transform.GetMatrix())\n        self.parent._render()\n        if reset_camera:\n            self.update_bounds_axes()\n            self.reset_camera()", "response": "Set the scale of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bounds(self):\n        the_bounds = [np.inf, -np.inf, np.inf, -np.inf, np.inf, -np.inf]\n\n        def _update_bounds(bounds):\n            def update_axis(ax):\n                if bounds[ax*2] < the_bounds[ax*2]:\n                    the_bounds[ax*2] = bounds[ax*2]\n                if bounds[ax*2+1] > the_bounds[ax*2+1]:\n                    the_bounds[ax*2+1] = bounds[ax*2+1]\n            for ax in range(3):\n                update_axis(ax)\n            return\n\n        for actor in self._actors.values():\n            if isinstance(actor, vtk.vtkCubeAxesActor):\n                continue\n            if ( hasattr(actor, 'GetBounds') and actor.GetBounds() is not None\n                 and id(actor) != id(self.bounding_box_actor)):\n                _update_bounds(actor.GetBounds())\n\n        return the_bounds", "response": "Returns the bounds of all actors present in the rendering window"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef center(self):\n        bounds = self.bounds\n        x = (bounds[1] + bounds[0])/2\n        y = (bounds[3] + bounds[2])/2\n        z = (bounds[5] + bounds[4])/2\n        return [x, y, z]", "response": "Center of the bounding box around all data present in the scene"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the default focal points and viewup. Uses ResetCamera to make a useful view.", "response": "def get_default_cam_pos(self):\n        \"\"\"\n        Returns the default focal points and viewup. Uses ResetCamera to\n        make a useful view.\n        \"\"\"\n        focal_pt = self.center\n        return [np.array(rcParams['camera']['position']) + np.array(focal_pt),\n                focal_pt, rcParams['camera']['viewup']]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the bounds axes of the render window.", "response": "def update_bounds_axes(self):\n        \"\"\"Update the bounds axes of the render window \"\"\"\n        if (hasattr(self, '_box_object') and self._box_object is not None\n                and self.bounding_box_actor is not None):\n            if not np.allclose(self._box_object.bounds, self.bounds):\n                color = self.bounding_box_actor.GetProperty().GetColor()\n                self.remove_bounding_box()\n                self.add_bounding_box(color=color)\n        if hasattr(self, 'cube_axes_actor'):\n            self.cube_axes_actor.SetBounds(self.bounds)\n            if not np.allclose(self.scale, [1.0, 1.0, 1.0]):\n                self.cube_axes_actor.SetUse2DMode(True)\n            else:\n                self.cube_axes_actor.SetUse2DMode(False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef view_isometric(self):\n        self.camera_position = self.get_default_cam_pos()\n        self.camera_set = False\n        return self.reset_camera()", "response": "Resets the camera to a default isometric view showing all the actors in the scene."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npoints the camera in the direction of the given vector", "response": "def view_vector(self, vector, viewup=None):\n        \"\"\"Point the camera in the direction of the given vector\"\"\"\n        focal_pt = self.center\n        if viewup is None:\n            viewup = rcParams['camera']['viewup']\n        cpos = [vector + np.array(focal_pt),\n                focal_pt, viewup]\n        self.camera_position = cpos\n        return self.reset_camera()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef view_xy(self, negative=False):\n        vec = np.array([0,0,1])\n        viewup = np.array([0,1,0])\n        if negative:\n            vec = np.array([0,0,-1])\n        return self.view_vector(vec, viewup)", "response": "View the XY plane of the current object"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nenabling eye dome lighting for this instance.", "response": "def enable_eye_dome_lighting(self):\n        \"\"\"Enable eye dome lighting (EDL)\"\"\"\n        if hasattr(self, 'edl_pass'):\n            return self\n        # create the basic VTK render steps\n        basic_passes = vtk.vtkRenderStepsPass()\n        # blur the resulting image\n        # The blur delegates rendering the unblured image to the basic_passes\n        self.edl_pass = vtk.vtkEDLShading()\n        self.edl_pass.SetDelegatePass(basic_passes)\n\n        # tell the renderer to use our render pass pipeline\n        self.glrenderer = vtk.vtkOpenGLRenderer.SafeDownCast(self)\n        self.glrenderer.SetPass(self.edl_pass)\n        return self.glrenderer"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef point_scalar(mesh, name):\n    vtkarr = mesh.GetPointData().GetArray(name)\n    if vtkarr:\n        if isinstance(vtkarr, vtk.vtkBitArray):\n            vtkarr = vtk_bit_array_to_char(vtkarr)\n        return vtk_to_numpy(vtkarr)", "response": "Returns a vtk array of point scalars of a vtk object"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a vtk cell scalar of a vtk object", "response": "def cell_scalar(mesh, name):\n    \"\"\" Returns cell scalars of a vtk object \"\"\"\n    vtkarr = mesh.GetCellData().GetArray(name)\n    if vtkarr:\n        if isinstance(vtkarr, vtk.vtkBitArray):\n            vtkarr = vtk_bit_array_to_char(vtkarr)\n        return vtk_to_numpy(vtkarr)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_scalar(mesh, name, preference='cell', info=False, err=False):\n    parr = point_scalar(mesh, name)\n    carr = cell_scalar(mesh, name)\n    if isinstance(preference, str):\n        if preference in ['cell', 'c', 'cells']:\n            preference = CELL_DATA_FIELD\n        elif preference in ['point', 'p', 'points']:\n            preference = POINT_DATA_FIELD\n        else:\n            raise RuntimeError('Data field ({}) not supported.'.format(preference))\n    if all([parr is not None, carr is not None]):\n        if preference == CELL_DATA_FIELD:\n            if info:\n                return carr, CELL_DATA_FIELD\n            else:\n                return carr\n        elif preference == POINT_DATA_FIELD:\n            if info:\n                return parr, POINT_DATA_FIELD\n            else:\n                return parr\n        else:\n            raise RuntimeError('Data field ({}) not supported.'.format(preference))\n    arr = None\n    field = None\n    if parr is not None:\n        arr = parr\n        field = 0\n    elif carr is not None:\n        arr = carr\n        field = 1\n    elif err:\n        raise KeyError('Data scalar ({}) not present in this dataset.'.format(name))\n    if info:\n        return arr, field\n    return arr", "response": "Returns the data for a single scalar in the specified mesh."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting numpy points to a vtkPoints object", "response": "def vtk_points(points, deep=True):\n    \"\"\" Convert numpy points to a vtkPoints object \"\"\"\n    if not points.flags['C_CONTIGUOUS']:\n        points = np.ascontiguousarray(points)\n    vtkpts = vtk.vtkPoints()\n    vtkpts.SetData(numpy_to_vtk(points, deep=deep))\n    return vtkpts"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lines_from_points(points):\n    # Assuming ordered points, create array defining line order\n    npoints = points.shape[0] - 1\n    lines = np.vstack((2 * np.ones(npoints, np.int),\n                       np.arange(npoints),\n                       np.arange(1, npoints + 1))).T.ravel()\n\n    return vtki.PolyData(points, lines)", "response": "Generates line from points."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a vtkPolyData object composed of vectors", "response": "def vector_poly_data(orig, vec):\n    \"\"\" Creates a vtkPolyData object composed of vectors \"\"\"\n\n    # shape, dimention checking\n    if not isinstance(orig, np.ndarray):\n        orig = np.asarray(orig)\n\n    if not isinstance(vec, np.ndarray):\n        vec = np.asarray(vec)\n\n    if orig.ndim != 2:\n        orig = orig.reshape((-1, 3))\n    elif orig.shape[1] != 3:\n        raise Exception('orig array must be 3D')\n\n    if vec.ndim != 2:\n        vec = vec.reshape((-1, 3))\n    elif vec.shape[1] != 3:\n        raise Exception('vec array must be 3D')\n\n    # Create vtk points and cells objects\n    vpts = vtk.vtkPoints()\n    vpts.SetData(numpy_to_vtk(np.ascontiguousarray(orig), deep=True))\n\n    npts = orig.shape[0]\n    cells = np.hstack((np.ones((npts, 1), 'int'),\n                       np.arange(npts).reshape((-1, 1))))\n\n    if cells.dtype != ctypes.c_int64 or cells.flags.c_contiguous:\n        cells = np.ascontiguousarray(cells, ctypes.c_int64)\n    cells = np.reshape(cells, (2*npts))\n    vcells = vtk.vtkCellArray()\n    vcells.SetCells(npts, numpy_to_vtkIdTypeArray(cells, deep=True))\n\n    # Create vtkPolyData object\n    pdata = vtk.vtkPolyData()\n    pdata.SetPoints(vpts)\n    pdata.SetVerts(vcells)\n\n    # Add vectors to polydata\n    name = 'vectors'\n    vtkfloat = numpy_to_vtk(np.ascontiguousarray(vec), deep=True)\n    vtkfloat.SetName(name)\n    pdata.GetPointData().AddArray(vtkfloat)\n    pdata.GetPointData().SetActiveVectors(name)\n\n    # Add magnitude of vectors to polydata\n    name = 'mag'\n    scalars = (vec * vec).sum(1)**0.5\n    vtkfloat = numpy_to_vtk(np.ascontiguousarray(scalars), deep=True)\n    vtkfloat.SetName(name)\n    pdata.GetPointData().AddArray(vtkfloat)\n    pdata.GetPointData().SetActiveScalars(name)\n\n    return vtki.PolyData(pdata)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef trans_from_matrix(matrix):\n    t = np.zeros((4, 4))\n    for i in range(4):\n        for j in range(4):\n            t[i, j] = matrix.GetElement(i, j)\n    return t", "response": "Convert a vtk matrix to a numpy. ndarray"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef wrap(vtkdataset):\n    wrappers = {\n        'vtkUnstructuredGrid' : vtki.UnstructuredGrid,\n        'vtkRectilinearGrid' : vtki.RectilinearGrid,\n        'vtkStructuredGrid' : vtki.StructuredGrid,\n        'vtkPolyData' : vtki.PolyData,\n        'vtkImageData' : vtki.UniformGrid,\n        'vtkStructuredPoints' : vtki.UniformGrid,\n        'vtkMultiBlockDataSet' : vtki.MultiBlock,\n        }\n    key = vtkdataset.GetClassName()\n    try:\n        wrapped = wrappers[key](vtkdataset)\n    except:\n        logging.warning('VTK data type ({}) is not currently supported by vtki.'.format(key))\n        return vtkdataset # if not supported just passes the VTK data object\n    return wrapped", "response": "This method wraps any given VTK data object into a VTK data object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a vtkImageData object to a vtkTexture object.", "response": "def image_to_texture(image):\n    \"\"\"Converts ``vtkImageData`` to a ``vtkTexture``\"\"\"\n    vtex = vtk.vtkTexture()\n    vtex.SetInputDataObject(image)\n    vtex.Update()\n    return vtex"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a NumPy image array to a vtk. vtkTexture", "response": "def numpy_to_texture(image):\n    \"\"\"Convert a NumPy image array to a vtk.vtkTexture\"\"\"\n    if not isinstance(image, np.ndarray):\n        raise TypeError('Unknown input type ({})'.format(type(image)))\n    if image.ndim != 3 or image.shape[2] != 3:\n        raise AssertionError('Input image must be nn by nm by RGB')\n    grid = vtki.UniformGrid((image.shape[1], image.shape[0], 1))\n    grid.point_arrays['Image'] = np.flip(image.swapaxes(0,1), axis=1).reshape((-1, 3), order='F')\n    grid.set_active_scalar('Image')\n    return image_to_texture(grid)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_inside_bounds(point, bounds):\n    if isinstance(point, (int, float)):\n        point = [point]\n    if isinstance(point, collections.Iterable) and not isinstance(point, collections.deque):\n        if len(bounds) < 2 * len(point) or len(bounds) % 2 != 0:\n            raise AssertionError('Bounds mismatch point dimensionality')\n        point = collections.deque(point)\n        bounds = collections.deque(bounds)\n        return is_inside_bounds(point, bounds)\n    if not isinstance(point, collections.deque):\n        raise TypeError('Unknown input data type ({}).'.format(type(point)))\n    if len(point) < 1:\n        return True\n    p = point.popleft()\n    lower, upper = bounds.popleft(), bounds.popleft()\n    if lower <= p <= upper:\n        return is_inside_bounds(point, bounds)\n    return False", "response": "Checks if a point is inside a set of bounds."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfits a plane to a set of points", "response": "def fit_plane_to_points(points, return_meta=False):\n    \"\"\"\n    Fits a plane to a set of points\n\n    Parameters\n    ----------\n    points : np.ndarray\n        Size n by 3 array of points to fit a plane through\n\n    return_meta : bool\n        If true, also returns the center and normal used to generate the plane\n    \"\"\"\n    data = np.array(points)\n    center = data.mean(axis=0)\n    result = np.linalg.svd(data - center)\n    normal = np.cross(result[2][0], result[2][1])\n    plane = vtki.Plane(center=center, direction=normal)\n    if return_meta:\n        return plane, center, normal\n    return plane"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclosing all open or active plotters", "response": "def close_all():\n    \"\"\"Close all open/active plotters\"\"\"\n    for key, p in _ALL_PLOTTERS.items():\n        p.close()\n    _ALL_PLOTTERS.clear()\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_plot_theme(theme):\n    if theme.lower() in ['paraview', 'pv']:\n        rcParams['background'] = PV_BACKGROUND\n        rcParams['cmap'] = 'coolwarm'\n        rcParams['font']['family'] = 'arial'\n        rcParams['font']['label_size'] = 16\n        rcParams['show_edges'] = False\n    elif theme.lower() in ['document', 'doc', 'paper', 'report']:\n        rcParams['background'] = 'white'\n        rcParams['cmap'] = 'viridis'\n        rcParams['font']['size'] = 18\n        rcParams['font']['title_size'] = 18\n        rcParams['font']['label_size'] = 18\n        rcParams['font']['color'] = 'black'\n        rcParams['show_edges'] = False\n        rcParams['color'] = 'tan'\n        rcParams['outline_color'] = 'black'\n    elif theme.lower() in ['night', 'dark']:\n        rcParams['background'] = 'black'\n        rcParams['cmap'] = 'viridis'\n        rcParams['font']['color'] = 'white'\n        rcParams['show_edges'] = False\n        rcParams['color'] = 'tan'\n        rcParams['outline_color'] = 'white'\n    elif theme.lower() in ['default']:\n        for k,v in DEFAULT_THEME.items():\n            rcParams[k] = v", "response": "Set the plotting parameters to a predefined theme"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the opacity transfer function results", "response": "def opacity_transfer_function(key, n_colors):\n    \"\"\"Get the opacity transfer function results: range from 0 to 255\n    \"\"\"\n    transfer_func = {\n        'linear': np.linspace(0, 255, n_colors, dtype=np.uint8),\n        'linear_r': np.linspace(0, 255, n_colors, dtype=np.uint8)[::-1],\n        'geom': np.geomspace(1e-6, 255, n_colors, dtype=np.uint8),\n        'geom_r': np.geomspace(255, 1e-6, n_colors, dtype=np.uint8),\n    }\n    try:\n        return transfer_func[key]\n    except KeyError:\n        raise KeyError('opactiy transfer function ({}) unknown.'.format(key))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nplotting a single object in a vtk - like object.", "response": "def plot(var_item, off_screen=None, full_screen=False, screenshot=None,\n         interactive=True, cpos=None, window_size=None,\n         show_bounds=False, show_axes=True, notebook=None, background=None,\n         text='', return_img=False, eye_dome_lighting=False, use_panel=None,\n         **kwargs):\n    \"\"\"\n    Convenience plotting function for a vtk or numpy object.\n\n    Parameters\n    ----------\n    item : vtk or numpy object\n        VTK object or numpy array to be plotted.\n\n    off_screen : bool\n        Plots off screen when True.  Helpful for saving screenshots\n        without a window popping up.\n\n    full_screen : bool, optional\n        Opens window in full screen.  When enabled, ignores window_size.\n        Default False.\n\n    screenshot : str or bool, optional\n        Saves screenshot to file when enabled.  See:\n        help(vtkinterface.Plotter.screenshot).  Default disabled.\n\n        When True, takes screenshot and returns numpy array of image.\n\n    window_size : list, optional\n        Window size in pixels.  Defaults to [1024, 768]\n\n    show_bounds : bool, optional\n        Shows mesh bounds when True.  Default False. Alias ``show_grid`` also\n        accepted.\n\n    notebook : bool, optional\n        When True, the resulting plot is placed inline a jupyter notebook.\n        Assumes a jupyter console is active.\n\n    show_axes : bool, optional\n        Shows a vtk axes widget.  Enabled by default.\n\n    text : str, optional\n        Adds text at the bottom of the plot.\n\n    **kwargs : optional keyword arguments\n        See help(Plotter.add_mesh) for additional options.\n\n    Returns\n    -------\n    cpos : list\n        List of camera position, focal point, and view up.\n\n    img :  numpy.ndarray\n        Array containing pixel RGB and alpha.  Sized:\n        [Window height x Window width x 3] for transparent_background=False\n        [Window height x Window width x 4] for transparent_background=True\n        Returned only when screenshot enabled\n\n    \"\"\"\n    if notebook is None:\n        if run_from_ipython():\n            try:\n                notebook = type(get_ipython()).__module__.startswith('ipykernel.')\n            except NameError:\n                pass\n\n    if notebook:\n        off_screen = notebook\n    plotter = Plotter(off_screen=off_screen, notebook=notebook)\n    if show_axes:\n        plotter.add_axes()\n\n    plotter.set_background(background)\n\n    if isinstance(var_item, list):\n        if len(var_item) == 2:  # might be arrows\n            isarr_0 = isinstance(var_item[0], np.ndarray)\n            isarr_1 = isinstance(var_item[1], np.ndarray)\n            if isarr_0 and isarr_1:\n                plotter.add_arrows(var_item[0], var_item[1])\n            else:\n                for item in var_item:\n                    plotter.add_mesh(item, **kwargs)\n        else:\n            for item in var_item:\n                plotter.add_mesh(item, **kwargs)\n    else:\n        plotter.add_mesh(var_item, **kwargs)\n\n    if text:\n        plotter.add_text(text)\n\n    if show_bounds or kwargs.get('show_grid', False):\n        if kwargs.get('show_grid', False):\n            plotter.show_grid()\n        else:\n            plotter.show_bounds()\n\n    if cpos is None:\n        cpos = plotter.get_default_cam_pos()\n        plotter.camera_position = cpos\n        plotter.camera_set = False\n    else:\n        plotter.camera_position = cpos\n\n    if eye_dome_lighting:\n        plotter.enable_eye_dome_lighting()\n\n    result = plotter.show(window_size=window_size,\n                          auto_close=False,\n                          interactive=interactive,\n                          full_screen=full_screen,\n                          screenshot=screenshot,\n                          return_img=return_img,\n                          use_panel=use_panel)\n\n    # close and return camera position and maybe image\n    plotter.close()\n\n    # Result will be handled by plotter.show(): cpos or [cpos, img]\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if x server is running and returns True if it is otherwise False.", "response": "def system_supports_plotting():\n    \"\"\"\n    Check if x server is running\n\n    Returns\n    -------\n    system_supports_plotting : bool\n        True when on Linux and running an xserver.  Returns None when\n        on a non-linux platform.\n\n    \"\"\"\n    try:\n        if os.environ['ALLOW_PLOTTING'].lower() == 'true':\n            return True\n    except KeyError:\n        pass\n    try:\n        p = Popen([\"xset\", \"-q\"], stdout=PIPE, stderr=PIPE)\n        p.communicate()\n        return p.returncode == 0\n    except:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef single_triangle():\n    points = np.zeros((3, 3))\n    points[1] = [1, 0, 0]\n    points[2] = [0.5, 0.707, 0]\n    cells = np.array([[3, 0, 1, 2]], ctypes.c_long)\n    return vtki.PolyData(points, cells)", "response": "A single PolyData triangle"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_color(color):\n    if color is None:\n        color = rcParams['color']\n    if isinstance(color, str):\n        return vtki.string_to_rgb(color)\n    elif len(color) == 3:\n        return color\n    else:\n        raise Exception(\"\"\"\n    Invalid color input\n    Must ba string, rgb list, or hex color string.  For example:\n        color='white'\n        color='w'\n        color=[1, 1, 1]\n        color='#FFFFFF'\"\"\")", "response": "Parses a color string into a vtk friendly rgb list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot a 2 by 2 comparison of data objects.", "response": "def plot_compare_four(data_a, data_b, data_c, data_d, disply_kwargs=None,\n                      plotter_kwargs=None, show_kwargs=None, screenshot=None,\n                      camera_position=None, outline=None, outline_color='k',\n                      labels=('A', 'B', 'C', 'D')):\n    \"\"\"Plot a 2 by 2 comparison of data objects. Plotting parameters and camera\n    positions will all be the same.\n    \"\"\"\n    datasets = [[data_a, data_b], [data_c, data_d]]\n    labels = [labels[0:2], labels[2:4]]\n\n    if plotter_kwargs is None:\n        plotter_kwargs = {}\n    if disply_kwargs is None:\n        disply_kwargs = {}\n    if show_kwargs is None:\n        show_kwargs = {}\n\n    p = vtki.Plotter(shape=(2,2), **plotter_kwargs)\n\n    for i in range(2):\n        for j in range(2):\n            p.subplot(i, j)\n            p.add_mesh(datasets[i][j], **disply_kwargs)\n            p.add_text(labels[i][j])\n            if is_vtki_obj(outline):\n                p.add_mesh(outline, color=outline_color)\n            if camera_position is not None:\n                p.camera_position = camera_position\n\n    return p.show(screenshot=screenshot, **show_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_focus(self, point):\n        if isinstance(point, np.ndarray):\n            if point.ndim != 1:\n                point = point.ravel()\n        self.camera.SetFocalPoint(point)\n        self._render()", "response": "sets focus to a point"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_position(self, point, reset=False):\n        if isinstance(point, np.ndarray):\n            if point.ndim != 1:\n                point = point.ravel()\n        self.camera.SetPosition(point)\n        if reset:\n            self.reset_camera()\n        self.camera_set = True\n        self._render()", "response": "sets camera position to a point"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset camera viewup vector", "response": "def set_viewup(self, vector):\n        \"\"\" sets camera viewup vector \"\"\"\n        if isinstance(vector, np.ndarray):\n            if vector.ndim != 1:\n                vector = vector.ravel()\n        self.camera.SetViewUp(vector)\n        self._render()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _render(self):\n        if hasattr(self, 'ren_win'):\n            if hasattr(self, 'render_trigger'):\n                self.render_trigger.emit()\n            elif not self._first_time:\n                self.render()", "response": "redraws render window if the render window exists"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding an interactive axes widget", "response": "def add_axes(self, interactive=None, color=None):\n        \"\"\" Add an interactive axes widget \"\"\"\n        if interactive is None:\n            interactive = rcParams['interactive']\n        if hasattr(self, 'axes_widget'):\n            self.axes_widget.SetInteractive(interactive)\n            self._update_axes_color(color)\n            return\n        self.axes_actor = vtk.vtkAxesActor()\n        self.axes_widget = vtk.vtkOrientationMarkerWidget()\n        self.axes_widget.SetOrientationMarker(self.axes_actor)\n        if hasattr(self, 'iren'):\n            self.axes_widget.SetInteractor(self.iren)\n            self.axes_widget.SetEnabled(1)\n            self.axes_widget.SetInteractive(interactive)\n        # Set the color\n        self._update_axes_color(color)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles key press events", "response": "def key_press_event(self, obj, event):\n        \"\"\" Listens for key press event \"\"\"\n        key = self.iren.GetKeySym()\n        log.debug('Key %s pressed' % key)\n        if key == 'q':\n            self.q_pressed = True\n            # Grab screenshot right before renderer closes\n            self.last_image = self.screenshot(True, return_img=True)\n        elif key == 'b':\n            self.observer = self.iren.AddObserver('LeftButtonPressEvent',\n                                                  self.left_button_down)\n        elif key == 'v':\n            self.isometric_view_interactive()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nregister the event for a left button down click", "response": "def left_button_down(self, obj, event_type):\n        \"\"\"Register the event for a left button down click\"\"\"\n        # Get 2D click location on window\n        click_pos = self.iren.GetEventPosition()\n\n        # Get corresponding click location in the 3D plot\n        picker = vtk.vtkWorldPointPicker()\n        picker.Pick(click_pos[0], click_pos[1], 0, self.renderer)\n        self.pickpoint = np.asarray(picker.GetPickPosition()).reshape((-1, 3))\n        if np.any(np.isnan(self.pickpoint)):\n            self.pickpoint[:] = 0"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the current interactive render window to isometric view", "response": "def isometric_view_interactive(self):\n        \"\"\" sets the current interactive render window to isometric view \"\"\"\n        interactor = self.iren.GetInteractorStyle()\n        renderer = interactor.GetCurrentRenderer()\n        renderer.view_isometric()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the current color and color of the current color.", "response": "def update(self, stime=1, force_redraw=True):\n        \"\"\"\n        Update window, redraw, process messages query\n\n        Parameters\n        ----------\n        stime : int, optional\n            Duration of timer that interrupt vtkRenderWindowInteractor in\n            milliseconds.\n\n        force_redraw : bool, optional\n            Call vtkRenderWindowInteractor.Render() immediately.\n        \"\"\"\n\n        if stime <= 0:\n            stime = 1\n\n        curr_time = time.time()\n        if Plotter.last_update_time > curr_time:\n            Plotter.last_update_time = curr_time\n\n        if not hasattr(self, 'iren'):\n            return\n\n        update_rate = self.iren.GetDesiredUpdateRate()\n        if (curr_time - Plotter.last_update_time) > (1.0/update_rate):\n            self.right_timer_id = self.iren.CreateRepeatingTimer(stime)\n\n            self.iren.Start()\n            self.iren.DestroyTimer(self.right_timer_id)\n\n            self._render()\n            Plotter.last_update_time = curr_time\n        else:\n            if force_redraw:\n                self.iren.Render()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_mesh(self, mesh, color=None, style=None, scalars=None,\n                 rng=None, stitle=None, show_edges=None,\n                 point_size=5.0, opacity=1.0, line_width=None,\n                 flip_scalars=False, lighting=None, n_colors=256,\n                 interpolate_before_map=False, cmap=None, label=None,\n                 reset_camera=None, scalar_bar_args=None,\n                 multi_colors=False, name=None, texture=None,\n                 render_points_as_spheres=None,\n                 render_lines_as_tubes=False, edge_color='black',\n                 ambient=0.0, show_scalar_bar=None, nan_color=None,\n                 nan_opacity=1.0, loc=None, backface_culling=False,\n                 rgb=False, categories=False, **kwargs):\n        \"\"\"\n        Adds a unstructured, structured, or surface mesh to the\n        plotting object.\n\n        Also accepts a 3D numpy.ndarray\n\n        Parameters\n        ----------\n        mesh : vtk unstructured, structured, polymesh, or 3D numpy.ndarray\n            A vtk unstructured, structured, or polymesh to plot.\n\n        color : string or 3 item list, optional, defaults to white\n            Either a string, rgb list, or hex color string.  For example:\n                color='white'\n                color='w'\n                color=[1, 1, 1]\n                color='#FFFFFF'\n\n            Color will be overridden when scalars are input.\n\n        style : string, optional\n            Visualization style of the vtk mesh.  One for the following:\n                style='surface'\n                style='wireframe'\n                style='points'\n\n            Defaults to 'surface'\n\n        scalars : numpy array, optional\n            Scalars used to \"color\" the mesh.  Accepts an array equal\n            to the number of cells or the number of points in the\n            mesh.  Array should be sized as a single vector. If both\n            color and scalars are None, then the active scalars are\n            used\n\n        rng : 2 item list, optional\n            Range of mapper for scalars.  Defaults to minimum and\n            maximum of scalars array.  Example: ``[-1, 2]``. ``clim``\n            is also an accepted alias for this.\n\n        stitle : string, optional\n            Scalar title.  By default there is no scalar legend bar.\n            Setting this creates the legend bar and adds a title to\n            it.  To create a bar with no title, use an empty string\n            (i.e. '').\n\n        show_edges : bool, optional\n            Shows the edges of a mesh.  Does not apply to a wireframe\n            representation.\n\n        point_size : float, optional\n            Point size.  Applicable when style='points'.  Default 5.0\n\n        opacity : float, optional\n            Opacity of mesh.  Should be between 0 and 1.  Default 1.0.\n            A string option can also be specified to map the scalar range\n            to the opacity. Options are: linear, linear_r, geom, geom_r\n\n        line_width : float, optional\n            Thickness of lines.  Only valid for wireframe and surface\n            representations.  Default None.\n\n        flip_scalars : bool, optional\n            Flip direction of cmap.\n\n        lighting : bool, optional\n            Enable or disable view direction lighting.  Default False.\n\n        n_colors : int, optional\n            Number of colors to use when displaying scalars.  Default\n            256.\n\n        interpolate_before_map : bool, optional\n            Enabling makes for a smoother scalar display.  Default\n            False\n\n        cmap : str, optional\n           cmap string.  See available matplotlib cmaps.  Only\n           applicable for when displaying scalars.  Defaults None\n           (rainbow).  Requires matplotlib.\n\n        multi_colors : bool, optional\n            If a ``MultiBlock`` dataset is given this will color each\n            block by a solid color using matplotlib's color cycler.\n\n        name : str, optional\n            The name for the added mesh/actor so that it can be easily\n            updated.  If an actor of this name already exists in the\n            rendering window, it will be replaced by the new actor.\n\n        texture : vtk.vtkTexture or np.ndarray or boolean, optional\n            A texture to apply if the input mesh has texture\n            coordinates.  This will not work with MultiBlock\n            datasets. If set to ``True``, the first avaialble texture\n            on the object will be used. If a string name is given, it\n            will pull a texture with that name associated to the input\n            mesh.\n\n        ambient : float, optional\n            When lighting is enabled, this is the amount of light from\n            0 to 1 that reaches the actor when not directed at the\n            light source emitted from the viewer.  Default 0.2.\n\n        nan_color : string or 3 item list, optional, defaults to gray\n            The color to use for all NaN values in the plotted scalar\n            array.\n\n        nan_opacity : float, optional\n            Opacity of NaN values.  Should be between 0 and 1.\n            Default 1.0\n\n        backface_culling : bool optional\n            Does not render faces that should not be visible to the\n            plotter.  This can be helpful for dense surface meshes,\n            especially when edges are visible, but can cause flat\n            meshes to be partially displayed.  Default False.\n\n        rgb : bool, optional\n            If an 2 dimensional array is passed as the scalars, plot those\n            values as RGB+A colors! ``rgba`` is also accepted alias for this.\n\n        categories : bool, optional\n            If fetching a colormap from matplotlib, this is the number of\n            categories to use in that colormap. If set to ``True``, then\n            the number of unique values in the scalar array will be used.\n\n        Returns\n        -------\n        actor: vtk.vtkActor\n            VTK actor of the mesh.\n        \"\"\"\n        # fixes lighting issue when using precalculated normals\n        if isinstance(mesh, vtk.vtkPolyData):\n            if mesh.GetPointData().HasArray('Normals'):\n                mesh.point_arrays['Normals'] = mesh.point_arrays.pop('Normals')\n\n        if scalar_bar_args is None:\n            scalar_bar_args = {}\n\n        if isinstance(mesh, np.ndarray):\n            mesh = vtki.PolyData(mesh)\n            style = 'points'\n\n        # Convert the VTK data object to a vtki wrapped object if neccessary\n        if not is_vtki_obj(mesh):\n            mesh = wrap(mesh)\n\n        if show_edges is None:\n            show_edges = rcParams['show_edges']\n\n        if show_scalar_bar is None:\n            show_scalar_bar = rcParams['show_scalar_bar']\n\n        if lighting is None:\n            lighting = rcParams['lighting']\n\n        if rng is None:\n            rng = kwargs.get('clim', None)\n\n        if render_points_as_spheres is None:\n            render_points_as_spheres = rcParams['render_points_as_spheres']\n\n        if name is None:\n            name = '{}({})'.format(type(mesh).__name__, str(hex(id(mesh))))\n\n        if isinstance(mesh, vtki.MultiBlock):\n            self.remove_actor(name, reset_camera=reset_camera)\n            # frist check the scalars\n            if rng is None and scalars is not None:\n                # Get the data range across the array for all blocks\n                # if scalar specified\n                if isinstance(scalars, str):\n                    rng = mesh.get_data_range(scalars)\n                else:\n                    # TODO: an array was given... how do we deal with\n                    #       that? Possibly a 2D arrays or list of\n                    #       arrays where first index corresponds to\n                    #       the block? This could get complicated real\n                    #       quick.\n                    raise RuntimeError('Scalar array must be given as a string name for multiblock datasets.')\n            if multi_colors:\n                # Compute unique colors for each index of the block\n                try:\n                    import matplotlib as mpl\n                    from itertools import cycle\n                    cycler = mpl.rcParams['axes.prop_cycle']\n                    colors = cycle(cycler)\n                except ImportError:\n                    multi_colors = False\n                    logging.warning('Please install matplotlib for color cycles')\n            # Now iteratively plot each element of the multiblock dataset\n            actors = []\n            for idx in range(mesh.GetNumberOfBlocks()):\n                if mesh[idx] is None:\n                    continue\n                # Get a good name to use\n                next_name = '{}-{}'.format(name, idx)\n                # Get the data object\n                if not is_vtki_obj(mesh[idx]):\n                    data = wrap(mesh.GetBlock(idx))\n                    if not is_vtki_obj(mesh[idx]):\n                        continue # move on if we can't plot it\n                else:\n                    data = mesh.GetBlock(idx)\n                if data is None:\n                    # Note that a block can exist but be None type\n                    continue\n                # Now check that scalars is available for this dataset\n                if isinstance(data, vtk.vtkMultiBlockDataSet) or get_scalar(data, scalars) is None:\n                    ts = None\n                else:\n                    ts = scalars\n                if multi_colors:\n                    color = next(colors)['color']\n                a = self.add_mesh(data, color=color, style=style,\n                                  scalars=ts, rng=rng, stitle=stitle,\n                                  show_edges=show_edges,\n                                  point_size=point_size, opacity=opacity,\n                                  line_width=line_width,\n                                  flip_scalars=flip_scalars,\n                                  lighting=lighting, n_colors=n_colors,\n                                  interpolate_before_map=interpolate_before_map,\n                                  cmap=cmap, label=label,\n                                  scalar_bar_args=scalar_bar_args,\n                                  reset_camera=reset_camera, name=next_name,\n                                  texture=None,\n                                  render_points_as_spheres=render_points_as_spheres,\n                                  render_lines_as_tubes=render_lines_as_tubes,\n                                  edge_color=edge_color,\n                                  show_scalar_bar=show_scalar_bar, nan_color=nan_color,\n                                  nan_opacity=nan_opacity,\n                                  loc=loc, rgb=rgb, **kwargs)\n                actors.append(a)\n                if (reset_camera is None and not self.camera_set) or reset_camera:\n                    cpos = self.get_default_cam_pos()\n                    self.camera_position = cpos\n                    self.camera_set = False\n                    self.reset_camera()\n            return actors\n\n        if nan_color is None:\n            nan_color = rcParams['nan_color']\n        nanr, nanb, nang = parse_color(nan_color)\n        nan_color = nanr, nanb, nang, nan_opacity\n        if color is True:\n            color = rcParams['color']\n\n        if mesh.n_points < 1:\n            raise RuntimeError('Empty meshes cannot be plotted. Input mesh has zero points.')\n\n        # set main values\n        self.mesh = mesh\n        self.mapper = vtk.vtkDataSetMapper()\n        self.mapper.SetInputData(self.mesh)\n        if isinstance(scalars, str):\n            self.mapper.SetArrayName(scalars)\n\n        actor, prop = self.add_actor(self.mapper,\n                                     reset_camera=reset_camera,\n                                     name=name, loc=loc, culling=backface_culling)\n\n        # Try to plot something if no preference given\n        if scalars is None and color is None and texture is None:\n            # Prefer texture first\n            if len(list(mesh.textures.keys())) > 0:\n                texture = True\n            # If no texture, plot any active scalar\n            else:\n                # Make sure scalar components are not vectors/tuples\n                scalars = mesh.active_scalar\n                if scalars is None:# or scalars.ndim != 1:\n                    scalars = None\n                else:\n                    if stitle is None:\n                        stitle = mesh.active_scalar_info[1]\n\n        if texture == True or isinstance(texture, (str, int)):\n            texture = mesh._activate_texture(texture)\n\n        if texture:\n            if isinstance(texture, np.ndarray):\n                texture = numpy_to_texture(texture)\n            if not isinstance(texture, (vtk.vtkTexture, vtk.vtkOpenGLTexture)):\n                raise TypeError('Invalid texture type ({})'.format(type(texture)))\n            if mesh.GetPointData().GetTCoords() is None:\n                raise AssertionError('Input mesh does not have texture coordinates to support the texture.')\n            actor.SetTexture(texture)\n            # Set color to white by default when using a texture\n            if color is None:\n                color = 'white'\n            if scalars is None:\n                show_scalar_bar = False\n            self.mapper.SetScalarModeToUsePointFieldData()\n\n        # Scalar formatting ===================================================\n        if cmap is None: # grab alias for cmaps: colormap\n            cmap = kwargs.get('colormap', None)\n        if cmap is None: # Set default map if matplotlib is avaialble\n            try:\n                import matplotlib\n                cmap = rcParams['cmap']\n            except ImportError:\n                pass\n        title = 'Data' if stitle is None else stitle\n        if scalars is not None:\n            # if scalars is a string, then get the first array found with that name\n            append_scalars = True\n            if isinstance(scalars, str):\n                title = scalars\n                scalars = get_scalar(mesh, scalars,\n                        preference=kwargs.get('preference', 'cell'), err=True)\n                if stitle is None:\n                    stitle = title\n                #append_scalars = False\n\n            if not isinstance(scalars, np.ndarray):\n                scalars = np.asarray(scalars)\n\n            if rgb is False or rgb is None:\n                rgb = kwargs.get('rgba', False)\n            if rgb:\n                if scalars.ndim != 2 or scalars.shape[1] < 3 or scalars.shape[1] > 4:\n                    raise ValueError('RGB array must be n_points/n_cells by 3/4 in shape.')\n\n            if scalars.ndim != 1:\n                if rgb:\n                    pass\n                elif scalars.ndim == 2 and (scalars.shape[0] == mesh.n_points or scalars.shape[0] == mesh.n_cells):\n                    scalars = np.linalg.norm(scalars.copy(), axis=1)\n                    title = '{}-normed'.format(title)\n                else:\n                    scalars = scalars.ravel()\n\n            if scalars.dtype == np.bool:\n                scalars = scalars.astype(np.float)\n\n            # Scalar interpolation approach\n            if scalars.shape[0] == mesh.n_points:\n                self.mesh._add_point_scalar(scalars, title, append_scalars)\n                self.mapper.SetScalarModeToUsePointData()\n                self.mapper.GetLookupTable().SetNumberOfTableValues(n_colors)\n                if interpolate_before_map:\n                    self.mapper.InterpolateScalarsBeforeMappingOn()\n            elif scalars.shape[0] == mesh.n_cells:\n                self.mesh._add_cell_scalar(scalars, title, append_scalars)\n                self.mapper.SetScalarModeToUseCellData()\n                self.mapper.GetLookupTable().SetNumberOfTableValues(n_colors)\n                if interpolate_before_map:\n                    self.mapper.InterpolateScalarsBeforeMappingOn()\n            else:\n                _raise_not_matching(scalars, mesh)\n\n            # Set scalar range\n            if rng is None:\n                rng = [np.nanmin(scalars), np.nanmax(scalars)]\n            elif isinstance(rng, float) or isinstance(rng, int):\n                rng = [-rng, rng]\n\n            if np.any(rng) and not rgb:\n                self.mapper.SetScalarRange(rng[0], rng[1])\n\n            # Flip if requested\n            table = self.mapper.GetLookupTable()\n            table.SetNanColor(nan_color)\n            if cmap is not None:\n                try:\n                    from matplotlib.cm import get_cmap\n                except ImportError:\n                    cmap = None\n                    logging.warning('Please install matplotlib for color maps.')\n            if cmap is not None:\n                try:\n                    from matplotlib.cm import get_cmap\n                except ImportError:\n                    raise Exception('cmap requires matplotlib')\n                if isinstance(cmap, str):\n                    if categories:\n                        if categories is True:\n                            categories = len(np.unique(scalars))\n                        cmap = get_cmap(cmap, categories)\n                    else:\n                        cmap = get_cmap(cmap)\n                    # ELSE: assume cmap is callable\n                ctable = cmap(np.linspace(0, 1, n_colors))*255\n                ctable = ctable.astype(np.uint8)\n                # Set opactities\n                if isinstance(opacity, str):\n                    ctable[:,-1] = opacity_transfer_function(opacity, n_colors)\n                if flip_scalars:\n                    ctable = np.ascontiguousarray(ctable[::-1])\n                table.SetTable(VN.numpy_to_vtk(ctable))\n\n            else:  # no cmap specified\n                if flip_scalars:\n                    table.SetHueRange(0.0, 0.66667)\n                else:\n                    table.SetHueRange(0.66667, 0.0)\n\n        else:\n            self.mapper.SetScalarModeToUseFieldData()\n\n        # select view style\n        if not style:\n            style = 'surface'\n        style = style.lower()\n        if style == 'wireframe':\n            prop.SetRepresentationToWireframe()\n            if color is None:\n                color = rcParams['outline_color']\n        elif style == 'points':\n            prop.SetRepresentationToPoints()\n        elif style == 'surface':\n            prop.SetRepresentationToSurface()\n        else:\n            raise Exception('Invalid style.  Must be one of the following:\\n' +\n                            '\\t\"surface\"\\n' +\n                            '\\t\"wireframe\"\\n' +\n                            '\\t\"points\"\\n')\n\n        prop.SetPointSize(point_size)\n        prop.SetAmbient(ambient)\n        # edge display style\n        if show_edges:\n            prop.EdgeVisibilityOn()\n\n        rgb_color = parse_color(color)\n        prop.SetColor(rgb_color)\n        if isinstance(opacity, (float, int)):\n            prop.SetOpacity(opacity)\n        prop.SetEdgeColor(parse_color(edge_color))\n\n        if render_points_as_spheres:\n            prop.SetRenderPointsAsSpheres(render_points_as_spheres)\n        if render_lines_as_tubes:\n            prop.SetRenderLinesAsTubes(render_lines_as_tubes)\n\n        # legend label\n        if label:\n            if not isinstance(label, str):\n                raise AssertionError('Label must be a string')\n            geom = single_triangle()\n            if scalars is not None:\n                geom = vtki.Box()\n                rgb_color = parse_color('black')\n            self._labels.append([geom, label, rgb_color])\n\n        # lighting display style\n        if not lighting:\n            prop.LightingOff()\n\n        # set line thickness\n        if line_width:\n            prop.SetLineWidth(line_width)\n\n        # Add scalar bar if available\n        if stitle is not None and show_scalar_bar and not rgb:\n            self.add_scalar_bar(stitle, **scalar_bar_args)\n\n        return actor", "response": "Adds a mesh to the vtk object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_scalar_bar_range(self, clim, name=None):\n        if isinstance(clim, float) or isinstance(clim, int):\n            clim = [-clim, clim]\n        if len(clim) != 2:\n            raise TypeError('clim argument must be a length 2 iterable of values: (min, max).')\n        if name is None:\n            if not hasattr(self, 'mapper'):\n                raise RuntimeError('This plotter does not have an active mapper.')\n            return self.mapper.SetScalarRange(*clim)\n        # Use the name to find the desired actor\n        def update_mapper(mapper):\n            return mapper.SetScalarRange(*clim)\n        try:\n            for m in self._scalar_bar_mappers[name]:\n                update_mapper(m)\n        except KeyError:\n            raise KeyError('Name ({}) not valid/not found in this plotter.')\n        return", "response": "Update the value range of the active or named scalar bar."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclear the plot by removing all actors and properties", "response": "def clear(self):\n        \"\"\" Clears plot by removing all actors and properties \"\"\"\n        for renderer in self.renderers:\n            renderer.RemoveAllViewProps()\n        self._scalar_bar_slots = set(range(MAX_N_COLOR_BARS))\n        self._scalar_bar_slot_lookup = {}\n        self._scalar_bar_ranges = {}\n        self._scalar_bar_mappers = {}\n        self._scalar_bar_actors = {}\n        self._scalar_bar_widgets = {}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_actor(self, actor, reset_camera=False):\n        for renderer in self.renderers:\n            renderer.remove_actor(actor, reset_camera)\n        return True", "response": "Removes an actor from the Plotter."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_actor(self, uinput, reset_camera=False, name=None, loc=None,\n                  culling=False):\n        \"\"\"\n        Adds an actor to render window.  Creates an actor if input is\n        a mapper.\n\n        Parameters\n        ----------\n        uinput : vtk.vtkMapper or vtk.vtkActor\n            vtk mapper or vtk actor to be added.\n\n        reset_camera : bool, optional\n            Resets the camera when true.\n\n        loc : int, tuple, or list\n            Index of the renderer to add the actor to.  For example,\n            ``loc=2`` or ``loc=(1, 1)``.  If None, selects the last\n            active Renderer.\n\n        culling : bool optional\n            Does not render faces that should not be visible to the\n            plotter.  This can be helpful for dense surface meshes,\n            especially when edges are visible, but can cause flat\n            meshes to be partially displayed.  Default False.\n\n        Returns\n        -------\n        actor : vtk.vtkActor\n            The actor.\n\n        actor_properties : vtk.Properties\n            Actor properties.\n\n        \"\"\"\n        # add actor to the correct render window\n        self._active_renderer_index = self.loc_to_index(loc)\n        renderer = self.renderers[self._active_renderer_index]\n        return renderer.add_actor(uinput, reset_camera, name, culling)", "response": "Adds an actor to the render window."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning index of the render window given a location index.", "response": "def loc_to_index(self, loc):\n        \"\"\"\n        Return index of the render window given a location index.\n\n        Parameters\n        ----------\n        loc : int, tuple, or list\n            Index of the renderer to add the actor to.  For example,\n            ``loc=2`` or ``loc=(1, 1)``.\n\n        Returns\n        -------\n        idx : int\n            Index of the render window.\n\n        \"\"\"\n        if loc is None:\n            return self._active_renderer_index\n        elif isinstance(loc, int):\n            return loc\n        elif isinstance(loc, collections.Iterable):\n            assert len(loc) == 2, '\"loc\" must contain two items'\n            return loc[0]*self.shape[0] + loc[1]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef index_to_loc(self, index):\n        sz = int(self.shape[0] * self.shape[1])\n        idxs = np.array([i for i in range(sz)], dtype=int).reshape(self.shape)\n        args = np.argwhere(idxs == index)\n        if len(args) < 1:\n            raise RuntimeError('Index ({}) is out of range.')\n        return args[0]", "response": "Convert a 1D index location to the 2D location on the plotting grid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding axes actor at the origin of a render window.", "response": "def add_axes_at_origin(self, loc=None):\n        \"\"\"\n        Add axes actor at the origin of a render window.\n\n        Parameters\n        ----------\n        loc : int, tuple, or list\n            Index of the renderer to add the actor to.  For example,\n            ``loc=2`` or ``loc=(1, 1)``.  When None, defaults to the\n            active render window.\n\n        Returns\n        --------\n        marker_actor : vtk.vtkAxesActor\n            vtkAxesActor actor\n        \"\"\"\n        self._active_renderer_index = self.loc_to_index(loc)\n        return self.renderers[self._active_renderer_index].add_axes_at_origin()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndisplay the bounds of the most recent input object.", "response": "def show_bounds(self, mesh=None, bounds=None, show_xaxis=True,\n                        show_yaxis=True, show_zaxis=True, show_xlabels=True,\n                        show_ylabels=True, show_zlabels=True, italic=False,\n                        bold=True, shadow=False, font_size=None,\n                        font_family=None, color=None,\n                        xlabel='X Axis', ylabel='Y Axis', zlabel='Z Axis',\n                        use_2d=False, grid=None, location='closest', ticks=None,\n                        all_edges=False, corner_factor=0.5, fmt=None,\n                        minor_ticks=False, loc=None, padding=0.0):\n        \"\"\"\n        Adds bounds axes.  Shows the bounds of the most recent input\n        mesh unless mesh is specified.\n\n        Parameters\n        ----------\n        mesh : vtkPolydata or unstructured grid, optional\n            Input mesh to draw bounds axes around\n\n        bounds : list or tuple, optional\n            Bounds to override mesh bounds.\n            [xmin, xmax, ymin, ymax, zmin, zmax]\n\n        show_xaxis : bool, optional\n            Makes x axis visible.  Default True.\n\n        show_yaxis : bool, optional\n            Makes y axis visible.  Default True.\n\n        show_zaxis : bool, optional\n            Makes z axis visible.  Default True.\n\n        show_xlabels : bool, optional\n            Shows x labels.  Default True.\n\n        show_ylabels : bool, optional\n            Shows y labels.  Default True.\n\n        show_zlabels : bool, optional\n            Shows z labels.  Default True.\n\n        italic : bool, optional\n            Italicises axis labels and numbers.  Default False.\n\n        bold : bool, optional\n            Bolds axis labels and numbers.  Default True.\n\n        shadow : bool, optional\n            Adds a black shadow to the text.  Default False.\n\n        font_size : float, optional\n            Sets the size of the label font.  Defaults to 16.\n\n        font_family : string, optional\n            Font family.  Must be either courier, times, or arial.\n\n        color : string or 3 item list, optional\n            Color of all labels and axis titles.  Default white.\n            Either a string, rgb list, or hex color string.  For example:\n\n                color='white'\n                color='w'\n                color=[1, 1, 1]\n                color='#FFFFFF'\n\n        xlabel : string, optional\n            Title of the x axis.  Default \"X Axis\"\n\n        ylabel : string, optional\n            Title of the y axis.  Default \"Y Axis\"\n\n        zlabel : string, optional\n            Title of the z axis.  Default \"Z Axis\"\n\n        use_2d : bool, optional\n            A bug with vtk 6.3 in Windows seems to cause this function\n            to crash this can be enabled for smoother plotting for\n            other enviornments.\n\n        grid : bool or str, optional\n            Add grid lines to the backface (``True``, ``'back'``, or\n            ``'backface'``) or to the frontface (``'front'``,\n            ``'frontface'``) of the axes actor.\n\n        location : str, optional\n            Set how the axes are drawn: either static (``'all'``),\n            closest triad (``front``), furthest triad (``'back'``),\n            static closest to the origin (``'origin'``), or outer\n            edges (``'outer'``) in relation to the camera\n            position. Options include: ``'all', 'front', 'back',\n            'origin', 'outer'``\n\n        ticks : str, optional\n            Set how the ticks are drawn on the axes grid. Options include:\n            ``'inside', 'outside', 'both'``\n\n        all_edges : bool, optional\n            Adds an unlabeled and unticked box at the boundaries of\n            plot. Useful for when wanting to plot outer grids while\n            still retaining all edges of the boundary.\n\n        corner_factor : float, optional\n            If ``all_edges````, this is the factor along each axis to\n            draw the default box. Dafuault is 0.5 to show the full box.\n\n        loc : int, tuple, or list\n            Index of the renderer to add the actor to.  For example,\n            ``loc=2`` or ``loc=(1, 1)``.  If None, selects the last\n            active Renderer.\n\n        padding : float, optional\n            An optional percent padding along each axial direction to cushion\n            the datasets in the scene from the axes annotations. Defaults to\n            have no padding\n\n        Returns\n        -------\n        cube_axes_actor : vtk.vtkCubeAxesActor\n            Bounds actor\n\n        Examples\n        --------\n        >>> import vtki\n        >>> from vtki import examples\n        >>> mesh = vtki.Sphere()\n        >>> plotter = vtki.Plotter()\n        >>> _ = plotter.add_mesh(mesh)\n        >>> _ = plotter.show_bounds(grid='front', location='outer', all_edges=True)\n        >>> plotter.show() # doctest:+SKIP\n        \"\"\"\n        kwargs = locals()\n        _ = kwargs.pop('self')\n        _ = kwargs.pop('loc')\n        self._active_renderer_index = self.loc_to_index(loc)\n        renderer = self.renderers[self._active_renderer_index]\n        renderer.show_bounds(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a bounding box to the current renderer.", "response": "def add_bounding_box(self, color=None, corner_factor=0.5, line_width=None,\n                         opacity=1.0, render_lines_as_tubes=False, lighting=None,\n                         reset_camera=None, loc=None):\n        \"\"\"\n        Adds an unlabeled and unticked box at the boundaries of\n        plot.  Useful for when wanting to plot outer grids while\n        still retaining all edges of the boundary.\n\n        Parameters\n        ----------\n        corner_factor : float, optional\n            If ``all_edges``, this is the factor along each axis to\n            draw the default box. Dafuault is 0.5 to show the full\n            box.\n\n        corner_factor : float, optional\n            This is the factor along each axis to draw the default\n            box. Dafuault is 0.5 to show the full box.\n\n        line_width : float, optional\n            Thickness of lines.\n\n        opacity : float, optional\n            Opacity of mesh.  Should be between 0 and 1.  Default 1.0\n\n        loc : int, tuple, or list\n            Index of the renderer to add the actor to.  For example,\n            ``loc=2`` or ``loc=(1, 1)``.  If None, selects the last\n            active Renderer.\n\n        \"\"\"\n        kwargs = locals()\n        _ = kwargs.pop('self')\n        _ = kwargs.pop('loc')\n        self._active_renderer_index = self.loc_to_index(loc)\n        renderer = self.renderers[self._active_renderer_index]\n        return renderer.add_bounding_box(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_bounding_box(self, loc=None):\n        self._active_renderer_index = self.loc_to_index(loc)\n        renderer = self.renderers[self._active_renderer_index]\n        renderer.remove_bounding_box()", "response": "Removes bounding box from the active renderer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving the bounds axes from the active renderer.", "response": "def remove_bounds_axes(self, loc=None):\n        \"\"\"\n        Removes bounds axes from the active renderer.\n\n        Parameters\n        ----------\n        loc : int, tuple, or list\n            Index of the renderer to add the actor to.  For example,\n            ``loc=2`` or ``loc=(1, 1)``.  If None, selects the last\n            active Renderer.\n        \"\"\"\n        self._active_renderer_index = self.loc_to_index(loc)\n        renderer = self.renderers[self._active_renderer_index]\n        renderer.remove_bounds_axes()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the active subplot.", "response": "def subplot(self, index_x, index_y):\n        \"\"\"\n        Sets the active subplot.\n\n        Parameters\n        ----------\n        index_x : int\n            Index of the subplot to activate in the x direction.\n\n        index_y : int\n            Index of the subplot to activate in the y direction.\n\n        \"\"\"\n        self._active_renderer_index = self.loc_to_index((index_x, index_y))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef show_grid(self, **kwargs):\n        kwargs.setdefault('grid', 'back')\n        kwargs.setdefault('location', 'outer')\n        kwargs.setdefault('ticks', 'both')\n        return self.show_bounds(**kwargs)", "response": "A wrapped implementation of show_bounds that can be used to change the behaviour of the matplotlib grid function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the scale of the active controller.", "response": "def set_scale(self, xscale=None, yscale=None, zscale=None, reset_camera=True):\n        \"\"\"\n        Scale all the datasets in the scene of the active renderer.\n\n        Scaling in performed independently on the X, Y and Z axis.\n        A scale of zero is illegal and will be replaced with one.\n\n        Parameters\n        ----------\n        xscale : float, optional\n            Scaling of the x axis.  Must be greater than zero.\n\n        yscale : float, optional\n            Scaling of the y axis.  Must be greater than zero.\n\n        zscale : float, optional\n            Scaling of the z axis.  Must be greater than zero.\n\n        reset_camera : bool, optional\n            Resets camera so all actors can be seen.\n\n        \"\"\"\n        self.renderer.set_scale(xscale, yscale, zscale, reset_camera)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _update_axes_color(self, color):\n        prop_x = self.axes_actor.GetXAxisCaptionActor2D().GetCaptionTextProperty()\n        prop_y = self.axes_actor.GetYAxisCaptionActor2D().GetCaptionTextProperty()\n        prop_z = self.axes_actor.GetZAxisCaptionActor2D().GetCaptionTextProperty()\n        if color is None:\n            color = rcParams['font']['color']\n        color = parse_color(color)\n        for prop in [prop_x, prop_y, prop_z]:\n            prop.SetColor(color[0], color[1], color[2])\n            prop.SetShadow(False)\n        return", "response": "Internal helper to set the axes label color"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a scalar bar to the current colorbar.", "response": "def add_scalar_bar(self, title=None, n_labels=5, italic=False,\n                       bold=True, title_font_size=None,\n                       label_font_size=None, color=None,\n                       font_family=None, shadow=False, mapper=None,\n                       width=None, height=None, position_x=None,\n                       position_y=None, vertical=None,\n                       interactive=False, fmt=None, use_opacity=True,\n                       outline=False):\n        \"\"\"\n        Creates scalar bar using the ranges as set by the last input\n        mesh.\n\n        Parameters\n        ----------\n        title : string, optional\n            Title of the scalar bar.  Default None\n\n        n_labels : int, optional\n            Number of labels to use for the scalar bar.\n\n        italic : bool, optional\n            Italicises title and bar labels.  Default False.\n\n        bold  : bool, optional\n            Bolds title and bar labels.  Default True\n\n        title_font_size : float, optional\n            Sets the size of the title font.  Defaults to None and is sized\n            automatically.\n\n        label_font_size : float, optional\n            Sets the size of the title font.  Defaults to None and is sized\n            automatically.\n\n        color : string or 3 item list, optional, defaults to white\n            Either a string, rgb list, or hex color string.  For example:\n                color='white'\n                color='w'\n                color=[1, 1, 1]\n                color='#FFFFFF'\n\n        font_family : string, optional\n            Font family.  Must be either courier, times, or arial.\n\n        shadow : bool, optional\n            Adds a black shadow to the text.  Defaults to False\n\n        width : float, optional\n            The percentage (0 to 1) width of the window for the colorbar\n\n        height : float, optional\n            The percentage (0 to 1) height of the window for the colorbar\n\n        position_x : float, optional\n            The percentage (0 to 1) along the windows's horizontal\n            direction to place the bottom left corner of the colorbar\n\n        position_y : float, optional\n            The percentage (0 to 1) along the windows's vertical\n            direction to place the bottom left corner of the colorbar\n\n        interactive : bool, optional\n            Use a widget to control the size and location of the scalar bar.\n\n        use_opacity : bool, optional\n            Optionally disply the opacity mapping on the scalar bar\n\n        outline : bool, optional\n            Optionally outline the scalar bar to make opacity mappings more\n            obvious.\n\n        Notes\n        -----\n        Setting title_font_size, or label_font_size disables automatic font\n        sizing for both the title and label.\n\n\n        \"\"\"\n        if font_family is None:\n            font_family = rcParams['font']['family']\n        if label_font_size is None:\n            label_font_size = rcParams['font']['label_size']\n        if title_font_size is None:\n            title_font_size = rcParams['font']['title_size']\n        if color is None:\n            color = rcParams['font']['color']\n        if fmt is None:\n            fmt = rcParams['font']['fmt']\n        if vertical is None:\n            if rcParams['colorbar_orientation'].lower() == 'vertical':\n                vertical = True\n        # Automatically choose size if not specified\n        if width is None:\n            if vertical:\n                width = rcParams['colorbar_vertical']['width']\n            else:\n                width = rcParams['colorbar_horizontal']['width']\n        if height is None:\n            if vertical:\n                height = rcParams['colorbar_vertical']['height']\n            else:\n                height = rcParams['colorbar_horizontal']['height']\n\n        # check if maper exists\n        if mapper is None:\n            if not hasattr(self, 'mapper'):\n                raise Exception('Mapper does not exist.  ' +\n                                'Add a mesh with scalars first.')\n            mapper = self.mapper\n\n        if title:\n            # Check that this data hasn't already been plotted\n            if title in list(self._scalar_bar_ranges.keys()):\n                rng = list(self._scalar_bar_ranges[title])\n                newrng = mapper.GetScalarRange()\n                oldmappers = self._scalar_bar_mappers[title]\n                # get max for range and reset everything\n                if newrng[0] < rng[0]:\n                    rng[0] = newrng[0]\n                if newrng[1] > rng[1]:\n                    rng[1] = newrng[1]\n                for m in oldmappers:\n                    m.SetScalarRange(rng[0], rng[1])\n                mapper.SetScalarRange(rng[0], rng[1])\n                self._scalar_bar_mappers[title].append(mapper)\n                self._scalar_bar_ranges[title] = rng\n                # Color bar already present and ready to be used so returning\n                return\n\n        # Automatically choose location if not specified\n        if position_x is None or position_y is None:\n            try:\n                slot = min(self._scalar_bar_slots)\n                self._scalar_bar_slots.remove(slot)\n                self._scalar_bar_slot_lookup[title] = slot\n            except:\n                raise RuntimeError('Maximum number of color bars reached.')\n            if position_x is None:\n                if vertical:\n                    position_x = rcParams['colorbar_vertical']['position_x']\n                    position_x -= slot * width\n                else:\n                    position_x = rcParams['colorbar_horizontal']['position_x']\n\n            if position_y is None:\n                if vertical:\n                    position_y = rcParams['colorbar_vertical']['position_y']\n                else:\n                    position_y = rcParams['colorbar_horizontal']['position_y']\n                    position_y += slot * height\n        # Adjust to make sure on the screen\n        if position_x + width > 1:\n            position_x -= width\n        if position_y + height > 1:\n            position_y -= height\n\n        # parse color\n        color = parse_color(color)\n\n        # Create scalar bar\n        self.scalar_bar = vtk.vtkScalarBarActor()\n        self.scalar_bar.SetLookupTable(mapper.GetLookupTable())\n        self.scalar_bar.SetNumberOfLabels(n_labels)\n\n        # edit the size of the colorbar\n        self.scalar_bar.SetHeight(height)\n        self.scalar_bar.SetWidth(width)\n        self.scalar_bar.SetPosition(position_x, position_y)\n\n        if fmt is not None:\n            self.scalar_bar.SetLabelFormat(fmt)\n\n        if vertical:\n            self.scalar_bar.SetOrientationToVertical()\n        else:\n            self.scalar_bar.SetOrientationToHorizontal()\n\n        if label_font_size is None or title_font_size is None:\n            self.scalar_bar.UnconstrainedFontSizeOn()\n\n        if n_labels:\n            label_text = self.scalar_bar.GetLabelTextProperty()\n            label_text.SetColor(color)\n            label_text.SetShadow(shadow)\n\n            # Set font\n            label_text.SetFontFamily(parse_font_family(font_family))\n            label_text.SetItalic(italic)\n            label_text.SetBold(bold)\n            if label_font_size:\n                label_text.SetFontSize(label_font_size)\n\n        # Set properties\n        if title:\n            rng = mapper.GetScalarRange()\n            self._scalar_bar_ranges[title] = rng\n            self._scalar_bar_mappers[title] = [mapper]\n\n            self.scalar_bar.SetTitle(title)\n            title_text = self.scalar_bar.GetTitleTextProperty()\n\n            title_text.SetJustificationToCentered()\n\n            title_text.SetItalic(italic)\n            title_text.SetBold(bold)\n            title_text.SetShadow(shadow)\n            if title_font_size:\n                title_text.SetFontSize(title_font_size)\n\n            # Set font\n            title_text.SetFontFamily(parse_font_family(font_family))\n\n            # set color\n            title_text.SetColor(color)\n\n            self._scalar_bar_actors[title] = self.scalar_bar\n\n        if interactive is None:\n            interactive = rcParams['interactive']\n            if shape != (1, 1):\n                interactive = False\n        elif interactive and self.shape != (1, 1):\n            err_str = 'Interactive scalar bars disabled for multi-renderer plots'\n            raise Exception(err_str)\n\n        if interactive and hasattr(self, 'iren'):\n            self.scalar_widget = vtk.vtkScalarBarWidget()\n            self.scalar_widget.SetScalarBarActor(self.scalar_bar)\n            self.scalar_widget.SetInteractor(self.iren)\n            self.scalar_widget.SetEnabled(1)\n            rep = self.scalar_widget.GetRepresentation()\n            # self.scalar_widget.On()\n            if vertical is True or vertical is None:\n                rep.SetOrientation(1)  # 0 = Horizontal, 1 = Vertical\n            else:\n                rep.SetOrientation(0)  # 0 = Horizontal, 1 = Vertical\n            self._scalar_bar_widgets[title] = self.scalar_widget\n\n        if use_opacity:\n            self.scalar_bar.SetUseOpacity(True)\n\n        if outline:\n            self.scalar_bar.SetDrawFrame(True)\n            frame_prop = self.scalar_bar.GetFrameProperty()\n            frame_prop.SetColor(color)\n        else:\n            self.scalar_bar.SetDrawFrame(False)\n\n        self.add_actor(self.scalar_bar, reset_camera=False)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_scalars(self, scalars, mesh=None, render=True):\n        if mesh is None:\n            mesh = self.mesh\n\n        if isinstance(mesh, (collections.Iterable, vtki.MultiBlock)):\n            # Recursive if need to update scalars on many meshes\n            for m in mesh:\n                self.update_scalars(scalars, mesh=m, render=False)\n            if render:\n                self.ren_win.Render()\n            return\n\n        if isinstance(scalars, str):\n            # Grab scalar array if name given\n            scalars = get_scalar(mesh, scalars)\n\n        if scalars is None:\n            if render:\n                self.ren_win.Render()\n            return\n\n        if scalars.shape[0] == mesh.GetNumberOfPoints():\n            data = mesh.GetPointData()\n        elif scalars.shape[0] == mesh.GetNumberOfCells():\n            data = mesh.GetCellData()\n        else:\n            _raise_not_matching(scalars, mesh)\n\n        vtk_scalars = data.GetScalars()\n        if vtk_scalars is None:\n            raise Exception('No active scalars')\n        s = VN.vtk_to_numpy(vtk_scalars)\n        s[:] = scalars\n        data.Modified()\n        try:\n            # Why are the points updated here? Not all datasets have points\n            # and only the scalar array is modified by this function...\n            mesh.GetPoints().Modified()\n        except:\n            pass\n\n        if render:\n            self.ren_win.Render()", "response": "Updates the scalars of the object in the plotter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_coordinates(self, points, mesh=None, render=True):\n        if mesh is None:\n            mesh = self.mesh\n\n        mesh.points = points\n\n        if render:\n            self._render()", "response": "Updates the coordinates of the object in the plotter."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncloses the current object and all associated data structures.", "response": "def close(self):\n        \"\"\" closes render window \"\"\"\n        # must close out axes marker\n        if hasattr(self, 'axes_widget'):\n            del self.axes_widget\n\n        # reset scalar bar stuff\n        self._scalar_bar_slots = set(range(MAX_N_COLOR_BARS))\n        self._scalar_bar_slot_lookup = {}\n        self._scalar_bar_ranges = {}\n        self._scalar_bar_mappers = {}\n\n        if hasattr(self, 'ren_win'):\n            self.ren_win.Finalize()\n            del self.ren_win\n\n        if hasattr(self, '_style'):\n            del self._style\n\n        if hasattr(self, 'iren'):\n            self.iren.RemoveAllObservers()\n            del self.iren\n\n        if hasattr(self, 'textActor'):\n            del self.textActor\n\n        # end movie\n        if hasattr(self, 'mwriter'):\n            try:\n                self.mwriter.close()\n            except BaseException:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_text(self, text, position=None, font_size=50, color=None,\n                 font=None, shadow=False, name=None, loc=None):\n        \"\"\"\n        Adds text to plot object in the top left corner by default\n\n        Parameters\n        ----------\n        text : str\n            The text to add the the rendering\n\n        position : tuple(float)\n            Length 2 tuple of the pixelwise position to place the bottom\n            left corner of the text box. Default is to find the top left corner\n            of the renderering window and place text box up there.\n\n        font : string, optional\n            Font name may be courier, times, or arial\n\n        shadow : bool, optional\n            Adds a black shadow to the text.  Defaults to False\n\n        name : str, optional\n            The name for the added actor so that it can be easily updated.\n            If an actor of this name already exists in the rendering window, it\n            will be replaced by the new actor.\n\n        loc : int, tuple, or list\n            Index of the renderer to add the actor to.  For example,\n            ``loc=2`` or ``loc=(1, 1)``.\n\n        Returns\n        -------\n        textActor : vtk.vtkTextActor\n            Text actor added to plot\n\n        \"\"\"\n        if font is None:\n            font = rcParams['font']['family']\n        if font_size is None:\n            font_size = rcParams['font']['size']\n        if color is None:\n            color = rcParams['font']['color']\n        if position is None:\n            # Set the position of the text to the top left corner\n            window_size = self.window_size\n            x = (window_size[0] * 0.02) / self.shape[0]\n            y = (window_size[1] * 0.85) / self.shape[0]\n            position = [x, y]\n\n        self.textActor = vtk.vtkTextActor()\n        self.textActor.SetPosition(position)\n        self.textActor.GetTextProperty().SetFontSize(font_size)\n        self.textActor.GetTextProperty().SetColor(parse_color(color))\n        self.textActor.GetTextProperty().SetFontFamily(FONT_KEYS[font])\n        self.textActor.GetTextProperty().SetShadow(shadow)\n        self.textActor.SetInput(text)\n        self.add_actor(self.textActor, reset_camera=False, name=name, loc=loc)\n        return self.textActor", "response": "Adds text to the top left corner of the text box."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef open_movie(self, filename, framerate=24):\n        if isinstance(vtki.FIGURE_PATH, str) and not os.path.isabs(filename):\n            filename = os.path.join(vtki.FIGURE_PATH, filename)\n        self.mwriter = imageio.get_writer(filename, fps=framerate)", "response": "Establishes a connection to the ffmpeg writer and opens the movie."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef open_gif(self, filename):\n        if filename[-3:] != 'gif':\n            raise Exception('Unsupported filetype.  Must end in .gif')\n        if isinstance(vtki.FIGURE_PATH, str) and not os.path.isabs(filename):\n            filename = os.path.join(vtki.FIGURE_PATH, filename)\n        self._gif_filename = os.path.abspath(filename)\n        self.mwriter = imageio.get_writer(filename, mode='I')", "response": "Open a gif file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting a single frame to the movie file.", "response": "def write_frame(self):\n        \"\"\" Writes a single frame to the movie file \"\"\"\n        if not hasattr(self, 'mwriter'):\n            raise AssertionError('This plotter has not opened a movie or GIF file.')\n        self.mwriter.append_data(self.image)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an array of image depth of the current render window", "response": "def image_depth(self):\n        \"\"\" Returns an image array of current render window \"\"\"\n        ifilter = vtk.vtkWindowToImageFilter()\n        ifilter.SetInput(self.ren_win)\n        ifilter.ReadFrontBufferOff()\n        ifilter.SetInputBufferTypeToZBuffer()\n        return self._run_image_filter(ifilter)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef image(self):\n        if not hasattr(self, 'ren_win') and hasattr(self, 'last_image'):\n            return self.last_image\n        ifilter = vtk.vtkWindowToImageFilter()\n        ifilter.SetInput(self.ren_win)\n        ifilter.ReadFrontBufferOff()\n        if self.image_transparent_background:\n            ifilter.SetInputBufferTypeToRGBA()\n        else:\n            ifilter.SetInputBufferTypeToRGB()\n        return self._run_image_filter(ifilter)", "response": "Returns an array of image objects of current render window"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd lines to the actor.", "response": "def add_lines(self, lines, color=(1, 1, 1), width=5, label=None, name=None):\n        \"\"\"\n        Adds lines to the plotting object.\n\n        Parameters\n        ----------\n        lines : np.ndarray or vtki.PolyData\n            Points representing line segments.  For example, two line segments\n            would be represented as:\n\n            np.array([[0, 0, 0], [1, 0, 0], [1, 0, 0], [1, 1, 0]])\n\n        color : string or 3 item list, optional, defaults to white\n            Either a string, rgb list, or hex color string.  For example:\n                color='white'\n                color='w'\n                color=[1, 1, 1]\n                color='#FFFFFF'\n\n        width : float, optional\n            Thickness of lines\n\n        name : str, optional\n            The name for the added actor so that it can be easily updated.\n            If an actor of this name already exists in the rendering window, it\n            will be replaced by the new actor.\n\n        Returns\n        -------\n        actor : vtk.vtkActor\n            Lines actor.\n\n        \"\"\"\n        if not isinstance(lines, np.ndarray):\n            raise Exception('Input should be an array of point segments')\n\n        lines = vtki.lines_from_points(lines)\n\n        # Create mapper and add lines\n        mapper = vtk.vtkDataSetMapper()\n        mapper.SetInputData(lines)\n\n        rgb_color = parse_color(color)\n\n        # legend label\n        if label:\n            if not isinstance(label, str):\n                raise AssertionError('Label must be a string')\n            self._labels.append([lines, label, rgb_color])\n\n        # Create actor\n        self.scalar_bar = vtk.vtkActor()\n        self.scalar_bar.SetMapper(mapper)\n        self.scalar_bar.GetProperty().SetLineWidth(width)\n        self.scalar_bar.GetProperty().EdgeVisibilityOn()\n        self.scalar_bar.GetProperty().SetEdgeColor(rgb_color)\n        self.scalar_bar.GetProperty().SetColor(rgb_color)\n        self.scalar_bar.GetProperty().LightingOff()\n\n        # Add to renderer\n        self.add_actor(self.scalar_bar, reset_camera=False, name=name)\n        return self.scalar_bar"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_point_labels(self, points, labels, italic=False, bold=True,\n                         font_size=None, text_color='k',\n                         font_family=None, shadow=False,\n                         show_points=True, point_color='k', point_size=5,\n                         name=None):\n        \"\"\"\n        Creates a point actor with one label from list labels assigned to\n        each point.\n\n        Parameters\n        ----------\n        points : np.ndarray\n            n x 3 numpy array of points.\n\n        labels : list\n            List of labels.  Must be the same length as points.\n\n        italic : bool, optional\n            Italicises title and bar labels.  Default False.\n\n        bold : bool, optional\n            Bolds title and bar labels.  Default True\n\n        font_size : float, optional\n            Sets the size of the title font.  Defaults to 16.\n\n        text_color : string or 3 item list, optional, defaults to black\n            Color of text.\n            Either a string, rgb list, or hex color string.  For example:\n\n                text_color='white'\n                text_color='w'\n                text_color=[1, 1, 1]\n                text_color='#FFFFFF'\n\n        font_family : string, optional\n            Font family.  Must be either courier, times, or arial.\n\n        shadow : bool, optional\n            Adds a black shadow to the text.  Defaults to False\n\n        show_points : bool, optional\n            Controls if points are visible.  Default True\n\n        point_color : string or 3 item list, optional, defaults to black\n            Color of points (if visible).\n            Either a string, rgb list, or hex color string.  For example:\n\n                text_color='white'\n                text_color='w'\n                text_color=[1, 1, 1]\n                text_color='#FFFFFF'\n\n        point_size : float, optional\n            Size of points (if visible)\n\n        name : str, optional\n            The name for the added actor so that it can be easily updated.\n            If an actor of this name already exists in the rendering window, it\n            will be replaced by the new actor.\n\n        Returns\n        -------\n        labelMapper : vtk.vtkvtkLabeledDataMapper\n            VTK label mapper.  Can be used to change properties of the labels.\n\n        \"\"\"\n        if font_family is None:\n            font_family = rcParams['font']['family']\n        if font_size is None:\n            font_size = rcParams['font']['size']\n\n        if len(points) != len(labels):\n            raise Exception('There must be one label for each point')\n\n        vtkpoints = vtki.PolyData(points)\n\n        vtklabels = vtk.vtkStringArray()\n        vtklabels.SetName('labels')\n        for item in labels:\n            vtklabels.InsertNextValue(str(item))\n        vtkpoints.GetPointData().AddArray(vtklabels)\n\n        # create label mapper\n        labelMapper = vtk.vtkLabeledDataMapper()\n        labelMapper.SetInputData(vtkpoints)\n        textprop = labelMapper.GetLabelTextProperty()\n        textprop.SetItalic(italic)\n        textprop.SetBold(bold)\n        textprop.SetFontSize(font_size)\n        textprop.SetFontFamily(parse_font_family(font_family))\n        textprop.SetColor(parse_color(text_color))\n        textprop.SetShadow(shadow)\n        labelMapper.SetLabelModeToLabelFieldData()\n        labelMapper.SetFieldDataName('labels')\n\n        labelActor = vtk.vtkActor2D()\n        labelActor.SetMapper(labelMapper)\n\n        # add points\n        if show_points:\n            style = 'points'\n        else:\n            style = 'surface'\n        self.add_mesh(vtkpoints, style=style, color=point_color,\n                      point_size=point_size)\n\n        self.add_actor(labelActor, reset_camera=False, name=name)\n        return labelMapper", "response": "Creates a point actor with one label from a list of points and one label from a list of labels."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_arrows(self, cent, direction, mag=1, **kwargs):\n        direction = direction.copy()\n        if cent.ndim != 2:\n            cent = cent.reshape((-1, 3))\n\n        if direction.ndim != 2:\n            direction = direction.reshape((-1, 3))\n\n        direction[:,0] *= mag\n        direction[:,1] *= mag\n        direction[:,2] *= mag\n\n        pdata = vtki.vector_poly_data(cent, direction)\n        # Create arrow object\n        arrow = vtk.vtkArrowSource()\n        arrow.Update()\n        glyph3D = vtk.vtkGlyph3D()\n        glyph3D.SetSourceData(arrow.GetOutput())\n        glyph3D.SetInputData(pdata)\n        glyph3D.SetVectorModeToUseVector()\n        glyph3D.Update()\n\n        arrows = wrap(glyph3D.GetOutput())\n\n        return self.add_mesh(arrows, **kwargs)", "response": "Adds arrows to the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef screenshot(self, filename=None, transparent_background=None,\n                   return_img=None, window_size=None):\n        \"\"\"\n        Takes screenshot at current camera position\n\n        Parameters\n        ----------\n        filename : str, optional\n            Location to write image to.  If None, no image is written.\n\n        transparent_background : bool, optional\n            Makes the background transparent.  Default False.\n\n        return_img : bool, optional\n            If a string filename is given and this is true, a NumPy array of\n            the image will be returned.\n\n        Returns\n        -------\n        img :  numpy.ndarray\n            Array containing pixel RGB and alpha.  Sized:\n            [Window height x Window width x 3] for transparent_background=False\n            [Window height x Window width x 4] for transparent_background=True\n\n        Examples\n        --------\n        >>> import vtki\n        >>> sphere = vtki.Sphere()\n        >>> plotter = vtki.Plotter()\n        >>> actor = plotter.add_mesh(sphere)\n        >>> plotter.screenshot('screenshot.png') # doctest:+SKIP\n        \"\"\"\n        if window_size is not None:\n            self.window_size = window_size\n\n        # configure image filter\n        if transparent_background is None:\n            transparent_background = rcParams['transparent_background']\n        self.image_transparent_background = transparent_background\n\n        # This if statement allows you to save screenshots of closed plotters\n        # This is needed for the sphinx-gallery work\n        if not hasattr(self, 'ren_win'):\n            # If plotter has been closed...\n            # check if last_image exists\n            if hasattr(self, 'last_image'):\n                # Save last image\n                return self._save_image(self.last_image, filename, return_img)\n            # Plotter hasn't been rendered or was improperly closed\n            raise AttributeError('This plotter is unable to save a screenshot.')\n\n        if isinstance(self, Plotter):\n            # TODO: we need a consistent rendering function\n            self.render()\n        else:\n            self._render()\n\n        # debug: this needs to be called twice for some reason,\n        img = self.image\n        img = self.image\n\n        return self._save_image(img, filename, return_img)", "response": "Takes a screenshot of the current camera position and saves it to a file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_legend(self, labels=None, bcolor=(0.5, 0.5, 0.5), border=False,\n                   size=None, name=None):\n        \"\"\"\n        Adds a legend to render window.  Entries must be a list\n        containing one string and color entry for each item.\n\n        Parameters\n        ----------\n        labels : list, optional\n            When set to None, uses existing labels as specified by\n\n            - add_mesh\n            - add_lines\n            - add_points\n\n            List contianing one entry for each item to be added to the\n            legend.  Each entry must contain two strings, [label,\n            color], where label is the name of the item to add, and\n            color is the color of the label to add.\n\n        bcolor : list or string, optional\n            Background color, either a three item 0 to 1 RGB color\n            list, or a matplotlib color string (e.g. 'w' or 'white'\n            for a white color).  If None, legend background is\n            disabled.\n\n        border : bool, optional\n            Controls if there will be a border around the legend.\n            Default False.\n\n        size : list, optional\n            Two float list, each float between 0 and 1.  For example\n            [0.1, 0.1] would make the legend 10% the size of the\n            entire figure window.\n\n        name : str, optional\n            The name for the added actor so that it can be easily updated.\n            If an actor of this name already exists in the rendering window, it\n            will be replaced by the new actor.\n\n        Returns\n        -------\n        legend : vtk.vtkLegendBoxActor\n            Actor for the legend.\n\n        Examples\n        --------\n        >>> import vtki\n        >>> from vtki import examples\n        >>> mesh = examples.load_hexbeam()\n        >>> othermesh = examples.load_uniform()\n        >>> plotter = vtki.Plotter()\n        >>> _ = plotter.add_mesh(mesh, label='My Mesh')\n        >>> _ = plotter.add_mesh(othermesh, 'k', label='My Other Mesh')\n        >>> _ = plotter.add_legend()\n        >>> plotter.show() # doctest:+SKIP\n\n        Alternative manual example\n\n        >>> import vtki\n        >>> from vtki import examples\n        >>> mesh = examples.load_hexbeam()\n        >>> othermesh = examples.load_uniform()\n        >>> legend_entries = []\n        >>> legend_entries.append(['My Mesh', 'w'])\n        >>> legend_entries.append(['My Other Mesh', 'k'])\n        >>> plotter = vtki.Plotter()\n        >>> _ = plotter.add_mesh(mesh)\n        >>> _ = plotter.add_mesh(othermesh, 'k')\n        >>> _ = plotter.add_legend(legend_entries)\n        >>> plotter.show() # doctest:+SKIP\n        \"\"\"\n        self.legend = vtk.vtkLegendBoxActor()\n\n        if labels is None:\n            # use existing labels\n            if not self._labels:\n                raise Exception('No labels input.\\n\\n' +\n                                'Add labels to individual items when adding them to' +\n                                'the plotting object with the \"label=\" parameter.  ' +\n                                'or enter them as the \"labels\" parameter.')\n\n            self.legend.SetNumberOfEntries(len(self._labels))\n            for i, (vtk_object, text, color) in enumerate(self._labels):\n                self.legend.SetEntry(i, vtk_object, text, parse_color(color))\n\n        else:\n            self.legend.SetNumberOfEntries(len(labels))\n            legendface = single_triangle()\n            for i, (text, color) in enumerate(labels):\n                self.legend.SetEntry(i, legendface, text, parse_color(color))\n\n        if size:\n            self.legend.SetPosition2(size[0], size[1])\n\n        if bcolor is None:\n            self.legend.UseBackgroundOff()\n        else:\n            self.legend.UseBackgroundOn()\n            self.legend.SetBackgroundColor(bcolor)\n\n        if border:\n            self.legend.BorderOn()\n        else:\n            self.legend.BorderOff()\n\n        # Add to renderer\n        self.add_actor(self.legend, reset_camera=False, name=name)\n        return self.legend", "response": "Adds a legend to the current figure window."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the background color of the available renderer objects.", "response": "def set_background(self, color, loc='all'):\n        \"\"\"\n        Sets background color\n\n        Parameters\n        ----------\n        color : string or 3 item list, optional, defaults to white\n            Either a string, rgb list, or hex color string.  For example:\n                color='white'\n                color='w'\n                color=[1, 1, 1]\n                color='#FFFFFF'\n\n        loc : int, tuple, list, or str, optional\n            Index of the renderer to add the actor to.  For example,\n            ``loc=2`` or ``loc=(1, 1)``.  If ``loc='all'`` then all\n            render windows will have their background set.\n\n        \"\"\"\n        if color is None:\n            color = rcParams['background']\n        if isinstance(color, str):\n            if color.lower() in 'paraview' or color.lower() in 'pv':\n                # Use the default ParaView background color\n                color = PV_BACKGROUND\n            else:\n                color = vtki.string_to_rgb(color)\n\n        if loc =='all':\n            for renderer in self.renderers:\n                renderer.SetBackground(color)\n        else:\n            renderer = self.renderers[self.loc_to_index(loc)]\n            renderer.SetBackground(color)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nenable picking of cells.", "response": "def enable_cell_picking(self, mesh=None, callback=None):\n        \"\"\"\n        Enables picking of cells.  Press r to enable retangle based\n        selection.  Press \"r\" again to turn it off.  Selection will be\n        saved to self.picked_cells.\n\n        Uses last input mesh for input\n\n        Parameters\n        ----------\n        mesh : vtk.UnstructuredGrid, optional\n            UnstructuredGrid grid to select cells from.  Uses last\n            input grid by default.\n\n        callback : function, optional\n            When input, calls this function after a selection is made.\n            The picked_cells are input as the first parameter to this function.\n\n        \"\"\"\n        if mesh is None:\n            if not hasattr(self, 'mesh'):\n                raise Exception('Input a mesh into the Plotter class first or '\n                                + 'or set it in this function')\n            mesh = self.mesh\n\n        def pick_call_back(picker, event_id):\n            extract = vtk.vtkExtractGeometry()\n            mesh.cell_arrays['orig_extract_id'] = np.arange(mesh.n_cells)\n            extract.SetInputData(mesh)\n            extract.SetImplicitFunction(picker.GetFrustum())\n            extract.Update()\n            self.picked_cells = vtki.wrap(extract.GetOutput())\n\n            if callback is not None:\n                callback(self.picked_cells)\n\n        area_picker = vtk.vtkAreaPicker()\n        area_picker.AddObserver(vtk.vtkCommand.EndPickEvent, pick_call_back)\n\n        self.enable_rubber_band_style()\n        self.iren.SetPicker(area_picker)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates an orbital path around the data scene.", "response": "def generate_orbital_path(self, factor=3., n_points=20, viewup=None, z_shift=None):\n        \"\"\"Genrates an orbital path around the data scene\n\n        Parameters\n        ----------\n        facotr : float\n            A scaling factor when biulding the orbital extent\n\n        n_points : int\n            number of points on the orbital path\n\n        viewup : list(float)\n            the normal to the orbital plane\n\n        z_shift : float, optional\n            shift the plane up/down from the center of the scene by this amount\n        \"\"\"\n        if viewup is None:\n            viewup = rcParams['camera']['viewup']\n        center = list(self.center)\n        bnds = list(self.bounds)\n        if z_shift is None:\n            z_shift = (bnds[5] - bnds[4]) * factor\n        center[2] = center[2] + z_shift\n        radius = (bnds[1] - bnds[0]) * factor\n        y = (bnds[3] - bnds[2]) * factor\n        if y > radius:\n            radius = y\n        return vtki.Polygon(center=center, radius=radius, normal=viewup, n_sides=n_points)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\norbit on the given path focusing on the focus point and viewup.", "response": "def orbit_on_path(self, path=None, focus=None, step=0.5, viewup=None, bkg=True):\n        \"\"\"Orbit on the given path focusing on the focus point\n\n        Parameters\n        ----------\n        path : vtki.PolyData\n            Path of orbital points. The order in the points is the order of\n            travel\n\n        focus : list(float) of length 3, optional\n            The point ot focus the camera.\n\n        step : float, optional\n            The timestep between flying to each camera position\n\n        viewup : list(float)\n            the normal to the orbital plane\n        \"\"\"\n        if focus is None:\n            focus = self.center\n        if viewup is None:\n            viewup = rcParams['camera']['viewup']\n        if path is None:\n            path = self.generate_orbital_path(viewup=viewup)\n        if not is_vtki_obj(path):\n            path = vtki.PolyData(path)\n        points = path.points\n\n        def orbit():\n            \"\"\"Internal thread for running the orbit\"\"\"\n            for point in points:\n                self.set_position(point)\n                self.set_focus(focus)\n                self.set_viewup(viewup)\n                time.sleep(step)\n\n\n        if bkg:\n            thread = Thread(target=orbit)\n            thread.start()\n        else:\n            orbit()\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export_vtkjs(self, filename, compress_arrays=False):\n        if not hasattr(self, 'ren_win'):\n            raise RuntimeError('Export must be called before showing/closing the scene.')\n        if isinstance(vtki.FIGURE_PATH, str) and not os.path.isabs(filename):\n            filename = os.path.join(vtki.FIGURE_PATH, filename)\n        return export_plotter_vtkjs(self, filename, compress_arrays=compress_arrays)", "response": "Export the current rendering scene as VTKjs scene for the current rendering scene in a web browser."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new window with the specified title window_size interactive and auto - close.", "response": "def show(self, title=None, window_size=None, interactive=True,\n             auto_close=True, interactive_update=False, full_screen=False,\n             screenshot=False, return_img=False, use_panel=None):\n        \"\"\"\n        Creates plotting window\n\n        Parameters\n        ----------\n        title : string, optional\n            Title of plotting window.\n\n        window_size : list, optional\n            Window size in pixels.  Defaults to [1024, 768]\n\n        interactive : bool, optional\n            Enabled by default.  Allows user to pan and move figure.\n\n        auto_close : bool, optional\n            Enabled by default.  Exits plotting session when user\n            closes the window when interactive is True.\n\n        interactive_update: bool, optional\n            Disabled by default.  Allows user to non-blocking draw,\n            user should call Update() in each iteration.\n\n        full_screen : bool, optional\n            Opens window in full screen.  When enabled, ignores\n            window_size.  Default False.\n\n        use_panel : bool, optional\n            If False, the interactive rendering from panel will not be used in\n            notebooks\n\n        Returns\n        -------\n        cpos : list\n            List of camera position, focal point, and view up\n\n        \"\"\"\n        if use_panel is None:\n            use_panel = rcParams['use_panel']\n        # reset unless camera for the first render unless camera is set\n        if self._first_time:  # and not self.camera_set:\n            for renderer in self.renderers:\n                if not renderer.camera_set:\n                    renderer.camera_position = renderer.get_default_cam_pos()\n                    renderer.ResetCamera()\n            self._first_time = False\n\n        if title:\n            self.ren_win.SetWindowName(title)\n\n        # if full_screen:\n        if full_screen:\n            self.ren_win.SetFullScreen(True)\n            self.ren_win.BordersOn()  # super buggy when disabled\n        else:\n            if window_size is None:\n                window_size = self.window_size\n            self.ren_win.SetSize(window_size[0], window_size[1])\n\n        # Render\n        log.debug('Rendering')\n        self.ren_win.Render()\n\n        # Keep track of image for sphinx-gallery\n        self.last_image = self.screenshot(screenshot, return_img=True)\n        disp = None\n\n        if interactive and (not self.off_screen):\n            try:  # interrupts will be caught here\n                log.debug('Starting iren')\n                self.update_style()\n                self.iren.Initialize()\n                if not interactive_update:\n                    self.iren.Start()\n            except KeyboardInterrupt:\n                log.debug('KeyboardInterrupt')\n                self.close()\n                raise KeyboardInterrupt\n        elif self.notebook and use_panel:\n            try:\n                from panel.pane import VTK as panel_display\n                disp = panel_display(self.ren_win, sizing_mode='stretch_width',\n                                     height=400)\n            except:\n                pass\n        # NOTE: after this point, nothing from the render window can be accessed\n        #       as if a user presed the close button, then it destroys the\n        #       the render view and a stream of errors will kill the Python\n        #       kernel if code here tries to access that renderer.\n        #       See issues #135 and #186 for insight before editing the\n        #       remainder of this function.\n\n        # Get camera position before closing\n        cpos = self.camera_position\n\n        # NOTE: our conversion to panel currently does not support mult-view\n        #       so we should display the static screenshot in notebooks for\n        #       multi-view plots until we implement this feature\n        # If notebook is true and panel display failed:\n        if self.notebook and (disp is None or self.shape != (1,1)):\n            import PIL.Image\n            # sanity check\n            try:\n                import IPython\n            except ImportError:\n                raise Exception('Install IPython to display image in a notebook')\n            disp = IPython.display.display(PIL.Image.fromarray(self.last_image))\n\n        # Cleanup\n        if auto_close:\n            self.close()\n\n        # Return the notebook display: either panel object or image display\n        if self.notebook:\n            return disp\n\n        # If user asked for screenshot, return as numpy array after camera\n        # position\n        if return_img or screenshot == True:\n            return cpos, self.last_image\n\n        # default to returning last used camera position\n        return cpos"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_structured():\n    x = np.arange(-10, 10, 0.25)\n    y = np.arange(-10, 10, 0.25)\n    x, y = np.meshgrid(x, y)\n    r = np.sqrt(x**2 + y**2)\n    z = np.sin(r)\n    return vtki.StructuredGrid(x, y, z)", "response": "Loads a simple StructuredGrid"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a plot class to plot two ants and airplane.", "response": "def plot_ants_plane(off_screen=False, notebook=None):\n    \"\"\"\n    Demonstrate how to create a plot class to plot multiple meshes while\n    adding scalars and text.\n\n    Plot two ants and airplane\n    \"\"\"\n\n    # load and shrink airplane\n    airplane = vtki.PolyData(planefile)\n    airplane.points /= 10\n    # pts = airplane.points # gets pointer to array\n    # pts /= 10  # shrink\n\n    # rotate and translate ant so it is on the plane\n    ant = vtki.PolyData(antfile)\n    ant.rotate_x(90)\n    ant.translate([90, 60, 15])\n\n    # Make a copy and add another ant\n    ant_copy = ant.copy()\n    ant_copy.translate([30, 0, -10])\n\n    # Create plotting object\n    plotter = vtki.Plotter(off_screen=off_screen, notebook=notebook)\n    plotter.add_mesh(ant, 'r')\n    plotter.add_mesh(ant_copy, 'b')\n\n    # Add airplane mesh and make the color equal to the Y position\n    plane_scalars = airplane.points[:, 1]\n    plotter.add_mesh(airplane, scalars=plane_scalars, stitle='Plane Y\\nLocation')\n    plotter.add_text('Ants and Plane Example')\n    plotter.plot()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot_wave(fps=30, frequency=1, wavetime=3, interactive=False,\n              off_screen=False, notebook=None):\n    \"\"\"\n    Plot a 3D moving wave in a render window.\n\n    Parameters\n    ----------\n    fps : int, optional\n        Maximum frames per second to display.  Defaults to 30.\n\n    frequency: float, optional\n        Wave cycles per second.  Defaults to 1\n\n    wavetime : float, optional\n        The desired total display time in seconds.  Defaults to 3 seconds.\n\n    interactive: bool, optional\n        Allows the user to set the camera position before the start of the\n        wave movement.  Default False.\n\n    off_screen : bool, optional\n        Enables off screen rendering when True.  Used for automated testing.\n        Disabled by default.\n\n    Returns\n    -------\n    points : np.ndarray\n        Position of points at last frame.\n\n    \"\"\"\n    # camera position\n    cpos = [(6.879481857604187, -32.143727535933195, 23.05622921691103),\n            (-0.2336056403734026, -0.6960083534590372, -0.7226721553894022),\n            (-0.008900669873416645, 0.6018246347860926, 0.7985786667826725)]\n\n    # Make data\n    X = np.arange(-10, 10, 0.25)\n    Y = np.arange(-10, 10, 0.25)\n    X, Y = np.meshgrid(X, Y)\n    R = np.sqrt(X**2 + Y**2)\n    Z = np.sin(R)\n\n    # Create and plot structured grid\n    sgrid = vtki.StructuredGrid(X, Y, Z)\n\n    # Get pointer to points\n    points = sgrid.points.copy()\n\n    # Start a plotter object and set the scalars to the Z height\n    plotter = vtki.Plotter(off_screen=off_screen, notebook=notebook)\n    plotter.add_mesh(sgrid, scalars=Z.ravel())\n    plotter.camera_position = cpos\n    plotter.plot(title='Wave Example', window_size=[800, 600],\n                 # auto_close=False, interactive=interactive)\n                 auto_close=False, interactive_update=True)\n\n    # Update Z and display a frame for each updated position\n    tdelay = 1. / fps\n    tlast = time.time()\n    tstart = time.time()\n    while time.time() - tstart < wavetime:\n        # get phase from start\n        telap = time.time() - tstart\n        phase = telap * 2 * np.pi * frequency\n        Z = np.sin(R + phase)\n        points[:, -1] = Z.ravel()\n\n        # update plotting object, but don't automatically render\n        plotter.update_coordinates(points, render=False)\n        plotter.update_scalars(Z.ravel(), render=False)\n\n        # Render and get time to render\n        rstart = time.time()\n        plotter.update()\n        # plotter.render()\n        rstop = time.time()\n\n        # time delay\n        tpast = time.time() - tlast\n        if tpast < tdelay and tpast >= 0:\n            time.sleep(tdelay - tpast)\n\n        # get render time and actual FPS\n        # rtime = rstop - rstart\n        # act_fps = 1 / (time.time() - tlast + 1E-10)\n        tlast = time.time()\n\n    # Close movie and delete object\n    plotter.close()\n\n    return points", "response": "Plots a 3D moving wave in a render window."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a plane from a normal vector and a origin vector.", "response": "def _generate_plane(normal, origin):\n    \"\"\" Returns a vtk.vtkPlane \"\"\"\n    plane = vtk.vtkPlane()\n    plane.SetNormal(normal[0], normal[1], normal[2])\n    plane.SetOrigin(origin[0], origin[1], origin[2])\n    return plane"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clip(dataset, normal='x', origin=None, invert=True):\n        if isinstance(normal, str):\n            normal = NORMALS[normal.lower()]\n        # find center of data if origin not specified\n        if origin is None:\n            origin = dataset.center\n        # create the plane for clipping\n        plane = _generate_plane(normal, origin)\n        # run the clip\n        alg = vtk.vtkClipDataSet()\n        alg.SetInputDataObject(dataset) # Use the grid as the data we desire to cut\n        alg.SetClipFunction(plane) # the the cutter to use the plane we made\n        alg.SetInsideOut(invert) # invert the clip if needed\n        alg.Update() # Perfrom the Cut\n        return _get_output(alg)", "response": "Clip a dataset by a plane by specifying the origin and normal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nclipping a dataset by a bounding box defined by the bounds.", "response": "def clip_box(dataset, bounds=None, invert=True, factor=0.35):\n        \"\"\"Clips a dataset by a bounding box defined by the bounds. If no bounds\n        are given, a corner of the dataset bounds will be removed.\n\n        Parameters\n        ----------\n        bounds : tuple(float)\n            Length 6 iterable of floats: (xmin, xmax, ymin, ymax, zmin, zmax)\n\n        invert : bool\n            Flag on whether to flip/invert the clip\n\n        factor : float, optional\n            If bounds are not given this is the factor along each axis to\n            extract the default box.\n\n        \"\"\"\n        if bounds is None:\n            def _get_quarter(dmin, dmax):\n                \"\"\"internal helper to get a section of the given range\"\"\"\n                return dmax - ((dmax - dmin) * factor)\n            xmin, xmax, ymin, ymax, zmin, zmax = dataset.bounds\n            xmin = _get_quarter(xmin, xmax)\n            ymin = _get_quarter(ymin, ymax)\n            zmin = _get_quarter(zmin, zmax)\n            bounds = [xmin, xmax, ymin, ymax, zmin, zmax]\n        if isinstance(bounds, (float, int)):\n            bounds = [bounds, bounds, bounds]\n        if len(bounds) == 3:\n            xmin, xmax, ymin, ymax, zmin, zmax = dataset.bounds\n            bounds = (xmin,xmin+bounds[0], ymin,ymin+bounds[1], zmin,zmin+bounds[2])\n        if not isinstance(bounds, collections.Iterable) or len(bounds) != 6:\n            raise AssertionError('Bounds must be a length 6 iterable of floats')\n        xmin, xmax, ymin, ymax, zmin, zmax = bounds\n        alg = vtk.vtkBoxClipDataSet()\n        alg.SetInputDataObject(dataset)\n        alg.SetBoxClip(xmin, xmax, ymin, ymax, zmin, zmax)\n        port = 0\n        if invert:\n            # invert the clip if needed\n            port = 1\n            alg.GenerateClippedOutputOn()\n        alg.Update()\n        return _get_output(alg, oport=port)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef slice(dataset, normal='x', origin=None, generate_triangles=False,\n              contour=False):\n        \"\"\"Slice a dataset by a plane at the specified origin and normal vector\n        orientation. If no origin is specified, the center of the input dataset will\n        be used.\n\n        Parameters\n        ----------\n        normal : tuple(float) or str\n            Length 3 tuple for the normal vector direction. Can also be\n            specified as a string conventional direction such as ``'x'`` for\n            ``(1,0,0)`` or ``'-x'`` for ``(-1,0,0)```, etc.\n\n        origin : tuple(float)\n            The center (x,y,z) coordinate of the plane on which the slice occurs\n\n        generate_triangles: bool, optional\n            If this is enabled (``False`` by default), the output will be\n            triangles otherwise, the output will be the intersection polygons.\n\n        contour : bool, optional\n            If True, apply a ``contour`` filter after slicing\n\n        \"\"\"\n        if isinstance(normal, str):\n            normal = NORMALS[normal.lower()]\n        # find center of data if origin not specified\n        if origin is None:\n            origin = dataset.center\n        if not is_inside_bounds(origin, dataset.bounds):\n            raise AssertionError('Slice is outside data bounds.')\n        # create the plane for clipping\n        plane = _generate_plane(normal, origin)\n        # create slice\n        alg = vtk.vtkCutter() # Construct the cutter object\n        alg.SetInputDataObject(dataset) # Use the grid as the data we desire to cut\n        alg.SetCutFunction(plane) # the the cutter to use the plane we made\n        if not generate_triangles:\n            alg.GenerateTrianglesOff()\n        alg.Update() # Perfrom the Cut\n        output = _get_output(alg)\n        if contour:\n            return output.contour()\n        return output", "response": "Slice a dataset by a plane at the specified origin and normal vector."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef slice_orthogonal(dataset, x=None, y=None, z=None,\n                         generate_triangles=False, contour=False):\n        \"\"\"Creates three orthogonal slices through the dataset on the three\n        caresian planes. Yields a MutliBlock dataset of the three slices\n\n        Parameters\n        ----------\n        x : float\n            The X location of the YZ slice\n\n        y : float\n            The Y location of the XZ slice\n\n        z : float\n            The Z location of the XY slice\n\n        generate_triangles: bool, optional\n            If this is enabled (``False`` by default), the output will be\n            triangles otherwise, the output will be the intersection polygons.\n\n        contour : bool, optional\n            If True, apply a ``contour`` filter after slicing\n\n        \"\"\"\n        output = vtki.MultiBlock()\n        # Create the three slices\n        if x is None:\n            x = dataset.center[0]\n        if y is None:\n            y = dataset.center[1]\n        if z is None:\n            z = dataset.center[2]\n        output[0, 'YZ'] = dataset.slice(normal='x', origin=[x,y,z], generate_triangles=generate_triangles)\n        output[1, 'XZ'] = dataset.slice(normal='y', origin=[x,y,z], generate_triangles=generate_triangles)\n        output[2, 'XY'] = dataset.slice(normal='z', origin=[x,y,z], generate_triangles=generate_triangles)\n        return output", "response": "Creates three orthogonal slices through the dataset on the three orthogonal planes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef slice_along_axis(dataset, n=5, axis='x', tolerance=None,\n                         generate_triangles=False, contour=False):\n        \"\"\"Create many slices of the input dataset along a specified axis.\n\n        Parameters\n        ----------\n        n : int\n            The number of slices to create\n\n        axis : str or int\n            The axis to generate the slices along. Perpendicular to the slices.\n            Can be string name (``'x'``, ``'y'``, or ``'z'``) or axis index\n            (``0``, ``1``, or ``2``).\n\n        tolerance : float, optional\n            The toleranceerance to the edge of the dataset bounds to create the slices\n\n        generate_triangles: bool, optional\n            If this is enabled (``False`` by default), the output will be\n            triangles otherwise, the output will be the intersection polygons.\n\n        contour : bool, optional\n            If True, apply a ``contour`` filter after slicing\n\n        \"\"\"\n        axes = {'x':0, 'y':1, 'z':2}\n        output = vtki.MultiBlock()\n        if isinstance(axis, int):\n            ax = axis\n            axis = list(axes.keys())[list(axes.values()).index(ax)]\n        elif isinstance(axis, str):\n            try:\n                ax = axes[axis]\n            except KeyError:\n                raise RuntimeError('Axis ({}) not understood'.format(axis))\n        # get the locations along that axis\n        if tolerance is None:\n            tolerance = (dataset.bounds[ax*2+1] - dataset.bounds[ax*2]) * 0.01\n        rng = np.linspace(dataset.bounds[ax*2]+tolerance, dataset.bounds[ax*2+1]-tolerance, n)\n        center = list(dataset.center)\n        # Make each of the slices\n        for i in range(n):\n            center[ax] = rng[i]\n            slc = DataSetFilters.slice(dataset, normal=axis, origin=center,\n                    generate_triangles=generate_triangles, contour=contour)\n            output[i, 'slice%.2d'%i] = slc\n        return output", "response": "Create many slices of the input dataset along a specified axis."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nthreshold the dataset by a percentage of its active scalars or as specified.", "response": "def threshold_percent(dataset, percent=0.50, scalars=None, invert=False,\n                          continuous=False, preference='cell'):\n        \"\"\"Thresholds the dataset by a percentage of its range on the active\n        scalar array or as specified\n\n        Parameters\n        ----------\n        percent : float or tuple(float), optional\n            The percentage (0,1) to threshold. If value is out of 0 to 1 range,\n            then it will be divided by 100 and checked to be in that range.\n\n        scalars : str, optional\n            Name of scalars to threshold on. Defaults to currently active scalars.\n\n        invert : bool, optional\n            When invert is True cells are kept when their values are below the\n            percentage of the range.  When invert is False, cells are kept when\n            their value is above the percentage of the range.\n            Default is False: yielding above the threshold \"value\".\n\n        continuous : bool, optional\n            When True, the continuous interval [minimum cell scalar,\n            maxmimum cell scalar] will be used to intersect the threshold bound,\n            rather than the set of discrete scalar values from the vertices.\n\n        preference : str, optional\n            When scalars is specified, this is the perfered scalar type to\n            search for in the dataset.  Must be either ``'point'`` or ``'cell'``\n\n        \"\"\"\n        if scalars is None:\n            _, tscalars = dataset.active_scalar_info\n        else:\n            tscalars = scalars\n        dmin, dmax = dataset.get_data_range(arr=tscalars, preference=preference)\n\n        def _check_percent(percent):\n            \"\"\"Make sure percent is between 0 and 1 or fix if between 0 and 100.\"\"\"\n            if percent >= 1:\n                percent = float(percent) / 100.0\n                if percent > 1:\n                    raise RuntimeError('Percentage ({}) is out of range (0, 1).'.format(percent))\n            if percent < 1e-10:\n                raise RuntimeError('Percentage ({}) is too close to zero or negative.'.format(percent))\n            return percent\n\n        def _get_val(percent, dmin, dmax):\n            \"\"\"Gets the value from a percentage of a range\"\"\"\n            percent = _check_percent(percent)\n            return dmin + float(percent) * (dmax - dmin)\n\n        # Compute the values\n        if isinstance(percent, collections.Iterable):\n            # Get two values\n            value = [_get_val(percent[0], dmin, dmax), _get_val(percent[1], dmin, dmax)]\n        else:\n            # Compute one value to threshold\n            value = _get_val(percent, dmin, dmax)\n        # Use the normal thresholding function on these values\n        return DataSetFilters.threshold(dataset, value=value, scalars=scalars,\n                    invert=invert, continuous=continuous, preference=preference)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nproduces an outline of the full extent for the input dataset.", "response": "def outline(dataset, generate_faces=False):\n        \"\"\"Produces an outline of the full extent for the input dataset.\n\n        Parameters\n        ----------\n        generate_faces : bool, optional\n            Generate solid faces for the box. This is off by default\n\n        \"\"\"\n        alg = vtk.vtkOutlineFilter()\n        alg.SetInputDataObject(dataset)\n        alg.SetGenerateFaces(generate_faces)\n        alg.Update()\n        return wrap(alg.GetOutputDataObject(0))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nproduces an outline of the corners for the input dataset.", "response": "def outline_corners(dataset, factor=0.2):\n        \"\"\"Produces an outline of the corners for the input dataset.\n\n        Parameters\n        ----------\n        factor : float, optional\n            controls the relative size of the corners to the length of the\n            corresponding bounds\n\n        \"\"\"\n        alg = vtk.vtkOutlineCornerFilter()\n        alg.SetInputDataObject(dataset)\n        alg.SetCornerFactor(factor)\n        alg.Update()\n        return wrap(alg.GetOutputDataObject(0))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts the outer surface of a volume or structured grid dataset as PolyData. This will extract all 0 1 and 2D cells producing the boundary faces of the dataset.", "response": "def extract_geometry(dataset):\n        \"\"\"Extract the outer surface of a volume or structured grid dataset as\n        PolyData. This will extract all 0D, 1D, and 2D cells producing the\n        boundary faces of the dataset.\n        \"\"\"\n        alg = vtk.vtkGeometryFilter()\n        alg.SetInputDataObject(dataset)\n        alg.Update()\n        return _get_output(alg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts all the internal and external edges of the input dataset as PolyData. This produces a full wireframe representation of the input dataset.", "response": "def wireframe(dataset):\n        \"\"\"Extract all the internal/external edges of the dataset as PolyData.\n        This produces a full wireframe representation of the input dataset.\n        \"\"\"\n        alg = vtk.vtkExtractEdges()\n        alg.SetInputDataObject(dataset)\n        alg.Update()\n        return _get_output(alg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates scalar values on a dataset.", "response": "def elevation(dataset, low_point=None, high_point=None, scalar_range=None,\n                  preference='point', set_active=True):\n        \"\"\"Generate scalar values on a dataset.  The scalar values lie within a\n        user specified range, and are generated by computing a projection of\n        each dataset point onto a line.\n        The line can be oriented arbitrarily.\n        A typical example is to generate scalars based on elevation or height\n        above a plane.\n\n        Parameters\n        ----------\n        low_point : tuple(float), optional\n            The low point of the projection line in 3D space. Default is bottom\n            center of the dataset. Otherwise pass a length 3 tuple(float).\n\n        high_point : tuple(float), optional\n            The high point of the projection line in 3D space. Default is top\n            center of the dataset. Otherwise pass a length 3 tuple(float).\n\n        scalar_range : str or tuple(float), optional\n            The scalar range to project to the low and high points on the line\n            that will be mapped to the dataset. If None given, the values will\n            be computed from the elevation (Z component) range between the\n            high and low points. Min and max of a range can be given as a length\n            2 tuple(float). If ``str`` name of scalara array present in the\n            dataset given, the valid range of that array will be used.\n\n        preference : str, optional\n            When a scalar name is specified for ``scalar_range``, this is the\n            perfered scalar type to search for in the dataset.\n            Must be either 'point' or 'cell'.\n\n        set_active : bool, optional\n            A boolean flag on whethter or not to set the new `Elevation` scalar\n            as the active scalar array on the output dataset.\n\n        Warning\n        -------\n        This will create a scalar array named `Elevation` on the point data of\n        the input dataset and overasdf write an array named `Elevation` if present.\n\n        \"\"\"\n        # Fix the projection line:\n        if low_point is None:\n            low_point = list(dataset.center)\n            low_point[2] = dataset.bounds[4]\n        if high_point is None:\n            high_point = list(dataset.center)\n            high_point[2] = dataset.bounds[5]\n        # Fix scalar_range:\n        if scalar_range is None:\n            scalar_range = (low_point[2], high_point[2])\n        elif isinstance(scalar_range, str):\n            scalar_range = dataset.get_data_range(arr=scalar_range, preference=preference)\n        elif isinstance(scalar_range, collections.Iterable):\n            if len(scalar_range) != 2:\n                raise AssertionError('scalar_range must have a length of two defining the min and max')\n        else:\n            raise RuntimeError('scalar_range argument ({}) not understood.'.format(type(scalar_range)))\n        # Construct the filter\n        alg = vtk.vtkElevationFilter()\n        alg.SetInputDataObject(dataset)\n        # Set the parameters\n        alg.SetScalarRange(scalar_range)\n        alg.SetLowPoint(low_point)\n        alg.SetHighPoint(high_point)\n        alg.Update()\n        # Decide on updating active scalar array\n        name = 'Elevation' # Note that this is added to the PointData\n        if not set_active:\n            name = None\n        return _get_output(alg, active_scalar=name, active_scalar_field='point')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncontouring an input dataset by an array of isosurfaces.", "response": "def contour(dataset, isosurfaces=10, scalars=None, compute_normals=False,\n                compute_gradients=False, compute_scalars=True,  rng=None,\n                preference='point'):\n        \"\"\"Contours an input dataset by an array. ``isosurfaces`` can be an integer\n        specifying the number of isosurfaces in the data range or an iterable set of\n        values for explicitly setting the isosurfaces.\n\n        Parameters\n        ----------\n        isosurfaces : int or iterable\n            Number of isosurfaces to compute across valid data range or an\n            iterable of float values to explicitly use as the isosurfaces.\n\n        scalars : str, optional\n            Name of scalars to threshold on. Defaults to currently active scalars.\n\n        compute_normals : bool, optional\n\n        compute_gradients : bool, optional\n            Desc\n\n        compute_scalars : bool, optional\n            Preserves the scalar values that are being contoured\n\n        rng : tuple(float), optional\n            If an integer number of isosurfaces is specified, this is the range\n            over which to generate contours. Default is the scalar arrays's full\n            data range.\n\n        preference : str, optional\n            When scalars is specified, this is the perfered scalar type to\n            search for in the dataset.  Must be either ``'point'`` or ``'cell'``\n\n        \"\"\"\n        # Make sure the input has scalars to contour on\n        if dataset.n_scalars < 1:\n            raise AssertionError('Input dataset for the contour filter must have scalar data.')\n        alg = vtk.vtkContourFilter()\n        alg.SetInputDataObject(dataset)\n        alg.SetComputeNormals(compute_normals)\n        alg.SetComputeGradients(compute_gradients)\n        alg.SetComputeScalars(compute_scalars)\n        # set the array to contour on\n        if scalars is None:\n            field, scalars = dataset.active_scalar_info\n        else:\n            _, field = get_scalar(dataset, scalars, preference=preference, info=True)\n        # NOTE: only point data is allowed? well cells works but seems buggy?\n        if field != vtki.POINT_DATA_FIELD:\n            raise AssertionError('Contour filter only works on Point data. Array ({}) is in the Cell data.'.format(scalars))\n        alg.SetInputArrayToProcess(0, 0, 0, field, scalars) # args: (idx, port, connection, field, name)\n        # set the isosurfaces\n        if isinstance(isosurfaces, int):\n            # generate values\n            if rng is None:\n                rng = dataset.get_data_range(scalars)\n            alg.GenerateValues(isosurfaces, rng)\n        elif isinstance(isosurfaces, collections.Iterable):\n            alg.SetNumberOfContours(len(isosurfaces))\n            for i, val in enumerate(isosurfaces):\n                alg.SetValue(i, val)\n        else:\n            raise RuntimeError('isosurfaces not understood.')\n        alg.Update()\n        return _get_output(alg)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef texture_map_to_plane(dataset, origin=None, point_u=None, point_v=None,\n                             inplace=False, name='Texture Coordinates'):\n        \"\"\"Texture map this dataset to a user defined plane. This is often used\n        to define a plane to texture map an image to this dataset. The plane\n        defines the spatial reference and extent of that image.\n\n        Parameters\n        ----------\n        origin : tuple(float)\n            Length 3 iterable of floats defining the XYZ coordinates of the\n            BOTTOM LEFT CORNER of the plane\n\n        point_u : tuple(float)\n            Length 3 iterable of floats defining the XYZ coordinates of the\n            BOTTOM RIGHT CORNER of the plane\n\n        point_v : tuple(float)\n            Length 3 iterable of floats defining the XYZ coordinates of the\n            TOP LEFT CORNER of the plane\n\n        inplace : bool, optional\n            If True, the new texture coordinates will be added to the dataset\n            inplace. If False (default), a new dataset is returned with the\n            textures coordinates\n\n        name : str, optional\n            The string name to give the new texture coordinates if applying\n            the filter inplace.\n\n        \"\"\"\n        alg = vtk.vtkTextureMapToPlane()\n        if origin is None or point_u is None or point_v is None:\n            alg.SetAutomaticPlaneGeneration(True)\n        else:\n            alg.SetOrigin(origin) # BOTTOM LEFT CORNER\n            alg.SetPoint1(point_u) # BOTTOM RIGHT CORNER\n            alg.SetPoint2(point_v) # TOP LEFT CORNER\n        alg.SetInputDataObject(dataset)\n        alg.Update()\n        output = _get_output(alg)\n        if not inplace:\n            return output\n        t_coords = output.GetPointData().GetTCoords()\n        t_coords.SetName(name)\n        otc = dataset.GetPointData().GetTCoords()\n        dataset.GetPointData().SetTCoords(t_coords)\n        dataset.GetPointData().AddArray(t_coords)\n        # CRITICAL:\n        dataset.GetPointData().AddArray(otc) # Add old ones back at the end\n        return", "response": "Texture map this dataset to a user defined plane."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute_cell_sizes(dataset, length=False, area=True, volume=True):\n        alg = vtk.vtkCellSizeFilter()\n        alg.SetInputDataObject(dataset)\n        alg.SetComputeArea(area)\n        alg.SetComputeVolume(volume)\n        alg.SetComputeLength(length)\n        alg.SetComputeVertexCount(False)\n        alg.Update()\n        return _get_output(alg)", "response": "This filter computes the sizes of the cells in the dataset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate points at the center of the cells in this dataset.", "response": "def cell_centers(dataset, vertex=True):\n        \"\"\"Generate points at the center of the cells in this dataset.\n        These points can be used for placing glyphs / vectors.\n\n        Parameters\n        ----------\n        vertex : bool\n            Enable/disable the generation of vertex cells.\n        \"\"\"\n        alg = vtk.vtkCellCenters()\n        alg.SetInputDataObject(dataset)\n        alg.SetVertexCells(vertex)\n        alg.Update()\n        output = _get_output(alg)\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef glyph(dataset, orient=True, scale=True, factor=1.0, geom=None):\n        if geom is None:\n            arrow = vtk.vtkArrowSource()\n            arrow.Update()\n            geom = arrow.GetOutput()\n        alg = vtk.vtkGlyph3D()\n        alg.SetSourceData(geom)\n        if isinstance(scale, str):\n            dataset.active_scalar_name = scale\n            scale = True\n        if scale:\n            if dataset.active_scalar is not None:\n                if dataset.active_scalar.ndim > 1:\n                    alg.SetScaleModeToScaleByVector()\n                else:\n                    alg.SetScaleModeToScaleByScalar()\n        if isinstance(orient, str):\n            dataset.active_vectors_name = orient\n            orient = True\n        alg.SetOrient(orient)\n        alg.SetInputData(dataset)\n        alg.SetVectorModeToUseVector()\n        alg.SetScaleFactor(factor)\n        alg.Update()\n        return _get_output(alg)", "response": "Returns a vtkGlyph3D object that represents the glyph at every key - point in the input dataset."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef connectivity(dataset, largest=False):\n        alg = vtk.vtkConnectivityFilter()\n        alg.SetInputData(dataset)\n        if largest:\n            alg.SetExtractionModeToLargestRegion()\n        else:\n            alg.SetExtractionModeToAllRegions()\n        alg.SetColorRegions(True)\n        alg.Update()\n        return _get_output(alg)", "response": "Find and label connected bodies and volumes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding label and split connected bodies into blocks.", "response": "def split_bodies(dataset, label=False):\n        \"\"\"Find, label, and split connected bodies/volumes. This splits\n        different connected bodies into blocks in a MultiBlock dataset.\n\n        Parameters\n        ----------\n        label : bool\n            A flag on whether to keep the ID arrays given by the\n            ``connectivity`` filter.\n        \"\"\"\n        # Get the connectivity and label different bodies\n        labeled = dataset.connectivity()\n        classifier = labeled.cell_arrays['RegionId']\n        bodies = vtki.MultiBlock()\n        for vid in np.unique(classifier):\n            # Now extract it:\n            b = labeled.threshold([vid-0.5, vid+0.5], scalars='RegionId')\n            if not label:\n                # strange behavior:\n                # must use this method rather than deleting from the point_arrays\n                # or else object is collected.\n                b._remove_cell_scalar('RegionId')\n                b._remove_point_scalar('RegionId')\n            bodies.append(b)\n\n        return bodies"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef warp_by_scalar(dataset, scalars=None, scale_factor=1.0, normal=None,\n                       inplace=False):\n        \"\"\"\n        Warp the dataset's points by a point data scalar array's values.\n        This modifies point coordinates by moving points along point normals by\n        the scalar amount times the scale factor.\n\n        Parameters\n        ----------\n        scalars : str, optional\n            Name of scalars to warb by. Defaults to currently active scalars.\n\n        scale_factor : float, optional\n            A scalaing factor to increase the scaling effect\n\n        normal : np.array, list, tuple of length 3\n            User specified normal. If given, data normals will be ignored and\n            the given normal will be used to project the warp.\n\n        inplace : bool\n            If True, the points of the give dataset will be updated.\n        \"\"\"\n        if scalars is None:\n            field, scalars = dataset.active_scalar_info\n        arr, field = get_scalar(dataset, scalars, preference='point', info=True)\n        if field != vtki.POINT_DATA_FIELD:\n            raise AssertionError('Dataset can only by warped by a point data array.')\n        # Run the algorithm\n        alg = vtk.vtkWarpScalar()\n        alg.SetInputDataObject(dataset)\n        alg.SetInputArrayToProcess(0, 0, 0, field, scalars) # args: (idx, port, connection, field, name)\n        alg.SetScaleFactor(scale_factor)\n        if normal is not None:\n            alg.SetNormal(normal)\n            alg.SetUseNormal(True)\n        alg.Update()\n        output = _get_output(alg)\n        if inplace:\n            dataset.points = output.points\n            return\n        return output", "response": "Warp the dataset s points by a point data scalar array."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntransforming cell data into point data.", "response": "def cell_data_to_point_data(dataset, pass_cell_data=False):\n        \"\"\"Transforms cell data (i.e., data specified per cell) into point data\n        (i.e., data specified at cell points).\n        The method of transformation is based on averaging the data values of\n        all cells using a particular point. Optionally, the input cell data can\n        be passed through to the output as well.\n\n        See aslo: :func:`vtki.DataSetFilters.point_data_to_cell_data`\n\n        Parameters\n        ----------\n        pass_cell_data : bool\n            If enabled, pass the input cell data through to the output\n        \"\"\"\n        alg = vtk.vtkCellDataToPointData()\n        alg.SetInputDataObject(dataset)\n        alg.SetPassCellData(pass_cell_data)\n        alg.Update()\n        return _get_output(alg, active_scalar=dataset.active_scalar_name)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntransforming point data into cell data.", "response": "def point_data_to_cell_data(dataset, pass_point_data=False):\n        \"\"\"Transforms point data (i.e., data specified per node) into cell data\n        (i.e., data specified within cells).\n        Optionally, the input point data can be passed through to the output.\n\n        See aslo: :func:`vtki.DataSetFilters.cell_data_to_point_data`\n\n        Parameters\n        ----------\n        pass_point_data : bool\n            If enabled, pass the input point data through to the output\n        \"\"\"\n        alg = vtk.vtkPointDataToCellData()\n        alg.SetInputDataObject(dataset)\n        alg.SetPassPointData(pass_point_data)\n        alg.Update()\n        return _get_output(alg, active_scalar=dataset.active_scalar_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef triangulate(dataset):\n        alg = vtk.vtkDataSetTriangleFilter()\n        alg.SetInputData(dataset)\n        alg.Update()\n        return _get_output(alg)", "response": "Returns a triangulated version of the input dataset."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing a 3D Delaunay triangulation of the input data.", "response": "def delaunay_3d(dataset, alpha=0, tol=0.001, offset=2.5):\n        \"\"\"Constructs a 3D Delaunay triangulation of the mesh.\n        This helps smooth out a rugged mesh.\n\n        Parameters\n        ----------\n        alpha : float, optional\n            Distance value to control output of this filter. For a non-zero\n            alpha value, only verts, edges, faces, or tetra contained within\n            the circumsphere (of radius alpha) will be output. Otherwise, only\n            tetrahedra will be output.\n\n        tol : float, optional\n            tolerance to control discarding of closely spaced points.\n            This tolerance is specified as a fraction of the diagonal length\n            of the bounding box of the points.\n\n        offset : float, optional\n            multiplier to control the size of the initial, bounding Delaunay\n            triangulation.\n        \"\"\"\n        alg = vtk.vtkDelaunay3D()\n        alg.SetInputData(dataset)\n        alg.SetAlpha(alpha)\n        alg.SetTolerance(tol)\n        alg.SetOffset(offset)\n        alg.Update()\n        return _get_output(alg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nselects the points that are inside a closed surface.", "response": "def select_enclosed_points(dataset, surface, tolerance=0.001,\n                inside_out=False, check_surface=True):\n        \"\"\"Mark points as to whether they are inside a closed surface.\n        This evaluates all the input points to determine whether they are in an\n        enclosed surface. The filter produces a (0,1) mask\n        (in the form of a vtkDataArray) that indicates whether points are\n        outside (mask value=0) or inside (mask value=1) a provided surface.\n        (The name of the output vtkDataArray is \"SelectedPointsArray\".)\n\n        The filter assumes that the surface is closed and manifold. A boolean\n        flag can be set to force the filter to first check whether this is\n        true. If false, all points will be marked outside. Note that if this\n        check is not performed and the surface is not closed, the results are\n        undefined.\n\n        This filter produces and output data array, but does not modify the\n        input dataset. If you wish to extract cells or poinrs, various\n        threshold filters are available (i.e., threshold the output array).\n\n        Parameters\n        ----------\n        surface : vtki.PolyData\n            Set the surface to be used to test for containment. This must be a\n            :class:`vtki.PolyData` object.\n\n        tolerance : float\n            The tolerance on the intersection. The tolerance is expressed as a\n            fraction of the bounding box of the enclosing surface.\n\n        inside_out : bool\n            By default, points inside the surface are marked inside or sent\n            to the output. If ``inside_out`` is ``True``, then the points\n            outside the surface are marked inside.\n\n        check_surface : bool\n            Specify whether to check the surface for closure. If on, then the\n            algorithm first checks to see if the surface is closed and\n            manifold.\n        \"\"\"\n        alg = vtk.vtkSelectEnclosedPoints()\n        alg.SetInputData(dataset)\n        alg.SetSurfaceData(surface)\n        alg.SetTolerance(tolerance)\n        alg.SetCheckSurface(check_surface)\n        alg.Update()\n        return _get_output(alg)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sample(dataset, target, tolerance=None, pass_cell_arrays=True,\n                    pass_point_arrays=True):\n        \"\"\"Resample scalar data from a passed mesh onto this mesh using\n        :class:`vtk.vtkResampleWithDataSet`.\n\n        Parameters\n        ----------\n        dataset: vtki.Common\n            The source vtk data object as the mesh to sample values on to\n\n        target: vtki.Common\n            The vtk data object to sample from - point and cell arrays from\n            this object are sampled onto the nodes of the ``dataset`` mesh\n\n        tolerance: flaot, optional\n            tolerance used to compute whether a point in the source is in a\n            cell of the input.  If not given, tolerance automatically generated.\n\n        pass_cell_arrays: bool, optional\n            Preserve source mesh's original cell data arrays\n\n        pass_point_arrays: bool, optional\n            Preserve source mesh's original point data arrays\n        \"\"\"\n        alg = vtk.vtkResampleWithDataSet() # Construct the ResampleWithDataSet object\n        alg.SetInputData(dataset)  # Set the Input data (actually the source i.e. where to sample from)\n        alg.SetSourceData(target) # Set the Source data (actually the target, i.e. where to sample to)\n        alg.SetPassCellArrays(pass_cell_arrays)\n        alg.SetPassPointArrays(pass_point_arrays)\n        if tolerance is not None:\n            alg.SetComputeTolerance(False)\n            alg.SetTolerance(tolerance)\n        alg.Update() # Perfrom the resampling\n        return _get_output(alg)", "response": "Resample scalar data from a source mesh onto a target mesh."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninterpolating values onto a given set of points on a given mesh.", "response": "def interpolate(dataset, points, sharpness=2, radius=1.0,\n            dimensions=(101, 101, 101), pass_cell_arrays=True, pass_point_arrays=True):\n        \"\"\"Interpolate values onto this mesh from the point data of a given\n        :class:`vtki.PolyData` object (typically a point cloud).\n\n        This uses a guassian interpolation kernel. Use the ``sharpness`` and\n        ``radius`` parameters to adjust this kernel.\n\n        Parameters\n        ----------\n        points : vtki.PolyData\n            The points whose values will be interpolated onto this mesh.\n\n        sharpness : float\n            Set / Get the sharpness (i.e., falloff) of the Gaussian. By\n            default Sharpness=2. As the sharpness increases the effects of\n            distant points are reduced.\n\n        radius : float\n            Specify the radius within which the basis points must lie.\n\n        dimensions : tuple(int)\n            When interpolating the points, they are first interpolating on to a\n            :class:`vtki.UniformGrid` with the same spatial extent -\n            ``dimensions`` is number of points along each axis for that grid.\n\n        pass_cell_arrays: bool, optional\n            Preserve source mesh's original cell data arrays\n\n        pass_point_arrays: bool, optional\n            Preserve source mesh's original point data arrays\n        \"\"\"\n        bounds = np.array(dataset.bounds)\n        dimensions = np.array(dimensions)\n        box = vtki.UniformGrid()\n        box.dimensions = dimensions\n        box.spacing = (bounds[1::2] - bounds[:-1:2]) / (dimensions - 1)\n        box.origin = bounds[::2]\n\n        gaussian_kernel = vtk.vtkGaussianKernel()\n        gaussian_kernel.SetSharpness(sharpness)\n        gaussian_kernel.SetRadius(radius)\n\n        interpolator = vtk.vtkPointInterpolator()\n        interpolator.SetInputData(box)\n        interpolator.SetSourceData(points)\n        interpolator.SetKernel(gaussian_kernel)\n        interpolator.Update()\n\n        return dataset.sample(interpolator.GetOutput(),\n                    pass_cell_arrays=pass_cell_arrays,\n                    pass_point_arrays=pass_point_arrays)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef streamlines(dataset, vectors=None, source_center=None,\n                    source_radius=None, n_points=100,\n                    integrator_type=45, integration_direction='both',\n                    surface_streamlines=False, initial_step_length=0.5,\n                    step_unit='cl', min_step_length=0.01, max_step_length=1.0,\n                    max_steps=2000, terminal_speed=1e-12, max_error=1e-6,\n                    max_time=None, compute_vorticity=True, rotation_scale=1.0,\n                    interpolator_type='point', start_position=(0.0, 0.0, 0.0),\n                    return_source=False):\n        \"\"\"Integrate a vector field to generate streamlines. The integration is\n        performed using a specified integrator, by default Runge-Kutta2.\n        This supports integration through any type of dataset.\n        Thus if the dataset contains 2D cells like polygons or triangles, the\n        integration is constrained to lie on the surface defined by 2D cells.\n\n        This produces polylines as the output, with each cell\n        (i.e., polyline) representing a streamline. The attribute values\n        associated with each streamline are stored in the cell data, whereas\n        those associated with streamline-points are stored in the point data.\n\n        This uses a Sphere as the source - set it's location and radius via\n        the ``source_center`` and ``source_radius`` keyword arguments.\n        You can retrieve the source as :class:`vtki.PolyData` by specifying\n        ``return_source=True``.\n\n        Parameters\n        ----------\n        vectors : str\n            The string name of the active vector field to integrate across\n\n        source_center : tuple(float)\n            Length 3 tuple of floats defining the center of the source\n            particles. Defaults to the center of the dataset\n\n        source_radius : float\n            Float radius of the source particle cloud. Defaults to one-tenth of\n            the diagonal of the dataset's spatial extent\n\n        n_points : int\n            Number of particles present in source sphere\n\n        integrator_type : int\n            The integrator type to be used for streamline generation.\n            The default is Runge-Kutta45. The recognized solvers are:\n            RUNGE_KUTTA2 (``2``),  RUNGE_KUTTA4 (``4``), and RUNGE_KUTTA45\n            (``45``). Options are ``2``, ``4``, or ``45``. Default is ``45``.\n\n        integration_direction : str\n            Specify whether the streamline is integrated in the upstream or\n            downstream directions (or both). Options are ``'both'``,\n            ``'backward'``, or ``'forward'``.\n\n        surface_streamlines : bool\n            Compute streamlines on a surface. Default ``False``\n\n        initial_step_length : float\n            Initial step size used for line integration, expressed ib length\n            unitsL or cell length units (see ``step_unit`` parameter).\n            either the starting size for an adaptive integrator, e.g., RK45, or\n            the constant / fixed size for non-adaptive ones, i.e., RK2 and RK4)\n\n        step_unit : str\n            Uniform integration step unit. The valid unit is now limited to\n            only LENGTH_UNIT (``'l'``) and CELL_LENGTH_UNIT (``'cl'``).\n            Default is CELL_LENGTH_UNIT: ``'cl'``.\n\n        min_step_length : float\n            Minimum step size used for line integration, expressed in length or\n            cell length units. Only valid for an adaptive integrator, e.g., RK45\n\n        max_step_length : float\n            Maxmimum step size used for line integration, expressed in length or\n            cell length units. Only valid for an adaptive integrator, e.g., RK45\n\n        max_steps : int\n            Maximum number of steps for integrating a streamline.\n            Defaults to ``2000``\n\n        terminal_speed : float\n            Terminal speed value, below which integration is terminated.\n\n        max_error : float\n            Maximum error tolerated throughout streamline integration.\n\n        max_time : float\n            Specify the maximum length of a streamline expressed in LENGTH_UNIT.\n\n        compute_vorticity : bool\n            Vorticity computation at streamline points (necessary for generating\n            proper stream-ribbons using the ``vtkRibbonFilter``.\n\n        interpolator_type : str\n            Set the type of the velocity field interpolator to locate cells\n            during streamline integration either by points or cells.\n            The cell locator is more robust then the point locator. Options\n            are ``'point'`` or ``'cell'`` (abreviations of ``'p'`` and ``'c'``\n            are also supported).\n\n        rotation_scale : float\n            This can be used to scale the rate with which the streamribbons\n            twist. The default is 1.\n\n        start_position : tuple(float)\n            Set the start position. Default is ``(0.0, 0.0, 0.0)``\n\n        return_source : bool\n            Return the source particles as :class:`vtki.PolyData` as well as the\n            streamlines. This will be the second value returned if ``True``.\n        \"\"\"\n        integration_direction = str(integration_direction).strip().lower()\n        if integration_direction not in ['both', 'back', 'backward', 'forward']:\n            raise RuntimeError(\"integration direction must be one of: 'backward', 'forward', or 'both' - not '{}'.\".format(integration_direction))\n        if integrator_type not in [2, 4, 45]:\n            raise RuntimeError('integrator type must be one of `2`, `4`, or `45`.')\n        if interpolator_type not in ['c', 'cell', 'p', 'point']:\n            raise RuntimeError(\"interpolator type must be either 'cell' or 'point'\")\n        if step_unit not in ['l', 'cl']:\n            raise RuntimeError(\"step unit must be either 'l' or 'cl'\")\n        step_unit = {'cl':vtk.vtkStreamTracer.CELL_LENGTH_UNIT,\n                     'l':vtk.vtkStreamTracer.LENGTH_UNIT}[step_unit]\n        if isinstance(vectors, str):\n            dataset.set_active_scalar(vectors)\n            dataset.set_active_vectors(vectors)\n        if max_time is None:\n            max_velocity = dataset.get_data_range()[-1]\n            max_time = 4.0 * dataset.GetLength() / max_velocity\n        # Generate the source\n        if source_center is None:\n            source_center = dataset.center\n        if source_radius is None:\n            source_radius = dataset.length / 10.0\n        source = vtk.vtkPointSource()\n        source.SetNumberOfPoints(n_points);\n        source.SetCenter(source_center);\n        source.SetRadius(source_radius);\n        # Build the algorithm\n        alg = vtk.vtkStreamTracer()\n        # Inputs\n        alg.SetInputDataObject(dataset)\n        # NOTE: not sure why we can't pass a PolyData object\n        #       setting the connection is the only I could get it to work\n        alg.SetSourceConnection(source.GetOutputPort())\n        # general parameters\n        alg.SetComputeVorticity(compute_vorticity)\n        alg.SetInitialIntegrationStep(initial_step_length)\n        alg.SetIntegrationStepUnit(step_unit)\n        alg.SetMaximumError(max_error)\n        alg.SetMaximumIntegrationStep(max_step_length)\n        alg.SetMaximumNumberOfSteps(max_steps)\n        alg.SetMaximumPropagation(max_time)\n        alg.SetMinimumIntegrationStep(min_step_length)\n        alg.SetRotationScale(rotation_scale)\n        alg.SetStartPosition(start_position)\n        alg.SetSurfaceStreamlines(surface_streamlines)\n        alg.SetTerminalSpeed(terminal_speed)\n        # Model parameters\n        if integration_direction == 'forward':\n            alg.SetIntegrationDirectionToForward()\n        elif integration_direction in ['backward', 'back']:\n            alg.SetIntegrationDirectionToBackward()\n        else:\n            alg.SetIntegrationDirectionToBoth()\n        # set integrator type\n        if integrator_type == 2:\n            alg.SetIntegratorTypeToRungeKutta2()\n        elif integrator_type == 4:\n            alg.SetIntegratorTypeToRungeKutta4()\n        else:\n            alg.SetIntegratorTypeToRungeKutta45()\n        # set interpolator type\n        if interpolator_type in ['c', 'cell']:\n            alg.SetInterpolatorTypeToCellLocator()\n        else:\n            alg.SetInterpolatorTypeToDataSetPointLocator()\n        # run the algorithm\n        alg.Update()\n        output = _get_output(alg)\n        if return_source:\n            source.Update()\n            src = vtki.wrap(source.GetOutput())\n            return output, src\n        return output", "response": "Integrate a vector field to generate streamlines."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads a surface mesh from a file.", "response": "def _load_file(self, filename):\n        \"\"\"\n        Load a surface mesh from a mesh file.\n\n        Mesh file may be an ASCII or binary ply, stl, or vtk mesh file.\n\n        Parameters\n        ----------\n        filename : str\n            Filename of mesh to be loaded.  File type is inferred from the\n            extension of the filename\n\n        Notes\n        -----\n        Binary files load much faster than ASCII.\n\n        \"\"\"\n        filename = os.path.abspath(os.path.expanduser(filename))\n        # test if file exists\n        if not os.path.isfile(filename):\n            raise Exception('File %s does not exist' % filename)\n\n        # Get extension\n        ext = vtki.get_ext(filename)\n\n        # Select reader\n        if ext == '.ply':\n            reader = vtk.vtkPLYReader()\n        elif ext == '.stl':\n            reader = vtk.vtkSTLReader()\n        elif ext == '.vtk':\n            reader = vtk.vtkPolyDataReader()\n        elif ext == '.vtp':\n            reader = vtk.vtkXMLPolyDataReader()\n        elif ext == '.obj':\n            reader = vtk.vtkOBJReader()\n        else:\n            raise TypeError('Filetype must be either \"ply\", \"stl\", \"vtk\", \"vtp\", or \"obj\".')\n\n        # Load file\n        reader.SetFileName(filename)\n        reader.Update()\n        self.ShallowCopy(reader.GetOutput())\n\n        # sanity check\n        if not np.any(self.points):\n            raise AssertionError('Empty or invalid file')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets faces without copying", "response": "def faces(self, faces):\n        \"\"\" set faces without copying \"\"\"\n        if faces.dtype != vtki.ID_TYPE:\n            faces = faces.astype(vtki.ID_TYPE)\n\n        # get number of faces\n        if faces.ndim == 1:\n            log.debug('efficiency warning')\n            c = 0\n            nfaces = 0\n            while c < faces.size:\n                c += faces[c] + 1\n                nfaces += 1\n        else:\n            nfaces = faces.shape[0]\n\n        vtkcells = vtk.vtkCellArray()\n        vtkcells.SetCells(nfaces, numpy_to_vtkIdTypeArray(faces, deep=False))\n        if faces.ndim > 1 and faces.shape[1] == 2:\n            self.SetVerts(vtkcells)\n        else:\n            self.SetPolys(vtkcells)\n        self._face_ref = faces\n        self.Modified()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets polygons and points from numpy arrays.", "response": "def _from_arrays(self, vertices, faces, deep=True, verts=False):\n        \"\"\"\n        Set polygons and points from numpy arrays\n\n        Parameters\n        ----------\n        vertices : np.ndarray of dtype=np.float32 or np.float64\n            Vertex array.  3D points.\n\n        faces : np.ndarray of dtype=np.int64\n            Face index array.  Faces can contain any number of points.\n\n        Examples\n        --------\n        >>> import numpy as np\n        >>> import vtki\n        >>> vertices = np.array([[0, 0, 0],\n        ...                      [1, 0, 0],\n        ...                      [1, 1, 0],\n        ...                      [0, 1, 0],\n        ...                      [0.5, 0.5, 1]])\n        >>> faces = np.hstack([[4, 0, 1, 2, 3],\n        ...                    [3, 0, 1, 4],\n        ...                    [3, 1, 2, 4]])  # one square and two triangles\n        >>> surf = vtki.PolyData(vertices, faces)\n\n        \"\"\"\n        if deep or verts:\n            vtkpoints = vtk.vtkPoints()\n            vtkpoints.SetData(numpy_to_vtk(vertices, deep=deep))\n            self.SetPoints(vtkpoints)\n\n            # Convert to a vtk array\n            vtkcells = vtk.vtkCellArray()\n            if faces.dtype != vtki.ID_TYPE:\n                faces = faces.astype(vtki.ID_TYPE)\n\n            # get number of faces\n            if faces.ndim == 1:\n                c = 0\n                nfaces = 0\n                while c < faces.size:\n                    c += faces[c] + 1\n                    nfaces += 1\n            else:\n                nfaces = faces.shape[0]\n\n            idarr = numpy_to_vtkIdTypeArray(faces.ravel(), deep=deep)\n            vtkcells.SetCells(nfaces, idarr)\n            if (faces.ndim > 1 and faces.shape[1] == 2) or verts:\n                self.SetVerts(vtkcells)\n            else:\n                self.SetPolys(vtkcells)\n        else:\n            self.points = vertices\n            self.faces = faces"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a mask of the points of a surface mesh that have a surfaceVersion greater than angle.", "response": "def edge_mask(self, angle):\n        \"\"\"\n        Returns a mask of the points of a surface mesh that have a surface\n        angle greater than angle\n\n        Parameters\n        ----------\n        angle : float\n            Angle to consider an edge.\n\n        \"\"\"\n        self.point_arrays['point_ind'] = np.arange(self.n_points)\n        featureEdges = vtk.vtkFeatureEdges()\n        featureEdges.SetInputData(self)\n        featureEdges.FeatureEdgesOn()\n        featureEdges.BoundaryEdgesOff()\n        featureEdges.NonManifoldEdgesOff()\n        featureEdges.ManifoldEdgesOff()\n        featureEdges.SetFeatureAngle(angle)\n        featureEdges.Update()\n        edges = _get_output(featureEdges)\n        orig_id = vtki.point_scalar(edges, 'point_ind')\n\n        return np.in1d(self.point_arrays['point_ind'], orig_id,\n                       assume_unique=True)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef boolean_cut(self, cut, tolerance=1E-5, inplace=False):\n        bfilter = vtk.vtkBooleanOperationPolyDataFilter()\n        bfilter.SetOperationToIntersection()\n        # bfilter.SetOperationToDifference()\n\n        bfilter.SetInputData(1, cut)\n        bfilter.SetInputData(0, self)\n        bfilter.ReorientDifferenceCellsOff()\n        bfilter.SetTolerance(tolerance)\n        bfilter.Update()\n\n        mesh = _get_output(bfilter)\n        if inplace:\n            self.overwrite(mesh)\n        else:\n            return mesh", "response": "Performs a Boolean cut using another mesh."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef boolean_add(self, mesh, inplace=False):\n        vtkappend = vtk.vtkAppendPolyData()\n        vtkappend.AddInputData(self)\n        vtkappend.AddInputData(mesh)\n        vtkappend.Update()\n\n        mesh = _get_output(vtkappend)\n        if inplace:\n            self.overwrite(mesh)\n        else:\n            return mesh", "response": "Add a mesh to the current mesh."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncombines two meshes and attempts to create a manifold mesh.", "response": "def boolean_union(self, mesh, inplace=False):\n        \"\"\"\n        Combines two meshes and attempts to create a manifold mesh.\n\n        Parameters\n        ----------\n        mesh : vtki.PolyData\n            The mesh to perform a union against.\n\n        inplace : bool, optional\n            Updates mesh in-place while returning nothing.\n\n        Returns\n        -------\n        union : vtki.PolyData\n            The union mesh when inplace=False.\n\n        \"\"\"\n        bfilter = vtk.vtkBooleanOperationPolyDataFilter()\n        bfilter.SetOperationToUnion()\n        bfilter.SetInputData(1, mesh)\n        bfilter.SetInputData(0, self)\n        bfilter.ReorientDifferenceCellsOff()\n        bfilter.Update()\n\n        mesh = _get_output(bfilter)\n        if inplace:\n            self.overwrite(mesh)\n        else:\n            return mesh"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef boolean_difference(self, mesh, inplace=False):\n        bfilter = vtk.vtkBooleanOperationPolyDataFilter()\n        bfilter.SetOperationToDifference()\n        bfilter.SetInputData(1, mesh)\n        bfilter.SetInputData(0, self)\n        bfilter.ReorientDifferenceCellsOff()\n        bfilter.Update()\n\n        mesh = _get_output(bfilter)\n        if inplace:\n            self.overwrite(mesh)\n        else:\n            return mesh", "response": "Combines two meshes and retains only the volume in common\n            between the two meshes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef curvature(self, curv_type='mean'):\n        curv_type = curv_type.lower()\n\n        # Create curve filter and compute curvature\n        curvefilter = vtk.vtkCurvatures()\n        curvefilter.SetInputData(self)\n        if curv_type == 'mean':\n            curvefilter.SetCurvatureTypeToMean()\n        elif curv_type == 'gaussian':\n            curvefilter.SetCurvatureTypeToGaussian()\n        elif curv_type == 'maximum':\n            curvefilter.SetCurvatureTypeToMaximum()\n        elif curv_type == 'minimum':\n            curvefilter.SetCurvatureTypeToMinimum()\n        else:\n            raise Exception('Curv_Type must be either \"Mean\", ' +\n                            '\"Gaussian\", \"Maximum\", or \"Minimum\"')\n        curvefilter.Update()\n\n        # Compute and return curvature\n        curv = _get_output(curvefilter)\n        return vtk_to_numpy(curv.GetPointData().GetScalars())", "response": "Returns the pointwise curvature of a mesh."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite a vtk object to disk.", "response": "def save(self, filename, binary=True):\n        \"\"\"\n        Writes a surface mesh to disk.\n\n        Written file may be an ASCII or binary ply, stl, or vtk mesh file.\n\n        Parameters\n        ----------\n        filename : str\n            Filename of mesh to be written.  File type is inferred from\n            the extension of the filename unless overridden with\n            ftype.  Can be one of the following types (.ply, .stl,\n            .vtk)\n\n        binary : bool, optional\n            Writes the file as binary when True and ASCII when False.\n\n        Notes\n        -----\n        Binary files write much faster than ASCII and have a smaller\n        file size.\n        \"\"\"\n        filename = os.path.abspath(os.path.expanduser(filename))\n        file_mode = True\n        # Check filetype\n        ftype = filename[-3:]\n        if ftype == 'ply':\n            writer = vtk.vtkPLYWriter()\n        elif ftype == 'vtp':\n            writer = vtk.vtkXMLPolyDataWriter()\n            file_mode = False\n            if binary:\n                writer.SetDataModeToBinary()\n            else:\n                writer.SetDataModeToAscii()\n        elif ftype == 'stl':\n            writer = vtk.vtkSTLWriter()\n        elif ftype == 'vtk':\n            writer = vtk.vtkPolyDataWriter()\n        else:\n            raise Exception('Filetype must be either \"ply\", \"stl\", or \"vtk\"')\n\n        writer.SetFileName(filename)\n        writer.SetInputData(self)\n        if binary and file_mode:\n            writer.SetFileTypeToBinary()\n        elif file_mode:\n            writer.SetFileTypeToASCII()\n        writer.Write()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_curvature(self, curv_type='mean', **kwargs):\n        return self.plot(scalars=self.curvature(curv_type),\n                         stitle='%s\\nCurvature' % curv_type, **kwargs)", "response": "Plot the curvature of the current set of items in the current set of items."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tri_filter(self, inplace=False):\n        trifilter = vtk.vtkTriangleFilter()\n        trifilter.SetInputData(self)\n        trifilter.PassVertsOff()\n        trifilter.PassLinesOff()\n        trifilter.Update()\n\n        mesh = _get_output(trifilter)\n        if inplace:\n            self.overwrite(mesh)\n        else:\n            return mesh", "response": "Returns a vtki. PolyData object containing only triangles."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadjusts the point coordinates using Laplacian smoothing.", "response": "def smooth(self, n_iter=20, convergence=0.0, edge_angle=15, feature_angle=45,\n               boundary_smoothing=True, feature_smoothing=False, inplace=False):\n        \"\"\"Adjust point coordinates using Laplacian smoothing.\n        The effect is to \"relax\" the mesh, making the cells better shaped and\n        the vertices more evenly distributed.\n\n        Parameters\n        ----------\n        n_iter : int\n            Number of iterations for Laplacian smoothing,\n\n        convergence : float, optional\n            Convergence criterion for the iteration process. Smaller numbers\n            result in more smoothing iterations. Range from (0 to 1).\n\n        edge_angle : float, optional\n            Edge angle to control smoothing along edges (either interior or boundary).\n\n        feature_angle : float, optional\n            Feature angle for sharp edge identification.\n\n        boundary_smoothing : bool, optional\n            Boolean flag to control smoothing of boundary edges.\n\n        feature_smoothing : bool, optional\n            Boolean flag to control smoothing of feature edges.\n\n        inplace : bool, optional\n            Updates mesh in-place while returning nothing.\n\n        Returns\n        -------\n        mesh : vtki.PolyData\n            Decimated mesh. None when inplace=True.\n\n        \"\"\"\n        alg = vtk.vtkSmoothPolyDataFilter()\n        alg.SetInputData(self)\n        alg.SetNumberOfIterations(n_iter)\n        alg.SetConvergence(convergence)\n        alg.SetFeatureEdgeSmoothing(feature_smoothing)\n        alg.SetFeatureAngle(feature_angle)\n        alg.SetEdgeAngle(edge_angle)\n        alg.SetBoundarySmoothing(boundary_smoothing)\n        alg.Update()\n\n        mesh = _get_output(alg)\n        if inplace:\n            self.overwrite(mesh)\n        else:\n            return mesh"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef decimate_pro(self, reduction, feature_angle=45.0, split_angle=75.0, splitting=True,\n                     pre_split_mesh=False, preserve_topology=False, inplace=False):\n        \"\"\"Reduce the number of triangles in a triangular mesh, forming a good\n        approximation to the original geometry. Based on the algorithm originally\n        described in \"Decimation of Triangle Meshes\", Proc Siggraph `92.\n\n        Parameters\n        ----------\n        reduction : float\n            Reduction factor. A value of 0.9 will leave 10 % of the original number\n            of vertices.\n\n        feature_angle : float, optional\n            Angle used to define what an edge is (i.e., if the surface normal between\n            two adjacent triangles is >= feature_angle, an edge exists).\n\n        split_angle : float, optional\n            Angle used to control the splitting of the mesh. A split line exists\n            when the surface normals between two edge connected triangles are >= split_angle.\n\n        splitting : bool, optional\n            Controls the splitting of the mesh at corners, along edges, at non-manifold\n            points, or anywhere else a split is required. Turning splitting off\n            will better preserve the original topology of the mesh, but may not\n            necessarily give the exact requested decimation.\n\n        pre_split_mesh : bool, optional\n            Separates the mesh into semi-planar patches, which are disconnected\n            from each other. This can give superior results in some cases. If pre_split_mesh\n            is set to True, the mesh is split with the specified split_angle. Otherwise\n            mesh splitting is deferred as long as possible.\n\n        preserve_topology : bool, optional\n            Controls topology preservation. If on, mesh splitting and hole elimination\n            will not occur. This may limit the maximum reduction that may be achieved.\n\n        inplace : bool, optional\n            Updates mesh in-place while returning nothing.\n\n        Returns\n        -------\n        mesh : vtki.PolyData\n            Decimated mesh. None when inplace=True.\n\n        \"\"\"\n\n        alg = vtk.vtkDecimatePro()\n        alg.SetInputData(self)\n        alg.SetTargetReduction(reduction)\n        alg.SetPreserveTopology(preserve_topology)\n        alg.SetFeatureAngle(feature_angle)\n        alg.SetSplitting(splitting)\n        alg.SetSplitAngle(split_angle)\n        alg.SetPreSplitMesh(pre_split_mesh)\n        alg.Update()\n\n        mesh = _get_output(alg)\n        if inplace:\n            self.overwrite(mesh)\n        else:\n            return mesh", "response": "Reduce the number of triangles in a triangular mesh forming a good\n            approximation to the original geometry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a tube around each input line.", "response": "def tube(self, radius=None, scalars=None, capping=True, n_sides=20,\n             radius_factor=10, preference='point', inplace=False):\n        \"\"\"Generate a tube around each input line. The radius of the tube can be\n        set to linearly vary with a scalar value.\n\n        Parameters\n        ----------\n        radius : float\n            Minimum tube radius (minimum because the tube radius may vary).\n\n        scalars : str, optional\n            Scalar array by which the radius varies\n\n        capping : bool\n            Turn on/off whether to cap the ends with polygons. Default True.\n\n        n_sides : int\n            Set the number of sides for the tube. Minimum of 3.\n\n        radius_factor : float\n            Maximum tube radius in terms of a multiple of the minimum radius.\n\n        preference : str\n            The field preference when searching for the scalar array by name\n\n        inplace : bool, optional\n            Updates mesh in-place while returning nothing.\n\n        Returns\n        -------\n        mesh : vtki.PolyData\n            Tube-filtered mesh. None when inplace=True.\n\n        \"\"\"\n        if n_sides < 3:\n            n_sides = 3\n        tube = vtk.vtkTubeFilter()\n        tube.SetInputDataObject(self)\n        # User Defined Parameters\n        tube.SetCapping(capping)\n        if radius is not None:\n            tube.SetRadius(radius)\n        tube.SetNumberOfSides(n_sides)\n        tube.SetRadiusFactor(radius_factor)\n        # Check if scalar array given\n        if scalars is not None:\n            if not isinstance(scalars, str):\n                raise TypeError('Scalar array must be given as a string name')\n            _, field = self.get_scalar(scalars, preference=preference, info=True)\n            # args: (idx, port, connection, field, name)\n            tube.SetInputArrayToProcess(0, 0, 0, field, scalars)\n            tube.SetVaryRadiusToVaryRadiusByScalar()\n        # Apply the filter\n        tube.Update()\n\n        mesh = _get_output(tube)\n        if inplace:\n            self.overwrite(mesh)\n        else:\n            return mesh"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef subdivide(self, nsub, subfilter='linear', inplace=False):\n        subfilter = subfilter.lower()\n        if subfilter == 'linear':\n            sfilter = vtk.vtkLinearSubdivisionFilter()\n        elif subfilter == 'butterfly':\n            sfilter = vtk.vtkButterflySubdivisionFilter()\n        elif subfilter == 'loop':\n            sfilter = vtk.vtkLoopSubdivisionFilter()\n        else:\n            raise Exception(\"Subdivision filter must be one of the following: \" +\n                            \"'butterfly', 'loop', or 'linear'\")\n\n        # Subdivide\n        sfilter.SetNumberOfSubdivisions(nsub)\n        sfilter.SetInputData(self)\n        sfilter.Update()\n\n        submesh = _get_output(sfilter)\n        if inplace:\n            self.overwrite(submesh)\n        else:\n            return submesh", "response": "Subdivide a single triangular object into four smaller triangles."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_edges(self, feature_angle=30, boundary_edges=True,\n                     non_manifold_edges=True, feature_edges=True,\n                     manifold_edges=True, inplace=False):\n        \"\"\"\n        Extracts edges from a surface.  From vtk documentation, the edges are\n        one of the following\n\n            1) boundary (used by one polygon) or a line cell\n            2) non-manifold (used by three or more polygons)\n            3) feature edges (edges used by two triangles and whose\n               dihedral angle > feature_angle)\n            4) manifold edges (edges used by exactly two polygons).\n\n        Parameters\n        ----------\n        feature_angle : float, optional\n            Defaults to 30 degrees.\n\n        boundary_edges : bool, optional\n            Defaults to True\n\n        non_manifold_edges : bool, optional\n            Defaults to True\n\n        feature_edges : bool, optional\n            Defaults to True\n\n        manifold_edges : bool, optional\n            Defaults to True\n\n        inplace : bool, optional\n            Return new mesh or overwrite input.\n\n        Returns\n        -------\n        edges : vtki.vtkPolyData\n            Extracted edges. None if inplace=True.\n\n        \"\"\"\n        featureEdges = vtk.vtkFeatureEdges()\n        featureEdges.SetInputData(self)\n        featureEdges.SetFeatureAngle(feature_angle)\n        featureEdges.SetManifoldEdges(manifold_edges)\n        featureEdges.SetNonManifoldEdges(non_manifold_edges)\n        featureEdges.SetBoundaryEdges(boundary_edges)\n        featureEdges.SetFeatureEdges(feature_edges)\n        featureEdges.SetColoring(False)\n        featureEdges.Update()\n\n        mesh = _get_output(featureEdges)\n        if inplace:\n            self.overwrite(mesh)\n        else:\n            return mesh", "response": "Extracts edges from a surface."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndecimating the number of triangles in a mesh.", "response": "def decimate(self, target_reduction, volume_preservation=False,\n                 attribute_error=False, scalars=True, vectors=True,\n                 normals=False, tcoords=True, tensors=True, scalars_weight=0.1,\n                 vectors_weight=0.1, normals_weight=0.1, tcoords_weight=0.1,\n                 tensors_weight=0.1, inplace=False):\n        \"\"\"\n        Reduces the number of triangles in a triangular mesh using\n        vtkQuadricDecimation.\n\n        Parameters\n        ----------\n        mesh : vtk.PolyData\n            Mesh to decimate\n\n        target_reduction : float\n            Fraction of the original mesh to remove.\n            TargetReduction is set to 0.9, this filter will try to reduce\n            the data set to 10% of its original size and will remove 90%\n            of the input triangles.\n\n        volume_preservation : bool, optional\n            Decide whether to activate volume preservation which greatly reduces\n            errors in triangle normal direction. If off, volume preservation is\n            disabled and if AttributeErrorMetric is active, these errors can be\n            large. Defaults to False.\n\n        attribute_error : bool, optional\n            Decide whether to include data attributes in the error metric. If\n            off, then only geometric error is used to control the decimation.\n            Defaults to False.\n\n        scalars : bool, optional\n            If attribute errors are to be included in the metric (i.e.,\n            AttributeErrorMetric is on), then the following flags control which\n            attributes are to be included in the error calculation. Defaults to\n            True.\n\n        vectors : bool, optional\n            See scalars parameter. Defaults to True.\n\n        normals : bool, optional\n            See scalars parameter. Defaults to False.\n\n        tcoords : bool, optional\n            See scalars parameter. Defaults to True.\n\n        tensors : bool, optional\n            See scalars parameter. Defaults to True.\n\n        scalars_weight : float, optional\n            The scaling weight contribution of the scalar attribute. These\n            values are used to weight the contribution of the attributes towards\n            the error metric. Defaults to 0.1.\n\n        vectors_weight : float, optional\n            See scalars weight parameter. Defaults to 0.1.\n\n        normals_weight : float, optional\n            See scalars weight parameter. Defaults to 0.1.\n\n        tcoords_weight : float, optional\n            See scalars weight parameter. Defaults to 0.1.\n\n        tensors_weight : float, optional\n            See scalars weight parameter. Defaults to 0.1.\n\n        inplace : bool, optional\n            Updates mesh in-place while returning nothing.\n\n        Returns\n        -------\n        outmesh : vtki.PolyData\n            Decimated mesh.  None when inplace=True.\n\n        \"\"\"\n        # create decimation filter\n        decimate = vtk.vtkQuadricDecimation()  # vtkDecimatePro as well\n\n        decimate.SetVolumePreservation(volume_preservation)\n        decimate.SetAttributeErrorMetric(attribute_error)\n        decimate.SetScalarsAttribute(scalars)\n        decimate.SetVectorsAttribute(vectors)\n        decimate.SetNormalsAttribute(normals)\n        decimate.SetTCoordsAttribute(tcoords)\n        decimate.SetTensorsAttribute(tensors)\n        decimate.SetScalarsWeight(scalars_weight)\n        decimate.SetVectorsWeight(vectors_weight)\n        decimate.SetNormalsWeight(normals_weight)\n        decimate.SetTCoordsWeight(tcoords_weight)\n        decimate.SetTensorsWeight(tensors_weight)\n        decimate.SetTargetReduction(target_reduction)\n\n        decimate.SetInputData(self)\n        decimate.Update()\n\n        mesh = _get_output(decimate)\n        if inplace:\n            self.overwrite(mesh)\n        else:\n            return mesh"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the coordinates for the center of mass of the mesh.", "response": "def center_of_mass(self, scalars_weight=False):\n        \"\"\"\n        Returns the coordinates for the center of mass of the mesh.\n\n        Parameters\n        ----------\n        scalars_weight : bool, optional\n            Flag for using the mesh scalars as weights. Defaults to False.\n\n        Return\n        ------\n        center : np.ndarray, float\n            Coordinates for the center of mass.\n        \"\"\"\n        comfilter = vtk.vtkCenterOfMass()\n        comfilter.SetInputData(self)\n        comfilter.SetUseScalarsAsWeights(scalars_weight)\n        comfilter.Update()\n        return np.array(comfilter.GetCenter())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing point and/or cell normals for a mesh. The filter can reorder polygons to insure consistent orientation across polygon neighbors. Sharp edges can be split and points duplicated with separate normals to give crisp (rendered) surface definition. It is also possible to globally flip the normal orientation. The algorithm works by determining normals for each polygon and then averaging them at shared points. When sharp edges are present, the edges are split and new points generated to prevent blurry edges (due to Gouraud shading). Parameters ---------- cell_normals : bool, optional Calculation of cell normals. Defaults to True. point_normals : bool, optional Calculation of point normals. Defaults to True. split_vertices : bool, optional Splitting of sharp edges. Defaults to False. flip_normals : bool, optional Set global flipping of normal orientation. Flipping modifies both the normal direction and the order of a cell's points. Defaults to False. consistent_normals : bool, optional Enforcement of consistent polygon ordering. Defaults to True. auto_orient_normals : bool, optional Turn on/off the automatic determination of correct normal orientation. NOTE: This assumes a completely closed surface (i.e. no boundary edges) and no non-manifold edges. If these constraints do not hold, all bets are off. This option adds some computational complexity, and is useful if you don't want to have to inspect the rendered image to determine whether to turn on the FlipNormals flag. However, this flag can work with the FlipNormals flag, and if both are set, all the normals in the output will point \"inward\". Defaults to False. non_manifold_traversal : bool, optional Turn on/off traversal across non-manifold edges. Changing this may prevent problems where the consistency of polygonal ordering is corrupted due to topological loops. Defaults to True. feature_angle : float, optional The angle that defines a sharp edge. If the difference in angle across neighboring polygons is greater than this value, the shared edge is considered \"sharp\". Defaults to 30.0. inplace : bool, optional Updates mesh in-place while returning nothing. Defaults to False. Returns ------- mesh : vtki.PolyData Updated mesh with cell and point normals if inplace=False Notes ----- Previous arrays named \"Normals\" will be overwritten. Normals are computed only for polygons and triangle strips. Normals are not computed for lines or vertices. Triangle strips are broken up into triangle polygons. You may want to restrip the triangles. May be easier to run mesh.point_normals or mesh.cell_normals", "response": "def compute_normals(self, cell_normals=True, point_normals=True, split_vertices=False,\n                        flip_normals=False, consistent_normals=True, auto_orient_normals=False,\n                        non_manifold_traversal=True, feature_angle=30.0, inplace=False):\n        \"\"\"\n        Compute point and/or cell normals for a mesh.\n\n        The filter can reorder polygons to insure consistent orientation across\n        polygon neighbors. Sharp edges can be split and points duplicated\n        with separate normals to give crisp (rendered) surface definition. It is\n        also possible to globally flip the normal orientation.\n\n        The algorithm works by determining normals for each polygon and then\n        averaging them at shared points. When sharp edges are present, the edges\n        are split and new points generated to prevent blurry edges (due to\n        Gouraud shading).\n\n        Parameters\n        ----------\n        cell_normals : bool, optional\n            Calculation of cell normals. Defaults to True.\n\n        point_normals : bool, optional\n            Calculation of point normals. Defaults to True.\n\n        split_vertices : bool, optional\n            Splitting of sharp edges. Defaults to False.\n\n        flip_normals : bool, optional\n            Set global flipping of normal orientation. Flipping modifies both\n            the normal direction and the order of a cell's points. Defaults to\n            False.\n\n        consistent_normals : bool, optional\n            Enforcement of consistent polygon ordering. Defaults to True.\n\n        auto_orient_normals : bool, optional\n            Turn on/off the automatic determination of correct normal\n            orientation. NOTE: This assumes a completely closed surface (i.e. no\n            boundary edges) and no non-manifold edges. If these constraints do\n            not hold, all bets are off. This option adds some computational\n            complexity, and is useful if you don't want to have to inspect the\n            rendered image to determine whether to turn on the FlipNormals flag.\n            However, this flag can work with the FlipNormals flag, and if both\n            are set, all the normals in the output will point \"inward\". Defaults\n            to False.\n\n        non_manifold_traversal : bool, optional\n            Turn on/off traversal across non-manifold edges. Changing this may\n            prevent problems where the consistency of polygonal ordering is\n            corrupted due to topological loops. Defaults to True.\n\n        feature_angle : float, optional\n            The angle that defines a sharp edge. If the difference in angle\n            across neighboring polygons is greater than this value, the shared\n            edge is considered \"sharp\". Defaults to 30.0.\n\n        inplace : bool, optional\n            Updates mesh in-place while returning nothing. Defaults to False.\n\n        Returns\n        -------\n        mesh : vtki.PolyData\n            Updated mesh with cell and point normals if inplace=False\n\n        Notes\n        -----\n        Previous arrays named \"Normals\" will be overwritten.\n\n        Normals are computed only for polygons and triangle strips. Normals are\n        not computed for lines or vertices.\n\n        Triangle strips are broken up into triangle polygons. You may want to\n        restrip the triangles.\n\n        May be easier to run mesh.point_normals or mesh.cell_normals\n\n        \"\"\"\n        normal = vtk.vtkPolyDataNormals()\n        normal.SetComputeCellNormals(cell_normals)\n        normal.SetComputePointNormals(point_normals)\n        normal.SetSplitting(split_vertices)\n        normal.SetFlipNormals(flip_normals)\n        normal.SetConsistency(consistent_normals)\n        normal.SetAutoOrientNormals(auto_orient_normals)\n        normal.SetNonManifoldTraversal(non_manifold_traversal)\n        normal.SetFeatureAngle(feature_angle)\n        normal.SetInputData(self)\n        normal.Update()\n\n        mesh = _get_output(normal)\n        if point_normals:\n            mesh.GetPointData().SetActiveNormals('Normals')\n        if cell_normals:\n            mesh.GetCellData().SetActiveNormals('Normals')\n\n\n        if inplace:\n            self.overwrite(mesh)\n        else:\n            return mesh"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nclips a vtki. PolyData or vtki. vtkPolyData with a plane.", "response": "def clip_with_plane(self, origin, normal, value=0, inplace=False):\n        \"\"\"\n        Clip a vtki.PolyData or vtk.vtkPolyData with a plane.\n\n        Can be used to open a mesh which has been closed along a well-defined\n        plane.\n\n        Parameters\n        ----------\n        origin : numpy.ndarray\n            3D point through which plane passes. Defines the plane together with\n            normal parameter.\n\n        normal : numpy.ndarray\n            3D vector defining plane normal.\n\n        value : float, optional\n            Scalar clipping value. The default value is 0.0.\n\n        inplace : bool, optional\n            Updates mesh in-place while returning nothing.\n\n        Returns\n        -------\n        mesh : vtki.PolyData\n            Updated mesh with cell and point normals if inplace=False. Otherwise None.\n\n        Notes\n        -----\n        Not guaranteed to produce a manifold output.\n\n        \"\"\"\n\n        plane = vtk.vtkPlane()\n        plane.SetOrigin(origin)\n        plane.SetNormal(normal)\n        plane.Modified()\n\n        clip = vtk.vtkClipPolyData()\n        clip.SetValue(value)\n        clip.GenerateClippedOutputOn()\n        clip.SetClipFunction(plane)\n\n        clip.SetInputData(self)\n        clip.Update()\n\n        mesh = _get_output(clip)\n        if inplace:\n            self.overwrite(mesh)\n        else:\n            return mesh"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract_largest(self, inplace=False):\n        mesh =  self.connectivity(largest=True)\n        if inplace:\n            self.overwrite(mesh)\n        else:\n            return mesh", "response": "Returns a vtki. PolyData object containing the largest connected set in the mesh."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fill_holes(self, hole_size, inplace=False):  # pragma: no cover\n        logging.warning('vtki.pointset.PolyData.fill_holes is known to segfault. ' +\n                        'Use at your own risk')\n        fill = vtk.vtkFillHolesFilter()\n        fill.SetHoleSize(hole_size)\n        fill.SetInputData(self)\n        fill.Update()\n\n        mesh = _get_output(fill)\n        if inplace:\n            self.overwrite(mesh)\n        else:\n            return mesh", "response": "Fill holes in a vtki. PolyData object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclean a mesh by merging duplicate points remove unused points and remove degenerate cells and remove unused points and remove unused cells.", "response": "def clean(self, point_merging=True, merge_tol=None, lines_to_points=True,\n              polys_to_lines=True, strips_to_polys=True, inplace=False):\n        \"\"\"\n        Cleans mesh by merging duplicate points, remove unused\n        points, and/or remove degenerate cells.\n\n        Parameters\n        ----------\n        point_merging : bool, optional\n            Enables point merging.  On by default.\n\n        merge_tol : float, optional\n            Set merging tolarance.  When enabled merging is set to\n            absolute distance\n\n        lines_to_points : bool, optional\n            Turn on/off conversion of degenerate lines to points.  Enabled by\n            default.\n\n        polys_to_lines : bool, optional\n            Turn on/off conversion of degenerate polys to lines.  Enabled by\n            default.\n\n        strips_to_polys : bool, optional\n            Turn on/off conversion of degenerate strips to polys.\n\n        inplace : bool, optional\n            Updates mesh in-place while returning nothing.  Default True.\n\n        Returns\n        -------\n        mesh : vtki.PolyData\n            Cleaned mesh.  None when inplace=True\n        \"\"\"\n        clean = vtk.vtkCleanPolyData()\n        clean.SetConvertLinesToPoints(lines_to_points)\n        clean.SetConvertPolysToLines(polys_to_lines)\n        clean.SetConvertStripsToPolys(strips_to_polys)\n        if merge_tol:\n            clean.ToleranceIsAbsoluteOn()\n            clean.SetAbsoluteTolerance(merge_tol)\n        clean.SetInputData(self)\n        clean.Update()\n\n        output = _get_output(clean)\n\n        # Check output so no segfaults occur\n        if output.n_points < 1:\n            raise AssertionError('Clean tolerance is too high. Empty mesh returned.')\n\n        if inplace:\n            self.overwrite(output)\n        else:\n            return output"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the total area of the mesh.", "response": "def area(self):\n        \"\"\"\n        Mesh surface area\n\n        Returns\n        -------\n        area : float\n            Total area of the mesh.\n\n        \"\"\"\n        mprop = vtk.vtkMassProperties()\n        mprop.SetInputData(self)\n        return mprop.GetSurfaceArea()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef volume(self):\n        mprop = vtk.vtkMassProperties()\n        mprop.SetInputData(self.tri_filter())\n        return mprop.GetVolume()", "response": "Return the total volume of the mesh"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef obbTree(self):\n        if not hasattr(self, '_obbTree'):\n            self._obbTree = vtk.vtkOBBTree()\n            self._obbTree.SetDataSet(self)\n            self._obbTree.BuildLocator()\n\n        return self._obbTree", "response": "Return the vtkOBBTree object for the specified object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the geodesic path between two vertices using Dijkstra s geodesic path algorithm.", "response": "def geodesic(self, start_vertex, end_vertex, inplace=False):\n        \"\"\"\n        Calculates the geodesic path betweeen two vertices using Dijkstra's\n        algorithm.\n\n        Parameters\n        ----------\n        start_vertex : int\n            Vertex index indicating the start point of the geodesic segment.\n\n        end_vertex : int\n            Vertex index indicating the end point of the geodesic segment.\n\n        Returns\n        -------\n        output : vtki.PolyData\n            PolyData object consisting of the line segment between the two given\n            vertices.\n\n        \"\"\"\n        if start_vertex < 0 or end_vertex > self.n_points - 1:\n            raise IndexError('Invalid indices.')\n\n        dijkstra = vtk.vtkDijkstraGraphGeodesicPath()\n        dijkstra.SetInputData(self)\n        dijkstra.SetStartVertex(start_vertex)\n        dijkstra.SetEndVertex(end_vertex)\n        dijkstra.Update()\n\n        output = _get_output(dijkstra)\n\n        if inplace:\n            self.overwrite(output)\n        else:\n            return output"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef geodesic_distance(self, start_vertex, end_vertex):\n        length = self.geodesic(start_vertex, end_vertex).GetLength()\n        return length", "response": "Calculates the geodesic distance between two vertices using Dijkstra s geodesic algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ray_trace(self, origin, end_point, first_point=False, plot=False,\n                  off_screen=False):\n        \"\"\"\n        Performs a single ray trace calculation given a mesh and a line segment\n        defined by an origin and end_point.\n\n        Parameters\n        ----------\n        origin : np.ndarray or list\n            Start of the line segment.\n\n        end_point : np.ndarray or list\n            End of the line segment.\n\n        first_point : bool, optional\n            Returns intersection of first point only.\n\n        plot : bool, optional\n            Plots ray trace results\n\n        off_screen : bool, optional\n            Plots off screen.  Used for unit testing.\n\n        Returns\n        -------\n        intersection_points : np.ndarray\n            Location of the intersection points.  Empty array if no\n            intersections.\n\n        intersection_cells : np.ndarray\n            Indices of the intersection cells.  Empty array if no\n            intersections.\n\n        \"\"\"\n        points = vtk.vtkPoints()\n        cell_ids = vtk.vtkIdList()\n        code = self.obbTree.IntersectWithLine(np.array(origin),\n                                              np.array(end_point),\n                                              points, cell_ids)\n\n        intersection_points = vtk_to_numpy(points.GetData())\n        if first_point and intersection_points.shape[0] >= 1:\n            intersection_points = intersection_points[0]\n\n        intersection_cells = []\n        if intersection_points.any():\n            if first_point:\n                ncells = 1\n            else:\n                ncells = cell_ids.GetNumberOfIds()\n            for i in range(ncells):\n                intersection_cells.append(cell_ids.GetId(i))\n        intersection_cells = np.array(intersection_cells)\n\n        if plot:\n            plotter = vtki.Plotter(off_screen=off_screen)\n            plotter.add_mesh(self, label='Test Mesh')\n            segment = np.array([origin, end_point])\n            plotter.add_lines(segment, 'b', label='Ray Segment')\n            plotter.add_mesh(intersection_points, 'r', point_size=10,\n                             label='Intersection Points')\n            plotter.add_legend()\n            plotter.add_axes()\n            plotter.show()\n\n        return intersection_points, intersection_cells", "response": "Performs a single ray trace calculation for a single line segment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot the boundaries of a mesh", "response": "def plot_boundaries(self, **kwargs):\n        \"\"\" Plots boundaries of a mesh \"\"\"\n        edges = self.extract_edges()\n\n        plotter = vtki.Plotter(off_screen=kwargs.pop('off_screen', False),\n                               notebook=kwargs.pop('notebook', None))\n        plotter.add_mesh(edges, 'r', style='wireframe', legend='Edges')\n        plotter.add_mesh(self, legend='Mesh', **kwargs)\n        return plotter.show()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplotting the point normals of a mesh.", "response": "def plot_normals(self, show_mesh=True, mag=1.0, flip=False,\n                     use_every=1, **kwargs):\n        \"\"\"\n        Plot the point normals of a mesh.\n        \"\"\"\n        plotter = vtki.Plotter(off_screen=kwargs.pop('off_screen', False),\n                               notebook=kwargs.pop('notebook', None))\n        if show_mesh:\n            plotter.add_mesh(self, **kwargs)\n\n        normals = self.point_normals\n        if flip:\n            normals *= -1\n        plotter.add_arrows(self.points[::use_every],\n                           normals[::use_every], mag=mag)\n        return plotter.show()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrebuild a new mesh by removing points.", "response": "def remove_points(self, remove, mode='any', keep_scalars=True, inplace=False):\n        \"\"\"\n        Rebuild a mesh by removing points.  Only valid for\n        all-triangle meshes.\n\n        Parameters\n        ----------\n        remove : np.ndarray\n            If remove is a bool array, points that are True will be\n            removed.  Otherwise, it is treated as a list of indices.\n\n        mode : str, optional\n            When 'all', only faces containing all points flagged for\n            removal will be removed.  Default 'all'\n\n        keep_scalars : bool, optional\n            When True, point and cell scalars will be passed on to the\n            new mesh.\n\n        inplace : bool, optional\n            Updates mesh in-place while returning nothing.\n\n        Returns\n        -------\n        mesh : vtki.PolyData\n            Mesh without the points flagged for removal.  Not returned\n            when inplace=False.\n\n        ridx : np.ndarray\n            Indices of new points relative to the original mesh.  Not\n            returned when inplace=False.\n\n        \"\"\"\n        if isinstance(remove, list):\n            remove = np.asarray(remove)\n\n        if remove.dtype == np.bool:\n            if remove.size != self.n_points:\n                raise AssertionError('Mask different size than n_points')\n            remove_mask = remove\n        else:\n            remove_mask = np.zeros(self.n_points, np.bool)\n            remove_mask[remove] = True\n\n        try:\n            f = self.faces.reshape(-1, 4)[:, 1:]\n        except:\n            raise Exception('Mesh must consist of only triangles')\n\n        vmask = remove_mask.take(f)\n        if mode == 'all':\n            fmask = ~(vmask).all(1)\n        else:\n            fmask = ~(vmask).any(1)\n\n        # Regenerate face and point arrays\n        uni = np.unique(f.compress(fmask, 0), return_inverse=True)\n        new_points = self.points.take(uni[0], 0)\n\n        nfaces = fmask.sum()\n        faces = np.empty((nfaces, 4), dtype=vtki.ID_TYPE)\n        faces[:, 0] = 3\n        faces[:, 1:] = np.reshape(uni[1], (nfaces, 3))\n\n        newmesh = PolyData(new_points, faces, deep=True)\n        ridx = uni[0]\n\n        # Add scalars back to mesh if requested\n        if keep_scalars:\n            for key in self.point_arrays:\n                newmesh.point_arrays[key] = self.point_arrays[key][ridx]\n\n            for key in self.cell_arrays:\n                try:\n                    newmesh.cell_arrays[key] = self.cell_arrays[key][fmask]\n                except:\n                    log.warning('Unable to pass cell key %s onto reduced mesh' %\n                                key)\n\n        # Return vtk surface and reverse indexing array\n        if inplace:\n            self.overwrite(newmesh)\n        else:\n            return newmesh, ridx"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef flip_normals(self):\n        if self.faces.size % 4:\n            raise Exception('Can only flip normals on an all triangular mesh')\n\n        f = self.faces.reshape((-1, 4))\n        f[:, 1:] = f[:, 1:][:, ::-1]", "response": "Flip normals of a triangular mesh by reversing the point ordering."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\napplying a delaunay 2D filter along the best fitting plane.", "response": "def delaunay_2d(self, tol=1e-05, alpha=0.0, offset=1.0, bound=False, inplace=False):\n        \"\"\"Apply a delaunay 2D filter along the best fitting plane\"\"\"\n        alg = vtk.vtkDelaunay2D()\n        alg.SetProjectionPlaneMode(vtk.VTK_BEST_FITTING_PLANE)\n        alg.SetInputDataObject(self)\n        alg.SetTolerance(tol)\n        alg.SetAlpha(alpha)\n        alg.SetOffset(offset)\n        alg.SetBoundingTriangulation(bound)\n        alg.Update()\n\n        mesh = _get_output(alg)\n        if inplace:\n            self.overwrite(mesh)\n        else:\n            return mesh"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot the curvature of the external surface of the grid.", "response": "def plot_curvature(self, curv_type='mean', **kwargs):\n        \"\"\"\n        Plots the curvature of the external surface of the grid\n\n        Parameters\n        ----------\n        curv_type : str, optional\n            One of the following strings indicating curvature types\n\n            - mean\n            - gaussian\n            - maximum\n            - minimum\n\n        **kwargs : optional\n            Optional keyword arguments.  See help(vtki.plot)\n\n        Returns\n        -------\n        cpos : list\n            Camera position, focal point, and view up.  Used for storing and\n            setting camera view.\n\n        \"\"\"\n        trisurf = self.extract_surface().tri_filter()\n        return trisurf.plot_curvature(curv_type, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting the surface mesh of the grid.", "response": "def extract_surface(self, pass_pointid=True, pass_cellid=True, inplace=False):\n        \"\"\"\n        Extract surface mesh of the grid\n\n        Parameters\n        ----------\n        pass_pointid : bool, optional\n            Adds a point scalar \"vtkOriginalPointIds\" that idenfities which\n            original points these surface points correspond to\n\n        pass_cellid : bool, optional\n            Adds a cell scalar \"vtkOriginalPointIds\" that idenfities which\n            original cells these surface cells correspond to\n\n        inplace : bool, optional\n            Return new mesh or overwrite input.\n\n        Returns\n        -------\n        extsurf : vtki.PolyData\n            Surface mesh of the grid\n        \"\"\"\n        surf_filter = vtk.vtkDataSetSurfaceFilter()\n        surf_filter.SetInputData(self)\n        if pass_pointid:\n            surf_filter.PassThroughCellIdsOn()\n        if pass_cellid:\n            surf_filter.PassThroughPointIdsOn()\n        surf_filter.Update()\n\n        mesh = _get_output(surf_filter)\n        if inplace:\n            self.overwrite(mesh)\n        else:\n            return mesh"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract edges from the surface of the cell.", "response": "def extract_edges(self, feature_angle=30, boundary_edges=True,\n                      non_manifold_edges=True, feature_edges=True,\n                      manifold_edges=True, inplace=False):\n        \"\"\"\n        Extracts edges from the surface of the grid.  From vtk documentation:\n\n        These edges are either\n            1) boundary (used by one polygon) or a line cell;\n            2) non-manifold (used by three or more polygons)\n            3) feature edges (edges used by two triangles and whose\n               dihedral angle > feature_angle)\n            4) manifold edges (edges used by exactly two polygons).\n\n        Parameters\n        ----------\n        feature_angle : float, optional\n            Defaults to 30 degrees.\n\n        boundary_edges : bool, optional\n            Defaults to True\n\n        non_manifold_edges : bool, optional\n            Defaults to True\n\n        feature_edges : bool, optional\n            Defaults to True\n\n        manifold_edges : bool, optional\n            Defaults to True\n\n        inplace : bool, optional\n            Return new mesh or overwrite input.\n\n        Returns\n        -------\n        edges : vtki.vtkPolyData\n            Extracted edges\n\n        \"\"\"\n        surf = self.extract_surface()\n        return surf.extract_edges(feature_angle, boundary_edges,\n                                  non_manifold_edges, feature_edges,\n                                  manifold_edges, inplace=inplace)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate VTK unstructured grid from numpy arrays.", "response": "def _from_arrays(self, offset, cells, cell_type, points, deep=True):\n        \"\"\"\n        Create VTK unstructured grid from numpy arrays\n\n        Parameters\n        ----------\n        offset : np.ndarray dtype=np.int64\n            Array indicating the start location of each cell in the cells\n            array.\n\n        cells : np.ndarray dtype=np.int64\n            Array of cells.  Each cell contains the number of points in the\n            cell and the node numbers of the cell.\n\n        cell_type : np.uint8\n            Cell types of each cell.  Each cell type numbers can be found from\n            vtk documentation.  See example below.\n\n        points : np.ndarray\n            Numpy array containing point locations.\n\n        Examples\n        --------\n        >>> import numpy\n        >>> import vtk\n        >>> import vtki\n        >>> offset = np.array([0, 9])\n        >>> cells = np.array([8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, 13, 14, 15])\n        >>> cell_type = np.array([vtk.VTK_HEXAHEDRON, vtk.VTK_HEXAHEDRON], np.int8)\n\n        >>> cell1 = np.array([[0, 0, 0],\n        ...                   [1, 0, 0],\n        ...                   [1, 1, 0],\n        ...                   [0, 1, 0],\n        ...                   [0, 0, 1],\n        ...                   [1, 0, 1],\n        ...                   [1, 1, 1],\n        ...                   [0, 1, 1]])\n\n        >>> cell2 = np.array([[0, 0, 2],\n        ...                   [1, 0, 2],\n        ...                   [1, 1, 2],\n        ...                   [0, 1, 2],\n        ...                   [0, 0, 3],\n        ...                   [1, 0, 3],\n        ...                   [1, 1, 3],\n        ...                   [0, 1, 3]])\n\n        >>> points = np.vstack((cell1, cell2))\n\n        >>> grid = vtki.UnstructuredGrid(offset, cells, cell_type, points)\n\n        \"\"\"\n\n        if offset.dtype != vtki.ID_TYPE:\n            offset = offset.astype(vtki.ID_TYPE)\n\n        if cells.dtype != vtki.ID_TYPE:\n            cells = cells.astype(vtki.ID_TYPE)\n\n        if not cells.flags['C_CONTIGUOUS']:\n            cells = np.ascontiguousarray(cells)\n\n        # if cells.ndim != 1:\n            # cells = cells.ravel()\n\n        if cell_type.dtype != np.uint8:\n            cell_type = cell_type.astype(np.uint8)\n\n        # Get number of cells\n        ncells = cell_type.size\n\n        # Convert to vtk arrays\n        cell_type = numpy_to_vtk(cell_type, deep=deep)\n        offset = numpy_to_vtkIdTypeArray(offset, deep=deep)\n\n        vtkcells = vtk.vtkCellArray()\n        vtkcells.SetCells(ncells, numpy_to_vtkIdTypeArray(cells.ravel(), deep=deep))\n\n        # Convert points to vtkPoints object\n        points = vtki.vtk_points(points, deep=deep)\n\n        # Create unstructured grid\n        self.SetPoints(points)\n        self.SetCells(cell_type, offset, vtkcells)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload an unstructured grid from a file.", "response": "def _load_file(self, filename):\n        \"\"\"\n        Load an unstructured grid from a file.\n\n        The file extension will select the type of reader to use.  A .vtk\n        extension will use the legacy reader, while .vtu will select the VTK\n        XML reader.\n\n        Parameters\n        ----------\n        filename : str\n            Filename of grid to be loaded.\n        \"\"\"\n        filename = os.path.abspath(os.path.expanduser(filename))\n        # check file exists\n        if not os.path.isfile(filename):\n            raise Exception('%s does not exist' % filename)\n\n        # Check file extention\n        if '.vtu' in filename:\n            reader = vtk.vtkXMLUnstructuredGridReader()\n        elif '.vtk' in filename:\n            reader = vtk.vtkUnstructuredGridReader()\n        else:\n            raise Exception('Extension should be either \".vtu\" or \".vtk\"')\n\n        # load file to self\n        reader.SetFileName(filename)\n        reader.Update()\n        grid = reader.GetOutput()\n        self.ShallowCopy(grid)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef linear_copy(self, deep=False):\n        lgrid = self.copy(deep)\n\n        # grab the vtk object\n        vtk_cell_type = numpy_to_vtk(self.GetCellTypesArray(), deep=True)\n        celltype = vtk_to_numpy(vtk_cell_type)\n        celltype[celltype == VTK_QUADRATIC_TETRA] = VTK_TETRA\n        celltype[celltype == VTK_QUADRATIC_PYRAMID] = VTK_PYRAMID\n        celltype[celltype == VTK_QUADRATIC_WEDGE] = VTK_WEDGE\n        celltype[celltype == VTK_QUADRATIC_HEXAHEDRON] = VTK_HEXAHEDRON\n\n        # track quad mask for later\n        quad_quad_mask = celltype == VTK_QUADRATIC_QUAD\n        celltype[quad_quad_mask] = VTK_QUAD\n\n        quad_tri_mask = celltype == VTK_QUADRATIC_TRIANGLE\n        celltype[quad_tri_mask] = VTK_TRIANGLE\n\n        vtk_offset = self.GetCellLocationsArray()\n        cells = vtk.vtkCellArray()\n        cells.DeepCopy(self.GetCells())\n        lgrid.SetCells(vtk_cell_type, vtk_offset, cells)\n\n        # fixing bug with display of quad cells\n        if np.any(quad_quad_mask):\n            quad_offset = lgrid.offset[quad_quad_mask]\n            base_point = lgrid.cells[quad_offset + 1]\n            lgrid.cells[quad_offset + 5] = base_point\n            lgrid.cells[quad_offset + 6] = base_point\n            lgrid.cells[quad_offset + 7] = base_point\n            lgrid.cells[quad_offset + 8] = base_point\n\n        if np.any(quad_tri_mask):\n            tri_offset = lgrid.offset[quad_tri_mask]\n            base_point = lgrid.cells[tri_offset + 1]\n            lgrid.cells[tri_offset + 4] = base_point\n            lgrid.cells[tri_offset + 5] = base_point\n            lgrid.cells[tri_offset + 6] = base_point\n\n        return lgrid", "response": "Returns a copy of the input unstructured grid containing only\n        linear cells."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a subset of the grid with cells in ind.", "response": "def extract_cells(self, ind):\n        \"\"\"\n        Returns a subset of the grid\n\n        Parameters\n        ----------\n        ind : np.ndarray\n            Numpy array of cell indices to be extracted.\n\n        Returns\n        -------\n        subgrid : vtki.UnstructuredGrid\n            Subselected grid\n\n        \"\"\"\n        if not isinstance(ind, np.ndarray):\n            ind = np.array(ind, np.ndarray)\n\n        if ind.dtype == np.bool:\n            ind = ind.nonzero()[0].astype(vtki.ID_TYPE)\n\n        if ind.dtype != vtki.ID_TYPE:\n            ind = ind.astype(vtki.ID_TYPE)\n\n        if not ind.flags.c_contiguous:\n            ind = np.ascontiguousarray(ind)\n\n        vtk_ind = numpy_to_vtkIdTypeArray(ind, deep=False)\n\n        # Create selection objects\n        selectionNode = vtk.vtkSelectionNode()\n        selectionNode.SetFieldType(vtk.vtkSelectionNode.CELL)\n        selectionNode.SetContentType(vtk.vtkSelectionNode.INDICES)\n        selectionNode.SetSelectionList(vtk_ind)\n\n        selection = vtk.vtkSelection()\n        selection.AddNode(selectionNode)\n\n        # extract\n        extract_sel = vtk.vtkExtractSelection()\n        extract_sel.SetInputData(0, self)\n        extract_sel.SetInputData(1, selection)\n        extract_sel.Update()\n        subgrid = _get_output(extract_sel)\n\n        # extracts only in float32\n        if self.points.dtype is not np.dtype('float32'):\n            ind = subgrid.point_arrays['vtkOriginalPointIds']\n            subgrid.points = self.points[ind]\n\n        return subgrid"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a new unstructured grid with the same size and grids merged.", "response": "def merge(self, grid=None, merge_points=True, inplace=False,\n              main_has_priority=True):\n        \"\"\"\n        Join one or many other grids to this grid.  Grid is updated\n        in-place by default.\n\n        Can be used to merge points of adjcent cells when no grids\n        are input.\n\n        Parameters\n        ----------\n        grid : vtk.UnstructuredGrid or list of vtk.UnstructuredGrids\n            Grids to merge to this grid.\n\n        merge_points : bool, optional\n            Points in exactly the same location will be merged between\n            the two meshes.\n\n        inplace : bool, optional\n            Updates grid inplace when True.\n\n        main_has_priority : bool, optional\n            When this parameter is true and merge_points is true,\n            the scalar arrays of the merging grids will be overwritten\n            by the original main mesh.\n\n        Returns\n        -------\n        merged_grid : vtk.UnstructuredGrid\n            Merged grid.  Returned when inplace is False.\n\n        Notes\n        -----\n        When two or more grids are joined, the type and name of each\n        scalar array must match or the arrays will be ignored and not\n        included in the final merged mesh.\n        \"\"\"\n        append_filter = vtk.vtkAppendFilter()\n        append_filter.SetMergePoints(merge_points)\n\n        if not main_has_priority:\n            append_filter.AddInputData(self)\n\n        if isinstance(grid, vtki.UnstructuredGrid):\n            append_filter.AddInputData(grid)\n        elif isinstance(grid, list):\n            grids = grid\n            for grid in grids:\n                append_filter.AddInputData(grid)\n\n        if main_has_priority:\n            append_filter.AddInputData(self)\n\n        append_filter.Update()\n        merged = _get_output(append_filter)\n        if inplace:\n            self.DeepCopy(merged)\n        else:\n            return merged"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delaunay_2d(self, tol=1e-05, alpha=0.0, offset=1.0, bound=False):\n        return PolyData(self.points).delaunay_2d(tol=tol, alpha=alpha, offset=offset, bound=bound)", "response": "Apply a delaunay 2D filter along the best fitting plane. This method returns a PolyData object with the points and perfoms extracted along the best fitting plane."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _from_arrays(self, x, y, z):\n        if not(x.shape == y.shape == z.shape):\n            raise Exception('Input point array shapes must match exactly')\n\n        # make the output points the same precision as the input arrays\n        points = np.empty((x.size, 3), x.dtype)\n        points[:, 0] = x.ravel('F')\n        points[:, 1] = y.ravel('F')\n        points[:, 2] = z.ravel('F')\n\n        # ensure that the inputs are 3D\n        dim = list(x.shape)\n        while len(dim) < 3:\n            dim.append(1)\n\n        # Create structured grid\n        self.SetDimensions(dim)\n        self.SetPoints(vtki.vtk_points(points))", "response": "Create VTK structured grid directly from numpy arrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save(self, filename, binary=True):\n        filename = os.path.abspath(os.path.expanduser(filename))\n        # Use legacy writer if vtk is in filename\n        if '.vtk' in filename:\n            writer = vtk.vtkStructuredGridWriter()\n            if binary:\n                writer.SetFileTypeToBinary()\n            else:\n                writer.SetFileTypeToASCII()\n        elif '.vts' in filename:\n            writer = vtk.vtkXMLStructuredGridWriter()\n            if binary:\n                writer.SetDataModeToBinary()\n            else:\n                writer.SetDataModeToAscii()\n        else:\n            raise Exception('Extension should be either \".vts\" (xml) or' +\n                            '\".vtk\" (legacy)')\n        # Write\n        writer.SetFileName(filename)\n        writer.SetInputData(self)\n        writer.Write()", "response": "Writes a structured grid to disk."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the dataset dimensions. Pass a length three tuple of integers", "response": "def dimensions(self, dims):\n        \"\"\"Sets the dataset dimensions. Pass a length three tuple of integers\"\"\"\n        nx, ny, nz = dims[0], dims[1], dims[2]\n        self.SetDimensions(nx, ny, nz)\n        self.Modified()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating VTK rectilinear grid directly from numpy arrays.", "response": "def _from_arrays(self, x, y, z):\n        \"\"\"\n        Create VTK rectilinear grid directly from numpy arrays. Each array\n        gives the uniques coordinates of the mesh along each axial direction.\n        To help ensure you are using this correctly, we take the unique values\n        of each argument.\n\n        Parameters\n        ----------\n        x : np.ndarray\n            Coordinates of the nodes in x direction.\n\n        y : np.ndarray\n            Coordinates of the nodes in y direction.\n\n        z : np.ndarray\n            Coordinates of the nodes in z direction.\n        \"\"\"\n        x = np.unique(x.ravel())\n        y = np.unique(y.ravel())\n        z = np.unique(z.ravel())\n        # Set the cell spacings and dimensions of the grid\n        self.SetDimensions(len(x), len(y), len(z))\n        self.SetXCoordinates(numpy_to_vtk(x))\n        self.SetYCoordinates(numpy_to_vtk(y))\n        self.SetZCoordinates(numpy_to_vtk(z))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a pointer to the points as a numpy array", "response": "def points(self):\n        \"\"\" returns a pointer to the points as a numpy object \"\"\"\n        x = vtk_to_numpy(self.GetXCoordinates())\n        y = vtk_to_numpy(self.GetYCoordinates())\n        z = vtk_to_numpy(self.GetZCoordinates())\n        xx, yy, zz = np.meshgrid(x,y,z, indexing='ij')\n        return np.c_[xx.ravel(), yy.ravel(), zz.ravel()]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset points without copying", "response": "def points(self, points):\n        \"\"\" set points without copying \"\"\"\n        if not isinstance(points, np.ndarray):\n            raise TypeError('Points must be a numpy array')\n        # get the unique coordinates along each axial direction\n        x = np.unique(points[:,0])\n        y = np.unique(points[:,1])\n        z = np.unique(points[:,2])\n        # Set the vtk coordinates\n        self._from_arrays(x, y, z)\n        #self._point_ref = points\n        self.Modified()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _load_file(self, filename):\n        filename = os.path.abspath(os.path.expanduser(filename))\n        # check file exists\n        if not os.path.isfile(filename):\n            raise Exception('{} does not exist'.format(filename))\n\n        # Check file extention\n        if '.vtr' in filename:\n            legacy_writer = False\n        elif '.vtk' in filename:\n            legacy_writer = True\n        else:\n            raise Exception(\n                'Extension should be either \".vtr\" (xml) or \".vtk\" (legacy)')\n\n        # Create reader\n        if legacy_writer:\n            reader = vtk.vtkRectilinearGridReader()\n        else:\n            reader = vtk.vtkXMLRectilinearGridReader()\n\n        # load file to self\n        reader.SetFileName(filename)\n        reader.Update()\n        grid = reader.GetOutput()\n        self.ShallowCopy(grid)", "response": "Loads a rectilinear grid from a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save(self, filename, binary=True):\n        filename = os.path.abspath(os.path.expanduser(filename))\n        # Use legacy writer if vtk is in filename\n        if '.vtk' in filename:\n            writer = vtk.vtkRectilinearGridWriter()\n            legacy = True\n        elif '.vtr' in filename:\n            writer = vtk.vtkXMLRectilinearGridWriter()\n            legacy = False\n        else:\n            raise Exception('Extension should be either \".vtr\" (xml) or' +\n                            '\".vtk\" (legacy)')\n        # Write\n        writer.SetFileName(filename)\n        writer.SetInputData(self)\n        if binary and legacy:\n            writer.SetFileTypeToBinary()\n        writer.Write()", "response": "Writes a rectilinear grid to disk."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _from_specs(self, dims, spacing=(1.0,1.0,1.0), origin=(0.0, 0.0, 0.0)):\n        xn, yn, zn = dims[0], dims[1], dims[2]\n        xs, ys, zs = spacing[0], spacing[1], spacing[2]\n        xo, yo, zo = origin[0], origin[1], origin[2]\n        self.SetDimensions(xn, yn, zn)\n        self.SetOrigin(xo, yo, zo)\n        self.SetSpacing(xs, ys, zs)", "response": "Create VTK image data directly from numpy arrays."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef points(self):\n        # Get grid dimensions\n        nx, ny, nz = self.dimensions\n        nx -= 1\n        ny -= 1\n        nz -= 1\n        # get the points and convert to spacings\n        dx, dy, dz = self.spacing\n        # Now make the cell arrays\n        ox, oy, oz = self.origin\n        x = np.insert(np.cumsum(np.full(nx, dx)), 0, 0.0) + ox\n        y = np.insert(np.cumsum(np.full(ny, dy)), 0, 0.0) + oy\n        z = np.insert(np.cumsum(np.full(nz, dz)), 0, 0.0) + oz\n        xx, yy, zz = np.meshgrid(x,y,z, indexing='ij')\n        return np.c_[xx.ravel(), yy.ravel(), zz.ravel()]", "response": "returns a pointer to the points as a numpy array"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef points(self, points):\n        if not isinstance(points, np.ndarray):\n            raise TypeError('Points must be a numpy array')\n        # get the unique coordinates along each axial direction\n        x = np.unique(points[:,0])\n        y = np.unique(points[:,1])\n        z = np.unique(points[:,2])\n        nx, ny, nz = len(x), len(y), len(z)\n        # TODO: this needs to be tested (unique might return a tuple)\n        dx, dy, dz = np.unique(np.diff(x)), np.unique(np.diff(y)), np.unique(np.diff(z))\n        ox, oy, oz = np.min(x), np.min(y), np.min(z)\n        # Build the vtk object\n        self._from_specs((nx,ny,nz), (dx,dy,dz), (ox,oy,oz))\n        #self._point_ref = points\n        self.Modified()", "response": "set the points without copying"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the origin. Pass a length three tuple of floats", "response": "def origin(self, origin):\n        \"\"\"Set the origin. Pass a length three tuple of floats\"\"\"\n        ox, oy, oz = origin[0], origin[1], origin[2]\n        self.SetOrigin(ox, oy, oz)\n        self.Modified()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the spacing in each axial direction. Pass a length three tuple of floats", "response": "def spacing(self, spacing):\n        \"\"\"Set the spacing in each axial direction. Pass a length three tuple of\n        floats\"\"\"\n        dx, dy, dz = spacing[0], spacing[1], spacing[2]\n        self.SetSpacing(dx, dy, dz)\n        self.Modified()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resample_image(arr, max_size=400):\n    dim = np.max(arr.shape[0:2])\n    if dim < max_size:\n        max_size = dim\n    x, y, _ = arr.shape\n    sx = int(np.ceil(x / max_size))\n    sy = int(np.ceil(y / max_size))\n    img = np.zeros((max_size, max_size, 3), dtype=arr.dtype)\n    arr = arr[0:-1:sx, 0:-1:sy, :]\n    xl = (max_size - arr.shape[0]) // 2\n    yl = (max_size - arr.shape[1]) // 2\n    img[xl:arr.shape[0]+xl, yl:arr.shape[1]+yl, :] = arr\n    return img", "response": "Resamples a square image to an image of max_size"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pad_image(arr, max_size=400):\n    dim = np.max(arr.shape)\n    img = np.zeros((dim, dim, 3), dtype=arr.dtype)\n    xl = (dim - arr.shape[0]) // 2\n    yl = (dim - arr.shape[1]) // 2\n    img[xl:arr.shape[0]+xl, yl:arr.shape[1]+yl, :] = arr\n    return resample_image(img, max_size=max_size)", "response": "Pads an image to a square then resamples to max_size"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nemit the DLG_accepted signal that the file dialog was closed properly.", "response": "def emit_accepted(self):\n        \"\"\"\n        Sends signal that the file dialog was closed properly.\n\n        Sends:\n        filename\n        \"\"\"\n        if self.result():\n            filename = self.selectedFiles()[0]\n            if os.path.isdir(os.path.dirname(filename)):\n                self.dlg_accepted.emit(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_scale(self, value):\n        self.plotter.set_scale(self.x_slider_group.value,\n                               self.y_slider_group.value,\n                               self.z_slider_group.value)", "response": "updates the scale of all actors in the plotter"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nopening scale axes dialog", "response": "def scale_axes_dialog(self, show=True):\n        \"\"\" Open scale axes dialog \"\"\"\n        return ScaleAxesDialog(self.app_window, self, show=show)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nclears all camera positions", "response": "def clear_camera_positions(self):\n        \"\"\" clears all camera positions \"\"\"\n        for action in self.saved_camera_menu.actions():\n            self.saved_camera_menu.removeAction(action)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_camera_position(self):\n        self.saved_camera_positions.append(self.camera_position)\n        ncam = len(self.saved_camera_positions)\n        camera_position = self.camera_position[:]  # py2.7 copy compatibility\n\n        def load_camera_position():\n            self.camera_position = camera_position\n\n        self.saved_camera_menu.addAction('Camera Position %2d' % ncam,\n                                         load_camera_position)", "response": "Saves camera position to saved camera menu for recall"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _spawn_background_rendering(self, rate=5.0):\n        self.render_trigger.connect(self.ren_win.Render)\n        twait = rate**-1\n\n        def render():\n            while self.active:\n                time.sleep(twait)\n                self._render()\n\n        self.render_thread = Thread(target=render)\n        self.render_thread.start()", "response": "Spawns a thread that updates the render window."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the app icon if the user has not resized the window.", "response": "def update_app_icon(self):\n        \"\"\"\n        Update the app icon if the user is not trying to resize the window.\n        \"\"\"\n        if os.name == 'nt' or not hasattr(self, '_last_window_size'):  # pragma: no cover\n            # DO NOT EVEN ATTEMPT TO UPDATE ICON ON WINDOWS\n            return\n        cur_time = time.time()\n        if self._last_window_size != self.window_size:  # pragma: no cover\n            # Window size hasn't remained constant since last render.\n            # This means the user is resizing it so ignore update.\n            pass\n        elif ((cur_time - self._last_update_time > BackgroundPlotter.ICON_TIME_STEP)\n                and self._last_camera_pos != self.camera_position):\n            # its been a while since last update OR\n            #   the camera position has changed and its been at leat one second\n\n            # Update app icon as preview of the window\n            img = pad_image(self.image)\n            qimage = QtGui.QImage(img.copy(), img.shape[1],\n                                  img.shape[0], QtGui.QImage.Format_RGB888)\n            icon = QtGui.QIcon(QtGui.QPixmap.fromImage(qimage))\n\n            self.app.setWindowIcon(icon)\n\n            # Update trackers\n            self._last_update_time = cur_time\n            self._last_camera_pos = self.camera_position\n        # Update trackers\n        self._last_window_size = self.window_size"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nspawns a save file dialog to export a vtkjs file.", "response": "def _qt_export_vtkjs(self, show=True):\n        \"\"\"\n        Spawn an save file dialog to export a vtkjs file.\n        \"\"\"\n        return FileDialog(self.app_window,\n                          filefilter=['VTK JS File(*.vtkjs)'],\n                          show=show,\n                          directory=os.getcwd(),\n                          callback=self.export_vtkjs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn render window size", "response": "def window_size(self):\n        \"\"\" returns render window size \"\"\"\n        the_size = self.app_window.baseSize()\n        return the_size.width(), the_size.height()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the render window size", "response": "def window_size(self, window_size):\n        \"\"\" set the render window size \"\"\"\n        BasePlotter.window_size.fset(self, window_size)\n        self.app_window.setBaseSize(*window_size)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hex_to_rgb(h):\n    h = h.lstrip('#')\n    return tuple(int(h[i:i+2], 16)/255. for i in (0, 2 ,4))", "response": "Converts a hex list or tuple to rgb tuple"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a literal color string to a color rgb.", "response": "def string_to_rgb(string):\n    \"\"\"\n    Converts a literal color string (i.e. white) to a color rgb.  Also accepts\n    hex strings or single characters from the following list.\n\n        b: blue\n        g: green\n        r: red\n        c: cyan\n        m: magenta\n        y: yellow\n        k: black\n        w: white\n\n    \"\"\"\n\n    # if a single character\n    if len(string) == 1:\n\n        # Convert from single character to full hex\n        if string.lower() not in color_char_to_word:\n            raise ValueError('Single character string must be one of the following:'\n                             '\\n%s' % str(color_char_to_word.keys()))\n\n        colorhex = hexcolors[color_char_to_word[string.lower()]]\n\n    # if a color name\n    elif string.lower() in hexcolors:\n        colorhex = hexcolors[string.lower()]\n\n    # try to convert to hex\n    else:\n        try:\n            return hex_to_rgb(string)\n        except:\n            raise ValueError('Invalid color string or hex string.')\n\n    return hex_to_rgb(colorhex)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets a file to write out the VTK errors", "response": "def set_error_output_file(filename):\n    \"\"\"Sets a file to write out the VTK errors\"\"\"\n    filename = os.path.abspath(os.path.expanduser(filename))\n    fileOutputWindow = vtk.vtkFileOutputWindow()\n    fileOutputWindow.SetFileName(filename)\n    outputWindow = vtk.vtkOutputWindow()\n    outputWindow.SetInstance(fileOutputWindow)\n    return fileOutputWindow, outputWindow"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send_errors_to_logging():\n    error_output = vtk.vtkStringOutputWindow()\n    error_win = vtk.vtkOutputWindow()\n    error_win.SetInstance(error_output)\n    obs = Observer()\n    return obs.observe(error_output)", "response": "Send all VTK error messages to Python s logging module"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef log_message(self, kind, alert):\n        if kind == 'ERROR':\n            logging.error(alert)\n        else:\n            logging.warning(alert)\n        return", "response": "Parses different event types and passes them to logging"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef observe(self, algorithm):\n        if self.__observing:\n            raise RuntimeError('This error observer is already observing an algorithm.')\n        if hasattr(algorithm, 'GetExecutive') and algorithm.GetExecutive() is not None:\n            algorithm.GetExecutive().AddObserver(self.event_type, self)\n        algorithm.AddObserver(self.event_type, self)\n        self.__observing = True\n        return", "response": "Make this an observer of an algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the data range of the array s component", "response": "def get_range_info(array, component):\n    \"\"\"Get the data range of the array's component\"\"\"\n    r = array.GetRange(component)\n    comp_range = {}\n    comp_range['min'] = r[0]\n    comp_range['max'] = r[1]\n    comp_range['component'] = array.GetComponentName(component)\n    return comp_range"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting reference to a single file in a directory.", "response": "def get_ref(dest_dir, md5):\n    \"\"\"Get reference\"\"\"\n    ref = {}\n    ref['id'] = md5\n    ref['encode'] = 'BigEndian' if sys.byteorder == 'big' else 'LittleEndian'\n    ref['basepath'] = dest_dir\n    return ref"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dump_data_array(dataset_dir, data_dir, array, root=None, compress=True):\n    if root is None:\n        root = {}\n    if not array:\n        return None\n\n    if array.GetDataType() == 12:\n        # IdType need to be converted to Uint32\n        array_size = array.GetNumberOfTuples() * array.GetNumberOfComponents()\n        new_array = vtk.vtkTypeUInt32Array()\n        new_array.SetNumberOfTuples(array_size)\n        for i in range(array_size):\n            new_array.SetValue(i, -1 if array.GetValue(i) <\n                              0 else array.GetValue(i))\n        pbuffer = memoryview(new_array)\n    else:\n        pbuffer = memoryview(array)\n\n    pMd5 = hashlib.md5(pbuffer).hexdigest()\n    ppath = os.path.join(data_dir, pMd5)\n    with open(ppath, 'wb') as f:\n        f.write(pbuffer)\n\n    if compress:\n        with open(ppath, 'rb') as f_in, gzip.open(os.path.join(data_dir, pMd5 + '.gz'), 'wb') as f_out:\n            shutil.copyfileobj(f_in, f_out)\n        # Close then remove.\n        os.remove(ppath)\n\n    root['ref'] = get_ref(os.path.relpath(data_dir, dataset_dir), pMd5)\n    root['vtkClass'] = 'vtkDataArray'\n    root['name'] = array.GetName()\n    root['dataType'] = jsMapping[arrayTypesMapping[array.GetDataType()]]\n    root['numberOfComponents'] = array.GetNumberOfComponents()\n    root['size'] = array.GetNumberOfComponents() * array.GetNumberOfTuples()\n    root['ranges'] = []\n    if root['numberOfComponents'] > 1:\n        for i in range(root['numberOfComponents']):\n            root['ranges'].append(get_range_info(array, i))\n        root['ranges'].append(get_range_info(array, -1))\n    else:\n        root['ranges'].append(get_range_info(array, 0))\n\n    return root", "response": "Dump a vtkjs data array to a vtkjs data structure"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dump_color_array(dataset_dir, data_dir, color_array_info, root=None, compress=True):\n    if root is None:\n        root = {}\n    root['pointData'] = {\n        'vtkClass': 'vtkDataSetAttributes',\n        \"activeGlobalIds\": -1,\n        \"activeNormals\": -1,\n        \"activePedigreeIds\": -1,\n        \"activeScalars\": -1,\n        \"activeTCoords\": -1,\n        \"activeTensors\": -1,\n        \"activeVectors\": -1,\n        \"arrays\": []\n    }\n    root['cellData'] = {\n        'vtkClass': 'vtkDataSetAttributes',\n        \"activeGlobalIds\": -1,\n        \"activeNormals\": -1,\n        \"activePedigreeIds\": -1,\n        \"activeScalars\": -1,\n        \"activeTCoords\": -1,\n        \"activeTensors\": -1,\n        \"activeVectors\": -1,\n        \"arrays\": []\n    }\n    root['fieldData'] = {\n        'vtkClass': 'vtkDataSetAttributes',\n        \"activeGlobalIds\": -1,\n        \"activeNormals\": -1,\n        \"activePedigreeIds\": -1,\n        \"activeScalars\": -1,\n        \"activeTCoords\": -1,\n        \"activeTensors\": -1,\n        \"activeVectors\": -1,\n        \"arrays\": []\n    }\n\n    colorArray = color_array_info['colorArray']\n    location = color_array_info['location']\n\n    dumped_array = dump_data_array(dataset_dir, data_dir, colorArray, {}, compress)\n\n    if dumped_array:\n        root[location]['activeScalars'] = 0\n        root[location]['arrays'].append({'data': dumped_array})\n\n    return root", "response": "Dump a vtkjs color array into a vtkjs object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndump vtkjs texture coordinates", "response": "def dump_t_coords(dataset_dir, data_dir, dataset, root=None, compress=True):\n    \"\"\"dump vtkjs texture coordinates\"\"\"\n    if root is None:\n        root = {}\n    tcoords = dataset.GetPointData().GetTCoords()\n    if tcoords:\n        dumped_array = dump_data_array(dataset_dir, data_dir, tcoords, {}, compress)\n        root['pointData']['activeTCoords'] = len(root['pointData']['arrays'])\n        root['pointData']['arrays'].append({'data': dumped_array})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dump_normals(dataset_dir, data_dir, dataset, root=None, compress=True):\n    if root is None:\n        root = {}\n    normals = dataset.GetPointData().GetNormals()\n    if normals:\n        dumped_array = dump_data_array(dataset_dir, data_dir, normals, {}, compress)\n        root['pointData']['activeNormals'] = len(root['pointData']['arrays'])\n        root['pointData']['arrays'].append({'data': dumped_array})", "response": "dump vtkjs normal vectors"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dump_all_arrays(dataset_dir, data_dir, dataset, root=None, compress=True):\n    if root is None:\n        root = {}\n    root['pointData'] = {\n        'vtkClass': 'vtkDataSetAttributes',\n        \"activeGlobalIds\": -1,\n        \"activeNormals\": -1,\n        \"activePedigreeIds\": -1,\n        \"activeScalars\": -1,\n        \"activeTCoords\": -1,\n        \"activeTensors\": -1,\n        \"activeVectors\": -1,\n        \"arrays\": []\n    }\n    root['cellData'] = {\n        'vtkClass': 'vtkDataSetAttributes',\n        \"activeGlobalIds\": -1,\n        \"activeNormals\": -1,\n        \"activePedigreeIds\": -1,\n        \"activeScalars\": -1,\n        \"activeTCoords\": -1,\n        \"activeTensors\": -1,\n        \"activeVectors\": -1,\n        \"arrays\": []\n    }\n    root['fieldData'] = {\n        'vtkClass': 'vtkDataSetAttributes',\n        \"activeGlobalIds\": -1,\n        \"activeNormals\": -1,\n        \"activePedigreeIds\": -1,\n        \"activeScalars\": -1,\n        \"activeTCoords\": -1,\n        \"activeTensors\": -1,\n        \"activeVectors\": -1,\n        \"arrays\": []\n    }\n\n    # Point data\n    pd = dataset.GetPointData()\n    pd_size = pd.GetNumberOfArrays()\n    for i in range(pd_size):\n        array = pd.GetArray(i)\n        if array:\n            dumped_array = dump_data_array(\n                dataset_dir, data_dir, array, {}, compress)\n            root['pointData']['activeScalars'] = 0\n            root['pointData']['arrays'].append({'data': dumped_array})\n\n    # Cell data\n    cd = dataset.GetCellData()\n    cd_size = pd.GetNumberOfArrays()\n    for i in range(cd_size):\n        array = cd.GetArray(i)\n        if array:\n            dumped_array = dump_data_array(\n                dataset_dir, data_dir, array, {}, compress)\n            root['cellData']['activeScalars'] = 0\n            root['cellData']['arrays'].append({'data': dumped_array})\n\n    return root", "response": "Dump all data arrays to vtkjs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dump_poly_data(dataset_dir, data_dir, dataset, color_array_info, root=None, compress=True):\n    if root is None:\n        root = {}\n    root['vtkClass'] = 'vtkPolyData'\n    container = root\n\n    # Points\n    points = dump_data_array(dataset_dir, data_dir,\n                           dataset.GetPoints().GetData(), {}, compress)\n    points['vtkClass'] = 'vtkPoints'\n    container['points'] = points\n\n    # Cells\n    _cells = container\n\n    # Verts\n    if dataset.GetVerts() and dataset.GetVerts().GetData().GetNumberOfTuples() > 0:\n        _verts = dump_data_array(dataset_dir, data_dir,\n                               dataset.GetVerts().GetData(), {}, compress)\n        _cells['verts'] = _verts\n        _cells['verts']['vtkClass'] = 'vtkCellArray'\n\n    # Lines\n    if dataset.GetLines() and dataset.GetLines().GetData().GetNumberOfTuples() > 0:\n        _lines = dump_data_array(dataset_dir, data_dir,\n                               dataset.GetLines().GetData(), {}, compress)\n        _cells['lines'] = _lines\n        _cells['lines']['vtkClass'] = 'vtkCellArray'\n\n    # Polys\n    if dataset.GetPolys() and dataset.GetPolys().GetData().GetNumberOfTuples() > 0:\n        _polys = dump_data_array(dataset_dir, data_dir,\n                               dataset.GetPolys().GetData(), {}, compress)\n        _cells['polys'] = _polys\n        _cells['polys']['vtkClass'] = 'vtkCellArray'\n\n    # Strips\n    if dataset.GetStrips() and dataset.GetStrips().GetData().GetNumberOfTuples() > 0:\n        _strips = dump_data_array(dataset_dir, data_dir,\n                                dataset.GetStrips().GetData(), {}, compress)\n        _cells['strips'] = _strips\n        _cells['strips']['vtkClass'] = 'vtkCellArray'\n\n    dump_color_array(dataset_dir, data_dir, color_array_info, container, compress)\n\n    # PointData TCoords\n    dump_t_coords(dataset_dir, data_dir, dataset, container, compress)\n    # dump_normals(dataset_dir, data_dir, dataset, container, compress)\n\n    return root", "response": "Dump a vtkPolyData object to vtkjs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndump image data to vtkjs", "response": "def dump_image_data(dataset_dir, data_dir, dataset, color_array_info, root=None, compress=True):\n    \"\"\"Dump image data object to vtkjs\"\"\"\n    if root is None:\n        root = {}\n    root['vtkClass'] = 'vtkImageData'\n    container = root\n\n    container['spacing'] = dataset.GetSpacing()\n    container['origin'] = dataset.GetOrigin()\n    container['extent'] = dataset.GetExtent()\n\n    dump_all_arrays(dataset_dir, data_dir, dataset, container, compress)\n\n    return root"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites dataset to vtkjs", "response": "def write_data_set(file_path, dataset, output_dir, color_array_info, new_name=None, compress=True):\n    \"\"\"write dataset to vtkjs\"\"\"\n    fileName = new_name if new_name else os.path.basename(file_path)\n    dataset_dir = os.path.join(output_dir, fileName)\n    data_dir = os.path.join(dataset_dir, 'data')\n\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    root = {}\n    root['metadata'] = {}\n    root['metadata']['name'] = fileName\n\n    writer = writer_mapping[dataset.GetClassName()]\n    if writer:\n        writer(dataset_dir, data_dir, dataset, color_array_info, root, compress)\n    else:\n        print(dataObject.GetClassName(), 'is not supported')\n\n    with open(os.path.join(dataset_dir, \"index.json\"), 'w') as f:\n        f.write(json.dumps(root, indent=2))\n\n    return dataset_dir"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef export_plotter_vtkjs(plotter, filename, compress_arrays=False):\n    sceneName = os.path.split(filename)[1]\n    doCompressArrays = compress_arrays\n\n    # Generate timestamp and use it to make subdirectory within the top level output dir\n    timeStamp = time.strftime(\"%a-%d-%b-%Y-%H-%M-%S\")\n    root_output_directory = os.path.split(filename)[0]\n    output_dir = os.path.join(root_output_directory, timeStamp)\n    mkdir_p(output_dir)\n\n    renderers = plotter.ren_win.GetRenderers()\n\n    scDirs = []\n    sceneComponents = []\n    textureToSave = {}\n\n    for rIdx in range(renderers.GetNumberOfItems()):\n        renderer = renderers.GetItemAsObject(rIdx)\n        renProps = renderer.GetViewProps()\n        for rpIdx in range(renProps.GetNumberOfItems()):\n            renProp = renProps.GetItemAsObject(rpIdx)\n            if not renProp.GetVisibility():\n                continue\n            if hasattr(renProp, 'GetMapper') and renProp.GetMapper() is not None:\n                mapper = renProp.GetMapper()\n                dataObject = mapper.GetInputDataObject(0, 0)\n                dataset = None\n                if dataObject is None:\n                    continue\n                if dataObject.IsA('vtkCompositeDataSet'):\n                    if dataObject.GetNumberOfBlocks() == 1:\n                        dataset = dataObject.GetBlock(0)\n                    else:\n                        gf = vtk.vtkCompositeDataGeometryFilter()\n                        gf.SetInputData(dataObject)\n                        gf.Update()\n                        dataset = gf.GetOutput()\n                else:\n                    dataset = mapper.GetInput()\n\n                if dataset and not isinstance(dataset, (vtk.vtkPolyData, vtk.vtkImageData)):\n                    # All data must be PolyData surfaces\n                    gf = vtk.vtkGeometryFilter()\n                    gf.SetInputData(dataset)\n                    gf.Update()\n                    dataset = gf.GetOutputDataObject(0)\n\n\n                if dataset:# and dataset.GetPoints(): # NOTE: vtkImageData does not have points\n                    componentName = 'data_%d_%d' % (\n                        rIdx, rpIdx)  # getComponentName(renProp)\n                    scalarVisibility = mapper.GetScalarVisibility()\n                    #arrayAccessMode = mapper.GetArrayAccessMode()\n                    #colorArrayName = mapper.GetArrayName() #TODO: if arrayAccessMode == 1 else mapper.GetArrayId()\n                    colorMode = mapper.GetColorMode()\n                    scalarMode = mapper.GetScalarMode()\n                    lookupTable = mapper.GetLookupTable()\n\n                    dsAttrs = None\n                    arrayLocation = ''\n\n                    if scalarVisibility:\n                        if scalarMode == 3 or scalarMode == 1:  # VTK_SCALAR_MODE_USE_POINT_FIELD_DATA or VTK_SCALAR_MODE_USE_POINT_DATA\n                            dsAttrs = dataset.GetPointData()\n                            arrayLocation = 'pointData'\n                        # VTK_SCALAR_MODE_USE_CELL_FIELD_DATA or VTK_SCALAR_MODE_USE_CELL_DATA\n                        elif scalarMode == 4 or scalarMode == 2:\n                            dsAttrs = dataset.GetCellData()\n                            arrayLocation = 'cellData'\n\n                    colorArray = None\n                    dataArray = None\n\n                    if dsAttrs:\n                        dataArray = dsAttrs.GetArray(0) # Force getting the active array\n\n                    if dataArray:\n                        # component = -1 => let specific instance get scalar from vector before mapping\n                        colorArray = lookupTable.MapScalars(\n                            dataArray, colorMode, -1)\n                        colorArrayName = '__CustomRGBColorArray__'\n                        colorArray.SetName(colorArrayName)\n                        colorMode = 0\n                    else:\n                        colorArrayName = ''\n\n                    color_array_info = {\n                        'colorArray': colorArray,\n                        'location': arrayLocation\n                    }\n\n                    scDirs.append(write_data_set('', dataset, output_dir, color_array_info,\n                                               new_name=componentName, compress=doCompressArrays))\n\n                    # Handle texture if any\n                    textureName = None\n                    if renProp.GetTexture() and renProp.GetTexture().GetInput():\n                        textureData = renProp.GetTexture().GetInput()\n                        textureName = 'texture_%d' % get_object_id(textureData)\n                        textureToSave[textureName] = textureData\n\n                    representation = renProp.GetProperty().GetRepresentation(\n                    ) if hasattr(renProp, 'GetProperty') else 2\n                    colorToUse = renProp.GetProperty().GetDiffuseColor(\n                    ) if hasattr(renProp, 'GetProperty') else [1, 1, 1]\n                    if representation == 1:\n                        colorToUse = renProp.GetProperty().GetColor() if hasattr(\n                            renProp, 'GetProperty') else [1, 1, 1]\n                    pointSize = renProp.GetProperty().GetPointSize(\n                    ) if hasattr(renProp, 'GetProperty') else 1.0\n                    opacity = renProp.GetProperty().GetOpacity() if hasattr(\n                        renProp, 'GetProperty') else 1.0\n                    edgeVisibility = renProp.GetProperty().GetEdgeVisibility(\n                    ) if hasattr(renProp, 'GetProperty') else false\n\n                    p3dPosition = renProp.GetPosition() if renProp.IsA(\n                        'vtkProp3D') else [0, 0, 0]\n                    p3dScale = renProp.GetScale() if renProp.IsA(\n                        'vtkProp3D') else [1, 1, 1]\n                    p3dOrigin = renProp.GetOrigin() if renProp.IsA(\n                        'vtkProp3D') else [0, 0, 0]\n                    p3dRotateWXYZ = renProp.GetOrientationWXYZ(\n                    ) if renProp.IsA('vtkProp3D') else [0, 0, 0, 0]\n\n                    sceneComponents.append({\n                        \"name\": componentName,\n                        \"type\": \"httpDataSetReader\",\n                        \"httpDataSetReader\": {\n                            \"url\": componentName\n                        },\n                        \"actor\": {\n                            \"origin\": p3dOrigin,\n                            \"scale\": p3dScale,\n                            \"position\": p3dPosition,\n                        },\n                        \"actorRotation\": p3dRotateWXYZ,\n                        \"mapper\": {\n                            \"colorByArrayName\": colorArrayName,\n                            \"colorMode\": colorMode,\n                            \"scalarMode\": scalarMode\n                        },\n                        \"property\": {\n                            \"representation\": representation,\n                            \"edgeVisibility\": edgeVisibility,\n                            \"diffuseColor\": colorToUse,\n                            \"pointSize\": pointSize,\n                            \"opacity\": opacity\n                        },\n                        \"lookupTable\": {\n                            \"tableRange\": lookupTable.GetRange(),\n                            \"hueRange\": lookupTable.GetHueRange() if hasattr(lookupTable, 'GetHueRange') else [0.5, 0]\n                        }\n                    })\n\n                    if textureName:\n                        sceneComponents[-1]['texture'] = textureName\n\n    # Save texture data if any\n    for key, val in textureToSave.items():\n        write_data_set('', val, output_dir, None, new_name=key,\n                     compress=doCompressArrays)\n\n    cameraClippingRange = plotter.camera.GetClippingRange()\n\n    sceneDescription = {\n      \"fetchGzip\": doCompressArrays,\n      \"background\": plotter.background_color,\n      \"camera\": {\n        \"focalPoint\": plotter.camera.GetFocalPoint(),\n        \"position\": plotter.camera.GetPosition(),\n        \"viewUp\": plotter.camera.GetViewUp(),\n        \"clippingRange\": [ elt for elt in cameraClippingRange ]\n      },\n      \"centerOfRotation\": plotter.camera.GetFocalPoint(),\n      \"scene\": sceneComponents\n    }\n\n    indexFilePath = os.path.join(output_dir, 'index.json')\n    with open(indexFilePath, 'w') as outfile:\n      json.dump(sceneDescription, outfile, indent=4)\n\n# -----------------------------------------------------------------------------\n\n    # Now zip up the results and get rid of the temp directory\n    sceneFileName = os.path.join(\n        root_output_directory, '%s%s' % (sceneName, FILENAME_EXTENSION))\n\n    try:\n        import zlib\n        compression = zipfile.ZIP_DEFLATED\n    except:\n        compression = zipfile.ZIP_STORED\n\n    zf = zipfile.ZipFile(sceneFileName, mode='w')\n\n    try:\n        for dirName, subdirList, fileList in os.walk(output_dir):\n            for fname in fileList:\n                fullPath = os.path.join(dirName, fname)\n                relPath = '%s/%s' % (sceneName,\n                                     os.path.relpath(fullPath, output_dir))\n                zf.write(fullPath, arcname=relPath, compress_type=compression)\n    finally:\n        zf.close()\n\n    shutil.rmtree(output_dir)\n\n    print('Finished exporting dataset to: ', sceneFileName)", "response": "Export a plotter s rendering window to VTKjs format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef axis_rotation(points, angle, inplace=False, deg=True, axis='z'):\n    axis = axis.lower()\n\n    # Copy original array to if not inplace\n    if not inplace:\n        points = points.copy()\n\n    # Convert angle to radians\n    if deg:\n        angle *= np.pi / 180\n\n    if axis == 'x':\n        y = points[:, 1] * np.cos(angle) - points[:, 2] * np.sin(angle)\n        z = points[:, 1] * np.sin(angle) + points[:, 2] * np.cos(angle)\n        points[:, 1] = y\n        points[:, 2] = z\n    elif axis == 'y':\n        x = points[:, 0] * np.cos(angle) + points[:, 2] * np.sin(angle)\n        z = - points[:, 0] * np.sin(angle) + points[:, 2] * np.cos(angle)\n        points[:, 0] = x\n        points[:, 2] = z\n    elif axis == 'z':\n        x = points[:, 0] * np.cos(angle) - points[:, 1] * np.sin(angle)\n        y = points[:, 0] * np.sin(angle) + points[:, 1] * np.cos(angle)\n        points[:, 0] = x\n        points[:, 1] = y\n    else:\n        raise Exception('invalid axis.  Must be either \"x\", \"y\", or \"z\"')\n\n    if not inplace:\n        return points", "response": "Rotates points angle in degrees about an axis"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the active scalar s field and name.", "response": "def active_scalar_info(self):\n        \"\"\"Return the active scalar's field and name: [field, name]\"\"\"\n        if not hasattr(self, '_active_scalar_info'):\n            self._active_scalar_info = [POINT_DATA_FIELD, None] # field and name\n        field, name = self._active_scalar_info\n\n        # rare error where scalar name isn't a valid scalar\n        if name not in self.point_arrays:\n            if name not in self.cell_arrays:\n                name = None\n\n        if name is None:\n            if self.n_scalars < 1:\n                return field, name\n            # find some array in the set field\n            parr = self.GetPointData().GetArrayName(0)\n            carr = self.GetCellData().GetArrayName(0)\n            if parr is not None:\n                self._active_scalar_info = [POINT_DATA_FIELD, parr]\n                self.GetPointData().SetActiveScalars(parr)\n            elif carr is not None:\n                self._active_scalar_info = [CELL_DATA_FIELD, carr]\n                self.GetCellData().SetActiveScalars(carr)\n        return self._active_scalar_info"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef active_vectors_info(self):\n        if not hasattr(self, '_active_vectors_info'):\n            self._active_vectors_info = [POINT_DATA_FIELD, None] # field and name\n        _, name = self._active_vectors_info\n\n        # rare error where scalar name isn't a valid scalar\n        if name not in self.point_arrays:\n            if name not in self.cell_arrays:\n                name = None\n\n        return self._active_vectors_info", "response": "Return the active scalar s field and name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef points(self):\n        vtk_data = self.GetPoints().GetData()\n        arr = vtk_to_numpy(vtk_data)\n        return vtki_ndarray(arr, vtk_data)", "response": "returns a pointer to the points as a numpy array"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef points(self, points):\n        if not isinstance(points, np.ndarray):\n            raise TypeError('Points must be a numpy array')\n        vtk_points = vtki.vtk_points(points, False)\n        self.SetPoints(vtk_points)\n        self.GetPoints().Modified()\n        self.Modified()", "response": "set points without copying"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a glyph representation of the active vector data as a set of arrows.", "response": "def arrows(self):\n        \"\"\"\n        Returns a glyph representation of the active vector data as\n        arrows.  Arrows will be located at the points of the mesh and\n        their size will be dependent on the length of the vector.\n        Their direction will be the \"direction\" of the vector\n\n        Returns\n        -------\n        arrows : vtki.PolyData\n            Active scalars represented as arrows.\n        \"\"\"\n        if self.active_vectors is None:\n            return\n\n        arrow = vtk.vtkArrowSource()\n        arrow.Update()\n\n        alg = vtk.vtkGlyph3D()\n        alg.SetSourceData(arrow.GetOutput())\n        alg.SetOrient(True)\n        alg.SetInputData(self)\n        alg.SetVectorModeToUseVector()\n        alg.SetScaleModeToScaleByVector()\n        alg.Update()\n        return vtki.wrap(alg.GetOutput())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef vectors(self, array):\n        if array.ndim != 2:\n            raise AssertionError('vector array must be a 2-dimensional array')\n        elif array.shape[1] != 3:\n            raise RuntimeError('vector array must be 3D')\n        elif array.shape[0] != self.n_points:\n            raise RuntimeError('Number of vectors be the same as the number of points')\n\n        self.point_arrays[DEFAULT_VECTOR_KEY] = array\n        self.active_vectors_name = DEFAULT_VECTOR_KEY", "response": "Sets the active vectors"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the array to use as the texture coordinates", "response": "def t_coords(self, t_coords):\n        \"\"\"Set the array to use as the texture coordinates\"\"\"\n        if not isinstance(t_coords, np.ndarray):\n            raise TypeError('Texture coordinates must be a numpy array')\n        if t_coords.ndim != 2:\n            raise AssertionError('Texture coordinates must be a 2-dimensional array')\n        if t_coords.shape[0] != self.n_points:\n            raise AssertionError('Number of texture coordinates ({}) must match number of points ({})'.format(t_coords.shape[0], self.n_points))\n        if t_coords.shape[1] != 2:\n            raise AssertionError('Texture coordinates must only have 2 components, not ({})'.format(t_coords.shape[1]))\n        if np.min(t_coords) < 0.0 or np.max(t_coords) > 1.0:\n            raise AssertionError('Texture coordinates must be within (0, 1) range.')\n        # convert the array\n        vtkarr = numpy_to_vtk(t_coords)\n        vtkarr.SetName('Texture Coordinates')\n        self.GetPointData().SetTCoords(vtkarr)\n        self.GetPointData().Modified()\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nactivate a texture and update the active texture coordinates.", "response": "def _activate_texture(mesh, name):\n        \"\"\"Grab a texture and update the active texture coordinates. This makes\n        sure to not destroy old texture coordinates\n\n        Parameters\n        ----------\n        name : str\n            The name of the texture and texture coordinates to activate\n\n        Return\n        ------\n        vtk.vtkTexture : The active texture\n        \"\"\"\n        if name == True or isinstance(name, int):\n            keys = list(mesh.textures.keys())\n            # Grab the first name availabe if True\n            idx = 0 if not isinstance(name, int) or name == True else name\n            if idx > len(keys):\n                idx = 0\n            try:\n                name = keys[idx]\n            except IndexError:\n                logging.warning('No textures associated with input mesh.')\n                return None\n        # Grab the texture object by name\n        try:\n            texture = mesh.textures[name]\n        except KeyError:\n            logging.warning('Texture ({}) not associated with this dataset'.format(name))\n            texture = None\n        else:\n            # Be sure to reset the tcoords if present\n            # Grab old coordinates\n            if name in mesh.scalar_names:\n                old_tcoord = mesh.GetPointData().GetTCoords()\n                mesh.GetPointData().SetTCoords(mesh.GetPointData().GetArray(name))\n                mesh.GetPointData().AddArray(old_tcoord)\n                mesh.Modified()\n        return texture"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the scalar by name and appropriately sets it as active", "response": "def set_active_scalar(self, name, preference='cell'):\n        \"\"\"Finds the scalar by name and appropriately sets it as active\"\"\"\n        _, field = get_scalar(self, name, preference=preference, info=True)\n        if field == POINT_DATA_FIELD:\n            self.GetPointData().SetActiveScalars(name)\n        elif field == CELL_DATA_FIELD:\n            self.GetCellData().SetActiveScalars(name)\n        else:\n            raise RuntimeError('Data field ({}) not useable'.format(field))\n        self._active_scalar_info = [field, name]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_active_vectors(self, name, preference='cell'):\n        _, field = get_scalar(self, name, preference=preference, info=True)\n        if field == POINT_DATA_FIELD:\n            self.GetPointData().SetActiveVectors(name)\n        elif field == CELL_DATA_FIELD:\n            self.GetCellData().SetActiveVectors(name)\n        else:\n            raise RuntimeError('Data field ({}) not useable'.format(field))\n        self._active_vectors_info = [field, name]", "response": "Finds the vectors by name and appropriately sets it as active"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rename_scalar(self, old_name, new_name, preference='cell'):\n        _, field = get_scalar(self, old_name, preference=preference, info=True)\n        if field == POINT_DATA_FIELD:\n            self.point_arrays[new_name] = self.point_arrays.pop(old_name)\n        elif field == CELL_DATA_FIELD:\n            self.cell_arrays[new_name] = self.cell_arrays.pop(old_name)\n        else:\n            raise RuntimeError('Array not found.')\n        if self.active_scalar_info[1] == old_name:\n            self.set_active_scalar(new_name, preference=field)", "response": "Changes the name of an array by searching for the array then renaming it"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the active scalar as an array", "response": "def active_scalar(self):\n        \"\"\"Returns the active scalar as an array\"\"\"\n        field, name = self.active_scalar_info\n        if name is None:\n            return None\n        if field == POINT_DATA_FIELD:\n            return self._point_scalar(name)\n        elif field == CELL_DATA_FIELD:\n            return self._cell_scalar(name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _point_scalar(self, name=None):\n        if name is None:\n            # use active scalar array\n            field, name = self.active_scalar_info\n            if field != POINT_DATA_FIELD:\n                raise RuntimeError('Must specify an array to fetch.')\n        vtkarr = self.GetPointData().GetArray(name)\n        if vtkarr is None:\n            raise AssertionError('({}) is not a point scalar'.format(name))\n\n        # numpy does not support bit array data types\n        if isinstance(vtkarr, vtk.vtkBitArray):\n            vtkarr = vtk_bit_array_to_char(vtkarr)\n            if name not in self._point_bool_array_names:\n                self._point_bool_array_names.append(name)\n\n        array = vtk_to_numpy(vtkarr)\n        if array.dtype == np.uint8 and name in self._point_bool_array_names:\n            array = array.view(np.bool)\n        return array", "response": "Returns a vtk array of point scalars."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding point scalars to the mesh.", "response": "def _add_point_scalar(self, scalars, name, set_active=False, deep=True):\n        \"\"\"\n        Adds point scalars to the mesh\n\n        Parameters\n        ----------\n        scalars : numpy.ndarray\n            Numpy array of scalars.  Must match number of points.\n\n        name : str\n            Name of point scalars to add.\n\n        set_active : bool, optional\n            Sets the scalars to the active plotting scalars.  Default False.\n\n        deep : bool, optional\n            Does not copy scalars when False.  A reference to the scalars\n            must be kept to avoid a segfault.\n\n        \"\"\"\n        if not isinstance(scalars, np.ndarray):\n            raise TypeError('Input must be a numpy.ndarray')\n\n        if scalars.shape[0] != self.n_points:\n            raise Exception('Number of scalars must match the number of ' +\n                            'points')\n\n        # need to track which arrays are boolean as all boolean arrays\n        # must be stored as uint8\n        if scalars.dtype == np.bool:\n            scalars = scalars.view(np.uint8)\n            if name not in self._point_bool_array_names:\n                self._point_bool_array_names.append(name)\n\n        if not scalars.flags.c_contiguous:\n            scalars = np.ascontiguousarray(scalars)\n\n        vtkarr = numpy_to_vtk(scalars, deep=deep)\n        vtkarr.SetName(name)\n        self.GetPointData().AddArray(vtkarr)\n        if set_active or self.active_scalar_info[1] is None:\n            self.GetPointData().SetActiveScalars(name)\n            self._active_scalar_info = [POINT_DATA_FIELD, name]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmake points double precision", "response": "def points_to_double(self):\n        \"\"\" Makes points double precision \"\"\"\n        if self.points.dtype != np.double:\n            self.points = self.points.astype(np.double)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rotate_x(self, angle):\n        axis_rotation(self.points, angle, inplace=True, axis='x')", "response": "Rotates the mesh about the x - axis."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rotate_y(self, angle):\n        axis_rotation(self.points, angle, inplace=True, axis='y')", "response": "Rotates the current object about the y - axis."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rotate_z(self, angle):\n        axis_rotation(self.points, angle, inplace=True, axis='z')", "response": "Rotates the object about the z - axis."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute a transformation in place using a 4x4 transformation matrix.", "response": "def transform(self, trans):\n        \"\"\"\n        Compute a transformation in place using a 4x4 transform.\n\n        Parameters\n        ----------\n        trans : vtk.vtkMatrix4x4, vtk.vtkTransform, or np.ndarray\n            Accepts a vtk transformation object or a 4x4 transformation matrix.\n\n        \"\"\"\n        if isinstance(trans, vtk.vtkMatrix4x4):\n            t = vtki.trans_from_matrix(trans)\n        elif isinstance(trans, vtk.vtkTransform):\n            t = vtki.trans_from_matrix(trans.GetMatrix())\n        elif isinstance(trans, np.ndarray):\n            if trans.shape[0] != 4 or trans.shape[1] != 4:\n                raise Exception('Transformation array must be 4x4')\n            t = trans\n        else:\n            raise TypeError('Input transform must be either:\\n'\n                            + '\\tvtk.vtkMatrix4x4\\n'\n                            + '\\tvtk.vtkTransform\\n'\n                            + '\\t4x4 np.ndarray\\n')\n\n        x = (self.points*t[0, :3]).sum(1) + t[0, -1]\n        y = (self.points*t[1, :3]).sum(1) + t[1, -1]\n        z = (self.points*t[2, :3]).sum(1) + t[2, -1]\n\n        # overwrite points\n        self.points[:, 0] = x\n        self.points[:, 1] = y\n        self.points[:, 2] = z"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _cell_scalar(self, name=None):\n        if name is None:\n            # use active scalar array\n            field, name = self.active_scalar_info\n            if field != CELL_DATA_FIELD:\n                raise RuntimeError('Must specify an array to fetch.')\n\n        vtkarr = self.GetCellData().GetArray(name)\n        if vtkarr is None:\n            raise AssertionError('({}) is not a cell scalar'.format(name))\n\n        # numpy does not support bit array data types\n        if isinstance(vtkarr, vtk.vtkBitArray):\n            vtkarr = vtk_bit_array_to_char(vtkarr)\n            if name not in self._cell_bool_array_names:\n                self._cell_bool_array_names.append(name)\n\n        array = vtk_to_numpy(vtkarr)\n        if array.dtype == np.uint8 and name in self._cell_bool_array_names:\n            array = array.view(np.bool)\n        return array", "response": "Returns the cell scalars of a vtk object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd cell scalars to the vtk object.", "response": "def _add_cell_scalar(self, scalars, name, set_active=False, deep=True):\n        \"\"\"\n        Adds cell scalars to the vtk object.\n\n        Parameters\n        ----------\n        scalars : numpy.ndarray\n            Numpy array of scalars.  Must match number of points.\n\n        name : str\n            Name of point scalars to add.\n\n        set_active : bool, optional\n            Sets the scalars to the active plotting scalars.  Default False.\n\n        deep : bool, optional\n            Does not copy scalars when False.  A reference to the scalars\n            must be kept to avoid a segfault.\n\n        \"\"\"\n        if not isinstance(scalars, np.ndarray):\n            raise TypeError('Input must be a numpy.ndarray')\n\n        if scalars.shape[0] != self.n_cells:\n            raise Exception('Number of scalars must match the number of cells (%d)'\n                            % self.n_cells)\n\n        if not scalars.flags.c_contiguous:\n            raise AssertionError('Array must be contigious')\n        if scalars.dtype == np.bool:\n            scalars = scalars.view(np.uint8)\n            self._cell_bool_array_names.append(name)\n\n        vtkarr = numpy_to_vtk(scalars, deep=deep)\n        vtkarr.SetName(name)\n        self.GetCellData().AddArray(vtkarr)\n        if set_active or self.active_scalar_info[1] is None:\n            self.GetCellData().SetActiveScalars(name)\n            self._active_scalar_info = [CELL_DATA_FIELD, name]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncopy vtki meta data onto this object from another object", "response": "def copy_meta_from(self, ido):\n        \"\"\"Copies vtki meta data onto this object from another object\"\"\"\n        self._active_scalar_info = ido.active_scalar_info\n        self._active_vectors_info = ido.active_vectors_info\n        if hasattr(ido, '_textures'):\n            self._textures = ido._textures"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a shallow copy of the object.", "response": "def copy(self, deep=True):\n        \"\"\"\n        Returns a copy of the object\n\n        Parameters\n        ----------\n        deep : bool, optional\n            When True makes a full copy of the object.\n\n        Returns\n        -------\n        newobject : same as input\n           Deep or shallow copy of the input.\n        \"\"\"\n        thistype = type(self)\n        newobject = thistype()\n        if deep:\n            newobject.DeepCopy(self)\n        else:\n            newobject.ShallowCopy(self)\n        newobject.copy_meta_from(self)\n        return newobject"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef point_arrays(self):\n        pdata = self.GetPointData()\n        narr = pdata.GetNumberOfArrays()\n\n        # Update data if necessary\n        if hasattr(self, '_point_arrays'):\n            keys = list(self._point_arrays.keys())\n            if narr == len(keys):\n                if keys:\n                    if self._point_arrays[keys[0]].size == self.n_points:\n                        return self._point_arrays\n                else:\n                    return self._point_arrays\n\n        # dictionary with callbacks\n        self._point_arrays = PointScalarsDict(self)\n\n        for i in range(narr):\n            name = pdata.GetArrayName(i)\n            self._point_arrays[name] = self._point_scalar(name)\n\n        self._point_arrays.enable_callback()\n        return self._point_arrays", "response": "Returns the all point arrays"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the all cell arrays", "response": "def cell_arrays(self):\n        \"\"\" Returns the all cell arrays \"\"\"\n        cdata = self.GetCellData()\n        narr = cdata.GetNumberOfArrays()\n\n        # Update data if necessary\n        if hasattr(self, '_cell_arrays'):\n            keys = list(self._cell_arrays.keys())\n            if narr == len(keys):\n                if keys:\n                    if self._cell_arrays[keys[0]].size == self.n_cells:\n                        return self._cell_arrays\n                else:\n                    return self._cell_arrays\n\n        # dictionary with callbacks\n        self._cell_arrays = CellScalarsDict(self)\n\n        for i in range(narr):\n            name = cdata.GetArrayName(i)\n            self._cell_arrays[name] = self._cell_scalar(name)\n\n        self._cell_arrays.enable_callback()\n        return self._cell_arrays"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef volume(self):\n        sizes = self.compute_cell_sizes(length=False, area=False, volume=True)\n        return np.sum(sizes.cell_arrays['Volume'])", "response": "Returns the total volume of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_data_range(self, arr=None, preference='cell'):\n        if arr is None:\n            # use active scalar array\n            _, arr = self.active_scalar_info\n        if isinstance(arr, str):\n            arr = get_scalar(self, arr, preference=preference)\n        # If array has no tuples return a NaN range\n        if arr is None or arr.size == 0:\n            return (np.nan, np.nan)\n        # Use the array range\n        return np.nanmin(arr), np.nanmax(arr)", "response": "Get the non - NaN min and max of a named scalar array."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsearches both point and cell data for a scalar", "response": "def get_scalar(self, name, preference='cell', info=False):\n        \"\"\" Searches both point and cell data for an array \"\"\"\n        return get_scalar(self, name, preference=preference, info=info)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the header stats of this dataset. If in IPython this will be formatted to HTML. Otherwise returns a console friendly string.", "response": "def head(self, display=True, html=None):\n        \"\"\"Return the header stats of this dataset. If in IPython, this will\n        be formatted to HTML. Otherwise returns a console friendly string\"\"\"\n        # Generate the output\n        if html:\n            fmt = \"\"\n            # HTML version\n            fmt += \"\\n\"\n            fmt += \"<table>\\n\"\n            fmt += \"<tr><th>{}</th><th>Information</th></tr>\\n\".format(type(self).__name__)\n            row = \"<tr><td>{}</td><td>{}</td></tr>\\n\"\n            # now make a call on the object to get its attributes as a list of len 2 tuples\n            for attr in self._get_attrs():\n                try:\n                    fmt += row.format(attr[0], attr[2].format(*attr[1]))\n                except:\n                    fmt += row.format(attr[0], attr[2].format(attr[1]))\n            fmt += row.format('N Scalars', self.n_scalars)\n            fmt += \"</table>\\n\"\n            fmt += \"\\n\"\n            if display:\n                from IPython.display import display, HTML\n                display(HTML(fmt))\n                return\n            return fmt\n        # Otherwise return a string that is Python console friendly\n        fmt = \"{} ({})\\n\".format(type(self).__name__, hex(id(self)))\n        # now make a call on the object to get its attributes as a list of len 2 tuples\n        row = \"  {}:\\t{}\\n\"\n        for attr in self._get_attrs():\n            try:\n                fmt += row.format(attr[0], attr[2].format(*attr[1]))\n            except:\n                fmt += row.format(attr[0], attr[2].format(attr[1]))\n        fmt += row.format('N Scalars', self.n_scalars)\n        return fmt"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _repr_html_(self):\n        fmt = \"\"\n        if self.n_scalars > 0:\n            fmt += \"<table>\"\n            fmt += \"<tr><th>Header</th><th>Data Arrays</th></tr>\"\n            fmt += \"<tr><td>\"\n        # Get the header info\n        fmt += self.head(display=False, html=True)\n        # Fill out scalar arrays\n        if self.n_scalars > 0:\n            fmt += \"</td><td>\"\n            fmt += \"\\n\"\n            fmt += \"<table>\\n\"\n            row = \"<tr><th>{}</th><th>{}</th><th>{}</th><th>{}</th><th>{}</th></tr>\\n\"\n            fmt += row.format(\"Name\", \"Field\", \"Type\", \"Min\", \"Max\")\n            row = \"<tr><td>{}</td><td>{}</td><td>{}</td><td>{:.3e}</td><td>{:.3e}</td></tr>\\n\"\n\n            def format_array(key, field):\n                \"\"\"internal helper to foramt array information for printing\"\"\"\n                arr = get_scalar(self, key)\n                dl, dh = self.get_data_range(key)\n                if key == self.active_scalar_info[1]:\n                    key = '<b>{}</b>'.format(key)\n                return row.format(key, field, arr.dtype, dl, dh)\n\n            for i in range(self.GetPointData().GetNumberOfArrays()):\n                key = self.GetPointData().GetArrayName(i)\n                fmt += format_array(key, field='Points')\n            for i in range(self.GetCellData().GetNumberOfArrays()):\n                key = self.GetCellData().GetArrayName(i)\n                fmt += format_array(key, field='Cells')\n            fmt += \"</table>\\n\"\n            fmt += \"\\n\"\n            fmt += \"</td></tr> </table>\"\n        return fmt", "response": "A pretty representation for Jupyter notebooks that includes header info scalar arrays and information about all scalar arrays."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef overwrite(self, mesh):\n        self.DeepCopy(mesh)\n        if is_vtki_obj(mesh):\n            self.copy_meta_from(mesh)", "response": "Overwrites this mesh inplace with the new mesh s geometries and data\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a new representation of this object as an unstructured grid.", "response": "def cast_to_unstructured_grid(self):\n        \"\"\"Get a new representation of this object as an\n        :class:`vtki.UnstructuredGrid`\n        \"\"\"\n        alg = vtk.vtkAppendFilter()\n        alg.AddInputData(self)\n        alg.Update()\n        return vtki.filters._get_output(alg)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pop(self, key):\n        arr = dict.pop(self, key).copy()\n        self.remover(key)\n        return arr", "response": "Get and remove an element by key name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate this dictionary with th key - value pairs from a given dictionary.", "response": "def update(self, data):\n        \"\"\"\n        Update this dictionary with th key-value pairs from a given\n        dictionary\n        \"\"\"\n        if not isinstance(data, dict):\n            raise TypeError('Data to update must be in a dictionary.')\n        for k, v in data.items():\n            arr = np.array(v)\n            try:\n                self[k] = arr\n            except TypeError:\n                logging.warning(\"Values under key ({}) not supported by VTK\".format(k))\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a SBCS model object representing the charset.", "response": "def convert_sbcs_model(old_model, alphabet):\n    \"\"\"Create a SingleByteCharSetModel object representing the charset.\"\"\"\n    # Setup tables necessary for computing transition frequencies for model\n    char_to_order = {i: order\n                     for i, order in enumerate(old_model['char_to_order_map'])}\n    pos_ratio = old_model['typical_positive_ratio']\n    keep_ascii_letters = old_model['keep_english_letter']\n\n    curr_model = SingleByteCharSetModel(charset_name=old_model['charset_name'],\n                                        language=old_model['language'],\n                                        char_to_order_map=char_to_order,\n                                        # language_model is filled in later\n                                        language_model=None,\n                                        typical_positive_ratio=pos_ratio,\n                                        keep_ascii_letters=keep_ascii_letters,\n                                        alphabet=alphabet)\n    return curr_model"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting old SingleByteCharSetModels for the given language into new SingleByteCharSetModels.", "response": "def convert_models_for_lang(language):\n    \"\"\"Convert old SingleByteCharSetModels for the given language\"\"\"\n    # Validate language\n    language = language.title()\n    lang_metadata = LANGUAGES.get(language)\n    if not lang_metadata:\n        raise ValueError('Unknown language: {}. If you are adding a model for a'\n                         ' new language, you must first update metadata/'\n                         'languages.py'.format(language))\n    lang_mod_name = 'lang{}model'.format(language.lower())\n    if not os.path.exists(os.path.join('chardet', lang_mod_name + '.py')):\n        print('Skipping {} because it does not have an old model.'\n              .format(language))\n        return\n    lang_mod = getattr(chardet, lang_mod_name)\n    print('\\n{}\\n----------------------------------------------------------------'\n          .format(language))\n    print('Keep ASCII Letters: {}'.format(lang_metadata.use_ascii))\n    print('Alphabet: {}'.format(lang_metadata.alphabet))\n\n    # Create char-to-order maps (aka char-to-rank dicts)\n    charset_models = {}\n    char_ranks = {}\n    order_to_chars = {}\n    for var_name in dir(lang_mod):\n        if not ('Model' in var_name and 'LangModel' not in var_name):\n            continue\n        old_model = getattr(lang_mod, var_name)\n        charset_name = old_model['charset_name']\n\n        print('Converting charset model for {}'.format(charset_name))\n        sys.stdout.flush()\n        charset_models[charset_name] = convert_sbcs_model(old_model,\n                                                          lang_metadata.alphabet)\n        # Since we don't know which charsets have which characters, we have to\n        # try to reconstruct char_ranks (for letters only, since that's all\n        # the old language models contain)\n        for byte_hex, order in iteritems(charset_models[charset_name].char_to_order_map):\n            # order 64 was basically ignored before because of the off by one\n            # error, but it's hard to know if training took that into account\n            if order > 64:\n                continue\n            # Convert to bytes in Python 2 and 3\n            char = bytes(bytearray((byte_hex,)))\n            try:\n                unicode_char = char.decode(charset_name)\n            except UnicodeDecodeError:\n                continue\n            if unicode_char not in char_ranks:\n                char_ranks[unicode_char] = order\n                order_to_chars[order] = unicode_char\n            elif char_ranks[unicode_char] != order:\n                raise ValueError('Unstable character ranking for {}'.format(unicode_char))\n\n    old_lang_model = getattr(lang_mod, '{}LangModel'.format(language.title()))\n    language_model = {}\n    # Preserve off-by-one error here by ignoring first column and row\n    for i in range(1, 64):\n        if i not in order_to_chars:\n            continue\n        lang_char = order_to_chars[i]\n        language_model[lang_char] = {}\n        for j in range(1, 64):\n            if j not in order_to_chars:\n                continue\n            lang_char2 = order_to_chars[j]\n            language_model[lang_char][lang_char2] = old_lang_model[(i * 64) + j]\n\n    # Write output files\n    print('Writing output file for {}\\n\\n'.format(language))\n    sys.stdout.flush()\n    with open('lang{}model.py'.format(language.lower()), 'w') as output_file:\n        upper_lang = language.upper()\n        # print header to set encoding\n        print('#!/usr/bin/env python\\n'\n              '# -*- coding: utf-8 -*-\\n\\n'\n              'from chardet.sbcharsetprober import SingleByteCharSetModel\\n\\n',\n              file=output_file)\n\n        lm_name = '{}_LANG_MODEL'.format(upper_lang)\n        print_language_model(lm_name, language_model, output_file, char_ranks)\n\n        print('# 255: Undefined characters that did not exist in training text\\n'\n              '# 254: Carriage/Return\\n'\n              '# 253: symbol (punctuation) that does not belong to word\\n'\n              '# 252: 0 - 9\\n'\n              '# 251: Control characters\\n\\n'\n              '# Character Mapping Table(s):',\n              file=output_file)\n        for charset_name, sbcs_model in iteritems(charset_models):\n            normal_name = normalize_name(charset_name)\n            char_to_order_name = ('{}_{}_CHAR_TO_ORDER'.format(normal_name,\n                                                               upper_lang))\n            print_char_to_order(char_to_order_name, sbcs_model.char_to_order_map,\n                                charset_name, output_file)\n\n            sbcs_model_name = '{}_{}_MODEL'.format(normal_name, upper_lang)\n            sbcs_model.char_to_order_map.clear()\n            sbcs_model_repr = (repr(sbcs_model)\n                               .replace('None', lm_name)\n                               .replace('{}', char_to_order_name)\n                               .replace(', ', (',\\n' +\n                                               ' ' * (len(sbcs_model_name) +\n                                                      26))))\n            print('{} = {}\\n'.format(sbcs_model_name, sbcs_model_repr),\n                  file=output_file)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_py_impl():\n    if hasattr(sys, 'pypy_version_info'):\n        pyimpl = 'PyPy'\n    elif sys.platform.startswith('java'):\n        pyimpl = 'Jython'\n    elif sys.platform == 'cli':\n        pyimpl = 'IronPython'\n    else:\n        pyimpl = 'CPython'\n    return pyimpl", "response": "Return what kind of Python this is"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef detect_all(byte_str):\n    if not isinstance(byte_str, bytearray):\n        if not isinstance(byte_str, bytes):\n            raise TypeError('Expected object of type bytes or bytearray, got: '\n                            '{0}'.format(type(byte_str)))\n        else:\n            byte_str = bytearray(byte_str)\n\n    detector = UniversalDetector()\n    detector.feed(byte_str)\n    detector.close()\n\n    if detector._input_state == InputState.HIGH_BYTE:\n        results = []\n        for prober in detector._charset_probers:\n            if prober.get_confidence() > detector.MINIMUM_THRESHOLD:\n                charset_name = prober.charset_name\n                lower_charset_name = prober.charset_name.lower()\n                # Use Windows encoding name instead of ISO-8859 if we saw any\n                # extra Windows-specific bytes\n                if lower_charset_name.startswith('iso-8859'):\n                    if detector._has_win_bytes:\n                        charset_name = detector.ISO_WIN_MAP.get(lower_charset_name,\n                                                            charset_name)\n                results.append({\n                    'encoding': charset_name,\n                    'confidence': prober.get_confidence()\n                })\n        if len(results) > 0:\n            return sorted(results, key=lambda result: -result['confidence'])\n\n    return [detector.result]", "response": "Detect all possible encodings of the given byte string."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsearches for Sentinel products and optionally download all the results and create a geojson file with the results.", "response": "def cli(user, password, geometry, start, end, uuid, name, download, sentinel, producttype,\n        instrument, cloud, footprints, path, query, url, order_by, limit):\n    \"\"\"Search for Sentinel products and, optionally, download all the results\n    and/or create a geojson file with the search result footprints.\n    Beyond your Copernicus Open Access Hub user and password, you must pass a geojson file\n    containing the geometry of the area you want to search for or the UUIDs of the products. If you\n    don't specify the start and end dates, it will search in the last 24 hours.\n    \"\"\"\n\n    _set_logger_handler()\n\n    if user is None or password is None:\n        try:\n            user, password = requests.utils.get_netrc_auth(url)\n        except TypeError:\n            pass\n\n    if user is None or password is None:\n        raise click.UsageError('Missing --user and --password. Please see docs '\n                               'for environment variables and .netrc support.')\n\n    api = SentinelAPI(user, password, url)\n\n    search_kwargs = {}\n    if sentinel and not (producttype or instrument):\n        search_kwargs[\"platformname\"] = \"Sentinel-\" + sentinel\n\n    if instrument and not producttype:\n        search_kwargs[\"instrumentshortname\"] = instrument\n\n    if producttype:\n        search_kwargs[\"producttype\"] = producttype\n\n    if cloud:\n        if sentinel not in ['2', '3']:\n            logger.error('Cloud cover is only supported for Sentinel 2 and 3.')\n            exit(1)\n        search_kwargs[\"cloudcoverpercentage\"] = (0, cloud)\n\n    if query is not None:\n        search_kwargs.update((x.split('=') for x in query))\n\n    if geometry is not None:\n        search_kwargs['area'] = geojson_to_wkt(read_geojson(geometry))\n\n    if uuid is not None:\n        uuid_list = [x.strip() for x in uuid]\n        products = {}\n        for productid in uuid_list:\n            try:\n                products[productid] = api.get_product_odata(productid)\n            except SentinelAPIError as e:\n                if 'Invalid key' in e.msg:\n                    logger.error('No product with ID \\'%s\\' exists on server', productid)\n                    exit(1)\n                else:\n                    raise\n    elif name is not None:\n        search_kwargs[\"identifier\"] = name[0] if len(name) == 1 else '(' + ' OR '.join(name) + ')'\n        products = api.query(order_by=order_by, limit=limit, **search_kwargs)\n    else:\n        start = start or \"19000101\"\n        end = end or \"NOW\"\n        products = api.query(date=(start, end),\n                             order_by=order_by, limit=limit, **search_kwargs)\n\n    if footprints is True:\n        footprints_geojson = api.to_geojson(products)\n        with open(os.path.join(path, \"search_footprints.geojson\"), \"w\") as outfile:\n            outfile.write(gj.dumps(footprints_geojson))\n\n    if download is True:\n        product_infos, triggered, failed_downloads = api.download_all(products, path)\n        if len(failed_downloads) > 0:\n            with open(os.path.join(path, \"corrupt_scenes.txt\"), \"w\") as outfile:\n                for failed_id in failed_downloads:\n                    outfile.write(\"%s : %s\\n\" % (failed_id, products[failed_id]['title']))\n    else:\n        for product_id, props in products.items():\n            if uuid is None:\n                logger.info('Product %s - %s', product_id, props['summary'])\n            else:  # querying uuids has no summary key\n                logger.info('Product %s - %s - %s MB', product_id, props['title'],\n                            round(int(props['size']) / (1024. * 1024.), 2))\n        if uuid is None:\n            logger.info('---')\n            logger.info('%s scenes found with a total size of %.2f GB',\n                        len(products), api.get_products_size(products))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef geojson_to_wkt(geojson_obj, feature_number=0, decimals=4):\n    if 'coordinates' in geojson_obj:\n        geometry = geojson_obj\n    elif 'geometry' in geojson_obj:\n        geometry = geojson_obj['geometry']\n    else:\n        geometry = geojson_obj['features'][feature_number]['geometry']\n\n    def ensure_2d(geometry):\n        if isinstance(geometry[0], (list, tuple)):\n            return list(map(ensure_2d, geometry))\n        else:\n            return geometry[:2]\n\n    def check_bounds(geometry):\n        if isinstance(geometry[0], (list, tuple)):\n            return list(map(check_bounds, geometry))\n        else:\n            if geometry[0] > 180 or geometry[0] < -180:\n                raise ValueError('Longitude is out of bounds, check your JSON format or data')\n            if geometry[1] > 90 or geometry[1] < -90:\n                raise ValueError('Latitude is out of bounds, check your JSON format or data')\n\n    # Discard z-coordinate, if it exists\n    geometry['coordinates'] = ensure_2d(geometry['coordinates'])\n    check_bounds(geometry['coordinates'])\n\n    wkt = geomet.wkt.dumps(geometry, decimals=decimals)\n    # Strip unnecessary spaces\n    wkt = re.sub(r'(?<!\\d) ', '', wkt)\n    return wkt", "response": "Convert a GeoJSON object to Well - Known Text."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef format_query_date(in_date):\n    if in_date is None:\n        return '*'\n    if isinstance(in_date, (datetime, date)):\n        return in_date.strftime('%Y-%m-%dT%H:%M:%SZ')\n    elif not isinstance(in_date, string_types):\n        raise ValueError('Expected a string or a datetime object. Received {}.'.format(in_date))\n\n    in_date = in_date.strip()\n    if in_date == '*':\n        # '*' can be used for one-sided range queries e.g. ingestiondate:[* TO NOW-1YEAR]\n        return in_date\n\n    # Reference: https://cwiki.apache.org/confluence/display/solr/Working+with+Dates\n\n    # ISO-8601 date or NOW\n    valid_date_pattern = r'^(?:\\d{4}-\\d\\d-\\d\\dT\\d\\d:\\d\\d:\\d\\d(?:\\.\\d+)?Z|NOW)'\n    # date arithmetic suffix is allowed\n    units = r'(?:YEAR|MONTH|DAY|HOUR|MINUTE|SECOND)'\n    valid_date_pattern += r'(?:[-+]\\d+{}S?)*'.format(units)\n    # dates can be rounded to a unit of time\n    # e.g. \"NOW/DAY\" for dates since 00:00 today\n    valid_date_pattern += r'(?:/{}S?)*$'.format(units)\n    in_date = in_date.strip()\n    if re.match(valid_date_pattern, in_date):\n        return in_date\n\n    try:\n        return datetime.strptime(in_date, '%Y%m%d').strftime('%Y-%m-%dT%H:%M:%SZ')\n    except ValueError:\n        raise ValueError('Unsupported date value {}'.format(in_date))", "response": "Formats a date string input as YYYY - MM - DDThh - mm - ssZ"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_scihub_response(response, test_json=True):\n    # Prevent requests from needing to guess the encoding\n    # SciHub appears to be using UTF-8 in all of their responses\n    response.encoding = 'utf-8'\n    try:\n        response.raise_for_status()\n        if test_json:\n            response.json()\n    except (requests.HTTPError, ValueError):\n        msg = \"Invalid API response.\"\n        try:\n            msg = response.headers['cause-message']\n        except:\n            try:\n                msg = response.json()['error']['message']['value']\n            except:\n                if not response.text.strip().startswith('{'):\n                    try:\n                        h = html2text.HTML2Text()\n                        h.ignore_images = True\n                        h.ignore_anchors = True\n                        msg = h.handle(response.text).strip()\n                    except:\n                        pass\n        api_error = SentinelAPIError(msg, response)\n        # Suppress \"During handling of the above exception...\" message\n        # See PEP 409\n        api_error.__cause__ = None\n        raise api_error", "response": "Check that the response from SciHub is valid JSON."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert the timestamp received from OData JSON API to a datetime object.", "response": "def _parse_odata_timestamp(in_date):\n    \"\"\"Convert the timestamp received from OData JSON API to a datetime object.\n    \"\"\"\n    timestamp = int(in_date.replace('/Date(', '').replace(')/', ''))\n    seconds = timestamp // 1000\n    ms = timestamp % 1000\n    return datetime.utcfromtimestamp(seconds) + timedelta(milliseconds=ms)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a query response to a dictionary.", "response": "def _parse_opensearch_response(products):\n    \"\"\"Convert a query response to a dictionary.\n\n    The resulting dictionary structure is {<product id>: {<property>: <value>}}.\n    The property values are converted to their respective Python types unless `parse_values`\n    is set to `False`.\n    \"\"\"\n\n    converters = {'date': _parse_iso_date, 'int': int, 'long': int, 'float': float, 'double': float}\n    # Keep the string type by default\n    default_converter = lambda x: x\n\n    output = OrderedDict()\n    for prod in products:\n        product_dict = {}\n        prod_id = prod['id']\n        output[prod_id] = product_dict\n        for key in prod:\n            if key == 'id':\n                continue\n            if isinstance(prod[key], string_types):\n                product_dict[key] = prod[key]\n            else:\n                properties = prod[key]\n                if isinstance(properties, dict):\n                    properties = [properties]\n                if key == 'link':\n                    for p in properties:\n                        name = 'link'\n                        if 'rel' in p:\n                            name = 'link_' + p['rel']\n                        product_dict[name] = p['href']\n                else:\n                    f = converters.get(key, default_converter)\n                    for p in properties:\n                        try:\n                            product_dict[p['name']] = f(p['content'])\n                        except KeyError:\n                            # Sentinel-3 has one element 'arr'\n                            # which violates the name:content convention\n                            product_dict[p['name']] = f(p['str'])\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef query(self, area=None, date=None, raw=None, area_relation='Intersects',\n              order_by=None, limit=None, offset=0, **keywords):\n        \"\"\"Query the OpenSearch API with the coordinates of an area, a date interval\n        and any other search keywords accepted by the API.\n\n        Parameters\n        ----------\n        area : str, optional\n            The area of interest formatted as a Well-Known Text string.\n        date : tuple of (str or datetime) or str, optional\n            A time interval filter based on the Sensing Start Time of the products.\n            Expects a tuple of (start, end), e.g. (\"NOW-1DAY\", \"NOW\").\n            The timestamps can be either a Python datetime or a string in one of the\n            following formats:\n\n                - yyyyMMdd\n                - yyyy-MM-ddThh:mm:ss.SSSZ (ISO-8601)\n                - yyyy-MM-ddThh:mm:ssZ\n                - NOW\n                - NOW-<n>DAY(S) (or HOUR(S), MONTH(S), etc.)\n                - NOW+<n>DAY(S)\n                - yyyy-MM-ddThh:mm:ssZ-<n>DAY(S)\n                - NOW/DAY (or HOUR, MONTH etc.) - rounds the value to the given unit\n\n            Alternatively, an already fully formatted string such as \"[NOW-1DAY TO NOW]\" can be\n            used as well.\n        raw : str, optional\n            Additional query text that will be appended to the query.\n        area_relation : {'Intersects', 'Contains', 'IsWithin'}, optional\n            What relation to use for testing the AOI. Case insensitive.\n\n                - Intersects: true if the AOI and the footprint intersect (default)\n                - Contains: true if the AOI is inside the footprint\n                - IsWithin: true if the footprint is inside the AOI\n\n        order_by: str, optional\n            A comma-separated list of fields to order by (on server side).\n            Prefix the field name by '+' or '-' to sort in ascending or descending order,\n            respectively. Ascending order is used if prefix is omitted.\n            Example: \"cloudcoverpercentage, -beginposition\".\n        limit: int, optional\n            Maximum number of products returned. Defaults to no limit.\n        offset: int, optional\n            The number of results to skip. Defaults to 0.\n        **keywords\n            Additional keywords can be used to specify other query parameters,\n            e.g. `relativeorbitnumber=70`.\n            See https://scihub.copernicus.eu/twiki/do/view/SciHubUserGuide/3FullTextSearch\n            for a full list.\n\n\n        Range values can be passed as two-element tuples, e.g. `cloudcoverpercentage=(0, 30)`.\n        `None` can be used in range values for one-sided ranges, e.g. `orbitnumber=(16302, None)`.\n        Ranges with no bounds (`orbitnumber=(None, None)`) will not be included in the query.\n\n        The time interval formats accepted by the `date` parameter can also be used with\n        any other parameters that expect time intervals (that is: 'beginposition', 'endposition',\n        'date', 'creationdate', and 'ingestiondate').\n\n        Returns\n        -------\n        dict[string, dict]\n            Products returned by the query as a dictionary with the product ID as the key and\n            the product's attributes (a dictionary) as the value.\n        \"\"\"\n        query = self.format_query(area, date, raw, area_relation, **keywords)\n\n        self.logger.debug(\"Running query: order_by=%s, limit=%s, offset=%s, query=%s\",\n                          order_by, limit, offset, query)\n        formatted_order_by = _format_order_by(order_by)\n        response, count = self._load_query(query, formatted_order_by, limit, offset)\n        self.logger.info(\"Found %s products\", count)\n        return _parse_opensearch_response(response)", "response": "Query the OpenSearch API with the coordinates of an area a date interval and optional keyword arguments accepted by the API."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a OpenSearch API query string.", "response": "def format_query(area=None, date=None, raw=None, area_relation='Intersects',\n                     **keywords):\n        \"\"\"Create a OpenSearch API query string.\n        \"\"\"\n        if area_relation.lower() not in {\"intersects\", \"contains\", \"iswithin\"}:\n            raise ValueError(\"Incorrect AOI relation provided ({})\".format(area_relation))\n\n        # Check for duplicate keywords\n        kw_lower = set(x.lower() for x in keywords)\n        if (len(kw_lower) != len(keywords) or\n                (date is not None and 'beginposition' in kw_lower) or\n                (area is not None and 'footprint' in kw_lower)):\n            raise ValueError(\"Query contains duplicate keywords. Note that query keywords are case-insensitive.\")\n\n        query_parts = []\n\n        if date is not None:\n            keywords['beginPosition'] = date\n\n        for attr, value in sorted(keywords.items()):\n            # Escape spaces, where appropriate\n            if isinstance(value, string_types):\n                value = value.strip()\n                if not any(value.startswith(s[0]) and value.endswith(s[1]) for s in ['[]', '{}', '//', '()']):\n                    value = re.sub(r'\\s', r'\\ ', value, re.M)\n\n            # Handle date keywords\n            # Keywords from https://github.com/SentinelDataHub/DataHubSystem/search?q=text/date+iso8601\n            date_attrs = ['beginposition', 'endposition', 'date', 'creationdate', 'ingestiondate']\n            if attr.lower() in date_attrs:\n                # Automatically format date-type attributes\n                if isinstance(value, string_types) and ' TO ' in value:\n                    # This is a string already formatted as a date interval,\n                    # e.g. '[NOW-1DAY TO NOW]'\n                    pass\n                elif not isinstance(value, string_types) and len(value) == 2:\n                    value = (format_query_date(value[0]), format_query_date(value[1]))\n                else:\n                    raise ValueError(\"Date-type query parameter '{}' expects a two-element tuple \"\n                                     \"of str or datetime objects. Received {}\".format(attr, value))\n\n            # Handle ranged values\n            if isinstance(value, (list, tuple)):\n                # Handle value ranges\n                if len(value) == 2:\n                    # Allow None to be used as a unlimited bound\n                    value = ['*' if x is None else x for x in value]\n                    if all(x == '*' for x in value):\n                        continue\n                    value = '[{} TO {}]'.format(*value)\n                else:\n                    raise ValueError(\"Invalid number of elements in list. Expected 2, received \"\n                                     \"{}\".format(len(value)))\n\n            query_parts.append('{}:{}'.format(attr, value))\n\n        if raw:\n            query_parts.append(raw)\n\n        if area is not None:\n            query_parts.append('footprint:\"{}({})\"'.format(area_relation, area))\n\n        return ' '.join(query_parts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef query_raw(self, query, order_by=None, limit=None, offset=0):\n        warnings.warn(\n            \"query_raw() has been merged with query(). use query(raw=...) instead.\",\n            PendingDeprecationWarning\n        )\n        return self.query(raw=query, order_by=order_by, limit=limit, offset=offset)", "response": "Query the OpenSearch API for the specified set of products."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the number of products matching a query.", "response": "def count(self, area=None, date=None, raw=None, area_relation='Intersects', **keywords):\n        \"\"\"Get the number of products matching a query.\n\n        Accepted parameters are identical to :meth:`SentinelAPI.query()`.\n\n        This is a significantly more efficient alternative to doing `len(api.query())`,\n        which can take minutes to run for queries matching thousands of products.\n\n        Returns\n        -------\n        int\n            The number of products matching a query.\n        \"\"\"\n        for kw in ['order_by', 'limit', 'offset']:\n            # Allow these function arguments to be included for compatibility with query(),\n            # but ignore them.\n            if kw in keywords:\n                del keywords[kw]\n        query = self.format_query(area, date, raw, area_relation, **keywords)\n        _, total_count = self._load_query(query, limit=0)\n        return total_count"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the products from a query response as a GeoJSON with the values in their appropriate Python types.", "response": "def to_geojson(products):\n        \"\"\"Return the products from a query response as a GeoJSON with the values in their\n        appropriate Python types.\n        \"\"\"\n        feature_list = []\n        for i, (product_id, props) in enumerate(products.items()):\n            props = props.copy()\n            props['id'] = product_id\n            poly = geomet.wkt.loads(props['footprint'])\n            del props['footprint']\n            del props['gmlfootprint']\n            # Fix \"'datetime' is not JSON serializable\"\n            for k, v in props.items():\n                if isinstance(v, (date, datetime)):\n                    props[k] = v.strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n            feature_list.append(\n                geojson.Feature(geometry=poly, id=i, properties=props)\n            )\n        return geojson.FeatureCollection(feature_list)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_dataframe(products):\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\"to_dataframe requires the optional dependency Pandas.\")\n\n        return pd.DataFrame.from_dict(products, orient='index')", "response": "Return the products from a query response as a Pandas DataFrame\n            with the values in their appropriate Python types."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_geodataframe(products):\n        try:\n            import geopandas as gpd\n            import shapely.wkt\n        except ImportError:\n            raise ImportError(\"to_geodataframe requires the optional dependencies GeoPandas and Shapely.\")\n\n        crs = {'init': 'epsg:4326'}  # WGS84\n        if len(products) == 0:\n            return gpd.GeoDataFrame(crs=crs)\n\n        df = SentinelAPI.to_dataframe(products)\n        geometry = [shapely.wkt.loads(fp) for fp in df['footprint']]\n        # remove useless columns\n        df.drop(['footprint', 'gmlfootprint'], axis=1, inplace=True)\n        return gpd.GeoDataFrame(df, crs=crs, geometry=geometry)", "response": "Return the products from a query response as a GeoPandas GeoDataFrame\n        with the values in their appropriate Python types."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\naccessing OData API to get info about a product.", "response": "def get_product_odata(self, id, full=False):\n        \"\"\"Access OData API to get info about a product.\n\n        Returns a dict containing the id, title, size, md5sum, date, footprint and download url\n        of the product. The date field corresponds to the Start ContentDate value.\n\n        If `full` is set to True, then the full, detailed metadata of the product is returned\n        in addition to the above.\n\n        Parameters\n        ----------\n        id : string\n            The UUID of the product to query\n        full : bool\n            Whether to get the full metadata for the Product. False by default.\n\n        Returns\n        -------\n        dict[str, Any]\n            A dictionary with an item for each metadata attribute\n\n        Notes\n        -----\n        For a full list of mappings between the OpenSearch (Solr) and OData attribute names\n        see the following definition files:\n        https://github.com/SentinelDataHub/DataHubSystem/blob/master/addon/sentinel-1/src/main/resources/META-INF/sentinel-1.owl\n        https://github.com/SentinelDataHub/DataHubSystem/blob/master/addon/sentinel-2/src/main/resources/META-INF/sentinel-2.owl\n        https://github.com/SentinelDataHub/DataHubSystem/blob/master/addon/sentinel-3/src/main/resources/META-INF/sentinel-3.owl\n        \"\"\"\n        url = urljoin(self.api_url, u\"odata/v1/Products('{}')?$format=json\".format(id))\n        if full:\n            url += '&$expand=Attributes'\n        response = self.session.get(url, auth=self.session.auth,\n                                    timeout=self.timeout)\n        _check_scihub_response(response)\n        values = _parse_odata_response(response.json()['d'])\n        return values"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _trigger_offline_retrieval(self, url):\n        with self.session.get(url, auth=self.session.auth, timeout=self.timeout) as r:\n            # check https://scihub.copernicus.eu/userguide/LongTermArchive#HTTP_Status_codes\n            if r.status_code == 202:\n                self.logger.info(\"Accepted for retrieval\")\n            elif r.status_code == 503:\n                self.logger.error(\"Request not accepted\")\n                raise SentinelAPILTAError('Request for retrieval from LTA not accepted', r)\n            elif r.status_code == 403:\n                self.logger.error(\"Requests exceed user quota\")\n                raise SentinelAPILTAError('Requests for retrieval from LTA exceed user quota', r)\n            elif r.status_code == 500:\n                # should not happen\n                self.logger.error(\"Trying to download an offline product\")\n                raise SentinelAPILTAError('Trying to download an offline product', r)\n            return r.status_code", "response": "Triggers the offline product retrieval of an offline product from the long term archive."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndownloads a product from the LTA and store it in the specified directory.", "response": "def download(self, id, directory_path='.', checksum=True):\n        \"\"\"Download a product.\n\n        Uses the filename on the server for the downloaded file, e.g.\n        \"S1A_EW_GRDH_1SDH_20141003T003840_20141003T003920_002658_002F54_4DD1.zip\".\n\n        Incomplete downloads are continued and complete files are skipped.\n\n        Parameters\n        ----------\n        id : string\n            UUID of the product, e.g. 'a8dd0cfd-613e-45ce-868c-d79177b916ed'\n        directory_path : string, optional\n            Where the file will be downloaded\n        checksum : bool, optional\n            If True, verify the downloaded file's integrity by checking its MD5 checksum.\n            Throws InvalidChecksumError if the checksum does not match.\n            Defaults to True.\n\n        Returns\n        -------\n        product_info : dict\n            Dictionary containing the product's info from get_product_info() as well as\n            the path on disk.\n\n        Raises\n        ------\n        InvalidChecksumError\n            If the MD5 checksum does not match the checksum on the server.\n        \"\"\"\n        product_info = self.get_product_odata(id)\n        path = join(directory_path, product_info['title'] + '.zip')\n        product_info['path'] = path\n        product_info['downloaded_bytes'] = 0\n\n        self.logger.info('Downloading %s to %s', id, path)\n\n        if exists(path):\n            # We assume that the product has been downloaded and is complete\n            return product_info\n\n        # An incomplete download triggers the retrieval from the LTA if the product is not online\n        if not product_info['Online']:\n            self.logger.warning(\n                'Product %s is not online. Triggering retrieval from long term archive.',\n                product_info['id'])\n            self._trigger_offline_retrieval(product_info['url'])\n            return product_info\n\n        # Use a temporary file for downloading\n        temp_path = path + '.incomplete'\n\n        skip_download = False\n        if exists(temp_path):\n            if getsize(temp_path) > product_info['size']:\n                self.logger.warning(\n                    \"Existing incomplete file %s is larger than the expected final size\"\n                    \" (%s vs %s bytes). Deleting it.\",\n                    str(temp_path), getsize(temp_path), product_info['size'])\n                remove(temp_path)\n            elif getsize(temp_path) == product_info['size']:\n                if self._md5_compare(temp_path, product_info['md5']):\n                    skip_download = True\n                else:\n                    # Log a warning since this should never happen\n                    self.logger.warning(\n                        \"Existing incomplete file %s appears to be fully downloaded but \"\n                        \"its checksum is incorrect. Deleting it.\",\n                        str(temp_path))\n                    remove(temp_path)\n            else:\n                # continue downloading\n                self.logger.info(\n                    \"Download will resume from existing incomplete file %s.\", temp_path)\n                pass\n\n        if not skip_download:\n            # Store the number of downloaded bytes for unit tests\n            product_info['downloaded_bytes'] = self._download(\n                product_info['url'], temp_path, self.session, product_info['size'])\n\n        # Check integrity with MD5 checksum\n        if checksum is True:\n            if not self._md5_compare(temp_path, product_info['md5']):\n                remove(temp_path)\n                raise InvalidChecksumError('File corrupt: checksums do not match')\n\n        # Download successful, rename the temporary file to its proper name\n        shutil.move(temp_path, path)\n        return product_info"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef download_all(self, products, directory_path='.', max_attempts=10, checksum=True):\n        product_ids = list(products)\n        self.logger.info(\"Will download %d products\", len(product_ids))\n        return_values = OrderedDict()\n        last_exception = None\n        for i, product_id in enumerate(products):\n            for attempt_num in range(max_attempts):\n                try:\n                    product_info = self.download(product_id, directory_path, checksum)\n                    return_values[product_id] = product_info\n                    break\n                except (KeyboardInterrupt, SystemExit):\n                    raise\n                except InvalidChecksumError as e:\n                    last_exception = e\n                    self.logger.warning(\n                        \"Invalid checksum. The downloaded file for '%s' is corrupted.\", product_id)\n                except SentinelAPILTAError as e:\n                    last_exception = e\n                    self.logger.exception(\"There was an error retrieving %s from the LTA\", product_id)\n                    break\n                except Exception as e:\n                    last_exception = e\n                    self.logger.exception(\"There was an error downloading %s\", product_id)\n            self.logger.info(\"%s/%s products downloaded\", i + 1, len(product_ids))\n        failed = set(products) - set(return_values)\n\n        # split up sucessfully processed products into downloaded and only triggered retrieval from the LTA\n        triggered = OrderedDict([(k, v) for k, v in return_values.items() if v['Online'] is False])\n        downloaded = OrderedDict([(k, v) for k, v in return_values.items() if v['Online'] is True])\n\n        if len(failed) == len(product_ids) and last_exception is not None:\n            raise last_exception\n        return downloaded, triggered, failed", "response": "Download a list of products and return a dictionary containing the product information for each product downloaded."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_products_size(products):\n        size_total = 0\n        for title, props in products.items():\n            size_product = props[\"size\"]\n            size_value = float(size_product.split(\" \")[0])\n            size_unit = str(size_product.split(\" \")[1])\n            if size_unit == \"MB\":\n                size_value /= 1024.\n            if size_unit == \"KB\":\n                size_value /= 1024. * 1024.\n            size_total += size_value\n        return round(size_total, 2)", "response": "Return the total file size in GB of all products in the OpenSearch response."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nquery the names of the products and return a dictionary of the products with the names in the order they appear.", "response": "def _query_names(self, names):\n        \"\"\"Find products by their names, e.g.\n        S1A_EW_GRDH_1SDH_20141003T003840_20141003T003920_002658_002F54_4DD1.\n\n        Note that duplicates exist on server, so multiple products can be returned for each name.\n\n        Parameters\n        ----------\n        names : list[string]\n            List of product names.\n\n        Returns\n        -------\n        dict[string, dict[str, dict]]\n            A dictionary mapping each name to a dictionary which contains the products with\n            that name (with ID as the key).\n        \"\"\"\n\n        def chunks(l, n):\n            \"\"\"Yield successive n-sized chunks from l.\"\"\"\n            for i in range(0, len(l), n):\n                yield l[i:i + n]\n\n        products = {}\n        # 40 names per query fits reasonably well inside the query limit\n        for chunk in chunks(names, 40):\n            query = \" OR \".join(chunk)\n            products.update(self.query(raw=query))\n\n        # Group the products\n        output = OrderedDict((name, dict()) for name in names)\n        for id, metadata in products.items():\n            name = metadata['identifier']\n            output[name][id] = metadata\n\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_files(self, paths=None, ids=None, directory=None, delete=False):\n        if not ids and not paths:\n            raise ValueError(\"Must provide either file paths or product IDs and a directory\")\n        if ids and not directory:\n            raise ValueError(\"Directory value missing\")\n        paths = paths or []\n        ids = ids or []\n\n        def name_from_path(path):\n            return splitext(basename(path))[0]\n\n        # Get product IDs corresponding to the files on disk\n        names = []\n        if paths:\n            names = list(map(name_from_path, paths))\n            result = self._query_names(names)\n            for product_dicts in result.values():\n                ids += list(product_dicts)\n        names_from_paths = set(names)\n        ids = set(ids)\n\n        # Collect the OData information for each product\n        # Product name -> list of matching odata dicts\n        product_infos = defaultdict(list)\n        for id in ids:\n            odata = self.get_product_odata(id)\n            name = odata['title']\n            product_infos[name].append(odata)\n\n            # Collect\n            if name not in names_from_paths:\n                paths.append(join(directory, name + '.zip'))\n\n        # Now go over the list of products and check them\n        corrupt = {}\n        for path in paths:\n            name = name_from_path(path)\n\n            if len(product_infos[name]) > 1:\n                self.logger.warning(\"{} matches multiple products on server\".format(path))\n\n            if not exists(path):\n                # We will consider missing files as corrupt also\n                self.logger.info(\"{} does not exist on disk\".format(path))\n                corrupt[path] = product_infos[name]\n                continue\n\n            is_fine = False\n            for product_info in product_infos[name]:\n                if (getsize(path) == product_info['size'] and\n                        self._md5_compare(path, product_info['md5'])):\n                    is_fine = True\n                    break\n            if not is_fine:\n                self.logger.info(\"{} is corrupt\".format(path))\n                corrupt[path] = product_infos[name]\n                if delete:\n                    remove(path)\n\n        return corrupt", "response": "Verify integrity of product files on disk."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomparing a given MD5 checksum with one calculated from a file.", "response": "def _md5_compare(self, file_path, checksum, block_size=2 ** 13):\n        \"\"\"Compare a given MD5 checksum with one calculated from a file.\"\"\"\n        with closing(self._tqdm(desc=\"MD5 checksumming\", total=getsize(file_path), unit=\"B\",\n                                unit_scale=True)) as progress:\n            md5 = hashlib.md5()\n            with open(file_path, \"rb\") as f:\n                while True:\n                    block_data = f.read(block_size)\n                    if not block_data:\n                        break\n                    md5.update(block_data)\n                    progress.update(len(block_data))\n            return md5.hexdigest().lower() == checksum.lower()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef transaction_retry(max_retries=1):\n    def _outer(fun):\n\n        @wraps(fun)\n        def _inner(*args, **kwargs):\n            _max_retries = kwargs.pop('exception_retry_count', max_retries)\n            for retries in count(0):\n                try:\n                    return fun(*args, **kwargs)\n                except Exception:   # pragma: no cover\n                    # Depending on the database backend used we can experience\n                    # various exceptions. E.g. psycopg2 raises an exception\n                    # if some operation breaks the transaction, so saving\n                    # the task result won't be possible until we rollback\n                    # the transaction.\n                    if retries >= _max_retries:\n                        raise\n                    try:\n                        rollback_unless_managed()\n                    except Exception:\n                        pass\n        return _inner\n\n    return _outer", "response": "Decorator for methods doing database operations."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_expired(self, expires):\n        meta = self.model._meta\n        with commit_on_success():\n            self.get_all_expired(expires).update(hidden=True)\n            cursor = self.connection_for_write().cursor()\n            cursor.execute(\n                'DELETE FROM {0.db_table} WHERE hidden=%s'.format(meta),\n                (True, ),\n            )", "response": "Delete all expired taskset results."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget task meta for task by task_id.", "response": "def get_task(self, task_id):\n        \"\"\"Get task meta for task by ``task_id``.\n\n        :keyword exception_retry_count: How many times to retry by\n            transaction rollback on exception. This could theoretically\n            happen in a race condition if another worker is trying to\n            create the same task. The default is to retry once.\n\n        \"\"\"\n        try:\n            return self.get(task_id=task_id)\n        except self.model.DoesNotExist:\n            if self._last_id == task_id:\n                self.warn_if_repeatable_read()\n            self._last_id = task_id\n            return self.model(task_id=task_id)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef store_result(self, task_id, result, status,\n                     traceback=None, children=None):\n        \"\"\"Store the result and status of a task.\n\n        :param task_id: task id\n\n        :param result: The return value of the task, or an exception\n            instance raised by the task.\n\n        :param status: Task status. See\n            :meth:`celery.result.AsyncResult.get_status` for a list of\n            possible status values.\n\n        :keyword traceback: The traceback at the point of exception (if the\n            task failed).\n\n        :keyword children: List of serialized results of subtasks\n            of this task.\n\n        :keyword exception_retry_count: How many times to retry by\n            transaction rollback on exception. This could theoretically\n            happen in a race condition if another worker is trying to\n            create the same task. The default is to retry twice.\n\n        \"\"\"\n        return self.update_or_create(task_id=task_id,\n                                     defaults={'status': status,\n                                               'result': result,\n                                               'traceback': traceback,\n                                               'meta': {'children': children}})", "response": "Store the result and status of a task."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the async result instance by taskset id.", "response": "def restore_taskset(self, taskset_id):\n        \"\"\"Get the async result instance by taskset id.\"\"\"\n        try:\n            return self.get(taskset_id=taskset_id)\n        except self.model.DoesNotExist:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_taskset(self, taskset_id):\n        s = self.restore_taskset(taskset_id)\n        if s:\n            s.delete()", "response": "Delete a saved taskset result."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef store_result(self, taskset_id, result):\n        return self.update_or_create(taskset_id=taskset_id,\n                                     defaults={'result': result})", "response": "Store the result instance of a taskset."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstores return value and status of an executed task.", "response": "def _store_result(self, task_id, result, status,\n                      traceback=None, request=None):\n        \"\"\"Store return value and status of an executed task.\"\"\"\n        self.TaskModel._default_manager.store_result(\n            task_id, result, status,\n            traceback=traceback, children=self.current_task_children(request),\n        )\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _save_group(self, group_id, result):\n        self.TaskSetModel._default_manager.store_result(group_id, result)\n        return result", "response": "Store the result of an executed group."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _restore_group(self, group_id):\n        meta = self.TaskSetModel._default_manager.restore_taskset(group_id)\n        if meta:\n            return meta.to_dict()", "response": "Get group metadata for a group by id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhandling a task event.", "response": "def handle_task(self, uuid_task, worker=None):\n        \"\"\"Handle snapshotted event.\"\"\"\n        uuid, task = uuid_task\n        if task.worker and task.worker.hostname:\n            worker = self.handle_worker(\n                (task.worker.hostname, task.worker),\n            )\n\n        defaults = {\n            'name': task.name,\n            'args': task.args,\n            'kwargs': task.kwargs,\n            'eta': correct_awareness(maybe_iso8601(task.eta)),\n            'expires': correct_awareness(maybe_iso8601(task.expires)),\n            'state': task.state,\n            'tstamp': fromtimestamp(task.timestamp),\n            'result': task.result or task.exception,\n            'traceback': task.traceback,\n            'runtime': task.runtime,\n            'worker': worker\n        }\n        # Some fields are only stored in the RECEIVED event,\n        # so we should remove these from default values,\n        # so that they are not overwritten by subsequent states.\n        [defaults.pop(attr, None) for attr in NOT_SAVED_ATTRIBUTES\n         if defaults[attr] is None]\n        return self.update_task(task.state,\n                                task_id=uuid, defaults=defaults)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle(self, addrport='', *args, **options):\n        server = WebserverThread(addrport, *args, **options)\n        server.start()\n        options['camera'] = 'djcelery.snapshot.Camera'\n        options['prog_name'] = 'djcelerymon'\n        ev.run(*args, **options)", "response": "Handle the management command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef respect_language(language):\n    if language:\n        prev = translation.get_language()\n        translation.activate(language)\n        try:\n            yield\n        finally:\n            translation.activate(prev)\n    else:\n        yield", "response": "Context manager that changes the current translation language for all code inside the current block."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef autodiscover():\n    global _RACE_PROTECTION\n\n    if _RACE_PROTECTION:\n        return\n    _RACE_PROTECTION = True\n    try:\n        return filter(None, [find_related_module(app, 'tasks')\n                             for app in settings.INSTALLED_APPS])\n    finally:\n        _RACE_PROTECTION = False", "response": "Include tasks for all applications in INSTALLED_APPS."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_related_module(app, related_name):\n\n    try:\n        app_path = importlib.import_module(app).__path__\n    except ImportError as exc:\n        warn('Autodiscover: Error importing %s.%s: %r' % (\n            app, related_name, exc,\n        ))\n        return\n    except AttributeError:\n        return\n\n    try:\n        f, _, _ = imp.find_module(related_name, app_path)\n        # f is returned None when app_path is a module\n        f and f.close()\n    except ImportError:\n        return\n\n    return importlib.import_module('{0}.{1}'.format(app, related_name))", "response": "Given an application name and a module name tries to find that\n    module in the application."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_configuration(self):\n        self.configured = True\n        # Default backend needs to be the database backend for backward\n        # compatibility.\n        backend = (getattr(settings, 'CELERY_RESULT_BACKEND', None) or\n                   getattr(settings, 'CELERY_BACKEND', None))\n        if not backend:\n            settings.CELERY_RESULT_BACKEND = 'database'\n        return DictAttribute(settings)", "response": "Load configuration from Django settings."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef on_task_init(self, task_id, task):\n        try:\n            is_eager = task.request.is_eager\n        except AttributeError:\n            is_eager = False\n        if not is_eager:\n            self.close_database()", "response": "Called before every task."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntaking from Django project (django/utils/encoding.py) & modified a bit to always have __unicode__ method available.", "response": "def python_2_unicode_compatible(cls):\n    \"\"\"Taken from Django project (django/utils/encoding.py) & modified a bit to\n    always have __unicode__ method available.\n    \"\"\"\n    if '__str__' not in cls.__dict__:\n        raise ValueError(\"@python_2_unicode_compatible cannot be applied \"\n                         \"to %s because it doesn't define __str__().\" %\n                         cls.__name__)\n\n    cls.__unicode__ = cls.__str__\n\n    if PY2:\n        cls.__str__ = lambda self: self.__unicode__().encode('utf-8')\n\n    return cls"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles the management command.", "response": "def handle(self, *args, **options):\n        \"\"\"Handle the management command.\"\"\"\n        if mon is None:\n            sys.stderr.write(MISSING)\n        else:\n            mon.run(**options)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a datetime into a human natural date string.", "response": "def naturaldate(date, include_seconds=False):\n    \"\"\"Convert datetime into a human natural date string.\"\"\"\n\n    if not date:\n        return ''\n\n    right_now = now()\n    today = datetime(right_now.year, right_now.month,\n                     right_now.day, tzinfo=right_now.tzinfo)\n    delta = right_now - date\n    delta_midnight = today - date\n\n    days = delta.days\n    hours = delta.seconds // 3600\n    minutes = delta.seconds // 60\n    seconds = delta.seconds\n\n    if days < 0:\n        return _('just now')\n\n    if days == 0:\n        if hours == 0:\n            if minutes > 0:\n                return ungettext(\n                    _('{minutes} minute ago'),\n                    _('{minutes} minutes ago'), minutes\n                ).format(minutes=minutes)\n            else:\n                if include_seconds and seconds:\n                    return ungettext(\n                        _('{seconds} second ago'),\n                        _('{seconds} seconds ago'), seconds\n                    ).format(seconds=seconds)\n                return _('just now')\n        else:\n            return ungettext(\n                _('{hours} hour ago'), _('{hours} hours ago'), hours\n            ).format(hours=hours)\n\n    if delta_midnight.days == 0:\n        return _('yesterday at {time}').format(time=date.strftime('%H:%M'))\n\n    count = 0\n    for chunk, pluralizefun in OLDER_CHUNKS:\n        if days >= chunk:\n            count = int(round((delta_midnight.days + 1) / chunk, 0))\n            fmt = pluralizefun(count)\n            return fmt.format(num=count)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef task_view(task):\n\n    def _applier(request, **options):\n        kwargs = request.POST if request.method == 'POST' else request.GET\n        # no multivalue\n        kwargs = {k: v for k, v in items(kwargs)}\n        if options:\n            kwargs.update(options)\n        result = task.apply_async(kwargs=kwargs)\n        return JsonResponse({'ok': 'true', 'task_id': result.task_id})\n\n    return _applier", "response": "Decorator that returns any task into a view that applies the task\n    asynchronously. Keyword arguments via URLconf etc. will\n    supercede GET or POST parameters when there are conflicts. Will return a JSON dictionary containing the keys ok and the task_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nviews applying a task.", "response": "def apply(request, task_name):\n    \"\"\"View applying a task.\n\n    **Note:** Please use this with caution. Preferably you shouldn't make this\n        publicly accessible without ensuring your code is safe!\n\n    \"\"\"\n    try:\n        task = tasks[task_name]\n    except KeyError:\n        raise Http404('apply: no such task')\n    return task_view(task)(request)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns task status and result in JSON format.", "response": "def task_status(request, task_id):\n    \"\"\"Returns task status and result in JSON format.\"\"\"\n    result = AsyncResult(task_id)\n    state, retval = result.state, result.result\n    response_data = {'id': task_id, 'status': state, 'result': retval}\n    if state in states.EXCEPTION_STATES:\n        traceback = result.traceback\n        response_data.update({'result': safe_repr(retval),\n                              'exc': get_full_cls_name(retval.__class__),\n                              'traceback': traceback})\n    return JsonResponse({'task': response_data})"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef task_webhook(fun):\n\n    @wraps(fun)\n    def _inner(*args, **kwargs):\n        try:\n            retval = fun(*args, **kwargs)\n        except Exception as exc:\n            response = {'status': 'failure', 'reason': safe_repr(exc)}\n        else:\n            response = {'status': 'success', 'retval': retval}\n\n        return JsonResponse(response)\n\n    return _inner", "response": "Decorator that returns a task webhook."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mk_tab_context_menu(callback_object):\n    # Store the menu in a temp variable in terminal so that popup() is happy. See:\n    #   https://stackoverflow.com/questions/28465956/\n    callback_object.context_menu = Gtk.Menu()\n    menu = callback_object.context_menu\n    mi1 = Gtk.MenuItem(_(\"New Tab\"))\n    mi1.connect(\"activate\", callback_object.on_new_tab)\n    menu.add(mi1)\n    mi2 = Gtk.MenuItem(_(\"Rename\"))\n    mi2.connect(\"activate\", callback_object.on_rename)\n    menu.add(mi2)\n    mi3 = Gtk.MenuItem(_(\"Close\"))\n    mi3.connect(\"activate\", callback_object.on_close)\n    menu.add(mi3)\n    menu.show_all()\n    return menu", "response": "Create the context menu for a notebook tab"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating the context menu for the notebook", "response": "def mk_notebook_context_menu(callback_object):\n    \"\"\"Create the context menu for the notebook\n    \"\"\"\n    callback_object.context_menu = Gtk.Menu()\n    menu = callback_object.context_menu\n    mi = Gtk.MenuItem(_(\"New Tab\"))\n    mi.connect(\"activate\", callback_object.on_new_tab)\n    menu.add(mi)\n    menu.add(Gtk.SeparatorMenuItem())\n    mi = Gtk.MenuItem(_(\"Save Tabs\"))\n    mi.connect(\"activate\", callback_object.on_save_tabs)\n    menu.add(mi)\n    mi = Gtk.MenuItem(_(\"Restore Tabs\"))\n    mi.connect(\"activate\", callback_object.on_restore_tabs_with_dialog)\n    menu.add(mi)\n    menu.add(Gtk.SeparatorMenuItem())\n    mi = Gtk.ImageMenuItem(\"gtk-preferences\")\n    mi.set_use_stock(True)\n    mi.connect(\"activate\", callback_object.on_show_preferences)\n    menu.add(mi)\n    mi = Gtk.ImageMenuItem(\"gtk-about\")\n    mi.set_use_stock(True)\n    mi.connect(\"activate\", callback_object.on_show_about)\n    menu.add(mi)\n    menu.add(Gtk.SeparatorMenuItem())\n    mi = Gtk.MenuItem(_(\"Quit\"))\n    mi.connect(\"activate\", callback_object.on_quit)\n    menu.add(mi)\n    menu.show_all()\n    return menu"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mk_terminal_context_menu(terminal, window, settings, callback_object):\n    # Store the menu in a temp variable in terminal so that popup() is happy. See:\n    #   https://stackoverflow.com/questions/28465956/\n    terminal.context_menu = Gtk.Menu()\n    menu = terminal.context_menu\n    mi = Gtk.MenuItem(_(\"Copy\"))\n    mi.connect(\"activate\", callback_object.on_copy_clipboard)\n    menu.add(mi)\n    if get_link_under_cursor(terminal) is not None:\n        mi = Gtk.MenuItem(_(\"Copy Url\"))\n        mi.connect(\"activate\", callback_object.on_copy_url_clipboard)\n        menu.add(mi)\n    mi = Gtk.MenuItem(_(\"Paste\"))\n    mi.connect(\"activate\", callback_object.on_paste_clipboard)\n    # check if clipboard has text, if not disable the paste menuitem\n    clipboard = Gtk.Clipboard.get_default(window.get_display())\n    mi.set_sensitive(clipboard.wait_is_text_available())\n    menu.add(mi)\n    menu.add(Gtk.SeparatorMenuItem())\n    mi = Gtk.MenuItem(_(\"Toggle Fullscreen\"))\n    mi.connect(\"activate\", callback_object.on_toggle_fullscreen)\n    menu.add(mi)\n    menu.add(Gtk.SeparatorMenuItem())\n    mi = Gtk.MenuItem(_(\"Split \u2015\"))\n    mi.connect(\"activate\", callback_object.on_split_horizontal)\n    menu.add(mi)\n    mi = Gtk.MenuItem(_(\"Split \uff5c\"))\n    mi.connect(\"activate\", callback_object.on_split_vertical)\n    menu.add(mi)\n    mi = Gtk.MenuItem(_(\"Close terminal\"))\n    mi.connect(\"activate\", callback_object.on_close_terminal)\n    menu.add(mi)\n    menu.add(Gtk.SeparatorMenuItem())\n    mi = Gtk.MenuItem(_(\"Save content...\"))\n    mi.connect(\"activate\", callback_object.on_save_to_file)\n    menu.add(mi)\n    mi = Gtk.MenuItem(_(\"Reset terminal\"))\n    mi.connect(\"activate\", callback_object.on_reset_terminal)\n    menu.add(mi)\n    # TODO SEARCH uncomment menu.add()\n    mi = Gtk.MenuItem(_(\"Find...\"))\n    mi.connect(\"activate\", callback_object.on_find)\n    # menu.add(mi)\n    menu.add(Gtk.SeparatorMenuItem())\n    mi = Gtk.MenuItem(_(\"Open link...\"))\n    mi.connect(\"activate\", callback_object.on_open_link)\n    link = get_link_under_cursor(terminal)\n    # TODO CONTEXTMENU this is a mess Quick open should also be sensible\n    # if the text in the selection is a url the current terminal\n    # implementation does not support this at the moment\n    if link:\n        if len(link) >= FILE_SELECTION_LENGTH:\n            mi.set_label(_(\"Open Link: {!s}...\").format(link[:FILE_SELECTION_LENGTH - 3]))\n        else:\n            mi.set_label(_(\"Open Link: {!s}\").format(link))\n        mi.set_sensitive(True)\n    else:\n        mi.set_sensitive(False)\n    menu.add(mi)\n    mi = Gtk.MenuItem(_(\"Search on Web\"))\n    mi.connect(\"activate\", callback_object.on_search_on_web)\n    selection = get_current_selection(terminal, window)\n    if selection:\n        search_text = selection.rstrip()\n        if len(search_text) > SEARCH_SELECTION_LENGTH:\n            search_text = search_text[:SEARCH_SELECTION_LENGTH - 3] + \"...\"\n        mi.set_label(_(\"Search on Web: '%s'\") % search_text)\n        mi.set_sensitive(True)\n    else:\n        mi.set_sensitive(False)\n    menu.add(mi)\n    mi = Gtk.MenuItem(_(\"Quick Open...\"))\n    mi.connect(\"activate\", callback_object.on_quick_open)\n    if selection:\n        filename = get_filename_under_cursor(terminal, selection)\n        if filename:\n            filename_str = str(filename)\n            if len(filename_str) > FILE_SELECTION_LENGTH:\n                mi.set_label(\n                    _(\"Quick Open: {!s}...\").format(filename_str[:FILE_SELECTION_LENGTH - 3])\n                )\n            else:\n                mi.set_label(_(\"Quick Open: {!s}\").format(filename_str))\n            mi.set_sensitive(True)\n        else:\n            mi.set_sensitive(False)\n    else:\n        mi.set_sensitive(False)\n    menu.add(mi)\n    customcommands = CustomCommands(settings, callback_object)\n    if customcommands.should_load():\n        submen = customcommands.build_menu()\n        if submen:\n            menu.add(Gtk.SeparatorMenuItem())\n            mi = Gtk.MenuItem(_(\"Custom Commands\"))\n            mi.set_submenu(submen)\n            menu.add(mi)\n    menu.add(Gtk.SeparatorMenuItem())\n    mi = Gtk.ImageMenuItem(\"gtk-preferences\")\n    mi.set_use_stock(True)\n    mi.connect(\"activate\", callback_object.on_show_preferences)\n    menu.add(mi)\n    mi = Gtk.ImageMenuItem(\"gtk-about\")\n    mi.set_use_stock(True)\n    mi.connect(\"activate\", callback_object.on_show_about)\n    menu.add(mi)\n    menu.add(Gtk.SeparatorMenuItem())\n    mi = Gtk.ImageMenuItem(_(\"Quit\"))\n    mi.connect(\"activate\", callback_object.on_quit)\n    menu.add(mi)\n    menu.show_all()\n    return menu", "response": "Create a context menu for a terminal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling when the gconf var window_ontop be changed", "response": "def ontop_toggled(self, settings, key, user_data):\n        \"\"\"If the gconf var window_ontop be changed, this method will\n        be called and will set the keep_above attribute in guake's\n        main window.\n        \"\"\"\n        self.guake.window.set_keep_above(settings.get_boolean(key))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tabbar_toggled(self, settings, key, user_data):\n        if settings.get_boolean(key):\n            for n in self.guake.notebook_manager.iter_notebooks():\n                n.set_property(\"show-tabs\", True)\n        else:\n            for n in self.guake.notebook_manager.iter_notebooks():\n                n.set_property(\"show-tabs\", False)", "response": "This method is called when the gconf var use_tabbar is changed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef alignment_changed(self, settings, key, user_data):\n        RectCalculator.set_final_window_rect(self.settings, self.guake.window)\n        self.guake.set_tab_position()\n        self.guake.force_move_if_shown()", "response": "Called when the gconf var window_halignment be changed"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall when the window height or width of the window has changed", "response": "def size_changed(self, settings, key, user_data):\n        \"\"\"If the gconf var window_height or window_width are changed,\n        this method will be called and will call the resize function\n        in guake.\n        \"\"\"\n        RectCalculator.set_final_window_rect(self.settings, self.guake.window)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cursor_blink_mode_changed(self, settings, key, user_data):\n        for term in self.guake.notebook_manager.iter_terminals():\n            term.set_property(\"cursor-blink-mode\", settings.get_int(key))", "response": "Called when cursor blink mode settings has been changed"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef history_size_changed(self, settings, key, user_data):\n        lines = settings.get_int(key)\n        for i in self.guake.notebook_manager.iter_terminals():\n            i.set_scrollback_lines(lines)", "response": "Called when the gconf var history_size be changed in all terminals."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls when the gconf var scroll_keystroke be changed in all terminals.", "response": "def keystroke_toggled(self, settings, key, user_data):\n        \"\"\"If the gconf var scroll_keystroke be changed, this method\n        will be called and will set the scroll_on_keystroke in all\n        terminals open.\n        \"\"\"\n        for i in self.guake.notebook_manager.iter_terminals():\n            i.set_scroll_on_keystroke(settings.get_boolean(key))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef default_font_toggled(self, settings, key, user_data):\n        font_name = None\n        if settings.get_boolean(key):\n            gio_settings = Gio.Settings('org.gnome.desktop.interface')\n            font_name = gio_settings.get_string('monospace-font-name')\n        else:\n            font_name = self.settings.styleFont.get_string('style')\n        if not font_name:\n            log.error(\"Error: unable to find font name (%s)\", font_name)\n            return\n        font = Pango.FontDescription(font_name)\n        if not font:\n            log.error(\"Error: unable to load font (%s)\", font_name)\n            return\n        for i in self.guake.notebook_manager.iter_terminals():\n            i.set_font(font)", "response": "This method is called when the gconf var use_default_font is changed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bold_is_bright_toggled(self, settings, key, user_data):\n        try:\n            for term in self.guake.notebook_manager.iter_terminals():\n                term.set_bold_is_bright(settings.get_boolean(key))\n        except:  # pylint: disable=bare-except\n            log.error(\"set_bold_is_bright not supported by your version of VTE\")", "response": "This method is called when the dconf var bold_is_bright is changed in the VTE terminal. It will be called when the VTE terminal is auto - brightened bold text."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fstyle_changed(self, settings, key, user_data):\n        font = Pango.FontDescription(settings.get_string(key))\n        for i in self.guake.notebook_manager.iter_terminals():\n            i.set_font(font)", "response": "Called when gconf var font style or font style is changed in all terminals."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef backspace_changed(self, settings, key, user_data):\n        for i in self.guake.notebook_manager.iter_terminals():\n            i.set_backspace_binding(self.getEraseBinding(settings.get_string(key)))", "response": "Called when the gconf var compat_backspace be changed in the gconf file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls when the gconf var compat_delete be changed in the gconf file.", "response": "def delete_changed(self, settings, key, user_data):\n        \"\"\"If the gconf var compat_delete be changed, this method\n        will be called and will change the binding configuration in\n        all terminals open.\n        \"\"\"\n        for i in self.guake.notebook_manager.iter_terminals():\n            i.set_delete_binding(self.getEraseBinding(settings.get_string(key)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalling when the gconf var abbreviate_tab_names is changed in the gconf file.", "response": "def abbreviate_tab_names_changed(self, settings, key, user_data):\n        \"\"\"If the gconf var abbreviate_tab_names be changed, this method will\n        be called and will update tab names.\n        \"\"\"\n        abbreviate_tab_names = settings.get_boolean('abbreviate-tab-names')\n        self.guake.abbreviate = abbreviate_tab_names\n        self.guake.recompute_tabs_titles()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreassigning an accel_group to guake main window and guake context menu and calls the load_accelerators method.", "response": "def reload_accelerators(self, *args):\n        \"\"\"Reassign an accel_group to guake main window and guake\n        context menu and calls the load_accelerators method.\n        \"\"\"\n        if self.accel_group:\n            self.guake.window.remove_accel_group(self.accel_group)\n        self.accel_group = Gtk.AccelGroup()\n        self.guake.window.add_accel_group(self.accel_group)\n        self.load_accelerators()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_accelerators(self):\n\n        def getk(x):\n            return self.guake.settings.keybindingsLocal.get_string(x)\n\n        key, mask = Gtk.accelerator_parse(getk('reset-terminal'))\n\n        if key > 0:\n            self.accel_group.connect(\n                key, mask, Gtk.AccelFlags.VISIBLE, self.guake.accel_reset_terminal\n            )\n\n        key, mask = Gtk.accelerator_parse(getk('quit'))\n        if key > 0:\n            self.accel_group.connect(key, mask, Gtk.AccelFlags.VISIBLE, self.guake.accel_quit)\n\n        key, mask = Gtk.accelerator_parse(getk('new-tab'))\n        if key > 0:\n            self.accel_group.connect(key, mask, Gtk.AccelFlags.VISIBLE, self.guake.accel_add)\n\n        key, mask = Gtk.accelerator_parse(getk('new-tab-home'))\n        if key > 0:\n            self.accel_group.connect(key, mask, Gtk.AccelFlags.VISIBLE, self.guake.accel_add_home)\n\n        key, mask = Gtk.accelerator_parse(getk('close-tab'))\n        if key > 0:\n\n            def x(*args):\n                prompt_cfg = self.guake.settings.general.get_int('prompt-on-close-tab')\n                self.guake.get_notebook().delete_page_current(prompt=prompt_cfg)\n\n            self.accel_group.connect(key, mask, Gtk.AccelFlags.VISIBLE, x)\n\n        key, mask = Gtk.accelerator_parse(getk('previous-tab'))\n        if key > 0:\n            self.accel_group.connect(key, mask, Gtk.AccelFlags.VISIBLE, self.guake.accel_prev)\n\n        key, mask = Gtk.accelerator_parse(getk('next-tab'))\n        if key > 0:\n            self.accel_group.connect(key, mask, Gtk.AccelFlags.VISIBLE, self.guake.accel_next)\n\n        key, mask = Gtk.accelerator_parse(getk('move-tab-left'))\n        if key > 0:\n            self.accel_group.connect(\n                key, mask, Gtk.AccelFlags.VISIBLE, self.guake.accel_move_tab_left\n            )\n\n        key, mask = Gtk.accelerator_parse(getk('move-tab-right'))\n        if key > 0:\n            self.accel_group.connect(\n                key, mask, Gtk.AccelFlags.VISIBLE, self.guake.accel_move_tab_right\n            )\n\n        key, mask = Gtk.accelerator_parse(getk('rename-current-tab'))\n        if key > 0:\n            self.accel_group.connect(\n                key, mask, Gtk.AccelFlags.VISIBLE, self.guake.accel_rename_current_tab\n            )\n\n        key, mask = Gtk.accelerator_parse(getk('clipboard-copy'))\n        if key > 0:\n            self.accel_group.connect(\n                key, mask, Gtk.AccelFlags.VISIBLE, self.guake.accel_copy_clipboard\n            )\n\n        key, mask = Gtk.accelerator_parse(getk('clipboard-paste'))\n        if key > 0:\n            self.accel_group.connect(\n                key, mask, Gtk.AccelFlags.VISIBLE, self.guake.accel_paste_clipboard\n            )\n\n        key, mask = Gtk.accelerator_parse(getk('toggle-fullscreen'))\n        if key > 0:\n            self.accel_group.connect(\n                key, mask, Gtk.AccelFlags.VISIBLE, self.guake.accel_toggle_fullscreen\n            )\n\n        key, mask = Gtk.accelerator_parse(getk('toggle-hide-on-lose-focus'))\n        if key > 0:\n            self.accel_group.connect(\n                key, mask, Gtk.AccelFlags.VISIBLE, self.guake.accel_toggle_hide_on_lose_focus\n            )\n\n        key, mask = Gtk.accelerator_parse(getk('zoom-in'))\n        if key > 0:\n            self.accel_group.connect(key, mask, Gtk.AccelFlags.VISIBLE, self.guake.accel_zoom_in)\n\n        key, mask = Gtk.accelerator_parse(getk('zoom-in-alt'))\n        if key > 0:\n            self.accel_group.connect(key, mask, Gtk.AccelFlags.VISIBLE, self.guake.accel_zoom_in)\n\n        key, mask = Gtk.accelerator_parse(getk('zoom-out'))\n        if key > 0:\n            self.accel_group.connect(key, mask, Gtk.AccelFlags.VISIBLE, self.guake.accel_zoom_out)\n\n        key, mask = Gtk.accelerator_parse(getk('increase-height'))\n        if key > 0:\n            self.accel_group.connect(\n                key, mask, Gtk.AccelFlags.VISIBLE, self.guake.accel_increase_height\n            )\n\n        key, mask = Gtk.accelerator_parse(getk('decrease-height'))\n        if key > 0:\n            self.accel_group.connect(\n                key, mask, Gtk.AccelFlags.VISIBLE, self.guake.accel_decrease_height\n            )\n\n        key, mask = Gtk.accelerator_parse(getk('increase-transparency'))\n        if key > 0:\n            self.accel_group.connect(\n                key, mask, Gtk.AccelFlags.VISIBLE, self.guake.accel_increase_transparency\n            )\n\n        key, mask = Gtk.accelerator_parse(getk('decrease-transparency'))\n        if key > 0:\n            self.accel_group.connect(\n                key, mask, Gtk.AccelFlags.VISIBLE, self.guake.accel_decrease_transparency\n            )\n\n        key, mask = Gtk.accelerator_parse(getk('toggle-transparency'))\n        if key > 0:\n            self.accel_group.connect(\n                key, mask, Gtk.AccelFlags.VISIBLE, self.guake.accel_toggle_transparency\n            )\n\n        for tab in range(1, 11):\n            key, mask = Gtk.accelerator_parse(getk('switch-tab%d' % tab))\n            if key > 0:\n                self.accel_group.connect(\n                    key, mask, Gtk.AccelFlags.VISIBLE, self.guake.gen_accel_switch_tabN(tab - 1)\n                )\n\n        key, mask = Gtk.accelerator_parse(getk('switch-tab-last'))\n        if key > 0:\n            self.accel_group.connect(\n                key, mask, Gtk.AccelFlags.VISIBLE, self.guake.accel_switch_tab_last\n            )\n\n        try:\n            key, mask = Gtk.accelerator_parse(getk('search-on-web'))\n            if key > 0:\n                self.accel_group.connect(\n                    key, mask, Gtk.AccelFlags.VISIBLE, self.guake.search_on_web\n                )\n        except Exception:\n            log.exception(\"Exception occured\")\n\n        key, mask = Gtk.accelerator_parse(getk('split-tab-vertical'))\n        if key > 0:\n            self.accel_group.connect(\n                key,\n                mask,\n                Gtk.AccelFlags.VISIBLE,\n                (\n                    lambda *args:  # keep make style from concat this lines\n                    self.guake.get_notebook().get_current_terminal().get_parent().split_v() or True\n                )\n            )\n        key, mask = Gtk.accelerator_parse(getk('split-tab-horizontal'))\n        if key > 0:\n            self.accel_group.connect(\n                key,\n                mask,\n                Gtk.AccelFlags.VISIBLE,\n                (\n                    lambda *args:  # keep make style from concat this lines\n                    self.guake.get_notebook().get_current_terminal().get_parent().split_h() or True\n                )\n            )\n        key, mask = Gtk.accelerator_parse(getk('close-terminal'))\n        if key > 0:\n            self.accel_group.connect(\n                key, mask, Gtk.AccelFlags.VISIBLE,\n                (lambda *args: self.guake.get_notebook().get_current_terminal().kill() or True)\n            )\n        key, mask = Gtk.accelerator_parse(getk('focus-terminal-up'))\n        if key > 0:\n            self.accel_group.connect(\n                key, mask, Gtk.AccelFlags.VISIBLE, (\n                    lambda *args: FocusMover(self.guake.window).\n                    move_up(self.guake.get_notebook().get_current_terminal()) or True\n                )\n            )\n        key, mask = Gtk.accelerator_parse(getk('focus-terminal-down'))\n        if key > 0:\n            self.accel_group.connect(\n                key, mask, Gtk.AccelFlags.VISIBLE, (\n                    lambda *args: FocusMover(self.guake.window).\n                    move_down(self.guake.get_notebook().get_current_terminal()) or True\n                )\n            )\n        key, mask = Gtk.accelerator_parse(getk('focus-terminal-right'))\n        if key > 0:\n            self.accel_group.connect(\n                key, mask, Gtk.AccelFlags.VISIBLE, (\n                    lambda *args: FocusMover(self.guake.window).\n                    move_right(self.guake.get_notebook().get_current_terminal()) or True\n                )\n            )\n        key, mask = Gtk.accelerator_parse(getk('focus-terminal-left'))\n        if key > 0:\n            self.accel_group.connect(\n                key, mask, Gtk.AccelFlags.VISIBLE, (\n                    lambda *args: FocusMover(self.guake.window).\n                    move_left(self.guake.get_notebook().get_current_terminal()) or True\n                )\n            )\n        key, mask = Gtk.accelerator_parse(getk('move-terminal-split-up'))\n        if key > 0:\n            self.accel_group.connect(\n                key,\n                mask,\n                Gtk.AccelFlags.VISIBLE,\n                (\n                    lambda *args:  # keep make style from concat this lines\n                    SplitMover.move_up(self.guake.get_notebook().get_current_terminal()) or True\n                )\n            )\n        key, mask = Gtk.accelerator_parse(getk('move-terminal-split-down'))\n        if key > 0:\n            self.accel_group.connect(\n                key,\n                mask,\n                Gtk.AccelFlags.VISIBLE,\n                (\n                    lambda *args:  # keep make style from concat this lines\n                    SplitMover.move_down(self.guake.get_notebook().get_current_terminal()) or True\n                )\n            )\n        key, mask = Gtk.accelerator_parse(getk('move-terminal-split-left'))\n        if key > 0:\n            self.accel_group.connect(\n                key,\n                mask,\n                Gtk.AccelFlags.VISIBLE,\n                (\n                    lambda *args:  # keep make style from concat this lines\n                    SplitMover.move_left(self.guake.get_notebook().get_current_terminal()) or True\n                )\n            )\n        key, mask = Gtk.accelerator_parse(getk('move-terminal-split-right'))\n        if key > 0:\n            self.accel_group.connect(\n                key,\n                mask,\n                Gtk.AccelFlags.VISIBLE,\n                (\n                    lambda *args:  # keep make style from concat this lines\n                    SplitMover.move_right(self.guake.get_notebook().get_current_terminal()) or True\n                )\n            )\n\n        key, mask = Gtk.accelerator_parse(getk('search-terminal'))\n        if key > 0:\n            self.accel_group.connect(\n                key, mask, Gtk.AccelFlags.VISIBLE, self.guake.accel_search_terminal\n            )", "response": "Reads all gconf paths under / apps / guake / keybindings and adds them to the main accel_group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrename an already added page by its index.", "response": "def rename_page(self, page_index, new_text, user_set=False):\n        \"\"\"Rename an already added page by its index. Use user_set to define\n        if the rename was triggered by the user (eg. rename dialog) or by\n        an update from the vte (eg. vte:window-title-changed)\n        \"\"\"\n        page = self.get_nth_page(page_index)\n        if not getattr(page, \"custom_label_set\", False) or user_set:\n            old_label = self.get_tab_label(page)\n            if isinstance(old_label, TabLabelEventBox):\n                old_label.set_text(new_text)\n            else:\n                label = TabLabelEventBox(self, new_text, self.guake.settings)\n                label.add_events(Gdk.EventMask.SCROLL_MASK)\n                label.connect('scroll-event', self.scroll_callback.on_scroll)\n\n                self.set_tab_label(page, label)\n            if user_set:\n                setattr(page, \"custom_label_set\", new_text != \"-\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main():\n    # Force to xterm-256 colors for compatibility with some old command line programs\n    os.environ[\"TERM\"] = \"xterm-256color\"\n\n    # Force use X11 backend underwayland\n    os.environ[\"GDK_BACKEND\"] = \"x11\"\n\n    # do not use version keywords here, pbr might be slow to find the version of Guake module\n    parser = OptionParser()\n    parser.add_option(\n        '-V',\n        '--version',\n        dest='version',\n        action='store_true',\n        default=False,\n        help=_('Show Guake version number and exit')\n    )\n\n    parser.add_option(\n        '-v',\n        '--verbose',\n        dest='verbose',\n        action='store_true',\n        default=False,\n        help=_('Enable verbose logging')\n    )\n\n    parser.add_option(\n        '-f',\n        '--fullscreen',\n        dest='fullscreen',\n        action='store_true',\n        default=False,\n        help=_('Put Guake in fullscreen mode')\n    )\n\n    parser.add_option(\n        '-t',\n        '--toggle-visibility',\n        dest='show_hide',\n        action='store_true',\n        default=False,\n        help=_('Toggles the visibility of the terminal window')\n    )\n\n    parser.add_option(\n        '--show',\n        dest=\"show\",\n        action='store_true',\n        default=False,\n        help=_('Shows Guake main window')\n    )\n\n    parser.add_option(\n        '--hide',\n        dest='hide',\n        action='store_true',\n        default=False,\n        help=_('Hides Guake main window')\n    )\n\n    parser.add_option(\n        '-p',\n        '--preferences',\n        dest='show_preferences',\n        action='store_true',\n        default=False,\n        help=_('Shows Guake preference window')\n    )\n\n    parser.add_option(\n        '-a',\n        '--about',\n        dest='show_about',\n        action='store_true',\n        default=False,\n        help=_('Shows Guake\\'s about info')\n    )\n\n    parser.add_option(\n        '-n',\n        '--new-tab',\n        dest='new_tab',\n        action='store',\n        default='',\n        help=_('Add a new tab (with current directory set to NEW_TAB)')\n    )\n\n    parser.add_option(\n        '-s',\n        '--select-tab',\n        dest='select_tab',\n        action='store',\n        default='',\n        help=_('Select a tab (SELECT_TAB is the index of the tab)')\n    )\n\n    parser.add_option(\n        '-g',\n        '--selected-tab',\n        dest='selected_tab',\n        action='store_true',\n        default=False,\n        help=_('Return the selected tab index.')\n    )\n\n    parser.add_option(\n        '-l',\n        '--selected-tablabel',\n        dest='selected_tablabel',\n        action='store_true',\n        default=False,\n        help=_('Return the selected tab label.')\n    )\n\n    parser.add_option(\n        '--split-vertical',\n        dest='split_vertical',\n        action='store_true',\n        default=False,\n        help=_('Split the selected tab vertically.')\n    )\n\n    parser.add_option(\n        '--split-horizontal',\n        dest='split_horizontal',\n        action='store_true',\n        default=False,\n        help=_('Split the selected tab horizontally.')\n    )\n\n    parser.add_option(\n        '-e',\n        '--execute-command',\n        dest='command',\n        action='store',\n        default='',\n        help=_('Execute an arbitrary command in the selected tab.')\n    )\n\n    parser.add_option(\n        '-i',\n        '--tab-index',\n        dest='tab_index',\n        action='store',\n        default='0',\n        help=_('Specify the tab to rename. Default is 0. Can be used to select tab by UUID.')\n    )\n\n    parser.add_option(\n        '--bgcolor',\n        dest='bgcolor',\n        action='store',\n        default='',\n        help=_('Set the hexadecimal (#rrggbb) background color of '\n               'the selected tab.')\n    )\n\n    parser.add_option(\n        '--fgcolor',\n        dest='fgcolor',\n        action='store',\n        default='',\n        help=_('Set the hexadecimal (#rrggbb) foreground color of the '\n               'selected tab.')\n    )\n\n    parser.add_option(\n        '--change-palette',\n        dest='palette_name',\n        action='store',\n        default='',\n        help=_('Change Guake palette scheme')\n    )\n\n    parser.add_option(\n        '--rename-tab',\n        dest='rename_tab',\n        metavar='TITLE',\n        action='store',\n        default='',\n        help=_(\n            'Rename the specified tab by --tab-index. Reset to default if TITLE is '\n            'a single dash \"-\".'\n        )\n    )\n\n    parser.add_option(\n        '-r',\n        '--rename-current-tab',\n        dest='rename_current_tab',\n        metavar='TITLE',\n        action='store',\n        default='',\n        help=_('Rename the current tab. Reset to default if TITLE is a '\n               'single dash \"-\".')\n    )\n\n    parser.add_option(\n        '-q',\n        '--quit',\n        dest='quit',\n        action='store_true',\n        default=False,\n        help=_('Says to Guake go away =(')\n    )\n\n    parser.add_option(\n        '-u',\n        '--no-startup-script',\n        dest='execute_startup_script',\n        action='store_false',\n        default=True,\n        help=_('Do not execute the start up script')\n    )\n\n    parser.add_option(\n        '--save-preferences',\n        dest='save_preferences',\n        action='store',\n        default=None,\n        help=_('Save Guake preferences to this filename')\n    )\n\n    parser.add_option(\n        '--restore-preferences',\n        dest='restore_preferences',\n        action='store',\n        default=None,\n        help=_('Restore Guake preferences from this file')\n    )\n\n    parser.add_option(\n        '--support',\n        dest='support',\n        action='store_true',\n        default=False,\n        help=_('Show support infomations')\n    )\n\n    # checking mandatory dependencies\n    import gi\n\n    try:\n        gi.require_version('Gtk', '3.0')\n        gi.require_version('Gdk', '3.0')\n    except ValueError:\n        print(\"[ERROR] Unable to start Guake, missing mandatory dependency: GtK 3.0\")\n        sys.exit(1)\n\n    try:\n        gi.require_version('Vte', '2.91')  # vte-0.42\n    except ValueError:\n        print(\"[ERROR] Unable to start Guake, missing mandatory dependency: Vte >= 0.42\")\n        sys.exit(1)\n\n    try:\n        gi.require_version('Keybinder', '3.0')\n    except ValueError:\n        print(\"[ERROR] Unable to start Guake, missing mandatory dependency: Keybinder 3\")\n        sys.exit(1)\n\n    options = parser.parse_args()[0]\n    if options.version:\n        from guake import gtk_version\n        from guake import guake_version\n        from guake import vte_version\n        from guake import vte_runtime_version\n        print('Guake Terminal: {}'.format(guake_version()))\n        print('VTE: {}'.format(vte_version()))\n        print('VTE runtime: {}'.format(vte_runtime_version()))\n        print('Gtk: {}'.format(gtk_version()))\n        sys.exit(0)\n\n    if options.save_preferences and options.restore_preferences:\n        parser.error('options --save-preferences and --restore-preferences are mutually exclusive')\n    if options.save_preferences:\n        save_preferences(options.save_preferences)\n        sys.exit(0)\n    elif options.restore_preferences:\n        restore_preferences(options.restore_preferences)\n        sys.exit(0)\n\n    if options.support:\n        print_support()\n        sys.exit(0)\n\n    import dbus\n\n    from guake.dbusiface import DBUS_NAME\n    from guake.dbusiface import DBUS_PATH\n    from guake.dbusiface import DbusManager\n    from guake.guake_logging import setupLogging\n\n    instance = None\n\n    # Trying to get an already running instance of guake. If it is not\n    # possible, lets create a new instance. This function will return\n    # a boolean value depending on this decision.\n    try:\n        bus = dbus.SessionBus()\n        remote_object = bus.get_object(DBUS_NAME, DBUS_PATH)\n        already_running = True\n    except dbus.DBusException:\n        # can now configure the logging\n        setupLogging(options.verbose)\n\n        # COLORTERM is an environment variable set by some terminal emulators such as\n        # gnome-terminal.\n        # To avoid confusing applications running inside Guake, clean up COLORTERM at startup.\n        if \"COLORTERM\" in os.environ:\n            del os.environ['COLORTERM']\n\n        log.info(\"Guake not running, starting it\")\n        # late loading of the Guake object, to speed up dbus comm\n        from guake.guake_app import Guake\n        instance = Guake()\n        remote_object = DbusManager(instance)\n        already_running = False\n\n    only_show_hide = True\n\n    if options.fullscreen:\n        remote_object.fullscreen()\n\n    if options.show:\n        remote_object.show_from_remote()\n\n    if options.hide:\n        remote_object.hide_from_remote()\n\n    if options.show_preferences:\n        remote_object.show_prefs()\n        only_show_hide = options.show\n\n    if options.new_tab:\n        remote_object.add_tab(options.new_tab)\n        only_show_hide = options.show\n\n    if options.select_tab:\n        selected = int(options.select_tab)\n        tab_count = int(remote_object.get_tab_count())\n        if 0 <= selected < tab_count:\n            remote_object.select_tab(selected)\n        else:\n            sys.stderr.write('invalid index: %d\\n' % selected)\n        only_show_hide = options.show\n\n    if options.selected_tab:\n        selected = remote_object.get_selected_tab()\n        sys.stdout.write('%d\\n' % selected)\n        only_show_hide = options.show\n\n    if options.selected_tablabel:\n        selectedlabel = remote_object.get_selected_tablabel()\n        sys.stdout.write('%s\\n' % selectedlabel)\n        only_show_hide = options.show\n\n    if options.split_vertical:\n        remote_object.v_split_current_terminal()\n        only_show_hide = options.show\n\n    if options.split_horizontal:\n        remote_object.h_split_current_terminal()\n        only_show_hide = options.show\n\n    if options.command:\n        remote_object.execute_command(options.command)\n        only_show_hide = options.show\n\n    if options.tab_index and options.rename_tab:\n        try:\n            remote_object.rename_tab_uuid(str(uuid.UUID(options.tab_index)), options.rename_tab)\n        except ValueError:\n            remote_object.rename_tab(int(options.tab_index), options.rename_tab)\n        only_show_hide = options.show\n\n    if options.bgcolor:\n        remote_object.set_bgcolor(options.bgcolor)\n        only_show_hide = options.show\n\n    if options.fgcolor:\n        remote_object.set_fgcolor(options.fgcolor)\n        only_show_hide = options.show\n\n    if options.palette_name:\n        remote_object.change_palette_name(options.palette_name)\n        only_show_hide = options.show\n\n    if options.rename_current_tab:\n        remote_object.rename_current_tab(options.rename_current_tab)\n        only_show_hide = options.show\n\n    if options.show_about:\n        remote_object.show_about()\n        only_show_hide = options.show\n\n    if options.quit:\n        try:\n            remote_object.quit()\n            return True\n        except dbus.DBusException:\n            return True\n\n    if already_running and only_show_hide:\n        # here we know that guake was called without any parameter and\n        # it is already running, so, lets toggle its visibility.\n        remote_object.show_hide()\n\n    if options.execute_startup_script:\n        if not already_running:\n            startup_script = instance.settings.general.get_string(\"startup-script\")\n            if startup_script:\n                log.info(\"Calling startup script: %s\", startup_script)\n                pid = subprocess.Popen([startup_script],\n                                       shell=True,\n                                       stdin=None,\n                                       stdout=None,\n                                       stderr=None,\n                                       close_fds=True)\n                log.info(\"Startup script started with pid: %s\", pid)\n                # Please ensure this is the last line !!!!\n    else:\n        log.info(\"--no-startup-script argument defined, so don't execute the startup script\")\n    if already_running:\n        log.info(\"Guake is already running\")\n    return already_running", "response": "Parse the command line parameters and decide if dbus methods\n    should be called or not."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bindtextdomain(app_name, locale_dir=None):\n\n    import locale\n    from locale import gettext as _\n\n    log.info(\"Local binding for app '%s', local dir: %s\", app_name, locale_dir)\n\n    locale.bindtextdomain(app_name, locale_dir)\n    locale.textdomain(app_name)", "response": "Binds the domain represented by app_name to the locale directory locale_dir."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls when prefs dialog is running in standalone mode.", "response": "def setup_standalone_signals(instance):\n    \"\"\"Called when prefs dialog is running in standalone mode. It\n    makes the delete event of dialog and click on close button finish\n    the application.\n    \"\"\"\n    window = instance.get_widget('config-window')\n    window.connect('delete-event', Gtk.main_quit)\n\n    # We need to block the execution of the already associated\n    # callback before connecting the new handler.\n    button = instance.get_widget('button1')\n    button.handler_block_by_func(instance.gtk_widget_destroy)\n    button.connect('clicked', Gtk.main_quit)\n\n    return instance"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nchange the activity of default_shell in dconf", "response": "def on_default_shell_changed(self, combo):\n        \"\"\"Changes the activity of default_shell in dconf\n        \"\"\"\n        citer = combo.get_active_iter()\n        if not citer:\n            return\n        shell = combo.get_model().get_value(citer, 0)\n        # we unset the value (restore to default) when user chooses to use\n        # user shell as guake shell interpreter.\n        if shell == USER_SHELL_VALUE:\n            self.settings.general.reset('default-shell')\n        else:\n            self.settings.general.set_string('default-shell', shell)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on_gtk_theme_name_changed(self, combo):\n        citer = combo.get_active_iter()\n        if not citer:\n            return\n        theme_name = combo.get_model().get_value(citer, 0)\n        self.settings.general.set_string('gtk-theme-name', theme_name)\n        select_gtk_theme(self.settings)", "response": "Set the gtk_theme_name property in dconf\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef on_gtk_prefer_dark_theme_toggled(self, chk):\n        self.settings.general.set_boolean('gtk-prefer-dark-theme', chk.get_active())\n        select_gtk_theme(self.settings)", "response": "Set the gtk_prefer_dark_theme property in dconf\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchanges the activity of start_at_login in dconf", "response": "def on_start_at_login_toggled(self, chk):\n        \"\"\"Changes the activity of start_at_login in dconf\n        \"\"\"\n        self.settings.general.set_boolean('start-at-login', chk.get_active())\n        refresh_user_start(self.settings)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchanges the value of max_tab_name_length in dconf", "response": "def on_max_tab_name_length_changed(self, spin):\n        \"\"\"Changes the value of max_tab_name_length in dconf\n        \"\"\"\n        val = int(spin.get_value())\n        self.settings.general.set_int('max-tab-name-length', val)\n        self.prefDlg.update_vte_subwidgets_states()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef on_right_align_toggled(self, chk):\n        v = chk.get_active()\n        self.settings.general.set_int('window-halignment', 1 if v else 0)", "response": "set the horizontal alignment setting."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the vertical alignment setting.", "response": "def on_bottom_align_toggled(self, chk):\n        \"\"\"set the vertical alignment setting.\n        \"\"\"\n        v = chk.get_active()\n        self.settings.general.set_int('window-valignment', ALIGN_BOTTOM if v else ALIGN_TOP)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the destination display in dconf.", "response": "def on_display_n_changed(self, combo):\n        \"\"\"Set the destination display in dconf.\n        \"\"\"\n\n        i = combo.get_active_iter()\n        if not i:\n            return\n\n        model = combo.get_model()\n        first_item_path = model.get_path(model.get_iter_first())\n\n        if model.get_path(i) == first_item_path:\n            val_int = ALWAYS_ON_PRIMARY\n        else:\n            val = model.get_value(i, 0)\n            val_int = int(val.split()[0])  # extracts 1 from '1' or from '1 (primary)'\n        self.settings.general.set_int('display-n', val_int)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on_window_height_value_changed(self, hscale):\n        val = hscale.get_value()\n        self.settings.general.set_int('window-height', int(val))", "response": "Changes the value of window_height in dconf\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef on_window_width_value_changed(self, wscale):\n        val = wscale.get_value()\n        self.settings.general.set_int('window-width', int(val))", "response": "Changes the value of window_width in dconf\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchanges the value of window_halignment in dconf", "response": "def on_window_halign_value_changed(self, halign_button):\n        \"\"\"Changes the value of window_halignment in dconf\n        \"\"\"\n        which_align = {\n            'radiobutton_align_left': ALIGN_LEFT,\n            'radiobutton_align_right': ALIGN_RIGHT,\n            'radiobutton_align_center': ALIGN_CENTER\n        }\n        if halign_button.get_active():\n            self.settings.general.set_int(\n                'window-halignment', which_align[halign_button.get_name()]\n            )\n        self.prefDlg.get_widget(\"window_horizontal_displacement\").set_sensitive(\n            which_align[halign_button.get_name()] != ALIGN_CENTER\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef on_history_size_value_changed(self, spin):\n        val = int(spin.get_value())\n        self.settings.general.set_int('history-size', val)\n        self._update_history_widgets()", "response": "Changes the value of history_size in dconf\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef on_transparency_value_changed(self, hscale):\n        value = hscale.get_value()\n        self.prefDlg.set_colors_from_settings()\n        self.settings.styleBackground.set_int('transparency', MAX_TRANSPARENCY - int(value))", "response": "Changes the value of background_transparency in dconf\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchanging the value of compat_backspace in dconf", "response": "def on_backspace_binding_changed(self, combo):\n        \"\"\"Changes the value of compat_backspace in dconf\n        \"\"\"\n        val = combo.get_active_text()\n        self.settings.general.set_string('compat-backspace', ERASE_BINDINGS[val])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchanging the value of window - vertical - displacement", "response": "def on_window_vertical_displacement_value_changed(self, spin):\n        \"\"\"Changes the value of window-vertical-displacement\n        \"\"\"\n        self.settings.general.set_int('window-vertical-displacement', int(spin.get_value()))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_window_horizontal_displacement_value_changed(self, spin):\n        self.settings.general.set_int('window-horizontal-displacement', int(spin.get_value()))", "response": "Changes the value of window - horizontal - displacement\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntoggling the use font background sensitivity", "response": "def toggle_use_font_background_sensitivity(self, chk):\n        \"\"\"If the user chooses to use the gnome default font\n        configuration it means that he will not be able to use the\n        font selector.\n        \"\"\"\n        self.get_widget('palette_16').set_sensitive(chk.get_active())\n        self.get_widget('palette_17').set_sensitive(chk.get_active())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_reset_compat_defaults_clicked(self, bnt):\n        self.settings.general.reset('compat-backspace')\n        self.settings.general.reset('compat-delete')\n        self.reload_erase_combos()", "response": "Reset default values to compat_backspace and compat_delete keys."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nchange the value of palette in dconf", "response": "def on_palette_name_changed(self, combo):\n        \"\"\"Changes the value of palette in dconf\n        \"\"\"\n        palette_name = combo.get_active_text()\n        if palette_name not in PALETTES:\n            return\n        self.settings.styleFont.set_string('palette', PALETTES[palette_name])\n        self.settings.styleFont.set_string('palette-name', palette_name)\n        self.set_palette_colors(PALETTES[palette_name])\n        self.update_demo_palette(PALETTES[palette_name])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nchange the value of cursor_shape in dconf", "response": "def on_cursor_shape_changed(self, combo):\n        \"\"\"Changes the value of cursor_shape in dconf\n        \"\"\"\n        index = combo.get_active()\n        self.settings.style.set_int('cursor-shape', index)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nchange the value of palette in dconf", "response": "def on_palette_color_set(self, btn):\n        \"\"\"Changes the value of palette in dconf\n        \"\"\"\n\n        palette = []\n        for i in range(18):\n            palette.append(hexify_color(self.get_widget('palette_%d' % i).get_color()))\n        palette = ':'.join(palette)\n        self.settings.styleFont.set_string('palette', palette)\n        self.settings.styleFont.set_string('palette-name', _('Custom'))\n        self.set_palette_name('Custom')\n        self.update_demo_palette(palette)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_palette_name(self, palette_name):\n        combo = self.get_widget('palette_name')\n        found = False\n        log.debug(\"wanting palette: %r\", palette_name)\n        for i in combo.get_model():\n            if i[0] == palette_name:\n                combo.set_active_iter(i.iter)\n                found = True\n                break\n        if not found:\n            combo.set_active(self.custom_palette_index)", "response": "Set the color of the color_name combo box."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_palette_colors(self, palette):\n        palette = palette.split(':')\n        for i, pal in enumerate(palette):\n            x, color = Gdk.Color.parse(pal)\n            self.get_widget('palette_%d' % i).set_color(color)", "response": "Updates the color buttons with the given palette"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreloads the combos of the current locale.", "response": "def reload_erase_combos(self, btn=None):\n        \"\"\"Read from dconf the value of compat_{backspace,delete} vars\n        and select the right option in combos.\n        \"\"\"\n        # backspace erase binding\n        combo = self.get_widget('backspace-binding-combobox')\n        binding = self.settings.general.get_string('compat-backspace')\n        for i in combo.get_model():\n            if ERASE_BINDINGS.get(i[0]) == binding:\n                combo.set_active_iter(i.iter)\n                break\n\n        # delete erase binding\n        combo = self.get_widget('delete-binding-combobox')\n        binding = self.settings.general.get_string('compat-delete')\n        for i in combo.get_model():\n            if ERASE_BINDINGS.get(i[0]) == binding:\n                combo.set_active_iter(i.iter)\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_configs(self):\n        self._load_default_shell_settings()\n\n        # restore tabs startup\n        value = self.settings.general.get_boolean('restore-tabs-startup')\n        self.get_widget('restore-tabs-startup').set_active(value)\n\n        # restore tabs notify\n        value = self.settings.general.get_boolean('restore-tabs-notify')\n        self.get_widget('restore-tabs-notify').set_active(value)\n\n        # save tabs when changed\n        value = self.settings.general.get_boolean('save-tabs-when-changed')\n        self.get_widget('save-tabs-when-changed').set_active(value)\n\n        # login shell\n        value = self.settings.general.get_boolean('use-login-shell')\n        self.get_widget('use_login_shell').set_active(value)\n\n        # tray icon\n        value = self.settings.general.get_boolean('use-trayicon')\n        self.get_widget('use_trayicon').set_active(value)\n\n        # popup\n        value = self.settings.general.get_boolean('use-popup')\n        self.get_widget('use_popup').set_active(value)\n\n        # workspace-specific tab sets\n        value = self.settings.general.get_boolean('workspace-specific-tab-sets')\n        self.get_widget('workspace-specific-tab-sets').set_active(value)\n\n        # prompt on quit\n        value = self.settings.general.get_boolean('prompt-on-quit')\n        self.get_widget('prompt_on_quit').set_active(value)\n\n        # prompt on close_tab\n        value = self.settings.general.get_int('prompt-on-close-tab')\n        self.get_widget('prompt_on_close_tab').set_active(value)\n        self.get_widget('prompt_on_quit').set_sensitive(value != 2)\n\n        # gtk theme theme\n        value = self.settings.general.get_string('gtk-theme-name')\n        combo = self.get_widget('gtk_theme_name')\n        for i in combo.get_model():\n            if i[0] == value:\n                combo.set_active_iter(i.iter)\n                break\n\n        # prefer gtk theme theme\n        value = self.settings.general.get_boolean('gtk-prefer-dark-theme')\n        self.get_widget('gtk_prefer_dark_theme').set_active(value)\n\n        # ontop\n        value = self.settings.general.get_boolean('window-ontop')\n        self.get_widget('window_ontop').set_active(value)\n\n        # tab ontop\n        value = self.settings.general.get_boolean('tab-ontop')\n        self.get_widget('tab_ontop').set_active(value)\n\n        # refocus\n        value = self.settings.general.get_boolean('window-refocus')\n        self.get_widget('window_refocus').set_active(value)\n\n        # losefocus\n        value = self.settings.general.get_boolean('window-losefocus')\n        self.get_widget('window_losefocus').set_active(value)\n\n        # use VTE titles\n        value = self.settings.general.get_boolean('use-vte-titles')\n        self.get_widget('use_vte_titles').set_active(value)\n\n        # set window title\n        value = self.settings.general.get_boolean('set-window-title')\n        self.get_widget('set_window_title').set_active(value)\n\n        # abbreviate tab names\n        self.get_widget('abbreviate_tab_names').set_sensitive(value)\n        value = self.settings.general.get_boolean('abbreviate-tab-names')\n        self.get_widget('abbreviate_tab_names').set_active(value)\n\n        # max tab name length\n        value = self.settings.general.get_int('max-tab-name-length')\n        self.get_widget('max_tab_name_length').set_value(value)\n\n        self.update_vte_subwidgets_states()\n\n        value = self.settings.general.get_int('window-height')\n        self.get_widget('window_height').set_value(value)\n\n        value = self.settings.general.get_int('window-width')\n        self.get_widget('window_width').set_value(value)\n\n        # window displacements\n        value = self.settings.general.get_int('window-vertical-displacement')\n        self.get_widget('window_vertical_displacement').set_value(value)\n\n        value = self.settings.general.get_int('window-horizontal-displacement')\n        self.get_widget('window_horizontal_displacement').set_value(value)\n\n        value = self.settings.general.get_int('window-halignment')\n        which_button = {\n            ALIGN_RIGHT: 'radiobutton_align_right',\n            ALIGN_LEFT: 'radiobutton_align_left',\n            ALIGN_CENTER: 'radiobutton_align_center'\n        }\n        self.get_widget(which_button[value]).set_active(True)\n        self.get_widget(\"window_horizontal_displacement\").set_sensitive(value != ALIGN_CENTER)\n\n        value = self.settings.general.get_boolean('open-tab-cwd')\n        self.get_widget('open_tab_cwd').set_active(value)\n\n        # tab bar\n        value = self.settings.general.get_boolean('window-tabbar')\n        self.get_widget('window_tabbar').set_active(value)\n\n        # start fullscreen\n        value = self.settings.general.get_boolean('start-fullscreen')\n        self.get_widget('start_fullscreen').set_active(value)\n\n        # start at GNOME login\n        value = self.settings.general.get_boolean('start-at-login')\n        self.get_widget('start_at_login').set_active(value)\n\n        # use audible bell\n        value = self.settings.general.get_boolean('use-audible-bell')\n        self.get_widget('use_audible_bell').set_active(value)\n\n        self._load_screen_settings()\n\n        value = self.settings.general.get_boolean('quick-open-enable')\n        self.get_widget('quick_open_enable').set_active(value)\n        self.get_widget('quick_open_command_line').set_sensitive(value)\n        self.get_widget('quick_open_in_current_terminal').set_sensitive(value)\n        text = Gtk.TextBuffer()\n        text = self.get_widget('quick_open_supported_patterns').get_buffer()\n        for title, matcher, _useless in QUICK_OPEN_MATCHERS:\n            text.insert_at_cursor(\"%s: %s\\n\" % (title, matcher))\n        self.get_widget('quick_open_supported_patterns').set_buffer(text)\n\n        value = self.settings.general.get_string('quick-open-command-line')\n        if value is None:\n            value = \"subl %(file_path)s:%(line_number)s\"\n        self.get_widget('quick_open_command_line').set_text(value)\n\n        value = self.settings.general.get_boolean('quick-open-in-current-terminal')\n        self.get_widget('quick_open_in_current_terminal').set_active(value)\n\n        value = self.settings.general.get_string('startup-script')\n        if value:\n            self.get_widget('startup_script').set_text(value)\n\n        # use display where the mouse is currently\n        value = self.settings.general.get_boolean('mouse-display')\n        self.get_widget('mouse_display').set_active(value)\n\n        # scrollbar\n        value = self.settings.general.get_boolean('use-scrollbar')\n        self.get_widget('use_scrollbar').set_active(value)\n\n        # history size\n        value = self.settings.general.get_int('history-size')\n        self.get_widget('history_size').set_value(value)\n\n        # infinite history\n        value = self.settings.general.get_boolean('infinite-history')\n        self.get_widget('infinite_history').set_active(value)\n\n        # scroll output\n        value = self.settings.general.get_boolean('scroll-output')\n        self.get_widget('scroll_output').set_active(value)\n\n        # scroll keystroke\n        value = self.settings.general.get_boolean('scroll-keystroke')\n        self.get_widget('scroll_keystroke').set_active(value)\n\n        # default font\n        value = self.settings.general.get_boolean('use-default-font')\n        self.get_widget('use_default_font').set_active(value)\n        self.get_widget('font_style').set_sensitive(not value)\n\n        # font\n        value = self.settings.styleFont.get_string('style')\n        if value:\n            self.get_widget('font_style').set_font_name(value)\n\n        # allow bold font\n        value = self.settings.styleFont.get_boolean('allow-bold')\n        self.get_widget('allow_bold').set_active(value)\n\n        # use bold is bright\n        value = self.settings.styleFont.get_boolean('bold-is-bright')\n        self.get_widget('bold_is_bright').set_active(value)\n\n        # palette\n        self.fill_palette_names()\n        value = self.settings.styleFont.get_string('palette-name')\n        self.set_palette_name(value)\n        value = self.settings.styleFont.get_string('palette')\n        self.set_palette_colors(value)\n        self.update_demo_palette(value)\n\n        # cursor shape\n        value = self.settings.style.get_int('cursor-shape')\n        self.set_cursor_shape(value)\n\n        # cursor blink\n        value = self.settings.style.get_int('cursor-blink-mode')\n        self.set_cursor_blink_mode(value)\n\n        value = self.settings.styleBackground.get_int('transparency')\n        self.get_widget('background_transparency').set_value(MAX_TRANSPARENCY - value)\n\n        value = self.settings.general.get_int('window-valignment')\n        self.get_widget('top_align').set_active(value)\n\n        # it's a separated method, to be reused.\n        self.reload_erase_combos()\n\n        # custom command context-menu configuration file\n        custom_command_file = self.settings.general.get_string('custom-command-file')\n        if custom_command_file:\n            custom_command_file_name = os.path.expanduser(custom_command_file)\n        else:\n            custom_command_file_name = None\n        custom_cmd_filter = Gtk.FileFilter()\n        custom_cmd_filter.set_name(_(\"JSON files\"))\n        custom_cmd_filter.add_pattern(\"*.json\")\n        self.get_widget('custom_command_file_chooser').add_filter(custom_cmd_filter)\n        all_files_filter = Gtk.FileFilter()\n        all_files_filter.set_name(_(\"All files\"))\n        all_files_filter.add_pattern(\"*\")\n        self.get_widget('custom_command_file_chooser').add_filter(all_files_filter)\n        if custom_command_file_name:\n            self.get_widget('custom_command_file_chooser').set_filename(custom_command_file_name)\n\n        # hooks\n        self._load_hooks_settings()", "response": "Load all configurations for all widgets in General Scrolling\n        and AppearanceTabs from dconf."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread the shells file and looks for installed shells to fill the default_shell combobox.", "response": "def populate_shell_combo(self):\n        \"\"\"Read the /etc/shells and looks for installed shells to\n        fill the default_shell combobox.\n        \"\"\"\n        cb = self.get_widget('default_shell')\n        # append user shell as first option\n        cb.append_text(USER_SHELL_VALUE)\n        if os.path.exists(SHELLS_FILE):\n            lines = open(SHELLS_FILE).readlines()\n            for i in lines:\n                possible = i.strip()\n                if possible and not possible.startswith('#') and os.path.exists(possible):\n                    cb.append_text(possible)\n\n        for i in get_binaries_from_path(PYTHONS):\n            cb.append_text(i)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the HOTKEYS global variable and inserts all data in the TreeStore used by the preferences window treeview.", "response": "def populate_keys_tree(self):\n        \"\"\"Reads the HOTKEYS global variable and insert all data in\n        the TreeStore used by the preferences window treeview.\n        \"\"\"\n        for group in HOTKEYS:\n            parent = self.store.append(None, [None, group['label'], None, None])\n            for item in group['keys']:\n                if item['key'] == \"show-hide\" or item['key'] == \"show-focus\":\n                    accel = self.settings.keybindingsGlobal.get_string(item['key'])\n                else:\n                    accel = self.settings.keybindingsLocal.get_string(item['key'])\n                gsettings_path = item['key']\n                keycode, mask = Gtk.accelerator_parse(accel)\n                keylabel = Gtk.accelerator_get_label(keycode, mask)\n                self.store.append(parent, [gsettings_path, item['label'], keylabel, accel])\n        self.get_widget('treeview-keys').expand_all()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the number of displays and populate this drop - down box with them all. Prepend the always on primary option.", "response": "def populate_display_n(self):\n        \"\"\"Get the number of displays and populate this drop-down box\n        with them all. Prepend the \"always on primary\" option.\n        \"\"\"\n        cb = self.get_widget('display_n')\n        screen = self.get_widget('config-window').get_screen()\n\n        cb.append_text(\"always on primary\")\n\n        for m in range(0, int(screen.get_n_monitors())):\n            if m == int(screen.get_primary_monitor()):\n                # TODO l10n\n                cb.append_text(str(m) + ' ' + '(primary)')\n            else:\n                cb.append_text(str(m))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef on_accel_cleared(self, cellrendereraccel, path):\n        dconf_path = self.store[path][HOTKET_MODEL_INDEX_DCONF]\n\n        if dconf_path == \"show-hide\":\n            # cannot disable 'show-hide' hotkey\n            log.warn(\"Cannot disable 'show-hide' hotkey\")\n            self.settings.keybindingsGlobal.set_string(dconf_path, old_accel)\n        else:\n            self.store[path][HOTKET_MODEL_INDEX_HUMAN_ACCEL] = \"\"\n            self.store[path][HOTKET_MODEL_INDEX_ACCEL] = \"None\"\n            if dconf_path == \"show-focus\":\n                self.settings.keybindingsGlobal.set_string(dconf_path, 'disabled')\n            else:\n                self.settings.keybindingsLocal.set_string(dconf_path, 'disabled')", "response": "This callback is called when the user has deleted an existing keybinding with the backspace\n            key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting editing the keybinding.", "response": "def start_editing(self, treeview, event):\n        \"\"\"Make the treeview grab the focus and start editing the cell\n        that the user has clicked to avoid confusion with two or three\n        clicks before editing a keybinding.\n\n        Thanks to gnome-keybinding-properties.c =)\n        \"\"\"\n        x, y = int(event.x), int(event.y)\n        ret = treeview.get_path_at_pos(x, y)\n        if not ret:\n            return False\n\n        path, column, cellx, celly = ret\n\n        treeview.row_activated(path, Gtk.TreeViewColumn(None))\n        treeview.set_cursor(path)\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_tabs_when_changed(func):\n\n    def wrapper(*args, **kwargs):\n        func(*args, **kwargs)\n        log.debug(\"mom, I've been called: %s %s\", func.__name__, func)\n\n        # Find me the Guake!\n        clsname = args[0].__class__.__name__\n        g = None\n        if clsname == 'Guake':\n            g = args[0]\n        elif getattr(args[0], 'get_guake', None):\n            g = args[0].get_guake()\n        elif getattr(args[0], 'get_notebook', None):\n            g = args[0].get_notebook().guake\n        elif getattr(args[0], 'guake', None):\n            g = args[0].guake\n        elif getattr(args[0], 'notebook', None):\n            g = args[0].notebook.guake\n\n        # Tada!\n        if g and g.settings.general.get_boolean('save-tabs-when-changed'):\n            g.save_tabs()\n\n    return wrapper", "response": "Decorator for save - tabs - when - changed"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the final size and location of the main window of guake.", "response": "def set_final_window_rect(cls, settings, window):\n        \"\"\"Sets the final size and location of the main window of guake. The height\n        is the window_height property, width is window_width and the\n        horizontal alignment is given by window_alignment.\n        \"\"\"\n        # fetch settings\n        height_percents = settings.general.get_int('window-height')\n\n        width_percents = settings.general.get_int('window-width')\n        halignment = settings.general.get_int('window-halignment')\n        valignment = settings.general.get_int('window-valignment')\n\n        vdisplacement = settings.general.get_int('window-vertical-displacement')\n        hdisplacement = settings.general.get_int('window-horizontal-displacement')\n\n        log.debug(\"set_final_window_rect\")\n        log.debug(\"  height_percents = %s\", height_percents)\n        log.debug(\"  width_percents = %s\", width_percents)\n        log.debug(\"  halignment = %s\", halignment)\n        log.debug(\"  valignment = %s\", valignment)\n        log.debug(\"  vdisplacement = %s\", vdisplacement)\n        log.debug(\"  hdisplacement = %s\", hdisplacement)\n\n        # get the rectangle just from the destination monitor\n        screen = window.get_screen()\n        monitor = cls.get_final_window_monitor(settings, window)\n        window_rect = screen.get_monitor_geometry(monitor)\n        log.debug(\"Current monitor geometry\")\n        log.debug(\"  window_rect.x: %s\", window_rect.x)\n        log.debug(\"  window_rect.y: %s\", window_rect.y)\n        log.debug(\"  window_rect.height: %s\", window_rect.height)\n        log.debug(\"  window_rect.width: %s\", window_rect.width)\n        log.debug(\"is unity: %s\", cls.is_using_unity(settings, window))\n\n        # TODO PORT remove this UNITY is DEAD\n        if cls.is_using_unity(settings, window):\n\n            # For Ubuntu 12.10 and above, try to use dconf:\n            # see if unity dock is hidden => unity_hide\n            # and the width of unity dock => unity_dock\n            # and the position of the unity dock. => unity_pos\n            # found = False\n            unity_hide = 0\n            unity_dock = 0\n            unity_pos = \"Left\"\n            # float() conversion might mess things up. Add 0.01 so the comparison will always be\n            # valid, even in case of float(\"10.10\") = 10.099999999999999\n            if float(platform.linux_distribution()[1]) + 0.01 >= 12.10:\n                try:\n                    unity_hide = int(\n                        subprocess.check_output([\n                            '/usr/bin/dconf', 'read',\n                            '/org/compiz/profiles/unity/plugins/unityshell/launcher-hide-mode'\n                        ])\n                    )\n                    unity_dock = int(\n                        subprocess.check_output([\n                            '/usr/bin/dconf', 'read',\n                            '/org/compiz/profiles/unity/plugins/unityshell/icon-size'\n                        ]) or \"48\"\n                    )\n                    unity_pos = subprocess.check_output([\n                        '/usr/bin/dconf', 'read', '/com/canonical/unity/launcher/launcher-position'\n                    ]) or \"Left\"\n                    # found = True\n                except Exception as e:\n                    # in case of error, just ignore it, 'found' will not be set to True and so\n                    # we execute the fallback\n                    pass\n            # FIXME: remove self.client dependency\n            # if not found:\n            #     # Fallback: try to bet from gconf\n            #     unity_hide = self.client.get_int(\n            #         KEY('/apps/compiz-1/plugins/unityshell/screen0/options/launcher_hide_mode')\n            #     )\n            #     unity_icon_size = self.client.get_int(\n            #         KEY('/apps/compiz-1/plugins/unityshell/screen0/options/icon_size')\n            #     )\n            #     unity_dock = unity_icon_size + 17\n\n            # launcher_hide_mode = 1 => autohide\n            # only adjust guake window width if Unity dock is positioned \"Left\" or \"Right\"\n            if unity_hide != 1 and unity_pos not in (\"Left\", \"Right\"):\n                log.debug(\n                    \"correcting window width because of launcher position %s \"\n                    \"and width %s (from %s to %s)\", unity_pos, unity_dock, window_rect.width,\n                    window_rect.width - unity_dock\n                )\n\n                window_rect.width = window_rect.width - unity_dock\n\n        total_width = window_rect.width\n        total_height = window_rect.height\n\n        log.debug(\"Correcteed monitor size:\")\n        log.debug(\"  total_width: %s\", total_width)\n        log.debug(\"  total_height: %s\", total_height)\n\n        window_rect.height = int(float(window_rect.height) * float(height_percents) / 100.0)\n        window_rect.width = int(float(window_rect.width) * float(width_percents) / 100.0)\n\n        if window_rect.width < total_width:\n            if halignment == ALIGN_CENTER:\n                # log.debug(\"aligning to center!\")\n                window_rect.x += (total_width - window_rect.width) / 2\n            elif halignment == ALIGN_LEFT:\n                # log.debug(\"aligning to left!\")\n                window_rect.x += 0 + hdisplacement\n            elif halignment == ALIGN_RIGHT:\n                # log.debug(\"aligning to right!\")\n                window_rect.x += total_width - window_rect.width - hdisplacement\n        if window_rect.height < total_height:\n            if valignment == ALIGN_BOTTOM:\n                window_rect.y += (total_height - window_rect.height)\n\n        if valignment == ALIGN_TOP:\n            window_rect.y += vdisplacement\n        elif valignment == ALIGN_BOTTOM:\n            window_rect.y -= vdisplacement\n\n        if width_percents == 100 and height_percents == 100:\n            log.debug(\"MAXIMIZING MAIN WINDOW\")\n            window.maximize()\n        elif not FullscreenManager(settings, window).is_fullscreen():\n            log.debug(\"RESIZING MAIN WINDOW TO THE FOLLOWING VALUES:\")\n            window.unmaximize()\n            log.debug(\"  window_rect.x: %s\", window_rect.x)\n            log.debug(\"  window_rect.y: %s\", window_rect.y)\n            log.debug(\"  window_rect.height: %s\", window_rect.height)\n            log.debug(\"  window_rect.width: %s\", window_rect.width)\n            # Note: move_resize is only on GTK3\n            window.resize(window_rect.width, window_rect.height)\n            window.move(window_rect.x, window_rect.y)\n            window.move(window_rect.x, window_rect.y)\n            log.debug(\"Updated window position: %r\", window.get_position())\n\n        return window_rect"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the final screen number for the main window of guake.", "response": "def get_final_window_monitor(cls, settings, window):\n        \"\"\"Gets the final screen number for the main window of guake.\n        \"\"\"\n\n        screen = window.get_screen()\n\n        # fetch settings\n        use_mouse = settings.general.get_boolean('mouse-display')\n        dest_screen = settings.general.get_int('display-n')\n\n        if use_mouse:\n\n            # TODO PORT get_pointer is deprecated\n            # https://developer.gnome.org/gtk3/stable/GtkWidget.html#gtk-widget-get-pointer\n            win, x, y, _ = screen.get_root_window().get_pointer()\n            dest_screen = screen.get_monitor_at_point(x, y)\n\n        # If Guake is configured to use a screen that is not currently attached,\n        # default to 'primary display' option.\n        n_screens = screen.get_n_monitors()\n        if dest_screen > n_screens - 1:\n            settings.general.set_boolean('mouse-display', False)\n            settings.general.set_int('display-n', dest_screen)\n            dest_screen = screen.get_primary_monitor()\n\n        # Use primary display if configured\n        if dest_screen == ALWAYS_ON_PRIMARY:\n            dest_screen = screen.get_primary_monitor()\n\n        return dest_screen"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef quit(self):\n        # Stop an open \"close tab\" dialog from obstructing a quit\n        response = self.run() == Gtk.ResponseType.YES\n        self.destroy()\n        # Keep Guake focussed after dismissing tab-close prompt\n        # if tab == -1:\n        #     self.window.present()\n        return response", "response": "Run the are you sure dialog for quitting Guake\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add_search_box(self):\n        self.search_revealer = Gtk.Revealer()\n        self.search_frame = Gtk.Frame(name='search-frame')\n        self.search_box = Gtk.HBox()\n\n        # Search\n        self.search_close_btn = Gtk.Button()\n        self.search_close_btn.set_can_focus(False)\n        close_icon = Gio.ThemedIcon(name='window-close-symbolic')\n        close_image = Gtk.Image.new_from_gicon(close_icon, Gtk.IconSize.BUTTON)\n        self.search_close_btn.set_image(close_image)\n        self.search_entry = Gtk.SearchEntry()\n        self.search_prev_btn = Gtk.Button()\n        self.search_prev_btn.set_can_focus(False)\n        prev_icon = Gio.ThemedIcon(name='go-up-symbolic')\n        prev_image = Gtk.Image.new_from_gicon(prev_icon, Gtk.IconSize.BUTTON)\n        self.search_prev_btn.set_image(prev_image)\n        self.search_next_btn = Gtk.Button()\n        self.search_next_btn.set_can_focus(False)\n        next_icon = Gio.ThemedIcon(name='go-down-symbolic')\n        next_image = Gtk.Image.new_from_gicon(next_icon, Gtk.IconSize.BUTTON)\n        self.search_next_btn.set_image(next_image)\n\n        # Pack into box\n        self.search_box.pack_start(self.search_close_btn, False, False, 0)\n        self.search_box.pack_start(self.search_entry, False, False, 0)\n        self.search_box.pack_start(self.search_prev_btn, False, False, 0)\n        self.search_box.pack_start(self.search_next_btn, False, False, 0)\n\n        # Add into frame\n        self.search_frame.add(self.search_box)\n\n        # Frame\n        self.search_frame.set_margin_end(12)\n        self.search_frame.get_style_context().add_class('background')\n        css_provider = Gtk.CssProvider()\n        css_provider.load_from_data(\n            b'#search-frame border {'\n            b'    padding: 5px 5px 5px 5px;'\n            b'    border: none;'\n            b'}'\n        )\n        Gtk.StyleContext.add_provider_for_screen(\n            Gdk.Screen.get_default(), css_provider, Gtk.STYLE_PROVIDER_PRIORITY_APPLICATION\n        )\n\n        # Add to revealer\n        self.search_revealer.add(self.search_frame)\n        self.search_revealer.set_transition_duration(500)\n        self.search_revealer.set_transition_type(Gtk.RevealerTransitionType.CROSSFADE)\n        self.search_revealer.set_valign(Gtk.Align.END)\n        self.search_revealer.set_halign(Gtk.Align.END)\n\n        # Welcome to the overlay\n        self.add_overlay(self.search_revealer)\n\n        # Events\n        self.search_entry.connect('key-press-event', self.on_search_entry_keypress)\n        self.search_entry.connect('changed', self.set_search)\n        self.search_entry.connect('activate', self.do_search)\n        self.search_entry.connect('focus-in-event', self.on_search_entry_focus_in)\n        self.search_entry.connect('focus-out-event', self.on_search_entry_focus_out)\n        self.search_next_btn.connect('clicked', self.on_search_next_clicked)\n        self.search_prev_btn.connect('clicked', self.on_search_prev_clicked)\n        self.search_close_btn.connect('clicked', self.close_search_box)\n        self.search_prev = True", "response": "Add the search box to the internal list of the available resources."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_terminal(self, terminal):\n        if self.terminal is not None:\n            raise RuntimeError(\"TerminalBox: terminal already set\")\n        self.terminal = terminal\n        self.terminal.connect(\"grab-focus\", self.on_terminal_focus)\n        self.terminal.connect(\"button-press-event\", self.on_button_press, None)\n        self.terminal.connect('child-exited', self.on_terminal_exited)\n\n        self.pack_start(self.terminal, True, True, 0)\n        self.terminal.show()\n        self.add_scroll_bar()", "response": "Sets the terminal widget."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_prefix_actions(self, prefix_actions_proxy):\n        prefix_s = \"prefix_\"\n        prefix_pos = len(prefix_s)\n\n        def is_method(t):\n            return callable(t[1])\n\n        def is_prefix_action(t):\n            return t[0].startswith(prefix_s)\n\n        def drop_prefix(k, w):\n            return (k[prefix_pos:], w)\n\n        members_t = inspect.getmembers(prefix_actions_proxy)\n        methods_t = filter(is_method, members_t)\n        prefix_actions_t = filter(is_prefix_action, methods_t)\n        prefix_actions_d = dict(map(drop_prefix, prefix_actions_t))\n\n        for widget in self.get_widgets():\n            prefixes = gtk.Widget.get_data(widget, \"prefixes\")\n            if prefixes:\n                for prefix in prefixes:\n                    if prefix in prefix_actions_d:\n                        prefix_action = prefix_actions_d[prefix]\n                        prefix_action(widget)", "response": "Adds a set of prefix actions to the internal list of all the widgets that have a prefix."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef custom_handler(self, glade, function_name, widget_name, str1, str2, int1, int2):\n        try:\n            handler = getattr(self, function_name)\n            return handler(str1, str2, int1, int2)\n        except AttributeError:\n            return None", "response": "Generic handler for creating custom widgets and custom widgets of a SimpleGladeApp."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexecute the command in the tab.", "response": "def execute_command(self, command, tab=None):\n        # TODO DBUS_ONLY\n        \"\"\"Execute the `command' in the `tab'. If tab is None, the\n        command will be executed in the currently selected\n        tab. Command should end with '\\n', otherwise it will be\n        appended to the string.\n        \"\"\"\n        # TODO CONTEXTMENU this has to be rewriten and only serves the\n        # dbus interface, maybe this should be moved to dbusinterface.py\n        if not self.get_notebook().has_page():\n            self.add_tab()\n\n        if command[-1] != '\\n':\n            command += '\\n'\n\n        terminal = self.get_notebook().get_current_terminal()\n        terminal.feed_child(command)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexecute the command in the tab whose terminal has the tab_uuid uuid", "response": "def execute_command_by_uuid(self, tab_uuid, command):\n        # TODO DBUS_ONLY\n        \"\"\"Execute the `command' in the tab whose terminal has the `tab_uuid' uuid\n        \"\"\"\n        if command[-1] != '\\n':\n            command += '\\n'\n        try:\n            tab_uuid = uuid.UUID(tab_uuid)\n            page_index, = (\n                index for index, t in enumerate(self.get_notebook().iter_terminals())\n                if t.get_uuid() == tab_uuid\n            )\n        except ValueError:\n            pass\n        else:\n            terminals = self.get_notebook().get_terminals_for_page(page_index)\n            for current_vte in terminals:\n                current_vte.feed_child(command)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhides terminal main window when it loses the focus and if the window_losefocus gconf variable is True.", "response": "def on_window_losefocus(self, window, event):\n        \"\"\"Hides terminal main window when it loses the focus and if\n        the window_losefocus gconf variable is True.\n        \"\"\"\n        if not HidePrevention(self.window).may_hide():\n            return\n\n        value = self.settings.general.get_boolean('window-losefocus')\n        visible = window.get_property('visible')\n        self.losefocus_time = get_server_time(self.window)\n        if visible and value:\n            log.info(\"Hiding on focus lose\")\n            self.hide()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef show_menu(self, status_icon, button, activate_time):\n        menu = self.get_widget('tray-menu')\n        menu.popup(None, None, None, Gtk.StatusIcon.position_menu, button, activate_time)", "response": "Show the tray icon menu."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntoggles the main window visibility and restores the focus to the terminal", "response": "def show_hide(self, *args):\n        \"\"\"Toggles the main window visibility\n        \"\"\"\n        log.debug(\"Show_hide called\")\n        if self.forceHide:\n            self.forceHide = False\n            return\n\n        if not HidePrevention(self.window).may_hide():\n            return\n\n        if not self.win_prepare():\n            return\n\n        if not self.window.get_property('visible'):\n            log.info(\"Showing the terminal\")\n            self.show()\n            self.set_terminal_focus()\n            return\n\n        # Disable the focus_if_open feature\n        #  - if doesn't work seamlessly on all system\n        #  - self.window.window.get_state doesn't provides us the right information on all\n        #    systems, especially on MATE/XFCE\n        #\n        # if self.client.get_bool(KEY('/general/focus_if_open')):\n        #     restore_focus = False\n        #     if self.window.window:\n        #         state = int(self.window.window.get_state())\n        #         if ((state & GDK_WINDOW_STATE_STICKY or\n        #                 state & GDK_WINDOW_STATE_WITHDRAWN\n        #              )):\n        #             restore_focus = True\n        #     else:\n        #         restore_focus = True\n        # if not self.hidden:\n        # restore_focus = True\n        #     if restore_focus:\n        #         log.debug(\"DBG: Restoring the focus to the terminal\")\n        #         self.hide()\n        #         self.show()\n        #         self.window.window.focus()\n        #         self.set_terminal_focus()\n        #         return\n\n        log.info(\"Hiding the terminal\")\n        self.hide()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef show(self):\n        self.hidden = False\n\n        # setting window in all desktops\n\n        window_rect = RectCalculator.set_final_window_rect(self.settings, self.window)\n        self.window.stick()\n\n        # add tab must be called before window.show to avoid a\n        # blank screen before adding the tab.\n        if not self.get_notebook().has_page():\n            self.add_tab()\n\n        self.window.set_keep_below(False)\n        self.window.show_all()\n        # this is needed because self.window.show_all() results in showing every\n        # thing which includes the scrollbar too\n        self.settings.general.triggerOnChangedValue(self.settings.general, \"use-scrollbar\")\n\n        # move the window even when in fullscreen-mode\n        log.debug(\"Moving window to: %r\", window_rect)\n        self.window.move(window_rect.x, window_rect.y)\n\n        # this works around an issue in fluxbox\n        if not FullscreenManager(self.settings, self.window).is_fullscreen():\n            self.settings.general.triggerOnChangedValue(self.settings.general, 'window-height')\n\n        time = get_server_time(self.window)\n\n        # TODO PORT this\n        # When minized, the window manager seems to refuse to resume\n        # log.debug(\"self.window: %s. Dir=%s\", type(self.window), dir(self.window))\n        # is_iconified = self.is_iconified()\n        # if is_iconified:\n        #     log.debug(\"Is iconified. Ubuntu Trick => \"\n        #               \"removing skip_taskbar_hint and skip_pager_hint \"\n        #               \"so deiconify can work!\")\n        #     self.get_widget('window-root').set_skip_taskbar_hint(False)\n        #     self.get_widget('window-root').set_skip_pager_hint(False)\n        #     self.get_widget('window-root').set_urgency_hint(False)\n        #     log.debug(\"get_skip_taskbar_hint: {}\".format(\n        #         self.get_widget('window-root').get_skip_taskbar_hint()))\n        #     log.debug(\"get_skip_pager_hint: {}\".format(\n        #         self.get_widget('window-root').get_skip_pager_hint()))\n        #     log.debug(\"get_urgency_hint: {}\".format(\n        #         self.get_widget('window-root').get_urgency_hint()))\n        #     glib.timeout_add_seconds(1, lambda: self.timeout_restore(time))\n        #\n\n        log.debug(\"order to present and deiconify\")\n        self.window.present()\n        self.window.deiconify()\n        self.window.show()\n        self.window.get_window().focus(time)\n        self.window.set_type_hint(Gdk.WindowTypeHint.DOCK)\n        self.window.set_type_hint(Gdk.WindowTypeHint.NORMAL)\n\n        # log.debug(\"Restoring skip_taskbar_hint and skip_pager_hint\")\n        # if is_iconified:\n        #     self.get_widget('window-root').set_skip_taskbar_hint(False)\n        #     self.get_widget('window-root').set_skip_pager_hint(False)\n        #     self.get_widget('window-root').set_urgency_hint(False)\n\n        # This is here because vte color configuration works only after the\n        # widget is shown.\n\n        self.settings.styleFont.triggerOnChangedValue(self.settings.styleFont, 'color')\n        self.settings.styleBackground.triggerOnChangedValue(self.settings.styleBackground, 'color')\n\n        log.debug(\"Current window position: %r\", self.window.get_position())\n\n        self.execute_hook('show')", "response": "Shows the main window and grabs the focus on it."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hide(self):\n        if not HidePrevention(self.window).may_hide():\n            return\n        self.hidden = True\n        self.get_widget('window-root').unstick()\n        self.window.hide()", "response": "Hides the main window of the terminal and sets the visible\n        flag to False."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload the configuration from the settings file.", "response": "def load_config(self):\n        \"\"\"\"Just a proxy for all the configuration stuff.\n        \"\"\"\n\n        self.settings.general.triggerOnChangedValue(self.settings.general, 'use-trayicon')\n        self.settings.general.triggerOnChangedValue(self.settings.general, 'prompt-on-quit')\n        self.settings.general.triggerOnChangedValue(self.settings.general, 'prompt-on-close-tab')\n        self.settings.general.triggerOnChangedValue(self.settings.general, 'window-tabbar')\n        self.settings.general.triggerOnChangedValue(self.settings.general, 'mouse-display')\n        self.settings.general.triggerOnChangedValue(self.settings.general, 'display-n')\n        self.settings.general.triggerOnChangedValue(self.settings.general, 'window-ontop')\n        if not FullscreenManager(self.settings, self.window).is_fullscreen():\n            self.settings.general.triggerOnChangedValue(self.settings.general, 'window-height')\n            self.settings.general.triggerOnChangedValue(self.settings.general, 'window-width')\n        self.settings.general.triggerOnChangedValue(self.settings.general, 'use-scrollbar')\n        self.settings.general.triggerOnChangedValue(self.settings.general, 'history-size')\n        self.settings.general.triggerOnChangedValue(self.settings.general, 'infinite-history')\n        self.settings.general.triggerOnChangedValue(self.settings.general, 'use-vte-titles')\n        self.settings.general.triggerOnChangedValue(self.settings.general, 'set-window-title')\n        self.settings.general.triggerOnChangedValue(self.settings.general, 'abbreviate-tab-names')\n        self.settings.general.triggerOnChangedValue(self.settings.general, 'max-tab-name-length')\n        self.settings.general.triggerOnChangedValue(self.settings.general, 'quick-open-enable')\n        self.settings.general.triggerOnChangedValue(\n            self.settings.general, 'quick-open-command-line'\n        )\n        self.settings.style.triggerOnChangedValue(self.settings.style, 'cursor-shape')\n        self.settings.styleFont.triggerOnChangedValue(self.settings.styleFont, 'style')\n        self.settings.styleFont.triggerOnChangedValue(self.settings.styleFont, 'palette')\n        self.settings.styleFont.triggerOnChangedValue(self.settings.styleFont, 'palette-name')\n        self.settings.styleFont.triggerOnChangedValue(self.settings.styleFont, 'allow-bold')\n        self.settings.styleBackground.triggerOnChangedValue(\n            self.settings.styleBackground, 'transparency'\n        )\n        self.settings.general.triggerOnChangedValue(self.settings.general, 'use-default-font')\n        self.settings.general.triggerOnChangedValue(self.settings.general, 'compat-backspace')\n        self.settings.general.triggerOnChangedValue(self.settings.general, 'compat-delete')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef accel_quit(self, *args):\n        procs = self.notebook_manager.get_running_fg_processes_count()\n        tabs = self.notebook_manager.get_n_pages()\n        notebooks = self.notebook_manager.get_n_notebooks()\n        prompt_cfg = self.settings.general.get_boolean('prompt-on-quit')\n        prompt_tab_cfg = self.settings.general.get_int('prompt-on-close-tab')\n        # \"Prompt on tab close\" config overrides \"prompt on quit\" config\n        if prompt_cfg or (prompt_tab_cfg == 1 and procs > 0) or (prompt_tab_cfg == 2):\n            log.debug(\"Remaining procs=%r\", procs)\n            if PromptQuitDialog(self.window, procs, tabs, notebooks).quit():\n                log.info(\"Quitting Guake\")\n                Gtk.main_quit()\n        else:\n            log.info(\"Quitting Guake\")\n            Gtk.main_quit()", "response": "Callback to prompt the user whether to quit Guake or not."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef accel_reset_terminal(self, *args):\n        # TODO KEYBINDINGS ONLY\n        \"\"\"Callback to reset and clean the terminal\"\"\"\n        HidePrevention(self.window).prevent()\n        current_term = self.get_notebook().get_current_terminal()\n        current_term.reset(True, True)\n        HidePrevention(self.window).allow()\n        return True", "response": "Callback to reset and clean the terminal"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef accel_zoom_out(self, *args):\n        for term in self.get_notebook().iter_terminals():\n            term.decrease_font_size()\n        return True", "response": "Callback to zoom out."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef accel_increase_height(self, *args):\n        height = self.settings.general.get_int('window-height')\n        self.settings.general.set_int('window-height', min(height + 2, 100))\n        return True", "response": "Callback to increase height."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef accel_increase_transparency(self, *args):\n        transparency = self.settings.styleBackground.get_int('transparency')\n        if int(transparency) - 2 > 0:\n            self.settings.styleBackground.set_int('transparency', int(transparency) - 2)\n        return True", "response": "Callback to increase transparency."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef accel_decrease_transparency(self, *args):\n        transparency = self.settings.styleBackground.get_int('transparency')\n        if int(transparency) + 2 < MAX_TRANSPARENCY:\n            self.settings.styleBackground.set_int('transparency', int(transparency) + 2)\n        return True", "response": "Callback to decrease transparency."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef accel_move_tab_left(self, *args):\n        # TODO KEYBINDINGS ONLY\n        \"\"\" Callback to move a tab to the left \"\"\"\n        pos = self.get_notebook().get_current_page()\n        if pos != 0:\n            self.move_tab(pos, pos - 1)\n        return True", "response": "Callback to move a tab to the left"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef gen_accel_switch_tabN(self, N):\n\n        def callback(*args):\n            if 0 <= N < self.get_notebook().get_n_pages():\n                self.get_notebook().set_current_page(N)\n            return True\n\n        return callback", "response": "Generates callback which will be called by accel key to go to the Nth tab."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating labels on all tabs. This is required when self. abbreviate changes .", "response": "def recompute_tabs_titles(self):\n        \"\"\"Updates labels on all tabs. This is required when `self.abbreviate`\n        changes\n        \"\"\"\n        use_vte_titles = self.settings.general.get_boolean(\"use-vte-titles\")\n        if not use_vte_titles:\n            return\n\n        # TODO NOTEBOOK this code only works if there is only one terminal in a\n        # page, this need to be rewritten\n        for terminal in self.get_notebook().iter_terminals():\n            page_num = self.get_notebook().page_num(terminal.get_parent())\n            self.get_notebook().rename_page(page_num, self.compute_tab_title(terminal), False)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nabbreviating and cut vte terminal title when necessary", "response": "def compute_tab_title(self, vte):\n        \"\"\"Abbreviate and cut vte terminal title when necessary\n        \"\"\"\n        vte_title = vte.get_window_title() or _(\"Terminal\")\n        try:\n            current_directory = vte.get_current_directory()\n            if self.abbreviate and vte_title.endswith(current_directory):\n                parts = current_directory.split('/')\n                parts = [s[:1] for s in parts[:-1]] + [parts[-1]]\n                vte_title = vte_title[:len(vte_title) - len(current_directory)] + '/'.join(parts)\n        except OSError:\n            pass\n        return TabNameUtils.shorten(vte_title, self.settings)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncloses the current tab.", "response": "def close_tab(self, *args):\n        \"\"\"Closes the current tab.\n        \"\"\"\n        prompt_cfg = self.settings.general.get_int('prompt-on-close-tab')\n        self.get_notebook().delete_page_current(prompt=prompt_cfg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrename an already added tab by its UUID", "response": "def rename_tab_uuid(self, term_uuid, new_text, user_set=True):\n        \"\"\"Rename an already added tab by its UUID\n        \"\"\"\n        term_uuid = uuid.UUID(term_uuid)\n        page_index, = (\n            index for index, t in enumerate(self.get_notebook().iter_terminals())\n            if t.get_uuid() == term_uuid\n        )\n        self.get_notebook().rename_page(page_index, new_text, user_set)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the uuid of the current selected terminal", "response": "def get_selected_uuidtab(self):\n        # TODO DBUS ONLY\n        \"\"\"Returns the uuid of the current selected terminal\n        \"\"\"\n        page_num = self.get_notebook().get_current_page()\n        terminals = self.get_notebook().get_terminals_for_page(page_num)\n        return str(terminals[0].get_uuid())"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsearches for the selected text on the web", "response": "def search_on_web(self, *args):\n        \"\"\"Search for the selected text on the web\n        \"\"\"\n        # TODO KEYBINDINGS ONLY\n        current_term = self.get_notebook().get_current_terminal()\n\n        if current_term.get_has_selection():\n            current_term.copy_clipboard()\n            guake_clipboard = Gtk.Clipboard.get_default(self.window.get_display())\n            search_query = guake_clipboard.wait_for_text()\n            search_query = quote_plus(search_query)\n            if search_query:\n                # TODO search provider should be selectable (someone might\n                # prefer bing.com, the internet is a strange place \u00af\\_(\u30c4)_/\u00af )\n                search_url = \"https://www.google.com/#q={!s}&safe=off\".format(search_query, )\n                Gtk.show_uri(self.window.get_screen(), search_url, get_server_time(self.window))\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef execute_hook(self, event_name):\n        hook = self.settings.hooks.get_string('{!s}'.format(event_name))\n        if hook is not None and hook != \"\":\n            hook = hook.split()\n            try:\n                subprocess.Popen(hook)\n            except OSError as oserr:\n                if oserr.errno == 8:\n                    log.error(\"Hook execution failed! Check shebang at first line of %s!\", hook)\n                    log.debug(traceback.format_exc())\n                else:\n                    log.error(str(oserr))\n            except Exception as e:\n                log.error(\"hook execution failed! %s\", e)\n                log.debug(traceback.format_exc())\n            else:\n                log.debug(\"hook on event %s has been executed\", event_name)", "response": "Execute shell commands related to current event_name"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list containing all site - packages directories and site - python directories.", "response": "def getsitepackages():\n    \"\"\"Returns a list containing all global site-packages directories\n    (and possibly site-python).\n\n    For each directory present in the global ``PREFIXES``, this function\n    will find its `site-packages` subdirectory depending on the system\n    environment, and will return a list of full paths.\n    \"\"\"\n    sitepackages = []\n    seen = set()\n\n    for prefix in PREFIXES:\n        if not prefix or prefix in seen:\n            continue\n        seen.add(prefix)\n\n        if sys.platform in ('os2emx', 'riscos'):\n            sitepackages.append(os.path.join(prefix, \"Lib\", \"site-packages\"))\n        elif os.sep == '/':\n            sitepackages.append(\n                os.path.join(prefix, \"lib64\", \"python\" + sys.version[:3], \"site-packages\")\n            )\n            sitepackages.append(\n                os.path.join(prefix, \"lib\", \"python\" + sys.version[:3], \"site-packages\")\n            )\n            sitepackages.append(os.path.join(prefix, \"lib\", \"site-python\"))\n        else:\n            sitepackages.append(prefix)\n            sitepackages.append(os.path.join(prefix, \"lib64\", \"site-packages\"))\n            sitepackages.append(os.path.join(prefix, \"lib\", \"site-packages\"))\n        if sys.platform == \"darwin\":\n            # for framework builds *only* we add the standard Apple\n            # locations.\n            from sysconfig import get_config_var\n            framework = get_config_var(\"PYTHONFRAMEWORK\")\n            if framework:\n                sitepackages.append(\n                    os.path.join(\"/Library\", framework, sys.version[:3], \"site-packages\")\n                )\n    return sitepackages"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_resource_dirs(resource):\n    dirs = [\n        os.path.join(dir, resource) for dir in\n        itertools.chain(GLib.get_system_data_dirs(), GUAKE_THEME_DIR, GLib.get_user_data_dir())\n    ]\n    dirs += [os.path.join(os.path.expanduser(\"~\"), \".{}\".format(resource))]\n\n    return [Path(dir) for dir in dirs if os.path.isdir(dir)]", "response": "Returns a list of all known resource dirs for a given resource."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconfigure all custom properties on the terminal object.", "response": "def configure_terminal(self):\n        \"\"\"Sets all customized properties on the terminal\n        \"\"\"\n        client = self.guake.settings.general\n        word_chars = client.get_string('word-chars')\n        if word_chars:\n            self.set_word_char_exceptions(word_chars)\n        self.set_audible_bell(client.get_boolean('use-audible-bell'))\n        self.set_sensitive(True)\n\n        cursor_blink_mode = self.guake.settings.style.get_int('cursor-blink-mode')\n        self.set_property('cursor-blink-mode', cursor_blink_mode)\n\n        if (Vte.MAJOR_VERSION, Vte.MINOR_VERSION) >= (0, 50):\n            self.set_allow_hyperlink(True)\n\n        if (Vte.MAJOR_VERSION, Vte.MINOR_VERSION) >= (0, 56):\n            try:\n                self.set_bold_is_bright(self.guake.settings.styleFont.get_boolean('bold-is-bright'))\n            except:  # pylint: disable=bare-except\n                log.error(\"set_bold_is_bright not supported by your version of VTE\")\n\n        # TODO PORT is this still the case with the newer vte version?\n        # -- Ubuntu has a patch to libvte which disables mouse scrolling in apps\n        # -- like vim and less by default. If this is the case, enable it back.\n        if hasattr(self, \"set_alternate_screen_scroll\"):\n            self.set_alternate_screen_scroll(True)\n\n        self.set_can_default(True)\n        self.set_can_focus(True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_matches(self):\n        try:\n            # NOTE: PCRE2_UTF | PCRE2_NO_UTF_CHECK | PCRE2_MULTILINE\n            # reference from vte/bindings/vala/app.vala, flags = 0x40080400u\n            # also ref: https://mail.gnome.org/archives/commits-list/2016-September/msg06218.html\n            VTE_REGEX_FLAGS = 0x40080400\n            for expr in TERMINAL_MATCH_EXPRS:\n                tag = self.match_add_regex(\n                    Vte.Regex.new_for_match(expr, len(expr), VTE_REGEX_FLAGS), 0\n                )\n                self.match_set_cursor_type(tag, Gdk.CursorType.HAND2)\n\n            for _useless, match, _otheruseless in QUICK_OPEN_MATCHERS:\n                tag = self.match_add_regex(\n                    Vte.Regex.new_for_match(match, len(match), VTE_REGEX_FLAGS), 0\n                )\n                self.match_set_cursor_type(tag, Gdk.CursorType.HAND2)\n        except (GLib.Error, AttributeError) as e:  # pylint: disable=catching-non-exception\n            try:\n                compile_flag = 0\n                if (Vte.MAJOR_VERSION, Vte.MINOR_VERSION) >= (0, 44):\n                    compile_flag = GLib.RegexCompileFlags.MULTILINE\n                for expr in TERMINAL_MATCH_EXPRS:\n                    tag = self.match_add_gregex(GLib.Regex.new(expr, compile_flag, 0), 0)\n                    self.match_set_cursor_type(tag, Gdk.CursorType.HAND2)\n\n                for _useless, match, _otheruseless in QUICK_OPEN_MATCHERS:\n                    tag = self.match_add_gregex(GLib.Regex.new(match, compile_flag, 0), 0)\n                    self.match_set_cursor_type(tag, Gdk.CursorType.HAND2)\n            except GLib.Error as e:  # pylint: disable=catching-non-exception\n                log.error(\n                    \"ERROR: PCRE2 does not seems to be enabled on your system. \"\n                    \"Quick Edit and other Ctrl+click features are disabled. \"\n                    \"Please update your VTE package or contact your distribution to ask \"\n                    \"to enable regular expression support in VTE. Exception: '%s'\", str(e)\n                )", "response": "Adds all regular expressions declared in\n            to the terminal to make vte\n            highlight text that matches them."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntests if the provided text matches a file on local server.", "response": "def is_file_on_local_server(self, text) -> Tuple[Optional[Path], Optional[int], Optional[int]]:\n        \"\"\"Test if the provided text matches a file on local server\n\n        Supports:\n         - absolute path\n         - relative path (using current working directory)\n         - file:line syntax\n         - file:line:colum syntax\n\n        Args:\n            text (str): candidate for file search\n\n        Returns\n            - Tuple(None, None, None) if the provided text does not match anything\n            - Tuple(file path, None, None) if only a file path is found\n            - Tuple(file path, linenumber, None) if line number is found\n            - Tuple(file path, linenumber, columnnumber) if line and column numbers are found\n        \"\"\"\n        lineno = None\n        colno = None\n        py_func = None\n        # \"<File>:<line>:<col>\"\n        m = re.compile(r\"(.*)\\:(\\d+)\\:(\\d+)$\").match(text)\n        if m:\n            text = m.group(1)\n            lineno = m.group(2)\n            colno = m.group(3)\n        else:\n            # \"<File>:<line>\"\n            m = re.compile(r\"(.*)\\:(\\d+)$\").match(text)\n            if m:\n                text = m.group(1)\n                lineno = m.group(2)\n            else:\n                # \"<File>::<python_function>\"\n                m = re.compile(r\"^(.*)\\:\\:([a-zA-Z0-9\\_]+)$\").match(text)\n                if m:\n                    text = m.group(1)\n                    py_func = m.group(2).strip()\n\n        def find_lineno(text, pt, lineno, py_func):\n            # print(\"text={!r}, pt={!r}, lineno={!r}, py_func={!r}\".format(text,\n            #                                                              pt, lineno, py_func))\n            if lineno:\n                return lineno\n            if not py_func:\n                return\n            with pt.open() as f:\n                for i, line in enumerate(f.readlines()):\n                    if line.startswith(\"def {}\".format(py_func)):\n                        return i + 1\n                        break\n\n        pt = Path(text)\n        log.debug(\"checking file existance: %r\", pt)\n        try:\n            if pt.exists():\n                lineno = find_lineno(text, pt, lineno, py_func)\n                log.info(\"File exists: %r, line=%r\", pt.absolute().as_posix(), lineno)\n                return (pt, lineno, colno)\n            log.debug(\"No file found matching: %r\", text)\n            cwd = self.get_current_directory()\n            pt = Path(cwd) / pt\n            log.debug(\"checking file existance: %r\", pt)\n            if pt.exists():\n                lineno = find_lineno(text, pt, lineno, py_func)\n                log.info(\"File exists: %r, line=%r\", pt.absolute().as_posix(), lineno)\n                return (pt, lineno, colno)\n            log.debug(\"file does not exist: %s\", str(pt))\n        except OSError:\n            log.debug(\"not a file name: %r\", text)\n        return (None, None, None)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef button_press(self, terminal, event):\n        self.matched_value = ''\n        if (Vte.MAJOR_VERSION, Vte.MINOR_VERSION) >= (0, 46):\n            matched_string = self.match_check_event(event)\n        else:\n            matched_string = self.match_check(\n                int(event.x / self.get_char_width()), int(event.y / self.get_char_height())\n            )\n\n        self.found_link = None\n\n        if event.button == 1 and (event.get_state() & Gdk.ModifierType.CONTROL_MASK):\n            if (Vte.MAJOR_VERSION, Vte.MINOR_VERSION) > (0, 50):\n                s = self.hyperlink_check_event(event)\n            else:\n                s = None\n            if s is not None:\n                self._on_ctrl_click_matcher((s, None))\n            elif self.get_has_selection():\n                self.quick_open()\n            elif matched_string and matched_string[0]:\n                self._on_ctrl_click_matcher(matched_string)\n        elif event.button == 3 and matched_string:\n            self.found_link = self.handleTerminalMatch(matched_string)\n            self.matched_value = matched_string[0]", "response": "Handles the button press event in the terminal widget."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_shell(self, pid):\n        try:\n            os.kill(pid, signal.SIGHUP)\n        except OSError:\n            pass\n        num_tries = 30\n\n        while num_tries > 0:\n            try:\n                # Try to wait for the pid to be closed. If it does not\n                # exist anymore, an OSError is raised and we can\n                # safely ignore it.\n                if os.waitpid(pid, os.WNOHANG)[0] != 0:\n                    break\n            except OSError:\n                break\n            sleep(0.1)\n            num_tries -= 1\n\n        if num_tries == 0:\n            try:\n                os.kill(pid, signal.SIGKILL)\n                os.waitpid(pid, 0)\n            except OSError:\n                # if this part of code was reached, means that SIGTERM\n                # did the work and SIGKILL wasnt needed.\n                pass", "response": "This function will kill the shell on a tab and then wait for it to finish."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a type and any value return True if that value is valid.", "response": "def is_valid_value(value, type):\n    # type: (Any, Any) -> List\n    \"\"\"Given a type and any value, return True if that value is valid.\"\"\"\n    if isinstance(type, GraphQLNonNull):\n        of_type = type.of_type\n        if value is None:\n            return [u'Expected \"{}\", found null.'.format(type)]\n\n        return is_valid_value(value, of_type)\n\n    if value is None:\n        return _empty_list\n\n    if isinstance(type, GraphQLList):\n        item_type = type.of_type\n        if not isinstance(value, string_types) and isinstance(value, Iterable):\n            errors = []\n            for i, item in enumerate(value):\n                item_errors = is_valid_value(item, item_type)\n                for error in item_errors:\n                    errors.append(u\"In element #{}: {}\".format(i, error))\n\n            return errors\n\n        else:\n            return is_valid_value(value, item_type)\n\n    if isinstance(type, GraphQLInputObjectType):\n        if not isinstance(value, Mapping):\n            return [u'Expected \"{}\", found not an object.'.format(type)]\n\n        fields = type.fields\n        errors = []\n\n        for provided_field in sorted(value.keys()):\n            if provided_field not in fields:\n                errors.append(u'In field \"{}\": Unknown field.'.format(provided_field))\n\n        for field_name, field in fields.items():\n            subfield_errors = is_valid_value(value.get(field_name), field.type)\n            errors.extend(\n                u'In field \"{}\": {}'.format(field_name, e) for e in subfield_errors\n            )\n\n        return errors\n\n    assert isinstance(type, (GraphQLScalarType, GraphQLEnumType)), \"Must be input type\"\n\n    # Scalar/Enum input checks to ensure the type can parse the value to\n    # a non-null value.\n    parse_result = type.parse_value(value)\n    if parse_result is None:\n        return [u'Expected type \"{}\", found {}.'.format(type, json.dumps(value))]\n\n    return _empty_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a unique id given a GraphQLSchema", "response": "def get_unique_schema_id(schema):\n    # type: (GraphQLSchema) -> str\n    \"\"\"Get a unique id given a GraphQLSchema\"\"\"\n    assert isinstance(schema, GraphQLSchema), (\n        \"Must receive a GraphQLSchema as schema. Received {}\"\n    ).format(repr(schema))\n\n    if schema not in _cached_schemas:\n        _cached_schemas[schema] = sha1(str(schema).encode(\"utf-8\")).hexdigest()\n    return _cached_schemas[schema]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_unique_document_id(query_str):\n    # type: (str) -> str\n    \"\"\"Get a unique id given a query_string\"\"\"\n    assert isinstance(query_str, string_types), (\n        \"Must receive a string as query_str. Received {}\"\n    ).format(repr(query_str))\n\n    if query_str not in _cached_queries:\n        _cached_queries[query_str] = sha1(str(query_str).encode(\"utf-8\")).hexdigest()\n    return _cached_queries[query_str]", "response": "Get a unique id given a query_string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_key_for_schema_and_document_string(self, schema, request_string):\n        # type: (GraphQLSchema, str) -> int\n        \"\"\"This method returns a unique key given a schema and a request_string\"\"\"\n        if self.use_consistent_hash:\n            schema_id = get_unique_schema_id(schema)\n            document_id = get_unique_document_id(request_string)\n            return hash((schema_id, document_id))\n        return hash((schema, request_string))", "response": "This method returns a unique key given a schema and a request string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngo through all of the implementations of type and interfaces and suggest the types that are referenced by the provided field.", "response": "def get_suggested_type_names(schema, output_type, field_name):\n    \"\"\"Go through all of the implementations of type, as well as the interfaces\n      that they implement. If any of those types include the provided field,\n      suggest them, sorted by how often the type is referenced,  starting\n      with Interfaces.\"\"\"\n\n    if isinstance(output_type, (GraphQLInterfaceType, GraphQLUnionType)):\n        suggested_object_types = []\n        interface_usage_count = OrderedDict()\n        for possible_type in schema.get_possible_types(output_type):\n            if not possible_type.fields.get(field_name):\n                return\n\n            # This object type defines this field.\n            suggested_object_types.append(possible_type.name)\n\n            for possible_interface in possible_type.interfaces:\n                if not possible_interface.fields.get(field_name):\n                    continue\n\n                # This interface type defines this field.\n                interface_usage_count[possible_interface.name] = (\n                    interface_usage_count.get(possible_interface.name, 0) + 1\n                )\n\n        # Suggest interface types based on how common they are.\n        suggested_interface_types = sorted(\n            list(interface_usage_count.keys()),\n            key=lambda k: interface_usage_count[k],\n            reverse=True,\n        )\n\n        # Suggest both interface and object types.\n        suggested_interface_types.extend(suggested_object_types)\n        return suggested_interface_types\n\n    # Otherwise, must be an Object type, which does not have possible fields.\n    return []"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_suggested_field_names(schema, graphql_type, field_name):\n\n    if isinstance(graphql_type, (GraphQLInterfaceType, GraphQLObjectType)):\n        possible_field_names = list(graphql_type.fields.keys())\n\n        return suggestion_list(field_name, possible_field_names)\n\n    # Otherwise, must be a Union type, which does not define fields.\n    return []", "response": "For the field name provided determine if there are any similar field names\n    that may be the result of a typo."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_code(\n        cls,\n        schema,  # type: GraphQLSchema\n        code,  # type: Union[str, Any]\n        uptodate=None,  # type: Optional[bool]\n        extra_namespace=None,  # type: Optional[Dict[str, Any]]\n    ):\n        # type: (...) -> GraphQLCompiledDocument\n        \"\"\"Creates a GraphQLDocument object from compiled code and the globals.  This\n        is used by the loaders and schema to create a document object.\n        \"\"\"\n        if isinstance(code, string_types):\n            filename = \"<document>\"\n            code = compile(code, filename, \"exec\")\n        namespace = {\"__file__\": code.co_filename}\n        exec(code, namespace)\n        if extra_namespace:\n            namespace.update(extra_namespace)\n        rv = cls._from_namespace(schema, namespace)\n        # rv._uptodate = uptodate\n        return rv", "response": "Creates a GraphQLCompiledDocument object from a compiled code and the globals."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives an invalid input string and a list of valid options returns a filtered list of valid options sorted based on their similarity with the input.", "response": "def suggestion_list(inp, options):\n    \"\"\"\n     Given an invalid input string and a list of valid options, returns a filtered\n     list of valid options sorted based on their similarity with the input.\n    \"\"\"\n    options_by_distance = OrderedDict()\n    input_threshold = len(inp) / 2\n\n    for option in options:\n        distance = lexical_distance(inp, option)\n        threshold = max(input_threshold, len(option) / 2, 1)\n        if distance <= threshold:\n            options_by_distance[option] = distance\n\n    return sorted(\n        list(options_by_distance.keys()), key=lambda k: options_by_distance[k]\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the lexical distance between two strings A and B.", "response": "def lexical_distance(a, b):\n    \"\"\"\n     Computes the lexical distance between strings A and B.\n     The \"distance\" between two strings is given by counting the minimum number\n     of edits needed to transform string A into string B. An edit can be an\n     insertion, deletion, or substitution of a single character, or a swap of two\n     adjacent characters.\n     This distance can be useful for detecting typos in input or sorting\n     @returns distance in number of edits\n    \"\"\"\n\n    d = [[i] for i in range(len(a) + 1)] or []\n    d_len = len(d) or 1\n    for i in range(d_len):\n        for j in range(1, len(b) + 1):\n            if i == 0:\n                d[i].append(j)\n            else:\n                d[i].append(0)\n\n    for i in range(1, len(a) + 1):\n        for j in range(1, len(b) + 1):\n            cost = 0 if a[i - 1] == b[j - 1] else 1\n\n            d[i][j] = min(d[i - 1][j] + 1, d[i][j - 1] + 1, d[i - 1][j - 1] + cost)\n\n            if i > 1 and j < 1 and a[i - 1] == b[j - 2] and a[i - 2] == b[j - 1]:\n                d[i][j] = min(d[i][j], d[i - 2][j - 2] + cost)\n\n    return d[len(a)][len(b)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a tuple of the graphql version.", "response": "def get_complete_version(version=None):\n    \"\"\"Returns a tuple of the graphql version. If version argument is non-empty,\n    then checks for correctness of the tuple provided.\n    \"\"\"\n    if version is None:\n        from graphql import VERSION as version\n    else:\n        assert len(version) == 5\n        assert version[3] in (\"alpha\", \"beta\", \"rc\", \"final\")\n\n    return version"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_token(source, from_position):\n    # type: (Source, int) -> Token\n    \"\"\"Gets the next token from the source starting at the given position.\n\n    This skips over whitespace and comments until it finds the next lexable\n    token, then lexes punctuators immediately or calls the appropriate\n    helper fucntion for more complicated tokens.\"\"\"\n    body = source.body\n    body_length = len(body)\n\n    position = position_after_whitespace(body, from_position)\n\n    if position >= body_length:\n        return Token(TokenKind.EOF, position, position)\n\n    code = char_code_at(body, position)\n    if code:\n        if code < 0x0020 and code not in (0x0009, 0x000A, 0x000D):\n            raise GraphQLSyntaxError(\n                source, position, u\"Invalid character {}.\".format(print_char_code(code))\n            )\n\n        kind = PUNCT_CODE_TO_KIND.get(code)\n        if kind is not None:\n            return Token(kind, position, position + 1)\n\n        if code == 46:  # .\n            if (\n                char_code_at(body, position + 1)\n                == char_code_at(body, position + 2)\n                == 46\n            ):\n                return Token(TokenKind.SPREAD, position, position + 3)\n\n        elif 65 <= code <= 90 or code == 95 or 97 <= code <= 122:\n            # A-Z, _, a-z\n            return read_name(source, position)\n\n        elif code == 45 or 48 <= code <= 57:  # -, 0-9\n            return read_number(source, position, code)\n\n        elif code == 34:  # \"\n            return read_string(source, position)\n\n    raise GraphQLSyntaxError(\n        source, position, u\"Unexpected character {}.\".format(print_char_code(code))\n    )", "response": "Reads the next token from the source starting at the given position."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading from body starting at start_position until it finds a non - whitespace or commented character then returns the position of the next non - whitespace character.", "response": "def position_after_whitespace(body, start_position):\n    # type: (str, int) -> int\n    \"\"\"Reads from body starting at start_position until it finds a\n    non-whitespace or commented character, then returns the position of\n    that character for lexing.\"\"\"\n    body_length = len(body)\n    position = start_position\n    while position < body_length:\n        code = char_code_at(body, position)\n        if code in ignored_whitespace_characters:\n            position += 1\n\n        elif code == 35:  # #, skip comments\n            position += 1\n            while position < body_length:\n                code = char_code_at(body, position)\n                if not (\n                    code is not None\n                    and (code > 0x001F or code == 0x0009)\n                    and code not in (0x000A, 0x000D)\n                ):\n                    break\n\n                position += 1\n        else:\n            break\n    return position"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_number(source, start, first_code):\n    # type: (Source, int, Optional[int]) -> Token\n    r\"\"\"Reads a number token from the source file, either a float\n    or an int depending on whether a decimal point appears.\n\n    Int:   -?(0|[1-9][0-9]*)\n    Float: -?(0|[1-9][0-9]*)(\\.[0-9]+)?((E|e)(+|-)?[0-9]+)?\"\"\"\n    code = first_code\n    body = source.body\n    position = start\n    is_float = False\n\n    if code == 45:  # -\n        position += 1\n        code = char_code_at(body, position)\n\n    if code == 48:  # 0\n        position += 1\n        code = char_code_at(body, position)\n\n        if code is not None and 48 <= code <= 57:\n            raise GraphQLSyntaxError(\n                source,\n                position,\n                u\"Invalid number, unexpected digit after 0: {}.\".format(\n                    print_char_code(code)\n                ),\n            )\n    else:\n        position = read_digits(source, position, code)\n        code = char_code_at(body, position)\n\n    if code == 46:  # .\n        is_float = True\n\n        position += 1\n        code = char_code_at(body, position)\n        position = read_digits(source, position, code)\n        code = char_code_at(body, position)\n\n    if code in (69, 101):  # E e\n        is_float = True\n        position += 1\n        code = char_code_at(body, position)\n        if code in (43, 45):  # + -\n            position += 1\n            code = char_code_at(body, position)\n\n        position = read_digits(source, position, code)\n\n    return Token(\n        TokenKind.FLOAT if is_float else TokenKind.INT,\n        start,\n        position,\n        body[start:position],\n    )", "response": "r Reads a number token from the source file either a float or an int depending on whether a decimal point appears."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread a string token from the source file.", "response": "def read_string(source, start):\n    # type: (Source, int) -> Token\n    \"\"\"Reads a string token from the source file.\n\n    \"([^\"\\\\\\u000A\\u000D\\u2028\\u2029]|(\\\\(u[0-9a-fA-F]{4}|[\"\\\\/bfnrt])))*\"\n    \"\"\"\n    body = source.body\n    body_length = len(body)\n\n    position = start + 1\n    chunk_start = position\n    code = 0  # type: Optional[int]\n    value = []  # type: List[str]\n    append = value.append\n\n    while position < body_length:\n        code = char_code_at(body, position)\n        if code in (\n            None,\n            # LineTerminator\n            0x000A,\n            0x000D,\n            # Quote\n            34,\n        ):\n            break\n\n        if code < 0x0020 and code != 0x0009:  # type: ignore\n            raise GraphQLSyntaxError(\n                source,\n                position,\n                u\"Invalid character within String: {}.\".format(print_char_code(code)),\n            )\n\n        position += 1\n        if code == 92:  # \\\n            append(body[chunk_start : position - 1])\n\n            code = char_code_at(body, position)\n            escaped = ESCAPED_CHAR_CODES.get(code)  # type: ignore\n            if escaped is not None:\n                append(escaped)\n\n            elif code == 117:  # u\n                char_code = uni_char_code(\n                    char_code_at(body, position + 1) or 0,\n                    char_code_at(body, position + 2) or 0,\n                    char_code_at(body, position + 3) or 0,\n                    char_code_at(body, position + 4) or 0,\n                )\n\n                if char_code < 0:\n                    raise GraphQLSyntaxError(\n                        source,\n                        position,\n                        u\"Invalid character escape sequence: \\\\u{}.\".format(\n                            body[position + 1 : position + 5]\n                        ),\n                    )\n\n                append(unichr(char_code))\n                position += 4\n            else:\n                raise GraphQLSyntaxError(\n                    source,\n                    position,\n                    u\"Invalid character escape sequence: \\\\{}.\".format(\n                        unichr(code)  # type: ignore\n                    ),\n                )\n\n            position += 1\n            chunk_start = position\n\n    if code != 34:  # Quote (\")\n        raise GraphQLSyntaxError(source, position, \"Unterminated string\")\n\n    append(body[chunk_start:position])\n    return Token(TokenKind.STRING, start, position + 1, u\"\".join(value))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading an alphanumeric + underscore name from the source.", "response": "def read_name(source, position):\n    # type: (Source, int) -> Token\n    \"\"\"Reads an alphanumeric + underscore name from the source.\n\n    [_A-Za-z][_0-9A-Za-z]*\"\"\"\n    body = source.body\n    body_length = len(body)\n    end = position + 1\n\n    while end != body_length:\n        code = char_code_at(body, end)\n        if not (\n            code is not None\n            and (\n                code == 95\n                or 48 <= code <= 57  # _\n                or 65 <= code <= 90  # 0-9\n                or 97 <= code <= 122  # A-Z  # a-z\n            )\n        ):\n            break\n\n        end += 1\n\n    return Token(TokenKind.NAME, position, end, body[position:end])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncompletes the value of a field.", "response": "def complete_value(\n    exe_context,  # type: ExecutionContext\n    return_type,  # type: Any\n    field_asts,  # type: List[Field]\n    info,  # type: ResolveInfo\n    path,  # type: List[Union[int, str]]\n    result,  # type: Any\n):\n    # type: (...) -> Any\n    \"\"\"\n    Implements the instructions for completeValue as defined in the\n    \"Field entries\" section of the spec.\n\n    If the field type is Non-Null, then this recursively completes the value for the inner type. It throws a field\n    error if that completion returns null, as per the \"Nullability\" section of the spec.\n\n    If the field type is a List, then this recursively completes the value for the inner type on each item in the\n    list.\n\n    If the field type is a Scalar or Enum, ensures the completed value is a legal value of the type by calling the\n    `serialize` method of GraphQL type definition.\n\n    If the field is an abstract type, determine the runtime type of the value and then complete based on that type.\n\n    Otherwise, the field type expects a sub-selection set, and will complete the value by evaluating all\n    sub-selections.\n    \"\"\"\n    # If field type is NonNull, complete for inner type, and throw field error\n    # if result is null.\n    if is_thenable(result):\n        return Promise.resolve(result).then(\n            lambda resolved: complete_value(\n                exe_context, return_type, field_asts, info, path, resolved\n            ),\n            lambda error: Promise.rejected(\n                GraphQLLocatedError(field_asts, original_error=error, path=path)\n            ),\n        )\n\n    # print return_type, type(result)\n    if isinstance(result, Exception):\n        raise GraphQLLocatedError(field_asts, original_error=result, path=path)\n\n    if isinstance(return_type, GraphQLNonNull):\n        return complete_nonnull_value(\n            exe_context, return_type, field_asts, info, path, result\n        )\n\n    # If result is null-like, return null.\n    if result is None:\n        return None\n\n    # If field type is List, complete each item in the list with the inner type\n    if isinstance(return_type, GraphQLList):\n        return complete_list_value(\n            exe_context, return_type, field_asts, info, path, result\n        )\n\n    # If field type is Scalar or Enum, serialize to a valid value, returning\n    # null if coercion is not possible.\n    if isinstance(return_type, (GraphQLScalarType, GraphQLEnumType)):\n        return complete_leaf_value(return_type, path, result)\n\n    if isinstance(return_type, (GraphQLInterfaceType, GraphQLUnionType)):\n        return complete_abstract_value(\n            exe_context, return_type, field_asts, info, path, result\n        )\n\n    if isinstance(return_type, GraphQLObjectType):\n        return complete_object_value(\n            exe_context, return_type, field_asts, info, path, result\n        )\n\n    assert False, u'Cannot complete value of unexpected type \"{}\".'.format(return_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncompletes a list value by completing each item in the list with the inner type.", "response": "def complete_list_value(\n    exe_context,  # type: ExecutionContext\n    return_type,  # type: GraphQLList\n    field_asts,  # type: List[Field]\n    info,  # type: ResolveInfo\n    path,  # type: List[Union[int, str]]\n    result,  # type: Any\n):\n    # type: (...) -> List[Any]\n    \"\"\"\n    Complete a list value by completing each item in the list with the inner type\n    \"\"\"\n    assert isinstance(result, Iterable), (\n        \"User Error: expected iterable, but did not find one \" + \"for field {}.{}.\"\n    ).format(info.parent_type, info.field_name)\n\n    item_type = return_type.of_type\n    completed_results = []\n    contains_promise = False\n\n    index = 0\n    for item in result:\n        completed_item = complete_value_catching_error(\n            exe_context, item_type, field_asts, info, path + [index], item\n        )\n        if not contains_promise and is_thenable(completed_item):\n            contains_promise = True\n\n        completed_results.append(completed_item)\n        index += 1\n\n    return Promise.all(completed_results) if contains_promise else completed_results"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncompletes a leaf value.", "response": "def complete_leaf_value(\n    return_type,  # type: Union[GraphQLEnumType, GraphQLScalarType]\n    path,  # type: List[Union[int, str]]\n    result,  # type: Any\n):\n    # type: (...) -> Union[int, str, float, bool]\n    \"\"\"\n    Complete a Scalar or Enum by serializing to a valid value, returning null if serialization is not possible.\n    \"\"\"\n    assert hasattr(return_type, \"serialize\"), \"Missing serialize method on type\"\n    serialized_result = return_type.serialize(result)\n\n    if serialized_result is None:\n        raise GraphQLError(\n            ('Expected a value of type \"{}\" but ' + \"received: {}\").format(\n                return_type, result\n            ),\n            path=path,\n        )\n    return serialized_result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef complete_abstract_value(\n    exe_context,  # type: ExecutionContext\n    return_type,  # type: Union[GraphQLInterfaceType, GraphQLUnionType]\n    field_asts,  # type: List[Field]\n    info,  # type: ResolveInfo\n    path,  # type: List[Union[int, str]]\n    result,  # type: Any\n):\n    # type: (...) -> Dict[str, Any]\n    \"\"\"\n    Complete an value of an abstract type by determining the runtime type of that value, then completing based\n    on that type.\n    \"\"\"\n    runtime_type = None  # type: Union[str, GraphQLObjectType, None]\n\n    # Field type must be Object, Interface or Union and expect sub-selections.\n    if isinstance(return_type, (GraphQLInterfaceType, GraphQLUnionType)):\n        if return_type.resolve_type:\n            runtime_type = return_type.resolve_type(result, info)\n        else:\n            runtime_type = get_default_resolve_type_fn(result, info, return_type)\n\n    if isinstance(runtime_type, string_types):\n        runtime_type = info.schema.get_type(runtime_type)  # type: ignore\n\n    if not isinstance(runtime_type, GraphQLObjectType):\n        raise GraphQLError(\n            (\n                \"Abstract type {} must resolve to an Object type at runtime \"\n                + 'for field {}.{} with value \"{}\", received \"{}\".'\n            ).format(\n                return_type, info.parent_type, info.field_name, result, runtime_type\n            ),\n            field_asts,\n        )\n\n    if not exe_context.schema.is_possible_type(return_type, runtime_type):\n        raise GraphQLError(\n            u'Runtime Object type \"{}\" is not a possible type for \"{}\".'.format(\n                runtime_type, return_type\n            ),\n            field_asts,\n        )\n\n    return complete_object_value(\n        exe_context, runtime_type, field_asts, info, path, result\n    )", "response": "Complete an abstract value."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef complete_object_value(\n    exe_context,  # type: ExecutionContext\n    return_type,  # type: GraphQLObjectType\n    field_asts,  # type: List[Field]\n    info,  # type: ResolveInfo\n    path,  # type: List[Union[int, str]]\n    result,  # type: Any\n):\n    # type: (...) -> Dict[str, Any]\n    \"\"\"\n    Complete an Object value by evaluating all sub-selections.\n    \"\"\"\n    if return_type.is_type_of and not return_type.is_type_of(result, info):\n        raise GraphQLError(\n            u'Expected value of type \"{}\" but got: {}.'.format(\n                return_type, type(result).__name__\n            ),\n            field_asts,\n        )\n\n    # Collect sub-fields to execute to complete this value.\n    subfield_asts = exe_context.get_sub_fields(return_type, field_asts)\n    return execute_fields(exe_context, return_type, result, subfield_asts, path, info)", "response": "Complete an object value by evaluating all sub - selections."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef complete_nonnull_value(\n    exe_context,  # type: ExecutionContext\n    return_type,  # type: GraphQLNonNull\n    field_asts,  # type: List[Field]\n    info,  # type: ResolveInfo\n    path,  # type: List[Union[int, str]]\n    result,  # type: Any\n):\n    # type: (...) -> Any\n    \"\"\"\n    Complete a NonNull value by completing the inner type\n    \"\"\"\n    completed = complete_value(\n        exe_context, return_type.of_type, field_asts, info, path, result\n    )\n    if completed is None:\n        raise GraphQLError(\n            \"Cannot return null for non-nullable field {}.{}.\".format(\n                info.parent_type, info.field_name\n            ),\n            field_asts,\n            path=path,\n        )\n\n    return completed", "response": "Complete a NonNull value by completing the inner type\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a type and a value AST node return the runtime value.", "response": "def value_from_ast(value_ast, type, variables=None):\n    # type: (Optional[Node], GraphQLType, Optional[Dict[str, Union[List, Dict, int, float, bool, str, None]]]) -> Union[List, Dict, int, float, bool, str, None]\n    \"\"\"Given a type and a value AST node known to match this type, build a\n    runtime value.\"\"\"\n    if isinstance(type, GraphQLNonNull):\n        # Note: we're not checking that the result of coerceValueAST is non-null.\n        # We're assuming that this query has been validated and the value used here is of the correct type.\n        return value_from_ast(value_ast, type.of_type, variables)\n\n    if value_ast is None:\n        return None\n\n    if isinstance(value_ast, ast.Variable):\n        variable_name = value_ast.name.value\n        if not variables or variable_name not in variables:\n            return None\n\n        # Note: we're not doing any checking that this variable is correct. We're assuming that this query\n        # has been validated and the variable usage here is of the correct type.\n        return variables.get(variable_name)\n\n    if isinstance(type, GraphQLList):\n        item_type = type.of_type\n        if isinstance(value_ast, ast.ListValue):\n            return [\n                value_from_ast(item_ast, item_type, variables)\n                for item_ast in value_ast.values\n            ]\n\n        else:\n            return [value_from_ast(value_ast, item_type, variables)]\n\n    if isinstance(type, GraphQLInputObjectType):\n        fields = type.fields\n        if not isinstance(value_ast, ast.ObjectValue):\n            return None\n\n        field_asts = {}\n\n        for field_ast in value_ast.fields:\n            field_asts[field_ast.name.value] = field_ast\n\n        obj = {}\n        for field_name, field in fields.items():\n            if field_name not in field_asts:\n                if field.default_value is not None:\n                    # We use out_name as the output name for the\n                    # dict if exists\n                    obj[field.out_name or field_name] = field.default_value\n\n                continue\n\n            field_ast = field_asts[field_name]\n            field_value_ast = field_ast.value\n            field_value = value_from_ast(field_value_ast, field.type, variables)\n\n            # We use out_name as the output name for the\n            # dict if exists\n            obj[field.out_name or field_name] = field_value\n\n        return type.create_container(obj)\n\n    assert isinstance(type, (GraphQLScalarType, GraphQLEnumType)), \"Must be input type\"\n\n    return type.parse_literal(value_ast)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstops the task thread.", "response": "def stop(self, timeout=None):\n        \"\"\"\n        Stops the task thread. Synchronous!\n        \"\"\"\n        with self._lock:\n            if self._thread:\n                self._queue.put_nowait(self._terminator)\n                self._thread.join(timeout=timeout)\n                self._thread = None\n                self._thread_for_pid = None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef document_from_string(self, schema, request_string):\n        # type: (GraphQLSchema, str) -> GraphQLDocument\n        \"\"\"This method returns a GraphQLQuery (from cache if present)\"\"\"\n        key = self.get_key_for_schema_and_document_string(schema, request_string)\n        if key not in self.cache_map:\n            # We return from the fallback\n            self.cache_map[key] = self.fallback_backend.document_from_string(\n                schema, request_string\n            )\n            # We ensure the main backend response is in the queue\n            self.get_worker().queue(self.queue_backend, key, schema, request_string)\n\n        return self.cache_map[key]", "response": "This method returns a GraphQLQuery from the cache if present otherwise returns from the fallback backend if present."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extend_schema(schema, documentAST=None):\n\n    assert isinstance(schema, GraphQLSchema), \"Must provide valid GraphQLSchema\"\n    assert documentAST and isinstance(\n        documentAST, ast.Document\n    ), \"Must provide valid Document AST\"\n\n    # Collect the type definitions and extensions found in the document.\n    type_definition_map = {}\n    type_extensions_map = defaultdict(list)\n\n    for _def in documentAST.definitions:\n        if isinstance(\n            _def,\n            (\n                ast.ObjectTypeDefinition,\n                ast.InterfaceTypeDefinition,\n                ast.EnumTypeDefinition,\n                ast.UnionTypeDefinition,\n                ast.ScalarTypeDefinition,\n                ast.InputObjectTypeDefinition,\n            ),\n        ):\n            # Sanity check that none of the defined types conflict with the\n            # schema's existing types.\n            type_name = _def.name.value\n            if schema.get_type(type_name):\n                raise GraphQLError(\n                    (\n                        'Type \"{}\" already exists in the schema. It cannot also '\n                        + \"be defined in this type definition.\"\n                    ).format(type_name),\n                    [_def],\n                )\n\n            type_definition_map[type_name] = _def\n        elif isinstance(_def, ast.TypeExtensionDefinition):\n            # Sanity check that this type extension exists within the\n            # schema's existing types.\n            extended_type_name = _def.definition.name.value\n            existing_type = schema.get_type(extended_type_name)\n            if not existing_type:\n                raise GraphQLError(\n                    (\n                        'Cannot extend type \"{}\" because it does not '\n                        + \"exist in the existing schema.\"\n                    ).format(extended_type_name),\n                    [_def.definition],\n                )\n            if not isinstance(existing_type, GraphQLObjectType):\n                raise GraphQLError(\n                    'Cannot extend non-object type \"{}\".'.format(extended_type_name),\n                    [_def.definition],\n                )\n\n            type_extensions_map[extended_type_name].append(_def)\n\n    # Below are functions used for producing this schema that have closed over\n    # this scope and have access to the schema, cache, and newly defined types.\n\n    def get_type_from_def(type_def):\n        type = _get_named_type(type_def.name)\n        assert type, \"Invalid schema\"\n        return type\n\n    def get_type_from_AST(astNode):\n        type = _get_named_type(astNode.name.value)\n        if not type:\n            raise GraphQLError(\n                (\n                    'Unknown type: \"{}\". Ensure that this type exists '\n                    + \"either in the original schema, or is added in a type definition.\"\n                ).format(astNode.name.value),\n                [astNode],\n            )\n        return type\n\n    # Given a name, returns a type from either the existing schema or an\n    # added type.\n    def _get_named_type(typeName):\n        cached_type_def = type_def_cache.get(typeName)\n        if cached_type_def:\n            return cached_type_def\n\n        existing_type = schema.get_type(typeName)\n        if existing_type:\n            type_def = extend_type(existing_type)\n            type_def_cache[typeName] = type_def\n            return type_def\n\n        type_ast = type_definition_map.get(typeName)\n        if type_ast:\n            type_def = build_type(type_ast)\n            type_def_cache[typeName] = type_def\n            return type_def\n\n    # Given a type's introspection result, construct the correct\n    # GraphQLType instance.\n    def extend_type(type):\n        if isinstance(type, GraphQLObjectType):\n            return extend_object_type(type)\n        if isinstance(type, GraphQLInterfaceType):\n            return extend_interface_type(type)\n        if isinstance(type, GraphQLUnionType):\n            return extend_union_type(type)\n        return type\n\n    def extend_object_type(type):\n        return GraphQLObjectType(\n            name=type.name,\n            description=type.description,\n            interfaces=lambda: extend_implemented_interfaces(type),\n            fields=lambda: extend_field_map(type),\n        )\n\n    def extend_interface_type(type):\n        return GraphQLInterfaceType(\n            name=type.name,\n            description=type.description,\n            fields=lambda: extend_field_map(type),\n            resolve_type=cannot_execute_client_schema,\n        )\n\n    def extend_union_type(type):\n        return GraphQLUnionType(\n            name=type.name,\n            description=type.description,\n            types=list(map(get_type_from_def, type.types)),\n            resolve_type=cannot_execute_client_schema,\n        )\n\n    def extend_implemented_interfaces(type):\n        interfaces = list(map(get_type_from_def, type.interfaces))\n\n        # If there are any extensions to the interfaces, apply those here.\n        extensions = type_extensions_map[type.name]\n        for extension in extensions:\n            for namedType in extension.definition.interfaces:\n                interface_name = namedType.name.value\n                if any([_def.name == interface_name for _def in interfaces]):\n                    raise GraphQLError(\n                        (\n                            'Type \"{}\" already implements \"{}\". '\n                            + \"It cannot also be implemented in this type extension.\"\n                        ).format(type.name, interface_name),\n                        [namedType],\n                    )\n                interfaces.append(get_type_from_AST(namedType))\n\n        return interfaces\n\n    def extend_field_map(type):\n        new_field_map = OrderedDict()\n        old_field_map = type.fields\n        for field_name, field in old_field_map.items():\n            new_field_map[field_name] = GraphQLField(\n                extend_field_type(field.type),\n                description=field.description,\n                deprecation_reason=field.deprecation_reason,\n                args=field.args,\n                resolver=cannot_execute_client_schema,\n            )\n\n        # If there are any extensions to the fields, apply those here.\n        extensions = type_extensions_map[type.name]\n        for extension in extensions:\n            for field in extension.definition.fields:\n                field_name = field.name.value\n                if field_name in old_field_map:\n                    raise GraphQLError(\n                        (\n                            'Field \"{}.{}\" already exists in the '\n                            + \"schema. It cannot also be defined in this type extension.\"\n                        ).format(type.name, field_name),\n                        [field],\n                    )\n                new_field_map[field_name] = GraphQLField(\n                    build_field_type(field.type),\n                    args=build_input_values(field.arguments),\n                    resolver=cannot_execute_client_schema,\n                )\n\n        return new_field_map\n\n    def extend_field_type(type):\n        if isinstance(type, GraphQLList):\n            return GraphQLList(extend_field_type(type.of_type))\n        if isinstance(type, GraphQLNonNull):\n            return GraphQLNonNull(extend_field_type(type.of_type))\n        return get_type_from_def(type)\n\n    def build_type(type_ast):\n        _type_build = {\n            ast.ObjectTypeDefinition: build_object_type,\n            ast.InterfaceTypeDefinition: build_interface_type,\n            ast.UnionTypeDefinition: build_union_type,\n            ast.ScalarTypeDefinition: build_scalar_type,\n            ast.EnumTypeDefinition: build_enum_type,\n            ast.InputObjectTypeDefinition: build_input_object_type,\n        }\n        func = _type_build.get(type(type_ast))\n        if func:\n            return func(type_ast)\n\n    def build_object_type(type_ast):\n        return GraphQLObjectType(\n            type_ast.name.value,\n            interfaces=lambda: build_implemented_interfaces(type_ast),\n            fields=lambda: build_field_map(type_ast),\n        )\n\n    def build_interface_type(type_ast):\n        return GraphQLInterfaceType(\n            type_ast.name.value,\n            fields=lambda: build_field_map(type_ast),\n            resolve_type=cannot_execute_client_schema,\n        )\n\n    def build_union_type(type_ast):\n        return GraphQLUnionType(\n            type_ast.name.value,\n            types=list(map(get_type_from_AST, type_ast.types)),\n            resolve_type=cannot_execute_client_schema,\n        )\n\n    def build_scalar_type(type_ast):\n        return GraphQLScalarType(\n            type_ast.name.value,\n            serialize=lambda *args, **kwargs: None,\n            # Note: validation calls the parse functions to determine if a\n            # literal value is correct. Returning null would cause use of custom\n            # scalars to always fail validation. Returning false causes them to\n            # always pass validation.\n            parse_value=lambda *args, **kwargs: False,\n            parse_literal=lambda *args, **kwargs: False,\n        )\n\n    def build_enum_type(type_ast):\n        return GraphQLEnumType(\n            type_ast.name.value,\n            values={v.name.value: GraphQLEnumValue() for v in type_ast.values},\n        )\n\n    def build_input_object_type(type_ast):\n        return GraphQLInputObjectType(\n            type_ast.name.value,\n            fields=lambda: build_input_values(type_ast.fields, GraphQLInputObjectField),\n        )\n\n    def build_implemented_interfaces(type_ast):\n        return list(map(get_type_from_AST, type_ast.interfaces))\n\n    def build_field_map(type_ast):\n        return {\n            field.name.value: GraphQLField(\n                build_field_type(field.type),\n                args=build_input_values(field.arguments),\n                resolver=cannot_execute_client_schema,\n            )\n            for field in type_ast.fields\n        }\n\n    def build_input_values(values, input_type=GraphQLArgument):\n        input_values = OrderedDict()\n        for value in values:\n            type = build_field_type(value.type)\n            input_values[value.name.value] = input_type(\n                type, default_value=value_from_ast(value.default_value, type)\n            )\n        return input_values\n\n    def build_field_type(type_ast):\n        if isinstance(type_ast, ast.ListType):\n            return GraphQLList(build_field_type(type_ast.type))\n        if isinstance(type_ast, ast.NonNullType):\n            return GraphQLNonNull(build_field_type(type_ast.type))\n        return get_type_from_AST(type_ast)\n\n    # If this document contains no new types, then return the same unmodified\n    # GraphQLSchema instance.\n    if not type_extensions_map and not type_definition_map:\n        return schema\n\n    # A cache to use to store the actual GraphQLType definition objects by name.\n    # Initialize to the GraphQL built in scalars and introspection types. All\n    # functions below are inline so that this type def cache is within the scope\n    # of the closure.\n\n    type_def_cache = {\n        \"String\": GraphQLString,\n        \"Int\": GraphQLInt,\n        \"Float\": GraphQLFloat,\n        \"Boolean\": GraphQLBoolean,\n        \"ID\": GraphQLID,\n        \"__Schema\": __Schema,\n        \"__Directive\": __Directive,\n        \"__DirectiveLocation\": __DirectiveLocation,\n        \"__Type\": __Type,\n        \"__Field\": __Field,\n        \"__InputValue\": __InputValue,\n        \"__EnumValue\": __EnumValue,\n        \"__TypeKind\": __TypeKind,\n    }\n\n    # Get the root Query, Mutation, and Subscription types.\n    query_type = get_type_from_def(schema.get_query_type())\n\n    existing_mutation_type = schema.get_mutation_type()\n    mutationType = (\n        existing_mutation_type and get_type_from_def(existing_mutation_type) or None\n    )\n\n    existing_subscription_type = schema.get_subscription_type()\n    subscription_type = (\n        existing_subscription_type\n        and get_type_from_def(existing_subscription_type)\n        or None\n    )\n\n    # Iterate through all types, getting the type definition for each, ensuring\n    # that any type not directly referenced by a field will get created.\n    types = [get_type_from_def(_def) for _def in schema.get_type_map().values()]\n\n    # Do the same with new types, appending to the list of defined types.\n    types += [get_type_from_AST(_def) for _def in type_definition_map.values()]\n\n    # Then produce and return a Schema with these types.\n    return GraphQLSchema(\n        query=query_type,\n        mutation=mutationType,\n        subscription=subscription_type,\n        # Copy directives.\n        directives=schema.get_directives(),\n        types=types,\n    )", "response": "Extends a schema by applying type extensions and definitions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ast_to_code(ast, indent=0):\n    # type: (Any, int) -> str\n    \"\"\"\n    Converts an ast into a python code representation of the AST.\n    \"\"\"\n    code = []\n\n    def append(line):\n        # type: (str) -> None\n        code.append((\"    \" * indent) + line)\n\n    if isinstance(ast, Node):\n        append(\"ast.{}(\".format(ast.__class__.__name__))\n        indent += 1\n        for i, k in enumerate(ast._fields, 1):\n            v = getattr(ast, k)\n            append(\"{}={},\".format(k, ast_to_code(v, indent)))\n        if ast.loc:\n            append(\"loc={}\".format(ast_to_code(ast.loc, indent)))\n\n        indent -= 1\n        append(\")\")\n\n    elif isinstance(ast, Loc):\n        append(\"loc({}, {})\".format(ast.start, ast.end))\n\n    elif isinstance(ast, list):\n        if ast:\n            append(\"[\")\n            indent += 1\n\n            for i, it in enumerate(ast, 1):\n                is_last = i == len(ast)\n                append(ast_to_code(it, indent) + (\",\" if not is_last else \"\"))\n\n            indent -= 1\n            append(\"]\")\n        else:\n            append(\"[]\")\n\n    else:\n        append(repr(ast))\n\n    return \"\\n\".join(code).strip()", "response": "Converts an AST into a python code representation of the AST."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef snake(s):\n    if len(s) < 2:\n        return s.lower()\n    out = s[0].lower()\n    for c in s[1:]:\n        if c.isupper():\n            out += \"_\"\n            c = c.lower()\n        out += c\n    return out", "response": "Convert from title or camelCase to snake_case."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_post_request(self, url, auth, json_payload):\n        response = requests.post(url, auth=auth, json=json_payload)\n        return response.json()", "response": "This function executes the request with the provided json payload and returns the json response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_field_def(\n    schema,  # type: GraphQLSchema\n    parent_type,  # type: Union[GraphQLInterfaceType, GraphQLObjectType]\n    field_ast,  # type: Field\n):\n    # type: (...) -> Optional[GraphQLField]\n    \"\"\"Not exactly the same as the executor's definition of get_field_def, in this\n    statically evaluated environment we do not always have an Object type,\n    and need to handle Interface and Union types.\"\"\"\n    name = field_ast.name.value\n    if name == \"__schema\" and schema.get_query_type() == parent_type:\n        return SchemaMetaFieldDef\n\n    elif name == \"__type\" and schema.get_query_type() == parent_type:\n        return TypeMetaFieldDef\n\n    elif name == \"__typename\" and isinstance(\n        parent_type, (GraphQLObjectType, GraphQLInterfaceType, GraphQLUnionType)\n    ):\n        return TypeNameMetaFieldDef\n\n    elif isinstance(parent_type, (GraphQLObjectType, GraphQLInterfaceType)):\n        return parent_type.fields.get(name)", "response": "Returns the definition of the field in this\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _find_conflicts_within_selection_set(\n    context,  # type: ValidationContext\n    cached_fields_and_fragment_names,  # type: Dict[SelectionSet, Tuple[Dict[str, List[Tuple[Union[GraphQLInterfaceType, GraphQLObjectType, None], Field, GraphQLField]]], List[str]]]\n    compared_fragments,  # type: PairSet\n    parent_type,  # type: Union[GraphQLInterfaceType, GraphQLObjectType, None]\n    selection_set,  # type: SelectionSet\n):\n    # type: (...) ->  List[Tuple[Tuple[str, str], List[Node], List[Node]]]\n    \"\"\"Find all conflicts found \"within\" a selection set, including those found via spreading in fragments.\n\n       Called when visiting each SelectionSet in the GraphQL Document.\n    \"\"\"\n    conflicts = []  # type: List[Tuple[Tuple[str, str], List[Node], List[Node]]]\n    field_map, fragment_names = _get_fields_and_fragments_names(\n        context, cached_fields_and_fragment_names, parent_type, selection_set\n    )\n\n    # (A) Find all conflicts \"within\" the fields of this selection set.\n    # Note: this is the *only place* `collect_conflicts_within` is called.\n    _collect_conflicts_within(\n        context,\n        conflicts,\n        cached_fields_and_fragment_names,\n        compared_fragments,\n        field_map,\n    )\n\n    # (B) Then collect conflicts between these fields and those represented by\n    # each spread fragment name found.\n    for i, fragment_name in enumerate(fragment_names):\n        _collect_conflicts_between_fields_and_fragment(\n            context,\n            conflicts,\n            cached_fields_and_fragment_names,\n            compared_fragments,\n            False,\n            field_map,\n            fragment_name,\n        )\n\n        # (C) Then compare this fragment with all other fragments found in this\n        # selection set to collect conflicts within fragments spread together.\n        # This compares each item in the list of fragment names to every other item\n        # in that same list (except for itself).\n        for other_fragment_name in fragment_names[i + 1 :]:\n            _collect_conflicts_between_fragments(\n                context,\n                conflicts,\n                cached_fields_and_fragment_names,\n                compared_fragments,\n                False,\n                fragment_name,\n                other_fragment_name,\n            )\n\n    return conflicts", "response": "Find all conflicts found within a SelectionSet."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding all conflicts between two selection sets.", "response": "def _find_conflicts_between_sub_selection_sets(\n    context,  # type: ValidationContext\n    cached_fields_and_fragment_names,  # type: Dict[SelectionSet, Tuple[Dict[str, List[Tuple[Union[GraphQLInterfaceType, GraphQLObjectType, None], Field, GraphQLField]]], List[str]]]\n    compared_fragments,  # type: PairSet\n    are_mutually_exclusive,  # type: bool\n    parent_type1,  # type: Union[GraphQLInterfaceType, GraphQLObjectType, None]\n    selection_set1,  # type: SelectionSet\n    parent_type2,  # type: Union[GraphQLInterfaceType, GraphQLObjectType, None]\n    selection_set2,  # type: SelectionSet\n):\n    # type: (...) ->  List[Tuple[Tuple[str, str], List[Node], List[Node]]]\n    \"\"\"Find all conflicts found between two selection sets.\n\n       Includes those found via spreading in fragments. Called when determining if conflicts exist\n       between the sub-fields of two overlapping fields.\n    \"\"\"\n    conflicts = []  # type: List[Tuple[Tuple[str, str], List[Node], List[Node]]]\n\n    field_map1, fragment_names1 = _get_fields_and_fragments_names(\n        context, cached_fields_and_fragment_names, parent_type1, selection_set1\n    )\n\n    field_map2, fragment_names2 = _get_fields_and_fragments_names(\n        context, cached_fields_and_fragment_names, parent_type2, selection_set2\n    )\n\n    # (H) First, collect all conflicts between these two collections of field.\n    _collect_conflicts_between(\n        context,\n        conflicts,\n        cached_fields_and_fragment_names,\n        compared_fragments,\n        are_mutually_exclusive,\n        field_map1,\n        field_map2,\n    )\n\n    # (I) Then collect conflicts between the first collection of fields and\n    # those referenced by each fragment name associated with the second.\n    for fragment_name2 in fragment_names2:\n        _collect_conflicts_between_fields_and_fragment(\n            context,\n            conflicts,\n            cached_fields_and_fragment_names,\n            compared_fragments,\n            are_mutually_exclusive,\n            field_map1,\n            fragment_name2,\n        )\n\n    # (I) Then collect conflicts between the second collection of fields and\n    #  those referenced by each fragment name associated with the first.\n    for fragment_name1 in fragment_names1:\n        _collect_conflicts_between_fields_and_fragment(\n            context,\n            conflicts,\n            cached_fields_and_fragment_names,\n            compared_fragments,\n            are_mutually_exclusive,\n            field_map2,\n            fragment_name1,\n        )\n\n    # (J) Also collect conflicts between any fragment names by the first and\n    # fragment names by the second. This compares each item in the first set of\n    # names to each item in the second set of names.\n    for fragment_name1 in fragment_names1:\n        for fragment_name2 in fragment_names2:\n            _collect_conflicts_between_fragments(\n                context,\n                conflicts,\n                cached_fields_and_fragment_names,\n                compared_fragments,\n                are_mutually_exclusive,\n                fragment_name1,\n                fragment_name2,\n            )\n\n    return conflicts"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _collect_conflicts_within(\n    context,  # type: ValidationContext\n    conflicts,  # type: List[Tuple[Tuple[str, str], List[Node], List[Node]]]\n    cached_fields_and_fragment_names,  # type: Dict[SelectionSet, Tuple[Dict[str, List[Tuple[Union[GraphQLInterfaceType, GraphQLObjectType, None], Field, GraphQLField]]], List[str]]]\n    compared_fragments,  # type: PairSet\n    field_map,  # type: Dict[str, List[Tuple[Union[GraphQLInterfaceType, GraphQLObjectType, None], Field, GraphQLField]]]\n):\n    # type: (...) -> None\n    \"\"\"Collect all Conflicts \"within\" one collection of fields.\"\"\"\n\n    # field map is a keyed collection, where each key represents a response\n    # name and the value at that key is a list of all fields which provide that\n    # response name. For every response name, if there are multiple fields, they\n    # must be compared to find a potential conflict.\n    for response_name, fields in list(field_map.items()):\n        # This compares every field in the list to every other field in this list\n        # (except to itself). If the list only has one item, nothing needs to\n        # be compared.\n        for i, field in enumerate(fields):\n            for other_field in fields[i + 1 :]:\n                # within one collection is never mutually exclusive\n                conflict = _find_conflict(\n                    context,\n                    cached_fields_and_fragment_names,\n                    compared_fragments,\n                    False,\n                    response_name,\n                    field,\n                    other_field,\n                )\n                if conflict:\n                    conflicts.append(conflict)", "response": "Collect all Conflicts within one collection of fields."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _collect_conflicts_between(\n    context,  # type: ValidationContext\n    conflicts,  # type: List[Tuple[Tuple[str, str], List[Node], List[Node]]]\n    cached_fields_and_fragment_names,  # type: Dict[SelectionSet, Tuple[Dict[str, List[Tuple[Union[GraphQLInterfaceType, GraphQLObjectType, None], Field, GraphQLField]]], List[str]]]\n    compared_fragments,  # type: PairSet\n    parent_fields_are_mutually_exclusive,  # type: bool\n    field_map1,  # type: Dict[str, List[Tuple[Union[GraphQLInterfaceType, GraphQLObjectType, None], Field, GraphQLField]]]\n    field_map2,  # type: Dict[str, List[Tuple[Union[GraphQLInterfaceType, GraphQLObjectType, None], Field, GraphQLField]]]\n):\n    # type: (...) -> None\n    \"\"\"Collect all Conflicts between two collections of fields.\n\n       This is similar to, but different from the `collect_conflicts_within` function above. This check assumes that\n       `collect_conflicts_within` has already been called on each provided collection of fields.\n       This is true because this validator traverses each individual selection set.\n    \"\"\"\n    # A field map is a keyed collection, where each key represents a response\n    # name and the value at that key is a list of all fields which provide that\n    # response name. For any response name which appears in both provided field\n    # maps, each field from the first field map must be compared to every field\n    # in the second field map to find potential conflicts.\n    for response_name, fields1 in list(field_map1.items()):\n        fields2 = field_map2.get(response_name)\n\n        if fields2:\n            for field1 in fields1:\n                for field2 in fields2:\n                    conflict = _find_conflict(\n                        context,\n                        cached_fields_and_fragment_names,\n                        compared_fragments,\n                        parent_fields_are_mutually_exclusive,\n                        response_name,\n                        field1,\n                        field2,\n                    )\n\n                    if conflict:\n                        conflicts.append(conflict)", "response": "Collect all Conflicts between two collections of fields."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _find_conflict(\n    context,  # type: ValidationContext\n    cached_fields_and_fragment_names,  # type: Dict[SelectionSet, Tuple[Dict[str, List[Tuple[Union[GraphQLInterfaceType, GraphQLObjectType, None], Field, GraphQLField]]], List[str]]]\n    compared_fragments,  # type: PairSet\n    parent_fields_are_mutually_exclusive,  # type: bool\n    response_name,  # type: str\n    field1,  # type: Tuple[Union[GraphQLInterfaceType, GraphQLObjectType, None], Field, GraphQLField]\n    field2,  # type: Tuple[Union[GraphQLInterfaceType, GraphQLObjectType, None], Field, GraphQLField]\n):\n    # type: (...) -> Optional[Tuple[Tuple[str, str], List[Node], List[Node]]]\n    \"\"\"Determines if there is a conflict between two particular fields.\"\"\"\n    parent_type1, ast1, def1 = field1\n    parent_type2, ast2, def2 = field2\n\n    # If it is known that two fields could not possibly apply at the same\n    # time, due to the parent types, then it is safe to permit them to diverge\n    # in aliased field or arguments used as they will not present any ambiguity\n    # by differing.\n    # It is known that two parent types could never overlap if they are\n    # different Object types. Interface or Union types might overlap - if not\n    # in the current state of the schema, then perhaps in some future version,\n    # thus may not safely diverge.\n\n    are_mutually_exclusive = parent_fields_are_mutually_exclusive or (\n        parent_type1 != parent_type2\n        and isinstance(parent_type1, GraphQLObjectType)\n        and isinstance(parent_type2, GraphQLObjectType)\n    )\n\n    # The return type for each field.\n    type1 = def1 and def1.type\n    type2 = def2 and def2.type\n\n    if not are_mutually_exclusive:\n        # Two aliases must refer to the same field.\n        name1 = ast1.name.value\n        name2 = ast2.name.value\n\n        if name1 != name2:\n            return (\n                (response_name, \"{} and {} are different fields\".format(name1, name2)),\n                [ast1],\n                [ast2],\n            )\n\n        # Two field calls must have the same arguments.\n        if not _same_arguments(ast1.arguments, ast2.arguments):\n            return ((response_name, \"they have differing arguments\"), [ast1], [ast2])\n\n    if type1 and type2 and do_types_conflict(type1, type2):\n        return (\n            (\n                response_name,\n                \"they return conflicting types {} and {}\".format(type1, type2),\n            ),\n            [ast1],\n            [ast2],\n        )\n\n    #  Collect and compare sub-fields. Use the same \"visited fragment names\" list\n    # for both collections so fields in a fragment reference are never\n    # compared to themselves.\n    selection_set1 = ast1.selection_set\n    selection_set2 = ast2.selection_set\n\n    if selection_set1 and selection_set2:\n        conflicts = _find_conflicts_between_sub_selection_sets(  # type: ignore\n            context,\n            cached_fields_and_fragment_names,\n            compared_fragments,\n            are_mutually_exclusive,\n            get_named_type(type1),  # type: ignore\n            selection_set1,\n            get_named_type(type2),  # type: ignore\n            selection_set2,\n        )\n\n        return _subfield_conflicts(conflicts, response_name, ast1, ast2)\n\n    return None", "response": "Determines if there is a conflict between two fields."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a reference to a fragment return the represented collection of fields as well as a list of nested fragment names referenced via fragment spreads.", "response": "def _get_referenced_fields_and_fragment_names(\n    context,  # ValidationContext\n    cached_fields_and_fragment_names,  # type: Dict[SelectionSet, Tuple[Dict[str, List[Tuple[Union[GraphQLInterfaceType, GraphQLObjectType, None], Field, GraphQLField]]], List[str]]]\n    fragment,  # type: InlineFragment\n):\n    # type: (...) -> Tuple[Dict[str, List[Tuple[Union[GraphQLInterfaceType, GraphQLObjectType, None], Field, GraphQLField]]], List[str]]\n    \"\"\"Given a reference to a fragment, return the represented collection of fields as well as a list of\n    nested fragment names referenced via fragment spreads.\"\"\"\n\n    # Short-circuit building a type from the AST if possible.\n    cached = cached_fields_and_fragment_names.get(fragment.selection_set)\n\n    if cached:\n        return cached\n\n    fragment_type = type_from_ast(  # type: ignore\n        context.get_schema(), fragment.type_condition\n    )\n\n    return _get_fields_and_fragments_names(  # type: ignore\n        context, cached_fields_and_fragment_names, fragment_type, fragment.selection_set\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a series of Conflicts which occurred between two sub - fields generate a single Conflict.", "response": "def _subfield_conflicts(\n    conflicts,  # type: List[Tuple[Tuple[str, str], List[Node], List[Node]]]\n    response_name,  # type: str\n    ast1,  # type: Node\n    ast2,  # type: Node\n):\n    # type: (...) -> Optional[Tuple[Tuple[str, str], List[Node], List[Node]]]\n    \"\"\"Given a series of Conflicts which occurred between two sub-fields, generate a single Conflict.\"\"\"\n    if conflicts:\n        return (  # type: ignore\n            (response_name, [conflict[0] for conflict in conflicts]),\n            tuple(itertools.chain([ast1], *[conflict[1] for conflict in conflicts])),\n            tuple(itertools.chain([ast2], *[conflict[2] for conflict in conflicts])),\n        )\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef collect_fields(\n    ctx,  # type: ExecutionContext\n    runtime_type,  # type: GraphQLObjectType\n    selection_set,  # type: SelectionSet\n    fields,  # type: DefaultOrderedDict\n    prev_fragment_names,  # type: Set[str]\n):\n    # type: (...) -> DefaultOrderedDict\n    \"\"\"\n    Given a selectionSet, adds all of the fields in that selection to\n    the passed in map of fields, and returns it at the end.\n\n    collect_fields requires the \"runtime type\" of an object. For a field which\n    returns and Interface or Union type, the \"runtime type\" will be the actual\n    Object type returned by that field.\n    \"\"\"\n    for selection in selection_set.selections:\n        directives = selection.directives\n\n        if isinstance(selection, ast.Field):\n            if not should_include_node(ctx, directives):\n                continue\n\n            name = get_field_entry_key(selection)\n            fields[name].append(selection)\n\n        elif isinstance(selection, ast.InlineFragment):\n            if not should_include_node(\n                ctx, directives\n            ) or not does_fragment_condition_match(ctx, selection, runtime_type):\n                continue\n\n            collect_fields(\n                ctx, runtime_type, selection.selection_set, fields, prev_fragment_names\n            )\n\n        elif isinstance(selection, ast.FragmentSpread):\n            frag_name = selection.name.value\n\n            if frag_name in prev_fragment_names or not should_include_node(\n                ctx, directives\n            ):\n                continue\n\n            prev_fragment_names.add(frag_name)\n            fragment = ctx.fragments[frag_name]\n            frag_directives = fragment.directives\n            if (\n                not fragment\n                or not should_include_node(ctx, frag_directives)\n                or not does_fragment_condition_match(ctx, fragment, runtime_type)\n            ):\n                continue\n\n            collect_fields(\n                ctx, runtime_type, fragment.selection_set, fields, prev_fragment_names\n            )\n\n    return fields", "response": "Given a selectionSet and a dict of fields collect all of the fields in that selection set."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining if a field should be included based on the include and include directives where the higher precidence than @include.", "response": "def should_include_node(ctx, directives):\n    # type: (ExecutionContext, Optional[List[Directive]]) -> bool\n    \"\"\"Determines if a field should be included based on the @include and\n    @skip directives, where @skip has higher precidence than @include.\"\"\"\n    # TODO: Refactor based on latest code\n    if directives:\n        skip_ast = None\n\n        for directive in directives:\n            if directive.name.value == GraphQLSkipDirective.name:\n                skip_ast = directive\n                break\n\n        if skip_ast:\n            args = get_argument_values(\n                GraphQLSkipDirective.args, skip_ast.arguments, ctx.variable_values\n            )\n            if args.get(\"if\") is True:\n                return False\n\n        include_ast = None\n\n        for directive in directives:\n            if directive.name.value == GraphQLIncludeDirective.name:\n                include_ast = directive\n                break\n\n        if include_ast:\n            args = get_argument_values(\n                GraphQLIncludeDirective.args, include_ast.arguments, ctx.variable_values\n            )\n\n            if args.get(\"if\") is False:\n                return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nimplements the logic to compute the key of a given field s entry", "response": "def get_field_entry_key(node):\n    # type: (Field) -> str\n    \"\"\"Implements the logic to compute the key of a given field's entry\"\"\"\n    if node.alias:\n        return node.alias.value\n    return node.name.value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef default_resolve_fn(source, info, **args):\n    # type: (Any, ResolveInfo, **Any) -> Optional[Any]\n    \"\"\"If a resolve function is not given, then a default resolve behavior is used which takes the property of the source object\n    of the same name as the field and returns it as the result, or if it's a function, returns the result of calling that function.\"\"\"\n    name = info.field_name\n    if isinstance(source, dict):\n        property = source.get(name)\n    else:\n        property = getattr(source, name, None)\n    if callable(property):\n        return property()\n    return property", "response": "Default resolve function for the base object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_field_def(\n    schema,  # type: GraphQLSchema\n    parent_type,  # type: GraphQLObjectType\n    field_name,  # type: str\n):\n    # type: (...) -> Optional[GraphQLField]\n    \"\"\"This method looks up the field on the given type defintion.\n    It has special casing for the two introspection fields, __schema\n    and __typename. __typename is special because it can always be\n    queried as a field, even in situations where no other fields\n    are allowed, like on a Union. __schema could get automatically\n    added to the query type, but that would require mutating type\n    definitions, which would cause issues.\"\"\"\n    if field_name == \"__schema\" and schema.get_query_type() == parent_type:\n        return SchemaMetaFieldDef\n    elif field_name == \"__type\" and schema.get_query_type() == parent_type:\n        return TypeMetaFieldDef\n    elif field_name == \"__typename\":\n        return TypeNameMetaFieldDef\n    return parent_type.fields.get(field_name)", "response": "This method returns the GraphQLField object that corresponds to the given field name on the given schema."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprepare an object map of variables of the correct type based on the provided variable definitions and arbitrary input.", "response": "def get_variable_values(\n    schema,  # type: GraphQLSchema\n    definition_asts,  # type: List[VariableDefinition]\n    inputs,  # type: Any\n):\n    # type: (...) -> Dict[str, Any]\n    \"\"\"Prepares an object map of variables of the correct type based on the provided variable definitions and arbitrary input.\n    If the input cannot be parsed to match the variable definitions, a GraphQLError will be thrown.\"\"\"\n    if inputs is None:\n        inputs = {}\n\n    values = {}\n    for def_ast in definition_asts:\n        var_name = def_ast.variable.name.value\n        var_type = type_from_ast(schema, def_ast.type)\n        value = inputs.get(var_name)\n\n        if not is_input_type(var_type):\n            raise GraphQLError(\n                'Variable \"${var_name}\" expected value of type \"{var_type}\" which cannot be used as an input type.'.format(\n                    var_name=var_name, var_type=print_ast(def_ast.type)\n                ),\n                [def_ast],\n            )\n        elif value is None:\n            if def_ast.default_value is not None:\n                values[var_name] = value_from_ast(\n                    def_ast.default_value, var_type\n                )  # type: ignore\n            if isinstance(var_type, GraphQLNonNull):\n                raise GraphQLError(\n                    'Variable \"${var_name}\" of required type \"{var_type}\" was not provided.'.format(\n                        var_name=var_name, var_type=var_type\n                    ),\n                    [def_ast],\n                )\n        else:\n            errors = is_valid_value(value, var_type)\n            if errors:\n                message = u\"\\n\" + u\"\\n\".join(errors)\n                raise GraphQLError(\n                    'Variable \"${}\" got invalid value {}.{}'.format(\n                        var_name, json.dumps(value, sort_keys=True), message\n                    ),\n                    [def_ast],\n                )\n            coerced_value = coerce_value(var_type, value)\n            if coerced_value is None:\n                raise Exception(\"Should have reported error.\")\n\n            values[var_name] = coerced_value\n\n    return values"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_argument_values(\n    arg_defs,  # type: Union[Dict[str, GraphQLArgument], Dict]\n    arg_asts,  # type: Optional[List[Argument]]\n    variables=None,  # type: Optional[Dict[str, Union[List, Dict, int, float, bool, str, None]]]\n):\n    # type: (...) -> Dict[str, Any]\n    \"\"\"Prepares an object map of argument values given a list of argument\n    definitions and list of argument AST nodes.\"\"\"\n    if not arg_defs:\n        return {}\n\n    if arg_asts:\n        arg_ast_map = {\n            arg.name.value: arg for arg in arg_asts\n        }  # type: Dict[str, Argument]\n    else:\n        arg_ast_map = {}\n\n    result = {}\n    for name, arg_def in arg_defs.items():\n        arg_type = arg_def.type\n        arg_ast = arg_ast_map.get(name)\n        if name not in arg_ast_map:\n            if arg_def.default_value is not None:\n                result[arg_def.out_name or name] = arg_def.default_value\n                continue\n            elif isinstance(arg_type, GraphQLNonNull):\n                raise GraphQLError(\n                    'Argument \"{name}\" of required type {arg_type}\" was not provided.'.format(\n                        name=name, arg_type=arg_type\n                    ),\n                    arg_asts,\n                )\n        elif isinstance(arg_ast.value, ast.Variable):  # type: ignore\n            variable_name = arg_ast.value.name.value  # type: ignore\n            if variables and variable_name in variables:\n                result[arg_def.out_name or name] = variables[variable_name]\n            elif arg_def.default_value is not None:\n                result[arg_def.out_name or name] = arg_def.default_value\n            elif isinstance(arg_type, GraphQLNonNull):\n                raise GraphQLError(\n                    'Argument \"{name}\" of required type {arg_type}\" provided the variable \"${variable_name}\" which was not provided'.format(\n                        name=name, arg_type=arg_type, variable_name=variable_name\n                    ),\n                    arg_asts,\n                )\n            continue\n\n        else:\n            value = value_from_ast(arg_ast.value, arg_type, variables)  # type: ignore\n            if value is None:\n                if arg_def.default_value is not None:\n                    value = arg_def.default_value\n                    result[arg_def.out_name or name] = value\n            else:\n                # We use out_name as the output name for the\n                # dict if exists\n                result[arg_def.out_name or name] = value\n\n    return result", "response": "Prepares an object map of argument values given a list of argument definitions and list of argument AST nodes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef coerce_value(type, value):\n    # type: (Any, Any) -> Union[List, Dict, int, float, bool, str, None]\n    \"\"\"Given a type and any value, return a runtime value coerced to match the type.\"\"\"\n    if isinstance(type, GraphQLNonNull):\n        # Note: we're not checking that the result of coerceValue is\n        # non-null.\n        # We only call this function after calling isValidValue.\n        return coerce_value(type.of_type, value)\n\n    if value is None:\n        return None\n\n    if isinstance(type, GraphQLList):\n        item_type = type.of_type\n        if not isinstance(value, string_types) and isinstance(value, Iterable):\n            return [coerce_value(item_type, item) for item in value]\n        else:\n            return [coerce_value(item_type, value)]\n\n    if isinstance(type, GraphQLInputObjectType):\n        fields = type.fields\n        obj = {}\n        for field_name, field in fields.items():\n            if field_name not in value:\n                if field.default_value is not None:\n                    field_value = field.default_value\n                    obj[field.out_name or field_name] = field_value\n            else:\n                field_value = coerce_value(field.type, value.get(field_name))\n                obj[field.out_name or field_name] = field_value\n\n        return type.create_container(obj)\n\n    assert isinstance(type, (GraphQLScalarType, GraphQLEnumType)), \"Must be input type\"\n\n    return type.parse_value(value)", "response": "Given a type and any value return a runtime value coerced to match the type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a GraphQL source parses it into a Document.", "response": "def parse(source, **kwargs):\n    # type: (Union[Source, str], **Any) -> Document\n    \"\"\"Given a GraphQL source, parses it into a Document.\"\"\"\n    options = {\"no_location\": False, \"no_source\": False}\n    options.update(kwargs)\n\n    if isinstance(source, string_types):\n        source_obj = Source(source)  # type: Source\n    else:\n        source_obj = source  # type: ignore\n\n    parser = Parser(source_obj, options)\n    return parse_document(parser)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef loc(parser, start):\n    # type: (Parser, int) -> Optional[Loc]\n    \"\"\"Returns a location object, used to identify the place in\n    the source that created a given parsed object.\"\"\"\n    if parser.options[\"no_location\"]:\n        return None\n\n    if parser.options[\"no_source\"]:\n        return Loc(start, parser.prev_end)\n\n    return Loc(start, parser.prev_end, parser.source)", "response": "Returns a location object used to identify the place in\n    the source that created a given parsed object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmove the internal parser object to the next lexed token.", "response": "def advance(parser):\n    # type: (Parser) -> None\n    \"\"\"Moves the internal parser object to the next lexed token.\"\"\"\n    prev_end = parser.token.end\n    parser.prev_end = prev_end\n    parser.token = parser.lexer.next_token(prev_end)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns true after advancing the parser.", "response": "def skip(parser, kind):\n    # type: (Parser, int) -> bool\n    \"\"\"If the next token is of the given kind, return true after advancing\n    the parser. Otherwise, do not change the parser state\n    and throw an error.\"\"\"\n    match = parser.token.kind == kind\n    if match:\n        advance(parser)\n\n    return match"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef expect(parser, kind):\n    # type: (Parser, int) -> Token\n    \"\"\"If the next token is of the given kind, return that token after\n    advancing the parser. Otherwise, do not change the parser state and\n    return False.\"\"\"\n    token = parser.token\n    if token.kind == kind:\n        advance(parser)\n        return token\n\n    raise GraphQLSyntaxError(\n        parser.source,\n        token.start,\n        u\"Expected {}, found {}\".format(\n            get_token_kind_desc(kind), get_token_desc(token)\n        ),\n    )", "response": "Returns the next token after\n    advancing the parser."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef expect_keyword(parser, value):\n    # type: (Parser, str) -> Token\n    \"\"\"If the next token is a keyword with the given value, return that\n    token after advancing the parser. Otherwise, do not change the parser\n    state and return False.\"\"\"\n    token = parser.token\n    if token.kind == TokenKind.NAME and token.value == value:\n        advance(parser)\n        return token\n\n    raise GraphQLSyntaxError(\n        parser.source,\n        token.start,\n        u'Expected \"{}\", found {}'.format(value, get_token_desc(token)),\n    )", "response": "Returns the next token after advancing the parser and raises an exception if the next token is not a keyword with the given value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unexpected(parser, at_token=None):\n    # type: (Parser, Optional[Any]) -> GraphQLSyntaxError\n    \"\"\"Helper function for creating an error when an unexpected lexed token\n    is encountered.\"\"\"\n    token = at_token or parser.token\n    return GraphQLSyntaxError(\n        parser.source, token.start, u\"Unexpected {}\".format(get_token_desc(token))\n    )", "response": "Helper function for creating an error when an unexpected lexed token is encountered."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a possibly empty list of parse nodes determined by the parse_fn.", "response": "def any(parser, open_kind, parse_fn, close_kind):\n    # type: (Parser, int, Callable, int) -> Any\n    \"\"\"Returns a possibly empty list of parse nodes, determined by\n    the parse_fn. This list begins with a lex token of openKind\n    and ends with a lex token of closeKind. Advances the parser\n    to the next lex token after the closing token.\"\"\"\n    expect(parser, open_kind)\n    nodes = []\n    while not skip(parser, close_kind):\n        nodes.append(parse_fn(parser))\n\n    return nodes"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a name lex token into a name parse node.", "response": "def parse_name(parser):\n    # type: (Parser) -> Name\n    \"\"\"Converts a name lex token into a name parse node.\"\"\"\n    token = expect(parser, TokenKind.NAME)\n    return ast.Name(value=token.value, loc=loc(parser, token.start))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_type(parser):\n    # type: (Parser) -> Union[NamedType, NonNullType, ListType]\n    \"\"\"Handles the 'Type': TypeName, ListType, and NonNullType\n    parsing rules.\"\"\"\n    start = parser.token.start\n    if skip(parser, TokenKind.BRACKET_L):\n        ast_type = parse_type(parser)\n        expect(parser, TokenKind.BRACKET_R)\n        ast_type = ast.ListType(type=ast_type, loc=loc(parser, start))  # type: ignore\n\n    else:\n        ast_type = parse_named_type(parser)\n\n    if skip(parser, TokenKind.BANG):\n        return ast.NonNullType(type=ast_type, loc=loc(parser, start))\n\n    return ast_type", "response": "Handles the Type : TypeName ListType and NonNullType\n    parsing rules."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a type system definition.", "response": "def parse_type_system_definition(parser):\n    # type: (Parser) -> Any\n    \"\"\"\n      TypeSystemDefinition :\n        - SchemaDefinition\n        - TypeDefinition\n        - TypeExtensionDefinition\n        - DirectiveDefinition\n\n      TypeDefinition :\n      - ScalarTypeDefinition\n      - ObjectTypeDefinition\n      - InterfaceTypeDefinition\n      - UnionTypeDefinition\n      - EnumTypeDefinition\n      - InputObjectTypeDefinition\n    \"\"\"\n    if not peek(parser, TokenKind.NAME):\n        raise unexpected(parser)\n\n    name = parser.token.value\n\n    if name == \"schema\":\n        return parse_schema_definition(parser)\n\n    elif name == \"scalar\":\n        return parse_scalar_type_definition(parser)\n\n    elif name == \"type\":\n        return parse_object_type_definition(parser)\n\n    elif name == \"interface\":\n        return parse_interface_type_definition(parser)\n\n    elif name == \"union\":\n        return parse_union_type_definition(parser)\n\n    elif name == \"enum\":\n        return parse_enum_type_definition(parser)\n\n    elif name == \"input\":\n        return parse_input_object_type_definition(parser)\n\n    elif name == \"extend\":\n        return parse_type_extension_definition(parser)\n\n    elif name == \"directive\":\n        return parse_directive_definition(parser)\n\n    raise unexpected(parser)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef quoted_or_list(items):\n    # type: (List[str]) -> str\n    \"\"\"Given [ A, B, C ] return '\"A\", \"B\" or \"C\"'.\"\"\"\n    selected = items[:MAX_LENGTH]\n    quoted_items = ('\"{}\"'.format(t) for t in selected)\n\n    def quoted_or_text(text, quoted_and_index):\n        index = quoted_and_index[0]\n        quoted_item = quoted_and_index[1]\n        text += (\n            (\", \" if len(selected) > 2 and not index == len(selected) - 1 else \" \")\n            + (\"or \" if index == len(selected) - 1 else \"\")\n            + quoted_item\n        )\n        return text\n\n    enumerated_items = enumerate(quoted_items)\n    first_item = next(enumerated_items)[1]\n    return functools.reduce(quoted_or_text, enumerated_items, first_item)", "response": "Given [ A B C ) return A B or C."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a Mapping of operation names and its associated types.", "response": "def operations_map(self):\n        # type: () -> Dict[Union[str, None], str]\n        \"\"\"\n        returns a Mapping of operation names and it's associated types.\n        E.g. {'myQuery': 'query', 'myMutation': 'mutation'}\n        \"\"\"\n        document_ast = self.document_ast\n        operations = {}  # type: Dict[Union[str, None], str]\n        for definition in document_ast.definitions:\n            if isinstance(definition, ast.OperationDefinition):\n                if definition.name:\n                    operations[definition.name.value] = definition.operation\n                else:\n                    operations[None] = definition.operation\n\n        return operations"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the operation type for the given operation_name.", "response": "def get_operation_type(self, operation_name):\n        # type: (Optional[str]) -> Optional[str]\n        \"\"\"\n        Returns the operation type ('query', 'mutation', 'subscription' or None)\n        for the given operation_name.\n        If no operation_name is provided (and only one operation exists) it will return the\n        operation type for that operation\n        \"\"\"\n        operations_map = self.operations_map\n        if not operation_name and len(operations_map) == 1:\n            return next(iter(operations_map.values()))\n        return operations_map.get(operation_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef select_if(df, fun):\n\n    def _filter_f(col):\n        try:\n            return fun(df[col])\n        except:\n            return False\n\n    cols = list(filter(_filter_f, df.columns))\n    return df[cols]", "response": "Selects columns where fun is true\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef drop_if(df, fun):\n\n    def _filter_f(col):\n        try:\n            return fun(df[col])\n        except:\n            return False\n\n    cols = list(filter(_filter_f, df.columns))\n    return df.drop(cols, axis=1)", "response": "Drops columns where fun is true\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mean(series):\n\n    if np.issubdtype(series.dtype, np.number):\n        return series.mean()\n    else:\n        return np.nan", "response": "Returns the mean of a series."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef first(series, order_by=None):\n\n    if order_by is not None:\n        series = order_series_by(series, order_by)\n    first_s = series.iloc[0]\n    return first_s", "response": "Returns the first value of a series."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef last(series, order_by=None):\n\n    if order_by is not None:\n        series = order_series_by(series, order_by)\n    last_s = series.iloc[series.size - 1]\n    return last_s", "response": "Returns the last value of a series."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef nth(series, n, order_by=None):\n\n    if order_by is not None:\n        series = order_series_by(series, order_by)\n    try:\n        return series.iloc[n]\n    except:\n        return np.nan", "response": "Returns the nth value of a series."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef median(series):\n\n    if np.issubdtype(series.dtype, np.number):\n        return series.median()\n    else:\n        return np.nan", "response": "Returns the median value of a series."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the variance of values in a series.", "response": "def var(series):\n    \"\"\"\n    Returns the variance of values in a series.\n\n    Args:\n        series (pandas.Series): column to summarize.\n    \"\"\"\n    if np.issubdtype(series.dtype, np.number):\n        return series.var()\n    else:\n        return np.nan"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sd(series):\n\n    if np.issubdtype(series.dtype, np.number):\n        return series.std()\n    else:\n        return np.nan", "response": "Returns the standard deviation of values in a series."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_join_parameters(join_kwargs):\n\n    by = join_kwargs.get('by', None)\n    suffixes = join_kwargs.get('suffixes', ('_x', '_y'))\n    if isinstance(by, tuple):\n        left_on, right_on = by\n    elif isinstance(by, list):\n        by = [x if isinstance(x, tuple) else (x, x) for x in by]\n        left_on, right_on = (list(x) for x in zip(*by))\n    else:\n        left_on, right_on = by, by\n    return left_on, right_on, suffixes", "response": "Returns the left right and left DataFrames on and suffixes for the columns to join the right and the left DataFrames on."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef inner_join(df, other, **kwargs):\n\n    left_on, right_on, suffixes = get_join_parameters(kwargs)\n    joined = df.merge(other, how='inner', left_on=left_on,\n                      right_on=right_on, suffixes=suffixes)\n    return joined", "response": "Inner join of two DataFrames."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef anti_join(df, other, **kwargs):\n\n    left_on, right_on, suffixes = get_join_parameters(kwargs)\n    if not right_on:\n        right_on = [col_name for col_name in df.columns.values.tolist() if col_name in other.columns.values.tolist()]\n        left_on = right_on\n    elif not isinstance(right_on, (list, tuple)):\n        right_on = [right_on]\n    other_reduced = other[right_on].drop_duplicates()\n    joined = df.merge(other_reduced, how='left', left_on=left_on,\n                      right_on=right_on, suffixes=('', '_y'),\n                      indicator=True).query('_merge==\"left_only\"')[df.columns.values.tolist()]\n    return joined", "response": "Joins two tables together and returns the rows in the left DataFrame that do not have a\n            match in the right DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bind_rows(df, other, join='outer', ignore_index=False):\n\n    df = pd.concat([df, other], join=join, ignore_index=ignore_index, axis=0)\n    return df", "response": "Binds DataFrames vertically to the bottom DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef arrange(df, *args, **kwargs):\n\n    flat_args = [a for a in flatten(args)]\n\n    series = [df[arg] if isinstance(arg, str) else\n              df.iloc[:, arg] if isinstance(arg, int) else\n              pd.Series(arg) for arg in flat_args]\n\n    sorter = pd.concat(series, axis=1).reset_index(drop=True)\n    sorter = sorter.sort_values(sorter.columns.tolist(), **kwargs)\n    return df.iloc[sorter.index, :]", "response": "Calls pandas. DataFrame. sort_values to sort a DataFrame according to the\n            criteria."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrenaming columns where keyword argument values are the current names of columns and keys are the new names for columns and values are the current names of columns and values are the new names for columns and values are the current names of columns.", "response": "def rename(df, **kwargs):\n    \"\"\"Renames columns, where keyword argument values are the current names\n    of columns and keys are the new names.\n\n    Args:\n        df (:obj:`pandas.DataFrame`): DataFrame passed in via `>>` pipe.\n\n    Kwargs:\n        **kwargs: key:value pairs where keys are new names for columns and\n            values are current names of columns.\n    \"\"\"\n\n    return df.rename(columns={v: k for k, v in kwargs.items()})"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngather the specified columns into two key - value columns.", "response": "def gather(df, key, values, *args, **kwargs):\n    \"\"\"\n    Melts the specified columns in your DataFrame into two key:value columns.\n\n    Args:\n        key (str): Name of identifier column.\n        values (str): Name of column that will contain values for the key.\n        *args (str, int, symbolic): Columns to \"melt\" into the new key and\n            value columns. If no args are specified, all columns are melted\n            into they key and value columns.\n\n    Kwargs:\n        add_id (bool): Boolean value indicating whether to add a `\"_ID\"`\n            column that will preserve information about the original rows\n            (useful for being able to re-widen the data later).\n\n    Example:\n        diamonds >> gather('variable', 'value', ['price', 'depth','x','y','z']) >> head(5)\n\n           carat      cut color clarity  table variable  value\n        0   0.23    Ideal     E     SI2   55.0    price  326.0\n        1   0.21  Premium     E     SI1   61.0    price  326.0\n        2   0.23     Good     E     VS1   65.0    price  327.0\n        3   0.29  Premium     I     VS2   58.0    price  334.0\n        4   0.31     Good     J     SI2   58.0    price  335.0\n    \"\"\"\n\n    if len(args) == 0:\n        args = df.columns.tolist()\n    else:\n        args = [a for a in flatten(args)]\n\n    if kwargs.get('add_id', False):\n        df = df.assign(_ID=np.arange(df.shape[0]))\n\n    columns = df.columns.tolist()\n    id_vars = [col for col in columns if col not in args]\n    return pd.melt(df, id_vars, list(args), key, values)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convert_type(df, columns):\n    # taken in part from the dplython package\n    out_df = df.copy()\n    for col in columns:\n        column_values = pd.Series(out_df[col].unique())\n        column_values = column_values[~column_values.isnull()]\n        # empty\n        if len(column_values) == 0:\n            continue\n        # boolean\n        if set(column_values.values) < {'True', 'False'}:\n            out_df[col] = out_df[col].map({'True': True, 'False': False})\n            continue\n        # numeric\n        if pd.to_numeric(column_values, errors='coerce').isnull().sum() == 0:\n            out_df[col] = pd.to_numeric(out_df[col], errors='ignore')\n            continue\n        # datetime\n        if pd.to_datetime(column_values, errors='coerce').isnull().sum() == 0:\n            out_df[col] = pd.to_datetime(out_df[col], errors='ignore',\n                                         infer_datetime_format=True)\n            continue\n\n    return out_df", "response": "Helper function that attempts to convert columns into their appropriate\nAttributeNames data type."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new base - level long - format DataFrame that is a wide format of the given key and value pairs.", "response": "def spread(df, key, values, convert=False):\n    \"\"\"\n    Transforms a \"long\" DataFrame into a \"wide\" format using a key and value\n    column.\n\n    If you have a mixed datatype column in your long-format DataFrame then the\n    default behavior is for the spread columns to be of type `object`, or\n    string. If you want to try to convert dtypes when spreading, you can set\n    the convert keyword argument in spread to True.\n\n    Args:\n        key (str, int, or symbolic): Label for the key column.\n        values (str, int, or symbolic): Label for the values column.\n\n    Kwargs:\n        convert (bool): Boolean indicating whether or not to try and convert\n            the spread columns to more appropriate data types.\n\n\n    Example:\n        widened = elongated >> spread(X.variable, X.value)\n        widened >> head(5)\n\n            _ID carat clarity color        cut depth price table     x     y     z\n        0     0  0.23     SI2     E      Ideal  61.5   326    55  3.95  3.98  2.43\n        1     1  0.21     SI1     E    Premium  59.8   326    61  3.89  3.84  2.31\n        2    10   0.3     SI1     J       Good    64   339    55  4.25  4.28  2.73\n        3   100  0.75     SI1     D  Very Good  63.2  2760    56   5.8  5.75  3.65\n        4  1000  0.75     SI1     D      Ideal  62.3  2898    55  5.83   5.8  3.62\n    \"\"\"\n\n    # Taken mostly from dplython package\n    columns = df.columns.tolist()\n    id_cols = [col for col in columns if not col in [key, values]]\n\n    temp_index = ['' for i in range(len(df))]\n    for id_col in id_cols:\n        temp_index += df[id_col].map(str)\n\n    out_df = df.assign(temp_index=temp_index)\n    out_df = out_df.set_index('temp_index')\n    spread_data = out_df[[key, values]]\n\n    if not all(spread_data.groupby([spread_data.index, key]).agg(\n            'count').reset_index()[values] < 2):\n        raise ValueError('Duplicate identifiers')\n\n    spread_data = spread_data.pivot(columns=key, values=values)\n\n    if convert and (out_df[values].dtype.kind in 'OSaU'):\n        columns_to_convert = [col for col in spread_data if col not in columns]\n        spread_data = convert_type(spread_data, columns_to_convert)\n\n    out_df = out_df[id_cols].drop_duplicates()\n    out_df = out_df.merge(spread_data, left_index=True, right_index=True).reset_index(drop=True)\n\n    out_df = (out_df >> arrange(id_cols)).reset_index(drop=True)\n\n    return out_df"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef separate(df, column, into, sep=\"[\\W_]+\", remove=True, convert=False,\n             extra='drop', fill='right'):\n    \"\"\"\n    Splits columns into multiple columns.\n\n    Args:\n        df (pandas.DataFrame): DataFrame passed in through the pipe.\n        column (str, symbolic): Label of column to split.\n        into (list): List of string names for new columns.\n\n    Kwargs:\n        sep (str or list): If a string, the regex string used to split the\n            column. If a list, a list of integer positions to split strings\n            on.\n        remove (bool): Boolean indicating whether to remove the original column.\n        convert (bool): Boolean indicating whether the new columns should be\n            converted to the appropriate type.\n        extra (str): either `'drop'`, where split pieces beyond the specified\n            new columns are dropped, or `'merge'`, where the final split piece\n            contains the remainder of the original column.\n        fill (str): either `'right'`, where `np.nan` values are filled in the\n            right-most columns for missing pieces, or `'left'` where `np.nan`\n            values are filled in the left-most columns.\n    \"\"\"\n\n    assert isinstance(into, (tuple, list))\n\n    if isinstance(sep, (tuple, list)):\n        inds = [0] + list(sep)\n        if len(inds) > len(into):\n            if extra == 'drop':\n                inds = inds[:len(into) + 1]\n            elif extra == 'merge':\n                inds = inds[:len(into)] + [None]\n        else:\n            inds = inds + [None]\n\n        splits = df[column].map(lambda x: [str(x)[slice(inds[i], inds[i + 1])]\n                                           if i < len(inds) - 1 else np.nan\n                                           for i in range(len(into))])\n\n    else:\n        maxsplit = len(into) - 1 if extra == 'merge' else 0\n        splits = df[column].map(lambda x: re.split(sep, x, maxsplit))\n\n    right_filler = lambda x: x + [np.nan for i in range(len(into) - len(x))]\n    left_filler = lambda x: [np.nan for i in range(len(into) - len(x))] + x\n\n    if fill == 'right':\n        splits = [right_filler(x) for x in splits]\n    elif fill == 'left':\n        splits = [left_filler(x) for x in splits]\n\n    for i, split_col in enumerate(into):\n        df[split_col] = [x[i] if not x[i] == '' else np.nan for x in splits]\n\n    if convert:\n        df = convert_type(df, into)\n\n    if remove:\n        df.drop(column, axis=1, inplace=True)\n\n    return df", "response": "Splits a column into multiple columns."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndoing the inverse of `separate`, joining columns together by a specified separator. Any columns that are not strings will be converted to strings. Args: df (pandas.DataFrame): DataFrame passed in through the pipe. colname (str): the name of the new joined column. *args: list of columns to be joined, which can be strings, symbolic, or integer positions. Kwargs: sep (str): the string separator to join the columns with. remove (bool): Boolean indicating whether or not to remove the original columns. na_action (str): can be one of `'maintain'` (the default), '`ignore'`, or `'as_string'`. The default will make the new column row a `NaN` value if any of the original column cells at that row contained `NaN`. '`ignore'` will treat any `NaN` value as an empty string during joining. `'as_string'` will convert any `NaN` value to the string `'nan'` prior to joining.", "response": "def unite(df, colname, *args, **kwargs):\n    \"\"\"\n    Does the inverse of `separate`, joining columns together by a specified\n    separator.\n\n    Any columns that are not strings will be converted to strings.\n\n    Args:\n        df (pandas.DataFrame): DataFrame passed in through the pipe.\n        colname (str): the name of the new joined column.\n        *args: list of columns to be joined, which can be strings, symbolic, or\n            integer positions.\n\n    Kwargs:\n        sep (str): the string separator to join the columns with.\n        remove (bool): Boolean indicating whether or not to remove the\n            original columns.\n        na_action (str): can be one of `'maintain'` (the default),\n            '`ignore'`, or `'as_string'`. The default will make the new column\n            row a `NaN` value if any of the original column cells at that\n            row contained `NaN`. '`ignore'` will treat any `NaN` value as an\n            empty string during joining. `'as_string'` will convert any `NaN`\n            value to the string `'nan'` prior to joining.\n    \"\"\"\n\n    to_unite = list([a for a in flatten(args)])\n    sep = kwargs.get('sep', '_')\n    remove = kwargs.get('remove', True)\n    # possible na_action values\n    # ignore: empty string\n    # maintain: keep as np.nan (default)\n    # as_string: becomes string 'nan'\n    na_action = kwargs.get('na_action', 'maintain')\n\n    # print(to_unite, sep, remove, na_action)\n\n    if na_action == 'maintain':\n        df[colname] = df[to_unite].apply(lambda x: np.nan if any(x.isnull())\n        else sep.join(x.map(str)), axis=1)\n    elif na_action == 'ignore':\n        df[colname] = df[to_unite].apply(lambda x: sep.join(x[~x.isnull()].map(str)),\n                                         axis=1)\n    elif na_action == 'as_string':\n        df[colname] = df[to_unite].astype(str).apply(lambda x: sep.join(x), axis=1)\n\n    if remove:\n        df.drop(to_unite, axis=1, inplace=True)\n\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate_set_ops(df, other):\n\n    if df.columns.values.tolist() != other.columns.values.tolist():\n        not_in_df = [col for col in other.columns if col not in df.columns]\n        not_in_other = [col for col in df.columns if col not in other.columns]\n        error_string = 'Error: not compatible.'\n        if len(not_in_df):\n            error_string += ' Cols in y but not x: ' + str(not_in_df) + '.'\n        if len(not_in_other):\n            error_string += ' Cols in x but not y: ' + str(not_in_other) + '.'\n        raise ValueError(error_string)\n    if len(df.index.names) != len(other.index.names):\n        raise ValueError('Index dimension mismatch')\n    if df.index.names != other.index.names:\n        raise ValueError('Index mismatch')\n    else:\n        return", "response": "Helper function to ensure that the dataframes are valid for set operations."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef union(df, other, index=False, keep='first'):\n    validate_set_ops(df, other)\n    stacked = df.append(other)\n    if index:\n        stacked_reset_indexes = stacked.reset_index()\n        index_cols = [col for col in stacked_reset_indexes.columns if col not in df.columns]\n        index_name = df.index.names\n        return_df = stacked_reset_indexes.drop_duplicates(keep=keep).set_index(index_cols)\n        return_df.index.names = index_name\n        return return_df\n    else:\n        return stacked.drop_duplicates(keep=keep)", "response": "Returns rows that appear in either DataFrame or the first."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef intersect(df, other, index=False, keep='first'):\n\n    validate_set_ops(df, other)\n    if index:\n        df_reset_index = df.reset_index()\n        other_reset_index = other.reset_index()\n        index_cols = [col for col in df_reset_index.columns if col not in df.columns]\n        df_index_names = df.index.names\n        return_df = (pd.merge(df_reset_index, other_reset_index,\n                              how='inner',\n                              left_on=df_reset_index.columns.values.tolist(),\n                              right_on=df_reset_index.columns.values.tolist())\n                     .set_index(index_cols))\n        return_df.index.names = df_index_names\n        return_df = return_df.drop_duplicates(keep=keep)\n        return return_df\n    else:\n        return_df = pd.merge(df, other,\n                             how='inner',\n                             left_on=df.columns.values.tolist(),\n                             right_on=df.columns.values.tolist())\n        return_df = return_df.drop_duplicates(keep=keep)\n        return return_df", "response": "Returns rows that appear in both DataFrames."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mutate_if(df, predicate, fun):\n    cols = list()\n    for col in df:\n        try:\n            if predicate(df[col]):\n                cols.append(col)\n        except:\n            pass\n    df[cols] = df[cols].apply(fun)\n    return df", "response": "Modify columns in place if the predicate returns True."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate columns and then returns those new columns and optionally specified original columns from the DataFrame.", "response": "def transmute(df, *keep_columns, **kwargs):\n    \"\"\"\n    Creates columns and then returns those new columns and optionally specified\n    original columns from the DataFrame.\n\n    This works like `mutate`, but designed to discard the original columns used\n    to create the new ones.\n\n    Args:\n        *keep_columns: Column labels to keep. Can be string, symbolic, or\n            integer position.\n\n    Kwargs:\n        **kwargs: keys are the names of the new columns, values indicate\n            what the new column values will be.\n\n    Example:\n        diamonds >> transmute(x_plus_y=X.x + X.y, y_div_z=(X.y / X.z)) >> head(3)\n\n            y_div_z  x_plus_y\n        0  1.637860      7.93\n        1  1.662338      7.73\n        2  1.761905      8.12\n    \"\"\"\n\n    keep_cols = []\n    for col in flatten(keep_columns):\n        try:\n            keep_cols.append(col.name)\n        except:\n            if isinstance(col, str):\n                keep_cols.append(col)\n            elif isinstance(col, int):\n                keep_cols.append(df.columns[col])\n\n    df = df.assign(**kwargs)\n    columns = [k for k in kwargs.keys()] + list(keep_cols)\n    return df[columns]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef order_series_by(series, order_series):\n\n    if isinstance(order_series, (list, tuple)):\n        sorter = pd.concat(order_series, axis=1)\n        sorter_columns = ['_sorter' + str(i) for i in range(len(order_series))]\n        sorter.columns = sorter_columns\n        sorter['series'] = series.values\n        sorted_series = sorter.sort_values(sorter_columns)['series']\n        return sorted_series\n    else:\n        sorted_series = pd.DataFrame({\n            'series': series.values,\n            'order': order_series.values\n        }).sort_values('order', ascending=True)['series']\n        return sorted_series", "response": "Returns a new Series object that is reordered according to a series."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntaking the first non - NaN value in order across the specified series and returns a new series.", "response": "def coalesce(*series):\n    \"\"\"\n    Takes the first non-NaN value in order across the specified series,\n    returning a new series. Mimics the coalesce function in dplyr and SQL.\n\n    Args:\n        *series: Series objects, typically represented in their symbolic form\n            (like X.series).\n\n    Example:\n        df = pd.DataFrame({\n            'a':[1,np.nan,np.nan,np.nan,np.nan],\n            'b':[2,3,np.nan,np.nan,np.nan],\n            'c':[np.nan,np.nan,4,5,np.nan],\n            'd':[6,7,8,9,np.nan]\n        })\n        df >> transmute(coal=coalesce(X.a, X.b, X.c, X.d))\n\n             coal\n        0       1\n        1       3\n        2       4\n        3       5\n        4  np.nan\n    \"\"\"\n\n    series = [pd.Series(s) for s in series]\n    coalescer = pd.concat(series, axis=1)\n    min_nonna = np.argmin(pd.isnull(coalescer).values, axis=1)\n    min_nonna = [coalescer.columns[i] for i in min_nonna]\n    return coalescer.lookup(np.arange(coalescer.shape[0]), min_nonna)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new series of logical conditions that are met in the given list of conditions.", "response": "def case_when(*conditions):\n    \"\"\"\n    Functions as a switch statement, creating a new series out of logical\n    conditions specified by 2-item lists where the left-hand item is the\n    logical condition and the right-hand item is the value where that\n    condition is true.\n\n    Conditions should go from the most specific to the most general. A\n    conditional that appears earlier in the series will \"overwrite\" one that\n    appears later. Think of it like a series of if-else statements.\n\n    The logicals and values of the condition pairs must be all the same\n    length, or length 1. Logicals can be vectors of booleans or a single\n    boolean (`True`, for example, can be the logical statement for the\n    final conditional to catch all remaining.).\n\n    Args:\n        *conditions: Each condition should be a list with two values. The first\n            value is a boolean or vector of booleans that specify indices in\n            which the condition is met. The second value is a vector of values\n            or single value specifying the outcome where that condition is met.\n\n    Example:\n        df = pd.DataFrame({\n            'num':np.arange(16)\n        })\n        df >> mutate(strnum=case_when([X.num % 15 == 0, 'fizzbuzz'],\n                                      [X.num % 3 == 0, 'fizz'],\n                                      [X.num % 5 == 0, 'buzz'],\n                                      [True, X.num.astype(str)]))\n\n            num    strnum\n        0     0  fizzbuzz\n        1     1         1\n        2     2         2\n        3     3      fizz\n        4     4         4\n        5     5      buzz\n        6     6      fizz\n        7     7         7\n        8     8         8\n        9     9      fizz\n        10   10      buzz\n        11   11        11\n        12   12      fizz\n        13   13        13\n        14   14        14\n        15   15  fizzbuzz\n    \"\"\"\n\n    lengths = []\n    for logical, outcome in conditions:\n        if isinstance(logical, collections.Iterable):\n            lengths.append(len(logical))\n        if isinstance(outcome, collections.Iterable) and not isinstance(outcome, str):\n            lengths.append(len(outcome))\n    unique_lengths = np.unique(lengths)\n    assert len(unique_lengths) == 1\n    output_len = unique_lengths[0]\n\n    output = []\n    for logical, outcome in conditions:\n        if isinstance(logical, bool):\n            logical = np.repeat(logical, output_len)\n        if isinstance(logical, pd.Series):\n            logical = logical.values\n        if not isinstance(outcome, collections.Iterable) or isinstance(outcome, str):\n            outcome = pd.Series(np.repeat(outcome, output_len))\n        outcome[~logical] = np.nan\n        output.append(outcome)\n\n    return coalesce(*output)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a series of symbolic values corresponding to the condition.", "response": "def if_else(condition, when_true, otherwise):\n    \"\"\"\n    Wraps creation of a series based on if-else conditional logic into a function\n    call.\n\n    Provide a boolean vector condition, value(s) when true, and value(s)\n    when false, and a vector will be returned the same length as the conditional\n    vector according to the logical statement.\n\n    Args:\n        condition: A boolean vector representing the condition. This is often\n            a logical statement with a symbolic series.\n        when_true: A vector the same length as the condition vector or a single\n            value to apply when the condition is `True`.\n        otherwise: A vector the same length as the condition vector or a single\n            value to apply when the condition is `False`.\n\n    Example:\n    df = pd.DataFrame\n    \"\"\"\n\n    if not isinstance(when_true, collections.Iterable) or isinstance(when_true, str):\n        when_true = np.repeat(when_true, len(condition))\n    if not isinstance(otherwise, collections.Iterable) or isinstance(otherwise, str):\n        otherwise = np.repeat(otherwise, len(condition))\n    assert (len(condition) == len(when_true)) and (len(condition) == len(otherwise))\n\n    if isinstance(when_true, pd.Series):\n        when_true = when_true.values\n    if isinstance(otherwise, pd.Series):\n        otherwise = otherwise.values\n\n    output = np.array([when_true[i] if c else otherwise[i]\n                       for i, c in enumerate(condition)])\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef na_if(series, *values):\n\n    series = pd.Series(series)\n    series[series.isin(values)] = np.nan\n    return series", "response": "If values in a series match a specified value change them to np. nan."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a boolean series specifying whether rows of the input series are between values a and b.", "response": "def between(series, a, b, inclusive=False):\n    \"\"\"\n    Returns a boolean series specifying whether rows of the input series\n    are between values `a` and `b`.\n\n    Args:\n        series: column to compare, typically symbolic.\n        a: value series must be greater than (or equal to if `inclusive=True`)\n            for the output series to be `True` at that position.\n        b: value series must be less than (or equal to if `inclusive=True`) for\n            the output series to be `True` at that position.\n\n    Kwargs:\n        inclusive (bool): If `True`, comparison is done with `>=` and `<=`.\n            If `False` (the default), comparison uses `>` and `<`.\n    \"\"\"\n\n    if inclusive == True:\n        met_condition = (series >= a) & (series <= b)\n    elif inclusive == False:\n        met_condition = (series > a) & (series < b)\n    return met_condition"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dense_rank(series, ascending=True):\n\n    ranks = series.rank(method='dense', ascending=ascending)\n    return ranks", "response": "Equivalent to series. rank."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef min_rank(series, ascending=True):\n\n    ranks = series.rank(method='min', ascending=ascending)\n    return ranks", "response": "Equivalent to series. rank ( method = min ascending = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cumany(series):\n\n    anys = series.expanding().apply(np.any).astype(bool)\n    return anys", "response": "Calculates cumulative any of values. Equivalent to\n    series. expanding().apply ( np. any )."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cumall(series):\n\n    alls = series.expanding().apply(np.all).astype(bool)\n    return alls", "response": "Calculates cumulative all of values. Equivalent to\n    series. expanding().apply ( np. all )."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the row number based on column rank", "response": "def row_number(series, ascending=True):\n    \"\"\"\n    Returns row number based on column rank\n    Equivalent to `series.rank(method='first', ascending=ascending)`.\n\n    Args:\n        series: column to rank.\n\n    Kwargs:\n        ascending (bool): whether to rank in ascending order (default is `True`).\n\n    Usage:\n    diamonds >> head() >> mutate(rn=row_number(X.x))\n\n       carat      cut color clarity  depth  table  price     x     y     z   rn\n    0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43  2.0\n    1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31  1.0\n    2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31  3.0\n    3   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63  4.0\n    4   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75  5.0\n    \"\"\"\n\n    series_rank = series.rank(method='first', ascending=ascending)\n    return series_rank"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nseeks the parser to the given position.", "response": "def seek(self, pos):\n        \"\"\"Seeks the parser to the given position.\n        \"\"\"\n        if self.debug:\n            logging.debug('seek: %r' % pos)\n        self.fp.seek(pos)\n        # reset the status for nextline()\n        self.bufpos = pos\n        self.buf = b''\n        self.charpos = 0\n        # reset the status for nexttoken()\n        self._parse1 = self._parse_main\n        self._curtoken = b''\n        self._curtokenpos = 0\n        self._tokens = []\n        return"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfetch a next line that ends either with \\\\ r or \\\\ n.", "response": "def nextline(self):\n        \"\"\"Fetches a next line that ends either with \\\\r or \\\\n.\n        \"\"\"\n        linebuf = b''\n        linepos = self.bufpos + self.charpos\n        eol = False\n        while 1:\n            self.fillbuf()\n            if eol:\n                c = self.buf[self.charpos]\n                # handle b'\\r\\n'\n                if c == b'\\n':\n                    linebuf += c\n                    self.charpos += 1\n                break\n            m = EOL.search(self.buf, self.charpos)\n            if m:\n                linebuf += self.buf[self.charpos:m.end(0)]\n                self.charpos = m.end(0)\n                if linebuf[-1] == b'\\r':\n                    eol = True\n                else:\n                    break\n            else:\n                linebuf += self.buf[self.charpos:]\n                self.charpos = len(self.buf)\n        if self.debug:\n            logging.debug('nextline: %r, %r' % (linepos, linebuf))\n        return (linepos, linebuf)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching a next line backward. This is used to locate the trailers at the end of a file. This is used to locate the trailers at the end of a file. This is used to locate the trailers at the end of a file.", "response": "def revreadlines(self):\n        \"\"\"Fetches a next line backward.\n\n        This is used to locate the trailers at the end of a file.\n        \"\"\"\n        self.fp.seek(0, 2)\n        pos = self.fp.tell()\n        buf = b''\n        while 0 < pos:\n            prevpos = pos\n            pos = max(0, pos-self.BUFSIZ)\n            self.fp.seek(pos)\n            s = self.fp.read(prevpos-pos)\n            if not s:\n                break\n            while 1:\n                n = max(s.rfind(b'\\r'), s.rfind(b'\\n'))\n                if n == -1:\n                    buf = s + buf\n                    break\n                yield s[n:]+buf\n                s = s[:n]\n                buf = b''\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nyielding a list of objects.", "response": "def nextobject(self):\n        \"\"\"Yields a list of objects.\n\n        Returns keywords, literals, strings, numbers, arrays and dictionaries.\n        Arrays and dictionaries are represented as Python lists and dictionaries.\n        \"\"\"\n        while not self.results:\n            (pos, token) = self.nexttoken()\n            #print (pos,token), (self.curtype, self.curstack)\n            if isinstance(token, (int, long, float, bool, str, PSLiteral)):\n                # normal token\n                self.push((pos, token))\n            elif token == KEYWORD_ARRAY_BEGIN:\n                # begin array\n                self.start_type(pos, 'a')\n            elif token == KEYWORD_ARRAY_END:\n                # end array\n                try:\n                    self.push(self.end_type('a'))\n                except PSTypeError:\n                    if STRICT:\n                        raise\n            elif token == KEYWORD_DICT_BEGIN:\n                # begin dictionary\n                self.start_type(pos, 'd')\n            elif token == KEYWORD_DICT_END:\n                # end dictionary\n                try:\n                    (pos, objs) = self.end_type('d')\n                    if len(objs) % 2 != 0:\n                        raise PSSyntaxError('Invalid dictionary construct: %r' % (objs,))\n                    # construct a Python dictionary.\n                    d = dict((literal_name(k), v) for (k, v) in choplist(2, objs) if v is not None)\n                    self.push((pos, d))\n                except PSTypeError:\n                    if STRICT:\n                        raise\n            elif token == KEYWORD_PROC_BEGIN:\n                # begin proc\n                self.start_type(pos, 'p')\n            elif token == KEYWORD_PROC_END:\n                # end proc\n                try:\n                    self.push(self.end_type('p'))\n                except PSTypeError:\n                    if STRICT:\n                        raise\n            else:\n                if self.debug:\n                    logging.debug('do_keyword: pos=%r, token=%r, stack=%r' % \\\n                                  (pos, token, self.curstack))\n                self.do_keyword(pos, token)\n            if self.context:\n                continue\n            else:\n                self.flush()\n        obj = self.results.pop(0)\n        if self.debug:\n            logging.debug('nextobject: %r' % (obj,))\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts Adobe glyph names to Unicode numbers.", "response": "def name2unicode(name):\n    \"\"\"Converts Adobe glyph names to Unicode numbers.\"\"\"\n    if name in glyphname2unicode:\n        return glyphname2unicode[name]\n    m = STRIP_NAME.search(name)\n    if not m:\n        raise KeyError(name)\n    return unichr(int(m.group(0)))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef lzwdecode(data):\n    fp = BytesIO(data)\n    return b''.join(LZWDecoder(fp).run())", "response": ">>> lzwdecode ( data ) >>>"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resolve1(x, default=None):\n    while isinstance(x, PDFObjRef):\n        x = x.resolve(default=default)\n    return x", "response": "Resolves an object.\n\n    If this is an array or dictionary, it may still contains\n    some indirect objects inside."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resolve_all(x, default=None):\n    while isinstance(x, PDFObjRef):\n        x = x.resolve(default=default)\n    if isinstance(x, list):\n        x = [resolve_all(v, default=default) for v in x]\n    elif isinstance(x, dict):\n        for (k, v) in x.iteritems():\n            x[k] = resolve_all(v, default=default)\n    return x", "response": "Recursively resolves the given object and all the internals."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ascii85decode(data):\n    n = b = 0\n    out = b''\n    for c in data:\n        if b'!' <= c and c <= b'u':\n            n += 1\n            b = b*85+(ord(c)-33)\n            if n == 5:\n                out += struct.pack('>L', b)\n                n = b = 0\n        elif c == b'z':\n            assert n == 0\n            out += b'\\0\\0\\0\\0'\n        elif c == b'~':\n            if n:\n                for _ in range(5-n):\n                    b = b*85+84\n                out += struct.pack('>L', b)[:n-1]\n            break\n    return out", "response": "Decode an ASCII85 encoded string into a string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef asciihexdecode(data):\n    decode = (lambda hx: chr(int(hx, 16)))\n    out = map(decode, hex_re.findall(data))\n    m = trail_re.search(data)\n    if m:\n        out.append(decode('%c0' % m.group(1)))\n    return b''.join(out)", "response": "ASCII HexDecode filter. PDFReference v1. 4 section 3. 1. 1\n ASCIIHexDecode filter produces one byte of binary data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread XRefs from the given location.", "response": "def read_xref_from(self, parser, start, xrefs):\n        \"\"\"Reads XRefs from the given location.\"\"\"\n        parser.seek(start)\n        parser.reset()\n        try:\n            (pos, token) = parser.nexttoken()\n        except PSEOF:\n            raise PDFNoValidXRef('Unexpected EOF')\n        if self.debug:\n            logging.info('read_xref_from: start=%d, token=%r' % (start, token))\n        if isinstance(token, int):\n            # XRefStream: PDF-1.5\n            parser.seek(pos)\n            parser.reset()\n            xref = PDFXRefStream()\n            xref.load(parser)\n        else:\n            if token is parser.KEYWORD_XREF:\n                parser.nextline()\n            xref = PDFXRef()\n            xref.load(parser)\n        xrefs.append(xref)\n        trailer = xref.get_trailer()\n        if self.debug:\n            logging.info('trailer: %r' % trailer)\n        if 'XRefStm' in trailer:\n            pos = int_value(trailer['XRefStm'])\n            self.read_xref_from(parser, pos, xrefs)\n        if 'Prev' in trailer:\n            # find previous xref\n            pos = int_value(trailer['Prev'])\n            self.read_xref_from(parser, pos, xrefs)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mult_matrix(m1, m0):\n    (a1, b1, c1, d1, e1, f1) = m1\n    (a0, b0, c0, d0, e0, f0) = m0\n    \"\"\"Returns the multiplication of two matrices.\"\"\"\n    return (a0*a1+c0*b1,    b0*a1+d0*b1,\n            a0*c1+c0*d1,    b0*c1+d0*d1,\n            a0*e1+c0*f1+e0, b0*e1+d0*f1+f0)", "response": "Returns the multiplication of two matrices."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef translate_matrix(m, v):\n    (a, b, c, d, e, f) = m\n    (x, y) = v\n    return (a, b, c, d, x*a+y*c+e, x*b+y*d+f)", "response": "Translates a matrix by x y."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef csort(objs, key):\n    idxs = dict((obj, i) for (i, obj) in enumerate(objs))\n    return sorted(objs, key=lambda obj: (key(obj), idxs[obj]))", "response": "Order - preserving sorting function."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsplitting a list into two classes according to the predicate.", "response": "def fsplit(pred, objs):\n    \"\"\"Split a list into two classes according to the predicate.\"\"\"\n    t = []\n    f = []\n    for obj in objs:\n        if pred(obj):\n            t.append(obj)\n        else:\n            f.append(obj)\n    return (t, f)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef drange(v0, v1, d):\n    assert v0 < v1\n    return xrange(int(v0)//d, int(v1+d)//d)", "response": "Returns a discrete range."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute a minimal rectangle that covers all the points.", "response": "def get_bound(pts):\n    \"\"\"Compute a minimal rectangle that covers all the points.\"\"\"\n    (x0, y0, x1, y1) = (INF, INF, -INF, -INF)\n    for (x, y) in pts:\n        x0 = min(x0, x)\n        y0 = min(y0, y)\n        x1 = max(x1, x)\n        y1 = max(y1, y)\n    return (x0, y0, x1, y1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pick(seq, func, maxobj=None):\n    maxscore = None\n    for obj in seq:\n        score = func(obj)\n        if maxscore is None or maxscore < score:\n            (maxscore, maxobj) = (score, obj)\n    return maxobj", "response": "Picks the object obj where func has the highest value."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef choplist(n, seq):\n    r = []\n    for x in seq:\n        r.append(x)\n        if len(r) == n:\n            yield tuple(r)\n            r = []\n    return", "response": "Groups every n elements of the list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef nunpack(s, default=0):\n    l = len(s)\n    if not l:\n        return default\n    elif l == 1:\n        return ord(s)\n    elif l == 2:\n        return struct.unpack('>H', s)[0]\n    elif l == 3:\n        return struct.unpack('>L', b'\\x00'+s)[0]\n    elif l == 4:\n        return struct.unpack('>L', s)[0]\n    else:\n        raise TypeError('invalid length: %d' % l)", "response": "Unpacks 1 to 4 byte integers ( big endian )."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef decode_text(s):\n    if s.startswith(b'\\xfe\\xff'):\n        return unicode(s[2:], 'utf-16be', 'ignore')\n    else:\n        return ''.join(PDFDocEncoding[ord(c)] for c in s)", "response": "Decodes a PDFDocEncoding string to Unicode."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef enc(x, codec='ascii'):\n    x = x.replace('&', '&amp;').replace('>', '&gt;').replace('<', '&lt;').replace('\"', '&quot;')\n    return x.encode(codec, 'xmlcharrefreplace')", "response": "Encodes a string for SGML and XML / HTML"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do_keyword(self, pos, token):\n\n        if token in (self.KEYWORD_XREF, self.KEYWORD_STARTXREF):\n            self.add_results(*self.pop(1))\n\n        elif token is self.KEYWORD_ENDOBJ:\n            self.add_results(*self.pop(4))\n\n        elif token is self.KEYWORD_NULL:\n            # null object\n            self.push((pos, None))\n\n        elif token is self.KEYWORD_R:\n            # reference to indirect object\n            try:\n                ((_, objid), (_, genno)) = self.pop(2)\n                (objid, genno) = (int(objid), int(genno))\n                obj = PDFObjRef(self.doc, objid, genno)\n                self.push((pos, obj))\n            except PSSyntaxError:\n                pass\n\n        elif token is self.KEYWORD_STREAM:\n            # stream object\n            ((_, dic),) = self.pop(1)\n            dic = dict_value(dic)\n            objlen = 0\n            if not self.fallback:\n                try:\n                    objlen = int_value(dic['Length'])\n                except KeyError:\n                    if STRICT:\n                        raise PDFSyntaxError('/Length is undefined: %r' % dic)\n            self.seek(pos)\n            try:\n                (_, line) = self.nextline()  # 'stream'\n            except PSEOF:\n                if STRICT:\n                    raise PDFSyntaxError('Unexpected EOF')\n                return\n            pos += len(line)\n            self.fp.seek(pos)\n            data = self.fp.read(objlen)\n            self.seek(pos+objlen)\n            while 1:\n                try:\n                    (linepos, line) = self.nextline()\n                except PSEOF:\n                    if STRICT:\n                        raise PDFSyntaxError('Unexpected EOF')\n                    break\n                if b'endstream' in line:\n                    i = line.index(b'endstream')\n                    objlen += i\n                    if self.fallback:\n                        data += line[:i]\n                    break\n                objlen += len(line)\n                if self.fallback:\n                    data += line\n            self.seek(pos+objlen)\n            # XXX limit objlen not to exceed object boundary\n            if self.debug:\n                logging.debug('Stream: pos=%d, objlen=%d, dic=%r, data=%r...' % \\\n                              (pos, objlen, dic, data[:10]))\n            obj = PDFStream(dic, data, self.doc.decipher)\n            self.push((pos, obj))\n\n        else:\n            # others\n            self.push((pos, token))\n\n        return", "response": "Handles PDF - related keywords."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rldecode(data):\n    decoded = []\n    i = 0\n    while i < len(data):\n        #print 'data[%d]=:%d:' % (i,ord(data[i]))\n        length = ord(data[i])\n        if length == 128:\n            break\n        if length >= 0 and length < 128:\n            run = data[i+1:(i+1)+(length+1)]\n            #print 'length=%d, run=%s' % (length+1,run)\n            decoded.append(run)\n            i = (i+1) + (length+1)\n        if length > 128:\n            run = data[i+1]*(257-length)\n            #print 'length=%d, run=%s' % (257-length,run)\n            decoded.append(run)\n            i = (i+1) + 1\n    return b''.join(decoded)", "response": "RunLengthDecode filter decodes data that has been encoded in a sequence of runs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate_help_text():\n    def generate_cmds_with_explanations(summary, cmds):\n        text = '{0}:\\n'.format(summary)\n        for cmd, explanation in cmds:\n            text += '\\t{0:<10}\\t{1:<20}\\n'.format(cmd, explanation)\n        return text + '\\n'\n\n    text = generate_cmds_with_explanations('Commands', ROOT_COMMANDS.items())\n    text += generate_cmds_with_explanations('Options', OPTION_NAMES.items())\n    text += generate_cmds_with_explanations('Actions', ACTIONS.items())\n    text += generate_cmds_with_explanations('Headers', HEADER_NAMES.items())\n    return text", "response": "Return a formatted string listing commands HTTPie options and HTTPie actions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef colformat(strings, num_sep_spaces=1, terminal_width=None):\n    if terminal_width is None:\n        terminal_width = get_terminal_size().columns\n\n    if not strings:\n        return\n\n    num_items = len(strings)\n    max_len = max([len(strip_ansi_escapes(s)) for s in strings])\n\n    num_columns = min(\n        int((terminal_width + num_sep_spaces) / (max_len + num_sep_spaces)),\n        num_items)\n    num_columns = max(1, num_columns)\n\n    num_lines = int(math.ceil(float(num_items) / num_columns))\n    num_columns = int(math.ceil(float(num_items) / num_lines))\n\n    num_elements_last_column = num_items % num_lines\n    if num_elements_last_column == 0:\n        num_elements_last_column = num_lines\n\n    lines = []\n    for i in range(num_lines):\n        line_size = num_columns\n        if i >= num_elements_last_column:\n            line_size -= 1\n        lines.append([None] * line_size)\n\n    for i, line in enumerate(lines):\n        line_size = len(line)\n        for j in range(line_size):\n            k = i + num_lines * j\n            item = strings[k]\n            if j % line_size != line_size - 1:\n                item_len = len(strip_ansi_escapes(item))\n                item = item + ' ' * (max_len - item_len)\n            line[j] = item\n\n    sep = ' ' * num_sep_spaces\n    for line in lines:\n        yield sep.join(line)", "response": "Format a list of strings like ls does multi - column output."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fuzzyfinder(text, collection):\n    suggestions = []\n    if not isinstance(text, six.text_type):\n        text = six.u(text)\n    pat = '.*?'.join(map(re.escape, text))\n    regex = re.compile(pat, flags=re.IGNORECASE)\n    for item in collection:\n        r = regex.search(item)\n        if r:\n            suggestions.append((len(r.group()), r.start(), item))\n\n    return (z for _, _, z in sorted(suggestions))", "response": "Fuzzy finder for a list of items."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading a Context object in place from user data directory.", "response": "def load_context(context, file_path=None):\n    \"\"\"Load a Context object in place from user data directory.\"\"\"\n    if not file_path:\n        file_path = _get_context_filepath()\n    if os.path.exists(file_path):\n        with io.open(file_path, encoding='utf-8') as f:\n            for line in f:\n                execute(line, context)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave a Context object to user data directory.", "response": "def save_context(context):\n    \"\"\"Save a Context object to user data directory.\"\"\"\n    file_path = _get_context_filepath()\n    content = format_to_http_prompt(context, excluded_options=EXCLUDED_OPTIONS)\n    with io.open(file_path, 'w', encoding='utf-8') as f:\n        f.write(content)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extract_args_for_httpie_main(context, method=None):\n    args = _extract_httpie_options(context)\n\n    if method:\n        args.append(method.upper())\n\n    args.append(context.url)\n    args += _extract_httpie_request_items(context)\n    return args", "response": "Transform a Context object to a list of arguments that can be passed to the HTTPie main function."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef format_to_httpie(context, method=None):\n    cmd = ['http'] + _extract_httpie_options(context, quote=True,\n                                             join_key_value=True)\n    if method:\n        cmd.append(method.upper())\n    cmd.append(context.url)\n    cmd += _extract_httpie_request_items(context, quote=True)\n    return ' '.join(cmd) + '\\n'", "response": "Format a Context object to an HTTPie command."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format_to_http_prompt(context, excluded_options=None):\n    cmds = _extract_httpie_options(context, quote=True, join_key_value=True,\n                                   excluded_keys=excluded_options)\n    cmds.append('cd ' + smart_quote(context.url))\n    cmds += _extract_httpie_request_items(context, quote=True)\n    return '\\n'.join(cmds) + '\\n'", "response": "Format a Context object to HTTP Prompt commands."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef initialize():\n    dst_path = get_user_config_path()\n    copied = False\n    if not os.path.exists(dst_path):\n        src_path = os.path.join(os.path.dirname(__file__), 'defaultconfig.py')\n        shutil.copyfile(src_path, dst_path)\n        copied = True\n    return copied, dst_path", "response": "Initialize a default config file if it doesn t exist yet."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_user():\n    config_path = get_user_config_path()\n    config = {}\n\n    # TODO: This may be overkill and too slow just for reading a config file\n    with open(config_path) as f:\n        code = compile(f.read(), config_path, 'exec')\n    exec(code, config)\n\n    keys = list(six.iterkeys(config))\n    for k in keys:\n        if k.startswith('_'):\n            del config[k]\n\n    return config", "response": "Read user config file and return it as a dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a FASTQ file from Illumina 1. 3 / 1. 5 quality scores into Sanger format.", "response": "def groom(in_file, data, in_qual=\"illumina\", out_dir=None, out_file=None):\n    \"\"\"\n    Grooms a FASTQ file from Illumina 1.3/1.5 quality scores into\n    sanger format, if it is not already in that format.\n    \"\"\"\n    if not out_file.endswith(\"gz\"):\n        out_file = \"%s.gz\" % out_file\n    seqtk = config_utils.get_program(\"seqtk\", data[\"config\"])\n    if in_qual == \"fastq-sanger\":\n        logger.info(\"%s is already in Sanger format.\" % in_file)\n        return out_file\n    with file_transaction(out_file) as tmp_out_file:\n        cmd = \"{seqtk} seq -Q64 {in_file} | gzip > {tmp_out_file}\".format(**locals())\n        do.run(cmd, \"Converting %s to Sanger format.\" % in_file)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfiltering reads in a fastq file that are shorter than a minimum length.", "response": "def filter_single_reads_by_length(in_file, quality_format, min_length=20,\n                                  out_file=None):\n    \"\"\"\n    removes reads from a fastq file which are shorter than a minimum\n    length\n\n    \"\"\"\n    logger.info(\"Removing reads in %s thare are less than %d bases.\"\n                % (in_file, min_length))\n    in_iterator = SeqIO.parse(in_file, quality_format)\n    out_iterator = (record for record in in_iterator if\n                    len(record.seq) > min_length)\n    with file_transaction(out_file) as tmp_out_file:\n        with open(tmp_out_file, \"w\") as out_handle:\n            SeqIO.write(out_iterator, out_handle, quality_format)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef filter_reads_by_length(fq1, fq2, quality_format, min_length=20):\n\n    logger.info(\"Removing reads in %s and %s that \"\n                \"are less than %d bases.\" % (fq1, fq2, min_length))\n    fq1_out = utils.append_stem(fq1, \".fixed\")\n    fq2_out = utils.append_stem(fq2, \".fixed\")\n    fq1_single = utils.append_stem(fq1, \".singles\")\n    fq2_single = utils.append_stem(fq2, \".singles\")\n    if all(map(utils.file_exists, [fq1_out, fq2_out, fq2_single, fq2_single])):\n        return [fq1_out, fq2_out]\n\n    fq1_in = SeqIO.parse(fq1, quality_format)\n    fq2_in = SeqIO.parse(fq2, quality_format)\n\n    out_files = [fq1_out, fq2_out, fq1_single, fq2_single]\n\n    with file_transaction(out_files) as tmp_out_files:\n        fq1_out_handle = open(tmp_out_files[0], \"w\")\n        fq2_out_handle = open(tmp_out_files[1], \"w\")\n        fq1_single_handle = open(tmp_out_files[2], \"w\")\n        fq2_single_handle = open(tmp_out_files[3], \"w\")\n\n        for fq1_record, fq2_record in zip(fq1_in, fq2_in):\n            if len(fq1_record.seq) >= min_length and len(fq2_record.seq) >= min_length:\n                fq1_out_handle.write(fq1_record.format(quality_format))\n                fq2_out_handle.write(fq2_record.format(quality_format))\n            else:\n                if len(fq1_record.seq) > min_length:\n                    fq1_single_handle.write(fq1_record.format(quality_format))\n                if len(fq2_record.seq) > min_length:\n                    fq2_single_handle.write(fq2_record.format(quality_format))\n        fq1_out_handle.close()\n        fq2_out_handle.close()\n        fq1_single_handle.close()\n        fq2_single_handle.close()\n\n    return [fq1_out, fq2_out]", "response": "Remove reads from a pair of fastq files that are shorter than min_length."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rstrip_extra(fname):\n    to_strip = (\"_R\", \".R\", \"-R\", \"_\", \"fastq\", \".\", \"-\")\n    while fname.endswith(to_strip):\n        for x in to_strip:\n            if fname.endswith(x):\n                fname = fname[:len(fname) - len(x)]\n                break\n    return fname", "response": "Strip extraneous non - discriminative filename info from the end of a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef combine_pairs(input_files, force_single=False, full_name=False, separators=None):\n    PAIR_FILE_IDENTIFIERS = set([\"1\", \"2\", \"3\", \"4\"])\n    if len(input_files) > 1000:\n        return fast_combine_pairs(input_files, force_single, full_name, separators)\n\n    pairs = []\n    used = set([])\n    used_separators = set([])\n    separators = separators if separators else (\"R\", \"_\", \"-\", \".\")\n    for in_file in input_files:\n        matches = set([])\n        if in_file in used:\n            continue\n        if not force_single:\n            for comp_file in input_files:\n                if comp_file in used or comp_file == in_file:\n                    continue\n                if full_name:\n                    in_file_name = in_file\n                    comp_file_name = comp_file\n                else:\n                    in_file_name = os.path.basename(in_file)\n                    comp_file_name = os.path.basename(comp_file)\n\n                a = rstrip_extra(utils.splitext_plus(in_file_name)[0])\n                b = rstrip_extra(utils.splitext_plus(comp_file_name)[0])\n                if len(a) != len(b):\n                    continue\n                s = dif(a,b)\n                # no differences, then its the same file stem\n                if len(s) == 0:\n                    logger.error(\"%s and %s have the same stem, so we don't know \"\n                                 \"how to assign it to the sample data in the CSV. To \"\n                                 \"get around this you can rename one of the files. \"\n                                 \"If they are meant to be the same sample run in two \"\n                                 \"lanes, combine them first with the \"\n                                 \"bcbio_prepare_samples.py script.\"\n                                 \"(http://bcbio-nextgen.readthedocs.io/en/latest/contents/configuration.html#multiple-files-per-sample)\"\n                                 % (in_file, comp_file))\n                    # continue\n                    sys.exit(1)\n                if len(s) > 1:\n                    continue #there is more than 1 difference\n                if (a[s[0]] in PAIR_FILE_IDENTIFIERS and\n                      b[s[0]] in PAIR_FILE_IDENTIFIERS):\n                    # if the 1/2 isn't the last digit before a separator, skip\n                    # this skips stuff like 2P 2A, often denoting replicates, not\n                    # read pairings\n                    if len(b) > (s[0] + 1):\n                        if (b[s[0]+1] not in (\"_\", \"-\", \".\")):\n                            continue\n                    # if the 1/2 is not a separator or prefaced with R, skip\n                    if b[s[0] - 1] in separators:\n                        used_separators.add(b[s[0] - 1])\n                        if len(used_separators) > 1:\n                            logger.warning(\"To split into paired reads multiple separators were used: %s\" % used_separators)\n                            logger.warning(\"This can lead to wrong assignation.\")\n                            logger.warning(\"Use --separator option in bcbio_prepare_samples.py to specify only one.\")\n                            logger.warning(\"For instance, --separator R.\")\n                        matches.update([in_file, comp_file])\n                        used.update([in_file, comp_file])\n\n            if matches:\n                pairs.append(sort_filenames(list(matches)))\n        if in_file not in used:\n            pairs.append([in_file])\n            used.add(in_file)\n    return pairs", "response": "Combine two file names into one file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fast_combine_pairs(files, force_single, full_name, separators):\n    files = sort_filenames(files)\n    chunks = tz.sliding_window(10, files)\n    pairs = [combine_pairs(chunk, force_single, full_name, separators) for chunk in chunks]\n    pairs = [y for x in pairs for y in x]\n    longest = defaultdict(list)\n    # for each file, save the longest pair it is in\n    for pair in pairs:\n        for file in pair:\n            if len(longest[file]) < len(pair):\n                longest[file] = pair\n    # keep only unique pairs\n    longest = {tuple(sort_filenames(x)) for x in longest.values()}\n    # ensure filenames are R1 followed by R2\n    return [sort_filenames(list(x)) for x in longest]", "response": "Combine files into a single or multiple file lists."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dif(a, b):\n    return [i for i in range(len(a)) if a[i] != b[i]]", "response": "returns a list of the indices of the elements in a that differ from b"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting N random headers from a fastq file without reading the whole thing into memory modified from: http://www.biostars.org/p/6544/ quick=True will just grab the first N reads rather than do a true downsampling", "response": "def downsample(f1, f2, data, N, quick=False):\n    \"\"\" get N random headers from a fastq file without reading the\n    whole thing into memory\n    modified from: http://www.biostars.org/p/6544/\n    quick=True will just grab the first N reads rather than do a true\n    downsampling\n    \"\"\"\n    if quick:\n        rand_records = range(N)\n    else:\n        records = sum(1 for _ in open(f1)) / 4\n        N = records if N > records else N\n        rand_records = sorted(random.sample(xrange(records), N))\n\n    fh1 = open_possible_gzip(f1)\n    fh2 = open_possible_gzip(f2) if f2 else None\n    outf1 = os.path.splitext(f1)[0] + \".subset\" + os.path.splitext(f1)[1]\n    outf2 = os.path.splitext(f2)[0] + \".subset\" + os.path.splitext(f2)[1] if f2 else None\n\n    if utils.file_exists(outf1):\n        if not outf2:\n            return outf1, outf2\n        elif utils.file_exists(outf2):\n            return outf1, outf2\n\n    out_files = (outf1, outf2) if outf2 else (outf1)\n\n    with file_transaction(out_files) as tx_out_files:\n        if isinstance(tx_out_files, six.string_types):\n            tx_out_f1 = tx_out_files\n        else:\n            tx_out_f1, tx_out_f2 = tx_out_files\n        sub1 = open_possible_gzip(tx_out_f1, \"w\")\n        sub2 = open_possible_gzip(tx_out_f2, \"w\") if outf2 else None\n        rec_no = - 1\n        for rr in rand_records:\n            while rec_no < rr:\n                rec_no += 1\n                for i in range(4): fh1.readline()\n                if fh2:\n                    for i in range(4): fh2.readline()\n            for i in range(4):\n                sub1.write(fh1.readline())\n                if sub2:\n                    sub2.write(fh2.readline())\n            rec_no += 1\n        fh1.close()\n        sub1.close()\n        if f2:\n            fh2.close()\n            sub2.close()\n\n    return outf1, outf2"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef estimate_read_length(fastq_file, quality_format=\"fastq-sanger\", nreads=1000):\n\n    in_handle = SeqIO.parse(open_fastq(fastq_file), quality_format)\n    read = next(in_handle)\n    average = len(read.seq)\n    for _ in range(nreads):\n        try:\n            average = (average + len(next(in_handle).seq)) / 2\n        except StopIteration:\n            break\n    in_handle.close()\n    return average", "response": "estimate average read length of a fastq file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef estimate_maximum_read_length(fastq_file, quality_format=\"fastq-sanger\",\n                                 nreads=1000):\n    \"\"\"\n    estimate average read length of a fastq file\n    \"\"\"\n    in_handle = SeqIO.parse(open_fastq(fastq_file), quality_format)\n    lengths = []\n    for _ in range(nreads):\n        try:\n            lengths.append(len(next(in_handle).seq))\n        except StopIteration:\n            break\n    in_handle.close()\n    return max(lengths)", "response": "estimate the maximum read length of a fastq file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nopen a fastq file using gzip if it is gzipped", "response": "def open_fastq(in_file):\n    \"\"\" open a fastq file, using gzip if it is gzipped\n    \"\"\"\n    if objectstore.is_remote(in_file):\n        return objectstore.open_file(in_file)\n    else:\n        return utils.open_gzipsafe(in_file)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns strelka2 variant calling.", "response": "def run(align_bams, items, ref_file, assoc_files, region, out_file):\n    \"\"\"Run strelka2 variant calling, either paired tumor/normal or germline calling.\n\n    region can be a single region or list of multiple regions for multicore calling.\n    \"\"\"\n    call_file = \"%s-raw.vcf.gz\" % utils.splitext_plus(out_file)[0]\n    strelka_work_dir = \"%s-work\" % utils.splitext_plus(out_file)[0]\n    paired = vcfutils.get_paired_bams(align_bams, items)\n    if paired:\n        assert paired.normal_bam, \"Strelka2 requires a normal sample\"\n        call_file = _run_somatic(paired, ref_file, assoc_files, region, call_file, strelka_work_dir)\n    else:\n        call_file = _run_germline(align_bams, items, ref_file,\n                                  assoc_files, region, call_file, strelka_work_dir)\n    return _af_annotate_and_filter(paired, items, call_file, out_file)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_region_bed(region, items, out_file, want_gzip=True):\n    variant_regions = bedutils.population_variant_regions(items, merged=True)\n    target = shared.subset_variant_regions(variant_regions, region, out_file, items)\n    if not target:\n        raise ValueError(\"Need BED input for strelka2 regions: %s %s\" % (region, target))\n    if not isinstance(target, six.string_types) or not os.path.isfile(target):\n        chrom, start, end = target\n        target = \"%s-regions.bed\" % utils.splitext_plus(out_file)[0]\n        with file_transaction(items[0], target) as tx_out_file:\n            with open(tx_out_file, \"w\") as out_handle:\n                out_handle.write(\"%s\\t%s\\t%s\\n\" % (chrom, start, end))\n    out_file = target\n    if want_gzip:\n        out_file = vcfutils.bgzip_and_index(out_file, items[0][\"config\"])\n    return out_file", "response": "Retrieve BED file of regions to analyze either single or multi - region."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate a coverage interval for the current region BED.", "response": "def coverage_interval_from_bed(bed_file, per_chrom=True):\n    \"\"\"Calculate a coverage interval for the current region BED.\n\n    This helps correctly work with cases of uneven coverage across an analysis\n    genome. strelka2 and other model based callers have flags for targeted and non\n    which depend on the local context.\n\n    Checks coverage per chromosome, avoiding non-standard chromosomes, if per_chrom is set.\n    Otherwise does a global check over all regions. The global check performs better for\n    strelka2 but not for DeepVariant:\n\n    https://github.com/bcbio/bcbio_validations/tree/master/deepvariant#deepvariant-v06-release-strelka2-stratification-and-initial-gatk-cnn\n    \"\"\"\n    total_starts = {}\n    total_ends = {}\n    bed_bases = collections.defaultdict(int)\n    with utils.open_gzipsafe(bed_file) as in_handle:\n        for line in in_handle:\n            parts = line.split()\n            if len(parts) >= 3:\n                chrom, start, end = parts[:3]\n                if chromhacks.is_autosomal(chrom):\n                    start = int(start)\n                    end = int(end)\n                    bed_bases[chrom] += (end - start)\n                    total_starts[chrom] = min([start, total_starts.get(chrom, sys.maxsize)])\n                    total_ends[chrom] = max([end, total_ends.get(chrom, 0)])\n    # can check per chromosome -- any one chromosome with larger, or over all regions\n    if per_chrom:\n        freqs = [float(bed_bases[c]) / float(total_ends[c] - total_starts[c]) for c in sorted(bed_bases.keys())]\n    elif len(bed_bases) > 0:\n        freqs = [sum([bed_bases[c] for c in sorted(bed_bases.keys())]) /\n                 sum([float(total_ends[c] - total_starts[c]) for c in sorted(bed_bases.keys())])]\n    else:\n        freqs = []\n    # Should be importing GENOME_COV_THRESH but get circular imports\n    if any([f >= 0.40 for f in freqs]):\n        return \"genome\"\n    else:\n        return \"targeted\""}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine if we should process a targeted region.", "response": "def _is_targeted_region(cur_bed, data):\n    \"\"\"Calculate if we should process region as a targeted or WGS.\n\n    Currently always based on total coverage interval, as that validates best and\n    is consistent between CWL (larger blocks) and non-CWL runs (smaller blocks).\n    We can check core usage and provide a consistent report when moving to CWL\n    exclusively.\n    \"\"\"\n    cores = dd.get_num_cores(data)\n    if cores > 0:  # Apply to all core setups now for consistency\n        return dd.get_coverage_interval(data) not in [\"genome\"]\n    else:\n        return coverage_interval_from_bed(cur_bed, per_chrom=False) == \"targeted\""}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves standard 0 / 1 0 0", "response": "def _tumor_normal_genotypes(ref, alt, info, fname, coords):\n    \"\"\"Retrieve standard 0/0, 0/1, 1/1 style genotypes from INFO field.\n\n    Normal -- NT field (ref, het, hom, conflict)\n    Tumor -- SGT field\n      - for SNPs specified as GG->TT for the normal and tumor diploid alleles. These\n        can also represent more complex alleles in which case we set at heterozygotes\n        pending longer term inclusion of genotypes in Strelka2 directly\n        (https://github.com/Illumina/strelka/issues/16)\n      - For indels, uses the ref, het, hom convention\n\n    ref: The REF allele from a VCF line\n    alt: A list of potentially multiple ALT alleles (rec.ALT.split(\";\"))\n    info: The VCF INFO field\n    fname, coords: not currently used, for debugging purposes\n    \"\"\"\n    known_names = set([\"het\", \"hom\", \"ref\", \"conflict\"])\n    def name_to_gt(val):\n        if val.lower() == \"het\":\n            return \"0/1\"\n        elif val.lower() == \"hom\":\n            return \"1/1\"\n        elif val.lower() in set([\"ref\", \"conflict\"]):\n            return \"0/0\"\n        else:\n            # Non-standard representations, het is our best imperfect representation\n            # print(fname, coords, ref, alt, info, val)\n            return \"0/1\"\n    def alleles_to_gt(val):\n        gt_indices = {gt.upper(): i for i, gt in enumerate([ref] + alt)}\n        tumor_gts = [gt_indices[x.upper()] for x in val if x in gt_indices]\n        if tumor_gts and val not in known_names:\n            if max(tumor_gts) == 0:\n                tumor_gt = \"0/0\"\n            elif 0 in tumor_gts:\n                tumor_gt = \"0/%s\" % min([x for x in tumor_gts if x > 0])\n            else:\n                tumor_gt = \"%s/%s\" % (min(tumor_gts), max(tumor_gts))\n        else:\n            tumor_gt = name_to_gt(val)\n        return tumor_gt\n    nt_val = [x.split(\"=\")[-1] for x in info if x.startswith(\"NT=\")][0]\n    normal_gt = name_to_gt(nt_val)\n    sgt_val = [x.split(\"=\")[-1] for x in info if x.startswith(\"SGT=\")]\n    if not sgt_val:\n        tumor_gt = \"0/0\"\n    else:\n        sgt_val = sgt_val[0].split(\"->\")[-1]\n        tumor_gt = alleles_to_gt(sgt_val)\n    return tumor_gt, normal_gt"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _af_annotate_and_filter(paired, items, in_file, out_file):\n    data = paired.tumor_data if paired else items[0]\n    min_freq = float(utils.get_in(data[\"config\"], (\"algorithm\", \"min_allele_fraction\"), 10)) / 100.0\n    logger.debug(\"Filtering Strelka2 calls with allele fraction threshold of %s\" % min_freq)\n    ungz_out_file = \"%s.vcf\" % utils.splitext_plus(out_file)[0]\n    if not utils.file_exists(ungz_out_file) and not utils.file_exists(ungz_out_file + \".gz\"):\n        with file_transaction(data, ungz_out_file) as tx_out_file:\n            vcf = cyvcf2.VCF(in_file)\n            vcf.add_format_to_header({\n                'ID': 'AF',\n                'Description': 'Allele frequency, as calculated in bcbio: AD/DP (germline), <ALT>U/DP (somatic snps), '\n                               'TIR/DPI (somatic indels)',\n                'Type': 'Float',\n                'Number': '.'})\n            vcf.add_filter_to_header({\n                'ID': 'MinAF',\n                'Description': 'Allele frequency is lower than %s%% ' % (min_freq*100) + (\n                    '(configured in bcbio as min_allele_fraction)'\n                    if utils.get_in(data[\"config\"], (\"algorithm\", \"min_allele_fraction\"))\n                    else '(default threshold in bcbio; override with min_allele_fraction in the algorithm section)')})\n            w = cyvcf2.Writer(tx_out_file, vcf)\n            tumor_index = vcf.samples.index(data['description'])\n            for rec in vcf:\n                if paired:  # somatic?\n                    if rec.is_snp:  # snps?\n                        alt_counts = rec.format(rec.ALT[0] + 'U')[:,0]  # {ALT}U=tier1_depth,tier2_depth\n                    else:  # indels\n                        alt_counts = rec.format('TIR')[:,0]  # TIR=tier1_depth,tier2_depth\n                    dp = rec.format('DP')[:,0]\n                elif rec.format(\"AD\") is not None:  # germline?\n                    alt_counts = rec.format('AD')[:,1:]  # AD=REF,ALT1,ALT2,...\n                    dp = np.sum(rec.format('AD')[:,0:], axis=1)[:, None]\n                else: # germline gVCF record\n                    alt_counts, dp = (None, None)\n                if dp is not None:\n                    with np.errstate(divide='ignore', invalid='ignore'):  # ignore division by zero and put AF=.0\n                        af = np.true_divide(alt_counts, dp)\n                        af[~np.isfinite(af)] = .0  # -inf inf NaN -> .0\n                    rec.set_format('AF', af)\n                    if paired and np.all(af[tumor_index] < min_freq):\n                        vcfutils.cyvcf_add_filter(rec, 'MinAF')\n                w.write_record(rec)\n            w.close()\n    return vcfutils.bgzip_and_index(ungz_out_file, data[\"config\"])", "response": "Annotate a VCF file with a single AF and dropping variants with AF < min_allele_fraction."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _postprocess_somatic(in_file, paired):\n    out_file = in_file.replace(\".vcf.gz\", \"-fixed.vcf\")\n    if not utils.file_exists(out_file) and not utils.file_exists(out_file + \".gz\"):\n        with file_transaction(paired.tumor_data, out_file) as tx_out_file:\n            with utils.open_gzipsafe(in_file) as in_handle:\n                with open(tx_out_file, \"w\") as out_handle:\n                    added_gt = False\n                    normal_index, tumor_index = (None, None)\n                    for line in in_handle:\n                        if line.startswith(\"##FORMAT\") and not added_gt:\n                            added_gt = True\n                            out_handle.write('##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">\\n')\n                            out_handle.write(line)\n                        elif line.startswith(\"#CHROM\"):\n                            assert added_gt\n                            parts = line.strip().split(\"\\t\")\n                            normal_index = parts.index(\"NORMAL\")\n                            tumor_index = parts.index(\"TUMOR\")\n                            line = line.replace(\"NORMAL\", paired.normal_name).replace(\"TUMOR\", paired.tumor_name)\n                            out_handle.write(line)\n                        elif line.startswith(\"#\"):\n                            out_handle.write(line)\n                        else:\n                            parts = line.rstrip().split(\"\\t\")\n                            tumor_gt, normal_gt = _tumor_normal_genotypes(parts[3], parts[4].split(\",\"),\n                                                                          parts[7].split(\";\"), in_file, parts[:2])\n                            parts[8] = \"GT:%s\" % parts[8]\n                            parts[normal_index] = \"%s:%s\" % (normal_gt, parts[normal_index])\n                            parts[tumor_index] = \"%s:%s\" % (tumor_gt, parts[tumor_index])\n                            out_handle.write(\"\\t\".join(parts) + \"\\n\")\n    return vcfutils.bgzip_and_index(out_file, paired.tumor_data[\"config\"])", "response": "Post - process somatic calls to provide standard output."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _run_workflow(data, workflow_file, work_dir):\n    utils.remove_safe(os.path.join(work_dir, \"workspace\"))\n    cmd = [utils.get_program_python(\"configureStrelkaGermlineWorkflow.py\"),\n           workflow_file, \"-m\", \"local\", \"-j\", dd.get_num_cores(data), \"--quiet\"]\n    do.run(cmd, \"Run Strelka2: %s\" % dd.get_sample_name(data))\n    utils.remove_safe(os.path.join(work_dir, \"workspace\"))", "response": "Run Strelka2 analysis inside prepared workflow directory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmerging strelka2 and Illumina compatible gVCFs with gvcfgenotyper.", "response": "def run_gvcfgenotyper(data, orig_region, vrn_files, out_file):\n    \"\"\"Merge strelka2 and Illumina compatible gVCFs with gvcfgenotyper.\n\n    https://github.com/Illumina/gvcfgenotyper\n\n    Also need to explore GLnexus (https://github.com/dnanexus-rnd/GLnexus)\n    \"\"\"\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            regions = _find_gvcf_blocks(vrn_files[0], bamprep.region_to_gatk(orig_region),\n                                        os.path.dirname(tx_out_file))\n            if len(regions) == 1:\n                _run_gvcfgenotyper(data, regions[0], vrn_files, tx_out_file)\n            else:\n                split_outs = [_run_gvcfgenotyper(data, r, vrn_files,\n                                                 \"%s-%s.vcf.gz\" % (utils.splitext_plus(out_file)[0],\n                                                                   r.replace(\":\", \"_\").replace(\"-\", \"_\")))\n                              for r in regions]\n                vcfutils.concat_variant_files(split_outs, tx_out_file, regions,\n                                              dd.get_ref_file(data), data[\"config\"])\n    return vcfutils.bgzip_and_index(out_file, data[\"config\"])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _run_gvcfgenotyper(data, region, vrn_files, out_file):\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            input_file = \"%s-inputs.txt\" % utils.splitext_plus(tx_out_file)[0]\n            with open(input_file, \"w\") as out_handle:\n                out_handle.write(\"%s\\n\" % \"\\n\".join(vrn_files))\n            cmd = [\"gvcfgenotyper\", \"-f\", dd.get_ref_file(data), \"-l\", input_file,\n                   \"-r\", region, \"-O\", \"z\", \"-o\", tx_out_file]\n            do.run(cmd, \"gvcfgenotyper: %s %s\" % (dd.get_sample_name(data), region))\n    return out_file", "response": "Run gvcfgenotyper on a single gVCF region."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _find_gvcf_blocks(vcf_file, region, tmp_dir):\n    region_file = os.path.join(tmp_dir, \"cur_region.bed\")\n    with open(region_file, \"w\") as out_handle:\n        chrom, coords = region.split(\":\")\n        start, end = coords.split(\"-\")\n        out_handle.write(\"\\t\".join([chrom, start, end]) + \"\\n\")\n    final_file = os.path.join(tmp_dir, \"split_regions.bed\")\n    cmd = \"gvcf_regions.py {vcf_file} | bedtools intersect -a - -b {region_file} > {final_file}\"\n    do.run(cmd.format(**locals()))\n    regions = []\n    with open(final_file) as in_handle:\n        for line in in_handle:\n            chrom, start, end = line.strip().split(\"\\t\")\n            regions.append(\"%s:%s-%s\" % (chrom, start, end))\n    return regions", "response": "Find gVCF blocks within a given region."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(samples, run_parallel):\n    to_process = []\n    extras = []\n    for data in (xs[0] for xs in samples):\n        hlacaller = tz.get_in([\"config\", \"algorithm\", \"hlacaller\"], data)\n        if hlacaller:\n            to_process.append(data)\n        else:\n            extras.append([data])\n    processed = run_parallel(\"call_hla\", ([x] for x in to_process))\n    return extras + processed", "response": "Run HLA detection on the input samples."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef align_bam(in_bam, ref_file, names, align_dir, data):\n    config = data[\"config\"]\n    out_file = os.path.join(align_dir, \"{0}-sort.bam\".format(names[\"lane\"]))\n    samtools = config_utils.get_program(\"samtools\", config)\n    bedtools = config_utils.get_program(\"bedtools\", config)\n    resources = config_utils.get_resources(\"samtools\", config)\n    num_cores = config[\"algorithm\"].get(\"num_cores\", 1)\n    # adjust memory for samtools since used for input and output\n    max_mem = config_utils.adjust_memory(resources.get(\"memory\", \"1G\"),\n                                         3, \"decrease\").upper()\n    if not utils.file_exists(out_file):\n        with tx_tmpdir(data) as work_dir:\n            with postalign.tobam_cl(data, out_file, bam.is_paired(in_bam)) as (tobam_cl, tx_out_file):\n                bwa_cmd = _get_bwa_mem_cmd(data, out_file, ref_file, \"-\")\n                tx_out_prefix = os.path.splitext(tx_out_file)[0]\n                prefix1 = \"%s-in1\" % tx_out_prefix\n                cmd = (\"unset JAVA_HOME && \"\n                       \"{samtools} sort -n -o -l 1 -@ {num_cores} -m {max_mem} {in_bam} {prefix1} \"\n                       \"| {bedtools} bamtofastq -i /dev/stdin -fq /dev/stdout -fq2 /dev/stdout \"\n                       \"| {bwa_cmd} | \")\n                cmd = cmd.format(**locals()) + tobam_cl\n                do.run(cmd, \"bwa mem alignment from BAM: %s\" % names[\"sample\"], None,\n                       [do.file_nonempty(tx_out_file), do.file_reasonable_size(tx_out_file, in_bam)])\n    return out_file", "response": "Perform direct alignment of an input BAM file with BWA."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_bwa_mem_cmd(data, out_file, ref_file, fastq1, fastq2=\"\"):\n    alt_file = ref_file + \".alt\"\n    if utils.file_exists(alt_file) and dd.get_hlacaller(data):\n        bwakit_dir = os.path.dirname(os.path.realpath(utils.which(\"run-bwamem\")))\n        hla_base = os.path.join(utils.safe_makedir(os.path.join(os.path.dirname(out_file), \"hla\")),\n                                os.path.basename(out_file) + \".hla\")\n        alt_cmd = (\" | {bwakit_dir}/k8 {bwakit_dir}/bwa-postalt.js -p {hla_base} {alt_file}\")\n    else:\n        alt_cmd = \"\"\n    if dd.get_aligner(data) == \"sentieon-bwa\":\n        bwa_exe = \"sentieon-bwa\"\n        exports = sentieon.license_export(data)\n    else:\n        bwa_exe = \"bwa\"\n        exports = \"\"\n    bwa = config_utils.get_program(bwa_exe, data[\"config\"])\n    num_cores = data[\"config\"][\"algorithm\"].get(\"num_cores\", 1)\n    bwa_resources = config_utils.get_resources(\"bwa\", data[\"config\"])\n    bwa_params = (\" \".join([str(x) for x in bwa_resources.get(\"options\", [])])\n                  if \"options\" in bwa_resources else \"\")\n    rg_info = novoalign.get_rg_info(data[\"rgnames\"])\n    # For UMI runs, pass along consensus tags\n    c_tags = \"-C\" if \"umi_bam\" in data else \"\"\n    pairing = \"-p\" if not fastq2 else \"\"\n    # Restrict seed occurances to 1/2 of default, manage memory usage for centromere repeats in hg38\n    # https://sourceforge.net/p/bio-bwa/mailman/message/31514937/\n    # http://ehc.ac/p/bio-bwa/mailman/message/32268544/\n    mem_usage = \"-c 250\"\n    bwa_cmd = (\"{exports}{bwa} mem {pairing} {c_tags} {mem_usage} -M -t {num_cores} {bwa_params} -R '{rg_info}' \"\n               \"-v 1 {ref_file} {fastq1} {fastq2} \")\n    return (bwa_cmd + alt_cmd).format(**locals())", "response": "Retrieve commands to use for piped bwa mem mapping."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if we can use memory.", "response": "def _can_use_mem(fastq_file, data, read_min_size=None):\n    \"\"\"bwa-mem handle longer (> 70bp) reads with improved piping.\n    Randomly samples 5000 reads from the first two million.\n    Default to no piping if more than 75% of the sampled reads are small.\n    If we've previously calculated minimum read sizes (from rtg SDF output)\n    we can skip the formal check.\n    \"\"\"\n    min_size = 70\n    if read_min_size and read_min_size >= min_size:\n        return True\n    thresh = 0.75\n    tocheck = 5000\n    shorter = 0\n    for count, size in fastq_size_output(fastq_file, tocheck):\n        if int(size) < min_size:\n            shorter += int(count)\n    return (float(shorter) / float(tocheck)) <= thresh"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms piped alignment of fastq files generating sorted output BAM.", "response": "def align_pipe(fastq_file, pair_file, ref_file, names, align_dir, data):\n    \"\"\"Perform piped alignment of fastq input files, generating sorted output BAM.\n    \"\"\"\n    pair_file = pair_file if pair_file else \"\"\n    # back compatible -- older files were named with lane information, use sample name now\n    if names[\"lane\"] != dd.get_sample_name(data):\n        out_file = os.path.join(align_dir, \"{0}-sort.bam\".format(names[\"lane\"]))\n    else:\n        out_file = None\n    if not out_file or not utils.file_exists(out_file):\n        umi_ext = \"-cumi\" if \"umi_bam\" in data else \"\"\n        out_file = os.path.join(align_dir, \"{0}-sort{1}.bam\".format(dd.get_sample_name(data), umi_ext))\n    qual_format = data[\"config\"][\"algorithm\"].get(\"quality_format\", \"\").lower()\n    min_size = None\n    if data.get(\"align_split\") or fastq_file.endswith(\".sdf\"):\n        if fastq_file.endswith(\".sdf\"):\n            min_size = rtg.min_read_size(fastq_file)\n        final_file = out_file\n        out_file, data = alignprep.setup_combine(final_file, data)\n        fastq_file, pair_file = alignprep.split_namedpipe_cls(fastq_file, pair_file, data)\n    else:\n        final_file = None\n        if qual_format == \"illumina\":\n            fastq_file = alignprep.fastq_convert_pipe_cl(fastq_file, data)\n            if pair_file:\n                pair_file = alignprep.fastq_convert_pipe_cl(pair_file, data)\n    rg_info = novoalign.get_rg_info(names)\n    if not utils.file_exists(out_file) and (final_file is None or not utils.file_exists(final_file)):\n        # If we cannot do piping, use older bwa aln approach\n        if (\"bwa-mem\" not in dd.get_tools_on(data) and\n              (\"bwa-mem\" in dd.get_tools_off(data) or not _can_use_mem(fastq_file, data, min_size))):\n            out_file = _align_backtrack(fastq_file, pair_file, ref_file, out_file,\n                                        names, rg_info, data)\n        else:\n            out_file = _align_mem(fastq_file, pair_file, ref_file, out_file,\n                                  names, rg_info, data)\n    data[\"work_bam\"] = out_file\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nperform bwa - mem alignment on supported read lengths.", "response": "def _align_mem(fastq_file, pair_file, ref_file, out_file, names, rg_info, data):\n    \"\"\"Perform bwa-mem alignment on supported read lengths.\n    \"\"\"\n    with postalign.tobam_cl(data, out_file, pair_file != \"\") as (tobam_cl, tx_out_file):\n        cmd = (\"unset JAVA_HOME && \"\n               \"%s | %s\" % (_get_bwa_mem_cmd(data, out_file, ref_file, fastq_file, pair_file), tobam_cl))\n        do.run(cmd, \"bwa mem alignment from fastq: %s\" % names[\"sample\"], None,\n                [do.file_nonempty(tx_out_file), do.file_reasonable_size(tx_out_file, fastq_file)])\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _align_backtrack(fastq_file, pair_file, ref_file, out_file, names, rg_info, data):\n    bwa = config_utils.get_program(\"bwa\", data[\"config\"])\n    config = data[\"config\"]\n    sai1_file = \"%s_1.sai\" % os.path.splitext(out_file)[0]\n    sai2_file = \"%s_2.sai\" % os.path.splitext(out_file)[0] if pair_file else \"\"\n    if not utils.file_exists(sai1_file):\n        with file_transaction(data, sai1_file) as tx_sai1_file:\n            _run_bwa_align(fastq_file, ref_file, tx_sai1_file, config)\n    if sai2_file and not utils.file_exists(sai2_file):\n        with file_transaction(data, sai2_file) as tx_sai2_file:\n            _run_bwa_align(pair_file, ref_file, tx_sai2_file, config)\n    with postalign.tobam_cl(data, out_file, pair_file != \"\") as (tobam_cl, tx_out_file):\n        align_type = \"sampe\" if sai2_file else \"samse\"\n        cmd = (\"unset JAVA_HOME && {bwa} {align_type} -r '{rg_info}' {ref_file} {sai1_file} {sai2_file} \"\n               \"{fastq_file} {pair_file} | \")\n        cmd = cmd.format(**locals()) + tobam_cl\n        do.run(cmd, \"bwa %s\" % align_type, data)\n    return out_file", "response": "Perform a BWA alignment using the AlignBacktrack algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef index_transcriptome(gtf_file, ref_file, data):\n    gtf_fasta = gtf.gtf_to_fasta(gtf_file, ref_file)\n    return build_bwa_index(gtf_fasta, data)", "response": "index a transcriptome using a GTF file and a reference FASTA file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef align_transcriptome(fastq_file, pair_file, ref_file, data):\n    work_bam = dd.get_work_bam(data)\n    base, ext = os.path.splitext(work_bam)\n    out_file = base + \".transcriptome\" + ext\n    if utils.file_exists(out_file):\n        data = dd.set_transcriptome_bam(data, out_file)\n        return data\n    # bwa mem needs phred+33 quality, so convert if it is Illumina\n    if dd.get_quality_format(data).lower() == \"illumina\":\n        logger.info(\"bwa mem does not support the phred+64 quality format, \"\n                    \"converting %s and %s to phred+33.\")\n        fastq_file = fastq.groom(fastq_file, data, in_qual=\"fastq-illumina\")\n        if pair_file:\n            pair_file = fastq.groom(pair_file, data, in_qual=\"fastq-illumina\")\n    bwa = config_utils.get_program(\"bwa\", data[\"config\"])\n    gtf_file = dd.get_gtf_file(data)\n    gtf_fasta = index_transcriptome(gtf_file, ref_file, data)\n    args = \" \".join(_bwa_args_from_config(data[\"config\"]))\n    num_cores = data[\"config\"][\"algorithm\"].get(\"num_cores\", 1)\n    samtools = config_utils.get_program(\"samtools\", data[\"config\"])\n    cmd = (\"{bwa} mem {args} -a -t {num_cores} {gtf_fasta} {fastq_file} \"\n           \"{pair_file} \")\n    with file_transaction(data, out_file) as tx_out_file:\n        message = \"Aligning %s and %s to the transcriptome.\" % (fastq_file, pair_file)\n        cmd += \"| \" + postalign.sam_to_sortbam_cl(data, tx_out_file, name_sort=True)\n        do.run(cmd.format(**locals()), message)\n    data = dd.set_transcriptome_bam(data, out_file)\n    return data", "response": "Align a set of fastq files to the transcriptome."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfilter a BWA alignment file for uniquely mapped reads.", "response": "def filter_multimappers(align_file, data):\n    \"\"\"\n    Filtering a BWA alignment file for uniquely mapped reads, from here:\n    https://bioinformatics.stackexchange.com/questions/508/obtaining-uniquely-mapped-reads-from-bwa-mem-alignment\n    \"\"\"\n    config = dd.get_config(data)\n    type_flag = \"\" if bam.is_bam(align_file) else \"S\"\n    base, ext = os.path.splitext(align_file)\n    out_file = base + \".unique\" + ext\n    bed_file = dd.get_variant_regions(data) or dd.get_sample_callable(data)\n    bed_cmd = '-L {0}'.format(bed_file) if bed_file else \" \"\n    if utils.file_exists(out_file):\n        return out_file\n    base_filter = '-F \"not unmapped {paired_filter} and not duplicate and [XA] == null and [SA] == null and not supplementary \" '\n    if bam.is_paired(align_file):\n        paired_filter = \"and paired and proper_pair\"\n    else:\n        paired_filter = \"\"\n    filter_string = base_filter.format(paired_filter=paired_filter)\n    sambamba = config_utils.get_program(\"sambamba\", config)\n    num_cores = dd.get_num_cores(data)\n    with file_transaction(out_file) as tx_out_file:\n        cmd = ('{sambamba} view -h{type_flag} '\n               '--nthreads {num_cores} '\n               '-f bam {bed_cmd} '\n               '{filter_string} '\n               '{align_file} '\n               '> {tx_out_file}')\n        message = \"Removing multimapped reads from %s.\" % align_file\n        do.run(cmd.format(**locals()), message)\n    bam.index(out_file, config)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run_main(workdir, config_file=None, fc_dir=None, run_info_yaml=None,\n             parallel=None, workflow=None):\n    \"\"\"Run variant analysis, handling command line options.\n    \"\"\"\n    # Set environment to standard to use periods for decimals and avoid localization\n    os.environ[\"LC_ALL\"] = \"C\"\n    os.environ[\"LC\"] = \"C\"\n    os.environ[\"LANG\"] = \"C\"\n    workdir = utils.safe_makedir(os.path.abspath(workdir))\n    os.chdir(workdir)\n    config, config_file = config_utils.load_system_config(config_file, workdir)\n    if config.get(\"log_dir\", None) is None:\n        config[\"log_dir\"] = os.path.join(workdir, DEFAULT_LOG_DIR)\n    if parallel[\"type\"] in [\"local\", \"clusterk\"]:\n        _setup_resources()\n        _run_toplevel(config, config_file, workdir, parallel,\n                      fc_dir, run_info_yaml)\n    elif parallel[\"type\"] == \"ipython\":\n        assert parallel[\"scheduler\"] is not None, \"IPython parallel requires a specified scheduler (-s)\"\n        if parallel[\"scheduler\"] != \"sge\":\n            assert parallel[\"queue\"] is not None, \"IPython parallel requires a specified queue (-q)\"\n        elif not parallel[\"queue\"]:\n            parallel[\"queue\"] = \"\"\n        _run_toplevel(config, config_file, workdir, parallel,\n                      fc_dir, run_info_yaml)\n    else:\n        raise ValueError(\"Unexpected type of parallel run: %s\" % parallel[\"type\"])", "response": "Run the variant analysis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _setup_resources():\n    target_procs = 10240\n    cur_proc, max_proc = resource.getrlimit(resource.RLIMIT_NPROC)\n    target_proc = min(max_proc, target_procs) if max_proc > 0 else target_procs\n    resource.setrlimit(resource.RLIMIT_NPROC, (max(cur_proc, target_proc), max_proc))\n    cur_hdls, max_hdls = resource.getrlimit(resource.RLIMIT_NOFILE)\n    target_hdls = min(max_hdls, target_procs) if max_hdls > 0 else target_procs\n    resource.setrlimit(resource.RLIMIT_NOFILE, (max(cur_hdls, target_hdls), max_hdls))", "response": "Attempt to increase resource limits up to hard limits. can\n    This allows us to avoid out of file handle limits where we can t do hard limits."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _run_toplevel(config, config_file, work_dir, parallel,\n                  fc_dir=None, run_info_yaml=None):\n    \"\"\"\n    Run toplevel analysis, processing a set of input files.\n    config_file -- Main YAML configuration file with system parameters\n    fc_dir -- Directory of fastq files to process\n    run_info_yaml -- YAML configuration file specifying inputs to process\n    \"\"\"\n    parallel = log.create_base_logger(config, parallel)\n    log.setup_local_logging(config, parallel)\n    logger.info(\"System YAML configuration: %s\" % os.path.abspath(config_file))\n    dirs = run_info.setup_directories(work_dir, fc_dir, config, config_file)\n    config_file = os.path.join(dirs[\"config\"], os.path.basename(config_file))\n    pipelines, config = _pair_samples_with_pipelines(run_info_yaml, config)\n    system.write_info(dirs, parallel, config)\n    with tx_tmpdir(config if parallel.get(\"type\") == \"local\" else None) as tmpdir:\n        tempfile.tempdir = tmpdir\n        for pipeline, samples in pipelines.items():\n            for xs in pipeline(config, run_info_yaml, parallel, dirs, samples):\n                pass", "response": "Run the top - level analysis on the input files."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds resource information to the parallel environment.", "response": "def _wres(parallel, progs, fresources=None, ensure_mem=None):\n    \"\"\"Add resource information to the parallel environment on required programs and files.\n\n    Enables spinning up required machines and operating in non-shared filesystem\n    environments.\n\n    progs -- Third party tools used in processing\n    fresources -- Required file-based resources needed. These will be transferred on non-shared\n                  filesystems.\n    ensure_mem -- Dictionary of required minimum memory for programs used. Ensures\n                  enough memory gets allocated on low-core machines.\n    \"\"\"\n    parallel = copy.deepcopy(parallel)\n    parallel[\"progs\"] = progs\n    if fresources:\n        parallel[\"fresources\"] = fresources\n    if ensure_mem:\n        parallel[\"ensure_mem\"] = ensure_mem\n    return parallel"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rnaseq_prep_samples(config, run_info_yaml, parallel, dirs, samples):\n    pipeline = dd.get_in_samples(samples, dd.get_analysis)\n    trim_reads_set = any([tz.get_in([\"algorithm\", \"trim_reads\"], d) for d in dd.sample_data_iterator(samples)])\n    resources = [\"picard\"]\n    needs_trimming = (_is_smallrnaseq(pipeline) or trim_reads_set)\n    if needs_trimming:\n        resources.append(\"atropos\")\n    with prun.start(_wres(parallel, resources),\n                    samples, config, dirs, \"trimming\",\n                    max_multicore=1 if not needs_trimming else None) as run_parallel:\n        with profile.report(\"organize samples\", dirs):\n            samples = run_parallel(\"organize_samples\", [[dirs, config, run_info_yaml,\n                                                            [x[0][\"description\"] for x in samples]]])\n            samples = run_parallel(\"prepare_sample\", samples)\n        if needs_trimming:\n            with profile.report(\"adapter trimming\", dirs):\n                if _is_smallrnaseq(pipeline):\n                    samples = run_parallel(\"trim_srna_sample\", samples)\n                else:\n                    samples = run_parallel(\"trim_sample\", samples)\n    return samples", "response": "Prepare samples for RNA - seq and small - RNAseq."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _pair_samples_with_pipelines(run_info_yaml, config):\n    samples = config_utils.load_config(run_info_yaml)\n    if isinstance(samples, dict):\n        resources = samples.pop(\"resources\")\n        samples = samples[\"details\"]\n    else:\n        resources = {}\n    ready_samples = []\n    for sample in samples:\n        if \"files\" in sample:\n            del sample[\"files\"]\n        # add any resources to this item to recalculate global configuration\n        usample = copy.deepcopy(sample)\n        usample.pop(\"algorithm\", None)\n        if \"resources\" not in usample:\n            usample[\"resources\"] = {}\n        for prog, pkvs in resources.items():\n            if prog not in usample[\"resources\"]:\n                usample[\"resources\"][prog] = {}\n            if pkvs is not None:\n                for key, val in pkvs.items():\n                    usample[\"resources\"][prog][key] = val\n        config = config_utils.update_w_custom(config, usample)\n        sample[\"resources\"] = {}\n        ready_samples.append(sample)\n    paired = [(x, _get_pipeline(x)) for x in ready_samples]\n    d = defaultdict(list)\n    for x in paired:\n        d[x[1]].append([x[0]])\n    return d, config", "response": "Map samples defined in input file to pipelines to run."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef umi_transform(data):\n    fq1 = data[\"files\"][0]\n    umi_dir = os.path.join(dd.get_work_dir(data), \"umis\")\n    safe_makedir(umi_dir)\n    transform = dd.get_umi_type(data)\n\n    if not transform:\n        logger.info(\"No UMI transform specified, assuming pre-transformed data.\")\n        if is_transformed(fq1):\n            logger.info(\"%s detected as pre-transformed, passing it on unchanged.\" % fq1)\n            data[\"files\"] = [fq1]\n            return data\n        else:\n            logger.error(\"No UMI transform was specified, but %s does not look \"\n                         \"pre-transformed. Assuming non-umi data.\" % fq1)\n            return data\n\n    if file_exists(transform):\n        transform_file = transform\n    else:\n        transform_file = get_transform_file(transform)\n        if not file_exists(transform_file):\n            logger.error(\n                \"The UMI transform can be specified as either a file or a \"\n                \"bcbio-supported transform. Either the file %s does not exist \"\n                \"or the transform is not supported by bcbio. Supported \"\n                \"transforms are %s.\"\n                % (dd.get_umi_type(data), \", \".join(SUPPORTED_TRANSFORMS)))\n            sys.exit(1)\n    out_base = dd.get_sample_name(data) + \".umitransformed.fq.gz\"\n    out_file = os.path.join(umi_dir, out_base)\n    if file_exists(out_file):\n        data[\"files\"] = [out_file]\n        return data\n    umis = config_utils.get_program(\"umis\", data, default=\"umis\")\n    cores = dd.get_num_cores(data)\n    # skip transformation if the file already looks transformed\n    with open_fastq(fq1) as in_handle:\n        read = next(in_handle)\n        if \"UMI_\" in read:\n            data[\"files\"] = [out_file]\n            return data\n\n    cmd = (\"{umis} fastqtransform {transform_file} \"\n           \"--cores {cores} \"\n           \"{fq1}\"\n           \"| seqtk seq -L 20 - | gzip > {tx_out_file}\")\n    message = (\"Inserting UMI and barcode information into the read name of %s\"\n               % fq1)\n    with file_transaction(out_file) as tx_out_file:\n        do.run(cmd.format(**locals()), message)\n    data[\"files\"] = [out_file]\n    return data", "response": "Transform each read in the UMI file into a single UMI file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses HLA calls from called genotype files.", "response": "def run(data):\n    \"\"\"HLA typing with bwakit, parsing output from called genotype files.\n    \"\"\"\n    bwakit_dir = os.path.dirname(os.path.realpath(utils.which(\"run-bwamem\")))\n    hla_fqs = tz.get_in([\"hla\", \"fastq\"], data, [])\n    if len(hla_fqs) > 0:\n        hla_base = os.path.commonprefix(hla_fqs)\n        while hla_base.endswith(\".\"):\n            hla_base = hla_base[:-1]\n        out_file = hla_base + \".top\"\n        if not utils.file_exists(out_file):\n            cmd = \"{bwakit_dir}/run-HLA {hla_base}\"\n            do.run(cmd.format(**locals()), \"HLA typing with bwakit\")\n            out_file = _organize_calls(out_file, hla_base, data)\n        data[\"hla\"].update({\"call_file\": out_file,\n                            \"hlacaller\": \"bwakit\"})\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\norganizing genotype calls into a single file.", "response": "def _organize_calls(out_file, hla_base, data):\n    \"\"\"Prepare genotype calls, reporting best call along with quality metrics.\n    \"\"\"\n    hla_truth = get_hla_truthset(data)\n    sample = dd.get_sample_name(data)\n    with file_transaction(data, out_file) as tx_out_file:\n        with open(tx_out_file, \"w\") as out_handle:\n            writer = csv.writer(out_handle)\n            writer.writerow([\"sample\", \"locus\", \"mismatches\", \"options\", \"alleles\", \"p-groups\", \"expected\",\n                             \"validates\"])\n            for genotype_file in glob.glob(\"%s.HLA-*.gt\" % (hla_base)):\n                hla_locus = os.path.basename(genotype_file).replace(\n                        \"%s.HLA-\" % os.path.basename(hla_base), \"\").replace(\".gt\", \"\")\n                with open(genotype_file) as in_handle:\n                    total_options = set([])\n                    for i, line in enumerate(in_handle):\n                        _, aone, atwo, m = line.split(\"\\t\")[:4]\n                        pgroups = (hla_groups.hla_protein(aone, data), hla_groups.hla_protein(atwo, data))\n                        if i == 0:\n                            call_alleles = [aone, atwo]\n                            call_pgroups = pgroups\n                            mismatches = m\n                        total_options.add(pgroups)\n                    if len(total_options) > 0:\n                        truth_alleles = tz.get_in([sample, hla_locus], hla_truth, [])\n                        writer.writerow([sample, hla_locus, mismatches, len(total_options),\n                                         \";\".join(call_alleles), \";\".join(call_pgroups),\n                                         \";\".join(truth_alleles), matches_truth(call_alleles, truth_alleles, data)])\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving expected truth calls for annotating HLA called output.", "response": "def get_hla_truthset(data):\n    \"\"\"Retrieve expected truth calls for annotating HLA called output.\n    \"\"\"\n    val_csv = tz.get_in([\"config\", \"algorithm\", \"hlavalidate\"], data)\n    out = {}\n    if val_csv and utils.file_exists(val_csv):\n        with open(val_csv) as in_handle:\n            reader = csv.reader(in_handle)\n            next(reader) # header\n            for sample, locus, alleles in (l for l in reader if l):\n                out = tz.update_in(out, [sample, locus], lambda x: [x.strip() for x in alleles.split(\";\")])\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sample_callable_bed(bam_file, ref_file, data):\n    from bcbio.heterogeneity import chromhacks\n    CovInfo = collections.namedtuple(\"CovInfo\", \"callable, raw_callable, depth_files\")\n    noalt_calling = \"noalt_calling\" in dd.get_tools_on(data) or \"altcontigs\" in dd.get_exclude_regions(data)\n    def callable_chrom_filter(r):\n        \"\"\"Filter to callable region, potentially limiting by chromosomes.\n        \"\"\"\n        return r.name == \"CALLABLE\" and (not noalt_calling or chromhacks.is_nonalt(r.chrom))\n    out_file = \"%s-callable_sample.bed\" % os.path.splitext(bam_file)[0]\n    with shared.bedtools_tmpdir(data):\n        sv_bed = regions.get_sv_bed(data)\n        callable_bed, depth_files = coverage.calculate(bam_file, data, sv_bed)\n        input_regions_bed = dd.get_variant_regions(data)\n        if not utils.file_uptodate(out_file, callable_bed):\n            with file_transaction(data, out_file) as tx_out_file:\n                callable_regions = pybedtools.BedTool(callable_bed)\n                filter_regions = callable_regions.filter(callable_chrom_filter)\n                if input_regions_bed:\n                    if not utils.file_uptodate(out_file, input_regions_bed):\n                        input_regions = pybedtools.BedTool(input_regions_bed)\n                        filter_regions.intersect(input_regions, nonamecheck=True).saveas(tx_out_file)\n                else:\n                    filter_regions.saveas(tx_out_file)\n    return CovInfo(out_file, callable_bed, depth_files)", "response": "Retrieve callable regions for a sample subset by defined analysis regions."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_ref_bedtool(ref_file, config, chrom=None):\n    broad_runner = broad.runner_from_path(\"picard\", config)\n    ref_dict = broad_runner.run_fn(\"picard_index_ref\", ref_file)\n    ref_lines = []\n    with pysam.Samfile(ref_dict, \"r\") as ref_sam:\n        for sq in ref_sam.header[\"SQ\"]:\n            if not chrom or sq[\"SN\"] == chrom:\n                ref_lines.append(\"%s\\t%s\\t%s\" % (sq[\"SN\"], 0, sq[\"LN\"]))\n    return pybedtools.BedTool(\"\\n\".join(ref_lines), from_string=True)", "response": "Retrieve a pybedtool BedTool object with reference sizes from input reference."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve coordinates of regions in reference genome with no mapping.", "response": "def _get_nblock_regions(in_file, min_n_size, ref_regions):\n    \"\"\"Retrieve coordinates of regions in reference genome with no mapping.\n    These are potential breakpoints for parallelizing analysis.\n    \"\"\"\n    out_lines = []\n    called_contigs = set([])\n    with utils.open_gzipsafe(in_file) as in_handle:\n        for line in in_handle:\n            contig, start, end, ctype = line.rstrip().split()\n            called_contigs.add(contig)\n            if (ctype in [\"REF_N\", \"NO_COVERAGE\", \"EXCESSIVE_COVERAGE\", \"LOW_COVERAGE\"] and\n                  int(end) - int(start) > min_n_size):\n                out_lines.append(\"%s\\t%s\\t%s\\n\" % (contig, start, end))\n    for refr in ref_regions:\n        if refr.chrom not in called_contigs:\n            out_lines.append(\"%s\\t%s\\t%s\\n\" % (refr.chrom, 0, refr.stop))\n    return pybedtools.BedTool(\"\\n\".join(out_lines), from_string=True)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _combine_regions(all_regions, ref_regions):\n    chrom_order = {}\n    for i, x in enumerate(ref_regions):\n        chrom_order[x.chrom] = i\n    def wchrom_key(x):\n        chrom, start, end = x\n        return (chrom_order[chrom], start, end)\n    all_intervals = []\n    for region_group in all_regions:\n        for region in region_group:\n            all_intervals.append((region.chrom, int(region.start), int(region.stop)))\n    all_intervals.sort(key=wchrom_key)\n    bed_lines = [\"%s\\t%s\\t%s\" % (c, s, e) for (c, s, e) in all_intervals]\n    return pybedtools.BedTool(\"\\n\".join(bed_lines), from_string=True)", "response": "Combine multiple BEDtools regions of regions into sorted final BEDtool."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _add_config_regions(nblock_regions, ref_regions, data):\n    input_regions_bed = dd.get_variant_regions(data)\n    if input_regions_bed:\n        input_regions = pybedtools.BedTool(input_regions_bed)\n        # work around problem with single region not subtracted correctly.\n        if len(input_regions) == 1:\n            str_regions = str(input_regions[0]).strip()\n            input_regions = pybedtools.BedTool(\"%s\\n%s\" % (str_regions, str_regions),\n                                               from_string=True)\n        input_nblock = ref_regions.subtract(input_regions, nonamecheck=True)\n        if input_nblock == ref_regions:\n            raise ValueError(\"Input variant_region file (%s) \"\n                             \"excludes all genomic regions. Do the chromosome names \"\n                             \"in the BED file match your genome (chr1 vs 1)?\" % input_regions_bed)\n        all_intervals = _combine_regions([input_nblock, nblock_regions], ref_regions)\n    else:\n        all_intervals = nblock_regions\n    if \"noalt_calling\" in dd.get_tools_on(data) or \"altcontigs\" in dd.get_exclude_regions(data):\n        from bcbio.heterogeneity import chromhacks\n        remove_intervals = ref_regions.filter(lambda r: not chromhacks.is_nonalt(r.chrom))\n        all_intervals = _combine_regions([all_intervals, remove_intervals], ref_regions)\n    return all_intervals.merge()", "response": "Add additional nblock regions based on user defined regions to call."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding blocks of regions for analysis from a BAM file.", "response": "def block_regions(callable_bed, in_bam, ref_file, data):\n    \"\"\"Find blocks of regions for analysis from mapped input BAM file.\n\n    Identifies islands of callable regions, surrounding by regions\n    with no read support, that can be analyzed independently.\n    \"\"\"\n    min_n_size = int(data[\"config\"][\"algorithm\"].get(\"nomap_split_size\", 250))\n    with shared.bedtools_tmpdir(data):\n        nblock_bed = \"%s-nblocks.bed\" % utils.splitext_plus(callable_bed)[0]\n        callblock_bed = \"%s-callableblocks.bed\" % utils.splitext_plus(callable_bed)[0]\n        if not utils.file_uptodate(nblock_bed, callable_bed):\n            ref_regions = get_ref_bedtool(ref_file, data[\"config\"])\n            nblock_regions = _get_nblock_regions(callable_bed, min_n_size, ref_regions)\n            nblock_regions = _add_config_regions(nblock_regions, ref_regions, data)\n            with file_transaction(data, nblock_bed, callblock_bed) as (tx_nblock_bed, tx_callblock_bed):\n                nblock_regions.filter(lambda r: len(r) > min_n_size).saveas(tx_nblock_bed)\n                if len(ref_regions.subtract(nblock_regions, nonamecheck=True)) > 0:\n                    ref_regions.subtract(tx_nblock_bed, nonamecheck=True).merge(d=min_n_size).saveas(tx_callblock_bed)\n                else:\n                    raise ValueError(\"No callable regions found in %s from BAM file %s. Some causes:\\n \"\n                                     \" - Alignment regions do not overlap with regions found \"\n                                     \"in your `variant_regions` BED: %s\\n\"\n                                     \" - There are no aligned reads in your BAM file that pass sanity checks \"\n                                     \" (mapping score > 1, non-duplicates, both ends of paired reads mapped)\"\n                                     % (dd.get_sample_name(data), in_bam, dd.get_variant_regions(data)))\n    return callblock_bed, nblock_bed"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nproviding statistics on sizes and number of analysis blocks.", "response": "def _analysis_block_stats(regions, samples):\n    \"\"\"Provide statistics on sizes and number of analysis blocks.\n    \"\"\"\n    prev = None\n    between_sizes = []\n    region_sizes = []\n    for region in regions:\n        if prev and prev.chrom == region.chrom:\n            between_sizes.append(region.start - prev.end)\n        region_sizes.append(region.end - region.start)\n        prev = region\n    def descriptive_stats(xs):\n        if len(xs) < 2:\n            return xs\n        parts = [\"min: %s\" % min(xs),\n                 \"5%%: %s\" % numpy.percentile(xs, 5),\n                 \"25%%: %s\" % numpy.percentile(xs, 25),\n                 \"median: %s\" % numpy.percentile(xs, 50),\n                 \"75%%: %s\" % numpy.percentile(xs, 75),\n                 \"95%%: %s\" % numpy.percentile(xs, 95),\n                 \"99%%: %s\" % numpy.percentile(xs, 99),\n                 \"max: %s\" % max(xs)]\n        return \"\\n\".join([\"  \" + x for x in parts])\n    logger.info(\"Identified %s parallel analysis blocks\\n\" % len(region_sizes) +\n                \"Block sizes:\\n%s\\n\" % descriptive_stats(region_sizes) +\n                \"Between block sizes:\\n%s\\n\" % descriptive_stats(between_sizes))\n    if len(region_sizes) == 0:\n        raise ValueError(\"No callable regions found in: %s\" %\n                         (\", \".join([dd.get_sample_name(x) for x in samples])))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _needs_region_update(out_file, samples):\n    nblock_files = [x[\"regions\"][\"nblock\"] for x in samples if \"regions\" in x]\n    # For older approaches and do not create a new set of analysis\n    # regions, since the new algorithm will re-do all BAM and variant\n    # steps with new regions\n    for nblock_file in nblock_files:\n        test_old = nblock_file.replace(\"-nblocks\", \"-analysisblocks\")\n        if os.path.exists(test_old):\n            return False\n    # Check if any of the local files have changed so we need to refresh\n    for noblock_file in nblock_files:\n        if not utils.file_uptodate(out_file, noblock_file):\n            return True\n    return False", "response": "Check if we need to update BED file of regions supporting back compatibility."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef combine_sample_regions(*samples):\n    samples = utils.unpack_worlds(samples)\n    samples = cwlutils.unpack_tarballs(samples, samples[0])\n    # back compatibility -- global file for entire sample set\n    global_analysis_file = os.path.join(samples[0][\"dirs\"][\"work\"], \"analysis_blocks.bed\")\n    if utils.file_exists(global_analysis_file) and not _needs_region_update(global_analysis_file, samples):\n        global_no_analysis_file = os.path.join(os.path.dirname(global_analysis_file), \"noanalysis_blocks.bed\")\n    else:\n        global_analysis_file = None\n    out = []\n    analysis_files = []\n    batches = []\n    with shared.bedtools_tmpdir(samples[0]):\n        for batch, items in vmulti.group_by_batch(samples, require_bam=False).items():\n            batches.append(items)\n            if global_analysis_file:\n                analysis_file, no_analysis_file = global_analysis_file, global_no_analysis_file\n            else:\n                analysis_file, no_analysis_file = _combine_sample_regions_batch(batch, items)\n            for data in items:\n                vr_file = dd.get_variant_regions(data)\n                if analysis_file:\n                    analysis_files.append(analysis_file)\n                    data[\"config\"][\"algorithm\"][\"callable_regions\"] = analysis_file\n                    data[\"config\"][\"algorithm\"][\"non_callable_regions\"] = no_analysis_file\n                    data[\"config\"][\"algorithm\"][\"callable_count\"] = pybedtools.BedTool(analysis_file).count()\n                elif vr_file:\n                    data[\"config\"][\"algorithm\"][\"callable_count\"] = pybedtools.BedTool(vr_file).count()\n                # attach a representative sample for calculating callable region\n                if not data.get(\"work_bam\"):\n                    for x in items:\n                        if x.get(\"work_bam\"):\n                            data[\"work_bam_callable\"] = x[\"work_bam\"]\n                out.append([data])\n        # Ensure output order matches input order, consistency for CWL-based runs\n        assert len(out) == len(samples)\n        sample_indexes = {dd.get_sample_name(d): i for i, d in enumerate(samples)}\n        def by_input_index(xs):\n            return sample_indexes[dd.get_sample_name(xs[0])]\n        out.sort(key=by_input_index)\n        if len(analysis_files) > 0:\n            final_regions = pybedtools.BedTool(analysis_files[0])\n            _analysis_block_stats(final_regions, batches[0])\n    return out", "response": "Combine all non - callable regions from all samples in a batch and produce a global set of callable regions."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncombine sample regions within a group of batched samples.", "response": "def _combine_sample_regions_batch(batch, items):\n    \"\"\"Combine sample regions within a group of batched samples.\n    \"\"\"\n    config = items[0][\"config\"]\n    work_dir = utils.safe_makedir(os.path.join(items[0][\"dirs\"][\"work\"], \"regions\"))\n    analysis_file = os.path.join(work_dir, \"%s-analysis_blocks.bed\" % batch)\n    no_analysis_file = os.path.join(work_dir, \"%s-noanalysis_blocks.bed\" % batch)\n    if not utils.file_exists(analysis_file) or _needs_region_update(analysis_file, items):\n        # Combine all nblocks into a final set of intersecting regions\n        # without callable bases. HT @brentp for intersection approach\n        # https://groups.google.com/forum/?fromgroups#!topic/bedtools-discuss/qA9wK4zN8do\n        bed_regions = [pybedtools.BedTool(x[\"regions\"][\"nblock\"])\n                       for x in items if \"regions\" in x and x[\"regions\"][\"nblock\"]]\n        if len(bed_regions) == 0:\n            analysis_file, no_analysis_file = None, None\n        else:\n            with file_transaction(items[0], analysis_file, no_analysis_file) as (tx_afile, tx_noafile):\n                def intersect_two(a, b):\n                    return a.intersect(b, nonamecheck=True).saveas()\n                nblock_regions = reduce(intersect_two, bed_regions).saveas(\n                    \"%s-nblock%s\" % utils.splitext_plus(tx_afile))\n                ref_file = tz.get_in([\"reference\", \"fasta\", \"base\"], items[0])\n                ref_regions = get_ref_bedtool(ref_file, config)\n                min_n_size = int(config[\"algorithm\"].get(\"nomap_split_size\", 250))\n                block_filter = NBlockRegionPicker(ref_regions, config, min_n_size)\n                final_nblock_regions = nblock_regions.filter(\n                    block_filter.include_block).saveas().each(block_filter.expand_block).saveas(\n                        \"%s-nblockfinal%s\" % utils.splitext_plus(tx_afile))\n                final_regions = ref_regions.subtract(final_nblock_regions, nonamecheck=True).\\\n                                saveas().merge(d=min_n_size)\n                _write_bed_regions(items[0], final_regions, tx_afile, tx_noafile)\n    if analysis_file and utils.file_exists(analysis_file):\n        return analysis_file, no_analysis_file\n    else:\n        return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving a set of split regions using the input BED for callable regions.", "response": "def get_split_regions(bed_file, data):\n    \"\"\"Retrieve a set of split regions using the input BED for callable regions.\n\n    Provides a less inclusive hook for parallelizing over multiple regions.\n    \"\"\"\n    out_file = \"%s-analysis_blocks.bed\" % utils.splitext_plus(bed_file)[0]\n    with shared.bedtools_tmpdir(data):\n        if not utils.file_uptodate(out_file, bed_file):\n            ref_regions = get_ref_bedtool(dd.get_ref_file(data), data[\"config\"])\n            nblock_regions = ref_regions.subtract(pybedtools.BedTool(bed_file)).saveas()\n            min_n_size = int(tz.get_in([\"config\", \"algorithm\", \"nomap_split_size\"], data, 250))\n            block_filter = NBlockRegionPicker(ref_regions, data[\"config\"], min_n_size)\n            final_nblock_regions = nblock_regions.filter(\n                block_filter.include_block).saveas().each(block_filter.expand_block).saveas()\n            with file_transaction(data, out_file) as tx_out_file:\n                final_regions = ref_regions.subtract(final_nblock_regions, nonamecheck=True).\\\n                                saveas().merge(d=min_n_size).saveas(tx_out_file)\n        chroms = set([])\n        with shared.bedtools_tmpdir(data):\n            for r in pybedtools.BedTool(bed_file):\n                chroms.add(r.chrom)\n        out = []\n        for r in pybedtools.BedTool(out_file):\n            if r.chrom in chroms:\n                out.append((r.chrom, r.start, r.stop))\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if inclusion of a block based on distance from previous.", "response": "def include_block(self, x):\n        \"\"\"Check for inclusion of block based on distance from previous.\n        \"\"\"\n        last_pos = self._chr_last_blocks.get(x.chrom, 0)\n        # Region excludes an entire chromosome, typically decoy/haplotypes\n        if last_pos <= self._end_buffer and x.stop >= self._ref_sizes.get(x.chrom, 0) - self._end_buffer:\n            return True\n        # Do not split on smaller decoy and haplotype chromosomes\n        elif self._ref_sizes.get(x.chrom, 0) <= self._target_size:\n            return False\n        elif (x.start - last_pos) > self._target_size:\n            self._chr_last_blocks[x.chrom] = x.stop\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef expand_block(self, feat):\n        chrom_end = self._ref_sizes.get(feat.chrom)\n        if chrom_end:\n            if feat.start < self._end_buffer:\n                feat.start = 0\n            if feat.stop >= chrom_end - self._end_buffer:\n                feat.stop = chrom_end\n        return feat", "response": "Expand any blocks which are near the start or end of a contig."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _keep_assembled_chrom(bam_file, genome, config):\n    fai = \"%s.fai\" % genome\n    chrom = []\n    with open(fai) as inh:\n        for line in inh:\n            c = line.split(\"\\t\")[0]\n            if c.find(\"_\") < 0:\n                chrom.append(c)\n    chroms = \" \".join(chrom)\n    out_file = utils.append_stem(bam_file, '_chrom')\n    samtools = config_utils.get_program(\"samtools\", config)\n    if not utils.file_exists(out_file):\n        with file_transaction(out_file) as tx_out:\n            cmd = \"{samtools} view -b {bam_file} {chroms} > {tx_out}\"\n            do.run(cmd.format(**locals()), \"Remove contigs from %s\" % bam_file)\n        bam.index(out_file, config)\n    return out_file", "response": "Keep assembled chromosomes from the BAM file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves regions from BAM files", "response": "def _prepare_bam(bam_file, bed_file, config):\n    \"\"\"Remove regions from bed files\"\"\"\n    if not bam_file or not bed_file:\n        return bam_file\n    out_file = utils.append_stem(bam_file, '_filter')\n    bedtools = config_utils.get_program(\"bedtools\", config)\n    if not utils.file_exists(out_file):\n        with file_transaction(out_file) as tx_out:\n            cmd = \"{bedtools} subtract -nonamecheck -A -a {bam_file} -b {bed_file} > {tx_out}\"\n            do.run(cmd.format(**locals()), \"Remove blacklist regions from %s\" % bam_file)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _bam_coverage(name, bam_input, data):\n    cmd = (\"{bam_coverage} -b {bam_input} -o {bw_output} \"\n          \"--binSize 20 --effectiveGenomeSize {size} \"\n          \"--smoothLength 60 --extendReads 150 --centerReads -p {cores}\")\n    size = bam.fasta.total_sequence_length(dd.get_ref_file(data))\n    cores = dd.get_num_cores(data)\n    try:\n        bam_coverage = config_utils.get_program(\"bamCoverage\", data)\n    except config_utils.CmdNotFound:\n        logger.info(\"No bamCoverage found, skipping bamCoverage.\")\n        return None\n    resources = config_utils.get_resources(\"bamCoverage\", data[\"config\"])\n    if resources:\n        options = resources.get(\"options\")\n        if options:\n            cmd += \" %s\" % \" \".join([str(x) for x in options])\n    bw_output = os.path.join(os.path.dirname(bam_input), \"%s.bw\" % name)\n    if utils.file_exists(bw_output):\n        return bw_output\n    with file_transaction(bw_output) as out_tx:\n        do.run(cmd.format(**locals()), \"Run bamCoverage in %s\" % name)\n    return bw_output", "response": "Run bamCoverage from deeptools"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(items):\n    paired = vcfutils.get_paired(items)\n    data = paired.tumor_data if paired else items[0]\n    work_dir = _sv_workdir(data)\n    variant_file = _get_out_file(work_dir, paired)\n    if not utils.file_exists(variant_file):\n        with file_transaction(data, work_dir) as tx_work_dir:\n            utils.safe_makedir(tx_work_dir)\n            tx_workflow_file = _prep_config(items, paired, tx_work_dir)\n            _run_workflow(items, paired, tx_workflow_file, tx_work_dir)\n    assert utils.file_exists(variant_file), \"Manta finished without output file %s\" % variant_file\n    variant_file = shared.annotate_with_depth(variant_file, items)\n    out = []\n    upload_counts = collections.defaultdict(int)\n    for data in items:\n        if \"break-point-inspector\" in dd.get_tools_on(data):\n            if paired and paired.normal_bam and paired.tumor_name == dd.get_sample_name(data):\n                variant_file = _run_break_point_inspector(data, variant_file, paired, work_dir)\n        if \"sv\" not in data:\n            data[\"sv\"] = []\n        final_vcf = shared.finalize_sv(variant_file, data, items)\n        vc = {\"variantcaller\": \"manta\",\n              \"do_upload\": upload_counts[final_vcf] == 0,  # only upload a single file per batch\n              \"vrn_file\": final_vcf}\n        evidence_bam = _get_evidence_bam(work_dir, data)\n        if evidence_bam:\n            vc[\"read_evidence\"] = evidence_bam\n        data[\"sv\"].append(vc)\n        upload_counts[final_vcf] += 1\n        out.append(data)\n    return out", "response": "Perform structural variations with Manta."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve manta output variant file depending on analysis.", "response": "def _get_out_file(work_dir, paired):\n    \"\"\"Retrieve manta output variant file, depending on analysis.\n    \"\"\"\n    if paired:\n        if paired.normal_bam:\n            base_file = \"somaticSV.vcf.gz\"\n        else:\n            base_file = \"tumorSV.vcf.gz\"\n    else:\n        base_file = \"diploidSV.vcf.gz\"\n    return os.path.join(work_dir, \"results\", \"variants\", base_file)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves the evidence BAM for the sample if it exists", "response": "def _get_evidence_bam(work_dir, data):\n    \"\"\"Retrieve evidence BAM for the sample if it exists\n    \"\"\"\n    evidence_bam = glob.glob(os.path.join(work_dir, \"results\", \"evidence\",\n                                            \"evidence_*.%s*.bam\" % (dd.get_sample_name(data))))\n    if evidence_bam:\n        return evidence_bam[0]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning manta analysis inside prepared workflow directory.", "response": "def _run_workflow(items, paired, workflow_file, work_dir):\n    \"\"\"Run manta analysis inside prepared workflow directory.\n    \"\"\"\n    utils.remove_safe(os.path.join(work_dir, \"workspace\"))\n    data = paired.tumor_data if paired else items[0]\n    cmd = [utils.get_program_python(\"configManta.py\"), workflow_file, \"-m\", \"local\", \"-j\", dd.get_num_cores(data)]\n    do.run(cmd, \"Run manta SV analysis\")\n    utils.remove_safe(os.path.join(work_dir, \"workspace\"))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _prep_config(items, paired, work_dir):\n    assert utils.which(\"configManta.py\"), \"Could not find installed configManta.py\"\n    out_file = os.path.join(work_dir, \"runWorkflow.py\")\n    if not utils.file_exists(out_file) or _out_of_date(out_file):\n        config_script = os.path.realpath(utils.which(\"configManta.py\"))\n        cmd = [utils.get_program_python(\"configManta.py\"), config_script]\n        if paired:\n            if paired.normal_bam:\n                cmd += [\"--normalBam=%s\" % paired.normal_bam, \"--tumorBam=%s\" % paired.tumor_bam]\n            else:\n                cmd += [\"--tumorBam=%s\" % paired.tumor_bam]\n        else:\n            cmd += [\"--bam=%s\" % dd.get_align_bam(data) for data in items]\n        data = paired.tumor_data if paired else items[0]\n        cmd += [\"--referenceFasta=%s\" % dd.get_ref_file(data), \"--runDir=%s\" % work_dir]\n        if dd.get_coverage_interval(data) not in [\"genome\"]:\n            cmd += [\"--exome\"]\n        for region in _maybe_limit_chromosomes(data):\n            cmd += [\"--region\", region]\n        resources = config_utils.get_resources(\"manta\", data[\"config\"])\n        if resources.get(\"options\"):\n            cmd += [str(x) for x in resources[\"options\"]]\n        # If we are removing polyX, avoid calling on small indels which require\n        # excessively long runtimes on noisy WGS runs\n        if \"polyx\" in dd.get_exclude_regions(data):\n            cmd += [\"--config\", _prep_streamlined_config(config_script, work_dir)]\n        do.run(cmd, \"Configure manta SV analysis\")\n    return out_file", "response": "Run initial configuration for Manta."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates manta INI file with steps that potentially increase runtimes.", "response": "def _prep_streamlined_config(config_script, work_dir):\n    \"\"\"Create manta INI file without steps that potentially increase runtimes.\n\n    This removes calling of small indels.\n    \"\"\"\n    new_min_size = 100\n    in_file = config_script + \".ini\"\n    out_file = os.path.join(work_dir, os.path.basename(in_file))\n    with open(in_file) as in_handle:\n        with open(out_file, \"w\") as out_handle:\n            for line in in_handle:\n                if line.startswith(\"minCandidateVariantSize\"):\n                    out_handle.write(\"minCandidateVariantSize = %s\\n\" % new_min_size)\n                else:\n                    out_handle.write(line)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlimiting chromosomes to avoid problematically named HLA contigs.", "response": "def _maybe_limit_chromosomes(data):\n    \"\"\"Potentially limit chromosomes to avoid problematically named HLA contigs.\n\n    HLAs have ':' characters in them which confuse downstream processing. If\n    we have no problematic chromosomes we don't limit anything.\n    \"\"\"\n    std_chroms = []\n    prob_chroms = []\n    noalt_calling = \"noalt_calling\" in dd.get_tools_on(data) or \"altcontigs\" in dd.get_exclude_regions(data)\n    for contig in ref.file_contigs(dd.get_ref_file(data)):\n        if contig.name.find(\":\") > 0 or (noalt_calling and not chromhacks.is_nonalt(contig.name)):\n            prob_chroms.append(contig.name)\n        else:\n            std_chroms.append(contig.name)\n    if len(prob_chroms) > 0:\n        return std_chroms\n    else:\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _out_of_date(rw_file):\n    with open(rw_file) as in_handle:\n        for line in in_handle:\n            if line.startswith(\"sys.path.append\"):\n                file_version = line.split(\"/lib/python\")[0].split(\"Cellar/manta/\")[-1]\n                if file_version != programs.get_version_manifest(\"manta\"):\n                    return True\n    return False", "response": "Check if a run workflow file points to an older version of manta and needs a refresh."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _freebayes_options_from_config(items, config, out_file, region=None):\n    opts = [\"--genotype-qualities\", \"--strict-vcf\"]\n    cur_ploidy = ploidy.get_ploidy(items, region)\n    base_ploidy = ploidy.get_ploidy(items)\n    opts += [\"--ploidy\", str(cur_ploidy)]\n    # Adjust min fraction when trying to call more sensitively in certain\n    # regions. This is primarily meant for pooled mitochondrial calling.\n    if (isinstance(region, (list, tuple)) and chromhacks.is_mitochondrial(region[0])\n          and cur_ploidy >= base_ploidy and \"--min-alternate-fraction\" not in opts and \"-F\" not in opts):\n        opts += [\"--min-alternate-fraction\", \"0.01\"]\n    variant_regions = bedutils.population_variant_regions(items, merged=True)\n    # Produce gVCF output\n    if any(\"gvcf\" in dd.get_tools_on(d) for d in items):\n        opts += [\"--gvcf\", \"--gvcf-chunk\", \"50000\"]\n    no_target_regions = False\n    target = shared.subset_variant_regions(variant_regions, region, out_file, items)\n    if target:\n        if isinstance(target, six.string_types) and os.path.isfile(target):\n            if os.path.getsize(target) == 0:\n                no_target_regions = True\n            else:\n                opts += [\"--targets\", target]\n        else:\n            opts += [\"--region\", region_to_freebayes(target)]\n    resources = config_utils.get_resources(\"freebayes\", config)\n    if resources.get(\"options\"):\n        opts += resources[\"options\"]\n    return opts, no_target_regions", "response": "Prepare standard options for FreeBayes calling from configuration."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _add_somatic_opts(opts, paired):\n    if \"--min-alternate-fraction\" not in opts and \"-F\" not in opts:\n        # add minimum reportable allele frequency\n        # FreeBayes defaults to 20%, but use 10% by default for the\n        # tumor case\n        min_af = float(utils.get_in(paired.tumor_config, (\"algorithm\",\n                                                          \"min_allele_fraction\"), 10)) / 100.0\n        opts += \" --min-alternate-fraction %s\" % min_af\n    # Recommended settings for cancer calling\n    opts += (\" --pooled-discrete --pooled-continuous \"\n             \"--report-genotype-likelihood-max --allele-balance-priors-off\")\n    return opts", "response": "Add somatic options to current set."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run_freebayes(align_bams, items, ref_file, assoc_files, region=None,\n                  out_file=None):\n    \"\"\"Run FreeBayes variant calling, either paired tumor/normal or germline calling.\n    \"\"\"\n    items = shared.add_highdepth_genome_exclusion(items)\n    if is_paired_analysis(align_bams, items):\n        paired = get_paired_bams(align_bams, items)\n        if not paired.normal_bam:\n            call_file = _run_freebayes_caller(align_bams, items, ref_file,\n                                              assoc_files, region, out_file, somatic=paired)\n        else:\n            call_file = _run_freebayes_paired([paired.tumor_bam, paired.normal_bam],\n                                              [paired.tumor_data, paired.normal_data],\n                                              ref_file, assoc_files, region, out_file)\n    else:\n        vcfutils.check_paired_problems(items)\n        call_file = _run_freebayes_caller(align_bams, items, ref_file,\n                                          assoc_files, region, out_file)\n\n    return call_file", "response": "Run FreeBayes variant calling."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetect SNPs and indels with FreeBayes. Performs post-filtering to remove very low quality variants which can cause issues feeding into GATK. Breaks variants into individual allelic primitives for analysis and evaluation.", "response": "def _run_freebayes_caller(align_bams, items, ref_file, assoc_files,\n                          region=None, out_file=None, somatic=None):\n    \"\"\"Detect SNPs and indels with FreeBayes.\n\n    Performs post-filtering to remove very low quality variants which\n    can cause issues feeding into GATK. Breaks variants into individual\n    allelic primitives for analysis and evaluation.\n    \"\"\"\n    config = items[0][\"config\"]\n    if out_file is None:\n        out_file = \"%s-variants.vcf.gz\" % os.path.splitext(align_bams[0])[0]\n    if not utils.file_exists(out_file):\n        if not utils.file_exists(out_file):\n            with file_transaction(items[0], out_file) as tx_out_file:\n                freebayes = config_utils.get_program(\"freebayes\", config)\n                input_bams = \" \".join(\"-b %s\" % x for x in align_bams)\n                opts, no_target_regions = _freebayes_options_from_config(items, config, out_file, region)\n                if no_target_regions:\n                    vcfutils.write_empty_vcf(tx_out_file, config, samples=[dd.get_sample_name(d) for d in items])\n                else:\n                    opts = \" \".join(opts)\n                    # Recommended options from 1000 genomes low-complexity evaluation\n                    # https://groups.google.com/d/msg/freebayes/GvxIzjcpbas/1G6e3ArxQ4cJ\n                    opts += \" --min-repeat-entropy 1\"\n                    # Remove partial observations, which cause a preference for heterozygote calls\n                    # https://github.com/ekg/freebayes/issues/234#issuecomment-205331765\n                    opts += \" --no-partial-observations\"\n                    if somatic:\n                        opts = _add_somatic_opts(opts, somatic)\n                    compress_cmd = \"| bgzip -c\" if out_file.endswith(\"gz\") else \"\"\n                    # For multi-sample outputs, ensure consistent order\n                    samples = (\"-s\" + \",\".join([dd.get_sample_name(d) for d in items])) if len(items) > 1 else \"\"\n                    fix_ambig = vcfutils.fix_ambiguous_cl()\n                    py_cl = config_utils.get_program(\"py\", config)\n                    cmd = (\"{freebayes} -f {ref_file} {opts} {input_bams} \"\n                           \"\"\"| bcftools filter -i 'ALT=\"<*>\" || QUAL > 5' \"\"\"\n                           \"| {fix_ambig} | bcftools view {samples} -a - | \"\n                           \"{py_cl} -x 'bcbio.variation.freebayes.remove_missingalt(x)' | \"\n                           \"vcfallelicprimitives -t DECOMPOSED --keep-geno | vcffixup - | vcfstreamsort | \"\n                           \"vt normalize -n -r {ref_file} -q - | vcfuniqalleles | vt uniq - 2> /dev/null \"\n                           \"{compress_cmd} > {tx_out_file}\")\n                    do.run(cmd.format(**locals()), \"Genotyping with FreeBayes\", {})\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetects SNPs and indels with FreeBayes for paired calling.", "response": "def _run_freebayes_paired(align_bams, items, ref_file, assoc_files,\n                          region=None, out_file=None):\n    \"\"\"Detect SNPs and indels with FreeBayes for paired tumor/normal samples.\n\n    Sources of options for FreeBayes:\n    mailing list: https://groups.google.com/d/msg/freebayes/dTWBtLyM4Vs/HAK_ZhJHguMJ\n    mailing list: https://groups.google.com/forum/#!msg/freebayes/LLH7ZfZlVNs/63FdD31rrfEJ\n    speedseq: https://github.com/cc2qe/speedseq/blob/e6729aa2589eca4e3a946f398c1a2bdc15a7300d/bin/speedseq#L916\n    sga/freebayes: https://github.com/jts/sga-extra/blob/7e28caf71e8107b697f9be7162050e4fa259694b/\n                   sga_generate_varcall_makefile.pl#L299\n    \"\"\"\n    config = items[0][\"config\"]\n    if out_file is None:\n        out_file = \"%s-paired-variants.vcf.gz\" % os.path.splitext(align_bams[0])[0]\n    if not utils.file_exists(out_file):\n        if not utils.file_exists(out_file):\n            with file_transaction(items[0], out_file) as tx_out_file:\n                paired = get_paired_bams(align_bams, items)\n                assert paired.normal_bam, \"Require normal BAM for FreeBayes paired calling and filtering\"\n\n                freebayes = config_utils.get_program(\"freebayes\", config)\n                opts, no_target_regions = _freebayes_options_from_config(items, config, out_file, region)\n                if no_target_regions:\n                    vcfutils.write_empty_vcf(tx_out_file, config,\n                                            samples=[x for x in [paired.tumor_name, paired.normal_name] if x])\n                else:\n                    opts = \" \".join(opts)\n                    opts += \" --min-repeat-entropy 1\"\n                    opts += \" --no-partial-observations\"\n                    opts = _add_somatic_opts(opts, paired)\n                    compress_cmd = \"| bgzip -c\" if out_file.endswith(\"gz\") else \"\"\n                    # For multi-sample outputs, ensure consistent order\n                    samples = (\"-s \" + \",\".join([dd.get_sample_name(d) for d in items])) if len(items) > 1 else \"\"\n                    fix_ambig = vcfutils.fix_ambiguous_cl()\n                    bcbio_py = sys.executable\n                    py_cl = os.path.join(os.path.dirname(sys.executable), \"py\")\n                    cl = (\"{freebayes} -f {ref_file} {opts} \"\n                          \"{paired.tumor_bam} {paired.normal_bam} \"\n                          \"\"\"| bcftools filter -i 'ALT=\"<*>\" || QUAL > 5' \"\"\"\n                          \"\"\"| {bcbio_py} -c 'from bcbio.variation import freebayes; \"\"\"\n                          \"\"\"freebayes.call_somatic(\"{paired.tumor_name}\", \"{paired.normal_name}\")' \"\"\"\n                          \"| {fix_ambig} | bcftools view {samples} -a - | \"\n                          \"{py_cl} -x 'bcbio.variation.freebayes.remove_missingalt(x)' | \"\n                          \"vcfallelicprimitives -t DECOMPOSED --keep-geno | vcffixup - | vcfstreamsort | \"\n                          \"vt normalize -n -r {ref_file} -q - | vcfuniqalleles | vt uniq - 2> /dev/null \"\n                          \"{compress_cmd} > {tx_out_file}\")\n                    do.run(cl.format(**locals()), \"Genotyping paired variants with FreeBayes\", {})\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nensure likelihoods for tumor and normal pass thresholds.", "response": "def _check_lods(parts, tumor_thresh, normal_thresh, indexes):\n    \"\"\"Ensure likelihoods for tumor and normal pass thresholds.\n\n    Skipped if no FreeBayes GL annotations available.\n    \"\"\"\n    try:\n        gl_index = parts[8].split(\":\").index(\"GL\")\n    except ValueError:\n        return True\n    try:\n        tumor_gls = [float(x) for x in parts[indexes[\"tumor\"]].strip().split(\":\")[gl_index].split(\",\") if x != \".\"]\n        if tumor_gls:\n            tumor_lod = max(tumor_gls[i] - tumor_gls[0] for i in range(1, len(tumor_gls)))\n        else:\n            tumor_lod = -1.0\n    # No GL information, no tumor call (so fail it)\n    except IndexError:\n        tumor_lod = -1.0\n    try:\n        normal_gls = [float(x) for x in parts[indexes[\"normal\"]].strip().split(\":\")[gl_index].split(\",\") if x != \".\"]\n        if normal_gls:\n            normal_lod = min(normal_gls[0] - normal_gls[i] for i in range(1, len(normal_gls)))\n        else:\n            normal_lod = normal_thresh\n    # No GL inofmration, no normal call (so pass it)\n    except IndexError:\n        normal_lod = normal_thresh\n    return normal_lod >= normal_thresh and tumor_lod >= tumor_thresh"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the frequency of tumor to normal passes a reasonable threshold.", "response": "def _check_freqs(parts, indexes):\n    \"\"\"Ensure frequency of tumor to normal passes a reasonable threshold.\n\n    Avoids calling low frequency tumors also present at low frequency in normals,\n    which indicates a contamination or persistent error.\n    \"\"\"\n    thresh_ratio = 2.7\n    try:  # FreeBayes\n        ao_index = parts[8].split(\":\").index(\"AO\")\n        ro_index = parts[8].split(\":\").index(\"RO\")\n    except ValueError:\n        ao_index, ro_index = None, None\n    try:  # VarDict\n        af_index = parts[8].split(\":\").index(\"AF\")\n    except ValueError:\n        af_index = None\n    if af_index is None and ao_index is None:\n        # okay to skip if a gVCF record\n        if parts[4].find(\"<*>\") == -1:\n            raise NotImplementedError(\"Unexpected format annotations: %s\" % parts[8])\n    def _calc_freq(item):\n        try:\n            if ao_index is not None and ro_index is not None:\n                ao = sum([int(x) for x in item.split(\":\")[ao_index].split(\",\")])\n                ro = int(item.split(\":\")[ro_index])\n                freq = ao / float(ao + ro)\n            elif af_index is not None:\n                freq = float(item.split(\":\")[af_index])\n            else:\n                freq = 0.0\n        except (IndexError, ValueError, ZeroDivisionError):\n            freq = 0.0\n        return freq\n    tumor_freq, normal_freq = _calc_freq(parts[indexes[\"tumor\"]]), _calc_freq(parts[indexes[\"normal\"]])\n    return normal_freq <= 0.001 or normal_freq <= tumor_freq / thresh_ratio"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef call_somatic(tumor_name, normal_name):\n    # Thresholds are like phred scores, so 3.5 = phred35\n    tumor_thresh, normal_thresh = 3.5, 3.5\n    new_headers = ['##INFO=<ID=SOMATIC,Number=0,Type=Flag,Description=\"Somatic event\">\\n',\n                   ('##FILTER=<ID=REJECT,Description=\"Not somatic due to normal call frequency '\n                    'or phred likelihoods: tumor: %s, normal %s.\">\\n')\n                   % (int(tumor_thresh * 10), int(normal_thresh * 10))]\n    def _output_filter_line(line, indexes):\n        parts = line.split(\"\\t\")\n        if _check_lods(parts, tumor_thresh, normal_thresh, indexes) and _check_freqs(parts, indexes):\n            parts[7] = parts[7] + \";SOMATIC\"\n        else:\n            if parts[6] in set([\".\", \"PASS\"]):\n                parts[6] = \"REJECT\"\n            else:\n                parts[6] += \";REJECT\"\n        line = \"\\t\".join(parts)\n        sys.stdout.write(line)\n    def _write_header(header):\n        for hline in header[:-1] + new_headers + [header[-1]]:\n            sys.stdout.write(hline)\n    header = []\n    indexes = None\n    for line in sys.stdin:\n        if not indexes:\n            if line.startswith(\"#\"):\n                header.append(line)\n            else:\n                parts = header[-1].rstrip().split(\"\\t\")\n                indexes = {\"tumor\": parts.index(tumor_name), \"normal\": parts.index(normal_name)}\n                _write_header(header)\n                _output_filter_line(line, indexes)\n        else:\n            _output_filter_line(line, indexes)\n    # no calls, only output the header\n    if not indexes:\n        _write_header(header)", "response": "This function will call SOMATIC variants from the input file and writes to stdout."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclean FreeBayes output to make post - processing with GATK happy.", "response": "def _clean_freebayes_output(line):\n    \"\"\"Clean FreeBayes output to make post-processing with GATK happy.\n\n    XXX Not applied on recent versions which fix issues to be more compatible\n    with bgzip output, but retained in case of need.\n\n    - Remove lines from FreeBayes outputs where REF/ALT are identical:\n      2       22816178        .       G       G       0.0339196\n      or there are multiple duplicate alleles:\n      4       60594753        .       TGAAA   T,T\n    - Remove Type=Int specifications which are not valid VCF and GATK chokes\n      on.\n    \"\"\"\n    if line.startswith(\"#\"):\n        line = line.replace(\"Type=Int,D\", \"Type=Integer,D\")\n        return line\n    else:\n        parts = line.split(\"\\t\")\n        alleles = [x.strip() for x in parts[4].split(\",\")] + [parts[3].strip()]\n        if len(alleles) == len(set(alleles)):\n            return line\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprovide framework to clean a file in - place with the specified clean function.", "response": "def clean_vcf_output(orig_file, clean_fn, config, name=\"clean\"):\n    \"\"\"Provide framework to clean a file in-place, with the specified clean\n    function.\n    \"\"\"\n    base, ext = utils.splitext_plus(orig_file)\n    out_file = \"{0}-{1}{2}\".format(base, name, ext)\n    if not utils.file_exists(out_file):\n        with open(orig_file) as in_handle:\n            with file_transaction(config, out_file) as tx_out_file:\n                with open(tx_out_file, \"w\") as out_handle:\n                    for line in in_handle:\n                        update_line = clean_fn(line)\n                        if update_line:\n                            out_handle.write(update_line)\n        move_vcf(orig_file, \"{0}.orig\".format(orig_file))\n        move_vcf(out_file, orig_file)\n        with open(out_file, \"w\") as out_handle:\n            out_handle.write(\"Moved to {0}\".format(orig_file))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_type(data):\n    if data[\"analysis\"].lower().startswith(\"var\") or dd.get_variantcaller(data):\n        return tz.get_in((\"config\", \"algorithm\", \"effects\"), data, \"snpeff\")", "response": "Retrieve the type of effects calculation to do."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _special_dbkey_maps(dbkey, ref_file):\n    remaps = {\"hg19\": \"GRCh37\",\n              \"hg38-noalt\": \"hg38\"}\n    if dbkey in remaps:\n        base_dir = os.path.normpath(os.path.join(os.path.dirname(ref_file), os.pardir))\n        vep_dir = os.path.normpath(os.path.join(base_dir, \"vep\"))\n        other_dir = os.path.relpath(os.path.normpath(os.path.join(base_dir, os.pardir, remaps[dbkey], \"vep\")),\n                                    base_dir)\n        if os.path.exists(os.path.join(base_dir, other_dir)):\n            if not os.path.lexists(vep_dir):\n                os.symlink(other_dir, vep_dir)\n            return vep_dir\n        else:\n            return None\n    else:\n        return None", "response": "Avoid duplicate VEP information for databases with chromosome differences like hg19 or hg38."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prep_vep_cache(dbkey, ref_file, tooldir=None, config=None):\n    if config is None: config = {}\n    resource_file = os.path.join(os.path.dirname(ref_file), \"%s-resources.yaml\" % dbkey)\n    if os.path.exists(resource_file):\n        with open(resource_file) as in_handle:\n            resources = yaml.safe_load(in_handle)\n        ensembl_name = tz.get_in([\"aliases\", \"ensembl\"], resources)\n        symlink_dir = _special_dbkey_maps(dbkey, ref_file)\n        if ensembl_name and ensembl_name.find(\"_vep_\") == -1:\n            raise ValueError(\"%s has ensembl an incorrect value.\"\n                             \"It should have _vep_ in the name.\"\n                             \"Remove line or fix the name to avoid error.\")\n        if symlink_dir and ensembl_name:\n            species, vepv = ensembl_name.split(\"_vep_\")\n            return symlink_dir, species\n        elif ensembl_name:\n            species, vepv = ensembl_name.split(\"_vep_\")\n            vep_dir = utils.safe_makedir(os.path.normpath(os.path.join(\n                os.path.dirname(os.path.dirname(ref_file)), \"vep\")))\n            out_dir = os.path.join(vep_dir, species, vepv)\n            if not os.path.exists(out_dir):\n                tmp_dir = utils.safe_makedir(os.path.join(vep_dir, species, \"txtmp\"))\n                eversion = vepv.split(\"_\")[0]\n                url = \"http://ftp.ensembl.org/pub/release-%s/variation/VEP/%s.tar.gz\" % (eversion, ensembl_name)\n                with utils.chdir(tmp_dir):\n                    subprocess.check_call([\"wget\", \"--no-check-certificate\", \"-c\", url])\n                vep_path = \"%s/bin/\" % tooldir if tooldir else \"\"\n                perl_exports = utils.get_perl_exports()\n                cmd = [\"%svep_install\" % vep_path, \"-a\", \"c\", \"-s\", ensembl_name,\n                       \"-c\", vep_dir, \"-u\", tmp_dir, \"--NO_UPDATE\", \"--VERSION\", eversion]\n                do.run(\"%s && %s\" % (perl_exports, \" \".join(cmd)), \"Prepare VEP directory for %s\" % ensembl_name)\n                cmd = [\"%svep_convert_cache\" % vep_path, \"--species\", species, \"--version\", vepv,\n                       \"--dir\", vep_dir, \"--force_overwrite\", \"--remove\"]\n                do.run(\"%s && %s\" % (perl_exports, \" \".join(cmd)), \"Convert VEP cache to tabix %s\" % ensembl_name)\n                for tmp_fname in os.listdir(tmp_dir):\n                    os.remove(os.path.join(tmp_dir, tmp_fname))\n                os.rmdir(tmp_dir)\n            tmp_dir = os.path.join(vep_dir, \"tmp\")\n            if os.path.exists(tmp_dir):\n                shutil.rmtree(tmp_dir)\n            return vep_dir, species\n    return None, None", "response": "Prepare VEP cache file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_vep(in_file, data):\n    if not vcfutils.vcf_has_variants(in_file):\n        return None\n    out_file = utils.append_stem(in_file, \"-vepeffects\")\n    assert in_file.endswith(\".gz\") and out_file.endswith(\".gz\")\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            vep_dir, ensembl_name = prep_vep_cache(data[\"genome_build\"],\n                                                   tz.get_in([\"reference\", \"fasta\", \"base\"], data))\n            if vep_dir:\n                cores = tz.get_in((\"config\", \"algorithm\", \"num_cores\"), data, 1)\n                fork_args = [\"--fork\", str(cores)] if cores > 1 else []\n                vep = config_utils.get_program(\"vep\", data[\"config\"])\n                # HGVS requires a bgzip compressed, faidx indexed input file or is unusable slow\n                if dd.get_ref_file_compressed(data):\n                    hgvs_compatible = True\n                    config_args = [\"--fasta\", dd.get_ref_file_compressed(data)]\n                else:\n                    hgvs_compatible = False\n                    config_args = [\"--fasta\", dd.get_ref_file(data)]\n                if vcfanno.is_human(data):\n                    plugin_fns = {\"loftee\": _get_loftee, \"maxentscan\": _get_maxentscan,\"genesplicer\": _get_genesplicer,\n                                  \"spliceregion\": _get_spliceregion, \"G2P\": _get_G2P}\n                    plugins = [\"loftee\", \"G2P\"]\n                    if \"vep_splicesite_annotations\" in dd.get_tools_on(data):\n                        # \"genesplicer\" too unstable so currently removed\n                        plugins += [\"maxentscan\", \"spliceregion\"]\n                    for plugin in plugins:\n                        plugin_args = plugin_fns[plugin](data)\n                        config_args += plugin_args\n                    config_args += [\"--sift\", \"b\", \"--polyphen\", \"b\"]\n                    if hgvs_compatible:\n                        config_args += [\"--hgvsg\",\"--hgvs\", \"--shift_hgvs\", \"1\"]\n                if (dd.get_effects_transcripts(data).startswith(\"canonical\")\n                      or tz.get_in((\"config\", \"algorithm\", \"clinical_reporting\"), data)):\n                    config_args += [\"--most_severe\"]\n                else:\n                    config_args += [\"--flag_pick_allele_gene\"]\n                if ensembl_name.endswith(\"_merged\"):\n                    config_args += [\"--merged\"]\n                    ensembl_name = ensembl_name.replace(\"_merged\", \"\")\n                resources = config_utils.get_resources(\"vep\", data[\"config\"])\n                extra_args = [str(x) for x in resources.get(\"options\", [])]\n                cmd = [vep, \"--vcf\", \"-o\", \"stdout\", \"-i\", in_file] + fork_args + extra_args + \\\n                      [\"--species\", ensembl_name,\n                       \"--no_stats\", \"--cache\",\n                        \"--offline\", \"--dir\", vep_dir,\n                       \"--symbol\", \"--numbers\", \"--biotype\", \"--total_length\", \"--canonical\",\n                       \"--gene_phenotype\", \"--ccds\", \"--uniprot\", \"--domains\", \"--regulatory\",\n                       \"--protein\", \"--tsl\", \"--appris\", \"--af\", \"--max_af\", \"--af_1kg\", \"--af_esp\", \"--af_gnomad\",\n                       \"--pubmed\", \"--variant_class\", \"--allele_number\"] + config_args\n                perl_exports = utils.get_perl_exports()\n                # Remove empty fields (';;') which can cause parsing errors downstream\n                cmd = \"%s && %s | sed '/^#/! s/;;/;/g' | bgzip -c > %s\" % (perl_exports, \" \".join(cmd), tx_out_file)\n                do.run(cmd, \"Ensembl variant effect predictor\", data)\n    if utils.file_exists(out_file):\n        return vcfutils.bgzip_and_index(out_file, data[\"config\"])", "response": "Annotate input VCF file with Ensembl variant effect predictor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_loftee(data):\n    ancestral_file = tz.get_in((\"genome_resources\", \"variation\", \"ancestral\"), data)\n    if not ancestral_file or not os.path.exists(ancestral_file):\n        ancestral_file = \"false\"\n    vep = config_utils.get_program(\"vep\", data[\"config\"])\n    args = [\"--plugin\", \"LoF,human_ancestor_fa:%s,loftee_path:%s\" %\n            (ancestral_file, os.path.dirname(os.path.realpath(vep)))]\n    return args", "response": "Retrieve LOFTEE.\n    parameters for LOFTEE."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the maxentscan command line for this variant.", "response": "def _get_maxentscan(data):\n    \"\"\"\n    The plugin executes the logic from one of the scripts depending on which\n    splice region the variant overlaps:\n        score5.pl : last 3 bases of exon    --> first 6 bases of intron\n        score3.pl : last 20 bases of intron --> first 3 bases of exon\n    The plugin reports the reference, alternate and difference (REF - ALT) maximumentropy scores.\n    https://github.com/Ensembl/VEP_plugins/blob/master/MaxEntScan.pm\n    \"\"\"\n\n    maxentscan_dir = os.path.dirname(os.path.realpath(config_utils.get_program(\"maxentscan_score3.pl\", data[\"config\"])))\n    if maxentscan_dir and os.path.exists(maxentscan_dir):\n        return [\"--plugin\", \"MaxEntScan,%s\" % (maxentscan_dir)]\n    else:\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_genesplicer(data):\n\n    genesplicer_exec = os.path.realpath(config_utils.get_program(\"genesplicer\", data[\"config\"]))\n    genesplicer_training = tz.get_in((\"genome_resources\", \"variation\", \"genesplicer\"), data)\n    if genesplicer_exec and os.path.exists(genesplicer_exec) and genesplicer_training and os.path.exists(genesplicer_training) :\n        return [\"--plugin\", \"GeneSplicer,%s,%s\" % (genesplicer_exec,genesplicer_training)]\n    else:\n        return []", "response": "Retrieve the genesplicer executable and training paths."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of args to pass to G2P", "response": "def _get_G2P(data):\n    \"\"\"\n    A VEP plugin that uses G2P allelic requirements to assess variants in genes\n    for potential phenotype involvement.\n    \"\"\"\n    G2P_file = os.path.realpath(tz.get_in((\"genome_resources\", \"variation\", \"genotype2phenotype\"), data))\n    args = [\"--plugin\", \"G2P,file:%s\" % (G2P_file)]\n    if G2P_file:\n        return args\n    else:\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve snpEff arguments supplied through input configuration.", "response": "def _snpeff_args_from_config(data):\n    \"\"\"Retrieve snpEff arguments supplied through input configuration.\n    \"\"\"\n    config = data[\"config\"]\n    args = [\"-hgvs\"]\n    # General supplied arguments\n    resources = config_utils.get_resources(\"snpeff\", config)\n    if resources.get(\"options\"):\n        args += [str(x) for x in resources.get(\"options\", [])]\n    # cancer specific calling arguments\n    if vcfutils.get_paired_phenotype(data):\n        args += [\"-cancer\"]\n\n    effects_transcripts = dd.get_effects_transcripts(data)\n    if effects_transcripts in set([\"canonical_cancer\"]):\n        _, snpeff_base_dir = get_db(data)\n        canon_list_file = os.path.join(snpeff_base_dir, \"transcripts\", \"%s.txt\" % effects_transcripts)\n        if not utils.file_exists(canon_list_file):\n            raise ValueError(\"Cannot find expected file for effects_transcripts: %s\" % canon_list_file)\n        args += [\"-canonList\", canon_list_file]\n    elif effects_transcripts == \"canonical\" or tz.get_in((\"config\", \"algorithm\", \"clinical_reporting\"), data):\n        args += [\"-canon\"]\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_db(data):\n    snpeff_db = utils.get_in(data, (\"genome_resources\", \"aliases\", \"snpeff\"))\n    snpeff_base_dir = None\n    if snpeff_db:\n        snpeff_base_dir = utils.get_in(data, (\"reference\", \"snpeff\"))\n        if not (isinstance(snpeff_base_dir, six.string_types) and os.path.isdir(snpeff_base_dir)):\n            snpeff_base_dir = utils.get_in(data, (\"reference\", \"snpeff\", snpeff_db))\n        if not snpeff_base_dir:\n            # We need to mask '.' characters for CWL/WDL processing, check for them here\n            snpeff_base_dir = utils.get_in(data, (\"reference\", \"snpeff\", snpeff_db.replace(\".\", \"_\")))\n            if snpeff_base_dir:\n                snpeff_db = snpeff_db.replace(\"_\", \".\")\n        if isinstance(snpeff_base_dir, dict) and snpeff_base_dir.get(\"base\"):\n            snpeff_base_dir = snpeff_base_dir[\"base\"]\n        if (snpeff_base_dir and isinstance(snpeff_base_dir, six.string_types) and os.path.isfile(snpeff_base_dir)):\n            snpeff_base_dir = os.path.dirname(snpeff_base_dir)\n        if (snpeff_base_dir and isinstance(snpeff_base_dir, six.string_types)\n              and snpeff_base_dir.endswith(\"%s%s\" % (os.path.sep, snpeff_db))):\n            snpeff_base_dir = os.path.dirname(snpeff_base_dir)\n        if not snpeff_base_dir:\n            ref_file = utils.get_in(data, (\"reference\", \"fasta\", \"base\"))\n            snpeff_base_dir = utils.safe_makedir(os.path.normpath(os.path.join(\n                os.path.dirname(os.path.dirname(ref_file)), \"snpeff\")))\n            # back compatible retrieval of genome from installation directory\n            if \"config\" in data and not os.path.exists(os.path.join(snpeff_base_dir, snpeff_db)):\n                snpeff_base_dir, snpeff_db = _installed_snpeff_genome(snpeff_db, data[\"config\"])\n        if snpeff_base_dir.endswith(\"/%s\" % snpeff_db):\n            snpeff_base_dir = os.path.dirname(snpeff_base_dir)\n    return snpeff_db, snpeff_base_dir", "response": "Retrieve a snpEff database name and location relative to reference file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_snpeff_cmd(cmd_name, datadir, data, out_file):\n    resources = config_utils.get_resources(\"snpeff\", data[\"config\"])\n    jvm_opts = resources.get(\"jvm_opts\", [\"-Xms750m\", \"-Xmx3g\"])\n    # scale by cores, defaulting to 2x base usage to ensure we have enough memory\n    # for single core runs to use with human genomes.\n    # Sets a maximum amount of memory to avoid core dumps exceeding 32Gb\n    # We shouldn't need that much memory for snpEff, so avoid issues\n    # https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html#compressed_oops\n    jvm_opts = config_utils.adjust_opts(jvm_opts, {\"algorithm\": {\"memory_adjust\":\n                                                                 {\"direction\": \"increase\",\n                                                                  \"maximum\": \"30000M\",\n                                                                  \"magnitude\": max(2, dd.get_cores(data))}}})\n    memory = \" \".join(jvm_opts)\n    snpeff = config_utils.get_program(\"snpEff\", data[\"config\"])\n    java_args = \"-Djava.io.tmpdir=%s\" % utils.safe_makedir(os.path.join(os.path.dirname(out_file), \"tmp\"))\n    export = \"unset JAVA_HOME && export PATH=%s:\\\"$PATH\\\" && \" % (utils.get_java_binpath())\n    cmd = \"{export} {snpeff} {memory} {java_args} {cmd_name} -dataDir {datadir}\"\n    return cmd.format(**locals())", "response": "Retrieve snpEff base command line."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning effects prediction with snpEff.", "response": "def _run_snpeff(snp_in, out_format, data):\n    \"\"\"Run effects prediction with snpEff, skipping if snpEff database not present.\n    \"\"\"\n    snpeff_db, datadir = get_db(data)\n    if not snpeff_db:\n        return None, None\n\n    assert os.path.exists(os.path.join(datadir, snpeff_db)), \\\n        \"Did not find %s snpEff genome data in %s\" % (snpeff_db, datadir)\n    ext = utils.splitext_plus(snp_in)[1] if out_format == \"vcf\" else \".tsv\"\n    out_file = \"%s-effects%s\" % (utils.splitext_plus(snp_in)[0], ext)\n    stats_file = \"%s-stats.html\" % utils.splitext_plus(out_file)[0]\n    csv_file = \"%s-stats.csv\" % utils.splitext_plus(out_file)[0]\n    if not utils.file_exists(out_file):\n        config_args = \" \".join(_snpeff_args_from_config(data))\n        if ext.endswith(\".gz\"):\n            bgzip_cmd = \"| %s -c\" % tools.get_bgzip_cmd(data[\"config\"])\n        else:\n            bgzip_cmd = \"\"\n        with file_transaction(data, out_file) as tx_out_file:\n            snpeff_cmd = _get_snpeff_cmd(\"eff\", datadir, data, tx_out_file)\n            cmd = (\"{snpeff_cmd} {config_args} -noLog -i vcf -o {out_format} \"\n                   \"-csvStats {csv_file} -s {stats_file} {snpeff_db} {snp_in} {bgzip_cmd} > {tx_out_file}\")\n            do.run(cmd.format(**locals()), \"snpEff effects\", data)\n    if ext.endswith(\".gz\"):\n        out_file = vcfutils.bgzip_and_index(out_file, data[\"config\"])\n    return out_file, [stats_file, csv_file]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds the most recent installed genome for the given base name.", "response": "def _installed_snpeff_genome(base_name, config):\n    \"\"\"Find the most recent installed genome for snpEff with the given name.\n    \"\"\"\n    snpeff_config_file = os.path.join(config_utils.get_program(\"snpeff\", config, \"dir\"),\n                                      \"snpEff.config\")\n    if os.path.exists(snpeff_config_file):\n        data_dir = _find_snpeff_datadir(snpeff_config_file)\n        dbs = [d for d in sorted(glob.glob(os.path.join(data_dir, \"%s*\" % base_name)), reverse=True)\n               if os.path.isdir(d)]\n    else:\n        data_dir = None\n        dbs = []\n    if len(dbs) == 0:\n        raise ValueError(\"No database found in %s for %s\" % (data_dir, base_name))\n    else:\n        return data_dir, os.path.split(dbs[0])[-1]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nperform piped alignment of fastq files generating sorted deduplicated BAM.", "response": "def align(fastq_file, pair_file, index_dir, names, align_dir, data):\n    \"\"\"Perform piped alignment of fastq input files, generating sorted, deduplicated BAM.\n    \"\"\"\n    umi_ext = \"-cumi\" if \"umi_bam\" in data else \"\"\n    out_file = os.path.join(align_dir, \"{0}-sort{1}.bam\".format(dd.get_sample_name(data), umi_ext))\n    num_cores = data[\"config\"][\"algorithm\"].get(\"num_cores\", 1)\n    rg_info = novoalign.get_rg_info(names)\n    preset = \"sr\"\n\n    pair_file = pair_file if pair_file else \"\"\n    if data.get(\"align_split\"):\n        final_file = out_file\n        out_file, data = alignprep.setup_combine(final_file, data)\n        fastq_file, pair_file = alignprep.split_namedpipe_cls(fastq_file, pair_file, data)\n    else:\n        final_file = None\n\n    if not utils.file_exists(out_file) and (final_file is None or not utils.file_exists(final_file)):\n        with postalign.tobam_cl(data, out_file, pair_file != \"\") as (tobam_cl, tx_out_file):\n            index_file = None\n            # Skip trying to use indices now as they provide only slight speed-ups\n            # and give inconsitent outputs in BAM headers\n            # If a single index present, index_dir points to that\n            # if index_dir and os.path.isfile(index_dir):\n            #     index_dir = os.path.dirname(index_dir)\n            #     index_file = os.path.join(index_dir, \"%s-%s.mmi\" % (dd.get_genome_build(data), preset))\n            if not index_file or not os.path.exists(index_file):\n                index_file = dd.get_ref_file(data)\n            cmd = (\"minimap2 -a -x {preset} -R '{rg_info}' -t {num_cores} {index_file} \"\n                   \"{fastq_file} {pair_file} | \")\n            do.run(cmd.format(**locals()) + tobam_cl, \"minimap2 alignment: %s\" % dd.get_sample_name(data))\n    data[\"work_bam\"] = out_file\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remap_index_fn(ref_file):\n    index_dir = os.path.join(os.path.dirname(ref_file), os.pardir, \"minimap2\")\n    if os.path.exists(index_dir) and os.path.isdir(index_dir):\n        return index_dir\n    else:\n        return os.path.dirname(ref_file)", "response": "remap the index file to the base directory"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_new_csv(samples, args):\n    out_fn = os.path.splitext(args.csv)[0] + \"-merged.csv\"\n    logger.info(\"Preparing new csv: %s\" % out_fn)\n    with file_transaction(out_fn) as tx_out:\n        with open(tx_out, 'w') as handle:\n            handle.write(_header(args.csv))\n            for s in samples:\n                sample_name = s['name'] if isinstance(s['out_file'], list) else os.path.basename(s['out_file'])\n                handle.write(\"%s,%s,%s\\n\" % (sample_name, s['name'], \",\".join(s['anno'])))", "response": "create new csv file that can be used with bcbio - w template"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses csv file with one line per file and return a dictionary of samples to process.", "response": "def _get_samples_to_process(fn, out_dir, config, force_single, separators):\n    \"\"\"parse csv file with one line per file. It will merge\n    all files that have the same description name\"\"\"\n    out_dir = os.path.abspath(out_dir)\n    samples = defaultdict(list)\n    with open(fn) as handle:\n        for l in handle:\n            if l.find(\"description\") > 0:\n                logger.info(\"Skipping header.\")\n                continue\n            cols = l.strip().split(\",\")\n            if len(cols) > 0:\n                if len(cols) < 2:\n                    raise ValueError(\"Line needs 2 values: file and name.\")\n                if utils.file_exists(cols[0]) or is_gsm(cols[0]) or is_srr(cols[0]):\n                    if cols[0].find(\" \") > -1:\n                        new_name = os.path.abspath(cols[0].replace(\" \", \"_\"))\n                        logger.warning(\"Space finds in %s. Linked to %s.\" % (cols[0], new_name))\n                        logger.warning(\"Please, avoid names with spaces in the future.\")\n                        utils.symlink_plus(os.path.abspath(cols[0]), new_name)\n                        cols[0] = new_name\n                    samples[cols[1]].append(cols)\n                else:\n                    logger.info(\"skipping %s, File doesn't exist.\" % cols[0])\n    for sample, items in samples.items():\n        if is_fastq(items[0][0], True):\n            fn = \"fq_merge\"\n            ext = \".fastq.gz\"\n        elif is_bam(items[0][0]):\n            fn = \"bam_merge\"\n            ext = \".bam\"\n        elif is_gsm(items[0][0]):\n            fn = \"query_gsm\"\n            ext = \".fastq.gz\"\n        elif is_srr(items[0][0]):\n            fn = \"query_srr\"\n            ext = \".fastq.gz\"\n        files = [os.path.abspath(fn_file[0]) if utils.file_exists(fn_file[0]) else fn_file[0] for fn_file in items]\n        samples[sample] = [{'files': _check_paired(files, force_single, separators),\n                            'out_file': os.path.join(out_dir, sample + ext),\n                            'fn': fn, 'anno': items[0][2:], 'config': config,\n                            'name': sample, 'out_dir': out_dir}]\n    return [samples[sample] for sample in samples]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if files are the same and use full path then", "response": "def _check_stems(files):\n    \"\"\"check if stem names are the same and use full path then\"\"\"\n    used = set()\n    for fn in files:\n        if os.path.basename(fn) in used:\n            logger.warning(\"%s stem is multiple times in your file list, \"\n                         \"so we don't know \"\n                         \"how to assign it to the sample data in the CSV. \"\n                         \"We are gonna use full path to make a difference, \"\n                         \"that means paired files should be in the same folder. \"\n                         \"If this is a problem, you should rename the files you want \"\n                         \"to merge. Sorry, no possible magic here.\" % os.path.basename(fn)\n                         )\n            return True\n        used.add(os.path.basename(fn))\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_paired(files, force_single, separators):\n    full_name = _check_stems(files)\n    if files[0].endswith(\".bam\"):\n        return files\n    elif is_gsm(files[0]):\n        return files\n    return combine_pairs(files, force_single, full_name, separators)", "response": "check if files are fastq. gz and paired"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_sample(sample):\n    upload_config = sample.get(\"upload\")\n    if upload_config:\n        approach = _approaches[upload_config.get(\"method\", \"filesystem\")]\n        for finfo in _get_files(sample):\n            approach.update_file(finfo, sample, upload_config)\n    return [[sample]]", "response": "Upload results of processing from an analysis pipeline sample."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving files for the sample dispatching by analysis type.", "response": "def _get_files(sample):\n    \"\"\"Retrieve files for the sample, dispatching by analysis type.\n\n    Each file is a dictionary containing the path plus associated\n    metadata about the file and pipeline versions.\n    \"\"\"\n    analysis = sample.get(\"analysis\")\n    if analysis.lower() in [\"variant\", \"snp calling\", \"variant2\", \"standard\"]:\n        return _get_files_variantcall(sample)\n    elif analysis.lower() in [\"rna-seq\", \"fastrna-seq\"]:\n        return _get_files_rnaseq(sample)\n    elif analysis.lower() in [\"smallrna-seq\"]:\n        return _get_files_srnaseq(sample)\n    elif analysis.lower() in [\"chip-seq\"]:\n        return _get_files_chipseq(sample)\n    elif analysis.lower() in [\"scrna-seq\"]:\n        return _get_files_scrnaseq(sample)\n    else:\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds top level information about the sample or flowcell to output.", "response": "def _add_meta(xs, sample=None, config=None):\n    \"\"\"Add top level information about the sample or flowcell to output.\n\n    Sorts outputs into sample names (sample input) and project (config input).\n    \"\"\"\n    out = []\n    for x in xs:\n        if not isinstance(x[\"path\"], six.string_types) or not os.path.exists(x[\"path\"]):\n            raise ValueError(\"Unexpected path for upload: %s\" % x)\n        x[\"mtime\"] = shared.get_file_timestamp(x[\"path\"])\n        if sample:\n            sample_name = dd.get_sample_name(sample)\n            if \"sample\" not in x:\n                x[\"sample\"] = sample_name\n            elif x[\"sample\"] != sample_name:\n                x[\"run\"] = sample_name\n        if config:\n            fc_name = config.get(\"fc_name\") or \"project\"\n            fc_date = config.get(\"fc_date\") or datetime.datetime.now().strftime(\"%Y-%m-%d\")\n            x[\"run\"] = \"%s_%s\" % (fc_date, fc_name)\n        out.append(x)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn output files for the variant calling pipeline.", "response": "def _get_files_variantcall(sample):\n    \"\"\"Return output files for the variant calling pipeline.\n    \"\"\"\n    out = []\n    algorithm = sample[\"config\"][\"algorithm\"]\n    out = _maybe_add_summary(algorithm, sample, out)\n    out = _maybe_add_alignment(algorithm, sample, out)\n    out = _maybe_add_callable(sample, out)\n    out = _maybe_add_disambiguate(algorithm, sample, out)\n    out = _maybe_add_variant_file(algorithm, sample, out)\n    out = _maybe_add_sv(algorithm, sample, out)\n    out = _maybe_add_hla(algorithm, sample, out)\n    out = _maybe_add_heterogeneity(algorithm, sample, out)\n\n    out = _maybe_add_validate(algorithm, sample, out)\n    return _add_meta(out, sample)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _maybe_add_callable(data, out):\n    callable_bed = dd.get_sample_callable(data)\n    if callable_bed:\n        out.append({\"path\": callable_bed, \"type\": \"bed\", \"ext\": \"callable\"})\n    perbase_bed = tz.get_in([\"depth\", \"variant_regions\", \"per_base\"], data)\n    if perbase_bed:\n        out.append({\"path\": perbase_bed, \"type\": \"bed.gz\", \"ext\": \"depth-per-base\"})\n    return out", "response": "Add callable and depth regions to output folder."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving batch name for use in SV calling outputs.", "response": "def _get_batch_name(sample):\n    \"\"\"Retrieve batch name for use in SV calling outputs.\n\n    Handles multiple batches split via SV calling.\n    \"\"\"\n    batch = dd.get_batch(sample) or dd.get_sample_name(sample)\n    if isinstance(batch, (list, tuple)) and len(batch) > 1:\n        batch = dd.get_sample_name(sample)\n    return batch"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if a sample file is the same as the population file.", "response": "def _sample_variant_file_in_population(x):\n    \"\"\"Check if a sample file is the same as the population file.\n\n    This is true for batches where we don't extract into samples and do not\n    run decomposition for gemini.\n    '\"\"\"\n    if \"population\" in x:\n        a = _get_project_vcf(x)\n        b = _get_variant_file(x, (\"vrn_file\",))\n        decomposed = tz.get_in((\"population\", \"decomposed\"), x)\n        if (a and b and not decomposed and len(a) > 0 and len(b) > 0 and\n              vcfutils.get_samples(a[0][\"path\"]) == vcfutils.get_samples(b[0][\"path\"])):\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_variant_file(x, key, suffix=\"\", sample=None, ignore_do_upload=False):\n    out = []\n    fname = utils.get_in(x, key)\n    upload_key = list(key)\n    upload_key[-1] = \"do_upload\"\n    do_upload = tz.get_in(tuple(upload_key), x, True)\n    if fname and (ignore_do_upload or do_upload):\n        if fname.endswith(\".vcf.gz\"):\n            out.append({\"path\": fname,\n                        \"type\": \"vcf.gz\",\n                        \"ext\": \"%s%s\" % (x[\"variantcaller\"], suffix),\n                        \"variantcaller\": x[\"variantcaller\"]})\n            if utils.file_exists(fname + \".tbi\"):\n                out.append({\"path\": fname + \".tbi\",\n                            \"type\": \"vcf.gz.tbi\",\n                            \"index\": True,\n                            \"ext\": \"%s%s\" % (x[\"variantcaller\"], suffix),\n                            \"variantcaller\": x[\"variantcaller\"]})\n        elif fname.endswith((\".vcf\", \".bed\", \".bedpe\", \".bedgraph\", \".cnr\", \".cns\", \".cnn\", \".txt\", \".tsv\")):\n            ftype = utils.splitext_plus(fname)[-1][1:]\n            if ftype == \"txt\":\n                extended_ftype = fname.split(\"-\")[-1]\n                if \"/\" not in extended_ftype:\n                    ftype = extended_ftype\n            out.append({\"path\": fname,\n                        \"type\": ftype,\n                        \"ext\": \"%s%s\" % (x[\"variantcaller\"], suffix),\n                        \"variantcaller\": x[\"variantcaller\"]})\n    if sample:\n        out_sample = []\n        for x in out:\n            x[\"sample\"] = sample\n            out_sample.append(x)\n        return out_sample\n    else:\n        return out", "response": "Retrieve VCF file with the given key if it exists handling bgzipped."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _flatten_file_with_secondary(input, out_dir):\n    out = []\n    orig_dir = os.path.dirname(input[\"base\"])\n    for finfo in [input[\"base\"]] + input.get(\"secondary\", []):\n        cur_dir = os.path.dirname(finfo)\n        if cur_dir != orig_dir and cur_dir.startswith(orig_dir):\n            cur_out_dir = os.path.join(out_dir, cur_dir.replace(orig_dir + \"/\", \"\"))\n        else:\n            cur_out_dir = out_dir\n        out.append({\"path\": finfo, \"dir\": cur_out_dir})\n    return out", "response": "Flatten file representation with secondary indices (CWL - like )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_batch(x, sample):\n    added = False\n    for batch in sorted(dd.get_batches(sample) or [], key=len, reverse=True):\n        if batch and os.path.basename(x[\"path\"]).startswith((\"%s-\" % batch, \"%s.vcf\" % batch)):\n            x[\"batch\"] = batch\n            added = True\n            break\n    if not added:\n        x[\"batch\"] = dd.get_sample_name(sample)\n    return x", "response": "Potentially add batch name to an upload file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_project_vcf(x, suffix=\"\"):\n    vcfs = _get_variant_file(x, (\"population\", \"vcf\"), suffix=suffix)\n    if not vcfs:\n        vcfs = _get_variant_file(x, (\"vrn_file_batch\", ), suffix=suffix, ignore_do_upload=True)\n        if not vcfs and x.get(\"variantcaller\") == \"ensemble\":\n            vcfs = _get_variant_file(x, (\"vrn_file\", ), suffix=suffix)\n    return vcfs", "response": "Get our project VCF from the population or the variant batch file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves output files associated with an entire analysis project.", "response": "def _get_files_project(sample, upload_config):\n    \"\"\"Retrieve output files associated with an entire analysis project.\n    \"\"\"\n    out = [{\"path\": sample[\"provenance\"][\"programs\"]}]\n    if os.path.exists(tz.get_in([\"provenance\", \"data\"], sample) or \"\"):\n        out.append({\"path\": sample[\"provenance\"][\"data\"]})\n    for fname in [\"bcbio-nextgen.log\", \"bcbio-nextgen-commands.log\"]:\n        if os.path.exists(os.path.join(log.get_log_dir(sample[\"config\"]), fname)):\n            out.append({\"path\": os.path.join(log.get_log_dir(sample[\"config\"]), fname),\n                        \"type\": \"external_command_log\",\n                        \"ext\": \"\"})\n\n    if \"summary\" in sample and sample[\"summary\"].get(\"project\"):\n        out.append({\"path\": sample[\"summary\"][\"project\"]})\n    if \"summary\" in sample and sample[\"summary\"].get(\"metadata\"):\n        out.append({\"path\": sample[\"summary\"][\"metadata\"]})\n    mixup_check = tz.get_in([\"summary\", \"mixup_check\"], sample)\n    if mixup_check:\n        out.append({\"path\": sample[\"summary\"][\"mixup_check\"],\n                    \"type\": \"directory\", \"ext\": \"mixup_check\"})\n\n    report = os.path.join(dd.get_work_dir(sample), \"report\")\n    if utils.file_exists(report):\n        out.append({\"path\": report,\n                    \"type\": \"directory\", \"ext\": \"report\"})\n\n    multiqc = tz.get_in([\"summary\", \"multiqc\"], sample)\n    if multiqc:\n        out.extend(_flatten_file_with_secondary(multiqc, \"multiqc\"))\n\n    if sample.get(\"seqcluster\", {}):\n        out.append({\"path\": sample[\"seqcluster\"].get(\"out_dir\"),\n                    \"type\": \"directory\", \"ext\": \"seqcluster\"})\n\n    if sample.get(\"mirge\", {}):\n        for fn in sample[\"mirge\"]:\n            out.append({\"path\": fn,\n                        \"dir\": \"mirge\"})\n\n    if sample.get(\"report\", None):\n        out.append({\"path\": os.path.dirname(sample[\"report\"]),\n                    \"type\": \"directory\", \"ext\": \"seqclusterViz\"})\n\n    for x in sample.get(\"variants\", []):\n        if \"pop_db\" in x:\n            out.append({\"path\": x[\"pop_db\"],\n                        \"type\": \"sqlite\",\n                        \"variantcaller\": x[\"variantcaller\"]})\n    for x in sample.get(\"variants\", []):\n        if \"population\" in x:\n            pop_db = tz.get_in([\"population\", \"db\"], x)\n            if pop_db:\n                out.append({\"path\": pop_db,\n                            \"type\": \"sqlite\",\n                            \"variantcaller\": x[\"variantcaller\"]})\n            suffix = \"-annotated-decomposed\" if tz.get_in((\"population\", \"decomposed\"), x) else \"-annotated\"\n            vcfs = _get_project_vcf(x, suffix)\n            out.extend([_add_batch(f, sample) for f in vcfs])\n    for x in sample.get(\"variants\", []):\n        if x.get(\"validate\") and x[\"validate\"].get(\"grading_summary\"):\n            out.append({\"path\": x[\"validate\"][\"grading_summary\"]})\n            break\n    sv_project = set([])\n    for svcall in sample.get(\"sv\", []):\n        if svcall.get(\"variantcaller\") == \"seq2c\":\n            if svcall.get(\"calls_all\") and svcall[\"calls_all\"] not in sv_project:\n                out.append({\"path\": svcall[\"coverage_all\"], \"batch\": \"seq2c\", \"ext\": \"coverage\", \"type\": \"tsv\"})\n                out.append({\"path\": svcall[\"read_mapping\"], \"batch\": \"seq2c\", \"ext\": \"read_mapping\", \"type\": \"txt\"})\n                out.append({\"path\": svcall[\"calls_all\"], \"batch\": \"seq2c\", \"ext\": \"calls\", \"type\": \"tsv\"})\n                sv_project.add(svcall[\"calls_all\"])\n    if \"coverage\" in sample:\n        cov_db = tz.get_in([\"coverage\", \"summary\"], sample)\n        if cov_db:\n            out.append({\"path\": cov_db, \"type\": \"sqlite\", \"ext\": \"coverage\"})\n        all_coverage = tz.get_in([\"coverage\", \"all\"], sample)\n        if all_coverage:\n            out.append({\"path\": all_coverage, \"type\": \"bed\", \"ext\": \"coverage\"})\n\n    if dd.get_mirna_counts(sample):\n        out.append({\"path\": dd.get_mirna_counts(sample)})\n    if dd.get_isomir_counts(sample):\n        out.append({\"path\": dd.get_isomir_counts(sample)})\n    if dd.get_novel_mirna_counts(sample):\n        out.append({\"path\": dd.get_novel_mirna_counts(sample)})\n    if dd.get_novel_isomir_counts(sample):\n        out.append({\"path\": dd.get_novel_isomir_counts(sample)})\n    if dd.get_combined_counts(sample):\n        count_file = dd.get_combined_counts(sample)\n        if sample[\"analysis\"].lower() == \"scrna-seq\":\n            out.append({\"path\": count_file,\n                        \"type\": \"mtx\"})\n            out.append({\"path\": count_file + \".rownames\",\n                        \"type\": \"rownames\"})\n            out.append({\"path\": count_file + \".colnames\",\n                        \"type\": \"colnames\"})\n            out.append({\"path\": count_file + \".metadata\",\n                        \"type\": \"metadata\"})\n            umi_file = os.path.splitext(count_file)[0] + \"-dupes.mtx\"\n            if utils.file_exists(umi_file):\n                out.append({\"path\": umi_file,\n                            \"type\": \"mtx\"})\n                out.append({\"path\": umi_file + \".rownames\",\n                            \"type\": \"rownames\"})\n                out.append({\"path\": umi_file + \".colnames\",\n                            \"type\": \"colnames\"})\n            if dd.get_combined_histogram(sample):\n                out.append({\"path\": dd.get_combined_histogram(sample),\n                            \"type\": \"txt\"})\n            rda = os.path.join(os.path.dirname(count_file), \"se.rda\")\n            if utils.file_exists(rda):\n                out.append({\"path\": rda,\n                            \"type\": \"rda\"})\n        else:\n            out.append({\"path\": dd.get_combined_counts(sample)})\n    if dd.get_annotated_combined_counts(sample):\n        out.append({\"path\": dd.get_annotated_combined_counts(sample)})\n    if dd.get_combined_fpkm(sample):\n        out.append({\"path\": dd.get_combined_fpkm(sample)})\n    if dd.get_combined_fpkm_isoform(sample):\n        out.append({\"path\": dd.get_combined_fpkm_isoform(sample)})\n    if dd.get_transcript_assembler(sample):\n        out.append({\"path\": dd.get_merged_gtf(sample)})\n    if dd.get_dexseq_counts(sample):\n        out.append({\"path\": dd.get_dexseq_counts(sample)})\n        out.append({\"path\": \"%s.ann\" % dd.get_dexseq_counts(sample)})\n    if dd.get_express_counts(sample):\n        out.append({\"path\": dd.get_express_counts(sample)})\n    if dd.get_express_fpkm(sample):\n        out.append({\"path\": dd.get_express_fpkm(sample)})\n    if dd.get_express_tpm(sample):\n        out.append({\"path\": dd.get_express_tpm(sample)})\n    if dd.get_isoform_to_gene(sample):\n        out.append({\"path\": dd.get_isoform_to_gene(sample)})\n    if dd.get_square_vcf(sample):\n        out.append({\"path\": dd.get_square_vcf(sample)})\n    if dd.get_sailfish_transcript_tpm(sample):\n        out.append({\"path\": dd.get_sailfish_transcript_tpm(sample)})\n    if dd.get_sailfish_gene_tpm(sample):\n        out.append({\"path\": dd.get_sailfish_gene_tpm(sample)})\n    if dd.get_tx2gene(sample):\n        out.append({\"path\": dd.get_tx2gene(sample)})\n    if dd.get_spikein_counts(sample):\n        out.append({\"path\": dd.get_spikein_counts(sample)})\n    transcriptome_dir = os.path.join(dd.get_work_dir(sample), \"inputs\",\n                                     \"transcriptome\")\n    if os.path.exists(transcriptome_dir):\n        out.append({\"path\": transcriptome_dir, \"type\": \"directory\",\n                    \"ext\": \"transcriptome\"})\n    return _add_meta(out, config=upload_config)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprovide a dictionary remapping original read indexes to new indexes.", "response": "def _id_remapper(orig, new):\n    \"\"\"Provide a dictionary remapping original read indexes to new indexes.\n\n    When re-ordering the header, the individual read identifiers need to be\n    updated as well.\n    \"\"\"\n    new_chrom_to_index = {}\n    for i_n, (chr_n, _) in enumerate(new):\n        new_chrom_to_index[chr_n] = i_n\n    remap_indexes = {}\n    for i_o, (chr_o, _) in enumerate(orig):\n        if chr_o in new_chrom_to_index.keys():\n            remap_indexes[i_o] = new_chrom_to_index[chr_o]\n    remap_indexes[None] = None\n    return remap_indexes"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncleans illegal characters in input fasta file which cause problems downstream.", "response": "def _clean_rec_name(rec):\n    \"\"\"Clean illegal characters in input fasta file which cause problems downstream.\n    \"\"\"\n    out_id = []\n    for char in list(rec.id):\n        if char in ALLOWED_CONTIG_NAME_CHARS:\n            out_id.append(char)\n        else:\n            out_id.append(\"_\")\n    rec.id = \"\".join(out_id)\n    rec.description = \"\"\n    return rec"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(_, data, out_dir):\n    # logger.info(\"Number of aligned reads < than 0.60 in %s: %s\" % (dd.get_sample_name(data), ratio))\n    logger.info(\"Running kraken to determine contaminant: %s\" % dd.get_sample_name(data))\n    # ratio = bam.get_aligned_reads(bam_file, data)\n    out = out_stats = None\n    db = tz.get_in([\"config\", \"algorithm\", \"kraken\"], data)\n    if db and isinstance(db, (list, tuple)):\n        db = db[0]\n    kraken_cmd = config_utils.get_program(\"kraken\", data[\"config\"])\n    if db == \"minikraken\":\n        db = os.path.join(install._get_data_dir(), \"genomes\", \"kraken\", \"minikraken\")\n\n    if not os.path.exists(db):\n        logger.info(\"kraken: no database found %s, skipping\" % db)\n        return {\"kraken_report\": \"null\"}\n\n    if not os.path.exists(os.path.join(out_dir, \"kraken_out\")):\n        work_dir = os.path.dirname(out_dir)\n        utils.safe_makedir(work_dir)\n        num_cores = data[\"config\"][\"algorithm\"].get(\"num_cores\", 1)\n        fn_file = data[\"files_orig\"][0] if dd.get_save_diskspace(data) else data[\"files\"][0]\n        if fn_file.endswith(\"bam\"):\n            logger.info(\"kraken: need fastq files as input\")\n            return {\"kraken_report\": \"null\"}\n        with tx_tmpdir(data) as tx_tmp_dir:\n            with utils.chdir(tx_tmp_dir):\n                out = os.path.join(tx_tmp_dir, \"kraken_out\")\n                out_stats = os.path.join(tx_tmp_dir, \"kraken_stats\")\n                cat = \"zcat\" if fn_file.endswith(\".gz\") else \"cat\"\n                cl = (\"{cat} {fn_file} | {kraken_cmd} --db {db} --quick \"\n                      \"--preload --min-hits 2 \"\n                      \"--threads {num_cores} \"\n                      \"--output {out} --fastq-input /dev/stdin  2> {out_stats}\").format(**locals())\n                do.run(cl, \"kraken: %s\" % dd.get_sample_name(data))\n                if os.path.exists(out_dir):\n                    shutil.rmtree(out_dir)\n                shutil.move(tx_tmp_dir, out_dir)\n    metrics = _parse_kraken_output(out_dir, db, data)\n    return metrics", "response": "Run kraken and return report."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse kraken output from stderr and generate kraken report", "response": "def _parse_kraken_output(out_dir, db, data):\n    \"\"\"Parse kraken stat info comming from stderr,\n       generating report with kraken-report\n    \"\"\"\n    in_file = os.path.join(out_dir, \"kraken_out\")\n    stat_file = os.path.join(out_dir, \"kraken_stats\")\n    out_file = os.path.join(out_dir, \"kraken_summary\")\n    kraken_cmd = config_utils.get_program(\"kraken-report\", data[\"config\"])\n    classify = unclassify = None\n    with open(stat_file, 'r') as handle:\n        for line in handle:\n            if line.find(\" classified\") > -1:\n                classify = line[line.find(\"(\") + 1:line.find(\")\")]\n            if line.find(\" unclassified\") > -1:\n                unclassify = line[line.find(\"(\") + 1:line.find(\")\")]\n    if os.path.getsize(in_file) > 0 and not os.path.exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            cl = (\"{kraken_cmd} --db {db} {in_file} > {tx_out_file}\").format(**locals())\n            do.run(cl, \"kraken report: %s\" % dd.get_sample_name(data))\n    kraken = {\"kraken_clas\": classify, \"kraken_unclas\": unclassify}\n    kraken_sum = _summarize_kraken(out_file)\n    kraken.update(kraken_sum)\n    return kraken"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _summarize_kraken(fn):\n    kraken = {}\n    list_sp, list_value = [], []\n    with open(fn) as handle:\n        for line in handle:\n            cols = line.strip().split(\"\\t\")\n            sp = cols[5].strip()\n            if len(sp.split(\" \")) > 1 and not sp.startswith(\"cellular\"):\n                list_sp.append(sp)\n                list_value.append(cols[0])\n    kraken = {\"kraken_sp\": list_sp, \"kraken_value\": list_value}\n    return kraken", "response": "get the value at species level"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the main CWL and sample JSON files from a bcbio generated directory.", "response": "def _get_main_and_json(directory):\n    \"\"\"Retrieve the main CWL and sample JSON files from a bcbio generated directory.\n    \"\"\"\n    directory = os.path.normpath(os.path.abspath(directory))\n    checker_main = os.path.normpath(os.path.join(directory, os.path.pardir, \"checker-workflow-wrapping-tool.cwl\"))\n    if checker_main and os.path.exists(checker_main):\n        main_cwl = [checker_main]\n    else:\n        main_cwl = glob.glob(os.path.join(directory, \"main-*.cwl\"))\n        main_cwl = [x for x in main_cwl if not x.find(\"-pack\") >= 0]\n        assert len(main_cwl) == 1, \"Did not find main CWL in %s\" % directory\n    main_json = glob.glob(os.path.join(directory, \"main-*-samples.json\"))\n    assert len(main_json) == 1, \"Did not find main json in %s\" % directory\n    project_name = os.path.basename(directory).split(\"-workflow\")[0]\n    return main_cwl[0], main_json[0], project_name"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _run_tool(cmd, use_container=True, work_dir=None, log_file=None):\n    if isinstance(cmd, (list, tuple)):\n        cmd = \" \".join([str(x) for x in cmd])\n    cmd = utils.local_path_export(at_start=use_container) + cmd\n    if log_file:\n        cmd += \" 2>&1 | tee -a %s\" % log_file\n    try:\n        print(\"Running: %s\" % cmd)\n        subprocess.check_call(cmd, shell=True)\n    finally:\n        if use_container and work_dir:\n            _chown_workdir(work_dir)", "response": "Run with injection of bcbio path."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _pack_cwl(unpacked_cwl):\n    out_file = \"%s-pack%s\" % os.path.splitext(unpacked_cwl)\n    cmd = \"cwltool --pack {unpacked_cwl} > {out_file}\"\n    _run_tool(cmd.format(**locals()))\n    return out_file", "response": "Pack CWL into a single document for submission."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nensures work directory files owned by original user.", "response": "def _chown_workdir(work_dir):\n    \"\"\"Ensure work directory files owned by original user.\n\n    Docker runs can leave root owned files making cleanup difficult.\n    Skips this if it fails, avoiding errors where we run remotely\n    and don't have docker locally.\n    \"\"\"\n    cmd = (\"\"\"docker run --rm -v %s:%s quay.io/bcbio/bcbio-base /bin/bash -c 'chown -R %s %s'\"\"\" %\n           (work_dir, work_dir, os.getuid(), work_dir))\n    try:\n        subprocess.check_call(cmd, shell=True)\n    except subprocess.CalledProcessError:\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\navoid referencing minimal bcbio_nextgen in bcbio_vm installation.", "response": "def _remove_bcbiovm_path():\n    \"\"\"Avoid referencing minimal bcbio_nextgen in bcbio_vm installation.\n    \"\"\"\n    cur_path = os.path.dirname(os.path.realpath(sys.executable))\n    paths = os.environ[\"PATH\"].split(\":\")\n    if cur_path in paths:\n        paths.remove(cur_path)\n        os.environ[\"PATH\"] = \":\".join(paths)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning with cwltool -- reference implementation.", "response": "def _run_cwltool(args):\n    \"\"\"Run with cwltool -- reference implementation.\n    \"\"\"\n    main_file, json_file, project_name = _get_main_and_json(args.directory)\n    work_dir = utils.safe_makedir(os.path.join(os.getcwd(), \"cwltool_work\"))\n    tmp_dir = utils.safe_makedir(os.path.join(work_dir, \"tmpcwl\"))\n    log_file = os.path.join(work_dir, \"%s-cwltool.log\" % project_name)\n    os.environ[\"TMPDIR\"] = tmp_dir\n    flags = [\"--tmpdir-prefix\", tmp_dir, \"--tmp-outdir-prefix\", tmp_dir]\n    if args.no_container:\n        _remove_bcbiovm_path()\n        flags += [\"--no-container\", \"--preserve-environment\", \"PATH\", \"--preserve-environment\", \"HOME\"]\n    cmd = [\"cwltool\"] + flags + args.toolargs + [\"--\", main_file, json_file]\n    with utils.chdir(work_dir):\n        _run_tool(cmd, not args.no_container, work_dir, log_file=log_file)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _run_arvados(args):\n    assert not args.no_container, \"Arvados runs require containers\"\n    assert \"ARVADOS_API_TOKEN\" in os.environ and \"ARVADOS_API_HOST\" in os.environ, \\\n        \"Need to set ARVADOS_API_TOKEN and ARVADOS_API_HOST in environment to run\"\n    main_file, json_file, project_name = _get_main_and_json(args.directory)\n    flags = [\"--enable-reuse\", \"--api\", \"containers\", \"--submit\", \"--no-wait\"]\n    cmd = [\"arvados-cwl-runner\"] + flags + args.toolargs + [main_file, json_file]\n    _run_tool(cmd)", "response": "Run CWL on Arvados."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _run_toil(args):\n    main_file, json_file, project_name = _get_main_and_json(args.directory)\n    work_dir = utils.safe_makedir(os.path.join(os.getcwd(), \"toil_work\"))\n    tmp_dir = utils.safe_makedir(os.path.join(work_dir, \"tmpdir\"))\n    os.environ[\"TMPDIR\"] = tmp_dir\n    log_file = os.path.join(work_dir, \"%s-toil.log\" % project_name)\n    jobstore = os.path.join(work_dir, \"cwltoil_jobstore\")\n    flags = [\"--jobStore\", jobstore, \"--logFile\", log_file, \"--workDir\", tmp_dir, \"--linkImports\"]\n    if os.path.exists(jobstore):\n        flags += [\"--restart\"]\n    # caching causes issues for batch systems\n    if \"--batchSystem\" in args.toolargs:\n        flags += [\"--disableCaching\"]\n    flags += args.toolargs\n    if args.no_container:\n        _remove_bcbiovm_path()\n        flags += [\"--no-container\", \"--preserve-environment\", \"PATH\", \"HOME\"]\n    cmd = [\"cwltoil\"] + flags + [\"--\", main_file, json_file]\n    with utils.chdir(work_dir):\n        _run_tool(cmd, not args.no_container, work_dir)\n        for tmpdir in (glob.glob(os.path.join(work_dir, \"out_tmpdir*\")) +\n                       glob.glob(os.path.join(work_dir, \"tmp*\"))):\n            if os.path.isdir(tmpdir):\n                shutil.rmtree(tmpdir)", "response": "Run CWL with Toil.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning CWL with rabix bunny.", "response": "def _run_bunny(args):\n    \"\"\"Run CWL with rabix bunny.\n    \"\"\"\n    main_file, json_file, project_name = _get_main_and_json(args.directory)\n    work_dir = utils.safe_makedir(os.path.join(os.getcwd(), \"bunny_work\"))\n    flags = [\"-b\", work_dir]\n    log_file = os.path.join(work_dir, \"%s-bunny.log\" % project_name)\n    if os.path.exists(work_dir):\n        caches = [os.path.join(work_dir, d) for d in os.listdir(work_dir)\n                  if os.path.isdir(os.path.join(work_dir, d))]\n        if caches:\n            flags += [\"--cache-dir\", max(caches, key=os.path.getmtime)]\n    if args.no_container:\n        _remove_bcbiovm_path()\n        flags += [\"--no-container\"]\n    cmd = [\"rabix\"] + flags + [main_file, json_file]\n    with utils.chdir(work_dir):\n        _run_tool(cmd, not args.no_container, work_dir, log_file)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning a WES endpoint and return the result.", "response": "def _run_wes(args):\n    \"\"\"Run CWL using a Workflow Execution Service (WES) endpoint\n    \"\"\"\n    main_file, json_file, project_name = _get_main_and_json(args.directory)\n    main_file = _pack_cwl(main_file)\n    if args.host and \"stratus\" in args.host:\n        _run_wes_stratus(args, main_file, json_file)\n    else:\n        opts = [\"--no-wait\"]\n        if args.host:\n            opts += [\"--host\", args.host]\n        if args.auth:\n            opts += [\"--auth\", args.auth]\n        cmd = [\"wes-client\"] + opts + [main_file, json_file]\n        _run_tool(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _run_wes_stratus(args, main_file, json_file):\n    import requests\n    base_url = args.host\n    if not base_url.startswith(\"http\"):\n        base_url = \"https://%s\" % base_url\n    with open(main_file) as in_handle:\n        r = requests.post(\"%s/v1/workflows\" % base_url,\n                          headers={\"Content-Type\": \"application/json\",\n                                   \"Authorization\": \"Bearer %s\" % args.auth},\n                          data=in_handle.read())\n    print(r.status_code)\n    print(r.text)", "response": "Run WES on Illumina stratus endpoint server which wes - client doesn t support."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nestimating Java memory requirements based on number of samples.", "response": "def _estimate_runner_memory(json_file):\n    \"\"\"Estimate Java memory requirements based on number of samples.\n\n    A rough approach to selecting correct allocated memory for Cromwell.\n    \"\"\"\n    with open(json_file) as in_handle:\n        sinfo = json.load(in_handle)\n    num_parallel = 1\n    for key in [\"config__algorithm__variantcaller\", \"description\"]:\n        item_counts = []\n        n = 0\n        for val in (sinfo.get(key) or []):\n            n += 1\n            if val:\n                if isinstance(val, (list, tuple)):\n                    item_counts.append(len(val))\n                else:\n                    item_counts.append(1)\n        print(key, n, item_counts)\n        if n and item_counts:\n            num_parallel = n * max(item_counts)\n            break\n    if num_parallel < 25:\n        return \"3g\"\n    if num_parallel < 150:\n        return \"6g\"\n    elif num_parallel < 500:\n        return \"12g\"\n    else:\n        return \"24g\""}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _run_cromwell(args):\n    main_file, json_file, project_name = _get_main_and_json(args.directory)\n    work_dir = utils.safe_makedir(os.path.join(os.getcwd(), \"cromwell_work\"))\n    final_dir = utils.safe_makedir(os.path.join(work_dir, \"final\"))\n    if args.no_container:\n        _remove_bcbiovm_path()\n    log_file = os.path.join(work_dir, \"%s-cromwell.log\" % project_name)\n    metadata_file = os.path.join(work_dir, \"%s-metadata.json\" % project_name)\n    option_file = os.path.join(work_dir, \"%s-options.json\" % project_name)\n    cromwell_opts = {\"final_workflow_outputs_dir\": final_dir,\n                     \"default_runtime_attributes\": {\"bootDiskSizeGb\": 20}}\n    with open(option_file, \"w\") as out_handle:\n        json.dump(cromwell_opts, out_handle)\n\n    cmd = [\"cromwell\", \"-Xms1g\", \"-Xmx%s\" % _estimate_runner_memory(json_file),\n           \"run\", \"--type\", \"CWL\",\n           \"-Dconfig.file=%s\" % hpc.create_cromwell_config(args, work_dir, json_file)]\n    cmd += hpc.args_to_cromwell_cl(args)\n    cmd += [\"--metadata-output\", metadata_file, \"--options\", option_file,\n            \"--inputs\", json_file, main_file]\n    with utils.chdir(work_dir):\n        _run_tool(cmd, not args.no_container, work_dir, log_file)\n        if metadata_file and utils.file_exists(metadata_file):\n            with open(metadata_file) as in_handle:\n                metadata = json.load(in_handle)\n            if metadata[\"status\"] == \"Failed\":\n                _cromwell_debug(metadata)\n                sys.exit(1)\n            else:\n                _cromwell_move_outputs(metadata, final_dir)", "response": "Run CWL with Cromwell."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nformat Cromwell failures to make debugging easier.", "response": "def _cromwell_debug(metadata):\n    \"\"\"Format Cromwell failures to make debugging easier.\n    \"\"\"\n    def get_failed_calls(cur, key=None):\n        if key is None: key = []\n        out = []\n        if isinstance(cur, dict) and \"failures\" in cur and \"callRoot\" in cur:\n            out.append((key, cur))\n        elif isinstance(cur, dict):\n            for k, v in cur.items():\n                out.extend(get_failed_calls(v, key + [k]))\n        elif isinstance(cur, (list, tuple)):\n            for i, v in enumerate(cur):\n                out.extend(get_failed_calls(v, key + [i]))\n        return out\n    print(\"Failed bcbio Cromwell run\")\n    print(\"-------------------------\")\n    for fail_k, fail_call in get_failed_calls(metadata[\"calls\"]):\n        root_dir = os.path.join(\"cromwell_work\", os.path.relpath(fail_call[\"callRoot\"]))\n        print(\"Failure in step: %s\" % \".\".join([str(x) for x in fail_k]))\n        print(\"  bcbio log file     : %s\" % os.path.join(root_dir, \"execution\", \"log\", \"bcbio-nextgen-debug.log\"))\n        print(\"  bcbio commands file: %s\" % os.path.join(root_dir, \"execution\", \"log\",\n                                                         \"bcbio-nextgen-commands.log\"))\n        print(\"  Cromwell directory : %s\" % root_dir)\n        print()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmoves Cromwell outputs to the final upload directory.", "response": "def _cromwell_move_outputs(metadata, final_dir):\n    \"\"\"Move Cromwell outputs to the final upload directory.\n    \"\"\"\n    sample_key = [k for k in metadata[\"outputs\"].keys() if k.endswith((\"rgnames__sample\", \"rgnames__sample_out\"))][0]\n    project_dir = utils.safe_makedir(os.path.join(final_dir, \"project\"))\n    samples = metadata[\"outputs\"][sample_key]\n    def _copy_with_secondary(f, dirname):\n        if len(f[\"secondaryFiles\"]) > 1:\n            dirname = utils.safe_makedir(os.path.join(dirname, os.path.basename(os.path.dirname(f[\"location\"]))))\n        if not objectstore.is_remote(f[\"location\"]):\n            finalf = os.path.join(dirname, os.path.basename(f[\"location\"]))\n            if not utils.file_uptodate(finalf, f[\"location\"]):\n                shutil.copy(f[\"location\"], dirname)\n        [_copy_with_secondary(sf, dirname) for sf in f[\"secondaryFiles\"]]\n    def _write_to_dir(val, dirname):\n        if isinstance(val, (list, tuple)):\n            [_write_to_dir(v, dirname) for v in val]\n        else:\n            _copy_with_secondary(val, dirname)\n    for k, vals in metadata[\"outputs\"].items():\n        if k != sample_key:\n            if k.endswith((\"summary__multiqc\")):\n                vs = [v for v in vals if v]\n                assert len(vs) == 1\n                _write_to_dir(vs[0], project_dir)\n            elif len(vals) == len(samples):\n                for s, v in zip(samples, vals):\n                    if v:\n                        _write_to_dir(v, utils.safe_makedir(os.path.join(final_dir, s)))\n            elif len(vals) == 1:\n                _write_to_dir(vals[0], project_dir)\n            elif len(vals) > 0:\n                raise ValueError(\"Unexpected sample and outputs: %s %s %s\" % (k, samples, vals))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _run_sbgenomics(args):\n    assert not args.no_container, \"Seven Bridges runs require containers\"\n    main_file, json_file, project_name = _get_main_and_json(args.directory)\n    flags = []\n    cmd = [\"sbg-cwl-runner\"] + flags + args.toolargs + [main_file, json_file]\n    _run_tool(cmd)", "response": "Run CWL on SevenBridges platform and Cancer Genomics Cloud."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _run_funnel(args):\n    host = \"localhost\"\n    port = \"8088\"\n    main_file, json_file, project_name = _get_main_and_json(args.directory)\n    work_dir = utils.safe_makedir(os.path.join(os.getcwd(), \"funnel_work\"))\n    log_file = os.path.join(work_dir, \"%s-funnel.log\" % project_name)\n    # Create bunny configuration directory with TES backend\n    orig_config_dir = os.path.join(os.path.dirname(os.path.realpath(utils.which(\"rabix\"))), \"config\")\n    work_config_dir = utils.safe_makedir(os.path.join(work_dir, \"rabix_config\"))\n    for fname in os.listdir(orig_config_dir):\n        if fname == \"core.properties\":\n            with open(os.path.join(orig_config_dir, fname)) as in_handle:\n                with open(os.path.join(work_config_dir, fname), \"w\") as out_handle:\n                    for line in in_handle:\n                        if line.startswith(\"backend.embedded.types\"):\n                            line = \"backend.embedded.types=TES\\n\"\n                        out_handle.write(line)\n        else:\n            shutil.copy(os.path.join(orig_config_dir, fname), os.path.join(work_config_dir, fname))\n    flags = [\"-c\", work_config_dir,\n             \"-tes-url=http://%s:%s\" % (host, port), \"-tes-storage=%s\" % work_dir]\n    if args.no_container:\n        _remove_bcbiovm_path()\n        flags += [\"--no-container\"]\n    cmd = [\"rabix\"] + flags + [main_file, json_file]\n    funnelp = subprocess.Popen([\"funnel\", \"server\", \"run\",\n                                \"--Server.HostName\", host, \"--Server.HTTPPort\", port,\n                                \"--LocalStorage.AllowedDirs\", work_dir,\n                                \"--Worker.WorkDir\", os.path.join(work_dir, \"funnel-work\")])\n    try:\n        with utils.chdir(work_dir):\n            _run_tool(cmd, not args.no_container, work_dir, log_file)\n    finally:\n        funnelp.kill()", "response": "Run funnel TES server with rabix bunny for CWL."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun qualimap to assess alignment quality metrics.", "response": "def run(bam_file, data, out_dir):\n    \"\"\"Run qualimap to assess alignment quality metrics.\n    \"\"\"\n    # Qualimap results should be saved to a directory named after sample.\n    # MultiQC (for parsing additional data) picks the sample name after the dir as follows:\n    #   <sample name>/raw_data_qualimapReport/insert_size_histogram.txt\n    results_dir = os.path.join(out_dir, dd.get_sample_name(data))\n    resources = config_utils.get_resources(\"qualimap\", data[\"config\"])\n    options = \" \".join(resources.get(\"options\", \"\"))\n    results_file = os.path.join(results_dir, \"genome_results.txt\")\n    report_file = os.path.join(results_dir, \"qualimapReport.html\")\n    utils.safe_makedir(results_dir)\n    pdf_file = \"qualimapReport.pdf\"\n    if not utils.file_exists(results_file) and not utils.file_exists(os.path.join(results_dir, pdf_file)):\n        if \"qualimap_full\" in tz.get_in((\"config\", \"algorithm\", \"tools_on\"), data, []):\n            logger.info(\"Full qualimap analysis for %s may be slow.\" % bam_file)\n            ds_bam = bam_file\n        else:\n            ds_bam = bam.downsample(bam_file, data, 1e7, work_dir=out_dir)\n            bam_file = ds_bam if ds_bam else bam_file\n        if options.find(\"PDF\") > -1:\n            options = \"%s -outfile %s\" % (options, pdf_file)\n        num_cores = data[\"config\"][\"algorithm\"].get(\"num_cores\", 1)\n        qualimap = config_utils.get_program(\"qualimap\", data[\"config\"])\n        max_mem = config_utils.adjust_memory(resources.get(\"memory\", \"1G\"),\n                                             num_cores)\n\n        with file_transaction(data, results_dir) as tx_results_dir:\n            utils.safe_makedir(tx_results_dir)\n\n            export = \"%s%s export JAVA_OPTS='-Xms32m -Xmx%s -Djava.io.tmpdir=%s' && \" % (\n                utils.java_freetype_fix(), utils.local_path_export(), max_mem, tx_results_dir)\n            cmd = (\"unset DISPLAY && {export} {qualimap} bamqc -bam {bam_file} -outdir {tx_results_dir} \"\n                   \"--skip-duplicated --skip-dup-mode 0 \"\n                   \"-nt {num_cores} {options}\")\n            species = None\n            if (tz.get_in((\"genome_resources\", \"aliases\", \"human\"), data, \"\")\n                  or dd.get_genome_build(data).startswith((\"hg\", \"GRCh\"))):\n                species = \"HUMAN\"\n            elif dd.get_genome_build(data).startswith((\"mm\", \"GRCm\")):\n                species = \"MOUSE\"\n            if species in [\"HUMAN\", \"MOUSE\"]:\n                cmd += \" -gd {species}\"\n            regions = (dd.get_coverage(data) if dd.get_coverage(data) not in [None, False, \"None\"]\n                       else dd.get_variant_regions_merged(data))\n            if regions:\n                regions = bedutils.merge_overlaps(bedutils.clean_file(regions, data), data)\n                bed6_regions = _bed_to_bed6(regions, out_dir)\n                cmd += \" -gff {bed6_regions}\"\n            bcbio_env = utils.get_bcbio_env()\n            do.run(cmd.format(**locals()), \"Qualimap: %s\" % dd.get_sample_name(data), env=bcbio_env)\n            tx_results_file = os.path.join(tx_results_dir, \"genome_results.txt\")\n            cmd = \"sed -i 's/bam file = .*/bam file = %s.bam/' %s\" % (dd.get_sample_name(data), tx_results_file)\n            do.run(cmd, \"Fix Name Qualimap for {}\".format(dd.get_sample_name(data)))\n    # Qualimap output folder (results_dir) needs to be named after the sample (see comments above). However, in order\n    # to keep its name after upload, we need to put  the base QC file (results_file) into the root directory (out_dir):\n    base_results_file = os.path.join(out_dir, os.path.basename(results_file))\n    shutil.copyfile(results_file, base_results_file)\n    return {\"base\": base_results_file,\n            \"secondary\": _find_qualimap_secondary_files(results_dir, base_results_file)}"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract useful metrics from the qualimap HTML report file.", "response": "def _parse_qualimap_metrics(report_file, data):\n    \"\"\"Extract useful metrics from the qualimap HTML report file.\n    \"\"\"\n    if not utils.file_exists(report_file):\n        return {}\n    from bs4 import BeautifulSoup\n    out = {}\n    parsers = {\"Globals\": _parse_qualimap_globals,\n               \"Globals (inside of regions)\": _parse_qualimap_globals_inregion,\n               \"Coverage\": _parse_qualimap_coverage,\n               \"Coverage (inside of regions)\": _parse_qualimap_coverage,\n               \"Insert size\": _parse_qualimap_insertsize,\n               \"Insert size (inside of regions)\": _parse_qualimap_insertsize}\n    with open(report_file) as in_handle:\n        root = BeautifulSoup(in_handle.read(), \"html.parser\")\n    for table in root.find_all(\"div\", class_=\"table-summary\"):\n        h3 = table.find(\"h3\")\n        if h3.text in parsers:\n            out.update(parsers[h3.text](table.find(\"table\")))\n    new_names = []\n    for metric in out:\n        if \"qualimap_full\" not in tz.get_in((\"config\", \"algorithm\", \"tools_on\"), data, []):\n            metric += \"_qualimap_1e7reads_est\"\n        new_names.append(metric)\n    out = dict(zip(new_names, out.values()))\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_qualimap_globals(table):\n    out = {}\n    want = {\"Mapped reads\": _parse_num_pct,\n            \"Duplication rate\": lambda k, v: {k: v}}\n    for row in table.find_all(\"tr\"):\n        col, val = [x.text for x in row.find_all(\"td\")]\n        if col in want:\n            out.update(want[col](col, val))\n    return out", "response": "Retrieve metrics of interest from globals table.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_qualimap_globals_inregion(table):\n    out = {}\n    for row in table.find_all(\"tr\"):\n        col, val = [x.text for x in row.find_all(\"td\")]\n        if col == \"Mapped reads\":\n            out.update(_parse_num_pct(\"%s (in regions)\" % col, val))\n    return out", "response": "Retrieve metrics from the global targeted region table."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_qualimap_coverage(table):\n    out = {}\n    for row in table.find_all(\"tr\"):\n        col, val = [x.text for x in row.find_all(\"td\")]\n        if col == \"Mean\":\n            out[\"Coverage (Mean)\"] = val\n    return out", "response": "Parse summary qualimap coverage metrics.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts BED file to required bed6 inputs.", "response": "def _bed_to_bed6(orig_file, out_dir):\n    \"\"\"Convert bed to required bed6 inputs.\n    \"\"\"\n    bed6_file = os.path.join(out_dir, \"%s-bed6%s\" % os.path.splitext(os.path.basename(orig_file)))\n    if not utils.file_exists(bed6_file):\n        with open(bed6_file, \"w\") as out_handle:\n            for i, region in enumerate(list(x) for x in pybedtools.BedTool(orig_file)):\n                region = [x for x in list(region) if x]\n                fillers = [str(i), \"1.0\", \"+\"]\n                full = region + fillers[:6 - len(region)]\n                out_handle.write(\"\\t\".join(full) + \"\\n\")\n    return bed6_file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetect duplicate entries in a single BAM file.", "response": "def _detect_duplicates(bam_file, out_dir, data):\n    \"\"\"\n    count duplicate percentage\n    \"\"\"\n    out_file = os.path.join(out_dir, \"dup_metrics.txt\")\n    if not utils.file_exists(out_file):\n        dup_align_bam = postalign.dedup_bam(bam_file, data)\n        logger.info(\"Detecting duplicates in %s.\" % dup_align_bam)\n        dup_count = readstats.number_of_mapped_reads(data, dup_align_bam, keep_dups=False)\n        tot_count = readstats.number_of_mapped_reads(data, dup_align_bam, keep_dups=True)\n        with file_transaction(data, out_file) as tx_out_file:\n            with open(tx_out_file, \"w\") as out_handle:\n                out_handle.write(\"%s\\n%s\\n\" % (dup_count, tot_count))\n    with open(out_file) as in_handle:\n        dupes = float(next(in_handle).strip())\n        total = float(next(in_handle).strip())\n    if total == 0:\n        rate = \"NA\"\n    else:\n        rate = dupes / total\n    return {\"Duplication Rate of Mapped\": rate}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntransforming interval format to browser coord", "response": "def _transform_browser_coor(rRNA_interval, rRNA_coor):\n    \"\"\"\n    transform interval format to browser coord: chr:start-end\n    \"\"\"\n    with open(rRNA_coor, 'w') as out_handle:\n        with open(rRNA_interval, 'r') as in_handle:\n            for line in in_handle:\n                c, bio, source, s, e = line.split(\"\\t\")[:5]\n                if bio.startswith(\"rRNA\"):\n                    out_handle.write((\"{0}:{1}-{2}\\n\").format(c, s, e))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_qualimap_rnaseq(table):\n    out = {}\n    for row in table.find_all(\"tr\"):\n        col, val = [x.text for x in row.find_all(\"td\")]\n        col = col.replace(\":\", \"\").strip()\n        val = val.replace(\",\", \"\")\n        m = {col: val}\n        if val.find(\"/\") > -1:\n            m = _parse_num_pct(col, val.replace(\"%\", \"\"))\n        out.update(m)\n    return out", "response": "Parse the qualimap RNA - QUALMAP table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_rnaseq_qualimap_metrics(report_file):\n    from bs4 import BeautifulSoup\n    out = {}\n    parsers = [\"Reads alignment\", \"Reads genomic origin\", \"Transcript coverage profile\"]\n    with open(report_file) as in_handle:\n        root = BeautifulSoup(in_handle.read(), \"html.parser\")\n    for table in root.find_all(\"div\", class_=\"table-summary\"):\n        h3 = table.find(\"h3\")\n        if h3.text in parsers:\n            out.update(_parse_qualimap_rnaseq(table.find(\"table\")))\n    return out", "response": "Extract useful metrics from the qualimap HTML report file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_rnaseq(bam_file, data, out_dir):\n    strandedness = {\"firststrand\": \"strand-specific-reverse\",\n                    \"secondstrand\": \"strand-specific-forward\",\n                    \"unstranded\": \"non-strand-specific\"}\n\n    # Qualimap results should be saved to a directory named after sample.\n    # MultiQC (for parsing additional data) picks the sample name after the dir as follows:\n    #   <sample name>/raw_data_qualimapReport/insert_size_histogram.txt\n    results_dir = os.path.join(out_dir, dd.get_sample_name(data))\n    results_file = os.path.join(results_dir, \"rnaseq_qc_results.txt\")\n    report_file = os.path.join(results_dir, \"qualimapReport.html\")\n    config = data[\"config\"]\n    gtf_file = dd.get_gtf_file(data)\n    library = strandedness[dd.get_strandedness(data)]\n    if not utils.file_exists(results_file):\n        with file_transaction(data, results_dir) as tx_results_dir:\n            utils.safe_makedir(tx_results_dir)\n            bam.index(bam_file, config)\n            cmd = _rnaseq_qualimap_cmd(data, bam_file, tx_results_dir, gtf_file, library)\n            do.run(cmd, \"Qualimap for {}\".format(dd.get_sample_name(data)))\n            tx_results_file = os.path.join(tx_results_dir, \"rnaseq_qc_results.txt\")\n            cmd = \"sed -i 's/bam file = .*/bam file = %s.bam/' %s\" % (dd.get_sample_name(data), tx_results_file)\n            do.run(cmd, \"Fix Name Qualimap for {}\".format(dd.get_sample_name(data)))\n    metrics = _parse_rnaseq_qualimap_metrics(report_file)\n    metrics.update(_detect_duplicates(bam_file, results_dir, data))\n    metrics.update(_detect_rRNA(data, results_dir))\n    metrics.update({\"Average_insert_size\": salmon.estimate_fragment_size(data)})\n    metrics = _parse_metrics(metrics)\n    # Qualimap output folder (results_dir) needs to be named after the sample (see comments above). However, in order\n    # to keep its name after upload, we need to put  the base QC file (results_file) into the root directory (out_dir):\n    base_results_file = os.path.join(out_dir, os.path.basename(results_file))\n    shutil.copyfile(results_file, base_results_file)\n    return {\"base\": base_results_file,\n            \"secondary\": _find_qualimap_secondary_files(results_dir, base_results_file),\n            \"metrics\": metrics}", "response": "Run qualimap for a rnaseq bam file and parse results"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating command lines for qualimap using RNA - seq.", "response": "def _rnaseq_qualimap_cmd(data, bam_file, out_dir, gtf_file=None, library=\"non-strand-specific\"):\n    \"\"\"\n    Create command lines for qualimap\n    \"\"\"\n    config = data[\"config\"]\n    qualimap = config_utils.get_program(\"qualimap\", config)\n    resources = config_utils.get_resources(\"qualimap\", config)\n    num_cores = resources.get(\"cores\", dd.get_num_cores(data))\n    max_mem = config_utils.adjust_memory(resources.get(\"memory\", \"2G\"),\n                                         num_cores)\n    export = \"%s%s\" % (utils.java_freetype_fix(), utils.local_path_export())\n    export = \"%s%s export JAVA_OPTS='-Xms32m -Xmx%s -Djava.io.tmpdir=%s' && \" % (\n        utils.java_freetype_fix(), utils.local_path_export(), max_mem, out_dir)\n    paired = \" --paired\" if bam.is_paired(bam_file) else \"\"\n    cmd = (\"unset DISPLAY && {export} {qualimap} rnaseq -outdir {out_dir} \"\n           \"-a proportional -bam {bam_file} -p {library}{paired} \"\n           \"-gtf {gtf_file}\").format(**locals())\n    return cmd"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds secondary files that are not duplicate.", "response": "def _find_qualimap_secondary_files(results_dir, base_file):\n    \"\"\"Retrieve additional files, avoiding double uploading the base file.\n    \"\"\"\n    def not_dup(x):\n        is_dup = (os.path.basename(x) == os.path.basename(base_file) and\n                  os.path.getsize(x) == os.path.getsize(base_file))\n        return not is_dup\n    def is_problem_file(x):\n        \"\"\"Problematic files with characters that make some CWL runners unhappy.\n        \"\"\"\n        return x.find(\"(\") >= 0 or x.find(\")\") >= 0 or x.find(\" \") >= 0\n    return list(filter(lambda x: not is_problem_file(x),\n                       filter(not_dup,\n                              glob.glob(os.path.join(results_dir, 'qualimapReport.html')) +\n                              glob.glob(os.path.join(results_dir, '*.txt')) +\n                              glob.glob(os.path.join(results_dir, \"css\", \"*\")) +\n                              glob.glob(os.path.join(results_dir, \"raw_data_qualimapReport\", \"*\")) +\n                              glob.glob(os.path.join(results_dir, \"images_qualimapReport\", \"*\")))))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncounts the number of duplicate start sites in a file.", "response": "def count_duplicate_starts(bam_file, sample_size=10000000):\n    \"\"\"\n    Return a set of x, y points where x is the number of reads sequenced and\n    y is the number of unique start sites identified\n    If sample size < total reads in a file the file will be downsampled.\n    \"\"\"\n    count = Counter()\n    with bam.open_samfile(bam_file) as samfile:\n        # unmapped reads should not be counted\n        filtered = ifilter(lambda x: not x.is_unmapped, samfile)\n        def read_parser(read):\n            return \":\".join([str(read.tid), str(read.pos)])\n        samples = utils.reservoir_sample(filtered, sample_size, read_parser)\n\n    count.update(samples)\n    return count"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a plot from individual summary csv files with classification metrics.", "response": "def classifyplot_from_plotfiles(plot_files, out_csv, outtype=\"png\", title=None, size=None):\n    \"\"\"Create a plot from individual summary csv files with classification metrics.\n    \"\"\"\n    dfs = [pd.read_csv(x) for x in plot_files]\n    samples = []\n    for df in dfs:\n        for sample in df[\"sample\"].unique():\n            if sample not in samples:\n                samples.append(sample)\n    df = pd.concat(dfs)\n    df.to_csv(out_csv, index=False)\n    return classifyplot_from_valfile(out_csv, outtype, title, size, samples)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef classifyplot_from_valfile(val_file, outtype=\"png\", title=None, size=None,\n                              samples=None, callers=None):\n    \"\"\"Create a plot from a summarized validation file.\n\n    Does new-style plotting of summarized metrics of\n    false negative rate and false discovery rate.\n    https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n    \"\"\"\n    mpl.use('Agg', force=True)\n    df = pd.read_csv(val_file)\n    grouped = df.groupby([\"sample\", \"caller\", \"vtype\"])\n    df = grouped.apply(_calculate_fnr_fdr)\n    df = df.reset_index()\n    if len(df) == 0:\n        return []\n    else:\n        out_file = \"%s.%s\" % (os.path.splitext(val_file)[0], outtype)\n        _do_classifyplot(df, out_file, title, size, samples, callers)\n        return [out_file]", "response": "Create a plot from a summarized validation file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the false negative rate and false discovery rate.", "response": "def _calculate_fnr_fdr(group):\n    \"\"\"Calculate the false negative rate (1 - sensitivity) and false discovery rate (1 - precision).\n    \"\"\"\n    data = {k: d[\"value\"] for k, d in group.set_index(\"metric\").T.to_dict().items()}\n    return pd.DataFrame([{\"fnr\": data[\"fn\"] / float(data[\"tp\"] + data[\"fn\"]) * 100.0 if data[\"tp\"] > 0 else 0.0,\n                          \"fdr\": data[\"fp\"] / float(data[\"tp\"] + data[\"fp\"]) * 100.0 if data[\"tp\"] > 0 else 0.0,\n                          \"tpr\": \"TP: %s FN: %s\" % (data[\"tp\"], data[\"fn\"]),\n                          \"spc\": \"FP: %s\" % (data[\"fp\"])}])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _do_classifyplot(df, out_file, title=None, size=None, samples=None, callers=None):\n    metric_labels = {\"fdr\": \"False discovery rate\",\n                     \"fnr\": \"False negative rate\"}\n    metrics = [(\"fnr\", \"tpr\"), (\"fdr\", \"spc\")]\n    is_mpl2 = LooseVersion(mpl.__version__) >= LooseVersion('2.0')\n    colors = [\"light grey\", \"greyish\"] * 10\n    data_dict = df.set_index([\"sample\", \"caller\", \"vtype\"]).T.to_dict()\n    plt.ioff()\n    plt.style.use('seaborn-white')\n    vtypes = sorted(df[\"vtype\"].unique(), reverse=True)\n    if not callers:\n        callers = sorted(df[\"caller\"].unique())\n    if not samples:\n        samples = sorted(df[\"sample\"].unique())\n    if len(samples) >= len(callers):\n        cats, groups = (samples, callers)\n        data_dict = df.set_index([\"sample\", \"caller\", \"vtype\"]).T.to_dict()\n    else:\n        cats, groups = (callers, samples)\n        data_dict = df.set_index([\"caller\", \"sample\", \"vtype\"]).T.to_dict()\n    fig, axs = plt.subplots(len(vtypes) * len(groups), len(metrics))\n    fig.text(.5, .95, title if title else \"\", horizontalalignment='center', size=14)\n    for vi, vtype in enumerate(vtypes):\n        for gi, group in enumerate(groups):\n            for mi, (metric, label) in enumerate(metrics):\n                row_plots = axs if len(vtypes) * len(groups) == 1 else axs[vi * len(groups) + gi]\n                cur_plot = row_plots if len(metrics) == 1 else row_plots[mi]\n                vals, labels = [], []\n                for cat in cats:\n                    cur_data = data_dict.get((cat, group, vtype))\n                    if cur_data:\n                        vals.append(cur_data[metric])\n                        labels.append(cur_data[label])\n                cur_plot.barh(np.arange(len(vals)), vals, color=sns.xkcd_palette([colors[vi]]))\n                all_vals = []\n                for k, d in data_dict.items():\n                    if k[-1] == vtype:\n                        for m in metrics:\n                            all_vals.append(d[m[0]])\n                metric_max = max(all_vals)\n                cur_plot.set_xlim(0, metric_max)\n                pad = 0.1 * metric_max\n                ai_adjust = 0.0 if is_mpl2 else 0.35\n                for ai, (val, label) in enumerate(zip(vals, labels)):\n                    cur_plot.annotate(label, (pad + (0 if max(vals) > metric_max / 2.0 else max(vals)),\n                                              ai + ai_adjust),\n                                      va='center', size=7)\n                cur_plot.locator_params(nbins=len(cats) + (2 if len(cats) > 2 else 1), axis=\"y\", tight=True)\n                if mi == 0:\n                    cur_plot.tick_params(axis='y', which='major', labelsize=8)\n                    plot_cats = ([\"\"] + cats) if is_mpl2 else cats\n                    plot_va = \"center\" if is_mpl2 else \"bottom\"\n                    cur_plot.set_yticklabels(plot_cats, size=8, va=plot_va)\n                    cur_plot.set_title(\"%s: %s\" % (vtype, group), fontsize=12, loc=\"left\")\n                else:\n                    cur_plot.get_yaxis().set_ticks([])\n                if gi == len(groups) - 1:\n                    cur_plot.tick_params(axis='x', which='major', labelsize=8)\n                    cur_plot.get_xaxis().set_major_formatter(\n                        mpl_ticker.FuncFormatter(lambda v, p: \"%s%%\" % (int(v) if round(v) == v else v)))\n                    if vi == len(vtypes) - 1:\n                        cur_plot.get_xaxis().set_label_text(metric_labels[metric], size=12)\n                else:\n                    cur_plot.get_xaxis().set_ticks([])\n                    cur_plot.spines['bottom'].set_visible(False)\n                cur_plot.spines['left'].set_visible(False)\n                cur_plot.spines['top'].set_visible(False)\n                cur_plot.spines['right'].set_visible(False)\n    x, y = (6, len(vtypes) * len(groups) + 1 * 0.5 * len(cats)) if size is None else size\n    fig.set_size_inches(x, y)\n    fig.tight_layout(rect=(0, 0, 1, 0.95))\n    plt.subplots_adjust(hspace=0.6)\n    fig.savefig(out_file)", "response": "Plot using classification - based plot using seaborn."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a series of validation results for a sample.", "response": "def create(plot_data, header, ploti, sample_config, out_file_base, outtype=\"png\",\n           title=None, size=None):\n    \"\"\"Create plots of validation results for a sample, labeling prep strategies.\n    \"\"\"\n    if mpl is None or plt is None or sns is None:\n        not_found = \", \".join([x for x in ['mpl', 'plt', 'sns'] if eval(x) is None])\n        logger.info(\"No validation plot. Missing imports: %s\" % not_found)\n        return None\n    mpl.use('Agg', force=True)\n\n    if header:\n        df = pd.DataFrame(plot_data, columns=header)\n    else:\n        df = plot_data\n    df[\"aligner\"] = [get_aligner(x, sample_config) for x in df[\"sample\"]]\n    df[\"bamprep\"] = [get_bamprep(x, sample_config) for x in df[\"sample\"]]\n    floors = get_group_floors(df, cat_labels)\n    df[\"value.floor\"] = [get_floor_value(x, cat, vartype, floors)\n                         for (x, cat, vartype) in zip(df[\"value\"], df[\"category\"], df[\"variant.type\"])]\n    out = []\n    for i, prep in enumerate(df[\"bamprep\"].unique()):\n        out.append(plot_prep_methods(df, prep, i + ploti, out_file_base, outtype, title, size))\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_prep_methods(df, prep, prepi, out_file_base, outtype, title=None,\n                      size=None):\n    \"\"\"Plot comparison between BAM preparation methods.\n    \"\"\"\n    samples = df[(df[\"bamprep\"] == prep)][\"sample\"].unique()\n    assert len(samples) >= 1, samples\n    out_file = \"%s-%s.%s\" % (out_file_base, samples[0], outtype)\n    df = df[df[\"category\"].isin(cat_labels)]\n    _seaborn(df, prep, prepi, out_file, title, size)\n    return out_file", "response": "Plot comparison between BAM preparation methods."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplots using seaborn wrapper around matplotlib. Seaborn.", "response": "def _seaborn(df, prep, prepi, out_file, title=None, size=None):\n    \"\"\"Plot using seaborn wrapper around matplotlib.\n    \"\"\"\n    plt.ioff()\n    sns.set(style='dark')\n    vtypes = df[\"variant.type\"].unique()\n    callers = sorted(df[\"caller\"].unique())\n    cats = _check_cats([\"concordant\", \"discordant-missing-total\",\n                        \"discordant-extra-total\", \"discordant-shared-total\"],\n                       vtypes, df, prep, callers)\n    fig, axs = plt.subplots(len(vtypes), len(cats))\n    width = 0.8\n    for i, vtype in enumerate(vtypes):\n        ax_row = axs[i] if len(vtypes) > 1 else axs\n        for j, cat in enumerate(cats):\n            vals, labels, maxval = _get_chart_info(df, vtype, cat, prep, callers)\n            if len(cats) == 1:\n                assert j == 0\n                ax = ax_row\n            else:\n                ax = ax_row[j]\n            if i == 0:\n                ax.set_title(cat_labels[cat], size=14)\n            ax.get_yaxis().set_ticks([])\n            if j == 0:\n                ax.set_ylabel(vtype_labels[vtype], size=14)\n            ax.bar(np.arange(len(callers)), vals, width=width)\n            ax.set_ylim(0, maxval)\n            if i == len(vtypes) - 1:\n                ax.set_xticks(np.arange(len(callers)) + width / 2.0)\n                ax.set_xticklabels([caller_labels.get(x, x).replace(\"__\", \"\\n\") if x else \"\"\n                                    for x in callers], size=8, rotation=45)\n            else:\n                ax.get_xaxis().set_ticks([])\n            _annotate(ax, labels, vals, np.arange(len(callers)), width)\n    fig.text(.5, .95, prep_labels.get(prep, \"\") if title is None else title, horizontalalignment='center', size=16)\n    fig.subplots_adjust(left=0.05, right=0.95, top=0.87, bottom=0.15, wspace=0.1, hspace=0.1)\n    x, y = (10, 5) if size is None else size\n    fig.set_size_inches(x, y)\n    fig.savefig(out_file)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_chart_info(df, vtype, cat, prep, callers):\n    maxval_raw = max(list(df[\"value.floor\"]))\n    curdf = df[(df[\"variant.type\"] == vtype) & (df[\"category\"] == cat)\n               & (df[\"bamprep\"] == prep)]\n    vals = []\n    labels = []\n    for c in callers:\n        row = curdf[df[\"caller\"] == c]\n        if len(row) > 0:\n            vals.append(list(row[\"value.floor\"])[0])\n            labels.append(list(row[\"value\"])[0])\n        else:\n            vals.append(1)\n            labels.append(\"\")\n    return vals, labels, maxval_raw", "response": "Retrieve values for a specific variant type category and prep method."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _annotate(ax, annotate, height, left, width):\n    annotate_yrange_factor = 0.010\n    xticks = np.array(left) + width / 2.0\n    ymin, ymax = ax.get_ylim()\n    yrange = ymax - ymin\n\n    # Reset ymax and ymin so there's enough room to see the annotation of\n    # the top-most\n    if ymax > 0:\n        ymax += yrange * 0.15\n    if ymin < 0:\n        ymin -= yrange * 0.15\n    ax.set_ylim(ymin, ymax)\n    yrange = ymax - ymin\n\n    offset_ = yrange * annotate_yrange_factor\n    if isinstance(annotate, collections.Iterable):\n        annotations = map(str, annotate)\n    else:\n        annotations = ['%.3f' % h if type(h) is np.float_ else str(h)\n                       for h in height]\n    for x, h, annotation in zip(xticks, height, annotations):\n        # Adjust the offset to account for negative bars\n        offset = offset_ if h >= 0 else -1 * offset_\n        verticalalignment = 'bottom' if h >= 0 else 'top'\n\n        if len(str(annotation)) > 6:\n            size = 7\n        elif len(str(annotation)) > 5:\n            size = 8\n        else:\n            size = 10\n        # Finally, add the text to the axes\n        ax.annotate(annotation, (x, h + offset),\n                    verticalalignment=verticalalignment,\n                    horizontalalignment='center',\n                    size=size)", "response": "Annotate the axes with the specified labels."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot faceted items with ggplot wrapper on top of matplotlib. XXX Not yet functional", "response": "def _ggplot(df, out_file):\n    \"\"\"Plot faceted items with ggplot wrapper on top of matplotlib.\n    XXX Not yet functional\n    \"\"\"\n    import ggplot as gg\n    df[\"variant.type\"] = [vtype_labels[x] for x in df[\"variant.type\"]]\n    df[\"category\"] = [cat_labels[x] for x in df[\"category\"]]\n    df[\"caller\"] = [caller_labels.get(x, None) for x in df[\"caller\"]]\n    p = (gg.ggplot(df, gg.aes(x=\"caller\", y=\"value.floor\")) + gg.geom_bar()\n         + gg.facet_wrap(\"variant.type\", \"category\")\n         + gg.theme_seaborn())\n    gg.ggsave(p, out_file)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmodifying values so all have the same relative scale for differences.", "response": "def get_floor_value(x, cat, vartype, floors):\n    \"\"\"Modify values so all have the same relative scale for differences.\n\n    Using the chosen base heights, adjusts an individual sub-plot to be consistent\n    relative to that height.\n    \"\"\"\n    all_base = floors[vartype]\n    cur_max = floors[(cat, vartype)]\n    if cur_max > all_base:\n        diff = cur_max - all_base\n        x = max(1, x - diff)\n    return x"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the floor for a given row of comparisons creating a normalized set of differences.", "response": "def get_group_floors(df, cat_labels):\n    \"\"\"Retrieve the floor for a given row of comparisons, creating a normalized set of differences.\n\n    We need to set non-zero floors so large numbers (like concordance) don't drown out small\n    numbers (like discordance). This defines the height for a row of comparisons as either\n    the minimum height of any sub-plot, or the maximum difference between higher and lower\n    (plus 10%).\n    \"\"\"\n    group_maxes = collections.defaultdict(list)\n    group_diffs = collections.defaultdict(list)\n    diff_pad = 0.1  # 10% padding onto difference to avoid large numbers looking like zero\n    for name, group in df.groupby([\"category\", \"variant.type\"]):\n        label, stype = name\n        if label in cat_labels:\n            diff = max(group[\"value\"]) - min(group[\"value\"])\n            group_diffs[stype].append(diff + int(diff_pad * diff))\n            group_maxes[stype].append(max(group[\"value\"]))\n        group_maxes[name].append(max(group[\"value\"]))\n    out = {}\n    for k, vs in group_maxes.items():\n        if k in group_diffs:\n            out[k] = max(max(group_diffs[stype]), min(vs))\n        else:\n            out[k] = min(vs)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef facet_freq_plot(freq_csv, caller):\n    out_file = \"%s.png\" % os.path.splitext(freq_csv)[0]\n    plt.ioff()\n    sns.set(style='dark')\n    df = pd.read_csv(freq_csv)\n    g = sns.FacetGrid(df, row=\"vtype\", col=\"valclass\", margin_titles=True,\n                      col_order=[\"TP\", \"FN\", \"FP\"], row_order=[\"snp\", \"indel\"],\n                      sharey=False)\n    g.map(plt.hist, \"freq\", bins=20, align=\"left\")\n    g.set(xlim=(0.0, 1.0))\n    g.fig.set_size_inches(8, 6)\n    g.fig.text(.05, .97, caller, horizontalalignment='center', size=14)\n    g.fig.savefig(out_file)", "response": "Prepare a facet plot of frequencies stratified by variant type and status."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse commandline arguments handling multiple cases.", "response": "def parse_cl_args(in_args):\n    \"\"\"Parse input commandline arguments, handling multiple cases.\n\n    Returns the main config file and set of kwargs.\n    \"\"\"\n    sub_cmds = {\"upgrade\": install.add_subparser,\n                \"runfn\": runfn.add_subparser,\n                \"graph\": graph.add_subparser,\n                \"version\": programs.add_subparser,\n                \"sequencer\": machine.add_subparser}\n    description = \"Community developed high throughput sequencing analysis.\"\n    parser = argparse.ArgumentParser(description=description)\n    sub_cmd = None\n    if len(in_args) > 0 and in_args[0] in sub_cmds:\n        subparser_help = \"bcbio-nextgen supplemental commands\"\n        subparsers = parser.add_subparsers(help=subparser_help)\n        sub_cmds[in_args[0]](subparsers)\n        sub_cmd = in_args[0]\n    else:\n        parser.add_argument(\"global_config\", nargs=\"?\",\n                            help=(\"Global YAML configuration file specifying \"\n                                  \"details about the system (optional, \"\n                                  \"defaults to installed bcbio_system.yaml)\"))\n        parser.add_argument(\"fc_dir\", nargs=\"?\",\n                            help=(\"A directory of Illumina output or fastq \"\n                                  \"files to process (optional)\"))\n        parser.add_argument(\"run_config\", nargs=\"*\",\n                            help=(\"YAML file with details about samples to \"\n                                  \"process (required, unless using Galaxy \"\n                                  \"LIMS as input)\")),\n        parser.add_argument(\"-n\", \"--numcores\", type=int, default=1,\n                            help=\"Total cores to use for processing\")\n        parser.add_argument(\"-t\", \"--paralleltype\",\n                            choices=[\"local\", \"ipython\"],\n                            default=\"local\", help=\"Approach to parallelization\")\n        parser.add_argument(\"-s\", \"--scheduler\",\n                            choices=[\"lsf\", \"sge\", \"torque\", \"slurm\", \"pbspro\"],\n                            help=\"Scheduler to use for ipython parallel\")\n        parser.add_argument(\"--local_controller\",\n                            default=False,\n                            action=\"store_true\",\n                            help=\"run controller locally\")\n        parser.add_argument(\"-q\", \"--queue\",\n                            help=(\"Scheduler queue to run jobs on, for \"\n                                  \"ipython parallel\"))\n        parser.add_argument(\"-r\", \"--resources\",\n                            help=(\"Cluster specific resources specifications. \"\n                                  \"Can be specified multiple times.\\n\"\n                                  \"Supports SGE, Torque, LSF and SLURM \"\n                                  \"parameters.\"), default=[], action=\"append\")\n        parser.add_argument(\"--timeout\", default=15, type=int,\n                            help=(\"Number of minutes before cluster startup \"\n                                  \"times out. Defaults to 15\"))\n        parser.add_argument(\"--retries\", default=0, type=int,\n                            help=(\"Number of retries of failed tasks during \"\n                                  \"distributed processing. Default 0 \"\n                                  \"(no retries)\"))\n        parser.add_argument(\"-p\", \"--tag\",\n                            help=\"Tag name to label jobs on the cluster\",\n                            default=\"\")\n        parser.add_argument(\"-w\", \"--workflow\",\n                            help=(\"Run a workflow with the given commandline \"\n                                  \"arguments\"))\n        parser.add_argument(\"--workdir\", default=os.getcwd(),\n                            help=(\"Directory to process in. Defaults to \"\n                                  \"current working directory\"))\n        parser.add_argument(\"-v\", \"--version\", help=\"Print current version\",\n                            action=\"store_true\")\n        # Hidden arguments passed downstream\n        parser.add_argument(\"--only-metadata\", help=argparse.SUPPRESS, action=\"store_true\", default=False)\n        parser.add_argument(\"--force-single\", help=\"Treat all files as single reads\",\n                            action=\"store_true\", default=False)\n        parser.add_argument(\"--separators\", help=\"semicolon separated list of separators that indicates paired files.\",\n                            default=\"R,_,-,.\")\n    args = parser.parse_args(in_args)\n    if hasattr(args, \"workdir\") and args.workdir:\n        args.workdir = utils.safe_makedir(os.path.abspath(args.workdir))\n    if hasattr(args, \"global_config\"):\n        error_msg = _sanity_check_args(args)\n        if error_msg:\n            parser.error(error_msg)\n        kwargs = {\"parallel\": clargs.to_parallel(args),\n                  \"workflow\": args.workflow,\n                  \"workdir\": args.workdir}\n        kwargs = _add_inputs_to_kwargs(args, kwargs, parser)\n        error_msg = _sanity_check_kwargs(kwargs)\n        if error_msg:\n            parser.error(error_msg)\n    else:\n        assert sub_cmd is not None\n        kwargs = {\"args\": args,\n                  \"config_file\": None,\n                  sub_cmd: True}\n    return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nensure dependent arguments are correctly specified", "response": "def _sanity_check_args(args):\n    \"\"\"Ensure dependent arguments are correctly specified\n    \"\"\"\n    if \"scheduler\" in args and \"queue\" in args:\n        if args.scheduler and not args.queue:\n            if args.scheduler != \"sge\":\n                return \"IPython parallel scheduler (-s) specified. This also requires a queue (-q).\"\n        elif args.queue and not args.scheduler:\n            return \"IPython parallel queue (-q) supplied. This also requires a scheduler (-s).\"\n        elif args.paralleltype == \"ipython\" and (not args.queue or not args.scheduler):\n            return \"IPython parallel requires queue (-q) and scheduler (-s) arguments.\""}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _add_inputs_to_kwargs(args, kwargs, parser):\n    inputs = [x for x in [args.global_config, args.fc_dir] + args.run_config\n              if x is not None]\n    global_config = \"bcbio_system.yaml\"  # default configuration if not specified\n    if kwargs.get(\"workflow\", \"\") == \"template\":\n        if args.only_metadata:\n            inputs.append(\"--only-metadata\")\n        if args.force_single:\n            inputs.append(\"--force-single\")\n        if args.separators:\n            inputs.extend([\"--separators\", args.separators])\n        kwargs[\"inputs\"] = inputs\n        return kwargs\n    elif len(inputs) == 1:\n        if os.path.isfile(inputs[0]):\n            fc_dir = None\n            run_info_yaml = inputs[0]\n        else:\n            fc_dir = inputs[0]\n            run_info_yaml = None\n    elif len(inputs) == 2:\n        if os.path.isfile(inputs[0]):\n            global_config = inputs[0]\n            if os.path.isfile(inputs[1]):\n                fc_dir = None\n                run_info_yaml = inputs[1]\n            else:\n                fc_dir = inputs[1]\n                run_info_yaml = None\n        else:\n            fc_dir, run_info_yaml = inputs\n    elif len(inputs) == 3:\n        global_config, fc_dir, run_info_yaml = inputs\n    elif args.version:\n        print(version.__version__)\n        sys.exit()\n    else:\n        print(\"Incorrect input arguments\", inputs)\n        parser.print_help()\n        sys.exit()\n    if fc_dir:\n        fc_dir = os.path.abspath(fc_dir)\n    if run_info_yaml:\n        run_info_yaml = os.path.abspath(run_info_yaml)\n    if kwargs.get(\"workflow\"):\n        kwargs[\"inputs\"] = inputs\n    kwargs[\"config_file\"] = global_config\n    kwargs[\"fc_dir\"] = fc_dir\n    kwargs[\"run_info_yaml\"] = run_info_yaml\n    return kwargs", "response": "Convert input system config flow cell directory and sample yaml to kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding commas to output counts.", "response": "def _add_commas(s, sep=','):\n    \"\"\"Add commas to output counts.\n\n    From: http://code.activestate.com/recipes/498181\n    \"\"\"\n    if len(s) <= 3:\n        return s\n\n    return _add_commas(s[:-3], sep) + sep + s[-3:]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert BED file to interval format.", "response": "def bed_to_interval(orig_bed, bam_file):\n    \"\"\"Add header and format BED bait and target files for Picard if necessary.\n    \"\"\"\n    with open(orig_bed) as in_handle:\n        line = in_handle.readline()\n    if line.startswith(\"@\"):\n        yield orig_bed\n    else:\n        with pysam.Samfile(bam_file, \"rb\") as bam_handle:\n            header = bam_handle.text\n        with tmpfile(dir=os.path.dirname(orig_bed), prefix=\"picardbed\") as tmp_bed:\n            with open(tmp_bed, \"w\") as out_handle:\n                out_handle.write(header)\n                with open(orig_bed) as in_handle:\n                    for i, line in enumerate(in_handle):\n                        parts = line.rstrip().split(\"\\t\")\n                        if len(parts) == 4:\n                            chrom, start, end, name = parts\n                            strand = \"+\"\n                        elif len(parts) >= 3:\n                            chrom, start, end = parts[:3]\n                            strand = \"+\"\n                            name = \"r%s\" % i\n                        out = [chrom, start, end, strand, name]\n                        out_handle.write(\"\\t\".join(out) + \"\\n\")\n            yield tmp_bed"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve a high level summary of interesting metrics.", "response": "def get_summary_metrics(self, align_metrics, dup_metrics,\n            insert_metrics=None, hybrid_metrics=None, vrn_vals=None,\n            rnaseq_metrics=None):\n        \"\"\"Retrieve a high level summary of interesting metrics.\n        \"\"\"\n        with open(align_metrics) as in_handle:\n            align_vals = self._parse_align_metrics(in_handle)\n        if dup_metrics:\n            with open(dup_metrics) as in_handle:\n                dup_vals = self._parse_dup_metrics(in_handle)\n        else:\n            dup_vals = {}\n        (insert_vals, hybrid_vals, rnaseq_vals) = (None, None, None)\n        if insert_metrics and file_exists(insert_metrics):\n            with open(insert_metrics) as in_handle:\n                insert_vals = self._parse_insert_metrics(in_handle)\n        if hybrid_metrics and file_exists(hybrid_metrics):\n            with open(hybrid_metrics) as in_handle:\n                hybrid_vals = self._parse_hybrid_metrics(in_handle)\n        if rnaseq_metrics and file_exists(rnaseq_metrics):\n            with open(rnaseq_metrics) as in_handle:\n                rnaseq_vals = self._parse_rnaseq_metrics(in_handle)\n\n        return self._tabularize_metrics(align_vals, dup_vals, insert_vals,\n                hybrid_vals, vrn_vals, rnaseq_vals)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_metrics(self, metrics_files):\n        extension_maps = dict(\n            align_metrics=(self._parse_align_metrics, \"AL\"),\n            dup_metrics=(self._parse_dup_metrics, \"DUP\"),\n            hs_metrics=(self._parse_hybrid_metrics, \"HS\"),\n            insert_metrics=(self._parse_insert_metrics, \"INS\"),\n            rnaseq_metrics=(self._parse_rnaseq_metrics, \"RNA\"))\n        all_metrics = dict()\n        for fname in metrics_files:\n            ext = os.path.splitext(fname)[-1][1:]\n            try:\n                parse_fn, prefix = extension_maps[ext]\n            except KeyError:\n                parse_fn = None\n            if parse_fn:\n                with open(fname) as in_handle:\n                    for key, val in parse_fn(in_handle).items():\n                        if not key.startswith(prefix):\n                            key = \"%s_%s\" % (prefix, key)\n                        all_metrics[key] = val\n        return all_metrics", "response": "Extract summary information for a lane of metrics files."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nproducing report metrics using Picard with sorted aligned BAM file.", "response": "def report(self, align_bam, ref_file, is_paired, bait_file, target_file,\n               variant_region_file, config):\n        \"\"\"Produce report metrics using Picard with sorted aligned BAM file.\n        \"\"\"\n        dup_metrics = self._get_current_dup_metrics(align_bam)\n        align_metrics = self._collect_align_metrics(align_bam, ref_file)\n        # Prefer the GC metrics in FastQC instead of Picard\n        # gc_graph, gc_metrics = self._gc_bias(align_bam, ref_file)\n        gc_graph = None\n        insert_graph, insert_metrics, hybrid_metrics = (None, None, None)\n        if is_paired:\n            insert_graph, insert_metrics = self._insert_sizes(align_bam)\n        if bait_file and target_file:\n            assert os.path.exists(bait_file), (bait_file, \"does not exist!\")\n            assert os.path.exists(target_file), (target_file, \"does not exist!\")\n            hybrid_metrics = self._hybrid_select_metrics(align_bam,\n                                                         bait_file, target_file)\n        elif (variant_region_file and\n              config[\"algorithm\"].get(\"coverage_interval\", \"\").lower() in [\"exome\"]):\n            assert os.path.exists(variant_region_file), (variant_region_file, \"does not exist\")\n            hybrid_metrics = self._hybrid_select_metrics(\n                align_bam, variant_region_file, variant_region_file)\n\n        vrn_vals = self._variant_eval_metrics(align_bam)\n        summary_info = self._parser.get_summary_metrics(align_metrics,\n                dup_metrics, insert_metrics, hybrid_metrics,\n                vrn_vals)\n        graphs = []\n        if gc_graph and os.path.exists(gc_graph):\n            graphs.append((gc_graph, \"Distribution of GC content across reads\"))\n        if insert_graph and os.path.exists(insert_graph):\n            graphs.append((insert_graph, \"Distribution of paired end insert sizes\"))\n        return summary_info, graphs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving duplicate information from input BAM file.", "response": "def _get_current_dup_metrics(self, align_bam):\n        \"\"\"Retrieve duplicate information from input BAM file.\n        \"\"\"\n        metrics_file = \"%s.dup_metrics\" % os.path.splitext(align_bam)[0]\n        if not file_exists(metrics_file):\n            dups = 0\n            with pysam.Samfile(align_bam, \"rb\") as bam_handle:\n                for read in bam_handle:\n                    if (read.is_paired and read.is_read1) or not read.is_paired:\n                        if read.is_duplicate:\n                            dups += 1\n            with open(metrics_file, \"w\") as out_handle:\n                out_handle.write(\"# custom bcbio-nextgen metrics\\n\")\n                out_handle.write(\"READ_PAIR_DUPLICATES\\t%s\\n\" % dups)\n        return metrics_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if a metrics file exists for the given BAM.", "response": "def _check_metrics_file(self, bam_name, metrics_ext):\n        \"\"\"Check for an existing metrics file for the given BAM.\n        \"\"\"\n        base, _ = os.path.splitext(bam_name)\n        try:\n            int(base[-1])\n            can_glob = False\n        except ValueError:\n            can_glob = True\n        check_fname = \"{base}{maybe_glob}.{ext}\".format(\n            base=base, maybe_glob=\"*\" if can_glob else \"\", ext=metrics_ext)\n        glob_fnames = glob.glob(check_fname)\n        if len(glob_fnames) > 0:\n            return glob_fnames[0]\n        else:\n            return \"{base}.{ext}\".format(base=base, ext=metrics_ext)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _hybrid_select_metrics(self, dup_bam, bait_file, target_file):\n        metrics = self._check_metrics_file(dup_bam, \"hs_metrics\")\n        if not file_exists(metrics):\n            with bed_to_interval(bait_file, dup_bam) as ready_bait:\n                with bed_to_interval(target_file, dup_bam) as ready_target:\n                    with file_transaction(metrics) as tx_metrics:\n                        opts = [(\"BAIT_INTERVALS\", ready_bait),\n                                (\"TARGET_INTERVALS\", ready_target),\n                                (\"INPUT\", dup_bam),\n                                (\"OUTPUT\", tx_metrics)]\n                        try:\n                            self._picard.run(\"CollectHsMetrics\", opts)\n                        # HsMetrics fails regularly with memory errors\n                        # so we catch and skip instead of aborting the\n                        # full process\n                        except subprocess.CalledProcessError:\n                            return None\n        return metrics", "response": "Generate metrics for hybrid selection efficiency."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds metrics for evaluating variant effectiveness.", "response": "def _variant_eval_metrics(self, dup_bam):\n        \"\"\"Find metrics for evaluating variant effectiveness.\n        \"\"\"\n        base, ext = os.path.splitext(dup_bam)\n        end_strip = \"-dup\"\n        base = base[:-len(end_strip)] if base.endswith(end_strip) else base\n        mfiles = glob.glob(\"%s*eval_metrics\" % base)\n        if len(mfiles) > 0:\n            with open(mfiles[0]) as in_handle:\n                # pull the metrics as JSON from the last line in the file\n                for line in in_handle:\n                    pass\n                metrics = json.loads(line)\n            return metrics\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nproducing summary metrics for a RNASeq experiment using Picard with a sorted aligned BAM file.", "response": "def report(self, align_bam, ref_file, gtf_file, is_paired=False, rrna_file=\"null\"):\n        \"\"\"Produce report metrics for a RNASeq experiment using Picard\n        with a sorted aligned BAM file.\n\n        \"\"\"\n\n        # collect duplication metrics\n        dup_metrics = self._get_current_dup_metrics(align_bam)\n        align_metrics = self._collect_align_metrics(align_bam, ref_file)\n        insert_graph, insert_metrics = (None, None)\n        if is_paired:\n            insert_graph, insert_metrics = self._insert_sizes(align_bam)\n\n        rnaseq_metrics = self._rnaseq_metrics(align_bam, gtf_file, rrna_file)\n\n        summary_info = self._parser.get_summary_metrics(align_metrics,\n                                                dup_metrics,\n                                                insert_metrics=insert_metrics,\n                                                rnaseq_metrics=rnaseq_metrics)\n        graphs = []\n        if insert_graph and file_exists(insert_graph):\n            graphs.append((insert_graph,\n                           \"Distribution of paired end insert sizes\"))\n        return summary_info, graphs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef standard_cl_params(items):\n    out = []\n    def _skip_duplicates(data):\n        return (dd.get_coverage_interval(data) == \"amplicon\" or\n                (dd.get_aligner(data) and not dd.get_mark_duplicates(data)))\n    if any(_skip_duplicates(d) for d in items):\n        broad_runner = broad.runner_from_config(items[0][\"config\"])\n        gatk_type = broad_runner.gatk_type()\n        if gatk_type == \"gatk4\":\n            out += [\"--disable-read-filter\", \"NotDuplicateReadFilter\"]\n        elif LooseVersion(broad_runner.gatk_major_version()) >= LooseVersion(\"3.5\"):\n            out += [\"-drf\", \"DuplicateRead\"]\n    return out", "response": "Shared command line parameters for GATK programs."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nshare preparation work for GATK variant calling.", "response": "def _shared_gatk_call_prep(align_bams, items, ref_file, region, out_file, num_cores=1):\n    \"\"\"Shared preparation work for GATK variant calling.\n    \"\"\"\n    data = items[0]\n    config = data[\"config\"]\n    broad_runner = broad.runner_from_config(config)\n    gatk_type = broad_runner.gatk_type()\n    for x in align_bams:\n        bam.index(x, config)\n    picard_runner = broad.runner_from_path(\"picard\", config)\n    picard_runner.run_fn(\"picard_index_ref\", ref_file)\n    params = [\"-R\", ref_file]\n    coverage_depth_min = tz.get_in([\"algorithm\", \"coverage_depth_min\"], config)\n    if coverage_depth_min and coverage_depth_min < 4:\n        confidence = \"4.0\"\n        params += [\"--standard_min_confidence_threshold_for_calling\", confidence]\n    for a in annotation.get_gatk_annotations(config):\n        params += [\"--annotation\", a]\n    for x in align_bams:\n        params += [\"-I\", x]\n    variant_regions = bedutils.population_variant_regions(items)\n    region = subset_variant_regions(variant_regions, region, out_file, items)\n    if region:\n        if gatk_type == \"gatk4\":\n            params += [\"-L\", bamprep.region_to_gatk(region), \"--interval-set-rule\", \"INTERSECTION\"]\n        else:\n            params += [\"-L\", bamprep.region_to_gatk(region), \"--interval_set_rule\", \"INTERSECTION\"]\n    params += standard_cl_params(items)\n    return broad_runner, params"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unified_genotyper(align_bams, items, ref_file, assoc_files,\n                       region=None, out_file=None):\n    \"\"\"Perform SNP genotyping on the given alignment file.\n    \"\"\"\n    if out_file is None:\n        out_file = \"%s-variants.vcf.gz\" % utils.splitext_plus(align_bams[0])[0]\n    if not utils.file_exists(out_file):\n        broad_runner, params = \\\n            _shared_gatk_call_prep(align_bams, items, ref_file, region, out_file)\n        with file_transaction(items[0], out_file) as tx_out_file:\n            params += [\"-T\", \"UnifiedGenotyper\",\n                       \"-o\", tx_out_file,\n                       \"-ploidy\", (str(ploidy.get_ploidy(items, region))\n                                   if broad_runner.gatk_type() == \"restricted\" else \"2\"),\n                       \"--genotype_likelihoods_model\", \"BOTH\"]\n            resources = config_utils.get_resources(\"gatk\", items[0][\"config\"])\n            if \"options\" in resources:\n                params += [str(x) for x in resources.get(\"options\", [])]\n            broad_runner.run_gatk(params)\n    return vcfutils.bgzip_and_index(out_file, items[0][\"config\"])", "response": "Perform unified genotyping on the given alignment file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining if this call feeds downstream into joint calls.", "response": "def _joint_calling(items):\n    \"\"\"Determine if this call feeds downstream into joint calls.\n    \"\"\"\n    jointcaller = tz.get_in((\"config\", \"algorithm\", \"jointcaller\"), items[0])\n    if jointcaller:\n        assert len(items) == 1, \"Can only do joint calling preparation with GATK with single samples\"\n        assert tz.get_in((\"metadata\", \"batch\"), items[0]) is not None, \\\n            \"Joint calling requires batched samples, %s has no metadata batch.\" % dd.get_sample_name(items[0])\n    return jointcaller"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef haplotype_caller(align_bams, items, ref_file, assoc_files,\n                       region=None, out_file=None):\n    \"\"\"Call variation with GATK's HaplotypeCaller.\n\n    This requires the full non open-source version of GATK.\n    \"\"\"\n    if out_file is None:\n        out_file = \"%s-variants.vcf.gz\" % utils.splitext_plus(align_bams[0])[0]\n    if not utils.file_exists(out_file):\n        num_cores = dd.get_num_cores(items[0])\n        broad_runner, params = \\\n            _shared_gatk_call_prep(align_bams, items, ref_file, region, out_file, num_cores)\n        gatk_type = broad_runner.gatk_type()\n        assert gatk_type in [\"restricted\", \"gatk4\"], \\\n            \"Require full version of GATK 2.4+, or GATK4 for haplotype calling\"\n        with file_transaction(items[0], out_file) as tx_out_file:\n            resources = config_utils.get_resources(\"gatk-spark\", items[0][\"config\"])\n            spark_opts = [str(x) for x in resources.get(\"options\", [])]\n            if _use_spark(num_cores, gatk_type, items, spark_opts):\n                params += [\"-T\", \"HaplotypeCallerSpark\"]\n                if spark_opts:\n                    params += spark_opts\n                else:\n                    params += [\"--spark-master\", \"local[%s]\" % num_cores,\n                               \"--conf\", \"spark.local.dir=%s\" % os.path.dirname(tx_out_file),\n                               \"--conf\", \"spark.driver.host=localhost\", \"--conf\", \"spark.network.timeout=800\",\n                               \"--conf\", \"spark.executor.heartbeatInterval=100\"]\n            else:\n                params += [\"-T\", \"HaplotypeCaller\"]\n            params += [\"--annotation\", \"ClippingRankSumTest\",\n                       \"--annotation\", \"DepthPerSampleHC\"]\n            # Enable hardware based optimizations in GATK 3.1+\n            if LooseVersion(broad_runner.gatk_major_version()) >= LooseVersion(\"3.1\"):\n                if _supports_avx():\n                    # Scale down HMM thread default to avoid overuse of cores\n                    # https://github.com/bcbio/bcbio-nextgen/issues/2442\n                    if gatk_type == \"gatk4\":\n                        params += [\"--native-pair-hmm-threads\", \"1\"]\n                    # GATK4 selects the right HMM optimization automatically with FASTEST_AVAILABLE\n                    # GATK3 needs to be explicitly set\n                    else:\n                        params += [\"--pair_hmm_implementation\", \"VECTOR_LOGLESS_CACHING\"]\n            resources = config_utils.get_resources(\"gatk-haplotype\", items[0][\"config\"])\n            if \"options\" in resources:\n                params += [str(x) for x in resources.get(\"options\", [])]\n            # Prepare gVCFs if doing joint calling\n            is_joint = False\n            if _joint_calling(items) or any(\"gvcf\" in dd.get_tools_on(d) for d in items):\n                is_joint = True\n                # If joint calling parameters not set in user options\n                if not any([x in [\"--emit-ref-confidence\", \"-ERC\", \"--emitRefConfidence\"] for x in params]):\n                    if gatk_type == \"gatk4\":\n                        params += [\"--emit-ref-confidence\", \"GVCF\"]\n                    else:\n                        params += [\"--emitRefConfidence\", \"GVCF\"]\n                        params += [\"--variant_index_type\", \"LINEAR\", \"--variant_index_parameter\", \"128000\"]\n                # Set GQ banding to not be single GQ resolution\n                # No recommended default but try to balance resolution and size\n                # http://gatkforums.broadinstitute.org/gatk/discussion/7051/recommendation-best-practices-gvcf-gq-bands\n\n                if not any([x in [\"-GQB\"] for x in params]):\n                    for boundary in [10, 20, 30, 40, 60, 80]:\n                        params += [\"-GQB\", str(boundary)]\n            # Enable non-diploid calling in GATK 3.3+\n            if LooseVersion(broad_runner.gatk_major_version()) >= LooseVersion(\"3.3\"):\n                params += [\"-ploidy\", str(ploidy.get_ploidy(items, region))]\n            if gatk_type == \"gatk4\":\n                # GATK4 Spark calling does not support bgzipped output, use plain VCFs\n                if is_joint and _use_spark(num_cores, gatk_type, items, spark_opts):\n                    tx_out_file = tx_out_file.replace(\".vcf.gz\", \".vcf\")\n                params += [\"--output\", tx_out_file]\n            else:\n                params += [\"-o\", tx_out_file]\n            broad_runner.new_resources(\"gatk-haplotype\")\n            memscale = {\"magnitude\": 0.9 * num_cores, \"direction\": \"increase\"} if num_cores > 1 else None\n            try:\n                broad_runner.run_gatk(params, os.path.dirname(tx_out_file), memscale=memscale,\n                                      parallel_gc=_use_spark(num_cores, gatk_type, items, spark_opts))\n            except subprocess.CalledProcessError as msg:\n                # Spark failing on regions without any reads, write an empty VCF instead\n                # https://github.com/broadinstitute/gatk/issues/4234\n                if (_use_spark(num_cores, gatk_type, items, spark_opts) and\n                      str(msg).find(\"java.lang.UnsupportedOperationException: empty collection\") >= 0 and\n                      str(msg).find(\"at org.apache.spark.rdd.RDD\") >= 0):\n                    vcfutils.write_empty_vcf(tx_out_file, samples=[dd.get_sample_name(d) for d in items])\n                else:\n                    raise\n            if tx_out_file.endswith(\".vcf\"):\n                vcfutils.bgzip_and_index(tx_out_file, items[0][\"config\"])\n\n\n    # avoid bug in GATK where files can get output as non-compressed\n    if out_file.endswith(\".gz\") and not os.path.exists(out_file + \".tbi\"):\n        with open(out_file, \"r\") as in_handle:\n            is_plain_text = in_handle.readline().startswith(\"##fileformat\")\n        if is_plain_text:\n            text_out_file = out_file\n            out_file = out_file.replace(\".vcf.gz\", \".vcf\")\n            shutil.move(text_out_file, out_file)\n    return vcfutils.bgzip_and_index(out_file, items[0][\"config\"])", "response": "Call variation with GATK s HaplotypeCaller."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _supports_avx():\n    if os.path.exists(\"/proc/cpuinfo\"):\n        with open(\"/proc/cpuinfo\") as in_handle:\n            for line in in_handle:\n                if line.startswith(\"flags\") and line.find(\"avx\") > 0:\n                    return True", "response": "Check for support for Intel AVX acceleration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving version information based on jar file.", "response": "def jar_versioner(program_name, jar_name):\n    \"\"\"Retrieve version information based on jar file.\n    \"\"\"\n    def get_version(config):\n        try:\n            pdir = config_utils.get_program(program_name, config, \"dir\")\n        # not configured\n        except ValueError:\n            return \"\"\n        jar = os.path.basename(config_utils.get_jar(jar_name, pdir))\n        for to_remove in [jar_name, \".jar\", \"-standalone\"]:\n            jar = jar.replace(to_remove, \"\")\n        if jar.startswith((\"-\", \".\")):\n            jar = jar[1:]\n        if not jar:\n            logger.warn(\"Unable to determine version for program '{}' from jar file {}\".format(\n                program_name, config_utils.get_jar(jar_name, pdir)))\n        return jar\n    return get_version"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_cl_version(p, config):\n    if not p.get(\"has_cl_version\", True):\n        return \"\"\n    try:\n        prog = config_utils.get_program(p[\"cmd\"], config)\n    except config_utils.CmdNotFound:\n\n        localpy_cmd = os.path.join(os.path.dirname(sys.executable), p[\"cmd\"])\n        if os.path.exists(localpy_cmd):\n            prog = localpy_cmd\n        else:\n            return \"\"\n    args = p.get(\"args\", \"\")\n\n    cmd = \"{prog} {args}\"\n    subp = subprocess.Popen(cmd.format(**locals()), stdout=subprocess.PIPE,\n                            stderr=subprocess.STDOUT,\n                            shell=True)\n    with contextlib.closing(subp.stdout) as stdout:\n        if p.get(\"stdout_flag\"):\n            v = _parse_from_stdoutflag(stdout, p[\"stdout_flag\"])\n        elif p.get(\"paren_flag\"):\n            v = _parse_from_parenflag(stdout, p[\"paren_flag\"])\n        else:\n            lines = [l.strip() for l in str(stdout.read()).split(\"\\n\") if l.strip()]\n            v = lines[-1]\n    if v.endswith(\".\"):\n        v = v[:-1]\n    return v", "response": "Retrieve version of a single commandline program."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_brew_versions():\n    from bcbio import install\n    tooldir = install.get_defaults().get(\"tooldir\")\n    brew_cmd = os.path.join(tooldir, \"bin\", \"brew\") if tooldir else \"brew\"\n    try:\n        vout = subprocess.check_output([brew_cmd, \"list\", \"--versions\"])\n    except OSError:  # brew not installed/used\n        vout = \"\"\n    out = {}\n    for vstr in vout.split(\"\\n\"):\n        if vstr.strip():\n            parts = vstr.rstrip().split()\n            name = parts[0]\n            v = parts[-1]\n            out[name] = v\n    return out", "response": "Retrieve versions of tools installed via brew."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_versions(config=None):\n    try:\n        from bcbio.pipeline import version\n        if hasattr(version, \"__version__\"):\n            bcbio_version = (\"%s-%s\" % (version.__version__, version.__git_revision__)\n                             if version.__git_revision__ else version.__version__)\n        else:\n            bcbio_version = \"\"\n    except ImportError:\n        bcbio_version = \"\"\n    out = [{\"program\": \"bcbio-nextgen\", \"version\": bcbio_version}]\n    manifest_dir = _get_manifest_dir(config)\n    manifest_vs = _get_versions_manifest(manifest_dir) if manifest_dir else []\n    if manifest_vs:\n        out += manifest_vs\n    else:\n        assert config is not None, \"Need configuration to retrieve from non-manifest installs\"\n        brew_vs = _get_brew_versions()\n        for p in _cl_progs:\n            out.append({\"program\": p[\"cmd\"],\n                        \"version\": (brew_vs[p[\"cmd\"]] if p[\"cmd\"] in brew_vs else\n                                    _get_cl_version(p, config))})\n        for p in _alt_progs:\n            out.append({\"program\": p[\"name\"],\n                        \"version\": (brew_vs[p[\"name\"]] if p[\"name\"] in brew_vs else\n                                    p[\"version_fn\"](config))})\n    out.sort(key=lambda x: x[\"program\"])\n    return out", "response": "Retrieve details on all programs available on the system."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the manifest directory from the data dictionary.", "response": "def _get_manifest_dir(data=None, name=None):\n    \"\"\"\n    get manifest directory from the data dictionary, falling back on alternatives\n    it prefers, in order:\n    1. locating it from the bcbio_system.yaml file\n    2. locating it from the galaxy directory\n    3. location it from the python executable.\n\n    it can accept either the data or config dictionary\n    \"\"\"\n    manifest_dir = None\n    if data:\n        bcbio_system = tz.get_in([\"config\", \"bcbio_system\"], data, None)\n        bcbio_system = bcbio_system if bcbio_system else data.get(\"bcbio_system\", None)\n        if bcbio_system:\n            sibling_dir = os.path.normpath(os.path.dirname(bcbio_system))\n        else:\n            sibling_dir = dd.get_galaxy_dir(data)\n        if sibling_dir:\n            manifest_dir = os.path.normpath(os.path.join(sibling_dir, os.pardir,\n                                                         \"manifest\"))\n    if not manifest_dir or not os.path.exists(manifest_dir):\n        manifest_dir = os.path.join(config_utils.get_base_installdir(), \"manifest\")\n        if not os.path.exists(manifest_dir) and name:\n            manifest_dir = os.path.join(config_utils.get_base_installdir(name), \"manifest\")\n    return manifest_dir"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_versions_manifest(manifest_dir):\n    all_pkgs = _manifest_progs + [p.get(\"name\", p[\"cmd\"]) for p in _cl_progs] + [p[\"name\"] for p in _alt_progs]\n    if os.path.exists(manifest_dir):\n        out = []\n        for plist in [\"toolplus\", \"python\", \"r\", \"debian\", \"custom\"]:\n            pkg_file = os.path.join(manifest_dir, \"%s-packages.yaml\" % plist)\n            if os.path.exists(pkg_file):\n                with open(pkg_file) as in_handle:\n                    pkg_info = yaml.safe_load(in_handle)\n                if not pkg_info:\n                    continue\n                added = []\n                for pkg in all_pkgs:\n                    if pkg in pkg_info:\n                        added.append(pkg)\n                        out.append({\"program\": pkg, \"version\": pkg_info[pkg][\"version\"]})\n                for x in added:\n                    all_pkgs.remove(x)\n        out.sort(key=lambda x: x[\"program\"])\n        for pkg in all_pkgs:\n            out.append({\"program\": pkg, \"version\": \"\"})\n        return out", "response": "Retrieve versions from a pre - existing manifest of installed software."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_versions(dirs, config=None, is_wrapper=False):\n    out_file = _get_program_file(dirs)\n    if is_wrapper:\n        assert utils.file_exists(out_file), \"Failed to create program versions from VM\"\n    elif out_file is None:\n        for p in _get_versions(config):\n            print(\"{program},{version}\".format(**p))\n    else:\n        with open(out_file, \"w\") as out_handle:\n            for p in _get_versions(config):\n                out_handle.write(\"{program},{version}\\n\".format(**p))\n    return out_file", "response": "Write CSV file with versions used in analysis pipeline."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve a version from the currently installed manifest.", "response": "def get_version_manifest(name, data=None, required=False):\n    \"\"\"Retrieve a version from the currently installed manifest.\n    \"\"\"\n    manifest_dir = _get_manifest_dir(data, name)\n    manifest_vs = _get_versions_manifest(manifest_dir) or []\n    for x in manifest_vs:\n        if x[\"program\"] == name:\n            v = x.get(\"version\", \"\")\n            if v:\n                return v\n    if required:\n        raise ValueError(\"Did not find %s in install manifest. Could not check version.\" % name)\n    return \"\""}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds command line option for exporting version information.", "response": "def add_subparser(subparsers):\n    \"\"\"Add command line option for exporting version information.\n    \"\"\"\n    parser = subparsers.add_parser(\"version\",\n                                   help=\"Export versions of used software to stdout or a file \")\n    parser.add_argument(\"--workdir\", help=\"Directory export programs to in workdir/provenance/programs.txt\",\n                        default=None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves the current version of the given program from cached names.", "response": "def get_version(name, dirs=None, config=None):\n    \"\"\"Retrieve the current version of the given program from cached names.\n    \"\"\"\n    if dirs:\n        p = _get_program_file(dirs)\n    else:\n        p = tz.get_in([\"resources\", \"program_versions\"], config)\n    if p:\n        with open(p) as in_handle:\n            for line in in_handle:\n                prog, version = line.rstrip().split(\",\")\n                if prog == name and version:\n                    return version\n        raise KeyError(\"Version information not found for %s in %s\" % (name, p))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprovide a range of options for HLA type with decreasing resolution.", "response": "def hla_choices(orig_hla, min_parts=2):\n    \"\"\"Provide a range of options for HLA type, with decreasing resolution.\n    \"\"\"\n    yield orig_hla\n    try:\n        int(orig_hla[-1])\n    except ValueError:\n        yield orig_hla[:-1]\n    hla_parts = orig_hla.split(\":\")\n    for sub_i in range(len(hla_parts) - min_parts + 1):\n        yield \":\".join(hla_parts[:len(hla_parts) - sub_i])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads HLAs and the pgroups they fall in.", "response": "def read_pgroups(in_file):\n    \"\"\"Read HLAs and the pgroups they fall in.\n    \"\"\"\n    out = {}\n    with open(in_file) as in_handle:\n        for line in (l for l in in_handle if not l.startswith(\"#\")):\n            locus, alleles, group = line.strip().split(\";\")\n            for allele in alleles.split(\"/\"):\n                out[\"HLA-%s%s\" % (locus, allele)] = group\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting HLA alleles from the hg38 fasta fai file.", "response": "def read_hlas(fasta_fai):\n    \"\"\"Get HLA alleles from the hg38 fasta fai file.\n    \"\"\"\n    out = []\n    with open(fasta_fai) as in_handle:\n        for line in in_handle:\n            if line.startswith(\"HLA\"):\n                out.append(line.split()[0])\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsplits a VCF file into separate files by chromosome.", "response": "def split_vcf(in_file, ref_file, config, out_dir=None):\n    \"\"\"Split a VCF file into separate files by chromosome.\n    \"\"\"\n    if out_dir is None:\n        out_dir = os.path.join(os.path.dirname(in_file), \"split\")\n    out_files = []\n    with open(ref.fasta_idx(ref_file, config)) as in_handle:\n        for line in in_handle:\n            chrom, size = line.split()[:2]\n            out_file = os.path.join(out_dir,\n                                    os.path.basename(replace_suffix(append_stem(in_file, \"-%s\" % chrom), \".vcf\")))\n            subset_vcf(in_file, (chrom, 0, size), out_file, config)\n            out_files.append(out_file)\n    return out_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprepares recalibration tables for BQSR recalibration.", "response": "def prep_recal(data):\n    \"\"\"Do pre-BQSR recalibration, calculation of recalibration tables.\n    \"\"\"\n    if dd.get_recalibrate(data) in [True, \"gatk\"]:\n        logger.info(\"Prepare BQSR tables with GATK: %s \" % str(dd.get_sample_name(data)))\n        dbsnp_file = tz.get_in((\"genome_resources\", \"variation\", \"dbsnp\"), data)\n        if not dbsnp_file:\n            logger.info(\"Skipping GATK BaseRecalibrator because no VCF file of known variants was found.\")\n            return data\n        broad_runner = broad.runner_from_config(data[\"config\"])\n        data[\"prep_recal\"] = _gatk_base_recalibrator(broad_runner, dd.get_align_bam(data),\n                                                     dd.get_ref_file(data), dd.get_platform(data),\n                                                     dbsnp_file,\n                                                     dd.get_variant_regions(data) or dd.get_sample_callable(data),\n                                                     data)\n    elif dd.get_recalibrate(data) == \"sentieon\":\n        logger.info(\"Prepare BQSR tables with sentieon: %s \" % str(dd.get_sample_name(data)))\n        data[\"prep_recal\"] = sentieon.bqsr_table(data)\n    elif dd.get_recalibrate(data):\n        raise NotImplementedError(\"Unsupported recalibration type: %s\" % (dd.get_recalibrate(data)))\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef apply_recal(data):\n    orig_bam = dd.get_align_bam(data) or dd.get_work_bam(data)\n    had_work_bam = \"work_bam\" in data\n    if dd.get_recalibrate(data) in [True, \"gatk\"]:\n        if data.get(\"prep_recal\"):\n            logger.info(\"Applying BQSR recalibration with GATK: %s \" % str(dd.get_sample_name(data)))\n            data[\"work_bam\"] = _gatk_apply_bqsr(data)\n    elif dd.get_recalibrate(data) == \"sentieon\":\n        if data.get(\"prep_recal\"):\n            logger.info(\"Applying BQSR recalibration with sentieon: %s \" % str(dd.get_sample_name(data)))\n            data[\"work_bam\"] = sentieon.apply_bqsr(data)\n    elif dd.get_recalibrate(data):\n        raise NotImplementedError(\"Unsupported recalibration type: %s\" % (dd.get_recalibrate(data)))\n    # CWL does not have work/alignment BAM separation\n    if not had_work_bam and dd.get_work_bam(data):\n        data[\"align_bam\"] = dd.get_work_bam(data)\n    if orig_bam != dd.get_work_bam(data) and orig_bam != dd.get_align_bam(data):\n        utils.save_diskspace(orig_bam, \"BAM recalibrated to %s\" % dd.get_work_bam(data), data[\"config\"])\n    return data", "response": "Apply recalibration tables to the sorted aligned BAM."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming base recalibration on non - restricted reads.", "response": "def _gatk_base_recalibrator(broad_runner, dup_align_bam, ref_file, platform,\n                            dbsnp_file, intervals, data):\n    \"\"\"Step 1 of GATK recalibration process, producing table of covariates.\n\n    For GATK 4 we use local multicore spark runs:\n    https://github.com/broadinstitute/gatk/issues/2345\n\n    For GATK3, Large whole genome BAM files take an excessively long time to recalibrate and\n    the extra inputs don't help much beyond a certain point. See the 'Downsampling analysis'\n    plots in the GATK documentation:\n\n    http://gatkforums.broadinstitute.org/discussion/44/base-quality-score-recalibrator#latest\n\n    This identifies large files and calculates the fraction to downsample to.\n\n    spark host and timeout settings help deal with runs on restricted systems\n    where we encounter network and timeout errors\n    \"\"\"\n    target_counts = 1e8  # 100 million reads per read group, 20x the plotted max\n    out_file = os.path.join(dd.get_work_dir(data), \"align\", dd.get_sample_name(data),\n                            \"%s-recal.grp\" % utils.splitext_plus(os.path.basename(dup_align_bam))[0])\n    if not utils.file_exists(out_file):\n        if has_aligned_reads(dup_align_bam, intervals):\n            with file_transaction(data, out_file) as tx_out_file:\n                gatk_type = broad_runner.gatk_type()\n                assert gatk_type in [\"restricted\", \"gatk4\"], \\\n                    \"Require full version of GATK 2.4+ or GATK4 for BQSR\"\n                params = [\"-I\", dup_align_bam]\n                cores = dd.get_num_cores(data)\n                if gatk_type == \"gatk4\":\n                    resources = config_utils.get_resources(\"gatk-spark\", data[\"config\"])\n                    spark_opts = [str(x) for x in resources.get(\"options\", [])]\n                    params += [\"-T\", \"BaseRecalibratorSpark\",\n                               \"--output\", tx_out_file, \"--reference\", dd.get_ref_file(data)]\n                    if spark_opts:\n                        params += spark_opts\n                    else:\n                        params += [\"--spark-master\", \"local[%s]\" % cores,\n                                   \"--conf\", \"spark.driver.host=localhost\", \"--conf\", \"spark.network.timeout=800\",\n                                   \"--conf\", \"spark.executor.heartbeatInterval=100\",\n                                   \"--conf\", \"spark.local.dir=%s\" % os.path.dirname(tx_out_file)]\n                    if dbsnp_file:\n                        params += [\"--known-sites\", dbsnp_file]\n                    if intervals:\n                        params += [\"-L\", intervals, \"--interval-set-rule\", \"INTERSECTION\"]\n                else:\n                    params += [\"-T\", \"BaseRecalibrator\",\n                                \"-o\", tx_out_file, \"-R\", ref_file]\n                    downsample_pct = bam.get_downsample_pct(dup_align_bam, target_counts, data)\n                    if downsample_pct:\n                        params += [\"--downsample_to_fraction\", str(downsample_pct),\n                                   \"--downsampling_type\", \"ALL_READS\"]\n                    if platform.lower() == \"solid\":\n                        params += [\"--solid_nocall_strategy\", \"PURGE_READ\",\n                                   \"--solid_recal_mode\", \"SET_Q_ZERO_BASE_N\"]\n                    if dbsnp_file:\n                        params += [\"--knownSites\", dbsnp_file]\n                    if intervals:\n                        params += [\"-L\", intervals, \"--interval_set_rule\", \"INTERSECTION\"]\n                memscale = {\"magnitude\": 0.9 * cores, \"direction\": \"increase\"} if cores > 1 else None\n                broad_runner.run_gatk(params, os.path.dirname(tx_out_file), memscale=memscale,\n                                      parallel_gc=True)\n        else:\n            with open(out_file, \"w\") as out_handle:\n                out_handle.write(\"# No aligned reads\")\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _gatk_apply_bqsr(data):\n    in_file = dd.get_align_bam(data) or dd.get_work_bam(data)\n    out_file = os.path.join(dd.get_work_dir(data), \"align\", dd.get_sample_name(data),\n                            \"%s-recal.bam\" % utils.splitext_plus(os.path.basename(in_file))[0])\n    if not utils.file_uptodate(out_file, in_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            broad_runner = broad.runner_from_config(data[\"config\"])\n            gatk_type = broad_runner.gatk_type()\n            cores = dd.get_num_cores(data)\n            if gatk_type == \"gatk4\":\n                resources = config_utils.get_resources(\"gatk-spark\", data[\"config\"])\n                spark_opts = [str(x) for x in resources.get(\"options\", [])]\n                params = [\"-T\", \"ApplyBQSRSpark\",\n                          \"--input\", in_file, \"--output\", tx_out_file, \"--bqsr-recal-file\", data[\"prep_recal\"],\n                          \"--static-quantized-quals\", \"10\", \"--static-quantized-quals\", \"20\",\n                          \"--static-quantized-quals\", \"30\"]\n                if spark_opts:\n                    params += spark_opts\n                else:\n                    params += [\"--spark-master\", \"local[%s]\" % cores,\n                               \"--conf\", \"spark.local.dir=%s\" % os.path.dirname(tx_out_file),\n                               \"--conf\", \"spark.driver.host=localhost\", \"--conf\", \"spark.network.timeout=800\"]\n            else:\n                params = [\"-T\", \"PrintReads\", \"-R\", dd.get_ref_file(data), \"-I\", in_file,\n                          \"-BQSR\", data[\"prep_recal\"], \"-o\", tx_out_file]\n            # Avoid problems with intel deflater for GATK 3.8 and GATK4\n            # https://github.com/bcbio/bcbio-nextgen/issues/2145#issuecomment-343095357\n            if gatk_type == \"gatk4\":\n                params += [\"--jdk-deflater\", \"--jdk-inflater\"]\n            elif LooseVersion(broad_runner.gatk_major_version()) > LooseVersion(\"3.7\"):\n                params += [\"-jdk_deflater\", \"-jdk_inflater\"]\n            memscale = {\"magnitude\": 0.9 * cores, \"direction\": \"increase\"} if cores > 1 else None\n            broad_runner.run_gatk(params, os.path.dirname(tx_out_file), memscale=memscale,\n                                  parallel_gc=True)\n    bam.index(out_file, data[\"config\"])\n    return out_file", "response": "Parallel BQSR support for GATK4."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating base logging for local or remote processes.", "response": "def create_base_logger(config=None, parallel=None):\n    \"\"\"Setup base logging configuration, also handling remote logging.\n\n    Correctly sets up for local, multiprocessing and distributed runs.\n    Creates subscribers for non-local runs that will be references from\n    local logging.\n\n    Retrieves IP address using tips from http://stackoverflow.com/a/1267524/252589\n    \"\"\"\n    if parallel is None: parallel = {}\n    parallel_type = parallel.get(\"type\", \"local\")\n    cores = parallel.get(\"cores\", 1)\n    if parallel_type == \"ipython\":\n        from bcbio.log import logbook_zmqpush\n        fqdn_ip = socket.gethostbyname(socket.getfqdn())\n        ips = [fqdn_ip] if (fqdn_ip and not fqdn_ip.startswith(\"127.\")) else []\n        if not ips:\n            ips = [ip for ip in socket.gethostbyname_ex(socket.gethostname())[2]\n                   if not ip.startswith(\"127.\")]\n        if not ips:\n            ips += [(s.connect(('8.8.8.8', 53)), s.getsockname()[0], s.close())[1] for s in\n                    [socket.socket(socket.AF_INET, socket.SOCK_DGRAM)]]\n        if not ips:\n            sys.stderr.write(\"Cannot resolve a local IP address that isn't 127.x.x.x \"\n                             \"Your machines might not have a local IP address \"\n                             \"assigned or are not able to resolve it.\\n\")\n            sys.exit(1)\n        uri = \"tcp://%s\" % ips[0]\n        subscriber = logbook_zmqpush.ZeroMQPullSubscriber()\n        mport = subscriber.socket.bind_to_random_port(uri)\n        wport_uri = \"%s:%s\" % (uri, mport)\n        parallel[\"log_queue\"] = wport_uri\n        subscriber.dispatch_in_background(_create_log_handler(config, True))\n    elif cores > 1:\n        subscriber = IOSafeMultiProcessingSubscriber(mpq)\n        subscriber.dispatch_in_background(_create_log_handler(config))\n    else:\n        # Do not need to setup anything for local logging\n        pass\n    return parallel"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setup_script_logging():\n    handlers = [logbook.NullHandler()]\n    format_str = (\"[{record.time:%Y-%m-%dT%H:%MZ}] \"\n                  \"{record.level_name}: {record.message}\")\n\n    handler = logbook.StreamHandler(sys.stderr, format_string=format_str,\n                                    level=\"DEBUG\")\n    handler.push_thread()\n    return handler", "response": "Setup logging for the script - like script."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates the WDL output using wdltool.", "response": "def _validate(wdl_file):\n    \"\"\"Run validation on the generated WDL output using wdltool.\n    \"\"\"\n    start_dir = os.getcwd()\n    os.chdir(os.path.dirname(wdl_file))\n    print(\"Validating\", wdl_file)\n    subprocess.check_call([\"wdltool\", \"validate\", wdl_file])\n    os.chdir(start_dir)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a cwl2wdl workflow into cwl2wdl style dictionaries for base and sub - workflows.", "response": "def _wf_to_dict(wf, records):\n    \"\"\"Parse a workflow into cwl2wdl style dictionaries for base and sub-workflows.\n    \"\"\"\n    inputs, outputs, records = _get_wf_inout(wf, records)\n    out = {\"name\": _id_to_name(_clean_id(wf.tool[\"id\"])), \"inputs\": inputs,\n           \"outputs\": outputs, \"steps\": [], \"subworkflows\": [],\n           \"requirements\": []}\n    for step in wf.steps:\n        is_subworkflow = isinstance(step.embedded_tool, cwltool.workflow.Workflow)\n        inputs, outputs, remapped, prescatter = _get_step_inout(step)\n        inputs, scatter = _organize_step_scatter(step, inputs, remapped)\n        if is_subworkflow:\n            wf_def, records = _wf_to_dict(step.embedded_tool, records)\n            out[\"subworkflows\"].append({\"id\": \"%s.%s\" % (wf_def[\"name\"], wf_def[\"name\"]), \"definition\": wf_def,\n                                        \"inputs\": inputs, \"outputs\": outputs, \"scatter\": scatter,\n                                        \"prescatter\": prescatter})\n        else:\n            task_def, records = _tool_to_dict(step.embedded_tool, records, remapped)\n            out[\"steps\"].append({\"task_id\": task_def[\"name\"], \"task_definition\": task_def,\n                                 \"inputs\": inputs, \"outputs\": outputs, \"scatter\": scatter,\n                                 \"prescatter\": prescatter})\n    return out, records"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_step_inout(step):\n    inputs = []\n    outputs = []\n    prescatter = collections.defaultdict(list)\n    remapped = {}\n    assert step.outputs_record_schema[\"type\"] == \"record\"\n    output_names = set([])\n    for outp in step.outputs_record_schema[\"fields\"]:\n        outputs.append({\"id\": outp[\"name\"]})\n        output_names.add(outp[\"name\"])\n    assert step.inputs_record_schema[\"type\"] == \"record\"\n    for inp in step.inputs_record_schema[\"fields\"]:\n        source = inp[\"source\"].split(\"#\")[-1].replace(\"/\", \".\")\n        # Check if we're unpacking from a record, and unpack from our object\n        if \"valueFrom\" in inp:\n            attr_access = \"['%s']\" % inp[\"name\"]\n            if inp[\"valueFrom\"].find(attr_access) > 0:\n                source += \".%s\" % inp[\"name\"]\n                if isinstance(inp[\"type\"], dict) and isinstance(inp[\"type\"].get(\"items\"), dict):\n                    if inp[\"type\"][\"items\"].get(\"type\") == \"array\" and \"inputBinding\" in inp[\"type\"]:\n                        source, prescatter = _unpack_object_array(inp, source, prescatter)\n        # Avoid clashing input and output names, WDL requires unique\n        if inp[\"name\"] in output_names:\n            new_name = inp[\"name\"] + \"_input\"\n            remapped[inp[\"name\"]] = new_name\n            inp[\"name\"] = new_name\n        inputs.append({\"id\": inp[\"name\"], \"value\": source})\n    return inputs, outputs, remapped, dict(prescatter)", "response": "Retrieve set of inputs and outputs connecting steps."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nunpack an array of items from a source object.", "response": "def _unpack_object_array(inp, source, prescatter):\n    \"\"\"Unpack Array[Object] with a scatter for referencing in input calls.\n\n    There is no shorthand syntax for referencing all items in an array, so\n    we explicitly unpack them with a scatter.\n    \"\"\"\n    raise NotImplementedError(\"Currently not used with record/struct/object improvements\")\n    base_rec, attr = source.rsplit(\".\", 1)\n    new_name = \"%s_%s_unpack\" % (inp[\"name\"], base_rec.replace(\".\", \"_\"))\n    prescatter[base_rec].append((new_name, attr, _to_variable_type(inp[\"type\"][\"items\"])))\n    return new_name, prescatter"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _organize_step_scatter(step, inputs, remapped):\n    def extract_scatter_id(inp):\n        _, ns_var = inp.split(\"#\")\n        _, var = ns_var.split(\"/\")\n        return var\n    scatter_local = {}\n    if \"scatter\" in step.tool:\n        assert step.tool[\"scatterMethod\"] == \"dotproduct\", \\\n            \"Only support dotproduct scattering in conversion to WDL\"\n        inp_val = collections.OrderedDict()\n        for x in inputs:\n            inp_val[x[\"id\"]] = x[\"value\"]\n        for scatter_key in [extract_scatter_id(x) for x in step.tool[\"scatter\"]]:\n            scatter_key = remapped.get(scatter_key) or scatter_key\n            val = inp_val[scatter_key]\n            if len(val.split(\".\")) in [1, 2]:\n                base_key = val\n                attr = None\n            elif len(val.split(\".\")) == 3:\n                orig_location, record, attr = val.split(\".\")\n                base_key = \"%s.%s\" % (orig_location, record)\n            else:\n                raise ValueError(\"Unexpected scatter input: %s\" % val)\n            local_ref = base_key.split(\".\")[-1] + \"_local\"\n            scatter_local[base_key] = local_ref\n            if attr:\n                local_ref += \".%s\" % attr\n            inp_val[scatter_key] = local_ref\n            inputs = [{\"id\": iid, \"value\": ival} for iid, ival in inp_val.items()]\n    return inputs, [(v, k) for k, v in scatter_local.items()]", "response": "Add scattering information from inputs remapping input variables."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a CWL record into a WDL struct.", "response": "def _record_to_struct(cur_rec, records):\n    \"\"\"Convert a CWL record into a WDL struct/Object.\n\n    Work in progress to support changes to WDL Objects to\n    be defined in structs.\n    \"\"\"\n    def to_camel_case(x):\n        def uppercase_word(w):\n            return w[0].upper() + w[1:]\n        return \"\".join(uppercase_word(w) for w in x.split(\"_\"))\n    struct_name = to_camel_case(cur_rec[\"name\"].split(\"/\")[-1])\n    if struct_name not in records:\n        records[struct_name] = collections.OrderedDict()\n        for field in cur_rec[\"fields\"]:\n            field_type, records = _to_variable_type(field[\"type\"], records)\n            records[struct_name][field[\"name\"].split(\"/\")[-1]] = field_type\n    return struct_name, records"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _to_variable_type(x, records):\n    var_mapping = {\"string\": \"String\", \"File\": \"File\", \"null\": \"String\",\n                   \"long\": \"Float\", \"double\": \"Float\", \"int\": \"Int\"}\n    if isinstance(x, dict):\n        if x[\"type\"] == \"record\":\n            struct_name, records = _record_to_struct(x, records)\n            return struct_name, records\n        else:\n            assert x[\"type\"] == \"array\", x\n            cur_type, records = _to_variable_type(x[\"items\"], records)\n            return \"Array[%s]\" % cur_type, records\n    elif isinstance(x, (list, tuple)):\n        vars = [v for v in x if v != \"null\"]\n        return var_mapping[vars[0]], records\n    else:\n        return var_mapping[x], records", "response": "Convert CWL variables to WDL variables handling nested arrays."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _variable_type_to_read_fn(vartype, records):\n    fn_map = {\"String\": \"read_string\", \"Array[String]\": \"read_lines\",\n              \"Array[Array[String]]\": \"read_tsv\",\n              \"Object\": \"read_object\", \"Array[Object]\": \"read_objects\",\n              \"Array[Array[Object]]\": \"read_objects\",\n              \"Int\": \"read_int\", \"Float\": \"read_float\"}\n    for rec_name in records.keys():\n        fn_map[\"%s\" % rec_name] = \"read_struct\"\n        fn_map[\"Array[%s]\" % rec_name] = \"read_struct\"\n        fn_map[\"Array[Array[%s]]\" % rec_name] = \"read_struct\"\n    # Read in Files as Strings\n    vartype = vartype.replace(\"File\", \"String\")\n    # Can't read arrays of Ints/Floats\n    vartype = vartype.replace(\"Array[Int]\", \"Array[String]\")\n    vartype = vartype.replace(\"Array[Float]\", \"Array[String]\")\n    return fn_map[vartype]", "response": "Convert a given variant type into corresponding WDL standard library functions."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a cwl2wdl Input object into a dictionary required for a cwl2wdl Input object.", "response": "def _input_to_dict(i, records, remapped=None):\n    \"\"\"Convert CWL input into dictionary required for a cwl2wdl Input object.\n    \"\"\"\n    if not remapped: remapped = {}\n    var_type, records = _to_variable_type(i[\"type\"], records)\n    if var_type.startswith(\"Array\") and \"inputBinding\" in i.get(\"type\", {}):\n        ib = i[\"type\"][\"inputBinding\"]\n    elif \"inputBinding\" in i:\n        ib = i[\"inputBinding\"]\n    else:\n        ib = {\"prefix\": None, \"itemSeparator\": \";;\", \"position\": None}\n    name = _id_to_localname(i[\"id\"]) if \"id\" in i else i[\"name\"]\n    return {\"name\": remapped.get(name) or name,\n            \"variable_type\": var_type,\n            \"prefix\": ib[\"prefix\"], \"separator\": ib[\"itemSeparator\"],\n            \"position\": ib[\"position\"], \"is_required\": True,\n            \"default\": i.get(\"default\", None), \"separate\": ib.get(\"separate\", True)}, records"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _tool_to_dict(tool, records, remapped):\n    requirements = _requirements_to_dict(tool.requirements + tool.hints)\n    inputs = []\n    outputs = []\n    for inp in tool.tool[\"inputs\"]:\n        ready_inp, records = _input_to_dict(inp, records, remapped)\n        inputs.append(ready_inp)\n    for outp in tool.tool[\"outputs\"]:\n        ready_outp, records = _output_to_dict(outp, records)\n        outputs.append(ready_outp)\n    out = {\"name\": _id_to_name(tool.tool[\"id\"]),\n           \"baseCommand\": \" \".join(tool.tool[\"baseCommand\"]),\n           \"arguments\": [_arg_to_dict(a, requirements) for a in tool.tool[\"arguments\"]],\n           \"inputs\": inputs,\n           \"outputs\": outputs,\n           \"requirements\": requirements,\n           \"stdin\": None, \"stdout\": None}\n    return out, records", "response": "Parse a cwl2wdl tool definition into a cwl2wdl style dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _requirements_to_dict(rs):\n    out = []\n    added = set([])\n    for r in rs:\n        if r[\"class\"] == \"DockerRequirement\" and \"docker\" not in added:\n            added.add(\"docker\")\n            out.append({\"requirement_type\": \"docker\", \"value\": r[\"dockerImageId\"]})\n        elif r[\"class\"] == \"ResourceRequirement\":\n            if \"coresMin\" in r and \"cpu\" not in added:\n                added.add(\"cpu\")\n                out.append({\"requirement_type\": \"cpu\", \"value\": r[\"coresMin\"]})\n            if \"ramMin\" in r and \"memory\" not in added:\n                added.add(\"memory\")\n                out.append({\"requirement_type\": \"memory\", \"value\": \"%s MB\" % r[\"ramMin\"]})\n            if \"tmpdirMin\" in r and \"disks\" not in added:\n                added.add(\"disks\")\n                out.append({\"requirement_type\": \"disks\", \"value\": \"local-disk %s HDD\" % r[\"tmpdirMin\"]})\n    return out", "response": "Convert supported requirements into dictionary for output."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving Java options adjusting memory for available cores.", "response": "def _get_jvm_opts(out_file, data):\n    \"\"\"Retrieve Java options, adjusting memory for available cores.\n    \"\"\"\n    resources = config_utils.get_resources(\"purple\", data[\"config\"])\n    jvm_opts = resources.get(\"jvm_opts\", [\"-Xms750m\", \"-Xmx3500m\"])\n    jvm_opts = config_utils.adjust_opts(jvm_opts, {\"algorithm\": {\"memory_adjust\":\n                                                                 {\"direction\": \"increase\",\n                                                                  \"maximum\": \"30000M\",\n                                                                  \"magnitude\": dd.get_cores(data)}}})\n    jvm_opts += broad.get_default_jvm_opts(os.path.dirname(out_file))\n    return jvm_opts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning PURPLE with pre - calculated AMBER and COBALT compatible inputs.", "response": "def _run_purple(paired, het_file, depth_file, vrn_files, work_dir):\n    \"\"\"Run PURPLE with pre-calculated AMBER and COBALT compatible inputs.\n    \"\"\"\n    purple_dir = utils.safe_makedir(os.path.join(work_dir, \"purple\"))\n    out_file = os.path.join(purple_dir, \"%s.purple.cnv\" % dd.get_sample_name(paired.tumor_data))\n    if not utils.file_exists(out_file):\n        with file_transaction(paired.tumor_data, out_file) as tx_out_file:\n            cmd = [\"PURPLE\"] + _get_jvm_opts(tx_out_file, paired.tumor_data) + \\\n                  [\"-amber\", os.path.dirname(het_file), \"-baf\", het_file,\n                   \"-cobalt\", os.path.dirname(depth_file),\n                   \"-gc_profile\", dd.get_variation_resources(paired.tumor_data)[\"gc_profile\"],\n                   \"-output_dir\", os.path.dirname(tx_out_file),\n                   \"-ref_genome\", \"hg38\" if dd.get_genome_build(paired.tumor_data) == \"hg38\" else \"hg19\",\n                   \"-run_dir\", work_dir,\n                   \"-threads\", dd.get_num_cores(paired.tumor_data),\n                   \"-tumor_sample\", dd.get_sample_name(paired.tumor_data),\n                   \"-ref_sample\", dd.get_sample_name(paired.normal_data)]\n            if vrn_files:\n                cmd += [\"-somatic_vcf\", vrn_files[0][\"vrn_file\"]]\n            # Avoid X11 display errors when writing plots\n            cmd = \"unset DISPLAY && %s\" % \" \".join([str(x) for x in cmd])\n            do.run(cmd, \"PURPLE: purity and ploidy estimation\")\n            for f in os.listdir(os.path.dirname(tx_out_file)):\n                if f != os.path.basename(tx_out_file):\n                    shutil.move(os.path.join(os.path.dirname(tx_out_file), f),\n                                os.path.join(purple_dir, f))\n    out_file_export = os.path.join(purple_dir, \"%s-purple-cnv.tsv\" % (dd.get_sample_name(paired.tumor_data)))\n    if not utils.file_exists(out_file_export):\n        utils.symlink_plus(out_file, out_file_export)\n    out = {\"variantcaller\": \"purple\", \"call_file\": out_file_export,\n           \"vrn_file\": titancna.to_vcf(out_file_export, \"PURPLE\", _get_header, _export_to_vcf,\n                                       paired.tumor_data),\n           \"plot\": {}, \"metrics\": {}}\n    for name, ext in [(\"copy_number\", \"copyNumber\"), (\"minor_allele\", \"minor_allele\"), (\"variant\", \"variant\")]:\n        plot_file = os.path.join(purple_dir, \"plot\", \"%s.%s.png\" % (dd.get_sample_name(paired.tumor_data), ext))\n        if os.path.exists(plot_file):\n            out[\"plot\"][name] = plot_file\n    purity_file = os.path.join(purple_dir, \"%s.purple.purity\" % dd.get_sample_name(paired.tumor_data))\n    with open(purity_file) as in_handle:\n        header = in_handle.readline().replace(\"#\", \"\").split(\"\\t\")\n        vals = in_handle.readline().split(\"\\t\")\n        for h, v in zip(header, vals):\n            try:\n                v = float(v)\n            except ValueError:\n                pass\n            out[\"metrics\"][h] = v\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a line of CollectAllelicCounts into AMBER line.", "response": "def _counts_to_amber(t_vals, n_vals):\n    \"\"\"Converts a line of CollectAllelicCounts into AMBER line.\n    \"\"\"\n    t_depth = int(t_vals[\"REF_COUNT\"]) + int(t_vals[\"ALT_COUNT\"])\n    n_depth = int(n_vals[\"REF_COUNT\"]) + int(n_vals[\"ALT_COUNT\"])\n    if n_depth > 0 and t_depth > 0:\n        t_baf = float(t_vals[\"ALT_COUNT\"]) / float(t_depth)\n        n_baf = float(n_vals[\"ALT_COUNT\"]) / float(n_depth)\n        return [t_vals[\"CONTIG\"], t_vals[\"POSITION\"], t_baf, _normalize_baf(t_baf), t_depth,\n                n_baf, _normalize_baf(n_baf), n_depth]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert tumor and normal counts from GATK CollectAllelicCounts into Amber format.", "response": "def _count_files_to_amber(tumor_counts, normal_counts, work_dir, data):\n    \"\"\"Converts tumor and normal counts from GATK CollectAllelicCounts into Amber format.\n    \"\"\"\n    amber_dir = utils.safe_makedir(os.path.join(work_dir, \"amber\"))\n    out_file = os.path.join(amber_dir, \"%s.amber.baf\" % dd.get_sample_name(data))\n\n    if not utils.file_uptodate(out_file, tumor_counts):\n        with file_transaction(data, out_file) as tx_out_file:\n            with open(tumor_counts) as tumor_handle:\n                with open(normal_counts) as normal_handle:\n                    with open(tx_out_file, \"w\") as out_handle:\n                        writer = csv.writer(out_handle, delimiter=\"\\t\")\n                        writer.writerow([\"Chromosome\", \"Position\", \"TumorBAF\", \"TumorModifiedBAF\", \"TumorDepth\",\n                                         \"NormalBAF\", \"NormalModifiedBAF\", \"NormalDepth\"])\n                        header = None\n                        for t, n in zip(tumor_handle, normal_handle):\n                            if header is None and t.startswith(\"CONTIG\"):\n                                header = t.strip().split()\n                            elif header is not None:\n                                t_vals = dict(zip(header, t.strip().split()))\n                                n_vals = dict(zip(header, n.strip().split()))\n                                amber_line = _counts_to_amber(t_vals, n_vals)\n                                if amber_line:\n                                    writer.writerow(amber_line)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new AMBER file for the given method and input files.", "response": "def _amber_het_file(method, vrn_files, work_dir, paired):\n    \"\"\"Create file of BAFs in normal heterozygous positions compatible with AMBER.\n\n    Two available methods:\n      - pon -- Use panel of normals with likely heterozygous sites.\n      - variants -- Use pre-existing variant calls, filtered to likely heterozygotes.\n\n    https://github.com/hartwigmedical/hmftools/tree/master/amber\n    https://github.com/hartwigmedical/hmftools/blob/637e3db1a1a995f4daefe2d0a1511a5bdadbeb05/hmf-common/src/test/resources/amber/new.amber.baf\n    \"\"\"\n    assert vrn_files, \"Did not find compatible variant calling files for PURPLE inputs\"\n    from bcbio.heterogeneity import bubbletree\n\n    if method == \"variants\":\n        amber_dir = utils.safe_makedir(os.path.join(work_dir, \"amber\"))\n        out_file = os.path.join(amber_dir, \"%s.amber.baf\" % dd.get_sample_name(paired.tumor_data))\n        prep_file = bubbletree.prep_vrn_file(vrn_files[0][\"vrn_file\"], vrn_files[0][\"variantcaller\"],\n                                             work_dir, paired, AmberWriter)\n        utils.symlink_plus(prep_file, out_file)\n        pcf_file = out_file + \".pcf\"\n        if not utils.file_exists(pcf_file):\n            with file_transaction(paired.tumor_data, pcf_file) as tx_out_file:\n                r_file = os.path.join(os.path.dirname(tx_out_file), \"bafSegmentation.R\")\n                with open(r_file, \"w\") as out_handle:\n                    out_handle.write(_amber_seg_script)\n                cmd = \"%s && %s --no-environ %s %s %s\" % (utils.get_R_exports(), utils.Rscript_cmd(), r_file,\n                                                          out_file, pcf_file)\n                do.run(cmd, \"PURPLE: AMBER baf segmentation\")\n    else:\n        assert method == \"pon\"\n        out_file = _run_amber(paired, work_dir)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns AMBER on the given sample.", "response": "def _run_amber(paired, work_dir, lenient=False):\n    \"\"\"AMBER: calculate allele frequencies at likely heterozygous sites.\n\n    lenient flag allows amber runs on small test sets.\n    \"\"\"\n    amber_dir = utils.safe_makedir(os.path.join(work_dir, \"amber\"))\n    out_file = os.path.join(amber_dir, \"%s.amber.baf\" % dd.get_sample_name(paired.tumor_data))\n    if not utils.file_exists(out_file) or not utils.file_exists(out_file + \".pcf\"):\n        with file_transaction(paired.tumor_data, out_file) as tx_out_file:\n            key = \"germline_het_pon\"\n            het_bed = tz.get_in([\"genome_resources\", \"variation\", key], paired.tumor_data)\n            cmd = [\"AMBER\"] + _get_jvm_opts(tx_out_file, paired.tumor_data) + \\\n                  [\"-threads\", dd.get_num_cores(paired.tumor_data),\n                   \"-tumor\", dd.get_sample_name(paired.tumor_data),\n                   \"-tumor_bam\", dd.get_align_bam(paired.tumor_data),\n                   \"-reference\", dd.get_sample_name(paired.normal_data),\n                   \"-reference_bam\", dd.get_align_bam(paired.normal_data),\n                   \"-ref_genome\", dd.get_ref_file(paired.tumor_data),\n                   \"-bed\", het_bed,\n                   \"-output_dir\", os.path.dirname(tx_out_file)]\n            if lenient:\n                cmd += [\"-max_het_af_percent\", \"1.0\"]\n            try:\n                do.run(cmd, \"PURPLE: AMBER baf generation\")\n            except subprocess.CalledProcessError as msg:\n                if not lenient and _amber_allowed_errors(str(msg)):\n                    return _run_amber(paired, work_dir, True)\n            for f in os.listdir(os.path.dirname(tx_out_file)):\n                if f != os.path.basename(tx_out_file):\n                    shutil.move(os.path.join(os.path.dirname(tx_out_file), f),\n                                os.path.join(amber_dir, f))\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _run_cobalt(paired, work_dir):\n    cobalt_dir = utils.safe_makedir(os.path.join(work_dir, \"cobalt\"))\n    out_file = os.path.join(cobalt_dir, \"%s.cobalt\" % dd.get_sample_name(paired.tumor_data))\n    if not utils.file_exists(out_file):\n        with file_transaction(paired.tumor_data, out_file) as tx_out_file:\n            cmd = [\"COBALT\"] + _get_jvm_opts(tx_out_file, paired.tumor_data) + \\\n                  [\"-reference\", paired.normal_name, \"-reference_bam\", paired.normal_bam,\n                   \"-tumor\", paired.tumor_name, \"-tumor_bam\", paired.tumor_bam,\n                   \"-threads\", dd.get_num_cores(paired.tumor_data),\n                   \"-output_dir\", os.path.dirname(tx_out_file),\n                   \"-gc_profile\", dd.get_variation_resources(paired.tumor_data)[\"gc_profile\"]]\n            cmd = \"%s && %s\" % (utils.get_R_exports(), \" \".join([str(x) for x in cmd]))\n            do.run(cmd, \"PURPLE: COBALT read depth normalization\")\n            for f in os.listdir(os.path.dirname(tx_out_file)):\n                if f != os.path.basename(tx_out_file):\n                    shutil.move(os.path.join(os.path.dirname(tx_out_file), f),\n                                os.path.join(cobalt_dir, f))\n    return out_file", "response": "Run Cobalt for counting read depth across genomic windows."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _cobalt_ratio_file(paired, work_dir):\n    cobalt_dir = utils.safe_makedir(os.path.join(work_dir, \"cobalt\"))\n    out_file = os.path.join(cobalt_dir, \"%s.cobalt\" % dd.get_sample_name(paired.tumor_data))\n    if not utils.file_exists(out_file):\n        cnr_file = tz.get_in([\"depth\", \"bins\", \"normalized\"], paired.tumor_data)\n        with file_transaction(paired.tumor_data, out_file) as tx_out_file:\n            with open(tx_out_file, \"w\") as out_handle:\n                writer = csv.writer(out_handle, delimiter=\"\\t\")\n                writer.writerow([\"Chromosome\", \"Position\", \"ReferenceReadCount\", \"TumorReadCount\",\n                                 \"ReferenceGCRatio\", \"TumorGCRatio\", \"ReferenceGCDiploidRatio\"])\n        raise NotImplementedError\n    return out_file", "response": "Convert CNVkit binning counts into a single CNVkit ratio file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts PURPLE custom output into VCF.", "response": "def _export_to_vcf(cur):\n    \"\"\"Convert PURPLE custom output into VCF.\n    \"\"\"\n    if float(cur[\"copyNumber\"]) > 2.0:\n        svtype = \"DUP\"\n    elif float(cur[\"copyNumber\"]) < 2.0:\n        svtype = \"DEL\"\n    else:\n        svtype = None\n    if svtype:\n        info = [\"END=%s\" % cur[\"end\"], \"SVLEN=%s\" % (int(cur[\"end\"]) - int(cur[\"start\"])),\n                \"SVTYPE=%s\" % svtype, \"CN=%s\" % cur[\"copyNumber\"], \"PROBES=%s\" % cur[\"depthWindowCount\"]]\n        return [cur[\"chromosome\"], cur[\"start\"], \".\", \"N\", \"<%s>\" % svtype, \".\", \".\",\n                \";\".join(info), \"GT\", \"0/1\"]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_fragment_length(data):\n    h5 = kallisto.get_kallisto_h5(data)\n    cutoff = 0.95\n    with h5py.File(h5) as f:\n        x = np.asarray(f['aux']['fld'], dtype='float64')\n    y = np.cumsum(x)/np.sum(x)\n    fraglen = np.argmax(y > cutoff)\n    return(fraglen)", "response": "get fragment length from kallisto data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes a pizzly GTF file for each gene", "response": "def make_pizzly_gtf(gtf_file, out_file, data):\n    \"\"\"\n    pizzly needs the GTF to be in gene -> transcript -> exon order for each\n    gene. it also wants the gene biotype set as the source\n    \"\"\"\n    if file_exists(out_file):\n        return out_file\n    db = gtf.get_gtf_db(gtf_file)\n    with file_transaction(data, out_file) as tx_out_file:\n        with open(tx_out_file, \"w\") as out_handle:\n            for gene in db.features_of_type(\"gene\"):\n                children = [x for x in db.children(id=gene)]\n                for child in children:\n                    if child.attributes.get(\"gene_biotype\", None):\n                        gene_biotype = child.attributes.get(\"gene_biotype\")\n                gene.attributes['gene_biotype'] = gene_biotype\n                gene.source = gene_biotype[0]\n                print(gene, file=out_handle)\n                for child in children:\n                    child.source = gene_biotype[0]\n                    # gffread produces a version-less FASTA file\n                    child.attributes.pop(\"transcript_version\", None)\n                    print(child, file=out_handle)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvalidates a caller VCF against truth within callable regions using SURVIVOR.", "response": "def _validate_caller_vcf(call_vcf, truth_vcf, callable_bed, svcaller, work_dir, data):\n    \"\"\"Validate a caller VCF against truth within callable regions using SURVIVOR.\n\n    Combines files with SURIVOR merge and counts (https://github.com/fritzsedlazeck/SURVIVOR/)\n    \"\"\"\n    stats = _calculate_comparison_stats(truth_vcf)\n    call_vcf = _prep_vcf(call_vcf, callable_bed, dd.get_sample_name(data), dd.get_sample_name(data),\n                         stats, work_dir, data)\n    truth_vcf = _prep_vcf(truth_vcf, callable_bed, vcfutils.get_samples(truth_vcf)[0],\n                          \"%s-truth\" % dd.get_sample_name(data), stats, work_dir, data)\n    cmp_vcf = _survivor_merge(call_vcf, truth_vcf, stats, work_dir, data)\n    return _comparison_stats_from_merge(cmp_vcf, stats, svcaller, data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts true or false positive and negatives from a merged SURIVOR VCF.", "response": "def _comparison_stats_from_merge(in_file, stats, svcaller, data):\n    \"\"\"Extract true/false positive/negatives from a merged SURIVOR VCF.\n    \"\"\"\n    truth_stats = {\"tp\": [], \"fn\": [], \"fp\": []}\n\n    samples = [\"truth\" if x.endswith(\"-truth\") else \"eval\" for x in vcfutils.get_samples(in_file)]\n    with open(in_file) as in_handle:\n        for call in (l.rstrip().split(\"\\t\") for l in in_handle if not l.startswith(\"#\")):\n            supp_vec_str = [x for x in call[7].split(\";\") if x.startswith(\"SUPP_VEC=\")][0]\n            _, supp_vec = supp_vec_str.split(\"=\")\n            calls = dict(zip(samples, [int(x) for x in supp_vec]))\n            if calls[\"truth\"] and calls[\"eval\"]:\n                metric = \"tp\"\n            elif calls[\"truth\"]:\n                metric = \"fn\"\n            else:\n                metric = \"fp\"\n            truth_stats[metric].append(_summarize_call(call))\n    return _to_csv(truth_stats, stats, dd.get_sample_name(data), svcaller)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _survivor_merge(call_vcf, truth_vcf, stats, work_dir, data):\n    out_file = os.path.join(work_dir, \"eval-merge.vcf\")\n    if not utils.file_uptodate(out_file, call_vcf):\n        in_call_vcf = call_vcf.replace(\".vcf.gz\", \".vcf\")\n        if not utils.file_exists(in_call_vcf):\n            with file_transaction(data, in_call_vcf) as tx_in_call_vcf:\n                do.run(\"gunzip -c {call_vcf} > {tx_in_call_vcf}\".format(**locals()))\n        in_truth_vcf = truth_vcf.replace(\".vcf.gz\", \".vcf\")\n        if not utils.file_exists(in_truth_vcf):\n            with file_transaction(data, in_truth_vcf) as tx_in_truth_vcf:\n                do.run(\"gunzip -c {truth_vcf} > {tx_in_truth_vcf}\".format(**locals()))\n        in_list_file = os.path.join(work_dir, \"eval-inputs.txt\")\n        with open(in_list_file, \"w\") as out_handle:\n            out_handle.write(\"%s\\n%s\\n\" % (in_call_vcf, in_truth_vcf))\n        with file_transaction(data, out_file) as tx_out_file:\n            cmd = (\"SURVIVOR merge {in_list_file} {stats[merge_size]} 1 0 0 0 {stats[min_size]} {tx_out_file}\")\n            do.run(cmd.format(**locals()), \"Merge SV files for validation: %s\" % dd.get_sample_name(data))\n    return out_file", "response": "Perform a merge of two callsets using SURVIVOR."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nidentify calls to validate from the input truth VCF.", "response": "def _calculate_comparison_stats(truth_vcf):\n    \"\"\"Identify calls to validate from the input truth VCF.\n    \"\"\"\n    # Avoid very small events for average calculations\n    min_stat_size = 50\n    min_median_size = 250\n    sizes = []\n    svtypes = set([])\n    with utils.open_gzipsafe(truth_vcf) as in_handle:\n        for call in (l.rstrip().split(\"\\t\") for l in in_handle if not l.startswith(\"#\")):\n            stats = _summarize_call(call)\n            if stats[\"size\"] > min_stat_size:\n                sizes.append(stats[\"size\"])\n            svtypes.add(stats[\"svtype\"])\n    pct10 = int(np.percentile(sizes, 10))\n    pct25 = int(np.percentile(sizes, 25))\n    pct50 = int(np.percentile(sizes, 50))\n    pct75 = int(np.percentile(sizes, 75))\n    ranges_detailed = [(int(min(sizes)), pct10), (pct10, pct25), (pct25, pct50),\n                       (pct50, pct75), (pct75, max(sizes))]\n    ranges_split = [(int(min(sizes)), pct50), (pct50, max(sizes))]\n    return {\"min_size\": int(min(sizes) * 0.95), \"max_size\": int(max(sizes) + 1.05),\n            \"svtypes\": svtypes, \"merge_size\": int(np.percentile([x for x in sizes if x > min_median_size], 50)),\n            \"ranges\": []}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve start and end for a VCF record", "response": "def _get_start_end(parts, index=7):\n    \"\"\"Retrieve start and end for a VCF record, skips BNDs without END coords\n    \"\"\"\n    start = parts[1]\n    end = [x.split(\"=\")[-1] for x in parts[index].split(\";\") if x.startswith(\"END=\")]\n    if end:\n        end = end[0]\n        return start, end\n    return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprovide summary metrics on size and svtype for a SV call.", "response": "def _summarize_call(parts):\n    \"\"\"Provide summary metrics on size and svtype for a SV call.\n    \"\"\"\n    svtype = [x.split(\"=\")[1] for x in parts[7].split(\";\") if x.startswith(\"SVTYPE=\")]\n    svtype = svtype[0] if svtype else \"\"\n    start, end = _get_start_end(parts)\n    return {\"svtype\": svtype, \"size\": int(end) - int(start)}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npreparing a VCF for SV validation.", "response": "def _prep_vcf(in_file, region_bed, sample, new_sample, stats, work_dir, data):\n    \"\"\"Prepare VCF for SV validation:\n\n    - Subset to passing variants\n    - Subset to genotyped variants -- removes reference and no calls\n    - Selects and names samples\n    - Subset to callable regions\n    - Remove larger annotations which slow down VCF processing\n    \"\"\"\n    in_file = vcfutils.bgzip_and_index(in_file, data, remove_orig=False)\n    out_file = os.path.join(work_dir, \"%s-vprep.vcf.gz\" % utils.splitext_plus(os.path.basename(in_file))[0])\n    if not utils.file_uptodate(out_file, in_file):\n        callable_bed = _prep_callable_bed(region_bed, work_dir, stats, data)\n        with file_transaction(data, out_file) as tx_out_file:\n            ann_remove = _get_anns_to_remove(in_file)\n            ann_str = \" | bcftools annotate -x {ann_remove}\" if ann_remove else \"\"\n            cmd = (\"bcftools view -T {callable_bed} -f 'PASS,.' --min-ac '1:nref' -s {sample} {in_file} \"\n                   + ann_str +\n                   r\"| sed 's|\\t{sample}|\\t{new_sample}|' \"\n                   \"| bgzip -c > {out_file}\")\n            do.run(cmd.format(**locals()), \"Create SV validation VCF for %s\" % new_sample)\n    return vcfutils.bgzip_and_index(out_file, data[\"config\"])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsorts and merge callable BED regions to prevent SV double counting.", "response": "def _prep_callable_bed(in_file, work_dir, stats, data):\n    \"\"\"Sort and merge callable BED regions to prevent SV double counting\n    \"\"\"\n    out_file = os.path.join(work_dir, \"%s-merge.bed.gz\" % utils.splitext_plus(os.path.basename(in_file))[0])\n    gsort = config_utils.get_program(\"gsort\", data)\n    if not utils.file_uptodate(out_file, in_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            fai_file = ref.fasta_idx(dd.get_ref_file(data))\n            cmd = (\"{gsort} {in_file} {fai_file} | bedtools merge -i - -d {stats[merge_size]} | \"\n                   \"bgzip -c > {tx_out_file}\")\n            do.run(cmd.format(**locals()), \"Prepare SV callable BED regions\")\n    return vcfutils.bgzip_and_index(out_file, data[\"config\"])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_anns_to_remove(in_file):\n    to_remove = [\"ANN\", \"LOF\"]\n    to_remove_str = tuple([\"##INFO=<ID=%s\" % x for x in to_remove])\n    cur_remove = []\n    with utils.open_gzipsafe(in_file) as in_handle:\n        for line in in_handle:\n            if not line.startswith(\"#\"):\n                break\n            elif line.startswith(to_remove_str):\n                cur_id = line.split(\"ID=\")[-1].split(\",\")[0]\n                cur_remove.append(\"INFO/%s\" % cur_id)\n    return \",\".join(cur_remove)", "response": "Find larger annotations that slow down processing."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a CNV to an event name.", "response": "def cnv_to_event(name, data):\n    \"\"\"Convert a CNV to an event name.\n    \"\"\"\n    cur_ploidy = ploidy.get_ploidy([data])\n    if name.startswith(\"cnv\"):\n        num = max([int(x) for x in name.split(\"_\")[0].replace(\"cnv\", \"\").split(\";\")])\n        if num < cur_ploidy:\n            return \"DEL\"\n        elif num > cur_ploidy:\n            return \"DUP\"\n        else:\n            return name\n    else:\n        return name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nevaluate a single ensemble result against a specific caller and SV type.", "response": "def _evaluate_one(caller, svtype, size_range, ensemble, truth, data):\n    \"\"\"Compare a ensemble results for a caller against a specific caller and SV type.\n    \"\"\"\n    def cnv_matches(name):\n        return cnv_to_event(name, data) == svtype\n    def is_breakend(name):\n        return name.startswith(\"BND\")\n    def in_size_range(max_buffer=0):\n        def _work(feat):\n            minf, maxf = size_range\n            buffer = min(max_buffer, int(((maxf + minf) / 2.0) / 10.0))\n            size = feat.end - feat.start\n            return size >= max([0, minf - buffer]) and size < maxf + buffer\n        return _work\n    def is_caller_svtype(feat):\n        for name in feat.name.split(\",\"):\n            if ((name.startswith(svtype) or cnv_matches(name) or is_breakend(name))\n                  and (caller == \"sv-ensemble\" or name.endswith(caller))):\n                return True\n        return False\n    minf, maxf = size_range\n    efeats = pybedtools.BedTool(ensemble).filter(in_size_range(0)).filter(is_caller_svtype).saveas().sort().merge()\n    tfeats = pybedtools.BedTool(truth).filter(in_size_range(0)).sort().merge().saveas()\n    etotal = efeats.count()\n    ttotal = tfeats.count()\n    match = efeats.intersect(tfeats, u=True).sort().merge().saveas().count()\n    return {\"sensitivity\": _stat_str(match, ttotal),\n            \"precision\": _stat_str(match, etotal)}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _plot_evaluation_event(df_csv, svtype):\n    titles = {\"INV\": \"Inversions\", \"DEL\": \"Deletions\", \"DUP\": \"Duplications\",\n              \"INS\": \"Insertions\"}\n    out_file = \"%s-%s.png\" % (os.path.splitext(df_csv)[0], svtype)\n    sns.set(style='white')\n    if not utils.file_uptodate(out_file, df_csv):\n        metrics = [\"sensitivity\", \"precision\"]\n        df = pd.read_csv(df_csv).fillna(\"0%\")\n        df = df[(df[\"svtype\"] == svtype)]\n        event_sizes = _find_events_to_include(df, EVENT_SIZES)\n        fig, axs = plt.subplots(len(event_sizes), len(metrics), tight_layout=True)\n        if len(event_sizes) == 1:\n            axs = [axs]\n        callers = sorted(df[\"caller\"].unique())\n        if \"sv-ensemble\" in callers:\n            callers.remove(\"sv-ensemble\")\n            callers.append(\"sv-ensemble\")\n        for i, size in enumerate(event_sizes):\n            size_label = \"%s to %sbp\" % size\n            size = \"%s-%s\" % size\n            for j, metric in enumerate(metrics):\n                ax = axs[i][j]\n                ax.get_xaxis().set_ticks([])\n                ax.spines['bottom'].set_visible(False)\n                ax.spines['left'].set_visible(False)\n                ax.spines['top'].set_visible(False)\n                ax.spines['right'].set_visible(False)\n                ax.set_xlim(0, 125.0)\n                if i == 0:\n                    ax.set_title(metric, size=12, y=1.2)\n                vals, labels = _get_plot_val_labels(df, size, metric, callers)\n                ax.barh(range(1,len(vals)+1), vals)\n                if j == 0:\n                    ax.tick_params(axis='y', which='major', labelsize=8)\n                    ax.locator_params(axis=\"y\", tight=True)\n                    ax.set_yticks(range(1,len(callers)+1,1))\n                    ax.set_yticklabels(callers, va=\"center\")\n                    ax.text(100, len(callers)+1, size_label, fontsize=10)\n                else:\n                    ax.get_yaxis().set_ticks([])\n                for ai, (val, label) in enumerate(zip(vals, labels)):\n                    ax.annotate(label, (val + 0.75, ai + 1), va='center', size=7)\n        if svtype in titles:\n            fig.text(0.025, 0.95, titles[svtype], size=14)\n        fig.set_size_inches(7, len(event_sizes) + 1)\n        fig.savefig(out_file)\n    return out_file", "response": "Provide plot of evaluation metrics for an SV event."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nproviding evaluations for multiple callers split by structural variant type.", "response": "def evaluate(data):\n    \"\"\"Provide evaluations for multiple callers split by structural variant type.\n    \"\"\"\n    work_dir = utils.safe_makedir(os.path.join(data[\"dirs\"][\"work\"], \"structural\",\n                                               dd.get_sample_name(data), \"validate\"))\n    truth_sets = tz.get_in([\"config\", \"algorithm\", \"svvalidate\"], data)\n    if truth_sets and data.get(\"sv\"):\n        if isinstance(truth_sets, dict):\n            val_summary, df_csv = _evaluate_multi(data[\"sv\"], truth_sets, work_dir, data)\n            summary_plots = _plot_evaluation(df_csv)\n            data[\"sv-validate\"] = {\"csv\": val_summary, \"plot\": summary_plots, \"df\": df_csv}\n        else:\n            assert isinstance(truth_sets, six.string_types) and utils.file_exists(truth_sets), truth_sets\n            val_summary = _evaluate_vcf(data[\"sv\"], truth_sets, work_dir, data)\n            title = \"%s structural variants\" % dd.get_sample_name(data)\n            summary_plots = validateplot.classifyplot_from_valfile(val_summary, outtype=\"png\", title=title)\n            data[\"sv-validate\"] = {\"csv\": val_summary, \"plot\": summary_plots[0] if len(summary_plots) > 0 else None}\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_tumor_params(paired, items, gatk_type):\n    params = []\n    if not paired:\n        raise ValueError(\"Specified MuTect2 calling but 'tumor' phenotype not present in batch\\n\"\n                         \"https://bcbio-nextgen.readthedocs.org/en/latest/contents/\"\n                         \"pipelines.html#cancer-variant-calling\\n\"\n                         \"for samples: %s\" % \", \" .join([dd.get_sample_name(x) for x in items]))\n    if gatk_type == \"gatk4\":\n        params += [\"-I\", paired.tumor_bam]\n        params += [\"--tumor-sample\", paired.tumor_name]\n    else:\n        params += [\"-I:tumor\", paired.tumor_bam]\n    if paired.normal_bam is not None:\n        if gatk_type == \"gatk4\":\n            params += [\"-I\", paired.normal_bam]\n            params += [\"--normal-sample\", paired.normal_name]\n        else:\n            params += [\"-I:normal\", paired.normal_bam]\n    if paired.normal_panel is not None:\n        panel_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(items[0]), \"mutect2\", \"panels\"))\n        normal_panel = vcfutils.bgzip_and_index(paired.normal_panel, items[0][\"config\"], out_dir=panel_dir)\n        if gatk_type == \"gatk4\":\n            params += [\"--panel-of-normals\", normal_panel]\n        else:\n            params += [\"--normal_panel\", normal_panel]\n    return params", "response": "Add tumor and normal BAM input parameters to command line."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds parameters for selecting by region to command line.", "response": "def _add_region_params(region, out_file, items, gatk_type):\n    \"\"\"Add parameters for selecting by region to command line.\n    \"\"\"\n    params = []\n    variant_regions = bedutils.population_variant_regions(items)\n    region = subset_variant_regions(variant_regions, region, out_file, items)\n    if region:\n        if gatk_type == \"gatk4\":\n            params += [\"-L\", bamprep.region_to_gatk(region), \"--interval-set-rule\", \"INTERSECTION\"]\n        else:\n            params += [\"-L\", bamprep.region_to_gatk(region), \"--interval_set_rule\", \"INTERSECTION\"]\n    params += gatk.standard_cl_params(items)\n    return params"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nensure inputs to calling are indexed as expected.", "response": "def _prep_inputs(align_bams, ref_file, items):\n    \"\"\"Ensure inputs to calling are indexed as expected.\n    \"\"\"\n    broad_runner = broad.runner_from_path(\"picard\", items[0][\"config\"])\n    broad_runner.run_fn(\"picard_index_ref\", ref_file)\n    for x in align_bams:\n        bam.index(x, items[0][\"config\"])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalling variation with GATK s MuTect2.", "response": "def mutect2_caller(align_bams, items, ref_file, assoc_files,\n                       region=None, out_file=None):\n    \"\"\"Call variation with GATK's MuTect2.\n\n    This requires the full non open-source version of GATK 3.5+.\n    \"\"\"\n    if out_file is None:\n        out_file = \"%s-variants.vcf.gz\" % utils.splitext_plus(align_bams[0])[0]\n    if not utils.file_exists(out_file):\n        paired = vcfutils.get_paired_bams(align_bams, items)\n        broad_runner = broad.runner_from_config(items[0][\"config\"])\n        gatk_type = broad_runner.gatk_type()\n        _prep_inputs(align_bams, ref_file, items)\n        with file_transaction(items[0], out_file) as tx_out_file:\n            params = [\"-T\", \"Mutect2\" if gatk_type == \"gatk4\" else \"MuTect2\",\n                      \"--annotation\", \"ClippingRankSumTest\",\n                      \"--annotation\", \"DepthPerSampleHC\"]\n            if gatk_type == \"gatk4\":\n                params += [\"--reference\", ref_file]\n            else:\n                params += [\"-R\", ref_file]\n            for a in annotation.get_gatk_annotations(items[0][\"config\"], include_baseqranksum=False):\n                params += [\"--annotation\", a]\n            # Avoid issues with BAM CIGAR reads that GATK doesn't like\n            if gatk_type == \"gatk4\":\n                params += [\"--read-validation-stringency\", \"LENIENT\"]\n            params += _add_tumor_params(paired, items, gatk_type)\n            params += _add_region_params(region, out_file, items, gatk_type)\n            # Avoid adding dbSNP/Cosmic so they do not get fed to variant filtering algorithm\n            # Not yet clear how this helps or hurts in a general case.\n            #params += _add_assoc_params(assoc_files)\n            resources = config_utils.get_resources(\"mutect2\", items[0][\"config\"])\n            if \"options\" in resources:\n                params += [str(x) for x in resources.get(\"options\", [])]\n            assert LooseVersion(broad_runner.gatk_major_version()) >= LooseVersion(\"3.5\"), \\\n                \"Require full version of GATK 3.5+ for mutect2 calling\"\n            broad_runner.new_resources(\"mutect2\")\n            gatk_cmd = broad_runner.cl_gatk(params, os.path.dirname(tx_out_file))\n            if gatk_type == \"gatk4\":\n                tx_raw_prefilt_file = \"%s-raw%s\" % utils.splitext_plus(tx_out_file)\n                tx_raw_file = \"%s-raw-filt%s\" % utils.splitext_plus(tx_out_file)\n                filter_cmd = _mutect2_filter(broad_runner, tx_raw_prefilt_file, tx_raw_file, ref_file)\n                cmd = \"{gatk_cmd} -O {tx_raw_prefilt_file} && {filter_cmd}\"\n            else:\n                tx_raw_file = \"%s-raw%s\" % utils.splitext_plus(tx_out_file)\n                cmd = \"{gatk_cmd} > {tx_raw_file}\"\n            do.run(cmd.format(**locals()), \"MuTect2\")\n            out_file = _af_filter(paired.tumor_data, tx_raw_file, out_file)\n    return vcfutils.bgzip_and_index(out_file, items[0][\"config\"])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _mutect2_filter(broad_runner, in_file, out_file, ref_file):\n    params = [\"-T\", \"FilterMutectCalls\", \"--reference\", ref_file, \"--variant\", in_file, \"--output\", out_file]\n    return broad_runner.cl_gatk(params, os.path.dirname(out_file))", "response": "Filter of MuTect2 calls."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _af_filter(data, in_file, out_file):\n    min_freq = float(utils.get_in(data[\"config\"], (\"algorithm\", \"min_allele_fraction\"), 10)) / 100.0\n    logger.debug(\"Filtering MuTect2 calls with allele fraction threshold of %s\" % min_freq)\n    ungz_out_file = \"%s.vcf\" % utils.splitext_plus(out_file)[0]\n    if not utils.file_exists(ungz_out_file) and not utils.file_exists(ungz_out_file + \".gz\"):\n        with file_transaction(data, ungz_out_file) as tx_out_file:\n            vcf = cyvcf2.VCF(in_file)\n            vcf.add_filter_to_header({\n                'ID': 'MinAF',\n                'Description': 'Allele frequency is lower than %s%% ' % (min_freq*100) + (\n                    '(configured in bcbio as min_allele_fraction)'\n                    if utils.get_in(data[\"config\"], (\"algorithm\", \"min_allele_fraction\"))\n                    else '(default threshold in bcbio; override with min_allele_fraction in the algorithm section)')})\n            w = cyvcf2.Writer(tx_out_file, vcf)\n            # GATK 3.x can produce VCFs without sample names for empty VCFs\n            try:\n                tumor_index = vcf.samples.index(dd.get_sample_name(data))\n            except ValueError:\n                tumor_index = None\n            for rec in vcf:\n                if tumor_index is not None and np.all(rec.format('AF')[tumor_index] < min_freq):\n                    vcfutils.cyvcf_add_filter(rec, 'MinAF')\n                w.write_record(rec)\n            w.close()\n    return vcfutils.bgzip_and_index(ungz_out_file, data[\"config\"])", "response": "Filter variants with AF below min_allele_fraction."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the file to an iRODS repository.", "response": "def update_file(finfo, sample_info, config):\n    \"\"\"\n    Update the file to an iRODS repository.\n    \"\"\"\n    ffinal = filesystem.update_file(finfo, sample_info, config, pass_uptodate=True)\n\n    _upload_dir_icommands_cli(config.get(\"dir\"), config.get(\"folder\"), config)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _upload_dir_icommands_cli(local_dir, irods_dir, config=None, metadata=None):\n\n    args = [\"-K\",\"-v\",\"-a\",\"-r\"]\n    if config:\n        if config.get(\"resource\"):\n            args += [\"-R\", config.get(\"resource\")]\n\n    _check_create_collection(irods_dir,isdir=True)\n    cmd = [\"irsync\"] + args + [local_dir, \"i:\"+irods_dir]\n    do.run(cmd, \"Uploading to iRODS\")", "response": "Upload directory recursively via the standard icommands CLI."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving available callers for the provided stage.", "response": "def _get_callers(items, stage, special_cases=False):\n    \"\"\"Retrieve available callers for the provided stage.\n\n    Handles special cases like CNVkit that can be in initial or standard\n    depending on if fed into Lumpy analysis.\n    \"\"\"\n    callers = utils.deepish_copy(_CALLERS[stage])\n    if special_cases and \"cnvkit\" in callers:\n        has_lumpy = any(\"lumpy\" in get_svcallers(d) or \"lumpy\" in d[\"config\"][\"algorithm\"].get(\"svcaller_orig\", [])\n                        for d in items)\n        if has_lumpy and any(\"lumpy_usecnv\" in dd.get_tools_on(d) for d in items):\n            if stage != \"initial\":\n                del callers[\"cnvkit\"]\n        else:\n            if stage != \"standard\":\n                del callers[\"cnvkit\"]\n    return callers"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve configured structural variation caller handling multiple.", "response": "def _handle_multiple_svcallers(data, stage):\n    \"\"\"Retrieve configured structural variation caller, handling multiple.\n    \"\"\"\n    svs = get_svcallers(data)\n    # special cases -- prioritization\n    if stage == \"ensemble\" and dd.get_svprioritize(data):\n        svs.append(\"prioritize\")\n    out = []\n    for svcaller in svs:\n        if svcaller in _get_callers([data], stage):\n            base = copy.deepcopy(data)\n            # clean SV callers present in multiple rounds and not this caller\n            final_svs = []\n            for sv in data.get(\"sv\", []):\n                if (stage == \"ensemble\" or sv[\"variantcaller\"] == svcaller or sv[\"variantcaller\"] not in svs\n                      or svcaller not in _get_callers([data], stage, special_cases=True)):\n                    final_svs.append(sv)\n            base[\"sv\"] = final_svs\n            base[\"config\"][\"algorithm\"][\"svcaller\"] = svcaller\n            base[\"config\"][\"algorithm\"][\"svcaller_orig\"] = svs\n            out.append(base)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef finalize_sv(samples, config):\n    by_bam = collections.OrderedDict()\n    for x in samples:\n        batch = dd.get_batch(x) or [dd.get_sample_name(x)]\n        try:\n            by_bam[x[\"align_bam\"], tuple(batch)].append(x)\n        except KeyError:\n            by_bam[x[\"align_bam\"], tuple(batch)] = [x]\n    by_batch = collections.OrderedDict()\n    lead_batches = {}\n    for grouped_calls in by_bam.values():\n        def orig_svcaller_order(x):\n            orig_callers = tz.get_in([\"config\", \"algorithm\", \"svcaller_orig\"], x)\n            cur_caller = tz.get_in([\"config\", \"algorithm\", \"svcaller\"], x)\n            return orig_callers.index(cur_caller)\n        sorted_svcalls = sorted([x for x in grouped_calls if \"sv\" in x],\n                                key=orig_svcaller_order)\n        final = grouped_calls[0]\n        if len(sorted_svcalls) > 0:\n            final[\"sv\"] = reduce(operator.add, [x[\"sv\"] for x in sorted_svcalls])\n        final[\"config\"][\"algorithm\"][\"svcaller\"] = final[\"config\"][\"algorithm\"].pop(\"svcaller_orig\")\n        batch = dd.get_batch(final) or dd.get_sample_name(final)\n        batches = batch if isinstance(batch, (list, tuple)) else [batch]\n        if len(batches) > 1:\n            lead_batches[(dd.get_sample_name(final), dd.get_phenotype(final) == \"germline\")] = batches[0]\n        for batch in batches:\n            try:\n                by_batch[batch].append(final)\n            except KeyError:\n                by_batch[batch] = [final]\n    out = []\n    for batch, items in by_batch.items():\n        if any(\"svplots\" in dd.get_tools_on(d) for d in items):\n            items = plot.by_regions(items)\n        for data in items:\n            if lead_batches.get((dd.get_sample_name(data), dd.get_phenotype(data) == \"germline\")) in [batch, None]:\n                out.append([data])\n    return out", "response": "Combine results from multiple sv callers into a single ordered sv key."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprepares a set of samples for parallel structural variant calling.", "response": "def batch_for_sv(samples):\n    \"\"\"Prepare a set of samples for parallel structural variant calling.\n\n    CWL input target -- groups samples into batches and structural variant\n    callers for parallel processing.\n    \"\"\"\n    samples = cwlutils.assign_complex_to_samples(samples)\n    to_process, extras, background = _batch_split_by_sv(samples, \"standard\")\n    out = [cwlutils.samples_to_records(xs) for xs in to_process.values()] + extras\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(samples, run_parallel, stage):\n    to_process, extras, background = _batch_split_by_sv(samples, stage)\n    processed = run_parallel(\"detect_sv\", ([xs, background, stage]\n                                           for xs in to_process.values()))\n    finalized = (run_parallel(\"finalize_sv\", [([xs[0] for xs in processed], processed[0][0][\"config\"])])\n                 if len(processed) > 0 else [])\n    return extras + finalized", "response": "Run structural variation detection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetect structural variation for a given set of items.", "response": "def detect_sv(items, all_items=None, stage=\"standard\"):\n    \"\"\"Top level parallel target for examining structural variation.\n    \"\"\"\n    items = [utils.to_single_data(x) for x in items]\n    items = cwlutils.unpack_tarballs(items, items[0])\n    svcaller = items[0][\"config\"][\"algorithm\"].get(\"svcaller\")\n    caller_fn = _get_callers(items, stage, special_cases=True).get(svcaller)\n    out = []\n    if svcaller and caller_fn:\n        if (all_items and svcaller in _NEEDS_BACKGROUND and\n                not vcfutils.is_paired_analysis([x.get(\"align_bam\") for x in items], items)):\n            names = set([dd.get_sample_name(x) for x in items])\n            background = [x for x in all_items if dd.get_sample_name(x) not in names]\n            for svdata in caller_fn(items, background):\n                out.append([svdata])\n        else:\n            for svdata in caller_fn(items):\n                out.append([svdata])\n    else:\n        for data in items:\n            out.append([data])\n    # Avoid nesting of callers for CWL runs for easier extraction\n    if cwlutils.is_cwl_run(items[0]):\n        out_cwl = []\n        for data in [utils.to_single_data(x) for x in out]:\n            # Run validation directly from CWL runs since we're single stage\n            data = validate.evaluate(data)\n            data[\"svvalidate\"] = {\"summary\": tz.get_in([\"sv-validate\", \"csv\"], data)}\n            svs = data.get(\"sv\")\n            if svs:\n                assert len(svs) == 1, svs\n                data[\"sv\"] = svs[0]\n            else:\n                data[\"sv\"] = {}\n            data = _add_supplemental(data)\n            out_cwl.append([data])\n        return out_cwl\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add_supplemental(data):\n    if \"supplemental\" not in data[\"sv\"]:\n        data[\"sv\"][\"supplemental\"] = []\n    if data[\"sv\"].get(\"variantcaller\"):\n        cur_name = _useful_basename(data)\n        for k in [\"cns\", \"vrn_bed\"]:\n            if data[\"sv\"].get(k) and os.path.exists(data[\"sv\"][k]):\n                dname, orig = os.path.split(data[\"sv\"][k])\n                orig_base, orig_ext = utils.splitext_plus(orig)\n                orig_base = _clean_name(orig_base, data)\n                if orig_base:\n                    fname = \"%s-%s%s\" % (cur_name, orig_base, orig_ext)\n                else:\n                    fname = \"%s%s\" % (cur_name, orig_ext)\n                sup_out_file = os.path.join(dname, fname)\n                utils.symlink_plus(data[\"sv\"][k], sup_out_file)\n                data[\"sv\"][\"supplemental\"].append(sup_out_file)\n    return data", "response": "Add additional supplemental files to CWL sv output give useful names."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves standard prefixes from a filename before renaming with useful names.", "response": "def _clean_name(fname, data):\n    \"\"\"Remove standard prefixes from a filename before renaming with useful names.\n    \"\"\"\n    for to_remove in dd.get_batches(data) + [dd.get_sample_name(data), data[\"sv\"][\"variantcaller\"]]:\n        for ext in (\"-\", \"_\"):\n            if fname.startswith(\"%s%s\" % (to_remove, ext)):\n                fname = fname[len(to_remove) + len(ext):]\n        if fname.startswith(to_remove):\n            fname = fname[len(to_remove):]\n    return fname"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _useful_basename(data):\n    names = dd.get_batches(data)\n    if not names:\n        names = [dd.get_sample_name(data)]\n    batch_name = names[0]\n    return \"%s-%s\" % (batch_name, data[\"sv\"][\"variantcaller\"])", "response": "Provide a useful file basename for outputs referencing batch and sample and caller."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngroups a set of items by sample names + multiple callers for prioritization", "response": "def _group_by_sample(items):\n    \"\"\"Group a set of items by sample names + multiple callers for prioritization\n    \"\"\"\n    by_sample = collections.defaultdict(list)\n    for d in items:\n        by_sample[dd.get_sample_name(d)].append(d)\n    out = []\n    for sample_group in by_sample.values():\n        cur = utils.deepish_copy(sample_group[0])\n        svs = []\n        for d in sample_group:\n            svs.append(d[\"sv\"])\n        cur[\"sv\"] = svs\n        out.append(cur)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstandardizes cnv_reference background to support multiple callers.", "response": "def standardize_cnv_reference(data):\n    \"\"\"Standardize cnv_reference background to support multiple callers.\n    \"\"\"\n    out = tz.get_in([\"config\", \"algorithm\", \"background\", \"cnv_reference\"], data, {})\n    cur_callers = set(data[\"config\"][\"algorithm\"].get(\"svcaller\")) & _CNV_REFERENCE\n    if isinstance(out, six.string_types):\n        if not len(cur_callers) == 1:\n            raise ValueError(\"Multiple CNV callers and single background reference for %s: %s\" %\n                                data[\"description\"], list(cur_callers))\n        else:\n            out = {cur_callers.pop(): out}\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nusing more resources up to available limits for multiple QC samples.", "response": "def parallel_multiplier(items):\n    \"\"\"Use more resources (up to available limits) if we have multiple QC samples/svcallers.\n    \"\"\"\n    machines = []\n    for data in (xs[0] for xs in items):\n        machines.append(max(1, len(get_svcallers(data)), len(dd.get_algorithm_qc(data))))\n    return sum(machines)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef align(fastq_file, pair_file, ref_file, names, align_dir, data,\n          extra_args=None):\n    \"\"\"Alignment with bowtie2.\n    \"\"\"\n    config = data[\"config\"]\n    analysis_config = ANALYSIS.get(data[\"analysis\"].lower())\n    assert analysis_config, \"Analysis %s is not supported by bowtie2\" % (data[\"analysis\"])\n    out_file = os.path.join(align_dir, \"{0}-sort.bam\".format(dd.get_sample_name(data)))\n    if data.get(\"align_split\"):\n        final_file = out_file\n        out_file, data = alignprep.setup_combine(final_file, data)\n        fastq_file, pair_file = alignprep.split_namedpipe_cls(fastq_file, pair_file, data)\n    else:\n        final_file = None\n    if not utils.file_exists(out_file) and (final_file is None or not utils.file_exists(final_file)):\n        with postalign.tobam_cl(data, out_file, pair_file is not None) as (tobam_cl, tx_out_file):\n            cl = [config_utils.get_program(\"bowtie2\", config)]\n            cl += extra_args if extra_args is not None else []\n            cl += [\"-q\",\n                   \"-x\", ref_file]\n            cl += analysis_config.get(\"params\", [])\n            if pair_file:\n                cl += [\"-1\", fastq_file, \"-2\", pair_file]\n            else:\n                cl += [\"-U\", fastq_file]\n            if names and \"rg\" in names:\n                cl += [\"--rg-id\", names[\"rg\"]]\n                for key, tag in [(\"sample\", \"SM\"), (\"pl\", \"PL\"), (\"pu\", \"PU\"), (\"lb\", \"LB\")]:\n                    if names.get(key):\n                        cl += [\"--rg\", \"%s:%s\" % (tag, names[key])]\n            cl += _bowtie2_args_from_config(config, cl)\n            cl = [str(i) for i in cl]\n            cmd = \"unset JAVA_HOME && \" + \" \".join(cl) + \" | \" + tobam_cl\n            do.run(cmd, \"Aligning %s and %s with Bowtie2.\" % (fastq_file, pair_file))\n    return out_file", "response": "Align fastq_file with bowtie2."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a bowtie2 index for a transcriptome", "response": "def index_transcriptome(gtf_file, ref_file, data):\n    \"\"\"\n    use a GTF file and a reference FASTA file to index the transcriptome\n    \"\"\"\n    gtf_fasta = gtf.gtf_to_fasta(gtf_file, ref_file)\n    bowtie2_index = os.path.splitext(gtf_fasta)[0]\n    bowtie2_build = config_utils.get_program(\"bowtie2\", data[\"config\"]) + \"-build\"\n    cmd = \"{bowtie2_build} --offrate 1 {gtf_fasta} {bowtie2_index}\".format(**locals())\n    message = \"Creating transcriptome index of %s with bowtie2.\" % (gtf_fasta)\n    do.run(cmd, message)\n    return bowtie2_index"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nalign a fastq file with a pair of transcriptomes.", "response": "def align_transcriptome(fastq_file, pair_file, ref_file, data):\n    \"\"\"\n    bowtie2 with settings for aligning to the transcriptome for eXpress/RSEM/etc\n    \"\"\"\n    work_bam = dd.get_work_bam(data)\n    base, ext = os.path.splitext(work_bam)\n    out_file = base + \".transcriptome\" + ext\n    if utils.file_exists(out_file):\n        data = dd.set_transcriptome_bam(data, out_file)\n        return data\n    bowtie2 = config_utils.get_program(\"bowtie2\", data[\"config\"])\n    gtf_file = dd.get_gtf_file(data)\n    gtf_index = index_transcriptome(gtf_file, ref_file, data)\n    num_cores = data[\"config\"][\"algorithm\"].get(\"num_cores\", 1)\n    fastq_cmd = \"-1 %s\" % fastq_file if pair_file else \"-U %s\" % fastq_file\n    pair_cmd = \"-2 %s \" % pair_file if pair_file else \"\"\n    cmd = (\"{bowtie2} -p {num_cores} -a -X 600 --rdg 6,5 --rfg 6,5 --score-min L,-.6,-.4 --no-discordant --no-mixed -x {gtf_index} {fastq_cmd} {pair_cmd} \")\n    with file_transaction(data, out_file) as tx_out_file:\n        message = \"Aligning %s and %s to the transcriptome.\" % (fastq_file, pair_file)\n        cmd += \"| \" + postalign.sam_to_sortbam_cl(data, tx_out_file, name_sort=True)\n        do.run(cmd.format(**locals()), message)\n    data = dd.set_transcriptome_bam(data, out_file)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_cromwell_config(args, work_dir, sample_file):\n    docker_attrs = [\"String? docker\", \"String? docker_user\"]\n    cwl_attrs = [\"Int? cpuMin\", \"Int? cpuMax\", \"Int? memoryMin\", \"Int? memoryMax\", \"String? outDirMin\",\n                 \"String? outDirMax\", \"String? tmpDirMin\", \"String? tmpDirMax\"]\n    out_file = os.path.join(work_dir, \"bcbio-cromwell.conf\")\n    run_config = _load_custom_config(args.runconfig) if args.runconfig else {}\n    # Avoid overscheduling jobs for local runs by limiting concurrent jobs\n    # Longer term would like to keep these within defined core window\n    joblimit = args.joblimit\n    if joblimit == 0 and not args.scheduler:\n        joblimit = 1\n    file_types = _get_filesystem_types(args, sample_file)\n    std_args = {\"docker_attrs\": \"\" if args.no_container else \"\\n        \".join(docker_attrs),\n                \"submit_docker\": 'submit-docker: \"\"' if args.no_container else \"\",\n                \"joblimit\": \"concurrent-job-limit = %s\" % (joblimit) if joblimit > 0 else \"\",\n                \"cwl_attrs\": \"\\n        \".join(cwl_attrs),\n                \"filesystem\": _get_filesystem_config(file_types),\n                \"database\": run_config.get(\"database\", DATABASE_CONFIG % {\"work_dir\": work_dir})}\n    cl_args, conf_args, scheduler, cloud_type = _args_to_cromwell(args)\n    std_args[\"engine\"] = _get_engine_filesystem_config(file_types, args, conf_args)\n    conf_args.update(std_args)\n    main_config = {\"hpc\": (HPC_CONFIGS[scheduler] % conf_args) if scheduler else \"\",\n                   \"cloud\": (CLOUD_CONFIGS[cloud_type] % conf_args) if cloud_type else \"\",\n                   \"work_dir\": work_dir}\n    main_config.update(std_args)\n    # Local run always seems to need docker set because of submit-docker in default configuration\n    # Can we unset submit-docker based on configuration so it doesn't inherit?\n    # main_config[\"docker_attrs\"] = \"\\n        \".join(docker_attrs)\n    with open(out_file, \"w\") as out_handle:\n        out_handle.write(CROMWELL_CONFIG % main_config)\n    return out_file", "response": "Create a cromwell configuration within the current working directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving a list of file paths recursively traversing the", "response": "def _get_file_paths(cur):\n    \"\"\"Retrieve a list of file paths, recursively traversing the\n    \"\"\"\n    out = []\n    if isinstance(cur, (list, tuple)):\n        for x in cur:\n            new = _get_file_paths(x)\n            if new:\n                out.extend(new)\n    elif isinstance(cur, dict):\n        if \"class\" in cur:\n            out.append(cur[\"path\"])\n        else:\n            for k, v in cur.items():\n                new = _get_file_paths(v)\n                if new:\n                    out.extend(new)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _load_custom_config(run_config):\n    from pyhocon import ConfigFactory, HOCONConverter, ConfigTree\n    conf = ConfigFactory.parse_file(run_config)\n    out = {}\n    if \"database\" in conf:\n        out[\"database\"] = HOCONConverter.to_hocon(ConfigTree({\"database\": conf.get_config(\"database\")}))\n    return out", "response": "Load custom configuration input HOCON file for cromwell."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts input arguments into cromwell inputs for config and command line.", "response": "def _args_to_cromwell(args):\n    \"\"\"Convert input arguments into cromwell inputs for config and command line.\n    \"\"\"\n    default_config = {\"slurm\": {\"timelimit\": \"1-00:00\", \"account\": \"\"},\n                      \"sge\": {\"memtype\": \"mem_free\", \"pename\": \"smp\"},\n                      \"lsf\": {\"walltime\": \"24:00\", \"account\": \"\"},\n                      \"htcondor\": {},\n                      \"torque\": {\"walltime\": \"24:00:00\", \"account\": \"\"},\n                      \"pbspro\": {\"walltime\": \"24:00:00\", \"account\": \"\",\n                                 \"cpu_and_mem\": \"-l select=1:ncpus=${cpu}:mem=${memory_mb}mb\"}}\n    prefixes = {(\"account\", \"slurm\"): \"-A \", (\"account\", \"pbspro\"): \"-A \"}\n    custom = {(\"noselect\", \"pbspro\"): (\"cpu_and_mem\", \"-l ncpus=${cpu} -l mem=${memory_mb}mb\")}\n    cl = []\n    config = {}\n    # HPC scheduling\n    if args.scheduler:\n        if args.scheduler not in default_config:\n            raise ValueError(\"Scheduler not yet supported by Cromwell: %s\" % args.scheduler)\n        if not args.queue and args.scheduler not in [\"htcondor\"]:\n            raise ValueError(\"Need to set queue (-q) for running with an HPC scheduler\")\n        config = default_config[args.scheduler]\n        cl.append(\"-Dbackend.default=%s\" % args.scheduler.upper())\n        config[\"queue\"] = args.queue\n        for rs in args.resources:\n            for r in rs.split(\";\"):\n                parts = r.split(\"=\")\n                if len(parts) == 2:\n                    key, val = parts\n                    config[key] = prefixes.get((key, args.scheduler), \"\") + val\n                elif len(parts) == 1 and (parts[0], args.scheduler) in custom:\n                    key, val = custom[(parts[0], args.scheduler)]\n                    config[key] = val\n    cloud_type = None\n    if args.cloud_project:\n        if args.cloud_root and args.cloud_root.startswith(\"gs:\"):\n            cloud_type = \"PAPI\"\n            cloud_root = args.cloud_root\n            cloud_region = None\n        elif ((args.cloud_root and args.cloud_root.startswith(\"s3:\")) or\n              (args.cloud_project and args.cloud_project.startswith(\"arn:\"))):\n            cloud_type = \"AWSBATCH\"\n            cloud_root = args.cloud_root\n            if not cloud_root.startswith(\"s3://\"):\n                cloud_root = \"s3://%s\" % cloud_root\n            # split region from input Amazon Resource Name, ie arn:aws:batch:us-east-1:\n            cloud_region = args.cloud_project.split(\":\")[3]\n        else:\n            raise ValueError(\"Unexpected inputs for Cromwell Cloud support: %s %s\" %\n                             (args.cloud_project, args.cloud_root))\n        config = {\"cloud_project\": args.cloud_project, \"cloud_root\": cloud_root, \"cloud_region\": cloud_region}\n        cl.append(\"-Dbackend.default=%s\" % cloud_type)\n    return cl, config, args.scheduler, cloud_type"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving the types of inputs and staging based on sample JSON and arguments.", "response": "def _get_filesystem_types(args, sample_file):\n    \"\"\"Retrieve the types of inputs and staging based on sample JSON and arguments.\n    \"\"\"\n    out = set([])\n    ext = \"\" if args.no_container else \"_container\"\n    with open(sample_file) as in_handle:\n        for f in _get_file_paths(json.load(in_handle)):\n            if f.startswith(\"gs:\"):\n                out.add(\"gcp%s\" % ext)\n            elif f.startswith(\"s3:\"):\n                out.add(\"s3%s\" % ext)\n            elif f.startswith((\"https:\", \"http:\")):\n                out.add(\"http%s\" % ext)\n            else:\n                out.add(\"local%s\" % ext)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_filesystem_config(file_types):\n    out = \"     filesystems {\\n\"\n    for file_type in sorted(list(file_types)):\n        if file_type in _FILESYSTEM_CONFIG:\n            out += _FILESYSTEM_CONFIG[file_type]\n    out += \"      }\\n\"\n    return out", "response": "Retrieve filesystem configuration including support for specified file types."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_engine_filesystem_config(file_types, args, conf_args):\n    file_types = [x.replace(\"_container\", \"\") for x in list(file_types)]\n    out = \"\"\n    if \"gcp\" in file_types:\n        out += _AUTH_CONFIG_GOOGLE\n    if \"s3\" in file_types:\n        out += _AUTH_CONFIG_AWS % conf_args[\"cloud_region\"]\n    if \"gcp\" in file_types or \"http\" in file_types or \"s3\" in file_types:\n        out += \"engine {\\n\"\n        out += \"  filesystems {\\n\"\n        if \"gcp\" in file_types:\n            out += '    gcs {\\n'\n            out += '      auth = \"gcp-auth\"\\n'\n            if args.cloud_project:\n                out += '      project = \"%s\"\\n' % args.cloud_project\n            out += '    }\\n'\n        if \"http\" in file_types:\n            out += '    http {}\\n'\n        if \"s3\" in file_types:\n            out += '    s3 { auth = \"default\" }'\n        out += \"  }\\n\"\n        out += \"}\\n\"\n\n    return out", "response": "Retrieve the authorization and engine filesystem configuration."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning Scalpel indel calling.", "response": "def run_scalpel(align_bams, items, ref_file, assoc_files, region=None,\n                  out_file=None):\n    \"\"\"Run Scalpel indel calling, either paired tumor/normal or germline calling.\n    \"\"\"\n    if region is None:\n        message = (\"A region must be provided for Scalpel\")\n        raise ValueError(message)\n    if is_paired_analysis(align_bams, items):\n        call_file = _run_scalpel_paired(align_bams, items, ref_file,\n                                          assoc_files, region, out_file)\n    else:\n        call_file = _run_scalpel_caller(align_bams, items, ref_file,\n                                          assoc_files, region, out_file)\n    return call_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _run_scalpel_caller(align_bams, items, ref_file, assoc_files,\n                          region=None, out_file=None):\n    \"\"\"Detect indels with Scalpel.\n\n    Single sample mode.\n    \"\"\"\n    config = items[0][\"config\"]\n    if out_file is None:\n        out_file = \"%s-variants.vcf.gz\" % os.path.splitext(align_bams[0])[0]\n    if not utils.file_exists(out_file):\n        with file_transaction(config, out_file) as tx_out_file:\n            if len(align_bams) > 1:\n                message = (\"Scalpel does not currently support batch calling!\")\n                raise ValueError(message)\n            input_bams = \" \".join(\"%s\" % x for x in align_bams)\n            tmp_path = \"%s-scalpel-work\" % utils.splitext_plus(out_file)[0]\n            tx_tmp_path = \"%s-scalpel-work\" % utils.splitext_plus(tx_out_file)[0]\n            if os.path.exists(tmp_path):\n                utils.remove_safe(tmp_path)\n            opts = \" \".join(_scalpel_options_from_config(items, config, out_file, region, tmp_path))\n            opts += \" --dir %s\" % tx_tmp_path\n            min_cov = \"3\"  # minimum coverage\n            opts += \" --mincov %s\" % min_cov\n            perl_exports = utils.get_perl_exports(os.path.dirname(tx_out_file))\n            cmd = (\"{perl_exports} && \"\n                   \"scalpel-discovery --single {opts} --ref {ref_file} --bam {input_bams} \")\n            do.run(cmd.format(**locals()), \"Genotyping with Scalpel\", {})\n            shutil.move(tx_tmp_path, tmp_path)\n            # parse produced variant file further\n            scalpel_tmp_file = bgzip_and_index(os.path.join(tmp_path, \"variants.indel.vcf\"), config)\n            compress_cmd = \"| bgzip -c\" if out_file.endswith(\"gz\") else \"\"\n            bcftools_cmd_chi2 = get_scalpel_bcftools_filter_expression(\"chi2\", config)\n            sample_name_str = items[0][\"name\"][1]\n            fix_ambig = vcfutils.fix_ambiguous_cl()\n            add_contig = vcfutils.add_contig_to_header_cl(dd.get_ref_file(items[0]), tx_out_file)\n            cl2 = (\"{bcftools_cmd_chi2} {scalpel_tmp_file} | \"\n                   r\"sed 's/FORMAT\\tsample\\(_name\\)\\{{0,1\\}}/FORMAT\\t{sample_name_str}/g' \"\n                   \"| {fix_ambig} | vcfallelicprimitives -t DECOMPOSED --keep-geno | vcffixup - | vcfstreamsort \"\n                   \"| {add_contig} {compress_cmd} > {tx_out_file}\")\n            do.run(cl2.format(**locals()), \"Finalising Scalpel variants\", {})\n    return out_file", "response": "Run scalpel calling on a set of aligned BAM files."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetects indels with Scalpel.", "response": "def _run_scalpel_paired(align_bams, items, ref_file, assoc_files,\n                          region=None, out_file=None):\n    \"\"\"Detect indels with Scalpel.\n\n    This is used for paired tumor / normal samples.\n    \"\"\"\n    config = items[0][\"config\"]\n    if out_file is None:\n        out_file = \"%s-paired-variants.vcf.gz\" % os.path.splitext(align_bams[0])[0]\n    if not utils.file_exists(out_file):\n        with file_transaction(config, out_file) as tx_out_file:\n            paired = get_paired_bams(align_bams, items)\n            if not paired.normal_bam:\n                ann_file = _run_scalpel_caller(align_bams, items, ref_file,\n                                               assoc_files, region, out_file)\n                return ann_file\n            vcfstreamsort = config_utils.get_program(\"vcfstreamsort\", config)\n            perl_exports = utils.get_perl_exports(os.path.dirname(tx_out_file))\n            tmp_path = \"%s-scalpel-work\" % utils.splitext_plus(out_file)[0]\n            db_file = os.path.join(tmp_path, \"main\", \"somatic.db\")\n            if not os.path.exists(db_file + \".dir\"):\n                if os.path.exists(tmp_path):\n                    utils.remove_safe(tmp_path)\n                opts = \" \".join(_scalpel_options_from_config(items, config, out_file, region, tmp_path))\n                opts += \" --ref {}\".format(ref_file)\n                opts += \" --dir %s\" % tmp_path\n                # caling\n                cl = (\"{perl_exports} && \"\n                      \"scalpel-discovery --somatic {opts} --tumor {paired.tumor_bam} --normal {paired.normal_bam}\")\n                do.run(cl.format(**locals()), \"Genotyping paired variants with Scalpel\", {})\n            # filtering to adjust input parameters\n            bed_opts = \" \".join(_scalpel_bed_file_opts(items, config, out_file, region, tmp_path))\n            use_defaults = True\n            if use_defaults:\n                scalpel_tmp_file = os.path.join(tmp_path, \"main/somatic.indel.vcf\")\n            # Uses default filters but can tweak min-alt-count-tumor and min-phred-fisher\n            # to swap precision for sensitivity\n            else:\n                scalpel_tmp_file = os.path.join(tmp_path, \"main/somatic-indel-filter.vcf.gz\")\n                with file_transaction(config, scalpel_tmp_file) as tx_indel_file:\n                    cmd = (\"{perl_exports} && \"\n                           \"scalpel-export --somatic {bed_opts} --ref {ref_file} --db {db_file} \"\n                           \"--min-alt-count-tumor 5 --min-phred-fisher 10 --min-vaf-tumor 0.1 \"\n                           \"| bgzip -c > {tx_indel_file}\")\n                    do.run(cmd.format(**locals()), \"Scalpel somatic indel filter\", {})\n            scalpel_tmp_file = bgzip_and_index(scalpel_tmp_file, config)\n            scalpel_tmp_file_common = bgzip_and_index(os.path.join(tmp_path, \"main/common.indel.vcf\"), config)\n            compress_cmd = \"| bgzip -c\" if out_file.endswith(\"gz\") else \"\"\n            bcftools_cmd_chi2 = get_scalpel_bcftools_filter_expression(\"chi2\", config)\n            bcftools_cmd_common = get_scalpel_bcftools_filter_expression(\"reject\", config)\n            fix_ambig = vcfutils.fix_ambiguous_cl()\n            add_contig = vcfutils.add_contig_to_header_cl(dd.get_ref_file(items[0]), tx_out_file)\n            cl2 = (\"vcfcat <({bcftools_cmd_chi2} {scalpel_tmp_file}) \"\n                   \"<({bcftools_cmd_common} {scalpel_tmp_file_common}) | \"\n                   \" {fix_ambig} | {vcfstreamsort} | {add_contig} {compress_cmd} > {tx_out_file}\")\n            do.run(cl2.format(**locals()), \"Finalising Scalpel variants\", {})\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nnormalizes variants and reruns SnpEFF for resulting VCF", "response": "def normalize(in_file, data, passonly=False, normalize_indels=True, split_biallelic=True,\n              rerun_effects=True, remove_oldeffects=False, nonrefonly=False, work_dir=None):\n    \"\"\"Normalizes variants and reruns SnpEFF for resulting VCF\n    \"\"\"\n    if remove_oldeffects:\n        out_file = \"%s-noeff-nomultiallelic%s\" % utils.splitext_plus(in_file)\n    else:\n        out_file = \"%s-nomultiallelic%s\" % utils.splitext_plus(in_file)\n    if work_dir:\n        out_file = os.path.join(work_dir, os.path.basename(out_file))\n    if not utils.file_exists(out_file):\n        if vcfutils.vcf_has_variants(in_file):\n            ready_ma_file = _normalize(in_file, data, passonly=passonly,\n                                       normalize_indels=normalize_indels,\n                                       split_biallelic=split_biallelic,\n                                       remove_oldeffects=remove_oldeffects,\n                                       nonrefonly=nonrefonly,\n                                       work_dir=work_dir)\n            if rerun_effects:\n                ann_ma_file, _ = effects.add_to_vcf(ready_ma_file, data)\n                if ann_ma_file:\n                    ready_ma_file = ann_ma_file\n            utils.symlink_plus(ready_ma_file, out_file)\n        else:\n            utils.symlink_plus(in_file, out_file)\n    return vcfutils.bgzip_and_index(out_file, data[\"config\"])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _normalize(in_file, data, passonly=False, normalize_indels=True, split_biallelic=True,\n               remove_oldeffects=False, nonrefonly=False, work_dir=None):\n    \"\"\"Convert multi-allelic variants into single allelic.\n\n    `vt normalize` has the -n flag passed (skipping reference checks) because\n    of errors where the reference genome has non GATCN ambiguous bases. These\n    are not supported in VCF, so you'll have a mismatch of N in VCF versus R\n    (or other ambiguous bases) in the genome.\n    \"\"\"\n    if remove_oldeffects:\n        out_file = \"%s-noeff-decompose%s\" % utils.splitext_plus(in_file)\n        old_effects = [a for a in [\"CSQ\", \"ANN\"] if a in cyvcf2.VCF(in_file)]\n        if old_effects:\n            clean_effects_cmd = \" | bcftools annotate -x %s \" % (\",\".join([\"INFO/%s\" % x for x in old_effects]))\n        else:\n            clean_effects_cmd = \"\"\n    else:\n        clean_effects_cmd = \"\"\n        out_file = \"%s-decompose%s\" % utils.splitext_plus(in_file)\n    if passonly or nonrefonly:\n        subset_vcf_cmd = \" | bcftools view \"\n        if passonly:\n            subset_vcf_cmd += \"-f 'PASS,.' \"\n        if nonrefonly:\n            subset_vcf_cmd += \"--min-ac 1:nref \"\n    else:\n        subset_vcf_cmd = \"\"\n    if work_dir:\n        out_file = os.path.join(work_dir, os.path.basename(out_file))\n    if not utils.file_exists(out_file):\n        ref_file = dd.get_ref_file(data)\n        assert out_file.endswith(\".vcf.gz\")\n        with file_transaction(data, out_file) as tx_out_file:\n            cmd = (\"gunzip -c \" + in_file +\n                   subset_vcf_cmd + clean_effects_cmd +\n                   (\" | vcfallelicprimitives -t DECOMPOSED --keep-geno\" if split_biallelic else \"\") +\n                   \" | sed 's/ID=AD,Number=./ID=AD,Number=R/'\" +\n                   \" | vt decompose -s - \" +\n                   ((\" | vt normalize -n -r \" + ref_file + \" - \") if normalize_indels else \"\") +\n                   \" | awk '{ gsub(\\\"./-65\\\", \\\"./.\\\"); print $0 }'\" +\n                   \" | sed -e 's/Number=A/Number=1/g'\" +\n                   \" | bgzip -c > \" + tx_out_file\n                   )\n            do.run(cmd, \"Multi-allelic to single allele\")\n    return vcfutils.bgzip_and_index(out_file, data[\"config\"])", "response": "Convert multi - allelic variants into single - allelic variants."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sleuthify_sailfish(sailfish_dir):\n    if not R_package_path(\"wasabi\"):\n        return None\n    else:\n        rscript = Rscript_cmd()\n        cmd = \"\"\"{rscript} --no-environ -e 'library(\"wasabi\"); prepare_fish_for_sleuth(c(\"{sailfish_dir}\"))'\"\"\"\n        do.run(cmd.format(**locals()), \"Converting Sailfish to Sleuth format.\")\n    return os.path.join(sailfish_dir, \"abundance.h5\")", "response": "Convert Sailfish to Sleuth format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_combined_fasta(data):\n    out_dir = os.path.join(dd.get_work_dir(data), \"inputs\", \"transcriptome\")\n    items = disambiguate.split([data])\n    fasta_files = []\n    for i in items:\n        odata = i[0]\n        gtf_file = dd.get_gtf_file(odata)\n        ref_file = dd.get_ref_file(odata)\n        out_file = os.path.join(out_dir, dd.get_genome_build(odata) + \".fa\")\n        if file_exists(out_file):\n            fasta_files.append(out_file)\n        else:\n            out_file = gtf.gtf_to_fasta(gtf_file, ref_file, out_file=out_file)\n            fasta_files.append(out_file)\n    out_stem = os.path.join(out_dir, dd.get_genome_build(data))\n    if dd.get_disambiguate(data):\n        out_stem = \"-\".join([out_stem] + (dd.get_disambiguate(data) or []))\n    combined_file = out_stem + \".fa\"\n    if file_exists(combined_file):\n        return combined_file\n\n    fasta_file_string = \" \".join(fasta_files)\n    cmd = \"cat {fasta_file_string} > {tx_out_file}\"\n    with file_transaction(data, combined_file) as tx_out_file:\n        do.run(cmd.format(**locals()), \"Combining transcriptome FASTA files.\")\n    return combined_file", "response": "Create a combined FASTA file of all transcripts for all genomes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _libtype_string(fq1, fq2, strandedness):\n    libtype = \"-l I\" if fq2 else \"-l \"\n    strand = _sailfish_strand_string(strandedness)\n    return libtype + strand", "response": "Returns a string that can be used to create a libtype string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pick_kmersize(fq):\n    if bam.is_bam(fq):\n        readlength = bam.estimate_read_length(fq)\n    else:\n        readlength = fastq.estimate_read_length(fq)\n    halfread = int(round(readlength / 2))\n    if halfread >= 31:\n        kmersize = 31\n    else:\n        kmersize = halfread\n    if kmersize % 2 == 0:\n        kmersize += 1\n    return kmersize", "response": "pick an appropriate kmer size based off of BIOSTARs. org."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prep_samples_and_config(run_folder, ldetails, fastq_dir, config):\n    fastq_final_dir = utils.safe_makedir(os.path.join(fastq_dir, \"merged\"))\n    cores = utils.get_in(config, (\"algorithm\", \"num_cores\"), 1)\n    ldetails = joblib.Parallel(cores)(joblib.delayed(_prep_sample_and_config)(x, fastq_dir, fastq_final_dir)\n                                      for x in _group_same_samples(ldetails))\n    config_file = _write_sample_config(run_folder, [x for x in ldetails if x])\n    return config_file, fastq_final_dir", "response": "Prepare sample fastq files and provide global sample configuration for the flowcell."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _prep_sample_and_config(ldetail_group, fastq_dir, fastq_final_dir):\n    files = []\n    print(\"->\", ldetail_group[0][\"name\"], len(ldetail_group))\n    for read in [\"R1\", \"R2\"]:\n        fastq_inputs = sorted(list(set(reduce(operator.add,\n                                              (_get_fastq_files(x, read, fastq_dir) for x in ldetail_group)))))\n        if len(fastq_inputs) > 0:\n            files.append(_concat_bgzip_fastq(fastq_inputs, fastq_final_dir, read, ldetail_group[0]))\n    if len(files) > 0:\n        if _non_empty(files[0]):\n            out = ldetail_group[0]\n            out[\"files\"] = files\n            return out", "response": "Prepare output fastq file and configuration for a single sample."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _write_sample_config(run_folder, ldetails):\n    out_file = os.path.join(run_folder, \"%s.yaml\" % os.path.basename(run_folder))\n    with open(out_file, \"w\") as out_handle:\n        fc_name, fc_date = flowcell.parse_dirname(run_folder)\n        out = {\"details\": sorted([_prepare_sample(x, run_folder) for x in ldetails],\n                                 key=operator.itemgetter(\"name\", \"description\")),\n               \"fc_name\": fc_name,\n               \"fc_date\": fc_date}\n        yaml.safe_dump(out, out_handle, default_flow_style=False, allow_unicode=False)\n    return out_file", "response": "Generate a bcbio - nextgen YAML configuration file for processing a sample."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _prepare_sample(data, run_folder):\n    want = set([\"description\", \"files\", \"genome_build\", \"name\", \"analysis\", \"upload\", \"algorithm\"])\n    out = {}\n    for k, v in data.items():\n        if k in want:\n            out[k] = _relative_paths(v, run_folder)\n    if \"algorithm\" not in out:\n        analysis, algorithm = _select_default_algorithm(out.get(\"analysis\"))\n        out[\"algorithm\"] = algorithm\n        out[\"analysis\"] = analysis\n    description = \"%s-%s\" % (out[\"name\"], clean_name(out[\"description\"]))\n    out[\"name\"] = [out[\"name\"], description]\n    out[\"description\"] = description\n    return out", "response": "Extract passed keywords from input LIMS information."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _select_default_algorithm(analysis):\n    if not analysis or analysis == \"Standard\":\n        return \"Standard\", {\"aligner\": \"bwa\", \"platform\": \"illumina\", \"quality_format\": \"Standard\",\n                            \"recalibrate\": False, \"realign\": False, \"mark_duplicates\": True,\n                            \"variantcaller\": False}\n    elif \"variant\" in analysis:\n        try:\n            config, _ = template.name_to_config(analysis)\n        except ValueError:\n            config, _ = template.name_to_config(\"freebayes-variant\")\n        return \"variant\", config[\"details\"][0][\"algorithm\"]\n    else:\n        return analysis, {}", "response": "Provide default algorithm sections from templates or standard\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _relative_paths(xs, base_path):\n    if isinstance(xs, six.string_types):\n        if xs.startswith(base_path):\n            return xs.replace(base_path + \"/\", \"\", 1)\n        else:\n            return xs\n    elif isinstance(xs, (list, tuple)):\n        return [_relative_paths(x, base_path) for x in xs]\n    elif isinstance(xs, dict):\n        out = {}\n        for k, v in xs.items():\n            out[k] = _relative_paths(v, base_path)\n        return out\n    else:\n        return xs", "response": "Adjust paths to be relative to the provided base path."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves the fastq files corresponding to the sample and read number.", "response": "def _get_fastq_files(ldetail, read, fastq_dir):\n    \"\"\"Retrieve fastq files corresponding to the sample and read number.\n    \"\"\"\n    return glob.glob(os.path.join(fastq_dir, \"Project_%s\" % ldetail[\"project_name\"],\n                                  \"Sample_%s\" % ldetail[\"name\"],\n                                  \"%s_*_%s_*.fastq.gz\" % (ldetail[\"name\"], read)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconcatenates multiple input fastq files preparing a bgzipped output file.", "response": "def _concat_bgzip_fastq(finputs, out_dir, read, ldetail):\n    \"\"\"Concatenate multiple input fastq files, preparing a bgzipped output file.\n    \"\"\"\n    out_file = os.path.join(out_dir, \"%s_%s.fastq.gz\" % (ldetail[\"name\"], read))\n    if not utils.file_exists(out_file):\n        with file_transaction(out_file) as tx_out_file:\n            subprocess.check_call(\"zcat %s | bgzip -c > %s\" % (\" \".join(finputs), tx_out_file), shell=True)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmoves samples into groups -- same groups have identical names.", "response": "def _group_same_samples(ldetails):\n    \"\"\"Move samples into groups -- same groups have identical names.\n    \"\"\"\n    sample_groups = collections.defaultdict(list)\n    for ldetail in ldetails:\n        sample_groups[ldetail[\"name\"]].append(ldetail)\n    return sorted(sample_groups.values(), key=lambda xs: xs[0][\"name\"])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_runinfo(galaxy_url, galaxy_apikey, run_folder, storedir):\n    galaxy_api = GalaxyApiAccess(galaxy_url, galaxy_apikey)\n    fc_name, fc_date = flowcell.parse_dirname(run_folder)\n    galaxy_info = galaxy_api.run_details(fc_name, fc_date)\n    if \"error\" in galaxy_info:\n        return galaxy_info\n    if not galaxy_info[\"run_name\"].startswith(fc_date) and not galaxy_info[\"run_name\"].endswith(fc_name):\n        raise ValueError(\"Galaxy NGLIMS information %s does not match flowcell %s %s\" %\n                         (galaxy_info[\"run_name\"], fc_date, fc_name))\n    ldetails = _flatten_lane_details(galaxy_info)\n    out = []\n    for item in ldetails:\n        # Do uploads for all non-controls\n        if item[\"description\"] != \"control\" or item[\"project_name\"] != \"control\":\n            item[\"upload\"] = {\"method\": \"galaxy\", \"run_id\": galaxy_info[\"run_id\"],\n                              \"fc_name\": fc_name, \"fc_date\": fc_date,\n                              \"dir\": storedir,\n                              \"galaxy_url\": galaxy_url, \"galaxy_api_key\": galaxy_apikey}\n            for k in [\"lab_association\", \"private_libs\", \"researcher\", \"researcher_id\", \"sample_id\",\n                      \"galaxy_library\", \"galaxy_role\"]:\n                item[\"upload\"][k] = item.pop(k, \"\")\n        out.append(item)\n    return out", "response": "Retrieve flattened run information for a processed directory from Galaxy NGLIMS API."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _flatten_lane_details(runinfo):\n    out = []\n    for ldetail in runinfo[\"details\"]:\n        # handle controls\n        if \"project_name\" not in ldetail and ldetail[\"description\"] == \"control\":\n            ldetail[\"project_name\"] = \"control\"\n        for i, barcode in enumerate(ldetail.get(\"multiplex\", [{}])):\n            cur = copy.deepcopy(ldetail)\n            cur[\"name\"] = \"%s-%s\" % (ldetail[\"name\"], i + 1)\n            cur[\"description\"] = barcode.get(\"name\", ldetail[\"description\"])\n            cur[\"bc_index\"] = barcode.get(\"sequence\", \"\")\n            cur[\"project_name\"] = clean_name(ldetail[\"project_name\"])\n            out.append(cur)\n    return out", "response": "Provide flattened lane information with multiplexed barcodes separated."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef grouped_parallel_split_combine(args, split_fn, group_fn, parallel_fn,\n                                   parallel_name, combine_name,\n                                   file_key, combine_arg_keys,\n                                   split_outfile_i=-1):\n    \"\"\"Parallel split runner that allows grouping of samples during processing.\n\n    This builds on parallel_split_combine to provide the additional ability to\n    group samples and subsequently split them back apart. This allows analysis\n    of related samples together. In addition to the arguments documented in\n    parallel_split_combine, this needs:\n\n    group_fn: A function that groups samples together given their configuration\n      details.\n    \"\"\"\n    grouped_args = group_fn(args)\n    split_args, combine_map, finished_out, extras = _get_split_tasks(grouped_args, split_fn, file_key,\n                                                                     split_outfile_i)\n    final_output = parallel_fn(parallel_name, split_args)\n    combine_args, final_args = _organize_output(final_output, combine_map,\n                                                file_key, combine_arg_keys)\n    parallel_fn(combine_name, combine_args)\n    return finished_out + final_args + extras", "response": "This function is used to group samples together in a single parallel split."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parallel_split_combine(args, split_fn, parallel_fn,\n                           parallel_name, combiner,\n                           file_key, combine_arg_keys, split_outfile_i=-1):\n    \"\"\"Split, run split items in parallel then combine to output file.\n\n    split_fn: Split an input file into parts for processing. Returns\n      the name of the combined output file along with the individual\n      split output names and arguments for the parallel function.\n    parallel_fn: Reference to run_parallel function that will run\n      single core, multicore, or distributed as needed.\n    parallel_name: The name of the function, defined in\n      bcbio.distributed.tasks/multitasks/ipythontasks to run in parallel.\n    combiner: The name of the function, also from tasks, that combines\n      the split output files into a final ready to run file. Can also\n      be a callable function if combining is delayed.\n    split_outfile_i: the location of the output file in the arguments\n      generated by the split function. Defaults to the last item in the list.\n    \"\"\"\n    args = [x[0] for x in args]\n    split_args, combine_map, finished_out, extras = _get_split_tasks(args, split_fn, file_key,\n                                                                     split_outfile_i)\n    split_output = parallel_fn(parallel_name, split_args)\n    if isinstance(combiner, six.string_types):\n        combine_args, final_args = _organize_output(split_output, combine_map,\n                                                    file_key, combine_arg_keys)\n        parallel_fn(combiner, combine_args)\n    elif callable(combiner):\n        final_args = combiner(split_output, combine_map, file_key)\n    return finished_out + final_args + extras", "response": "Split a list of files into separate files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_extra_args(extra_args, arg_keys):\n    # XXX back compatible hack -- should have a way to specify these.\n    single_keys = set([\"sam_ref\", \"config\"])\n    out = []\n    for i, arg_key in enumerate(arg_keys):\n        vals = [xs[i] for xs in extra_args]\n        if arg_key in single_keys:\n            out.append(vals[-1])\n        else:\n            out.append(vals)\n    return out", "response": "Retrieve extra arguments to pass along to combine function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _organize_output(output, combine_map, file_key, combine_arg_keys):\n    out_map = collections.defaultdict(list)\n    extra_args = collections.defaultdict(list)\n    final_args = collections.OrderedDict()\n    extras = []\n    for data in output:\n        cur_file = data.get(file_key)\n        if not cur_file:\n            extras.append([data])\n        else:\n            cur_out = combine_map[cur_file]\n            out_map[cur_out].append(cur_file)\n            extra_args[cur_out].append([data[x] for x in combine_arg_keys])\n            data[file_key] = cur_out\n            if cur_out not in final_args:\n                final_args[cur_out] = [data]\n            else:\n                extras.append([data])\n    combine_args = [[v, k] + _get_extra_args(extra_args[k], combine_arg_keys)\n                    for (k, v) in out_map.items()]\n    return combine_args, list(final_args.values()) + extras", "response": "Combine output details for parallelization."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsplit up input files and arguments returning arguments for parallel processing.", "response": "def _get_split_tasks(args, split_fn, file_key, outfile_i=-1):\n    \"\"\"Split up input files and arguments, returning arguments for parallel processing.\n\n    outfile_i specifies the location of the output file in the arguments to\n    the processing function. Defaults to the last item in the list.\n    \"\"\"\n    split_args = []\n    combine_map = {}\n    finished_map = collections.OrderedDict()\n    extras = []\n    for data in args:\n        out_final, out_parts = split_fn(data)\n        for parts in out_parts:\n            split_args.append([utils.deepish_copy(data)] + list(parts))\n        for part_file in [x[outfile_i] for x in out_parts]:\n            combine_map[part_file] = out_final\n        if len(out_parts) == 0:\n            if out_final is not None:\n                if out_final not in finished_map:\n                    data[file_key] = out_final\n                    finished_map[out_final] = [data]\n                else:\n                    extras.append([data])\n            else:\n                extras.append([data])\n    return split_args, combine_map, list(finished_map.values()), extras"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming alignment of fastq files using novoalign.", "response": "def align(fastq_file, pair_file, index_dir, names, align_dir, data):\n    \"\"\"Perform piped alignment of fastq input files, generating sorted, deduplicated BAM.\n\n    Pipes in input, handling paired and split inputs, using interleaving magic\n    from: https://biowize.wordpress.com/2015/03/26/the-fastest-darn-fastq-decoupling-procedure-i-ever-done-seen/\n\n    Then converts a tab delimited set of outputs into interleaved fastq.\n\n    awk changes spaces to underscores since SNAP only takes the initial name.\n    SNAP requires /1 and /2 at the end of read names. If these are not present\n    in the initial fastq may need to expand awk code to do this.\n    \"\"\"\n    out_file = os.path.join(align_dir, \"{0}-sort.bam\".format(dd.get_sample_name(data)))\n    num_cores = data[\"config\"][\"algorithm\"].get(\"num_cores\", 1)\n    resources = config_utils.get_resources(\"snap\", data[\"config\"])\n    rg_info = novoalign.get_rg_info(names)\n\n    if data.get(\"align_split\"):\n        final_file = out_file\n        out_file, data = alignprep.setup_combine(final_file, data)\n        fastq_file, pair_file = alignprep.split_namedpipe_cls(fastq_file, pair_file, data)\n        fastq_file = fastq_file[2:-1]\n        if pair_file:\n            pair_file = pair_file[2:-1]\n            stream_input = (r\"paste <({fastq_file} | paste - - - -) \"\n                            r\"<({pair_file} | paste - - - -) | \"\n                            r\"\"\"awk 'BEGIN {{FS=\"\\t\"; OFS=\"\\n\"}} \"\"\"\n                            r\"\"\"{{ \"\"\"\n                            r\"\"\"split($1, P1, \" \"); split($5, P5, \" \"); \"\"\"\n                            r\"\"\"if ($1 !~ /\\/1$/) $1 = P1[1]\"/1\"; if ($5 !~ /\\/2$/) $5 = P5[1]\"/2\"; \"\"\"\n                            r\"\"\"gsub(\" \", \"_\", $1); gsub(\" \", \"_\", $5); \"\"\"\n                            r\"\"\"print $1, $2, \"+\", $4, $5, $6, \"+\", $8}}' \"\"\")\n        else:\n            stream_input = fastq_file[2:-1]\n    else:\n        final_file = None\n        assert fastq_file.endswith(\".gz\")\n        if pair_file:\n            stream_input = (r\"paste <(zcat {fastq_file} | paste - - - -) \"\n                            r\"<(zcat {pair_file} | paste - - - -) | \"\n                            r\"\"\"awk 'BEGIN {{FS=\"\\t\"; OFS=\"\\n\"}} \"\"\"\n                            r\"\"\"{{ \"\"\"\n                            r\"\"\"split($1, P1, \" \"); split($5, P5, \" \"); \"\"\"\n                            r\"\"\"if ($1 !~ /\\/1$/) $1 = P1[1]\"/1\"; if ($5 !~ /\\/2$/) $5 = P5[1]\"/2\"; \"\"\"\n                            r\"\"\"gsub(\" \", \"_\", $1); gsub(\" \", \"_\", $5); \"\"\"\n                            r\"\"\"print $1, $2, \"+\", $4, $5, $6, \"+\", $8}}' \"\"\")\n        else:\n            stream_input = \"zcat {fastq_file}\"\n\n    pair_file = pair_file if pair_file else \"\"\n    if not utils.file_exists(out_file) and (final_file is None or not utils.file_exists(final_file)):\n        with postalign.tobam_cl(data, out_file, pair_file is not None) as (tobam_cl, tx_out_file):\n            if pair_file:\n                sub_cmd = \"paired\"\n                input_cmd = \"-pairedInterleavedFastq -\"\n            else:\n                sub_cmd = \"single\"\n                input_cmd = \"-fastq -\"\n            stream_input = stream_input.format(**locals())\n            tmp_dir = os.path.dirname(tx_out_file)\n            cmd = (\"export TMPDIR={tmp_dir} && unset JAVA_HOME && {stream_input} | \"\n                   \"snap-aligner {sub_cmd} {index_dir} {input_cmd} \"\n                   \"-R '{rg_info}' -t {num_cores} -M -o -sam - | \")\n            do.run(cmd.format(**locals()) + tobam_cl, \"SNAP alignment: %s\" % names[\"sample\"])\n    data[\"work_bam\"] = out_file\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmaps sequence references to snap reference directory using standard layout.", "response": "def remap_index_fn(ref_file):\n    \"\"\"Map sequence references to snap reference directory, using standard layout.\n    \"\"\"\n    snap_dir = os.path.join(os.path.dirname(ref_file), os.pardir, \"snap\")\n    assert os.path.exists(snap_dir) and os.path.isdir(snap_dir), snap_dir\n    return snap_dir"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if we should use a general binning approach for a sample.", "response": "def use_general_sv_bins(data):\n    \"\"\"Check if we should use a general binning approach for a sample.\n\n    Checks if CNVkit is enabled and we haven't already run CNVkit.\n    \"\"\"\n    if any([c in dd.get_svcaller(data) for c in [\"cnvkit\", \"titancna\", \"purecn\", \"gatk-cnv\"]]):\n        if not _get_original_coverage(data):\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bin_approach(data):\n    for approach in [\"cnvkit\", \"gatk-cnv\"]:\n        if approach in dd.get_svcaller(data):\n            return approach\n    norm_file = tz.get_in([\"depth\", \"bins\", \"normalized\"], data)\n    if norm_file.endswith((\"-crstandardized.tsv\", \"-crdenoised.tsv\")):\n        return \"gatk-cnv\"\n    if norm_file.endswith(\".cnr\"):\n        return \"cnvkit\"", "response": "Check for binning approach from configuration or normalized file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _cnvkit_by_type(items, background):\n    if len(items + background) == 1:\n        return _run_cnvkit_single(items[0])\n    elif vcfutils.get_paired_phenotype(items[0]):\n        return _run_cnvkit_cancer(items, background)\n    else:\n        return _run_cnvkit_population(items, background)", "response": "Dispatch to specific CNVkit functionality based on input type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _associate_cnvkit_out(ckouts, items, is_somatic=False):\n    assert len(ckouts) == len(items)\n    out = []\n    upload_counts = collections.defaultdict(int)\n    for ckout, data in zip(ckouts, items):\n        ckout = copy.deepcopy(ckout)\n        ckout[\"variantcaller\"] = \"cnvkit\"\n        if utils.file_exists(ckout[\"cns\"]) and _cna_has_values(ckout[\"cns\"]):\n            ckout = _add_seg_to_output(ckout, data)\n            ckout = _add_gainloss_to_output(ckout, data)\n            ckout = _add_segmetrics_to_output(ckout, data)\n            ckout = _add_variantcalls_to_output(ckout, data, items, is_somatic)\n            # ckout = _add_coverage_bedgraph_to_output(ckout, data)\n            ckout = _add_cnr_bedgraph_and_bed_to_output(ckout, data)\n            if \"svplots\" in dd.get_tools_on(data):\n                ckout = _add_plots_to_output(ckout, data)\n            ckout[\"do_upload\"] = upload_counts[ckout.get(\"vrn_file\")] == 0\n        if \"sv\" not in data:\n            data[\"sv\"] = []\n        data[\"sv\"].append(ckout)\n        if ckout.get(\"vrn_file\"):\n            upload_counts[ckout[\"vrn_file\"]] += 1\n        out.append(data)\n    return out", "response": "Associate cnvkit output with individual items."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _run_cnvkit_single(data, background=None):\n    if not background:\n        background = []\n    ckouts = _run_cnvkit_shared([data], background)\n    if not ckouts:\n        return [data]\n    else:\n        assert len(ckouts) == 1\n        return _associate_cnvkit_out(ckouts, [data])", "response": "Process a single input file with BAM or uniform background."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _run_cnvkit_cancer(items, background):\n    paired = vcfutils.get_paired_bams([x[\"align_bam\"] for x in items], items)\n    normal_data = [x for x in items if dd.get_sample_name(x) != paired.tumor_name]\n    tumor_ready, normal_ready = _match_batches(paired.tumor_data, normal_data[0] if normal_data else None)\n    ckouts = _run_cnvkit_shared([tumor_ready], [normal_ready] if normal_ready else [])\n    if not ckouts:\n        return items\n    assert len(ckouts) == 1\n    tumor_data = _associate_cnvkit_out(ckouts, [paired.tumor_data], is_somatic=True)\n    return tumor_data + normal_data", "response": "Run CNVkit on a list of items."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds the unique names for the tumor and normal batches.", "response": "def _match_batches(tumor, normal):\n    \"\"\"Fix batch names for shared tumor/normals to ensure matching\n     \"\"\"\n    def _get_batch(x):\n        b = dd.get_batch(x)\n        return [b] if not isinstance(b, (list, tuple)) else b\n    if normal:\n        tumor = copy.deepcopy(tumor)\n        normal = copy.deepcopy(normal)\n        cur_batch = list(set(_get_batch(tumor)) & set(_get_batch(normal)))\n        assert len(cur_batch) == 1, \"No batch overlap: %s and %s\" % (_get_batch(tumor), _get_batch(normal))\n        cur_batch = cur_batch[0]\n        tumor[\"metadata\"][\"batch\"] = cur_batch\n        normal[\"metadata\"][\"batch\"] = cur_batch\n    return tumor, normal"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _run_cnvkit_population(items, background):\n    if background and len(background) > 0:\n        inputs = items\n    else:\n        inputs, background = shared.find_case_control(items)\n\n    # if we have case/control organized background or a single sample\n    if len(inputs) == 1 or len(background) > 0:\n        ckouts = _run_cnvkit_shared(inputs, background)\n        return _associate_cnvkit_out(ckouts, inputs) + background\n    # otherwise run each sample with the others in the batch as background\n    else:\n        out = []\n        for cur_input in items:\n            background = [d for d in items if dd.get_sample_name(d) != dd.get_sample_name(cur_input)]\n            ckouts = _run_cnvkit_shared([cur_input], background)\n            out.extend(_associate_cnvkit_out(ckouts, [cur_input]))\n        return out", "response": "Run CNVkit on a population of samples."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _prep_cmd(cmd, tx_out_file):\n    cmd = \" \".join(cmd) if isinstance(cmd, (list, tuple)) else cmd\n    return \"export TMPDIR=%s && %s\" % (os.path.dirname(tx_out_file), cmd)", "response": "Wrap CNVkit commands ensuring we use local temporary directories."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _bam_to_outbase(bam_file, work_dir, data):\n    batch = dd.get_batch(data) or dd.get_sample_name(data)\n    out_base = os.path.splitext(os.path.basename(bam_file))[0].split(\".\")[0]\n    base = os.path.join(work_dir, out_base)\n    return \"%s-%s\" % (base, batch), base", "response": "Convert an input BAM file into CNVkit expected output."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_original_coverage(data, itype=\"target\"):\n    work_dir = os.path.join(_sv_workdir(data), \"raw\")\n    work_bam = dd.get_work_bam(data) or dd.get_align_bam(data)\n    out = []\n    base, _ = _bam_to_outbase(work_bam, work_dir, data)\n    target_cnn = \"%s.targetcoverage.cnn\" % base\n    anti_cnn = \"%s.antitargetcoverage.cnn\" % base\n    if os.path.exists(target_cnn) and os.path.exists(anti_cnn):\n        out.append({\"bam\": work_bam, \"file\": target_cnn, \"cnntype\": \"target\",\n                    \"itype\": itype, \"sample\": dd.get_sample_name(data)})\n        out.append({\"bam\": work_bam, \"file\": anti_cnn, \"cnntype\": \"antitarget\",\n                    \"itype\": itype, \"sample\": dd.get_sample_name(data)})\n    return out", "response": "Back compatible with target and antitarget coverage files."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _run_cnvkit_shared(inputs, backgrounds):\n    if tz.get_in([\"depth\", \"bins\", \"normalized\"], inputs[0]):\n        ckouts = []\n        for data in inputs:\n            cnr_file = tz.get_in([\"depth\", \"bins\", \"normalized\"], data)\n            cns_file = os.path.join(_sv_workdir(data), \"%s.cns\" % dd.get_sample_name(data))\n            cns_file = _cnvkit_segment(cnr_file, dd.get_coverage_interval(data), data,\n                                       inputs + backgrounds, cns_file)\n            ckouts.append({\"cnr\": cnr_file, \"cns\": cns_file,\n                           \"background\": tz.get_in([\"depth\", \"bins\", \"background\"], data)})\n        return ckouts\n    else:\n        return _run_cnvkit_shared_orig(inputs, backgrounds)", "response": "Shared functionality to run CNVkit over multiple BAM files."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbacks compatible : get pre - existing target BEDs.", "response": "def _get_original_targets(data):\n    \"\"\"Back compatible: get pre-existing target BEDs.\n    \"\"\"\n    work_dir = os.path.join(_sv_workdir(data), \"raw\")\n    batch = dd.get_batch(data) or dd.get_sample_name(data)\n    return (glob.glob(os.path.join(work_dir, \"*-%s.target.bed\" % batch))[0],\n            glob.glob(os.path.join(work_dir, \"*-%s.antitarget.bed\" % batch))[0])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_general_coverage(data, itype):\n    work_bam = dd.get_align_bam(data) or dd.get_work_bam(data)\n    return [{\"bam\": work_bam, \"file\": tz.get_in([\"depth\", \"bins\", \"target\"], data),\n             \"cnntype\": \"target\", \"itype\": itype, \"sample\": dd.get_sample_name(data)},\n            {\"bam\": work_bam, \"file\": tz.get_in([\"depth\", \"bins\", \"antitarget\"], data),\n             \"cnntype\": \"antitarget\", \"itype\": itype, \"sample\": dd.get_sample_name(data)}]", "response": "Retrieve coverage information from new shared SV bins."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning CNVkit with full normalization and segmentation.", "response": "def _run_cnvkit_shared_orig(inputs, backgrounds):\n    \"\"\"Original CNVkit implementation with full normalization and segmentation.\n    \"\"\"\n    work_dir = _sv_workdir(inputs[0])\n    raw_work_dir = utils.safe_makedir(os.path.join(work_dir, \"raw\"))\n    background_name = dd.get_sample_name(backgrounds[0]) if backgrounds else \"flat\"\n    background_cnn = os.path.join(raw_work_dir, \"%s_background.cnn\" % (background_name))\n    ckouts = []\n    for cur_input in inputs:\n        cur_raw_work_dir = utils.safe_makedir(os.path.join(_sv_workdir(cur_input), \"raw\"))\n        out_base, out_base_old = _bam_to_outbase(dd.get_align_bam(cur_input), cur_raw_work_dir, cur_input)\n        if utils.file_exists(out_base_old + \".cns\"):\n            out_base = out_base_old\n        ckouts.append({\"cnr\": \"%s.cnr\" % out_base,\n                       \"cns\": \"%s.cns\" % out_base})\n    if not utils.file_exists(ckouts[0][\"cns\"]):\n        cov_interval = dd.get_coverage_interval(inputs[0])\n        samples_to_run = list(zip([\"background\"] * len(backgrounds), backgrounds)) + \\\n                         list(zip([\"evaluate\"] * len(inputs), inputs))\n        # New style shared SV bins\n        if tz.get_in([\"depth\", \"bins\", \"target\"], inputs[0]):\n            target_bed = tz.get_in([\"depth\", \"bins\", \"target\"], inputs[0])\n            antitarget_bed = tz.get_in([\"depth\", \"bins\", \"antitarget\"], inputs[0])\n            raw_coverage_cnns = reduce(operator.add,\n                                       [_get_general_coverage(cdata, itype) for itype, cdata in samples_to_run])\n        # Back compatible with pre-existing runs\n        else:\n            target_bed, antitarget_bed = _get_original_targets(inputs[0])\n            raw_coverage_cnns = reduce(operator.add,\n                                       [_get_original_coverage(cdata, itype) for itype, cdata in samples_to_run])\n        # Currently metrics not calculated due to speed and needing re-evaluation\n        # We could re-enable with larger truth sets to evaluate background noise\n        # But want to reimplement in a more general fashion as part of normalization\n        if False:\n            coverage_cnns = reduce(operator.add,\n                                [_cnvkit_metrics(cnns, target_bed, antitarget_bed, cov_interval,\n                                                    inputs + backgrounds)\n                                    for cnns in tz.groupby(\"bam\", raw_coverage_cnns).values()])\n            background_cnn = cnvkit_background(_select_background_cnns(coverage_cnns),\n                                                background_cnn, inputs, target_bed, antitarget_bed)\n        else:\n            coverage_cnns = raw_coverage_cnns\n            background_cnn = cnvkit_background([x[\"file\"] for x in coverage_cnns if x[\"itype\"] == \"background\"],\n                                                background_cnn, inputs, target_bed, antitarget_bed)\n        parallel = {\"type\": \"local\", \"cores\": dd.get_cores(inputs[0]), \"progs\": [\"cnvkit\"]}\n        fixed_cnrs = run_multicore(_cnvkit_fix,\n                                   [(cnns, background_cnn, inputs, ckouts) for cnns in\n                                    tz.groupby(\"bam\", [x for x in coverage_cnns\n                                                       if x[\"itype\"] == \"evaluate\"]).values()],\n                                   inputs[0][\"config\"], parallel)\n        [_cnvkit_segment(cnr, cov_interval, data, inputs + backgrounds) for cnr, data in fixed_cnrs]\n    return ckouts"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms segmentation and copy number calling on normalized inputs.", "response": "def _cnvkit_segment(cnr_file, cov_interval, data, items, out_file=None, detailed=False):\n    \"\"\"Perform segmentation and copy number calling on normalized inputs\n    \"\"\"\n    if not out_file:\n        out_file = \"%s.cns\" % os.path.splitext(cnr_file)[0]\n    if not utils.file_uptodate(out_file, cnr_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            if not _cna_has_values(cnr_file):\n                with open(tx_out_file, \"w\") as out_handle:\n                    out_handle.write(\"chromosome\\tstart\\tend\\tgene\\tlog2\\tprobes\\tCN1\\tCN2\\tbaf\\tweight\\n\")\n            else:\n                # Scale cores to avoid memory issues with segmentation\n                # https://github.com/etal/cnvkit/issues/346\n                if cov_interval == \"genome\":\n                    cores = max(1, dd.get_cores(data) // 2)\n                else:\n                    cores = dd.get_cores(data)\n                cmd = [_get_cmd(), \"segment\", \"-p\", str(cores), \"-o\", tx_out_file, cnr_file]\n                small_vrn_files = _compatible_small_variants(data, items)\n                if len(small_vrn_files) > 0 and _cna_has_values(cnr_file) and cov_interval != \"genome\":\n                    cmd += [\"--vcf\", small_vrn_files[0].name, \"--sample-id\", small_vrn_files[0].sample]\n                    if small_vrn_files[0].normal:\n                        cmd += [\"--normal-id\", small_vrn_files[0].normal]\n                resources = config_utils.get_resources(\"cnvkit_segment\", data[\"config\"])\n                user_options = resources.get(\"options\", [])\n                cmd += [str(x) for x in user_options]\n                if cov_interval == \"genome\" and \"--threshold\" not in user_options:\n                    cmd += [\"--threshold\", \"0.00001\"]\n                # For tumors, remove very low normalized regions, avoiding upcaptured noise\n                # https://github.com/bcbio/bcbio-nextgen/issues/2171#issuecomment-348333650\n                # unless we want detailed segmentation for downstream tools\n                paired = vcfutils.get_paired(items)\n                if paired:\n                    #if detailed:\n                    #    cmd += [\"-m\", \"hmm-tumor\"]\n                    if \"--drop-low-coverage\" not in user_options:\n                        cmd += [\"--drop-low-coverage\"]\n                # preferentially use conda installed Rscript\n                export_cmd = (\"%s && export TMPDIR=%s && \"\n                              % (utils.get_R_exports(), os.path.dirname(tx_out_file)))\n                do.run(export_cmd + \" \".join(cmd), \"CNVkit segment\")\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nestimating noise of a sample using a flat background.", "response": "def _cnvkit_metrics(cnns, target_bed, antitarget_bed, cov_interval, items):\n    \"\"\"Estimate noise of a sample using a flat background.\n\n    Only used for panel/targeted data due to memory issues with whole genome\n    samples.\n\n    \"\"\"\n    if cov_interval == \"genome\":\n        return cnns\n    target_cnn = [x[\"file\"] for x in cnns if x[\"cnntype\"] == \"target\"][0]\n    background_file = \"%s-flatbackground.cnn\" % utils.splitext_plus(target_cnn)[0]\n    background_file = cnvkit_background([], background_file, items, target_bed, antitarget_bed)\n    cnr_file, data = _cnvkit_fix_base(cnns, background_file, items, \"-flatbackground\")\n    cns_file = _cnvkit_segment(cnr_file, cov_interval, data)\n    metrics_file = \"%s-metrics.txt\" % utils.splitext_plus(target_cnn)[0]\n    if not utils.file_exists(metrics_file):\n        with file_transaction(data, metrics_file) as tx_metrics_file:\n            cmd = [_get_cmd(), \"metrics\", \"-o\", tx_metrics_file, \"-s\", cns_file, \"--\", cnr_file]\n            do.run(_prep_cmd(cmd, tx_metrics_file), \"CNVkit metrics\")\n    metrics = _read_metrics_file(metrics_file)\n    out = []\n    for cnn in cnns:\n        cnn[\"metrics\"] = metrics\n        out.append(cnn)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nnormalizes samples correcting sources of bias.", "response": "def _cnvkit_fix(cnns, background_cnn, items, ckouts):\n    \"\"\"Normalize samples, correcting sources of bias.\n    \"\"\"\n    return [_cnvkit_fix_base(cnns, background_cnn, items, ckouts)]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nselecting cnns to use for background calculations.", "response": "def _select_background_cnns(cnns):\n    \"\"\"Select cnns to use for background calculations.\n\n    Uses background samples in cohort, and will remove CNNs with high\n    on target variability. Uses (number of segments * biweight midvariance) as metric\n    for variability with higher numbers being more unreliable.\n    \"\"\"\n    min_for_variability_analysis = 20\n    pct_keep = 0.10\n    b_cnns = [x for x in cnns if x[\"itype\"] == \"background\" and x.get(\"metrics\")]\n    assert len(b_cnns) % 2 == 0, \"Expect even set of target/antitarget cnns for background\"\n    if len(b_cnns) >= min_for_variability_analysis:\n        b_cnns_w_metrics = []\n        for b_cnn in b_cnns:\n            unreliability = b_cnn[\"metrics\"][\"segments\"] * b_cnn[\"metrics\"][\"bivar\"]\n            b_cnns_w_metrics.append((unreliability, b_cnn))\n        b_cnns_w_metrics.sort()\n        to_keep = int(math.ceil(pct_keep * len(b_cnns) / 2.0) * 2)\n        b_cnns = [x[1] for x in b_cnns_w_metrics][:to_keep]\n        assert len(b_cnns) % 2 == 0, \"Expect even set of target/antitarget cnns for background\"\n    return [x[\"file\"] for x in b_cnns]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate background reference handling flat case with no normal sample.", "response": "def cnvkit_background(background_cnns, out_file, items, target_bed=None, antitarget_bed=None):\n    \"\"\"Calculate background reference, handling flat case with no normal sample.\n    \"\"\"\n    if not utils.file_exists(out_file):\n        with file_transaction(items[0], out_file) as tx_out_file:\n            cmd = [_get_cmd(), \"reference\", \"-f\", dd.get_ref_file(items[0]), \"-o\", tx_out_file]\n            gender = _get_batch_gender(items)\n            if gender:\n                cmd += [\"--sample-sex\", gender]\n            if len(background_cnns) == 0:\n                assert target_bed and antitarget_bed, \"Missing CNNs and target BEDs for flat background\"\n                cmd += [\"-t\", target_bed, \"-a\", antitarget_bed]\n            else:\n                cmd += background_cnns\n            do.run(_prep_cmd(cmd, tx_out_file), \"CNVkit background\")\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_batch_gender(items):\n    genders = set([population.get_gender(x) for x in items])\n    if len(genders) == 1:\n        gender = genders.pop()\n        if gender != \"unknown\":\n            return gender", "response": "Retrieve gender for a batch of items if consistent."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef targets_w_bins(cnv_file, access_file, target_anti_fn, work_dir, data):\n    target_file = os.path.join(work_dir, \"%s-target.bed\" % dd.get_sample_name(data))\n    anti_file = os.path.join(work_dir, \"%s-antitarget.bed\" % dd.get_sample_name(data))\n    if not utils.file_exists(target_file):\n        target_bin, _ = target_anti_fn()\n        with file_transaction(data, target_file) as tx_out_file:\n            cmd = [_get_cmd(), \"target\", cnv_file, \"--split\", \"-o\", tx_out_file,\n                   \"--avg-size\", str(target_bin)]\n            do.run(_prep_cmd(cmd, tx_out_file), \"CNVkit target\")\n    if not os.path.exists(anti_file):\n        _, anti_bin = target_anti_fn()\n        with file_transaction(data, anti_file) as tx_out_file:\n            # Create access file without targets to avoid overlap\n            # antitarget in cnvkit is meant to do this but appears to not always happen\n            # after chromosome 1\n            tx_access_file = os.path.join(os.path.dirname(tx_out_file), os.path.basename(access_file))\n            pybedtools.BedTool(access_file).subtract(cnv_file).saveas(tx_access_file)\n            cmd = [_get_cmd(), \"antitarget\", \"-g\", tx_access_file, cnv_file, \"-o\", tx_out_file,\n                   \"--avg-size\", str(anti_bin)]\n            do.run(_prep_cmd(cmd, tx_out_file), \"CNVkit antitarget\")\n    return target_file, anti_file", "response": "Calculate target and anti - target files with pre - determined bins."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves target and antitarget BEDs from background CNN file.", "response": "def targets_from_background(back_cnn, work_dir, data):\n    \"\"\"Retrieve target and antitarget BEDs from background CNN file.\n    \"\"\"\n    target_file = os.path.join(work_dir, \"%s.target.bed\" % dd.get_sample_name(data))\n    anti_file = os.path.join(work_dir, \"%s.antitarget.bed\" % dd.get_sample_name(data))\n    if not utils.file_exists(target_file):\n        with file_transaction(data, target_file) as tx_out_file:\n            out_base = tx_out_file.replace(\".target.bed\", \"\")\n            cmd = [_get_cmd(\"reference2targets.py\"), \"-o\", out_base, back_cnn]\n            do.run(_prep_cmd(cmd, tx_out_file), \"CNVkit targets from background\")\n            shutil.copy(out_base + \".antitarget.bed\", anti_file)\n    return target_file, anti_file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_seg_to_output(out, data, enumerate_chroms=False):\n    out_file = \"%s.seg\" % os.path.splitext(out[\"cns\"])[0]\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            cmd = [os.path.join(os.path.dirname(sys.executable), \"cnvkit.py\"), \"export\",\n                   \"seg\"]\n            if enumerate_chroms:\n                cmd += [\"--enumerate-chroms\"]\n            cmd += [\"-o\", tx_out_file, out[\"cns\"]]\n            do.run(cmd, \"CNVkit export seg\")\n    out[\"seg\"] = out_file\n    return out", "response": "Export outputs to seg format compatible with IGV and GenePattern.\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compatible_small_variants(data, items):\n    from bcbio import heterogeneity\n    VarFile = collections.namedtuple(\"VarFile\", [\"name\", \"sample\", \"normal\"])\n    out = []\n    paired = vcfutils.get_paired(items)\n    for v in heterogeneity.get_variants(data, include_germline=not paired):\n        vrn_file = v[\"vrn_file\"]\n        base, ext = utils.splitext_plus(os.path.basename(vrn_file))\n        if paired:\n            out.append(VarFile(vrn_file, paired.tumor_name, paired.normal_name))\n        else:\n            out.append(VarFile(vrn_file, dd.get_sample_name(data), None))\n    return out", "response": "Retrieve small variant VCFs compatible with CNVkit."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds variant calls to the output file.", "response": "def _add_variantcalls_to_output(out, data, items, is_somatic=False):\n    \"\"\"Call ploidy and convert into VCF and BED representations.\n    \"\"\"\n    call_file = \"%s-call%s\" % os.path.splitext(out[\"cns\"])\n    if not utils.file_exists(call_file):\n        with file_transaction(data, call_file) as tx_call_file:\n            filters = [\"--filter\", \"cn\"]\n            cmd = [os.path.join(os.path.dirname(sys.executable), \"cnvkit.py\"), \"call\"] + \\\n                  filters + \\\n                   [\"--ploidy\", str(ploidy.get_ploidy([data])),\n                    \"-o\", tx_call_file, out[\"cns\"]]\n            small_vrn_files = _compatible_small_variants(data, items)\n            if len(small_vrn_files) > 0 and _cna_has_values(out[\"cns\"]):\n                cmd += [\"--vcf\", small_vrn_files[0].name, \"--sample-id\", small_vrn_files[0].sample]\n                if small_vrn_files[0].normal:\n                    cmd += [\"--normal-id\", small_vrn_files[0].normal]\n                if not is_somatic:\n                    cmd += [\"-m\", \"clonal\"]\n            gender = _get_batch_gender(items)\n            if gender:\n                cmd += [\"--sample-sex\", gender]\n            do.run(cmd, \"CNVkit call ploidy\")\n    calls = {}\n    for outformat in [\"bed\", \"vcf\"]:\n        out_file = \"%s.%s\" % (os.path.splitext(call_file)[0], outformat)\n        calls[outformat] = out_file\n        if not os.path.exists(out_file):\n            with file_transaction(data, out_file) as tx_out_file:\n                cmd = [os.path.join(os.path.dirname(sys.executable), \"cnvkit.py\"), \"export\",\n                       outformat, \"--sample-id\", dd.get_sample_name(data),\n                       \"--ploidy\", str(ploidy.get_ploidy([data])),\n                       \"-o\", tx_out_file, call_file]\n                do.run(cmd, \"CNVkit export %s\" % outformat)\n    out[\"call_file\"] = call_file\n    out[\"vrn_bed\"] = annotate.add_genes(calls[\"bed\"], data)\n    effects_vcf, _ = effects.add_to_vcf(calls[\"vcf\"], data, \"snpeff\")\n    out[\"vrn_file\"] = effects_vcf or calls[\"vcf\"]\n    out[\"vrn_file\"] = shared.annotate_with_depth(out[\"vrn_file\"], items)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _add_segmetrics_to_output(out, data):\n    out_file = \"%s-segmetrics.txt\" % os.path.splitext(out[\"cns\"])[0]\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            cmd = [os.path.join(os.path.dirname(sys.executable), \"cnvkit.py\"), \"segmetrics\",\n                   \"--median\", \"--iqr\", \"--ci\", \"--pi\",\n                   \"-s\", out[\"cns\"], \"-o\", tx_out_file, out[\"cnr\"]]\n            # Use less fine grained bootstrapping intervals for whole genome runs\n            if dd.get_coverage_interval(data) == \"genome\":\n                cmd += [\"--alpha\", \"0.1\", \"--bootstrap\", \"50\"]\n            else:\n                cmd += [\"--alpha\", \"0.01\", \"--bootstrap\", \"500\"]\n            do.run(cmd, \"CNVkit segmetrics\")\n    out[\"segmetrics\"] = out_file\n    return out", "response": "Add metrics for measuring reliability of CNV estimates."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds gainloss based on genes.", "response": "def _add_gainloss_to_output(out, data):\n    \"\"\"Add gainloss based on genes, helpful for identifying changes in smaller genes.\n    \"\"\"\n    out_file = \"%s-gainloss.txt\" % os.path.splitext(out[\"cns\"])[0]\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            cmd = [os.path.join(os.path.dirname(sys.executable), \"cnvkit.py\"), \"gainloss\",\n                   \"-s\", out[\"cns\"], \"-o\", tx_out_file, out[\"cnr\"]]\n            gender = _get_batch_gender([data])\n            if gender:\n                cmd += [\"--sample-sex\", gender]\n            do.run(cmd, \"CNVkit gainloss\")\n    out[\"gainloss\"] = out_file\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_coverage_bedgraph_to_output(out, data):\n    out_file = \"%s.coverage.bedgraph\" % os.path.splitext(out[\"cns\"])[0]\n    if utils.file_exists(out_file):\n        out[\"bedgraph\"] = out_file\n        return out\n    bam_file = dd.get_align_bam(data)\n    bedtools = config_utils.get_program(\"bedtools\", data[\"config\"])\n    samtools = config_utils.get_program(\"samtools\", data[\"config\"])\n    cns_file = out[\"cns\"]\n    bed_file = tempfile.NamedTemporaryFile(suffix=\".bed\", delete=False).name\n    with file_transaction(data, out_file) as tx_out_file:\n        cmd = (\"sed 1d {cns_file} | cut -f1,2,3 > {bed_file}; \"\n               \"{samtools} view -b -L {bed_file} {bam_file} | \"\n               \"{bedtools} genomecov -bg -ibam - -g {bed_file} >\"\n               \"{tx_out_file}\").format(**locals())\n        do.run(cmd, \"CNVkit bedGraph conversion\")\n        os.remove(bed_file)\n    out[\"bedgraph\"] = out_file\n    return out", "response": "Add BedGraph representation of coverage to the output"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_plots_to_output(out, data):\n    out[\"plot\"] = {}\n    diagram_plot = _add_diagram_plot(out, data)\n    if diagram_plot:\n        out[\"plot\"][\"diagram\"] = diagram_plot\n    scatter = _add_scatter_plot(out, data)\n    if scatter:\n        out[\"plot\"][\"scatter\"] = scatter\n    scatter_global = _add_global_scatter_plot(out, data)\n    if scatter_global:\n        out[\"plot\"][\"scatter_global\"] = scatter_global\n    return out", "response": "Add plots summarizing called copy number values."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves larger chromosomes avoiding the smaller ones for plotting.", "response": "def _get_larger_chroms(ref_file):\n    \"\"\"Retrieve larger chromosomes, avoiding the smaller ones for plotting.\n    \"\"\"\n    from scipy.cluster.vq import kmeans, vq\n    all_sizes = []\n    for c in ref.file_contigs(ref_file):\n        all_sizes.append(float(c.size))\n    all_sizes.sort()\n    if len(all_sizes) > 5:\n        # separate out smaller chromosomes and haplotypes with kmeans\n        centroids, _ = kmeans(np.array(all_sizes), 2)\n        idx, _ = vq(np.array(all_sizes), centroids)\n        little_sizes = tz.first(tz.partitionby(lambda xs: xs[0], zip(idx, all_sizes)))\n        little_sizes = [x[1] for x in little_sizes]\n        # create one more cluster with the smaller, removing the haplotypes\n        centroids2, _ = kmeans(np.array(little_sizes), 2)\n        idx2, _ = vq(np.array(little_sizes), centroids2)\n        little_sizes2 = tz.first(tz.partitionby(lambda xs: xs[0], zip(idx2, little_sizes)))\n        little_sizes2 = [x[1] for x in little_sizes2]\n        # get any chromosomes not in haplotype/random bin\n        thresh = max(little_sizes2)\n    else:\n        thresh = 0\n    larger_chroms = []\n    for c in ref.file_contigs(ref_file):\n        if c.size > thresh:\n            larger_chroms.append(c.name)\n    return larger_chroms"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _remove_haplotype_chroms(in_file, data):\n    larger_chroms = set(_get_larger_chroms(dd.get_ref_file(data)))\n    out_file = os.path.join(_sv_workdir(data), \"%s-chromfilter%s\" % utils.splitext_plus(os.path.basename(in_file)))\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            with open(in_file) as in_handle:\n                with open(tx_out_file, \"w\") as out_handle:\n                    for line in in_handle:\n                        if ((line.find(\"chromosome\") >= 0 and line.find(\"start\") >= 0)\n                              or line.split()[0] in larger_chroms):\n                            out_handle.write(line)\n    return out_file", "response": "Remove shorter haplotype chromosomes from cns and cnr files for plotting."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if cnr or cns files are empty", "response": "def _cnx_is_empty(in_file):\n    \"\"\"Check if cnr or cns files are empty (only have a header)\n    \"\"\"\n    with open(in_file) as in_handle:\n        for i, line in enumerate(in_handle):\n            if i > 0:\n                return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef segment_from_cnr(cnr_file, data, out_base):\n    cns_file = _cnvkit_segment(cnr_file, dd.get_coverage_interval(data),\n                               data, [data], out_file=\"%s.cns\" % out_base, detailed=True)\n    out = _add_seg_to_output({\"cns\": cns_file}, data, enumerate_chroms=False)\n    return out[\"seg\"]", "response": "Provide segmentation on a cnr file used in external PureCN integration."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef export_theta(ckout, data):\n    cns_file = chromhacks.bed_to_standardonly(ckout[\"cns\"], data, headers=\"chromosome\")\n    cnr_file = chromhacks.bed_to_standardonly(ckout[\"cnr\"], data, headers=\"chromosome\")\n    out_file = \"%s-theta.input\" % utils.splitext_plus(cns_file)[0]\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            cmd = [_get_cmd(), \"export\", \"theta\", cns_file, cnr_file, \"-o\", tx_out_file]\n            do.run(_prep_cmd(cmd, tx_out_file), \"Export CNVkit calls as inputs for TheTA2\")\n    ckout[\"theta_input\"] = out_file\n    return ckout", "response": "Provide updated set of data with export information for TheTA2 input."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nestimate expression of a sequence of transcripts using Stringtie.", "response": "def _stringtie_expression(bam, data, out_dir=\".\"):\n    \"\"\"\n    only estimate expression the Stringtie, do not assemble new transcripts\n    \"\"\"\n    gtf_file = dd.get_gtf_file(data)\n    num_cores = dd.get_num_cores(data)\n    error_message = \"The %s file for %s is missing. StringTie has an error.\"\n    stringtie = config_utils.get_program(\"stringtie\", data, default=\"stringtie\")\n    # don't assemble transcripts unless asked\n    exp_flag = (\"-e\" if \"stringtie\" not in dd.get_transcript_assembler(data)\n                else \"\")\n    base_cmd = (\"{stringtie} {exp_flag} -b {out_dir} -p {num_cores} -G {gtf_file} \"\n                \"-o {out_gtf} {bam}\")\n    transcript_file = os.path.join(out_dir, \"t_data.ctab\")\n    exon_file = os.path.join(out_dir, \"e_data.ctab\")\n    out_gtf = os.path.join(out_dir, \"stringtie-assembly.gtf\")\n    if file_exists(transcript_file):\n        return exon_file, transcript_file, out_gtf\n    cmd = base_cmd.format(**locals())\n    do.run(cmd, \"Running Stringtie on %s.\" % bam)\n    assert file_exists(exon_file), error_message % (\"exon\", exon_file)\n    assert file_exists(transcript_file), error_message % (\"transcript\", transcript_file)\n    return transcript_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun Stringtie expression from Stringtie using bcbio datadict does not do transcriptome assembly", "response": "def run_stringtie_expression(data):\n    \"\"\"\n    estimate expression from Stringtie, using the bcbio datadict\n    does not do transcriptome assembly\n    \"\"\"\n    bam = dd.get_work_bam(data)\n    sample_name = dd.get_sample_name(data)\n    out_dir = os.path.join(\"stringtie\", sample_name)\n    isoform_fpkm = os.path.join(out_dir, sample_name + \".isoform.fpkm\")\n    gene_fpkm = os.path.join(out_dir, sample_name + \".fpkm\")\n    assembly = os.path.abspath(os.path.join(out_dir, \"stringtie-assembly.gtf\"))\n    if file_exists(isoform_fpkm) and file_exists(gene_fpkm):\n        data = dd.set_stringtie_dir(data, out_dir)\n        data = dd.set_fpkm(data, gene_fpkm)\n        data = dd.set_fpkm_isoform(data, isoform_fpkm)\n        if \"stringtie\" in dd.get_transcript_assembler(data):\n            assembled_gtfs = dd.get_assembled_gtf(data)\n            assembled_gtfs.append(assembly)\n            data = dd.set_assembled_gtf(data, assembled_gtfs)\n        return data\n    with file_transaction(data, out_dir) as tx_out_dir:\n        transcript_file = _stringtie_expression(bam, data, tx_out_dir)\n        df = _parse_ballgown(transcript_file)\n        _write_fpkms(df, tx_out_dir, sample_name)\n    data = dd.set_stringtie_dir(data, out_dir)\n    data = dd.set_fpkm(data, gene_fpkm)\n    data = dd.set_fpkm_isoform(data, isoform_fpkm)\n    if \"stringtie\" in dd.get_transcript_assembler(data):\n        assembled_gtfs = dd.get_assembled_gtf(data)\n        assembled_gtfs.append(assembly)\n        data = dd.set_assembled_gtf(data, assembled_gtfs)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_file(finfo, sample_info, config, pass_uptodate=False):\n    storage_dir = utils.safe_makedir(_get_storage_dir(finfo, config))\n    if finfo.get(\"type\") == \"directory\":\n        return _copy_finfo_directory(finfo, storage_dir)\n    else:\n        return _copy_finfo(finfo, storage_dir, pass_uptodate=pass_uptodate)", "response": "Update the file in local filesystem storage."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndries update the file", "response": "def get_upload_path(finfo, sample_info, config):\n    \"\"\"\"Dry\" update the file: only return the upload path\n    \"\"\"\n    try:\n        storage_dir = _get_storage_dir(finfo, config)\n    except ValueError:\n        return None\n    \n    if finfo.get(\"type\") == \"directory\":\n        return _get_dir_upload_path(finfo, storage_dir)\n    else:\n        return _get_file_upload_path(finfo, storage_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncopy a file into the output storage directory.", "response": "def _copy_finfo(finfo, storage_dir, pass_uptodate=False):\n    \"\"\"Copy a file into the output storage directory.\n    \"\"\"\n    out_file = _get_file_upload_path(finfo, storage_dir)\n    if not shared.up_to_date(out_file, finfo):\n        logger.info(\"Storing in local filesystem: %s\" % out_file)\n        shutil.copy(finfo[\"path\"], out_file)\n        return out_file\n    if pass_uptodate:\n        return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _copy_finfo_directory(finfo, out_dir):\n    out_dir = _get_dir_upload_path(finfo, out_dir)\n    if not shared.up_to_date(out_dir, finfo):\n        logger.info(\"Storing directory in local filesystem: %s\" % out_dir)\n        if os.path.exists(out_dir):\n            shutil.rmtree(out_dir)\n        shutil.copytree(finfo[\"path\"], out_dir)\n        for tmpdir in [\"tx\", \"tmp\"]:\n            if os.path.exists(os.path.join(out_dir, tmpdir)):\n                shutil.rmtree(os.path.join(out_dir, tmpdir))\n        os.utime(out_dir, None)\n    return out_dir", "response": "Copy a directory into the final output directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _write_newname_file(in_file, out_file, mappings):\n    with utils.open_gzipsafe(in_file) as in_handle:\n        with open(out_file, \"w\") as out_handle:\n            for line in in_handle:\n                if line.startswith(\"#\"):\n                    out_handle.write(line)\n                else:\n                    parts = line.split(\"\\t\")\n                    new_contig = mappings.get(parts[0])\n                    if new_contig:\n                        parts[0] = new_contig\n                        out_handle.write(\"\\t\".join(parts))", "response": "Re - write an input file with contigs matching the correct reference."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the contigs in the input file match the defined contigs in the reference genome.", "response": "def _matches_contigs(in_file, contigs, checked_file):\n    \"\"\"Check if the contigs in the input file match the defined contigs in the reference genome.\n    \"\"\"\n    tocheck_contigs = 2\n    if utils.file_exists(checked_file):\n        with open(checked_file) as in_handle:\n            return in_handle.read().strip() == \"match\"\n    else:\n        with utils.open_gzipsafe(in_file) as in_handle:\n            to_check = set([])\n            for line in in_handle:\n                if not line.startswith(\"#\"):\n                    to_check.add(line.split()[0])\n                if len(to_check) >= tocheck_contigs:\n                    break\n        with open(checked_file, \"w\") as out_handle:\n            if any([c not in contigs for c in to_check]):\n                out_handle.write(\"different\")\n                return False\n            else:\n                out_handle.write(\"match\")\n                return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef align_bam(in_bam, ref_file, names, align_dir, data):\n    config = data[\"config\"]\n    out_file = os.path.join(align_dir, \"{0}-sort.bam\".format(names[\"lane\"]))\n    novoalign = config_utils.get_program(\"novoalign\", config)\n    samtools = config_utils.get_program(\"samtools\", config)\n    resources = config_utils.get_resources(\"novoalign\", config)\n    num_cores = config[\"algorithm\"].get(\"num_cores\", 1)\n    max_mem = resources.get(\"memory\", \"4G\").upper()\n    extra_novo_args = \" \".join(_novoalign_args_from_config(config, False))\n\n    if not file_exists(out_file):\n        with tx_tmpdir(data, base_dir=align_dir) as work_dir:\n            with postalign.tobam_cl(data, out_file, bam.is_paired(in_bam)) as (tobam_cl, tx_out_file):\n                rg_info = get_rg_info(names)\n                tx_out_prefix = os.path.splitext(tx_out_file)[0]\n                prefix1 = \"%s-in1\" % tx_out_prefix\n                cmd = (\"unset JAVA_HOME && \"\n                       \"{samtools} sort -n -o -l 1 -@ {num_cores} -m {max_mem} {in_bam} {prefix1} \"\n                       \"| {novoalign} -o SAM '{rg_info}' -d {ref_file} -f /dev/stdin \"\n                       \"  -F BAMPE -c {num_cores} {extra_novo_args} | \")\n                cmd = (cmd + tobam_cl).format(**locals())\n                do.run(cmd, \"Novoalign: %s\" % names[\"sample\"], None,\n                       [do.file_nonempty(tx_out_file), do.file_reasonable_size(tx_out_file, in_bam)])\n    return out_file", "response": "Perform realignment of input BAM file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef align_pipe(fastq_file, pair_file, ref_file, names, align_dir, data):\n    pair_file = pair_file if pair_file else \"\"\n    # back compatible -- older files were named with lane information, use sample name now\n    out_file = os.path.join(align_dir, \"{0}-sort.bam\".format(names[\"lane\"]))\n    if not utils.file_exists(out_file):\n        out_file = os.path.join(align_dir, \"{0}-sort.bam\".format(dd.get_sample_name(data)))\n    if data.get(\"align_split\") or fastq_file.endswith(\".sdf\"):\n        final_file = out_file\n        out_file, data = alignprep.setup_combine(final_file, data)\n        fastq_file, pair_file = alignprep.split_namedpipe_cls(fastq_file, pair_file, data)\n    else:\n        final_file = None\n    samtools = config_utils.get_program(\"samtools\", data[\"config\"])\n    novoalign = config_utils.get_program(\"novoalign\", data[\"config\"])\n    resources = config_utils.get_resources(\"novoalign\", data[\"config\"])\n    num_cores = data[\"config\"][\"algorithm\"].get(\"num_cores\", 1)\n    max_mem = resources.get(\"memory\", \"1G\")\n    extra_novo_args = \" \".join(_novoalign_args_from_config(data[\"config\"]))\n    rg_info = get_rg_info(names)\n    if not utils.file_exists(out_file) and (final_file is None or not utils.file_exists(final_file)):\n        with tx_tmpdir(data) as work_dir:\n            with postalign.tobam_cl(data, out_file, pair_file != \"\") as (tobam_cl, tx_out_file):\n                tx_out_prefix = os.path.splitext(tx_out_file)[0]\n                cmd = (\"unset JAVA_HOME && \"\n                       \"{novoalign} -o SAM '{rg_info}' -d {ref_file} -f {fastq_file} {pair_file} \"\n                       \"  -c {num_cores} {extra_novo_args} | \")\n                cmd = (cmd + tobam_cl).format(**locals())\n                do.run(cmd, \"Novoalign: %s\" % names[\"sample\"], None,\n                       [do.file_nonempty(tx_out_file), do.file_reasonable_size(tx_out_file, fastq_file)])\n    data[\"work_bam\"] = out_file\n    return data", "response": "Perform piped alignment of fastq files generating sorted output BAM."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nselect novoalign options based on configuration parameters.", "response": "def _novoalign_args_from_config(config, need_quality=True):\n    \"\"\"Select novoalign options based on configuration parameters.\n    \"\"\"\n    if need_quality:\n        qual_format = config[\"algorithm\"].get(\"quality_format\", \"\").lower()\n        qual_flags = [\"-F\", \"ILMFQ\" if qual_format == \"illumina\" else \"STDFQ\"]\n    else:\n        qual_flags = []\n    multi_mappers = config[\"algorithm\"].get(\"multiple_mappers\")\n    if multi_mappers is True:\n        multi_flag = \"Random\"\n    elif isinstance(multi_mappers, six.string_types):\n        multi_flag = multi_mappers\n    else:\n        multi_flag = \"None\"\n    multi_flags = [\"-r\"] + multi_flag.split()\n    resources = config_utils.get_resources(\"novoalign\", config)\n    # default arguments for improved variant calling based on\n    # comparisons to reference materials: turn off soft clipping and recalibrate\n    if resources.get(\"options\") is None:\n        extra_args = [\"-o\", \"FullNW\", \"-k\"]\n    else:\n        extra_args = [str(x) for x in resources.get(\"options\", [])]\n    return qual_flags + multi_flags + extra_args"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remap_index_fn(ref_file):\n    checks = [os.path.splitext(ref_file)[0].replace(\"/seq/\", \"/novoalign/\"),\n              os.path.splitext(ref_file)[0] + \".ndx\",\n              ref_file + \".bs.ndx\",\n              ref_file + \".ndx\"]\n    for check in checks:\n        if os.path.exists(check):\n            return check\n    return checks[0]", "response": "Map sequence references to equivalent novoalign indexes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving export statement for sentieon license server.", "response": "def license_export(data):\n    \"\"\"Retrieve export statement for sentieon license server.\n    \"\"\"\n    resources = config_utils.get_resources(\"sentieon\", data[\"config\"])\n    server = resources.get(\"keyfile\")\n    if not server:\n        server = tz.get_in([\"resources\", \"sentieon\", \"keyfile\"], data)\n    if not server:\n        raise ValueError(\"Need to set resources keyfile with URL:port of license server, local license file or \"\n                         \"environmental variables to export \\n\"\n                         \"http://bcbio-nextgen.readthedocs.io/en/latest/contents/configuration.html#resources\\n\"\n                         \"Configuration: %s\" % pprint.pformat(data))\n    if isinstance(server, six.string_types):\n        return \"export SENTIEON_LICENSE=%s && \" % server\n    else:\n        assert isinstance(server, dict), server\n        exports = \"\"\n        for key, val in server.items():\n            exports += \"export %s=%s && \" % (key.upper(), val)\n        return exports"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_interval(variant_regions, region, out_file, items):\n    target = shared.subset_variant_regions(variant_regions, region, out_file, items)\n    if target:\n        if isinstance(target, six.string_types) and os.path.isfile(target):\n            return \"--interval %s\" % target\n        else:\n            return \"--interval %s\" % bamprep.region_to_gatk(target)\n    else:\n        return \"\"", "response": "Retrieve interval to run analysis in. Handles no targets BED and regions\n\n    region can be a single region or a list of multiple regions for multicore calling."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_tnscope(align_bams, items, ref_file, assoc_files,\n                     region=None, out_file=None):\n    \"\"\"Call variants with Sentieon's TNscope somatic caller.\n    \"\"\"\n    if out_file is None:\n        out_file = \"%s-variants.vcf.gz\" % utils.splitext_plus(align_bams[0])[0]\n    if not utils.file_exists(out_file):\n        variant_regions = bedutils.population_variant_regions(items, merged=True)\n        interval = _get_interval(variant_regions, region, out_file, items)\n        with file_transaction(items[0], out_file) as tx_out_file:\n            paired = vcfutils.get_paired_bams(align_bams, items)\n            assert paired and paired.normal_bam, \"Require normal BAM for Sentieon TNscope\"\n            dbsnp = \"--dbsnp %s\" % (assoc_files.get(\"dbsnp\")) if \"dbsnp\" in assoc_files else \"\"\n            license = license_export(items[0])\n            cores = dd.get_num_cores(items[0])\n            cmd = (\"{license}sentieon driver -t {cores} -r {ref_file} \"\n                   \"-i {paired.tumor_bam} -i {paired.normal_bam} {interval} \"\n                   \"--algo TNscope \"\n                   \"--tumor_sample {paired.tumor_name} --normal_sample {paired.normal_name} \"\n                   \"{dbsnp} {tx_out_file}\")\n            do.run(cmd.format(**locals()), \"Sentieon TNscope\")\n    return out_file", "response": "Call variants with Sentieon s TNscope somatic caller."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_haplotyper(align_bams, items, ref_file, assoc_files,\n                     region=None, out_file=None):\n    \"\"\"Call variants with Sentieon's haplotyper (GATK HaplotypeCaller like).\n    \"\"\"\n    if out_file is None:\n        out_file = \"%s-variants.vcf.gz\" % utils.splitext_plus(align_bams[0])[0]\n    if not utils.file_exists(out_file):\n        variant_regions = bedutils.population_variant_regions(items, merged=True)\n        interval = _get_interval(variant_regions, region, out_file, items)\n        with file_transaction(items[0], out_file) as tx_out_file:\n            dbsnp = \"--dbsnp %s\" % (assoc_files.get(\"dbsnp\")) if \"dbsnp\" in assoc_files else \"\"\n            bams = \" \".join([\"-i %s\" % x for x in align_bams])\n            license = license_export(items[0])\n            cores = dd.get_num_cores(items[0])\n            out_mode = \"--emit_mode gvcf\" if joint.want_gvcf(items) else \"\"\n            cmd = (\"{license}sentieon driver -t {cores} -r {ref_file} \"\n                   \"{bams} {interval} --algo Haplotyper {out_mode} {dbsnp} {tx_out_file}\")\n            do.run(cmd.format(**locals()), \"Sentieon Haplotyper\")\n    return out_file", "response": "Call variants with Sentieon s haplotyper."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nproducing joint called variants from input gVCF files.", "response": "def run_gvcftyper(vrn_files, out_file, region, data):\n    \"\"\"Produce joint called variants from input gVCF files.\n    \"\"\"\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            license = license_export(data)\n            ref_file = dd.get_ref_file(data)\n            input_files = \" \".join(vrn_files)\n            region = bamprep.region_to_gatk(region)\n            cmd = (\"{license}sentieon driver -r {ref_file} --interval {region} \"\n                   \"--algo GVCFtyper {tx_out_file} {input_files}\")\n            do.run(cmd.format(**locals()), \"Sentieon GVCFtyper\")\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating recalibration tables for BQSR.", "response": "def bqsr_table(data):\n    \"\"\"Generate recalibration tables as inputs to BQSR.\n    \"\"\"\n    in_file = dd.get_align_bam(data)\n    out_file = \"%s-recal-table.txt\" % utils.splitext_plus(in_file)[0]\n    if not utils.file_uptodate(out_file, in_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            assoc_files = dd.get_variation_resources(data)\n            known = \"-k %s\" % (assoc_files.get(\"dbsnp\")) if \"dbsnp\" in assoc_files else \"\"\n            license = license_export(data)\n            cores = dd.get_num_cores(data)\n            ref_file = dd.get_ref_file(data)\n            cmd = (\"{license}sentieon driver -t {cores} -r {ref_file} \"\n                   \"-i {in_file} --algo QualCal {known} {tx_out_file}\")\n            do.run(cmd.format(**locals()), \"Sentieon QualCal generate table\")\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a fastq or BAM input into a SDF indexed file.", "response": "def to_sdf(files, data):\n    \"\"\"Convert a fastq or BAM input into a SDF indexed file.\n    \"\"\"\n    # BAM\n    if len(files) == 1 and files[0].endswith(\".bam\"):\n        qual = []\n        format = [\"-f\", \"sam-pe\" if bam.is_paired(files[0]) else \"sam-se\"]\n        inputs = [files[0]]\n    # fastq\n    else:\n        qual = [\"-q\", \"illumina\" if dd.get_quality_format(data).lower() == \"illumina\" else \"sanger\"]\n        format = [\"-f\", \"fastq\"]\n        if len(files) == 2:\n            inputs = [\"-l\", files[0], \"-r\", files[1]]\n        else:\n            assert len(files) == 1\n            inputs = [files[0]]\n    work_dir = utils.safe_makedir(os.path.join(data[\"dirs\"][\"work\"], \"align_prep\"))\n    out_file = os.path.join(work_dir,\n                            \"%s.sdf\" % utils.splitext_plus(os.path.basename(os.path.commonprefix(files)))[0])\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            cmd = _rtg_cmd([\"rtg\", \"format\", \"-o\", tx_out_file] + format + qual + inputs)\n            do.run(cmd, \"Format inputs to indexed SDF\")\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving the splits of the dataset.", "response": "def calculate_splits(sdf_file, split_size):\n    \"\"\"Retrieve\n    \"\"\"\n    counts = _sdfstats(sdf_file)[\"counts\"]\n    splits = []\n    cur = 0\n    for i in range(counts // split_size + (0 if counts % split_size == 0 else 1)):\n        splits.append(\"%s-%s\" % (cur, min(counts, cur + split_size)))\n        cur += split_size\n    return splits"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a command line to provide streaming fastq input.", "response": "def to_fastq_apipe_cl(sdf_file, start=None, end=None):\n    \"\"\"Return a command lines to provide streaming fastq input.\n\n    For paired end, returns a forward and reverse command line. For\n    single end returns a single command line and None for the pair.\n    \"\"\"\n    cmd = [\"rtg\", \"sdf2fastq\", \"--no-gzip\", \"-o\", \"-\"]\n    if start is not None:\n        cmd += [\"--start-id=%s\" % start]\n    if end is not None:\n        cmd += [\"--end-id=%s\" % end]\n    if is_paired(sdf_file):\n        out = []\n        for ext in [\"left\", \"right\"]:\n            out.append(\"<(%s)\" % _rtg_cmd(cmd + [\"-i\", os.path.join(sdf_file, ext)]))\n        return out\n    else:\n        cmd += [\"-i\", sdf_file]\n        return [\"<(%s)\" % _rtg_cmd(cmd), None]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving tabix command handling new bcftools tabix and older tabix.", "response": "def get_tabix_cmd(config):\n    \"\"\"Retrieve tabix command, handling new bcftools tabix and older tabix.\n    \"\"\"\n    try:\n        bcftools = config_utils.get_program(\"bcftools\", config)\n        # bcftools has terrible error codes and stderr output, swallow those.\n        bcftools_tabix = subprocess.check_output(\"{bcftools} 2>&1; echo $?\".format(**locals()),\n                                                 shell=True).decode().find(\"tabix\") >= 0\n    except config_utils.CmdNotFound:\n        bcftools_tabix = False\n    if bcftools_tabix:\n        return \"{0} tabix\".format(bcftools)\n    else:\n        tabix = config_utils.get_program(\"tabix\", config)\n        return tabix"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_bgzip_cmd(config, is_retry=False):\n    num_cores = tz.get_in([\"algorithm\", \"num_cores\"], config, 1)\n    cmd = config_utils.get_program(\"bgzip\", config)\n    if (not is_retry and num_cores > 1 and\n          \"pbgzip\" not in dd.get_tools_off({\"config\": config})):\n        cmd += \" --threads %s\" % num_cores\n    return cmd", "response": "Retrieve command to use for bgzip parallel threads."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_empty(bam_file):\n    bam_file = objectstore.cl_input(bam_file)\n    cmd = (\"set -o pipefail; \"\n           \"samtools view {bam_file} | head -1 | wc -l\")\n    p = subprocess.Popen(cmd.format(**locals()), shell=True,\n                         executable=do.find_bash(),\n                         stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n                         preexec_fn=lambda: signal.signal(signal.SIGPIPE, signal.SIG_DFL))\n    stdout, stderr = p.communicate()\n    stdout = stdout.decode()\n    stderr = stderr.decode()\n    if ((p.returncode == 0 or p.returncode == 141) and\n         (stderr == \"\" or (stderr.startswith(\"gof3r\") and stderr.endswith(\"broken pipe\")))):\n        return int(stdout) == 0\n    else:\n        raise ValueError(\"Failed to check empty status of BAM file: %s\" % str(stderr))", "response": "Determine if a BAM file is empty"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fake_index(in_bam, data):\n    index_file = \"%s.bai\" % in_bam\n    if not utils.file_exists(index_file):\n        with file_transaction(data, index_file) as tx_out_file:\n            with open(tx_out_file, \"w\") as out_handle:\n                out_handle.write(\"name sorted -- no index\")\n    return index_file", "response": "Create a fake index file for namesorted BAMs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nindexing a BAM file.", "response": "def index(in_bam, config, check_timestamp=True):\n    \"\"\"Index a BAM file, skipping if index present.\n\n    Centralizes BAM indexing providing ability to switch indexing approaches.\n    \"\"\"\n    assert is_bam(in_bam), \"%s in not a BAM file\" % in_bam\n    index_file = \"%s.bai\" % in_bam\n    alt_index_file = \"%s.bai\" % os.path.splitext(in_bam)[0]\n    if check_timestamp:\n        bai_exists = utils.file_uptodate(index_file, in_bam) or utils.file_uptodate(alt_index_file, in_bam)\n    else:\n        bai_exists = utils.file_exists(index_file) or utils.file_exists(alt_index_file)\n    if not bai_exists:\n        # Remove old index files and re-run to prevent linking into tx directory\n        for fname in [index_file, alt_index_file]:\n            utils.remove_safe(fname)\n        samtools = config_utils.get_program(\"samtools\", config)\n        num_cores = config[\"algorithm\"].get(\"num_cores\", 1)\n        with file_transaction(config, index_file) as tx_index_file:\n            cmd = \"{samtools} index -@ {num_cores} {in_bam} {tx_index_file}\"\n            do.run(cmd.format(**locals()), \"Index BAM file: %s\" % os.path.basename(in_bam))\n    return index_file if utils.file_exists(index_file) else alt_index_file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove(in_bam):\n    if utils.file_exists(in_bam):\n        utils.remove_safe(in_bam)\n    if utils.file_exists(in_bam + \".bai\"):\n        utils.remove_safe(in_bam + \".bai\")", "response": "remove bam file and index if exists"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn index stats for the given BAM file using samtools idxstats.", "response": "def idxstats(in_bam, data):\n    \"\"\"Return BAM index stats for the given file, using samtools idxstats.\n    \"\"\"\n    index(in_bam, data[\"config\"], check_timestamp=False)\n    AlignInfo = collections.namedtuple(\"AlignInfo\", [\"contig\", \"length\", \"aligned\", \"unaligned\"])\n    samtools = config_utils.get_program(\"samtools\", data[\"config\"])\n    idxstats_out = subprocess.check_output([samtools, \"idxstats\", in_bam]).decode()\n    out = []\n    for line in idxstats_out.split(\"\\n\"):\n        if line.strip():\n            contig, length, aligned, unaligned = line.split(\"\\t\")\n            out.append(AlignInfo(contig, int(length), int(aligned), int(unaligned)))\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a fai index with only contigs in the input BAM file.", "response": "def fai_from_bam(ref_file, bam_file, out_file, data):\n    \"\"\"Create a fai index with only contigs in the input BAM file.\n    \"\"\"\n    contigs = set([x.contig for x in idxstats(bam_file, data)])\n    if not utils.file_uptodate(out_file, bam_file):\n        with open(ref.fasta_idx(ref_file, data[\"config\"])) as in_handle:\n            with file_transaction(data, out_file) as tx_out_file:\n                with open(tx_out_file, \"w\") as out_handle:\n                    for line in (l for l in in_handle if l.strip()):\n                        if line.split()[0] in contigs:\n                            out_handle.write(line)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ref_file_from_bam(bam_file, data):\n    new_ref = os.path.join(utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"inputs\", \"ref\")),\n                           \"%s-subset.fa\" % dd.get_genome_build(data))\n    if not utils.file_exists(new_ref):\n        with file_transaction(data, new_ref) as tx_out_file:\n            contig_file = \"%s-contigs.txt\" % utils.splitext_plus(new_ref)[0]\n            with open(contig_file, \"w\") as out_handle:\n                for contig in [x.contig for x in idxstats(bam_file, data) if x.contig != \"*\"]:\n                    out_handle.write(\"%s\\n\" % contig)\n            cmd = \"seqtk subseq -l 100 %s %s > %s\" % (dd.get_ref_file(data), contig_file, tx_out_file)\n            do.run(cmd, \"Subset %s to BAM file contigs\" % dd.get_genome_build(data))\n    ref.fasta_idx(new_ref, data[\"config\"])\n    runner = broad.runner_from_path(\"picard\", data[\"config\"])\n    runner.run_fn(\"picard_index_ref\", new_ref)\n    return {\"base\": new_ref}", "response": "Subset a BAM file to only a fraction of input contigs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_downsample_pct(in_bam, target_counts, data):\n    total = sum(x.aligned for x in idxstats(in_bam, data))\n    with pysam.Samfile(in_bam, \"rb\") as work_bam:\n        n_rgs = max(1, len(work_bam.header.get(\"RG\", [])))\n    rg_target = n_rgs * target_counts\n    if total > rg_target:\n        pct = float(rg_target) / float(total)\n        if pct < 0.9:\n            return pct", "response": "Retrieve percentage of file to downsample to get to target counts."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef downsample(in_bam, data, target_counts, work_dir=None):\n    index(in_bam, data[\"config\"], check_timestamp=False)\n    ds_pct = get_downsample_pct(in_bam, target_counts, data)\n    if ds_pct:\n        out_file = \"%s-downsample%s\" % os.path.splitext(in_bam)\n        if work_dir:\n            out_file = os.path.join(work_dir, os.path.basename(out_file))\n        if not utils.file_exists(out_file):\n            with file_transaction(data, out_file) as tx_out_file:\n                samtools = config_utils.get_program(\"samtools\", data[\"config\"])\n                num_cores = dd.get_num_cores(data)\n                ds_pct = \"42.\" + \"{ds_pct:.3}\".format(ds_pct=ds_pct).replace(\"0.\", \"\")\n                cmd = (\"{samtools} view -O BAM -@ {num_cores} -o {tx_out_file} \"\n                       \"-s {ds_pct} {in_bam}\")\n                do.run(cmd.format(**locals()), \"Downsample BAM file: %s\" % os.path.basename(in_bam))\n        return out_file", "response": "Downsample a BAM file to the specified number of target counts."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_maxcov_downsample_cl(data, in_pipe=None):\n    max_cov = _get_maxcov_downsample(data) if dd.get_aligner(data) not in [\"snap\"] else None\n    if max_cov:\n        if in_pipe == \"bamsormadup\":\n            prefix = \"level=0\"\n        elif in_pipe == \"samtools\":\n            prefix = \"-l 0\"\n        else:\n            prefix = \"\"\n        # Swap over to multiple cores until after testing\n        #core_arg = \"-t %s\" % dd.get_num_cores(data)\n        core_arg = \"\"\n        return (\"%s | variant - -b %s --mark-as-qc-fail --max-coverage %s\"\n                % (prefix, core_arg, max_cov))\n    else:\n        if in_pipe == \"bamsormadup\":\n            prefix = \"indexfilename={tx_out_file}.bai\"\n        else:\n            prefix = \"\"\n        return prefix", "response": "Retrieve command line for max coverage downsampling fitting into bamsormadup output."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate maximum coverage downsampling for whole genome samples.", "response": "def _get_maxcov_downsample(data):\n    \"\"\"Calculate maximum coverage downsampling for whole genome samples.\n\n    Returns None if we're not doing downsampling.\n    \"\"\"\n    from bcbio.bam import ref\n    from bcbio.ngsalign import alignprep, bwa\n    from bcbio.variation import coverage\n    fastq_file = data[\"files\"][0]\n    params = alignprep.get_downsample_params(data)\n    if params:\n        num_reads = alignprep.total_reads_from_grabix(fastq_file)\n        if num_reads:\n            vrs = dd.get_variant_regions_merged(data)\n            total_size = sum([c.size for c in ref.file_contigs(dd.get_ref_file(data), data[\"config\"])])\n            if vrs:\n                callable_size = pybedtools.BedTool(vrs).total_coverage()\n                genome_cov_pct = callable_size / float(total_size)\n            else:\n                callable_size = total_size\n                genome_cov_pct = 1.0\n            if (genome_cov_pct > coverage.GENOME_COV_THRESH\n                  and dd.get_coverage_interval(data) in [\"genome\", None, False]):\n                total_counts, total_sizes = 0, 0\n                for count, size in bwa.fastq_size_output(fastq_file, 5000):\n                    total_counts += int(count)\n                    total_sizes += (int(size) * int(count))\n                read_size = float(total_sizes) / float(total_counts)\n                avg_cov = float(num_reads * read_size) / callable_size\n                if avg_cov >= params[\"min_coverage_for_downsampling\"]:\n                    return int(avg_cov * params[\"maxcov_downsample_multiplier\"])\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_header(in_bam, rgnames, ref_file, config):\n    _check_bam_contigs(in_bam, ref_file, config)\n    _check_sample(in_bam, rgnames)", "response": "Ensure passed in BAM header matches reference file and read groups names."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_sample(in_bam, rgnames):\n    with pysam.Samfile(in_bam, \"rb\") as bamfile:\n        rg = bamfile.header.get(\"RG\", [{}])\n    msgs = []\n    warnings = []\n    if len(rg) > 1:\n        warnings.append(\"Multiple read groups found in input BAM. Expect single RG per BAM.\")\n    if len(rg) == 0:\n        msgs.append(\"No read groups found in input BAM. Expect single RG per BAM.\")\n    if len(rg) > 0 and any(x.get(\"SM\") != rgnames[\"sample\"] for x in rg):\n        msgs.append(\"Read group sample name (SM) does not match configuration `description`: %s vs %s\"\n                    % (rg[0].get(\"SM\"), rgnames[\"sample\"]))\n    if len(msgs) > 0:\n        raise ValueError(\"Problems with pre-aligned input BAM file: %s\\n\" % (in_bam)\n                         + \"\\n\".join(msgs) +\n                         \"\\nSetting `bam_clean: fixrg`\\n\"\n                         \"in the configuration can often fix this issue.\")\n    if warnings:\n        print(\"*** Potential problems in input BAM compared to reference:\\n%s\\n\" %\n              \"\\n\".join(warnings))", "response": "Check if input sample name matches expected run group names."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _check_bam_contigs(in_bam, ref_file, config):\n    # GATK allows chromosome M to be in multiple locations, skip checking it\n    allowed_outoforder = [\"chrM\", \"MT\"]\n    ref_contigs = [c.name for c in ref.file_contigs(ref_file, config)]\n    with pysam.Samfile(in_bam, \"rb\") as bamfile:\n        bam_contigs = [c[\"SN\"] for c in bamfile.header[\"SQ\"]]\n    extra_bcs = [x for x in bam_contigs if x not in ref_contigs]\n    extra_rcs = [x for x in ref_contigs if x not in bam_contigs]\n    problems = []\n    warnings = []\n    for bc, rc in zip_longest([x for x in bam_contigs if (x not in extra_bcs and\n                                                                     x not in allowed_outoforder)],\n                                         [x for x in ref_contigs if (x not in extra_rcs and\n                                                                     x not in allowed_outoforder)]):\n        if bc != rc:\n            if bc and rc:\n                problems.append(\"Reference mismatch. BAM: %s Reference: %s\" % (bc, rc))\n            elif bc:\n                warnings.append(\"Extra BAM chromosomes: %s\" % bc)\n            elif rc:\n                warnings.append(\"Extra reference chromosomes: %s\" % rc)\n    for bc in extra_bcs:\n        warnings.append(\"Extra BAM chromosomes: %s\" % bc)\n    for rc in extra_rcs:\n        warnings.append(\"Extra reference chromosomes: %s\" % rc)\n    if problems:\n        raise ValueError(\"Unexpected order, name or contig mismatches between input BAM and reference file:\\n%s\\n\"\n                         \"Setting `bam_clean: remove_extracontigs` in the configuration can often fix this issue.\"\n                         % \"\\n\".join(problems))\n    if warnings:\n        print(\"*** Potential problems in input BAM compared to reference:\\n%s\\n\" %\n              \"\\n\".join(warnings))", "response": "Check if the input BAM file matches the expected reference genome."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sort(in_bam, config, order=\"coordinate\", out_dir=None):\n    assert is_bam(in_bam), \"%s in not a BAM file\" % in_bam\n    if bam_already_sorted(in_bam, config, order):\n        return in_bam\n\n    sort_stem = _get_sort_stem(in_bam, order, out_dir)\n    sort_file = sort_stem + \".bam\"\n    if not utils.file_exists(sort_file):\n        samtools = config_utils.get_program(\"samtools\", config)\n        cores = config[\"algorithm\"].get(\"num_cores\", 1)\n        with file_transaction(config, sort_file) as tx_sort_file:\n            tx_sort_stem = os.path.splitext(tx_sort_file)[0]\n            tx_dir = utils.safe_makedir(os.path.dirname(tx_sort_file))\n            order_flag = \"-n\" if order == \"queryname\" else \"\"\n            resources = config_utils.get_resources(\"samtools\", config)\n            # Slightly decrease memory and allow more accurate representation\n            # in Mb to ensure fits within systems like SLURM\n            mem = config_utils.adjust_memory(resources.get(\"memory\", \"2G\"),\n                                             1.25, \"decrease\", out_modifier=\"M\").upper()\n            cmd = (\"{samtools} sort -@ {cores} -m {mem} -O BAM {order_flag} \"\n                   \"-T {tx_sort_stem}-sort -o {tx_sort_file} {in_bam}\")\n            do.run(cmd.format(**locals()), \"Sort BAM file %s: %s to %s\" %\n                   (order, os.path.basename(in_bam), os.path.basename(sort_file)))\n    return sort_file", "response": "Sort a BAM file into a single base."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nidentifies aligner from the BAM header ; handling pre - aligned inputs.", "response": "def aligner_from_header(in_bam):\n    \"\"\"Identify aligner from the BAM header; handling pre-aligned inputs.\n    \"\"\"\n    from bcbio.pipeline.alignment import TOOLS\n    with pysam.Samfile(in_bam, \"rb\") as bamfile:\n        for pg in bamfile.header.get(\"PG\", []):\n            for ka in TOOLS.keys():\n                if pg.get(\"PN\", \"\").lower().find(ka) >= 0:\n                    return ka"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets sample name from BAM file.", "response": "def sample_name(in_bam):\n    \"\"\"Get sample name from BAM file.\n    \"\"\"\n    with pysam.AlignmentFile(in_bam, \"rb\", check_sq=False) as in_pysam:\n        try:\n            if \"RG\" in in_pysam.header:\n                return in_pysam.header[\"RG\"][0][\"SM\"]\n        except ValueError:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef estimate_read_length(bam_file, nreads=1000):\n    with open_samfile(bam_file) as bam_handle:\n        reads = tz.itertoolz.take(nreads, bam_handle)\n        lengths = [len(x.seq) for x in reads]\n    return int(numpy.median(lengths))", "response": "estimate the median read length of a SAM file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nestimate the median fragment size of a SAM file", "response": "def estimate_fragment_size(bam_file, nreads=5000):\n    \"\"\"\n    estimate median fragment size of a SAM/BAM file\n    \"\"\"\n    with open_samfile(bam_file) as bam_handle:\n        reads = tz.itertoolz.take(nreads, bam_handle)\n        # it would be good to skip spliced paired reads.\n        lengths = [x.template_length for x in reads if x.template_length > 0]\n    if not lengths:\n        return 0\n    return int(numpy.median(lengths))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filter_primary(bam_file, data):\n    stem, ext = os.path.splitext(bam_file)\n    out_file = stem + \".primary\" + ext\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            cores = dd.get_num_cores(data)\n            cmd = (\"samtools view -@ {cores} -F 2304 -b {bam_file} > {tx_out_file}\")\n            do.run(cmd.format(**locals()), (\"Filtering primary alignments in %s.\" %\n                                            os.path.basename(bam_file)))\n    return out_file", "response": "Filter reads to primary only BAM."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nguesses the maximum MAPQ in a BAM file of reads with alignments", "response": "def estimate_max_mapq(in_bam, nreads=1e6):\n    \"\"\"Guess maximum MAPQ in a BAM file of reads with alignments\n    \"\"\"\n    with pysam.Samfile(in_bam, \"rb\") as work_bam:\n        reads = tz.take(int(nreads), work_bam)\n        return max([x.mapq for x in reads if not x.is_unmapped])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_cufflinks_mapq(in_bam, out_bam=None):\n    CUFFLINKSMAPQ = 255\n    if not out_bam:\n        out_bam = os.path.splitext(in_bam)[0] + \"-cufflinks.bam\"\n    if utils.file_exists(out_bam):\n        return out_bam\n    maxmapq = estimate_max_mapq(in_bam)\n    if maxmapq == CUFFLINKSMAPQ:\n        return in_bam\n    logger.info(\"Converting MAPQ scores in %s to be Cufflinks compatible.\" % in_bam)\n    with pysam.Samfile(in_bam, \"rb\") as in_bam_fh:\n        with pysam.Samfile(out_bam, \"wb\", template=in_bam_fh) as out_bam_fh:\n            for read in in_bam_fh:\n                if read.mapq == maxmapq and not read.is_unmapped:\n                    read.mapq = CUFFLINKSMAPQ\n                out_bam_fh.write(read)\n    return out_bam", "response": "Convert a sequence of reads in a BAM file to a sequence of unique reads."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving annotations to use for GATK VariantAnnotator.", "response": "def get_gatk_annotations(config, include_depth=True, include_baseqranksum=True,\n                         gatk_input=True):\n    \"\"\"Retrieve annotations to use for GATK VariantAnnotator.\n\n    If include_depth is false, we'll skip annotating DP. Since GATK downsamples\n    this will undercount on high depth sequencing and the standard outputs\n    from the original callers may be preferable.\n\n    BaseQRankSum can cause issues with some MuTect2 and other runs, so we\n    provide option to skip it.\n    \"\"\"\n    broad_runner = broad.runner_from_config(config)\n    anns = [\"MappingQualityRankSumTest\", \"MappingQualityZero\",\n            \"QualByDepth\", \"ReadPosRankSumTest\", \"RMSMappingQuality\"]\n    if include_baseqranksum:\n        anns += [\"BaseQualityRankSumTest\"]\n    # Some annotations not working correctly with external datasets and GATK 3\n    if gatk_input or broad_runner.gatk_type() == \"gatk4\":\n        anns += [\"FisherStrand\"]\n    if broad_runner.gatk_type() == \"gatk4\":\n        anns += [\"MappingQuality\"]\n    else:\n        anns += [\"GCContent\", \"HaplotypeScore\", \"HomopolymerRun\"]\n    if include_depth:\n        anns += [\"DepthPerAlleleBySample\"]\n        if broad_runner.gatk_type() in [\"restricted\", \"gatk4\"]:\n            anns += [\"Coverage\"]\n        else:\n            anns += [\"DepthOfCoverage\"]\n    return anns"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming cleanup and dbSNP annotation of the final VCF.", "response": "def finalize_vcf(in_file, variantcaller, items):\n    \"\"\"Perform cleanup and dbSNP annotation of the final VCF.\n\n    - Adds contigs to header for bcftools compatibility\n    - adds sample information for tumor/normal\n    \"\"\"\n    out_file = \"%s-annotated%s\" % utils.splitext_plus(in_file)\n    if not utils.file_uptodate(out_file, in_file):\n        header_cl = _add_vcf_header_sample_cl(in_file, items, out_file)\n        contig_cl = _add_contig_cl(in_file, items, out_file)\n        cls = [x for x in (contig_cl, header_cl) if x]\n        if cls:\n            post_cl = \" | \".join(cls) + \" | \"\n        else:\n            post_cl = None\n        dbsnp_file = tz.get_in((\"genome_resources\", \"variation\", \"dbsnp\"), items[0])\n        if dbsnp_file:\n            out_file = _add_dbsnp(in_file, dbsnp_file, items[0], out_file, post_cl)\n    if utils.file_exists(out_file):\n        return vcfutils.bgzip_and_index(out_file, items[0][\"config\"])\n    else:\n        return in_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting TUMOR and NORMAL names in output into sample IDs.", "response": "def _fix_generic_tn_names(paired):\n    \"\"\"Convert TUMOR/NORMAL names in output into sample IDs.\n    \"\"\"\n    def run(line):\n        parts = line.rstrip(\"\\n\\r\").split(\"\\t\")\n        if \"TUMOR\" in parts:\n            parts[parts.index(\"TUMOR\")] = paired.tumor_name\n        if \"TUMOUR\" in parts:\n            parts[parts.index(\"TUMOUR\")] = paired.tumor_name\n        if \"NORMAL\" in parts:\n            assert paired.normal_name\n            parts[parts.index(\"NORMAL\")] = paired.normal_name\n        return \"\\t\".join(parts) + \"\\n\"\n    return run"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_vcf_header_sample_cl(in_file, items, base_file):\n    paired = vcfutils.get_paired(items)\n    if paired:\n        toadd = [\"##SAMPLE=<ID=%s,Genomes=Tumor>\" % paired.tumor_name]\n        if paired.normal_name:\n            toadd.append(\"##SAMPLE=<ID=%s,Genomes=Germline>\" % paired.normal_name)\n            toadd.append(\"##PEDIGREE=<Derived=%s,Original=%s>\" % (paired.tumor_name, paired.normal_name))\n        new_header = _update_header(in_file, base_file, toadd, _fix_generic_tn_names(paired))\n        if vcfutils.vcf_has_variants(in_file):\n            cmd = \"bcftools reheader -h {new_header} | bcftools view \"\n            return cmd.format(**locals())", "response": "Add sample information to a VCF header."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _update_header(orig_vcf, base_file, new_lines, chrom_process_fn=None):\n    new_header = \"%s-sample_header.txt\" % utils.splitext_plus(base_file)[0]\n    with open(new_header, \"w\") as out_handle:\n        chrom_line = None\n        with utils.open_gzipsafe(orig_vcf) as in_handle:\n            for line in in_handle:\n                if line.startswith(\"##\"):\n                    out_handle.write(line)\n                else:\n                    chrom_line = line\n                    break\n        assert chrom_line is not None\n        for line in new_lines:\n            out_handle.write(line + \"\\n\")\n        if chrom_process_fn:\n            chrom_line = chrom_process_fn(chrom_line)\n        out_handle.write(chrom_line)\n    return new_header", "response": "Update header with new lines and remapping of generic sample names."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nannotate a VCF file with dbSNP.", "response": "def _add_dbsnp(orig_file, dbsnp_file, data, out_file=None, post_cl=None):\n    \"\"\"Annotate a VCF file with dbSNP.\n\n    vcfanno has flexible matching for NON_REF gVCF positions, matching\n    at position and REF allele, matching ALT NON_REF as a wildcard.\n    \"\"\"\n    orig_file = vcfutils.bgzip_and_index(orig_file, data[\"config\"])\n    if out_file is None:\n        out_file = \"%s-wdbsnp.vcf.gz\" % utils.splitext_plus(orig_file)[0]\n    if not utils.file_uptodate(out_file, orig_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            conf_file = os.path.join(os.path.dirname(out_file), \"dbsnp.conf\")\n            with open(conf_file, \"w\") as out_handle:\n                out_handle.write(_DBSNP_TEMPLATE % os.path.normpath(os.path.join(dd.get_work_dir(data), dbsnp_file)))\n            if not post_cl: post_cl = \"\"\n            cores = dd.get_num_cores(data)\n            cmd = (\"vcfanno -p {cores} {conf_file} {orig_file} | {post_cl} \"\n                   \"bgzip -c > {tx_out_file}\")\n            do.run(cmd.format(**locals()), \"Annotate with dbSNP\")\n    return vcfutils.bgzip_and_index(out_file, data[\"config\"])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_context_files(data):\n    ref_file = dd.get_ref_file(data)\n    all_files = []\n    for ext in [\".bed.gz\"]:\n        all_files += sorted(glob.glob(os.path.normpath(os.path.join(os.path.dirname(ref_file), os.pardir,\n                                                                    \"coverage\", \"problem_regions\", \"*\",\n                                                                    \"*%s\" % ext))))\n    return sorted(all_files)", "response": "Retrieve pre - installed annotation files for annotating genome context."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_genome_context(orig_file, data):\n    out_file = \"%s-context.vcf.gz\" % utils.splitext_plus(orig_file)[0]\n    if not utils.file_uptodate(out_file, orig_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            config_file = \"%s.toml\" % (utils.splitext_plus(tx_out_file)[0])\n            with open(config_file, \"w\") as out_handle:\n                all_names = []\n                for fname in dd.get_genome_context_files(data):\n                    bt = pybedtools.BedTool(fname)\n                    if bt.field_count() >= 4:\n                        d, base = os.path.split(fname)\n                        _, prefix = os.path.split(d)\n                        name = \"%s_%s\" % (prefix, utils.splitext_plus(base)[0])\n                        out_handle.write(\"[[annotation]]\\n\")\n                        out_handle.write('file = \"%s\"\\n' % fname)\n                        out_handle.write(\"columns = [4]\\n\")\n                        out_handle.write('names = [\"%s\"]\\n' % name)\n                        out_handle.write('ops = [\"uniq\"]\\n')\n                        all_names.append(name)\n                out_handle.write(\"[[postannotation]]\\n\")\n                out_handle.write(\"fields = [%s]\\n\" % (\", \".join(['\"%s\"' % n for n in all_names])))\n                out_handle.write('name = \"genome_context\"\\n')\n                out_handle.write('op = \"concat\"\\n')\n                out_handle.write('type = \"String\"\\n')\n            cmd = \"vcfanno {config_file} {orig_file} | bgzip -c > {tx_out_file}\"\n            do.run(cmd.format(**locals()), \"Annotate with problem annotations\", data)\n    return vcfutils.bgzip_and_index(out_file, data[\"config\"])", "response": "Annotate a file with annotations of genome context using vcfanno."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_bcl2fastq(run_folder, ss_csv, config):\n    bc_dir = os.path.join(run_folder, \"Data\", \"Intensities\", \"BaseCalls\")\n    output_dir = os.path.join(run_folder, \"fastq\")\n\n    if not os.path.exists(os.path.join(output_dir, \"Makefile\")):\n        subprocess.check_call([\"configureBclToFastq.pl\", \"--no-eamss\",\n                               \"--input-dir\", bc_dir, \"--output-dir\", output_dir,\n                               \"--sample-sheet\", ss_csv])\n    with utils.chdir(output_dir):\n        cores = str(utils.get_in(config, (\"algorithm\", \"num_cores\"), 1))\n        cmd = [\"make\", \"-j\", cores]\n        if \"submit_cmd\" in config[\"process\"] and \"bcl2fastq_batch\" in config[\"process\"]:\n            _submit_and_wait(cmd, cores, config, output_dir)\n        else:\n            subprocess.check_call(cmd)\n    return output_dir", "response": "Run bcl2fastq for de - multiplexing and fastq generation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _submit_and_wait(cmd, cores, config, output_dir):\n    batch_script = \"submit_bcl2fastq.sh\"\n    if not os.path.exists(batch_script + \".finished\"):\n        if os.path.exists(batch_script + \".failed\"):\n            os.remove(batch_script + \".failed\")\n        with open(batch_script, \"w\") as out_handle:\n            out_handle.write(config[\"process\"][\"bcl2fastq_batch\"].format(\n                cores=cores, bcl2fastq_cmd=\" \".join(cmd), batch_script=batch_script))\n        submit_cmd = utils.get_in(config, (\"process\", \"submit_cmd\"))\n        subprocess.check_call(submit_cmd.format(batch_script=batch_script), shell=True)\n        # wait until finished or failure checkpoint file\n        while 1:\n            if os.path.exists(batch_script + \".finished\"):\n                break\n            if os.path.exists(batch_script + \".failed\"):\n                raise ValueError(\"bcl2fastq batch script failed: %s\" %\n                                 os.path.join(output_dir, batch_script))\n            time.sleep(5)", "response": "Submit command with batch script specified in configuration wait until finished"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprepares a set of configuration items for input files.", "response": "def _prep_items_from_base(base, in_files, metadata, separators, force_single=False):\n    \"\"\"Prepare a set of configuration items for input files.\n    \"\"\"\n    details = []\n    in_files = _expand_dirs(in_files, KNOWN_EXTS)\n    in_files = _expand_wildcards(in_files)\n\n    ext_groups = collections.defaultdict(list)\n    for ext, files in itertools.groupby(\n            in_files, lambda x: KNOWN_EXTS.get(utils.splitext_plus(x)[-1].lower())):\n        ext_groups[ext].extend(list(files))\n    for ext, files in ext_groups.items():\n        if ext == \"bam\":\n            for f in files:\n                details.append(_prep_bam_input(f, base))\n        elif ext in [\"fastq\", \"fq\", \"fasta\"]:\n            files, glob_files = _find_glob_matches(files, metadata)\n            for fs in glob_files:\n                details.append(_prep_fastq_input(fs, base))\n            for fs in fastq.combine_pairs(files, force_single, separators=separators):\n                details.append(_prep_fastq_input(fs, base))\n        elif ext in [\"vcf\"]:\n            for f in files:\n                details.append(_prep_vcf_input(f, base))\n        else:\n            print(\"Ignoring unexpected input file types %s: %s\" % (ext, list(files)))\n    return details"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngrouping files that match by globs for merging rather than by explicit pairs.", "response": "def _find_glob_matches(in_files, metadata):\n    \"\"\"Group files that match by globs for merging, rather than by explicit pairs.\n    \"\"\"\n    reg_files = copy.deepcopy(in_files)\n    glob_files = []\n    for glob_search in [x for x in metadata.keys() if \"*\" in x]:\n        cur = []\n        for fname in in_files:\n            if fnmatch.fnmatch(fname, \"*/%s\" % glob_search):\n                cur.append(fname)\n                reg_files.remove(fname)\n        assert cur, \"Did not find file matches for %s\" % glob_search\n        glob_files.append(cur)\n    return reg_files, glob_files"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread template file into a dictionary to use as base for all samples.", "response": "def name_to_config(template):\n    \"\"\"Read template file into a dictionary to use as base for all samples.\n\n    Handles well-known template names, pulled from GitHub repository and local\n    files.\n    \"\"\"\n    if objectstore.is_remote(template):\n        with objectstore.open_file(template) as in_handle:\n            config = yaml.safe_load(in_handle)\n        with objectstore.open_file(template) as in_handle:\n            txt_config = in_handle.read()\n    elif os.path.isfile(template):\n        if template.endswith(\".csv\"):\n            raise ValueError(\"Expected YAML file for template and found CSV, are arguments switched? %s\" % template)\n        with open(template) as in_handle:\n            txt_config = in_handle.read()\n        with open(template) as in_handle:\n            config = yaml.safe_load(in_handle)\n    else:\n        base_url = \"https://raw.github.com/bcbio/bcbio-nextgen/master/config/templates/%s.yaml\"\n        try:\n            with contextlib.closing(urllib.request.urlopen(base_url % template)) as in_handle:\n                txt_config = in_handle.read().decode()\n            with contextlib.closing(urllib.request.urlopen(base_url % template)) as in_handle:\n                config = yaml.safe_load(in_handle)\n        except (urllib.error.HTTPError, urllib.error.URLError):\n            raise ValueError(\"Could not find template '%s' locally or in standard templates on GitHub\"\n                             % template)\n    return config, txt_config"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _write_config_file(items, global_vars, template, project_name, out_dir,\n                       remotes):\n    \"\"\"Write configuration file, adding required top level attributes.\n    \"\"\"\n    config_dir = utils.safe_makedir(os.path.join(out_dir, \"config\"))\n    out_config_file = os.path.join(config_dir, \"%s.yaml\" % project_name)\n    out = {\"fc_name\": project_name,\n           \"upload\": {\"dir\": \"../final\"},\n           \"details\": items}\n    if remotes.get(\"base\"):\n        r_base = objectstore.parse_remote(remotes.get(\"base\"))\n        out[\"upload\"][\"method\"] = r_base.store\n        out[\"upload\"][\"bucket\"] = r_base.bucket\n        out[\"upload\"][\"folder\"] = os.path.join(r_base.key, \"final\") if r_base.key else \"final\"\n        if r_base.region:\n            out[\"upload\"][\"region\"] = r_base.region\n    if global_vars:\n        out[\"globals\"] = global_vars\n    for k, v in template.items():\n        if k not in [\"details\"]:\n            out[k] = v\n    if os.path.exists(out_config_file):\n        shutil.move(out_config_file,\n                    out_config_file + \".bak%s\" % datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n    with open(out_config_file, \"w\") as out_handle:\n        yaml.safe_dump(out, out_handle, default_flow_style=False, allow_unicode=False)\n    return out_config_file", "response": "Write configuration file for a single item."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _set_global_vars(metadata):\n    fnames = collections.defaultdict(list)\n    for sample in metadata.keys():\n        for k, v in metadata[sample].items():\n            if isinstance(v, six.string_types) and os.path.isfile(v):\n                v = _expand_file(v)\n                metadata[sample][k] = v\n                fnames[v].append(k)\n    global_vars = {}\n    # Skip global vars -- more confusing than useful\n    # loc_counts = collections.defaultdict(int)\n    # global_var_sub = {}\n    # for fname, locs in fnames.items():\n    #     if len(locs) > 1:\n    #         loc_counts[locs[0]] += 1\n    #         name = \"%s%s\" % (locs[0], loc_counts[locs[0]])\n    #         global_var_sub[fname] = name\n    #         global_vars[name] = fname\n    # for sample in metadata.keys():\n    #     for k, v in metadata[sample].items():\n    #         if isinstance(v, six.string_types) and v in global_var_sub:\n    #             metadata[sample][k] = global_var_sub[v]\n    return metadata, global_vars", "response": "Identify files used multiple times in metadata and replace with global variables\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _clean_string(v, sinfo):\n    if isinstance(v, (list, tuple)):\n        return [_clean_string(x, sinfo) for x in v]\n    else:\n        assert isinstance(v, six.string_types), v\n        try:\n            if hasattr(v, \"decode\"):\n                return str(v.decode(\"ascii\"))\n            else:\n                return str(v.encode(\"ascii\").decode(\"ascii\"))\n        except UnicodeDecodeError as msg:\n            raise ValueError(\"Found unicode character in template CSV line %s:\\n%s\" % (sinfo, str(msg)))", "response": "Test for and clean unicode present in template CSVs."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_metadata(in_handle):\n    metadata = {}\n    reader = csv.reader(in_handle)\n    while 1:\n        header = next(reader)\n        if not header[0].startswith(\"#\"):\n            break\n    keys = [x.strip() for x in header[1:]]\n    for sinfo in (x for x in reader if x and not x[0].startswith(\"#\")):\n        sinfo = [_strip_and_convert_lists(x) for x in sinfo]\n        sample = sinfo[0]\n        if isinstance(sample, list):\n            sample = tuple(sample)\n        # sanity check to avoid duplicate rows\n        if sample in metadata:\n            raise ValueError(\"Sample %s present multiple times in metadata file.\\n\"\n                             \"If you need to specify multiple attributes as a list \"\n                             \"use a semi-colon to separate them on a single line.\\n\"\n                             \"https://bcbio-nextgen.readthedocs.org/en/latest/\"\n                             \"contents/configuration.html#automated-sample-configuration\\n\"\n                             \"Duplicate line is %s\" % (sample, sinfo))\n        vals = [_clean_string(v, sinfo) for v in sinfo[1:]]\n        metadata[sample] = dict(zip(keys, vals))\n    metadata, global_vars = _set_global_vars(metadata)\n    return metadata, global_vars", "response": "Reads metadata from a simple CSV structured input file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _pname_and_metadata(in_file):\n    if os.path.isfile(in_file):\n        with open(in_file) as in_handle:\n            md, global_vars = _parse_metadata(in_handle)\n        base = os.path.splitext(os.path.basename(in_file))[0]\n        md_file = in_file\n    elif objectstore.is_remote(in_file):\n        with objectstore.open_file(in_file) as in_handle:\n            md, global_vars = _parse_metadata(in_handle)\n        base = os.path.splitext(os.path.basename(in_file))[0]\n        md_file = None\n    else:\n        if in_file.endswith(\".csv\"):\n            raise ValueError(\"Did not find input metadata file: %s\" % in_file)\n        base, md, global_vars = _safe_name(os.path.splitext(os.path.basename(in_file))[0]), {}, {}\n        md_file = None\n    return _safe_name(base), md, global_vars, md_file", "response": "Retrieve the project name and metadata from the input metadata CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _handle_special_yaml_cases(v):\n    if \"::\" in v:\n        out = {}\n        for part in v.split(\"::\"):\n            k_part, v_part = part.split(\":\")\n            out[k_part] = v_part.split(\";\")\n        v = out\n    elif \";\" in v:\n        # split lists and remove accidental empty values\n        v = [x for x in v.split(\";\") if x != \"\"]\n    elif isinstance(v, list):\n        v = v\n    else:\n        try:\n            v = int(v)\n        except ValueError:\n            if v.lower() == \"true\":\n                v = True\n            elif v.lower() == \"false\":\n                    v = False\n    return v", "response": "Handle special YAML case for the archive."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds standard PED file attributes into metadata if not present.", "response": "def _add_ped_metadata(name, metadata):\n    \"\"\"Add standard PED file attributes into metadata if not present.\n\n    http://pngu.mgh.harvard.edu/~purcell/plink/data.shtml#ped\n    \"\"\"\n    ignore = set([\"-9\", \"undefined\", \"unknown\", \".\"])\n    def _ped_mapping(x, valmap):\n        try:\n            x = int(x)\n        except ValueError:\n            x = -1\n        for k, v in valmap.items():\n            if k == x:\n                return v\n        return None\n    def _ped_to_gender(x):\n        return _ped_mapping(x, {1: \"male\", 2: \"female\"})\n    def _ped_to_phenotype(x):\n        known_phenotypes = set([\"unaffected\", \"affected\", \"tumor\", \"normal\"])\n        if x in known_phenotypes:\n            return x\n        else:\n            return _ped_mapping(x, {1: \"unaffected\", 2: \"affected\"})\n    def _ped_to_batch(x):\n        if x not in ignore and x != \"0\":\n            return x\n    with open(metadata[\"ped\"]) as in_handle:\n        for line in in_handle:\n            parts = line.split(\"\\t\")[:6]\n            if parts[1] == str(name):\n                for index, key, convert_fn in [(4, \"sex\", _ped_to_gender), (0, \"batch\", _ped_to_batch),\n                                               (5, \"phenotype\", _ped_to_phenotype)]:\n                    val = convert_fn(parts[index])\n                    if val is not None and key not in metadata:\n                        metadata[key] = val\n                break\n    return metadata"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding metadata information from CSV file to current item.", "response": "def _add_metadata(item, metadata, remotes, only_metadata=False):\n    \"\"\"Add metadata information from CSV file to current item.\n\n    Retrieves metadata based on 'description' parsed from input CSV file.\n    Adds to object and handles special keys:\n    - `description`: A new description for the item. Used to relabel items\n       based on the pre-determined description from fastq name or BAM read groups.\n    - Keys matching supported names in the algorithm section map\n      to key/value pairs there instead of metadata.\n    \"\"\"\n    for check_key in [item[\"description\"]] + _get_file_keys(item) + _get_vrn_keys(item):\n        item_md = metadata.get(check_key)\n        if item_md:\n            break\n    if not item_md:\n        item_md = _find_glob_metadata(item[\"files\"], metadata)\n    if remotes.get(\"region\"):\n        item[\"algorithm\"][\"variant_regions\"] = remotes[\"region\"]\n    TOP_LEVEL = set([\"description\", \"genome_build\", \"lane\", \"vrn_file\", \"files\", \"analysis\"])\n    keep_sample = True\n    if item_md and len(item_md) > 0:\n        if \"metadata\" not in item:\n            item[\"metadata\"] = {}\n        for k, v in item_md.items():\n            if v:\n                if k in TOP_LEVEL:\n                    item[k] = v\n                elif k in run_info.ALGORITHM_KEYS:\n                    v = _handle_special_yaml_cases(v)\n                    item[\"algorithm\"][k] = v\n                else:\n                    v = _handle_special_yaml_cases(v)\n                    item[\"metadata\"][k] = v\n    elif len(metadata) > 0:\n        warn = \"Dropped sample\" if only_metadata else \"Added minimal sample information\"\n        print(\"WARNING: %s: metadata not found for %s, %s\" % (warn, item[\"description\"],\n                                                              [os.path.basename(f) for f in item[\"files\"]]))\n        keep_sample = not only_metadata\n    if tz.get_in([\"metadata\", \"ped\"], item):\n        item[\"metadata\"] = _add_ped_metadata(item[\"description\"], item[\"metadata\"])\n    return item if keep_sample else None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve remote inputs found in the same bucket as the template or metadata files.", "response": "def _retrieve_remote(fnames):\n    \"\"\"Retrieve remote inputs found in the same bucket as the template or metadata files.\n    \"\"\"\n    for fname in fnames:\n        if objectstore.is_remote(fname):\n            inputs = []\n            regions = []\n            remote_base = os.path.dirname(fname)\n            for rfname in objectstore.list(remote_base):\n                if rfname.endswith(tuple(KNOWN_EXTS.keys())):\n                    inputs.append(rfname)\n                elif rfname.endswith((\".bed\", \".bed.gz\")):\n                    regions.append(rfname)\n            return {\"base\": remote_base,\n                    \"inputs\": inputs,\n                    \"region\": regions[0] if len(regions) == 1 else None}\n    return {}"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts absolute paths in the input data to relative paths to the work directory.", "response": "def _convert_to_relpaths(data, work_dir):\n    \"\"\"Convert absolute paths in the input data to relative paths to the work directory.\n    \"\"\"\n    work_dir = os.path.abspath(work_dir)\n    data[\"files\"] = [os.path.relpath(f, work_dir) for f in data[\"files\"]]\n    for topk in [\"metadata\", \"algorithm\"]:\n        for k, v in data[topk].items():\n            if isinstance(v, six.string_types) and os.path.isfile(v) and os.path.isabs(v):\n                data[topk][k] = os.path.relpath(v, work_dir)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints warning if samples in CSV file are missing in folder", "response": "def _check_all_metadata_found(metadata, items):\n    \"\"\"Print warning if samples in CSV file are missing in folder\"\"\"\n    for name in metadata:\n        seen = False\n        for sample in items:\n            check_file = sample[\"files\"][0] if sample.get(\"files\") else sample[\"vrn_file\"]\n            if isinstance(name, (tuple, list)):\n                if check_file.find(name[0]) > -1:\n                    seen = True\n            elif check_file.find(name) > -1:\n                seen = True\n            elif \"*\" in name and fnmatch.fnmatch(check_file, \"*/%s\" % name):\n                seen = True\n        if not seen:\n            print(\"WARNING: sample not found %s\" % str(name))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncopies configuration files like PED inputs to working config directory.", "response": "def _copy_to_configdir(items, out_dir, args):\n    \"\"\"Copy configuration files like PED inputs to working config directory.\n    \"\"\"\n    out = []\n    for item in items:\n        ped_file = tz.get_in([\"metadata\", \"ped\"], item)\n        if ped_file and os.path.exists(ped_file):\n            ped_config_file = os.path.join(out_dir, \"config\", os.path.basename(ped_file))\n            if not os.path.exists(ped_config_file):\n                shutil.copy(ped_file, ped_config_file)\n            item[\"metadata\"][\"ped\"] = ped_config_file\n        out.append(item)\n    if hasattr(args, \"systemconfig\") and args.systemconfig:\n        shutil.copy(args.systemconfig, os.path.join(out_dir, \"config\", os.path.basename(args.systemconfig)))\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the configuration for this lane if a custom analysis is specified.", "response": "def update_w_custom(config, lane_info):\n    \"\"\"Update the configuration for this lane if a custom analysis is specified.\n    \"\"\"\n    name_remaps = {\"variant\": [\"SNP calling\", \"variant\", \"variant2\"],\n                   \"SNP calling\": [\"SNP calling\", \"variant\", \"variant2\"],\n                   \"variant2\": [\"SNP calling\", \"variant\", \"variant2\"]}\n    config = copy.deepcopy(config)\n    base_name = lane_info.get(\"analysis\")\n    if \"algorithm\" not in config:\n        config[\"algorithm\"] = {}\n    for analysis_type in name_remaps.get(base_name, [base_name]):\n        custom = config.get(\"custom_algorithms\", {}).get(analysis_type)\n        if custom:\n            for key, val in custom.items():\n                config[\"algorithm\"][key] = val\n    # apply any algorithm details specified with the lane\n    for key, val in lane_info.get(\"algorithm\", {}).items():\n        config[\"algorithm\"][key] = val\n    # apply any resource details specified with the lane\n    for prog, pkvs in lane_info.get(\"resources\", {}).items():\n        if prog not in config[\"resources\"]:\n            config[\"resources\"][prog] = {}\n        for key, val in pkvs.items():\n            config[\"resources\"][prog][key] = val\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads bcbio_system. yaml configuration file handling standard defaults.", "response": "def load_system_config(config_file=None, work_dir=None, allow_missing=False):\n    \"\"\"Load bcbio_system.yaml configuration file, handling standard defaults.\n\n    Looks for configuration file in default location within\n    final base directory from a standard installation. Handles both standard\n    installs (galaxy/bcbio_system.yaml) and docker installs (config/bcbio_system.yaml).\n    \"\"\"\n    docker_config = _get_docker_config()\n    if config_file is None:\n        config_file = \"bcbio_system.yaml\"\n    if not os.path.exists(config_file):\n        base_dir = get_base_installdir()\n        test_config = os.path.join(base_dir, \"galaxy\", config_file)\n        if os.path.exists(test_config):\n            config_file = test_config\n        elif allow_missing:\n            config_file = None\n        else:\n            raise ValueError(\"Could not find input system configuration file %s, \"\n                             \"including inside standard directory %s\" %\n                             (config_file, os.path.join(base_dir, \"galaxy\")))\n    config = load_config(config_file) if config_file else {}\n    if docker_config:\n        assert work_dir is not None, \"Need working directory to merge docker config\"\n        config_file = os.path.join(work_dir, \"%s-merged%s\" % os.path.splitext(os.path.basename(config_file)))\n        config = _merge_system_configs(config, docker_config, config_file)\n    if \"algorithm\" not in config:\n        config[\"algorithm\"] = {}\n    config[\"bcbio_system\"] = config_file\n    return config, config_file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _merge_system_configs(host_config, container_config, out_file=None):\n    out = copy.deepcopy(container_config)\n    for k, v in host_config.items():\n        if k in set([\"galaxy_config\"]):\n            out[k] = v\n        elif k == \"resources\":\n            for pname, resources in v.items():\n                if not isinstance(resources, dict) and pname not in out[k]:\n                    out[k][pname] = resources\n                else:\n                    for rname, rval in resources.items():\n                        if (rname in set([\"cores\", \"jvm_opts\", \"memory\"])\n                              or pname in set([\"gatk\", \"mutect\"])):\n                            if pname not in out[k]:\n                                out[k][pname] = {}\n                            out[k][pname][rname] = rval\n    # Ensure final file is relocatable by mapping back to reference directory\n    if \"bcbio_system\" in out and (\"galaxy_config\" not in out or not os.path.isabs(out[\"galaxy_config\"])):\n        out[\"galaxy_config\"] = os.path.normpath(os.path.join(os.path.dirname(out[\"bcbio_system\"]),\n                                                             os.pardir, \"galaxy\",\n                                                             \"universe_wsgi.ini\"))\n    if out_file:\n        with open(out_file, \"w\") as out_handle:\n            yaml.safe_dump(out, out_handle, default_flow_style=False, allow_unicode=False)\n    return out", "response": "Create a merged system configuration from external and internal specification."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmerge docker local resources and global resource specification in a set of arguments.", "response": "def merge_resources(args):\n    \"\"\"Merge docker local resources and global resource specification in a set of arguments.\n\n    Finds the `data` object within passed arguments and updates the resources\n    from a local docker configuration if present.\n    \"\"\"\n    docker_config = _get_docker_config()\n    if not docker_config:\n        return args\n    else:\n        def _update_resources(config):\n            config[\"resources\"] = _merge_system_configs(config, docker_config)[\"resources\"]\n            return config\n        return _update_config(args, _update_resources, allow_missing=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the config file and expand paths.", "response": "def load_config(config_file):\n    \"\"\"Load YAML config file, replacing environmental variables.\n    \"\"\"\n    with open(config_file) as in_handle:\n        config = yaml.safe_load(in_handle)\n    config = _expand_paths(config)\n    if 'resources' not in config:\n        config['resources'] = {}\n    # lowercase resource names, the preferred way to specify, for back-compatibility\n    newr = {}\n    for k, v in config[\"resources\"].items():\n        if k.lower() != k:\n            newr[k.lower()] = v\n    config[\"resources\"].update(newr)\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_resources(name, config):\n    return tz.get_in([\"resources\", name], config,\n                     tz.get_in([\"resources\", \"default\"], config, {}))", "response": "Retrieve resources for a program from multiple config sources."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_program(name, config, ptype=\"cmd\", default=None):\n    # support taking in the data dictionary\n    config = config.get(\"config\", config)\n    try:\n        pconfig = config.get(\"resources\", {})[name]\n        # If have leftover old\n    except KeyError:\n        pconfig = {}\n    old_config = config.get(\"program\", {}).get(name, None)\n    if old_config:\n        for key in [\"dir\", \"cmd\"]:\n            if not key in pconfig:\n                pconfig[key] = old_config\n    if ptype == \"cmd\":\n        return _get_program_cmd(name, pconfig, config, default)\n    elif ptype == \"dir\":\n        return _get_program_dir(name, pconfig)\n    else:\n        raise ValueError(\"Don't understand program type: %s\" % ptype)", "response": "Retrieve the program information from the configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves commandline of a program.", "response": "def _get_program_cmd(name, pconfig, config, default):\n    \"\"\"Retrieve commandline of a program.\n    \"\"\"\n    if pconfig is None:\n        return name\n    elif isinstance(pconfig, six.string_types):\n        return pconfig\n    elif \"cmd\" in pconfig:\n        return pconfig[\"cmd\"]\n    elif default is not None:\n        return default\n    else:\n        return name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_program_dir(name, config):\n    if config is None:\n        raise ValueError(\"Could not find directory in config for %s\" % name)\n    elif isinstance(config, six.string_types):\n        return config\n    elif \"dir\" in config:\n        return expand_path(config[\"dir\"])\n    else:\n        raise ValueError(\"Could not find directory in config for %s\" % name)", "response": "Retrieve directory for a program."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_jar(base_name, dname):\n    jars = glob.glob(os.path.join(expand_path(dname), \"%s*.jar\" % base_name))\n\n    if len(jars) == 1:\n        return jars[0]\n    elif len(jars) > 1:\n        raise ValueError(\"Found multiple jars for %s in %s. Need single jar: %s\" %\n                         (base_name, dname, jars))\n    else:\n        raise ValueError(\"Could not find java jar %s in %s\" %\n                         (base_name, dname))", "response": "Retrieve a jar in the provided directory"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_dataarg(args):\n    for i, arg in enumerate(args):\n        if is_nested_config_arg(arg):\n            return i, arg\n        elif is_std_config_arg(arg):\n            return i, {\"config\": arg}\n        elif isinstance(arg, (list, tuple)) and is_nested_config_arg(arg[0]):\n            return i, arg[0]\n    raise ValueError(\"Did not find configuration or data object in arguments: %s\" % args)", "response": "Retrieve the world data argument from a set of input parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_cores_to_config(args, cores_per_job, parallel=None):\n    def _update_cores(config):\n        config[\"algorithm\"][\"num_cores\"] = int(cores_per_job)\n        if parallel:\n            parallel.pop(\"view\", None)\n            config[\"parallel\"] = parallel\n        return config\n    return _update_config(args, _update_cores)", "response": "Add information about available cores for a job to a configuration dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the configuration of the specified list with the provided update function.", "response": "def _update_config(args, update_fn, allow_missing=False):\n    \"\"\"Update configuration, nested in argument list, with the provided update function.\n    \"\"\"\n    new_i = None\n    for i, arg in enumerate(args):\n        if (is_std_config_arg(arg) or is_nested_config_arg(arg) or\n              (isinstance(arg, (list, tuple)) and is_nested_config_arg(arg[0]))):\n            new_i = i\n            break\n    if new_i is None:\n        if allow_missing:\n            return args\n        else:\n            raise ValueError(\"Could not find configuration in args: %s\" % str(args))\n\n    new_arg = args[new_i]\n    if is_nested_config_arg(new_arg):\n        new_arg[\"config\"] = update_fn(copy.deepcopy(new_arg[\"config\"]))\n    elif is_std_config_arg(new_arg):\n        new_arg = update_fn(copy.deepcopy(new_arg))\n    elif isinstance(arg, (list, tuple)) and is_nested_config_arg(new_arg[0]):\n        new_arg_first = new_arg[0]\n        new_arg_first[\"config\"] = update_fn(copy.deepcopy(new_arg_first[\"config\"]))\n        new_arg = [new_arg_first] + new_arg[1:]\n    else:\n        raise ValueError(\"Unexpected configuration dictionary: %s\" % new_arg)\n    args = list(args)[:]\n    args[new_i] = new_arg\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a memory specification potentially with M or G into bytes.", "response": "def convert_to_bytes(mem_str):\n    \"\"\"Convert a memory specification, potentially with M or G, into bytes.\n    \"\"\"\n    if str(mem_str)[-1].upper().endswith(\"G\"):\n        return int(round(float(mem_str[:-1]) * 1024 * 1024))\n    elif str(mem_str)[-1].upper().endswith(\"M\"):\n        return int(round(float(mem_str[:-1]) * 1024))\n    else:\n        return int(round(float(mem_str)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef adjust_cores_to_mb_target(target_mb, mem_str, cores):\n    cur_mb = convert_to_bytes(mem_str) / 1024.0\n    scale = target_mb / cur_mb\n    if scale >= 1:\n        return cores\n    else:\n        return max(1, int(math.ceil(scale * cores)))", "response": "Scale cores to match a Mb target."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef adjust_memory(val, magnitude, direction=\"increase\", out_modifier=\"\", maximum=None):\n    modifier = val[-1:]\n    amount = float(val[:-1])\n    if direction == \"decrease\":\n        new_amount = amount / float(magnitude)\n        # dealing with a specifier like 1G, need to scale to Mb\n        if new_amount < 1 or (out_modifier.upper().startswith(\"M\") and modifier.upper().startswith(\"G\")):\n            if modifier.upper().startswith(\"G\"):\n                new_amount = (amount * 1024) / magnitude\n                modifier = \"M\" + modifier[1:]\n            else:\n                raise ValueError(\"Unexpected decrease in memory: %s by %s\" % (val, magnitude))\n        amount = int(new_amount)\n    elif direction == \"increase\" and magnitude > 1:\n        # for increases with multiple cores, leave small percentage of\n        # memory for system to maintain process running resource and\n        # avoid OOM killers\n        adjuster = 0.91\n        amount = int(math.ceil(amount * (adjuster * magnitude)))\n    if out_modifier.upper().startswith(\"G\") and modifier.upper().startswith(\"M\"):\n        modifier = out_modifier\n        amount = int(math.floor(amount / 1024.0))\n    if out_modifier.upper().startswith(\"M\") and modifier.upper().startswith(\"G\"):\n        modifier = out_modifier\n        modifier = int(amount * 1024)\n    if maximum:\n        max_modifier = maximum[-1]\n        max_amount = float(maximum[:-1])\n        if modifier.upper() == \"G\" and max_modifier.upper() == \"M\":\n            max_amount = max_amount / 1024.0\n        elif modifier.upper() == \"M\" and max_modifier.upper() == \"G\":\n            max_amount = max_amount * 1024.0\n        amount = min([amount, max_amount])\n    return \"{amount}{modifier}\".format(amount=int(math.floor(amount)), modifier=modifier)", "response": "Adjust memory based on number of cores utilized."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef adjust_opts(in_opts, config):\n    memory_adjust = config[\"algorithm\"].get(\"memory_adjust\", {})\n    out_opts = []\n    for opt in in_opts:\n        if opt.startswith(\"-Xmx\") or (opt.startswith(\"-Xms\") and memory_adjust.get(\"direction\") == \"decrease\"):\n            arg = opt[:4]\n            opt = \"{arg}{val}\".format(arg=arg,\n                                      val=adjust_memory(opt[4:],\n                                                        memory_adjust.get(\"magnitude\", 1),\n                                                        memory_adjust.get(\"direction\"),\n                                                        maximum=memory_adjust.get(\"maximum\")))\n        out_opts.append(opt)\n    return out_opts", "response": "Establish JVM opts adjusting memory for the context if needed."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if GATK s Variant Quality Score Recalibration is supported for all the variants in the specified algs.", "response": "def use_vqsr(algs, call_file=None):\n    \"\"\"Processing uses GATK's Variant Quality Score Recalibration.\n    \"\"\"\n    from bcbio.variation import vcfutils\n    vqsr_callers = set([\"gatk\", \"gatk-haplotype\"])\n    vqsr_sample_thresh = 50\n    vqsr_supported = collections.defaultdict(int)\n    coverage_intervals = set([])\n    for alg in algs:\n        callers = alg.get(\"variantcaller\")\n        if isinstance(callers, six.string_types):\n            callers = [callers]\n        if not callers:  # no variant calling, no VQSR\n            continue\n        if \"vqsr\" in (alg.get(\"tools_off\") or []):  # VQSR turned off\n            continue\n        for c in callers:\n            if c in vqsr_callers:\n                if \"vqsr\" in (alg.get(\"tools_on\") or []):  # VQSR turned on:\n                    vqsr_supported[c] += 1\n                    coverage_intervals.add(\"genome\")\n                # Do not try VQSR for gVCF inputs\n                elif call_file and vcfutils.is_gvcf_file(call_file):\n                    pass\n                else:\n                    coverage_intervals.add(alg.get(\"coverage_interval\", \"exome\").lower())\n                    vqsr_supported[c] += 1\n    if len(vqsr_supported) > 0:\n        num_samples = max(vqsr_supported.values())\n        if \"genome\" in coverage_intervals or num_samples >= vqsr_sample_thresh:\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef use_bcbio_variation_recall(algs):\n    for alg in algs:\n        jointcaller = alg.get(\"jointcaller\", [])\n        if not isinstance(jointcaller, (tuple, list)):\n            jointcaller = [jointcaller]\n        for caller in jointcaller:\n            if caller not in set([\"gatk-haplotype-joint\", None, False]):\n                return True\n    return False", "response": "Checks if processing uses bcbio - variation - recall."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if the path to a program can be found otherwise False.", "response": "def program_installed(program, data):\n    \"\"\"\n    returns True if the path to a program can be found\n    \"\"\"\n    try:\n        path = get_program(program, data)\n    except CmdNotFound:\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nensure samtools is installed and has parallel processing required for piped alignment and BAM merging.", "response": "def samtools(items):\n    \"\"\"Ensure samtools has parallel processing required for piped analysis.\n    \"\"\"\n    samtools = config_utils.get_program(\"samtools\", items[0][\"config\"])\n    p = subprocess.Popen([samtools, \"sort\", \"-h\"], stdout=subprocess.PIPE,\n                         stderr=subprocess.PIPE)\n    output, stderr = p.communicate()\n    p.stdout.close()\n    p.stderr.close()\n    if str(output).find(\"-@\") == -1 and str(stderr).find(\"-@\") == -1:\n        return (\"Installed version of samtools sort does not have support for \"\n                \"multithreading (-@ option) \"\n                \"required to support bwa piped alignment and BAM merging. \"\n                \"Please upgrade to the latest version \"\n                \"from http://samtools.sourceforge.net/\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if a caller needs external java for MuTect.", "response": "def _needs_java(data):\n    \"\"\"Check if a caller needs external java for MuTect.\n\n    No longer check for older GATK (<3.6) versions because of time cost; this\n    won't be relevant to most runs so we skip the sanity check.\n    \"\"\"\n    vc = dd.get_variantcaller(data)\n    if isinstance(vc, dict):\n        out = {}\n        for k, v in vc.items():\n            if not isinstance(v, (list, tuple)):\n                v = [v]\n            out[k] = v\n        vc = out\n    elif not isinstance(vc, (list, tuple)):\n        vc = [vc]\n    if \"mutect\" in vc or (\"somatic\" in vc and \"mutect\" in vc[\"somatic\"]):\n        return True\n    if \"gatk\" in vc or \"gatk-haplotype\" in vc or (\"germline\" in vc and \"gatk-haplotype\" in vc[\"germline\"]):\n        pass\n        # runner = broad.runner_from_config(data[\"config\"])\n        # version = runner.get_gatk_version()\n        # if LooseVersion(version) < LooseVersion(\"3.6\"):\n        #     return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck for presence of external Java 1. 7 for tools that require it.", "response": "def java(items):\n    \"\"\"Check for presence of external Java 1.7 for tools that require it.\n    \"\"\"\n    if any([_needs_java(d) for d in items]):\n        min_version = \"1.7\"\n        max_version = \"1.8\"\n        with setpath.orig_paths():\n            java = utils.which(\"java\")\n            if not java:\n                return (\"java not found on PATH. Java %s required for MuTect and GATK < 3.6.\" % min_version)\n            p = subprocess.Popen([java, \"-Xms250m\", \"-Xmx250m\", \"-version\"],\n                                 stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n            output, _ = p.communicate()\n            p.stdout.close()\n        version = \"\"\n        for line in output.split(\"\\n\"):\n            if line.startswith((\"java version\", \"openjdk version\")):\n                version = line.strip().split()[-1]\n                if version.startswith('\"'):\n                    version = version[1:]\n                if version.endswith('\"'):\n                    version = version[:-1]\n        if (not version or LooseVersion(version) >= LooseVersion(max_version) or\n            LooseVersion(version) < LooseVersion(min_version)):\n            return (\"java version %s required for running MuTect and GATK < 3.6.\\n\"\n                    \"It needs to be first on your PATH so running 'java -version' give the correct version.\\n\"\n                    \"Found version %s at %s\" % (min_version, version, java))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nperform upgrade of bcbio to latest release or from GitHub development version.", "response": "def upgrade_bcbio(args):\n    \"\"\"Perform upgrade of bcbio to latest release, or from GitHub development version.\n\n    Handles bcbio, third party tools and data.\n    \"\"\"\n    print(\"Upgrading bcbio\")\n    args = add_install_defaults(args)\n    if args.upgrade in [\"stable\", \"system\", \"deps\", \"development\"]:\n        if args.upgrade == \"development\":\n            anaconda_dir = _update_conda_devel()\n            _check_for_conda_problems()\n            print(\"Upgrading bcbio-nextgen to latest development version\")\n            pip_bin = os.path.join(os.path.dirname(os.path.realpath(sys.executable)), \"pip\")\n            git_tag = \"@%s\" % args.revision if args.revision != \"master\" else \"\"\n            _pip_safe_ssl([[pip_bin, \"install\", \"--upgrade\", \"--no-deps\",\n                            \"git+%s%s#egg=bcbio-nextgen\" % (REMOTES[\"gitrepo\"], git_tag)]], anaconda_dir)\n            print(\"Upgrade of bcbio-nextgen development code complete.\")\n        else:\n            _update_conda_packages()\n            _check_for_conda_problems()\n            print(\"Upgrade of bcbio-nextgen code complete.\")\n    if args.cwl and args.upgrade:\n        _update_bcbiovm()\n\n    try:\n        _set_matplotlib_default_backend()\n    except OSError:\n        pass\n\n    if args.tooldir:\n        with bcbio_tmpdir():\n            print(\"Upgrading third party tools to latest versions\")\n            _symlink_bcbio(args, script=\"bcbio_nextgen.py\")\n            _symlink_bcbio(args, script=\"bcbio_setup_genome.py\")\n            _symlink_bcbio(args, script=\"bcbio_prepare_samples.py\")\n            _symlink_bcbio(args, script=\"bcbio_fastq_umi_prep.py\")\n            if args.cwl:\n                _symlink_bcbio(args, \"bcbio_vm.py\", \"bcbiovm\")\n                _symlink_bcbio(args, \"python\", \"bcbiovm\", \"bcbiovm\")\n            upgrade_thirdparty_tools(args, REMOTES)\n            print(\"Third party tools upgrade complete.\")\n    if args.toolplus:\n        print(\"Installing additional tools\")\n        _install_toolplus(args)\n    if args.install_data:\n        for default in DEFAULT_INDEXES:\n            if default not in args.aligners:\n                args.aligners.append(default)\n        if len(args.aligners) == 0:\n            print(\"Warning: no aligners provided with `--aligners` flag\")\n        if len(args.genomes) == 0:\n            print(\"Data not installed, no genomes provided with `--genomes` flag\")\n        else:\n            with bcbio_tmpdir():\n                print(\"Upgrading bcbio-nextgen data files\")\n                upgrade_bcbio_data(args, REMOTES)\n                print(\"bcbio-nextgen data upgrade complete.\")\n    if args.isolate and args.tooldir:\n        print(\"Isolated tool installation not automatically added to environmental variables\")\n        print(\" Add:\\n  {t}/bin to PATH\".format(t=args.tooldir))\n    save_install_defaults(args)\n    args.datadir = _get_data_dir()\n    _install_container_bcbio_system(args.datadir)\n    print(\"Upgrade completed successfully.\")\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _pip_safe_ssl(cmds, anaconda_dir):\n    try:\n        for cmd in cmds:\n            subprocess.check_call(cmd)\n    except subprocess.CalledProcessError:\n        _set_pip_ssl(anaconda_dir)\n        for cmd in cmds:\n            subprocess.check_call(cmd)", "response": "Run pip and retry with conda SSL certificate if global certificate fails."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _set_pip_ssl(anaconda_dir):\n    if anaconda_dir:\n        cert_file = os.path.join(anaconda_dir, \"ssl\", \"cert.pem\")\n        if os.path.exists(cert_file):\n            os.environ[\"PIP_CERT\"] = cert_file", "response": "Set PIP SSL certificate to installed conda certificate to avoid SSL errors"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _set_matplotlib_default_backend():\n    if _matplotlib_installed():\n        import matplotlib\n        matplotlib.use('Agg', force=True)\n        config = matplotlib.matplotlib_fname()\n        if os.access(config, os.W_OK):\n            with file_transaction(config) as tx_out_file:\n                with open(config) as in_file, open(tx_out_file, \"w\") as out_file:\n                    for line in in_file:\n                        if line.split(\":\")[0].strip() == \"backend\":\n                            out_file.write(\"backend: agg\\n\")\n                        else:\n                            out_file.write(line)", "response": "Set the default backend for the current available item."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _symlink_bcbio(args, script=\"bcbio_nextgen.py\", env_name=None, prefix=None):\n    if env_name:\n        bcbio_anaconda = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(sys.executable))),\n                                      \"envs\", env_name, \"bin\", script)\n    else:\n        bcbio_anaconda = os.path.join(os.path.dirname(os.path.realpath(sys.executable)), script)\n    bindir = os.path.join(args.tooldir, \"bin\")\n    if not os.path.exists(bindir):\n        os.makedirs(bindir)\n    if prefix:\n        script = \"%s_%s\" % (prefix, script)\n    bcbio_final = os.path.join(bindir, script)\n    if not os.path.exists(bcbio_final):\n        if os.path.lexists(bcbio_final):\n            subprocess.check_call([\"rm\", \"-f\", bcbio_final])\n        subprocess.check_call([\"ln\", \"-s\", bcbio_anaconda, bcbio_final])", "response": "Ensure a bcbio - nextgen script symlink in final tool directory."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _install_container_bcbio_system(datadir):\n    base_file = os.path.join(datadir, \"config\", \"bcbio_system.yaml\")\n    if not os.path.exists(base_file):\n        return\n    expose_file = os.path.join(datadir, \"galaxy\", \"bcbio_system.yaml\")\n    expose = set([\"memory\", \"cores\", \"jvm_opts\"])\n    with open(base_file) as in_handle:\n        config = yaml.safe_load(in_handle)\n    if os.path.exists(expose_file):\n        with open(expose_file) as in_handle:\n            expose_config = yaml.safe_load(in_handle)\n    else:\n        expose_config = {\"resources\": {}}\n    for pname, vals in config[\"resources\"].items():\n        expose_vals = {}\n        for k, v in vals.items():\n            if k in expose:\n                expose_vals[k] = v\n        if len(expose_vals) > 0 and pname not in expose_config[\"resources\"]:\n            expose_config[\"resources\"][pname] = expose_vals\n    if expose_file and os.path.exists(os.path.dirname(expose_file)):\n        with open(expose_file, \"w\") as out_handle:\n            yaml.safe_dump(expose_config, out_handle, default_flow_style=False, allow_unicode=False)\n    return expose_file", "response": "Install limited bcbio_system. yaml file for setting core and memory usage."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nidentifying post - install conda problems and fix.", "response": "def _check_for_conda_problems():\n    \"\"\"Identify post-install conda problems and fix.\n\n    - libgcc upgrades can remove libquadmath, which moved to libgcc-ng\n    \"\"\"\n    conda_bin = _get_conda_bin()\n    channels = _get_conda_channels(conda_bin)\n    lib_dir = os.path.join(os.path.dirname(conda_bin), os.pardir, \"lib\")\n    for l in [\"libgomp.so.1\", \"libquadmath.so\"]:\n        if not os.path.exists(os.path.join(lib_dir, l)):\n            subprocess.check_call([conda_bin, \"install\", \"-f\", \"--yes\"] + channels + [\"libgcc-ng\"])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating or install a local bcbiovm install with tools and dependencies.", "response": "def _update_bcbiovm():\n    \"\"\"Update or install a local bcbiovm install with tools and dependencies.\n    \"\"\"\n    print(\"## CWL support with bcbio-vm\")\n    python_env = \"python=3\"\n    conda_bin, env_name = _add_environment(\"bcbiovm\", python_env)\n    channels = _get_conda_channels(conda_bin)\n    base_cmd = [conda_bin, \"install\", \"--yes\", \"--name\", env_name] + channels\n    subprocess.check_call(base_cmd + [python_env, \"nomkl\", \"bcbio-nextgen\"])\n    extra_uptodate = [\"cromwell\"]\n    subprocess.check_call(base_cmd + [python_env, \"bcbio-nextgen-vm\"] + extra_uptodate)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_conda_channels(conda_bin):\n    channels = [\"bioconda\", \"conda-forge\"]\n    out = []\n    config = yaml.safe_load(subprocess.check_output([conda_bin, \"config\", \"--show\"]))\n    for c in channels:\n        present = False\n        for orig_c in config.get(\"channels\") or []:\n            if orig_c.endswith((c, \"%s/\" % c)):\n                present = True\n                break\n        if not present:\n            out += [\"-c\", c]\n    return out", "response": "Retrieve default conda channels"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _update_conda_packages():\n    conda_bin = _get_conda_bin()\n    channels = _get_conda_channels(conda_bin)\n    assert conda_bin, (\"Could not find anaconda distribution for upgrading bcbio.\\n\"\n                       \"Using python at %s but could not find conda.\" % (os.path.realpath(sys.executable)))\n    req_file = \"bcbio-update-requirements.txt\"\n    if os.path.exists(req_file):\n        os.remove(req_file)\n    subprocess.check_call([\"wget\", \"-O\", req_file, \"--no-check-certificate\", REMOTES[\"requirements\"]])\n    subprocess.check_call([conda_bin, \"install\", \"--quiet\", \"--yes\"] + channels +\n                          [\"--file\", req_file])\n    if os.path.exists(req_file):\n        os.remove(req_file)\n    return os.path.dirname(os.path.dirname(conda_bin))", "response": "Update the anaconda packages."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _update_conda_devel():\n    conda_bin = _get_conda_bin()\n    channels = _get_conda_channels(conda_bin)\n    assert conda_bin, \"Could not find anaconda distribution for upgrading bcbio\"\n    subprocess.check_call([conda_bin, \"install\", \"--quiet\", \"--yes\"] + channels +\n                           [\"bcbio-nextgen>=%s\" % version.__version__.replace(\"a0\", \"a\")])\n    return os.path.dirname(os.path.dirname(conda_bin))", "response": "Update to the latest development conda package."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_genome_dir(gid, galaxy_dir, data):\n    if galaxy_dir:\n        refs = genome.get_refs(gid, None, galaxy_dir, data)\n        seq_file = tz.get_in([\"fasta\", \"base\"], refs)\n        if seq_file and os.path.exists(seq_file):\n            return os.path.dirname(os.path.dirname(seq_file))\n    else:\n        gdirs = glob.glob(os.path.join(_get_data_dir(), \"genomes\", \"*\", gid))\n        if len(gdirs) == 1 and os.path.exists(gdirs[0]):\n            return gdirs[0]", "response": "Return standard location of genome directories."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef upgrade_bcbio_data(args, remotes):\n    if hasattr(args, \"datadir\") and args.datadir and os.path.exists(args.datadir):\n        data_dir = args.datadir\n    else:\n        data_dir = _get_data_dir()\n    tooldir = args.tooldir or get_defaults().get(\"tooldir\")\n    galaxy_home = os.path.join(data_dir, \"galaxy\")\n    cbl = get_cloudbiolinux(remotes)\n    tool_data_table_conf_file = os.path.join(cbl[\"dir\"], \"installed_files\", \"tool_data_table_conf.xml\")\n    genome_opts = _get_biodata(cbl[\"biodata\"], args)\n    sys.path.insert(0, cbl[\"dir\"])\n    cbl_genomes = __import__(\"cloudbio.biodata.genomes\", fromlist=[\"genomes\"])\n    cbl_genomes.install_data_local(genome_opts, tooldir, data_dir, galaxy_home, tool_data_table_conf_file,\n                                   args.cores, [\"ggd\", \"s3\", \"raw\"])\n    _upgrade_genome_resources(galaxy_home, remotes[\"genome_resources\"])\n    _upgrade_snpeff_data(galaxy_home, args, remotes)\n    if \"vep\" in args.datatarget:\n        _upgrade_vep_data(galaxy_home, tooldir)\n    if \"kraken\" in args.datatarget:\n        _install_kraken_db(_get_data_dir(), args)\n    if args.cwl:\n        _prepare_cwl_tarballs(data_dir)", "response": "Upgrade required genome data files in place."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate CWL ready tarballs for complex directories.", "response": "def _prepare_cwl_tarballs(data_dir):\n    \"\"\"Create CWL ready tarballs for complex directories.\n\n    Avoids need for CWL runners to pass and serialize complex directories\n    of files, which is inconsistent between runners.\n    \"\"\"\n    for dbref_dir in filter(os.path.isdir, glob.glob(os.path.join(data_dir, \"genomes\", \"*\", \"*\"))):\n        base_dir, dbref = os.path.split(dbref_dir)\n        for indexdir in TARBALL_DIRECTORIES:\n            cur_target = os.path.join(dbref_dir, indexdir)\n            if os.path.isdir(cur_target):\n                # Some indices, like rtg, have a single nested directory\n                subdirs = [x for x in os.listdir(cur_target) if os.path.isdir(os.path.join(cur_target, x))]\n                if len(subdirs) == 1:\n                    cur_target = os.path.join(cur_target, subdirs[0])\n                create.directory_tarball(cur_target)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves latest version of genome resource YAML configuration files.", "response": "def _upgrade_genome_resources(galaxy_dir, base_url):\n    \"\"\"Retrieve latest version of genome resource YAML configuration files.\n    \"\"\"\n    import requests\n    for dbkey, ref_file in genome.get_builds(galaxy_dir):\n        # Check for a remote genome resources file\n        remote_url = base_url % dbkey\n        requests.packages.urllib3.disable_warnings()\n        r = requests.get(remote_url, verify=False)\n        if r.status_code == requests.codes.ok:\n            local_file = os.path.join(os.path.dirname(ref_file), os.path.basename(remote_url))\n            if os.path.exists(local_file):\n                with open(local_file) as in_handle:\n                    local_config = yaml.safe_load(in_handle)\n                remote_config = yaml.safe_load(r.text)\n                needs_update = remote_config[\"version\"] > local_config.get(\"version\", 0)\n                if needs_update:\n                    shutil.move(local_file, local_file + \".old%s\" % local_config.get(\"version\", 0))\n            else:\n                needs_update = True\n            if needs_update:\n                print(\"Updating %s genome resources configuration\" % dbkey)\n                with open(local_file, \"w\") as out_handle:\n                    out_handle.write(r.text)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninstalling or upgrade snpEff databases localized to reference directory.", "response": "def _upgrade_snpeff_data(galaxy_dir, args, remotes):\n    \"\"\"Install or upgrade snpEff databases, localized to reference directory.\n    \"\"\"\n    snpeff_version = effects.snpeff_version(args)\n    if not snpeff_version:\n        return\n    for dbkey, ref_file in genome.get_builds(galaxy_dir):\n        resource_file = os.path.join(os.path.dirname(ref_file), \"%s-resources.yaml\" % dbkey)\n        if os.path.exists(resource_file):\n            with open(resource_file) as in_handle:\n                resources = yaml.safe_load(in_handle)\n            snpeff_db, snpeff_base_dir = effects.get_db({\"genome_resources\": resources,\n                                                         \"reference\": {\"fasta\": {\"base\": ref_file}}})\n            if snpeff_db:\n                snpeff_db_dir = os.path.join(snpeff_base_dir, snpeff_db)\n                if os.path.exists(snpeff_db_dir) and _is_old_database(snpeff_db_dir, args):\n                    shutil.rmtree(snpeff_db_dir)\n                if not os.path.exists(snpeff_db_dir):\n                    print(\"Installing snpEff database %s in %s\" % (snpeff_db, snpeff_base_dir))\n                    dl_url = remotes[\"snpeff_dl_url\"].format(\n                        snpeff_ver=snpeff_version.replace(\".\", \"_\"),\n                        genome=snpeff_db)\n                    dl_file = os.path.basename(dl_url)\n                    with utils.chdir(snpeff_base_dir):\n                        subprocess.check_call([\"wget\", \"--no-check-certificate\", \"-c\", \"-O\", dl_file, dl_url])\n                        subprocess.check_call([\"unzip\", dl_file])\n                        os.remove(dl_file)\n                    dl_dir = os.path.join(snpeff_base_dir, \"data\", snpeff_db)\n                    shutil.move(dl_dir, snpeff_db_dir)\n                    os.rmdir(os.path.join(snpeff_base_dir, \"data\"))\n                if args.cwl:\n                    create.directory_tarball(snpeff_db_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _is_old_database(db_dir, args):\n    snpeff_version = effects.snpeff_version(args)\n    if LooseVersion(snpeff_version) >= LooseVersion(\"4.1\"):\n        pred_file = os.path.join(db_dir, \"snpEffectPredictor.bin\")\n        if not utils.file_exists(pred_file):\n            return True\n        with utils.open_gzipsafe(pred_file, is_gz=True) as in_handle:\n            version_info = in_handle.readline().strip().split(\"\\t\")\n        program, version = version_info[:2]\n        if not program.lower() == \"snpeff\" or LooseVersion(snpeff_version) > LooseVersion(version):\n            return True\n    return False", "response": "Check if we are running on an old database version."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_biodata(base_file, args):\n    with open(base_file) as in_handle:\n        config = yaml.safe_load(in_handle)\n    config[\"install_liftover\"] = False\n    config[\"genome_indexes\"] = args.aligners\n    ann_groups = config.pop(\"annotation_groups\", {})\n    config[\"genomes\"] = [_setup_genome_annotations(g, args, ann_groups)\n                         for g in config[\"genomes\"] if g[\"dbkey\"] in args.genomes]\n    return config", "response": "Retrieve biodata genome targets customized by install parameters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _setup_genome_annotations(g, args, ann_groups):\n    available_anns = g.get(\"annotations\", []) + g.pop(\"annotations_available\", [])\n    anns = []\n    for orig_target in args.datatarget:\n        if orig_target in ann_groups:\n            targets = ann_groups[orig_target]\n        else:\n            targets = [orig_target]\n        for target in targets:\n            if target in available_anns:\n                anns.append(target)\n    g[\"annotations\"] = anns\n    if \"variation\" not in args.datatarget and \"validation\" in g:\n        del g[\"validation\"]\n    return g", "response": "Configure genome annotations to install based on datatarget."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef upgrade_thirdparty_tools(args, remotes):\n    cbl = get_cloudbiolinux(remotes)\n    if args.toolconf and os.path.exists(args.toolconf):\n        package_yaml = args.toolconf\n    else:\n        package_yaml = os.path.join(cbl[\"dir\"], \"contrib\", \"flavor\",\n                                    \"ngs_pipeline_minimal\", \"packages-conda.yaml\")\n    sys.path.insert(0, cbl[\"dir\"])\n    cbl_conda = __import__(\"cloudbio.package.conda\", fromlist=[\"conda\"])\n    cbl_conda.install_in(_get_conda_bin(), args.tooldir, package_yaml)\n    manifest_dir = os.path.join(_get_data_dir(), \"manifest\")\n    print(\"Creating manifest of installed packages in %s\" % manifest_dir)\n    cbl_manifest = __import__(\"cloudbio.manifest\", fromlist=[\"manifest\"])\n    if os.path.exists(manifest_dir):\n        for fname in os.listdir(manifest_dir):\n            if not fname.startswith(\"toolplus\"):\n                os.remove(os.path.join(manifest_dir, fname))\n    cbl_manifest.create(manifest_dir, args.tooldir)", "response": "Install and update third party tools on the system."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninstalling additional tools we cannot distribute into the local manifest.", "response": "def _install_toolplus(args):\n    \"\"\"Install additional tools we cannot distribute, updating local manifest.\n    \"\"\"\n    manifest_dir = os.path.join(_get_data_dir(), \"manifest\")\n    toolplus_manifest = os.path.join(manifest_dir, \"toolplus-packages.yaml\")\n    system_config = os.path.join(_get_data_dir(), \"galaxy\", \"bcbio_system.yaml\")\n    # Handle toolplus installs inside Docker container\n    if not os.path.exists(system_config):\n        docker_system_config = os.path.join(_get_data_dir(), \"config\", \"bcbio_system.yaml\")\n        if os.path.exists(docker_system_config):\n            system_config = docker_system_config\n    toolplus_dir = os.path.join(_get_data_dir(), \"toolplus\")\n    for tool in args.toolplus:\n        if tool.name in set([\"gatk\", \"mutect\"]):\n            print(\"Installing %s\" % tool.name)\n            _install_gatk_jar(tool.name, tool.fname, toolplus_manifest, system_config, toolplus_dir)\n        else:\n            raise ValueError(\"Unexpected toolplus argument: %s %s\" % (tool.name, tool.fname))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _install_gatk_jar(name, fname, manifest, system_config, toolplus_dir):\n    if not fname.endswith(\".jar\"):\n        raise ValueError(\"--toolplus argument for %s expects a jar file: %s\" % (name, fname))\n    version = get_gatk_jar_version(name, fname)\n    store_dir = utils.safe_makedir(os.path.join(toolplus_dir, name, version))\n    shutil.copyfile(fname, os.path.join(store_dir, os.path.basename(fname)))\n    _update_system_file(system_config, name, {\"dir\": store_dir})\n    _update_manifest(manifest, name, version)", "response": "Install a GATK jar file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _update_manifest(manifest_file, name, version):\n    if os.path.exists(manifest_file):\n        with open(manifest_file) as in_handle:\n            manifest = yaml.safe_load(in_handle)\n    else:\n        manifest = {}\n    manifest[name] = {\"name\": name, \"version\": version}\n    with open(manifest_file, \"w\") as out_handle:\n        yaml.safe_dump(manifest, out_handle, default_flow_style=False, allow_unicode=False)", "response": "Update the toolplus manifest file with updated name and version"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the bcbio_system. yaml file with new resource information.", "response": "def _update_system_file(system_file, name, new_kvs):\n    \"\"\"Update the bcbio_system.yaml file with new resource information.\n    \"\"\"\n    if os.path.exists(system_file):\n        bak_file = system_file + \".bak%s\" % datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n        shutil.copyfile(system_file, bak_file)\n        with open(system_file) as in_handle:\n            config = yaml.safe_load(in_handle)\n    else:\n        utils.safe_makedir(os.path.dirname(system_file))\n        config = {}\n    new_rs = {}\n    added = False\n    for rname, r_kvs in config.get(\"resources\", {}).items():\n        if rname == name:\n            for k, v in new_kvs.items():\n                r_kvs[k] = v\n            added = True\n        new_rs[rname] = r_kvs\n    if not added:\n        new_rs[name] = new_kvs\n    config[\"resources\"] = new_rs\n    with open(system_file, \"w\") as out_handle:\n        yaml.safe_dump(config, out_handle, default_flow_style=False, allow_unicode=False)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _install_kraken_db(datadir, args):\n    import requests\n    kraken = os.path.join(datadir, \"genomes/kraken\")\n    url = \"https://ccb.jhu.edu/software/kraken/dl/minikraken.tgz\"\n    compress = os.path.join(kraken, os.path.basename(url))\n    base, ext = utils.splitext_plus(os.path.basename(url))\n    db = os.path.join(kraken, base)\n    tooldir = args.tooldir or get_defaults()[\"tooldir\"]\n    requests.packages.urllib3.disable_warnings()\n    last_mod = urllib.request.urlopen(url).info().get('Last-Modified')\n    last_mod = dateutil.parser.parse(last_mod).astimezone(dateutil.tz.tzutc())\n    if os.path.exists(os.path.join(tooldir, \"bin\", \"kraken\")):\n        if not os.path.exists(db):\n            is_new_version = True\n        else:\n            cur_file = glob.glob(os.path.join(kraken, \"minikraken_*\"))[0]\n            cur_version = datetime.datetime.utcfromtimestamp(os.path.getmtime(cur_file))\n            is_new_version = last_mod.date() > cur_version.date()\n            if is_new_version:\n                shutil.move(cur_file, cur_file.replace('minikraken', 'old'))\n        if not os.path.exists(kraken):\n            utils.safe_makedir(kraken)\n        if is_new_version:\n            if not os.path.exists(compress):\n                subprocess.check_call([\"wget\", \"-O\", compress, url, \"--no-check-certificate\"])\n            cmd = [\"tar\", \"-xzvf\", compress, \"-C\", kraken]\n            subprocess.check_call(cmd)\n            last_version = glob.glob(os.path.join(kraken, \"minikraken_*\"))\n            utils.symlink_plus(os.path.join(kraken, last_version[0]), os.path.join(kraken, \"minikraken\"))\n            utils.remove_safe(compress)\n        else:\n            print(\"You have the latest version %s.\" % last_mod)\n    else:\n        raise argparse.ArgumentTypeError(\"kraken not installed in tooldir %s.\" %\n                                         os.path.join(tooldir, \"bin\", \"kraken\"))", "response": "Install kraken minimal DB in genome folder."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the YAML configuration file used to store upgrade information.", "response": "def _get_install_config():\n    \"\"\"Return the YAML configuration file used to store upgrade information.\n    \"\"\"\n    try:\n        data_dir = _get_data_dir()\n    except ValueError:\n        return None\n    config_dir = utils.safe_makedir(os.path.join(data_dir, \"config\"))\n    return os.path.join(config_dir, \"install-params.yaml\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_install_defaults(args):\n    install_config = _get_install_config()\n    if install_config is None:\n        return\n    if utils.file_exists(install_config):\n        with open(install_config) as in_handle:\n            cur_config = yaml.safe_load(in_handle)\n    else:\n        cur_config = {}\n    if args.tooldir:\n        cur_config[\"tooldir\"] = args.tooldir\n    cur_config[\"isolate\"] = args.isolate\n    for attr in [\"genomes\", \"aligners\", \"datatarget\"]:\n        if not cur_config.get(attr):\n            cur_config[attr] = []\n        for x in getattr(args, attr):\n            if x not in cur_config[attr]:\n                cur_config[attr].append(x)\n    # toolplus -- save non-filename inputs\n    attr = \"toolplus\"\n    if not cur_config.get(attr):\n        cur_config[attr] = []\n    for x in getattr(args, attr):\n        if not x.fname:\n            if x.name not in cur_config[attr]:\n                cur_config[attr].append(x.name)\n    with open(install_config, \"w\") as out_handle:\n        yaml.safe_dump(cur_config, out_handle, default_flow_style=False, allow_unicode=False)", "response": "Save installation information to make future upgrades easier."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd any saved installation defaults to the upgrade.", "response": "def add_install_defaults(args):\n    \"\"\"Add any saved installation defaults to the upgrade.\n    \"\"\"\n    # Ensure we install data if we've specified any secondary installation targets\n    if len(args.genomes) > 0 or len(args.aligners) > 0 or len(args.datatarget) > 0:\n        args.install_data = True\n    install_config = _get_install_config()\n    if install_config is None or not utils.file_exists(install_config):\n        default_args = {}\n    else:\n        with open(install_config) as in_handle:\n            default_args = yaml.safe_load(in_handle)\n    # if we are upgrading to development, also upgrade the tools\n    if args.upgrade in [\"development\"] and (args.tooldir or \"tooldir\" in default_args):\n        args.tools = True\n    if args.tools and args.tooldir is None:\n        if \"tooldir\" in default_args:\n            args.tooldir = str(default_args[\"tooldir\"])\n        else:\n            raise ValueError(\"Default tool directory not yet saved in config defaults. \"\n                             \"Specify the '--tooldir=/path/to/tools' to upgrade tools. \"\n                             \"After a successful upgrade, the '--tools' parameter will \"\n                             \"work for future upgrades.\")\n    for attr in [\"genomes\", \"aligners\"]:\n        # don't upgrade default genomes if a genome was specified\n        if attr == \"genomes\" and len(args.genomes) > 0:\n            continue\n        for x in default_args.get(attr, []):\n            x = str(x)\n            new_val = getattr(args, attr)\n            if x not in getattr(args, attr):\n                new_val.append(x)\n            setattr(args, attr, new_val)\n    args = _datatarget_defaults(args, default_args)\n    if \"isolate\" in default_args and args.isolate is not True:\n        args.isolate = default_args[\"isolate\"]\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the data installation targets handling defaults.", "response": "def _datatarget_defaults(args, default_args):\n    \"\"\"Set data installation targets, handling defaults.\n\n    Sets variation, rnaseq, smallrna as default targets if we're not\n    isolated to a single method.\n\n    Provides back compatibility for toolplus specifications.\n    \"\"\"\n    default_data = default_args.get(\"datatarget\", [])\n    # back-compatible toolplus specifications\n    for x in default_args.get(\"toolplus\", []):\n        val = None\n        if x == \"data\":\n            val = \"gemini\"\n        elif x in [\"cadd\", \"dbnsfp\", \"dbscsnv\", \"kraken\", \"gnomad\"]:\n            val = x\n        if val and val not in default_data:\n            default_data.append(val)\n    new_val = getattr(args, \"datatarget\")\n    for x in default_data:\n        if x not in new_val:\n            new_val.append(x)\n    has_std_target = False\n    std_targets = [\"variation\", \"rnaseq\", \"smallrna\"]\n    for target in std_targets:\n        if target in new_val:\n            has_std_target = True\n            break\n    if not has_std_target:\n        new_val = new_val + std_targets\n    setattr(args, \"datatarget\", new_val)\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _check_toolplus(x):\n    if \"=\" in x and len(x.split(\"=\")) == 2:\n        name, fname = x.split(\"=\")\n        fname = os.path.normpath(os.path.realpath(fname))\n        if not os.path.exists(fname):\n            raise argparse.ArgumentTypeError(\"Unexpected --toolplus argument for %s. File does not exist: %s\"\n                                             % (name, fname))\n        return Tool(name, fname)\n    else:\n        raise argparse.ArgumentTypeError(\"Unexpected --toolplus argument. Expect toolname=filename.\")", "response": "Parse options for adding non - standard tools like GATK and MuTecT."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating all of the components of a CWL workflow from input steps.", "response": "def generate(variables, steps, final_outputs):\n    \"\"\"Generate all of the components of a CWL workflow from input steps.\n\n    file_vs and std_vs are the list of world variables, split into those that\n    reference files (and need declaration at each step) and those that don't\n    and can be safely passed to each step. This function keeps track of these\n    as they're added and updated by steps, passing this information on to\n    creation of the CWL files.\n    \"\"\"\n    file_vs, std_vs = _split_variables([_flatten_nested_input(v) for v in variables])\n    parallel_ids = []\n    for step in steps:\n        if hasattr(step, \"workflow\"):\n            wf_inputs = []\n            wf_outputs = []\n            wf_steps = []\n            for i, wf_step in enumerate(step.workflow):\n                inputs, parallel_ids, nested_inputs = _get_step_inputs(wf_step, file_vs, std_vs, parallel_ids, step)\n                # flatten inputs that are scattered on entering a sub-workflow\n                if step.parallel.endswith(\"-parallel\"):\n                    if i == 0:\n                        inputs = [_flatten_nested_input(v) for v in inputs]\n                        wf_scatter = [get_base_id(v[\"id\"]) for v in inputs]\n                    else:\n                        inputs = [_flatten_nested_input(v) if get_base_id(v[\"id\"]) in wf_scatter else v\n                                  for v in inputs]\n                wf_inputs, inputs = _merge_wf_inputs(inputs, wf_inputs, wf_outputs,\n                                                     step.internal, wf_step.parallel, nested_inputs)\n                outputs, file_vs, std_vs = _get_step_outputs(wf_step, wf_step.outputs, file_vs, std_vs)\n                wf_steps.append((\"step\", wf_step.name, wf_step.parallel, inputs,\n                                 outputs, wf_step.image, wf_step.programs, wf_step.disk, wf_step.cores,\n                                 wf_step.no_files))\n                parallel_ids = _find_split_vs(outputs, wf_step.parallel)\n                wf_outputs = _merge_wf_outputs(outputs, wf_outputs, wf_step.parallel)\n            yield \"wf_start\", wf_inputs\n            for wf_step in wf_steps:\n                yield wf_step\n            wf_outputs = [v for v in wf_outputs\n                          if v[\"id\"] not in set([\"%s\" % _get_string_vid(x) for x in step.internal])]\n            yield \"upload\", wf_outputs\n            wf_outputs, file_vs, std_vs = _get_step_outputs(step, wf_outputs, file_vs, std_vs)\n            yield \"wf_finish\", step.name, step.parallel, wf_inputs, wf_outputs, wf_scatter\n            file_vs = _extract_from_subworkflow(file_vs, step)\n            std_vs = _extract_from_subworkflow(std_vs, step)\n        elif hasattr(step, \"expression\"):\n            inputs, parallel_ids, nested_inputs = _get_step_inputs(step, file_vs, std_vs, parallel_ids)\n            outputs, file_vs, std_vs = _get_step_outputs(step, step.outputs, file_vs, std_vs)\n            parallel_ids = _find_split_vs(outputs, step.parallel)\n            yield (\"expressiontool\", step.name, inputs, outputs, step.expression, step.parallel)\n        else:\n            inputs, parallel_ids, nested_inputs = _get_step_inputs(step, file_vs, std_vs, parallel_ids)\n            outputs, file_vs, std_vs = _get_step_outputs(step, step.outputs, file_vs, std_vs)\n            parallel_ids = _find_split_vs(outputs, step.parallel)\n            yield (\"step\", step.name, step.parallel, inputs, outputs, step.image, step.programs,\n                   step.disk, step.cores, step.no_files)\n    yield \"upload\", [_get_upload_output(x, file_vs) for x in final_outputs]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _merge_wf_inputs(new, out, wf_outputs, to_ignore, parallel, nested_inputs):\n    internal_generated_ids = []\n    for vignore in to_ignore:\n        vignore_id = _get_string_vid(vignore)\n        # ignore anything we generate internally, but not those we need to pull in\n        # from the external process\n        if vignore_id not in [v[\"id\"] for v in wf_outputs]:\n            internal_generated_ids.append(vignore_id)\n    ignore_ids = set(internal_generated_ids + [v[\"id\"] for v in wf_outputs])\n    cur_ids = set([v[\"id\"] for v in out])\n    remapped_new = []\n    for v in new:\n        remapped_v = copy.deepcopy(v)\n        outv = copy.deepcopy(v)\n        outv[\"id\"] = get_base_id(v[\"id\"])\n        outv[\"source\"] = v[\"id\"]\n        if outv[\"id\"] not in cur_ids and outv[\"id\"] not in ignore_ids:\n            if nested_inputs and v[\"id\"] in nested_inputs:\n                outv = _flatten_nested_input(outv)\n            out.append(outv)\n        if remapped_v[\"id\"] in set([v[\"source\"] for v in out]):\n            remapped_v[\"source\"] = get_base_id(remapped_v[\"id\"])\n        remapped_new.append(remapped_v)\n    return out, remapped_new", "response": "Merge inputs for a sub - workflow adding any not present inputs in out."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmerging outputs for a sub - workflow.", "response": "def _merge_wf_outputs(new, cur, parallel):\n    \"\"\"Merge outputs for a sub-workflow, replacing variables changed in later steps.\n\n    ignore_ids are those used internally in a sub-workflow but not exposed to subsequent steps\n    \"\"\"\n    new_ids = set([])\n    out = []\n    for v in new:\n        outv = {}\n        outv[\"source\"] = v[\"id\"]\n        outv[\"id\"] = \"%s\" % get_base_id(v[\"id\"])\n        outv[\"type\"] = v[\"type\"]\n        if \"secondaryFiles\" in v:\n            outv[\"secondaryFiles\"] = v[\"secondaryFiles\"]\n        if tz.get_in([\"outputBinding\", \"secondaryFiles\"], v):\n            outv[\"secondaryFiles\"] = tz.get_in([\"outputBinding\", \"secondaryFiles\"], v)\n        new_ids.add(outv[\"id\"])\n        out.append(outv)\n    for outv in cur:\n        if outv[\"id\"] not in new_ids:\n            out.append(outv)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _extract_from_subworkflow(vs, step):\n    substep_ids = set([x.name for x in step.workflow])\n    out = []\n    for var in vs:\n        internal = False\n        parts = var[\"id\"].split(\"/\")\n        if len(parts) > 1:\n            if parts[0] in substep_ids:\n                internal = True\n        if not internal:\n            var.pop(\"source\", None)\n            out.append(var)\n    return out", "response": "Extract internal variable names when moving from sub - workflow to main."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_cwl_record(d):\n    if isinstance(d, dict):\n        if d.get(\"type\") == \"record\":\n            return d\n        else:\n            recs = list(filter(lambda x: x is not None, [is_cwl_record(v) for v in d.values()]))\n            return recs[0] if recs else None\n    else:\n        return None", "response": "Check if an input is a CWL record from any level of nesting."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves inputs for a single step from existing variables.", "response": "def _get_step_inputs(step, file_vs, std_vs, parallel_ids, wf=None):\n    \"\"\"Retrieve inputs for a step from existing variables.\n\n    Potentially nests inputs to deal with merging split variables. If\n    we split previously and are merging now, then we only nest those\n    coming from the split process.\n    \"\"\"\n    inputs = []\n    skip_inputs = set([])\n    for orig_input in [_get_variable(x, file_vs) for x in _handle_special_inputs(step.inputs, file_vs)]:\n        inputs.append(orig_input)\n    # Only add description and other information for non-record inputs, otherwise batched with records\n    if not any(is_cwl_record(x) for x in inputs):\n        inputs += [v for v in std_vs if get_base_id(v[\"id\"]) not in skip_inputs]\n    nested_inputs = []\n    if step.parallel in [\"single-merge\", \"batch-merge\"]:\n        if parallel_ids:\n            inputs = [_nest_variable(x) if x[\"id\"] in parallel_ids else x for x in inputs]\n            nested_inputs = parallel_ids[:]\n            parallel_ids = []\n    elif step.parallel in [\"multi-combined\"]:\n        assert len(parallel_ids) == 0\n        nested_inputs = [x[\"id\"] for x in inputs]\n        inputs = [_nest_variable(x) for x in inputs]\n    elif step.parallel in [\"multi-batch\"]:\n        assert len(parallel_ids) == 0\n        nested_inputs = [x[\"id\"] for x in inputs]\n        # If we're batching,with mixed records/inputs avoid double nesting records\n        inputs = [_nest_variable(x, check_records=(len(inputs) > 1)) for x in inputs]\n    # avoid inputs/outputs with the same name\n    outputs = [_get_string_vid(x[\"id\"]) for x in step.outputs]\n    final_inputs = []\n    for input in inputs:\n        input[\"wf_duplicate\"] = get_base_id(input[\"id\"]) in outputs\n        final_inputs.append(input)\n    return inputs, parallel_ids, nested_inputs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _flatten_nested_input(v):\n    v = copy.deepcopy(v)\n    if isinstance(v[\"type\"], dict) and v[\"type\"][\"type\"] == \"array\":\n        v[\"type\"] = v[\"type\"][\"items\"]\n    else:\n        assert isinstance(v[\"type\"], (list, tuple)), v\n        new_type = None\n        want_null = False\n        for x in v[\"type\"]:\n            if isinstance(x, dict) and x[\"type\"] == \"array\":\n                new_type = x[\"items\"]\n            elif isinstance(x, six.string_types) and x == \"null\":\n                want_null = True\n            else:\n                new_type = x\n        if want_null:\n            if not isinstance(new_type, (list, tuple)):\n                new_type = [new_type] if new_type is not None else []\n            for toadd in [\"null\", \"string\"]:\n                if toadd not in new_type:\n                    new_type.append(toadd)\n        assert new_type, v\n        v[\"type\"] = new_type\n    return v", "response": "Flatten a parallel scatter input"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _nest_variable(v, check_records=False):\n    if (check_records and is_cwl_record(v) and len(v[\"id\"].split(\"/\")) > 1 and\n         v.get(\"type\", {}).get(\"type\") == \"array\"):\n        return v\n    else:\n        v = copy.deepcopy(v)\n        v[\"type\"] = {\"type\": \"array\", \"items\": v[\"type\"]}\n        return v", "response": "Nest a variable when moving from scattered back to consolidated."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _clean_output(v):\n    out = copy.deepcopy(v)\n    outb = out.pop(\"outputBinding\", {})\n    if \"secondaryFiles\" in outb:\n        out[\"secondaryFiles\"] = outb[\"secondaryFiles\"]\n    return out", "response": "Remove output specific variables to allow variables to be inputs to next steps."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving an input variable from our existing pool of options.", "response": "def _get_variable(vid, variables):\n    \"\"\"Retrieve an input variable from our existing pool of options.\n    \"\"\"\n    if isinstance(vid, six.string_types):\n        vid = get_base_id(vid)\n    else:\n        vid = _get_string_vid(vid)\n    for v in variables:\n        if vid == get_base_id(v[\"id\"]):\n            return copy.deepcopy(v)\n    raise ValueError(\"Did not find variable %s in \\n%s\" % (vid, pprint.pformat(variables)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _handle_special_inputs(inputs, variables):\n    from bcbio import structural\n    optional = [[\"config\", \"algorithm\", \"coverage\"],\n                [\"config\", \"algorithm\", \"variant_regions\"],\n                [\"config\", \"algorithm\", \"sv_regions\"],\n                [\"config\", \"algorithm\", \"validate\"],\n                [\"config\", \"algorithm\", \"validate_regions\"]]\n    all_vs = set([get_base_id(v[\"id\"]) for v in variables])\n    out = []\n    for input in inputs:\n        if input == [\"reference\", \"aligner\", \"indexes\"]:\n            for v in variables:\n                vid = get_base_id(v[\"id\"]).split(\"__\")\n                if vid[0] == \"reference\" and vid[1] in alignment.TOOLS:\n                    out.append(vid)\n        elif input == [\"reference\", \"snpeff\", \"genome_build\"]:\n            found_indexes = False\n            for v in variables:\n                vid = get_base_id(v[\"id\"]).split(\"__\")\n                if vid[0] == \"reference\" and vid[1] == \"snpeff\":\n                    out.append(vid)\n                    found_indexes = True\n            assert found_indexes, \"Found no snpEff indexes in %s\" % [v[\"id\"] for v in variables]\n        elif input == [\"config\", \"algorithm\", \"background\", \"cnv_reference\"]:\n            for v in variables:\n                vid = get_base_id(v[\"id\"]).split(\"__\")\n                if (vid[:4] == [\"config\", \"algorithm\", \"background\", \"cnv_reference\"] and\n                      structural.supports_cnv_reference(vid[4])):\n                    out.append(vid)\n        elif input in optional:\n            if _get_string_vid(input) in all_vs:\n                out.append(input)\n        else:\n            out.append(input)\n    return out", "response": "This function handles special inputs that are not used by the CNV."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_record(name, field_defs, step_name, inputs, unlist, file_vs, std_vs, parallel):\n    if field_defs:\n        fields = []\n        inherit = []\n        inherit_all = False\n        inherit_exclude = []\n        for fdef in field_defs:\n            if not fdef.get(\"type\"):\n                if fdef[\"id\"] == \"inherit\":\n                    inherit_all = True\n                    inherit_exclude = fdef.get(\"exclude\", [])\n                else:\n                    inherit.append(fdef[\"id\"])\n            else:\n                cur = {\"name\": _get_string_vid(fdef[\"id\"]),\n                       \"type\": fdef[\"type\"]}\n                fields.append(_add_secondary_to_rec_field(fdef, cur))\n        if inherit_all:\n            fields.extend(_infer_record_outputs(inputs, unlist, file_vs, std_vs, parallel, exclude=inherit_exclude))\n        elif inherit:\n            fields.extend(_infer_record_outputs(inputs, unlist, file_vs, std_vs, parallel, inherit))\n    else:\n        fields = _infer_record_outputs(inputs, unlist, file_vs, std_vs, parallel)\n    out = {\"id\": \"%s/%s\" % (step_name, name),\n           \"type\": {\"name\": name,\n                    \"type\": \"record\",\n                    \"fields\": fields}}\n    if parallel in [\"batch-single\", \"multi-batch\"]:\n        out = _nest_variable(out)\n    return out", "response": "Create an output record by rearranging inputs."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninfers the outputs of a record from the original inputs", "response": "def _infer_record_outputs(inputs, unlist, file_vs, std_vs, parallel, to_include=None,\n                          exclude=None):\n    \"\"\"Infer the outputs of a record from the original inputs\n    \"\"\"\n    fields = []\n    unlist = set([_get_string_vid(x) for x in unlist])\n    input_vids = set([_get_string_vid(v) for v in _handle_special_inputs(inputs, file_vs)])\n    to_include = set([_get_string_vid(x) for x in to_include]) if to_include else None\n    to_exclude = tuple(set([_get_string_vid(x) for x in exclude])) if exclude else None\n    added = set([])\n    for raw_v in std_vs + [v for v in file_vs if get_base_id(v[\"id\"]) in input_vids]:\n        # unpack record inside this record and un-nested inputs to avoid double nested\n        cur_record = is_cwl_record(raw_v)\n        if cur_record:\n            # unlist = unlist | set([field[\"name\"] for field in cur_record[\"fields\"]])\n            nested_vs = [{\"id\": field[\"name\"], \"type\": field[\"type\"]} for field in cur_record[\"fields\"]]\n        else:\n            nested_vs = [raw_v]\n        for orig_v in nested_vs:\n            if (get_base_id(orig_v[\"id\"]) not in added\n                 and (not to_include or get_base_id(orig_v[\"id\"]) in to_include)):\n                if to_exclude is None or not get_base_id(orig_v[\"id\"]).startswith(to_exclude):\n                    cur_v = {}\n                    cur_v[\"name\"] = get_base_id(orig_v[\"id\"])\n                    cur_v[\"type\"] = orig_v[\"type\"]\n                    if cur_v[\"name\"] in unlist:\n                        cur_v = _flatten_nested_input(cur_v)\n                    fields.append(_add_secondary_to_rec_field(orig_v, cur_v))\n                    added.add(get_base_id(orig_v[\"id\"]))\n    return fields"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new output variable potentially over - writing existing or creating new.", "response": "def _create_variable(orig_v, step, variables):\n    \"\"\"Create a new output variable, potentially over-writing existing or creating new.\n    \"\"\"\n    # get current variable, and convert to be the output of our process step\n    try:\n        v = _get_variable(orig_v[\"id\"], variables)\n    except ValueError:\n        v = copy.deepcopy(orig_v)\n        if not isinstance(v[\"id\"], six.string_types):\n            v[\"id\"] = _get_string_vid(v[\"id\"])\n    for key, val in orig_v.items():\n        if key not in [\"id\", \"type\"]:\n            v[key] = val\n    if orig_v.get(\"type\") != \"null\":\n        v[\"type\"] = orig_v[\"type\"]\n    v[\"id\"] = \"%s/%s\" % (step.name, get_base_id(v[\"id\"]))\n    return v"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _merge_variables(new, cur):\n    new_added = set([])\n    out = []\n    for cur_var in cur:\n        updated = False\n        for new_var in new:\n            if get_base_id(new_var[\"id\"]) == get_base_id(cur_var[\"id\"]):\n                out.append(new_var)\n                new_added.add(new_var[\"id\"])\n                updated = True\n                break\n        if not updated:\n            out.append(cur_var)\n    for new_var in new:\n        if new_var[\"id\"] not in new_added:\n            out.append(new_var)\n    return out", "response": "Add any new variables to the world representation in cur."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsplits variables into always passed ( std ) and specified ( file ).", "response": "def _split_variables(variables):\n    \"\"\"Split variables into always passed (std) and specified (file).\n\n    We always pass some variables to each step but need to\n    explicitly define file and algorithm variables so they can\n    be linked in as needed.\n    \"\"\"\n    file_vs = []\n    std_vs = []\n    for v in variables:\n        cur_type = v[\"type\"]\n        while isinstance(cur_type, dict):\n            if \"items\" in cur_type:\n                cur_type = cur_type[\"items\"]\n            else:\n                cur_type = cur_type[\"type\"]\n        if (cur_type in [\"File\", \"null\", \"record\"] or\n              (isinstance(cur_type, (list, tuple)) and\n               (\"File\" in cur_type or {'items': 'File', 'type': 'array'} in cur_type))):\n            file_vs.append(v)\n        elif v[\"id\"] in ALWAYS_AVAILABLE:\n            std_vs.append(v)\n        else:\n            file_vs.append(v)\n    return file_vs, std_vs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsplit samples into all possible genomes for alignment.", "response": "def split(*items):\n    \"\"\"Split samples into all possible genomes for alignment.\n    \"\"\"\n    out = []\n    for data in [x[0] for x in items]:\n        dis_orgs = data[\"config\"][\"algorithm\"].get(\"disambiguate\")\n        if dis_orgs:\n            if not data.get(\"disambiguate\", None):\n                data[\"disambiguate\"] = {\"genome_build\": data[\"genome_build\"],\n                                        \"base\": True}\n            out.append([data])\n            # handle the instance where a single organism is disambiguated\n            if isinstance(dis_orgs, six.string_types):\n                dis_orgs = [dis_orgs]\n            for dis_org in dis_orgs:\n                dis_data = copy.deepcopy(data)\n                dis_data[\"disambiguate\"] = {\"genome_build\": dis_org}\n                dis_data[\"genome_build\"] = dis_org\n                dis_data[\"config\"][\"algorithm\"][\"effects\"] = False\n                dis_data = run_info.add_reference_resources(dis_data)\n                out.append([dis_data])\n        else:\n            out.append([data])\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resolve(items, run_parallel):\n    out = []\n    to_process = collections.defaultdict(list)\n    for data in [x[0] for x in items]:\n        if \"disambiguate\" in data:\n            split_part = tuple([int(x) for x in data[\"align_split\"].split(\"-\")]) if data.get(\"combine\") else None\n            to_process[(dd.get_sample_name(data), split_part)].append(data)\n        else:\n            out.append([data])\n    if len(to_process) > 0:\n        dis1 = run_parallel(\"run_disambiguate\",\n                            [(xs, xs[0][\"config\"]) for xs in to_process.values()])\n        disambigs_by_name = collections.defaultdict(list)\n        print(len(dis1))\n        for xs in dis1:\n            assert len(xs) == 1\n            data = xs[0]\n            disambigs_by_name[dd.get_sample_name(data)].append(data)\n        dis2 = run_parallel(\"disambiguate_merge_extras\",\n                            [(xs, xs[0][\"config\"]) for xs in disambigs_by_name.values()])\n    else:\n        dis2 = []\n    return out + dis2", "response": "Combine aligned and split samples into final set of disambiguated reads."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef merge_extras(items, config):\n    final = {}\n    for extra_name in items[0][\"disambiguate\"].keys():\n        in_files = []\n        for data in items:\n            in_files.append(data[\"disambiguate\"][extra_name])\n        out_file = \"%s-allmerged%s\" % os.path.splitext(in_files[0])\n        if in_files[0].endswith(\".bam\"):\n            merged_file = merge.merge_bam_files(in_files, os.path.dirname(out_file), items[0],\n                                                out_file=out_file)\n        else:\n            assert extra_name == \"summary\", extra_name\n            merged_file = _merge_summary(in_files, out_file, items[0])\n        final[extra_name] = merged_file\n    out = []\n    for data in items:\n        data[\"disambiguate\"] = final\n        out.append([data])\n    return out", "response": "Merge extra disambiguated reads into a final BAM file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _merge_summary(in_files, out_file, data):\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            with open(tx_out_file, \"w\") as out_handle:\n                for i, in_file in enumerate(in_files):\n                    with open(in_file) as in_handle:\n                        for j, line in enumerate(in_handle):\n                            if j == 0:\n                                if i == 0:\n                                    out_handle.write(line)\n                            else:\n                                out_handle.write(line)\n    return out_file", "response": "Create one big summary file for disambiguation from multiple splits."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(items, config):\n    assert len(items) == 2, \"Can only resolve two organism disambiguation\"\n    # check aligner, handling tophat/tophat2 distinctions\n    aligner = config[\"algorithm\"].get(\"aligner\")\n    aligner = \"tophat\" if aligner.startswith(\"tophat\") else aligner\n    assert aligner in [\"bwa\", \"hisat2\", \"tophat\", \"star\"], \"Disambiguation only supported for bwa, hisat2, star and tophat alignments.\"\n    if items[0][\"disambiguate\"].get(\"base\"):\n        data_a, data_b = items\n    else:\n        data_b, data_a = items\n    work_bam_a = bam.sort(data_a[\"work_bam\"], config, \"queryname\")\n    work_bam_b = bam.sort(data_b[\"work_bam\"], config, \"queryname\")\n    if data_a.get(\"align_split\"):\n        base_dir = utils.safe_makedir(os.path.normpath(os.path.join(os.path.dirname(work_bam_a),\n                                                                    os.pardir, os.pardir,\n                                                                    \"disambiguate_%s\" % aligner)))\n        out_dir = os.path.join(base_dir, \"_\".join([str(x) for x in data_a[\"align_split\"].split(\"-\")]))\n    else:\n        out_dir = os.path.normpath(os.path.join(os.path.dirname(work_bam_a),\n                                                os.pardir, \"disambiguate_%s\" % aligner))\n    base_name = os.path.join(out_dir, os.path.splitext(os.path.basename(work_bam_a))[0])\n    summary_file = \"%s_summary.txt\" % base_name\n    if not utils.file_exists(summary_file):\n        with file_transaction(items[0], out_dir) as tx_out_dir:\n            _run_cplusplus(work_bam_a, work_bam_b, tx_out_dir, aligner, os.path.basename(base_name), items)\n    data_a[\"disambiguate\"] = \\\n      {data_b[\"genome_build\"]: bam.sort(\"%s.disambiguatedSpeciesB.bam\" % base_name, config),\n       \"%s-ambiguous\" % data_a[\"genome_build\"]: bam.sort(\"%s.ambiguousSpeciesA.bam\" % base_name, config),\n       \"%s-ambiguous\" % data_b[\"genome_build\"]: bam.sort(\"%s.ambiguousSpeciesB.bam\" % base_name, config),\n       \"summary\": summary_file}\n    data_a[\"work_bam\"] = bam.sort(\"%s.disambiguatedSpeciesA.bam\" % base_name, config)\n    bam.index(dd.get_work_bam(data_a), data_a[\"config\"])\n    return [[data_a]]", "response": "Run third party disambiguation script resolving into single set of calls."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _run_python(work_bam_a, work_bam_b, out_dir, aligner, prefix, items):\n    Args = collections.namedtuple(\"Args\", \"A B output_dir intermediate_dir \"\n                                    \"no_sort prefix aligner\")\n    args = Args(work_bam_a, work_bam_b, out_dir, out_dir, True, \"\", aligner)\n    disambiguate_main(args)", "response": "Run python version of disambiguation\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning third party disambiguation script resolving into single set of calls.", "response": "def _run_cplusplus(work_bam_a, work_bam_b, out_dir, aligner, prefix, items):\n    \"\"\"Run third party disambiguation script, resolving into single set of calls.\n    \"\"\"\n    ngs_disambiguate = config_utils.get_program(\"ngs_disambiguate\", items[0][\"config\"])\n    cmd = [ngs_disambiguate, \"--no-sort\", \"--prefix\", prefix, \"--aligner\", aligner,\n           \"--output-dir\", out_dir, work_bam_a, work_bam_b]\n    do.run(cmd, \"Disambiguation\", items[0])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(args):\n    dirs, config, run_info_yaml = run_info.prep_system(args.sample_config, args.systemconfig)\n    integrations = args.integrations if hasattr(args, \"integrations\") else {}\n    world = run_info.organize(dirs, config, run_info_yaml, is_cwl=True, integrations=integrations)\n    create.from_world(world, run_info_yaml, integrations=integrations, add_container_tag=args.add_container_tag)", "response": "Run a CWL preparation pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef assign_interval(data):\n    if not dd.get_coverage_interval(data):\n        vrs = dd.get_variant_regions_merged(data)\n        callable_file = dd.get_sample_callable(data)\n        if vrs:\n            callable_size = pybedtools.BedTool(vrs).total_coverage()\n        else:\n            callable_size = pybedtools.BedTool(callable_file).total_coverage()\n        total_size = sum([c.size for c in ref.file_contigs(dd.get_ref_file(data), data[\"config\"])])\n        genome_cov_pct = callable_size / float(total_size)\n        if genome_cov_pct > GENOME_COV_THRESH:\n            cov_interval = \"genome\"\n            offtarget_pct = 0.0\n        elif not vrs:\n            cov_interval = \"regional\"\n            offtarget_pct = 0.0\n        else:\n            offtarget_pct = _count_offtarget(data, dd.get_align_bam(data) or dd.get_work_bam(data),\n                                             vrs or callable_file, \"variant_regions\")\n            if offtarget_pct > OFFTARGET_THRESH:\n                cov_interval = \"regional\"\n            else:\n                cov_interval = \"amplicon\"\n        logger.info(\"%s: Assigned coverage as '%s' with %.1f%% genome coverage and %.1f%% offtarget coverage\"\n                    % (dd.get_sample_name(data), cov_interval, genome_cov_pct * 100.0, offtarget_pct * 100.0))\n        data[\"config\"][\"algorithm\"][\"coverage_interval\"] = cov_interval\n    return data", "response": "Assign coverage interval based on percent of genome covered and relation to targets."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calculate(bam_file, data, sv_bed):\n    params = {\"min\": dd.get_coverage_depth_min(data)}\n    variant_regions = dd.get_variant_regions_merged(data)\n    if not variant_regions:\n        variant_regions = _create_genome_regions(data)\n    # Back compatible with previous pre-mosdepth callable files\n    callable_file = os.path.join(utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"align\",\n                                                                 dd.get_sample_name(data))),\n                                 \"%s-coverage.callable.bed\" % (dd.get_sample_name(data)))\n    if not utils.file_uptodate(callable_file, bam_file):\n        vr_quantize = (\"0:1:%s:\" % (params[\"min\"]), [\"NO_COVERAGE\", \"LOW_COVERAGE\", \"CALLABLE\"])\n        to_calculate = [(\"variant_regions\", variant_regions,\n                         vr_quantize, None, \"coverage_perbase\" in dd.get_tools_on(data)),\n                        (\"sv_regions\", bedutils.clean_file(sv_bed, data, prefix=\"svregions-\"),\n                         None, None, False),\n                        (\"coverage\", bedutils.clean_file(dd.get_coverage(data), data, prefix=\"cov-\"),\n                         None, DEPTH_THRESHOLDS, False)]\n        depth_files = {}\n        for target_name, region_bed, quantize, thresholds, per_base in to_calculate:\n            if region_bed:\n                cur_depth = {}\n                depth_info = run_mosdepth(data, target_name, region_bed, quantize=quantize, thresholds=thresholds,\n                                          per_base=per_base)\n                for attr in (\"dist\", \"regions\", \"thresholds\", \"per_base\"):\n                    val = getattr(depth_info, attr, None)\n                    if val:\n                        cur_depth[attr] = val\n                depth_files[target_name] = cur_depth\n                if target_name == \"variant_regions\":\n                    callable_file = depth_info.quantize\n    else:\n        depth_files = {}\n    final_callable = _subset_to_variant_regions(callable_file, variant_regions, data)\n    return final_callable, depth_files", "response": "Calculate coverage in parallel using mosdepth."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate whole genome contigs we want to process only non - alt contigs.", "response": "def _create_genome_regions(data):\n    \"\"\"Create whole genome contigs we want to process, only non-alts.\n\n    Skips problem contigs like HLAs for downstream analysis.\n    \"\"\"\n    work_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"coverage\", dd.get_sample_name(data)))\n    variant_regions = os.path.join(work_dir, \"target-genome.bed\")\n    with file_transaction(data, variant_regions) as tx_variant_regions:\n        with open(tx_variant_regions, \"w\") as out_handle:\n            for c in shared.get_noalt_contigs(data):\n                out_handle.write(\"%s\\t%s\\t%s\\n\" % (c.name, 0, c.size))\n    return variant_regions"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef regions_coverage(bed_file, target_name, data):\n    ready_bed = tz.get_in([\"depth\", target_name, \"regions\"], data)\n    if ready_bed:\n        return ready_bed\n    else:\n        return run_mosdepth(data, target_name, bed_file).regions", "response": "Generate coverage over regions of interest using mosdepth."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_mosdepth(data, target_name, bed_file, per_base=False, quantize=None, thresholds=None):\n    MosdepthCov = collections.namedtuple(\"MosdepthCov\", (\"dist\", \"per_base\", \"regions\", \"quantize\", \"thresholds\"))\n    bam_file = dd.get_align_bam(data) or dd.get_work_bam(data)\n    work_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"coverage\", dd.get_sample_name(data)))\n    prefix = os.path.join(work_dir, \"%s-%s\" % (dd.get_sample_name(data), target_name))\n    old_dist_file = \"%s.mosdepth.dist.txt\" % (prefix)\n    out = MosdepthCov((old_dist_file if utils.file_uptodate(old_dist_file, bam_file) else\n                       \"%s.mosdepth.%s.dist.txt\" % (prefix, \"region\" if bed_file else \"global\")),\n                      (\"%s.per-base.bed.gz\" % prefix) if per_base else None,\n                      (\"%s.regions.bed.gz\" % prefix) if bed_file else None,\n                      (\"%s.quantized.bed.gz\" % prefix) if quantize else None,\n                      (\"%s.thresholds.bed.gz\" % prefix) if thresholds else None)\n    if not utils.file_uptodate(out.dist, bam_file):\n        with file_transaction(data, out.dist) as tx_out_file:\n            tx_prefix = os.path.join(os.path.dirname(tx_out_file), os.path.basename(prefix))\n            num_cores = dd.get_cores(data)\n            bed_arg = (\"--by %s\" % bed_file) if bed_file else \"\"\n            perbase_arg = \"\" if per_base else \"--no-per-base\"\n            mapq_arg = \"-Q 1\" if (per_base or quantize) else \"\"\n            if quantize:\n                quant_arg = \"--quantize %s\" % quantize[0]\n                quant_export = \" && \".join([\"export MOSDEPTH_Q%s=%s\" % (i, x) for (i, x) in enumerate(quantize[1])])\n                quant_export += \" && \"\n            else:\n                quant_arg, quant_export = \"\", \"\"\n\n            thresholds_cmdl = (\"-T \" + \",\".join([str(t) for t in thresholds])) if out.thresholds else \"\"\n            cmd = (\"{quant_export}mosdepth -t {num_cores} -F 1804 {mapq_arg} {perbase_arg} {bed_arg} {quant_arg} \"\n                   \"{tx_prefix} {bam_file} {thresholds_cmdl}\")\n            message = \"Calculating coverage: %s %s\" % (dd.get_sample_name(data), target_name)\n            do.run(cmd.format(**locals()), message.format(**locals()))\n            if out.per_base:\n                shutil.move(os.path.join(os.path.dirname(tx_out_file), os.path.basename(out.per_base)), out.per_base)\n            if out.regions:\n                shutil.move(os.path.join(os.path.dirname(tx_out_file), os.path.basename(out.regions)), out.regions)\n            if out.quantize:\n                shutil.move(os.path.join(os.path.dirname(tx_out_file), os.path.basename(out.quantize)), out.quantize)\n            if out.thresholds:\n                shutil.move(os.path.join(os.path.dirname(tx_out_file), os.path.basename(out.thresholds)), out.thresholds)\n    return out", "response": "Run mosdepth generating distribution region depth and per - base depth."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate coverage at different completeness cutoff for region in coverage option.", "response": "def coverage_region_detailed_stats(target_name, bed_file, data, out_dir):\n    \"\"\"\n    Calculate coverage at different completeness cutoff\n    for region in coverage option.\n    \"\"\"\n    if bed_file and utils.file_exists(bed_file):\n        ready_depth = tz.get_in([\"depth\", target_name], data)\n        if ready_depth:\n            cov_file = ready_depth[\"regions\"]\n            dist_file = ready_depth[\"dist\"]\n            thresholds_file = ready_depth.get(\"thresholds\")\n            out_cov_file = os.path.join(out_dir, os.path.basename(cov_file))\n            out_dist_file = os.path.join(out_dir, os.path.basename(dist_file))\n            out_thresholds_file = os.path.join(out_dir, os.path.basename(thresholds_file)) \\\n                if thresholds_file and os.path.isfile(thresholds_file) else None\n            if not utils.file_uptodate(out_cov_file, cov_file):\n                utils.copy_plus(cov_file, out_cov_file)\n                utils.copy_plus(dist_file, out_dist_file)\n                utils.copy_plus(thresholds_file, out_thresholds_file) if out_thresholds_file else None\n            return [out_cov_file, out_dist_file] + ([out_thresholds_file] if out_thresholds_file else [])\n    return []"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_coords(data):\n    for category, vtypes in [(\"LOH\", {\"LOSS\", \"HETEROZYGOSITY\"}),\n                             (\"amplification\", {\"AMPLIFICATION\"})]:\n        out = tz.get_in([category, dd.get_genome_build(data)], _COORDS, {})\n        priority_file = dd.get_svprioritize(data)\n        if priority_file:\n            if os.path.basename(priority_file).find(\"civic\") >= 0:\n                for chrom, start, end, gene in _civic_regions(priority_file, vtypes, dd.get_disease(data)):\n                    out[gene] = (chrom, start, end)\n            elif os.path.basename(priority_file).find(\".bed\") >= 0:\n                for line in utils.open_gzipsafe(priority_file):\n                    parts = line.strip().split(\"\\t\")\n                    if len(parts) >= 4:\n                        chrom, start, end, gene = parts[:4]\n                        out[gene] = (chrom, int(start), int(end))\n        yield category, out", "response": "Retrieve coordinates of genes of interest for prioritization."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves gene regions and names filtered by variant_types and diseases.", "response": "def _civic_regions(civic_file, variant_types=None, diseases=None, drugs=None):\n    \"\"\"Retrieve gene regions and names filtered by variant_types and diseases.\n    \"\"\"\n    if isinstance(diseases, six.string_types):\n        diseases = [diseases]\n    with utils.open_gzipsafe(civic_file) as in_handle:\n        reader = csv.reader(in_handle, delimiter=\"\\t\")\n        for chrom, start, end, info_str in reader:\n            info = edn_loads(info_str)\n            if not variant_types or _matches(info[\"support\"][\"variants\"], variant_types):\n                if not diseases or _matches(info[\"support\"][\"diseases\"], diseases):\n                    if not drugs or _matches(info[\"support\"][\"drugs\"], drugs):\n                        yield (chrom, int(start), int(end), list(info[\"name\"])[0])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves status in regions of interest along with heterogeneity metrics.", "response": "def summary_status(call, data):\n    \"\"\"Retrieve status in regions of interest, along with heterogeneity metrics.\n\n    Provides output with overall purity and ploidy, along with region\n    specific calls.\n    \"\"\"\n    out_file = None\n    if call.get(\"vrn_file\") and os.path.exists(call.get(\"vrn_file\")):\n        out_file = os.path.join(os.path.dirname(call[\"vrn_file\"]),\n                                \"%s-%s-lohsummary.yaml\" % (dd.get_sample_name(data), call[\"variantcaller\"]))\n        if not utils.file_uptodate(out_file, call[\"vrn_file\"]):\n            out = {}\n            if call[\"variantcaller\"] == \"titancna\":\n                out.update(_titancna_summary(call, data))\n                pass\n            elif call[\"variantcaller\"] == \"purecn\":\n                out.update(_purecn_summary(call, data))\n            if out:\n                out[\"description\"] = dd.get_sample_name(data)\n                out[\"variantcaller\"] = call[\"variantcaller\"]\n                with file_transaction(data, out_file) as tx_out_file:\n                    with open(tx_out_file, \"w\") as out_handle:\n                        yaml.safe_dump(out, out_handle, default_flow_style=False, allow_unicode=False)\n    return out_file if out_file and os.path.exists(out_file) else None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if copy number changes match the expected svtype.", "response": "def _check_copy_number_changes(svtype, cn, minor_cn, data):\n    \"\"\"Check if copy number changes match the expected svtype.\n    \"\"\"\n    if svtype == \"LOH\" and minor_cn == 0:\n        return svtype\n    elif svtype == \"amplification\" and cn > dd.get_ploidy(data):\n        return svtype\n    else:\n        return \"std\""}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsummarize purity ploidy and LOH for TitanCNA.", "response": "def _titancna_summary(call, data):\n    \"\"\"Summarize purity, ploidy and LOH for TitanCNA.\n    \"\"\"\n    out = {}\n    for svtype, coords in get_coords(data):\n        cur_calls = {k: collections.defaultdict(int) for k in coords.keys()}\n        with open(call[\"subclones\"]) as in_handle:\n            header = in_handle.readline().strip().split()\n            for line in in_handle:\n                val = dict(zip(header, line.strip().split()))\n                start = int(val[\"Start_Position.bp.\"])\n                end = int(val[\"End_Position.bp.\"])\n                for region, cur_coords in coords.items():\n                    if val[\"Chromosome\"] == cur_coords[0] and are_overlapping((start, end), cur_coords[1:]):\n                        cur_calls[region][_check_copy_number_changes(svtype, _to_cn(val[\"Copy_Number\"]),\n                                                                     _to_cn(val[\"MinorCN\"]), data)] += 1\n        out[svtype] = {r: _merge_cn_calls(c, svtype) for r, c in cur_calls.items()}\n\n    with open(call[\"hetsummary\"]) as in_handle:\n        vals = dict(zip(in_handle.readline().strip().split(\"\\t\"), in_handle.readline().strip().split(\"\\t\")))\n    out[\"purity\"] = vals[\"purity\"]\n    out[\"ploidy\"] = vals[\"ploidy\"]\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _purecn_summary(call, data):\n    out = {}\n    for svtype, coords in get_coords(data):\n        cur_calls = {k: collections.defaultdict(int) for k in coords.keys()}\n        with open(call[\"loh\"]) as in_handle:\n            in_handle.readline()  # header\n            for line in in_handle:\n                _, chrom, start, end, _, cn, minor_cn = line.split(\",\")[:7]\n                start = int(start)\n                end = int(end)\n                for region, cur_coords in coords.items():\n                    if chrom == cur_coords[0] and are_overlapping((start, end), cur_coords[1:]):\n                        cur_calls[region][_check_copy_number_changes(svtype, _to_cn(cn), _to_cn(minor_cn), data)] += 1\n        out[svtype] = {r: _merge_cn_calls(c, svtype) for r, c in cur_calls.items()}\n    with open(call[\"hetsummary\"]) as in_handle:\n        vals = dict(zip(in_handle.readline().strip().replace('\"', '').split(\",\"),\n                        in_handle.readline().strip().split(\",\")))\n    out[\"purity\"] = vals[\"Purity\"]\n    out[\"ploidy\"] = vals[\"Ploidy\"]\n    return out", "response": "Summarize purity ploidy and LOH for PureCN."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a tuple of type information about the current locale and the type name of the current locale", "response": "def __get_type_from_char(self, c):\n        \"\"\"return a tuple of type information\n        * type name\n        * a flag to indicate if it's a collection\n        \"\"\"\n        if c.isdigit() or c =='-':\n            return (\"number\", False, None)\n        elif c == 't' or c == 'f': ## true/false\n            return (\"boolean\", False, None)\n        elif c == 'n': ## nil\n            return (\"nil\", False, None)\n        elif c == '\\\\' :\n            return (\"char\", False, None)\n        elif c == ':':\n            return (\"keyword\", False, None)\n        elif c == '\"':\n            return (\"string\", False, None)\n        elif c == '#':\n            if self.__read_and_back(1) == '{':\n                return (\"set\", True, \"}\")\n            if self.__read_and_back(1) == ':':\n                return (\"namespaced_dict\", True, \"}\")\n            if self.__read_and_back(4) == 'inst':\n                return (\"datetime\", False, None)\n            if self.__read_and_back(4) == 'uuid':\n                return (\"uuid\", False, None)\n        elif c == '{':\n            return (\"dict\", True, \"}\")\n        elif c == '(':\n            return (\"list\", True, \")\")\n        elif c == '[':\n            return ('list', True, \"]\")\n\n        return (None, False, None)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfixes problem alleles and reference variant bases in VCF line.", "response": "def fix_line_problems(parts):\n    \"\"\"Fix problem alleles and reference/variant bases in VCF line.\n    \"\"\"\n    varinfo = parts[:9]\n    genotypes = []\n    # replace haploid calls\n    for x in parts[9:]:\n        if len(x) == 1:\n            x = \"./.\"\n        genotypes.append(x)\n    if varinfo[3] == \"0\": varinfo[3] = \"N\"\n    if varinfo[4] == \"0\": varinfo[4] = \"N\"\n    return varinfo, genotypes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfix the VCF line in the VCF file.", "response": "def fix_vcf_line(parts, ref_base):\n    \"\"\"Orient VCF allele calls with respect to reference base.\n\n    Handles cases with ref and variant swaps. strand complements.\n    \"\"\"\n    swap = {\"1/1\": \"0/0\", \"0/1\": \"0/1\", \"0/0\": \"1/1\", \"./.\": \"./.\"}\n    complements = {\"G\": \"C\", \"A\": \"T\", \"C\": \"G\", \"T\": \"A\", \"N\": \"N\"}\n    varinfo, genotypes = fix_line_problems(parts)\n    ref, var = varinfo[3:5]\n    # non-reference regions or non-informative, can't do anything\n    if ref_base in [None, \"N\"] or set(genotypes) == set([\"./.\"]):\n        varinfo = None\n    # matching reference, all good\n    elif ref_base == ref:\n        assert ref_base == ref, (ref_base, parts)\n    # swapped reference and alternate regions\n    elif ref_base == var or ref in [\"N\", \"0\"]:\n        varinfo[3] = var\n        varinfo[4] = ref\n        genotypes = [swap[x] for x in genotypes]\n    # reference is on alternate strand\n    elif ref_base != ref and complements.get(ref) == ref_base:\n        varinfo[3] = complements[ref]\n        varinfo[4] = \",\".join([complements[v] for v in var.split(\",\")])\n    # unspecified alternative base\n    elif ref_base != ref and var in [\"N\", \"0\"]:\n        varinfo[3] = ref_base\n        varinfo[4] = ref\n        genotypes = [swap[x] for x in genotypes]\n    # swapped and on alternate strand\n    elif ref_base != ref and complements.get(var) == ref_base:\n        varinfo[3] = complements[var]\n        varinfo[4] = \",\".join([complements[v] for v in ref.split(\",\")])\n        genotypes = [swap[x] for x in genotypes]\n    else:\n        print \"Did not associate ref {0} with line: {1}\".format(\n            ref_base, varinfo)\n    if varinfo is not None:\n        return varinfo + genotypes"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfixes Genotyping VCF positions where the bases are all variants.", "response": "def fix_nonref_positions(in_file, ref_file):\n    \"\"\"Fix Genotyping VCF positions where the bases are all variants.\n\n    The plink/pseq output does not handle these correctly, and\n    has all reference/variant bases reversed.\n    \"\"\"\n    ignore_chrs = [\".\"]\n    ref2bit = twobit.TwoBitFile(open(ref_file))\n    out_file = in_file.replace(\"-raw.vcf\", \".vcf\")\n\n    with open(in_file) as in_handle:\n        with open(out_file, \"w\") as out_handle:\n            for line in in_handle:\n                if line.startswith(\"#\"):\n                    out_handle.write(line)\n                else:\n                    parts = line.rstrip(\"\\r\\n\").split(\"\\t\")\n                    pos = int(parts[1])\n                    # handle chr/non-chr naming\n                    if parts[0] not in ref2bit.keys() and parts[0].replace(\"chr\", \"\") in ref2bit.keys():\n                        parts[0] = parts[0].replace(\"chr\", \"\")\n                    # handle X chromosome\n                    elif parts[0] not in ref2bit.keys() and parts[0] == \"23\":\n                        for test in [\"X\", \"chrX\"]:\n                            if test in ref2bit.keys():\n                                parts[0] == test\n                    ref_base = None\n                    if parts[0] not in ignore_chrs:\n                        try:\n                            ref_base = ref2bit[parts[0]].get(pos-1, pos).upper()\n                        except Exception as msg:\n                            print \"Skipping line. Failed to retrieve reference base for %s\\n%s\" % (str(parts), msg)\n                    parts = fix_vcf_line(parts, ref_base)\n                    if parts is not None:\n                        out_handle.write(\"\\t\".join(parts) + \"\\n\")\n        return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(items, background=None):\n    if not background: background = []\n    names = [tz.get_in([\"rgnames\", \"sample\"], x) for x in items + background]\n    work_bams = [x[\"align_bam\"] for x in items + background]\n    if len(items + background) < 2:\n        raise ValueError(\"cn.mops only works on batches with multiple samples\")\n    data = items[0]\n    work_dir = utils.safe_makedir(os.path.join(data[\"dirs\"][\"work\"], \"structural\", names[0],\n                                               \"cn_mops\"))\n    parallel = {\"type\": \"local\", \"cores\": data[\"config\"][\"algorithm\"].get(\"num_cores\", 1),\n                \"progs\": [\"delly\"]}\n    with pysam.Samfile(work_bams[0], \"rb\") as pysam_work_bam:\n        chroms = [None] if _get_regional_bed_file(items[0]) else pysam_work_bam.references\n        out_files = run_multicore(_run_on_chrom, [(chrom, work_bams, names, work_dir, items)\n                                                  for chrom in chroms],\n                                  data[\"config\"], parallel)\n    out_file = _combine_out_files(out_files, work_dir, data)\n    out = []\n    for data in items:\n        if \"sv\" not in data:\n            data[\"sv\"] = []\n        data[\"sv\"].append({\"variantcaller\": \"cn_mops\",\n                           \"vrn_file\": _prep_sample_cnvs(out_file, data)})\n        out.append(data)\n    return out", "response": "Detect copy number variations from batched set of samples using cn. mops.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _combine_out_files(chr_files, work_dir, data):\n    out_file = \"%s.bed\" % sshared.outname_from_inputs(chr_files)\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            with open(tx_out_file, \"w\") as out_handle:\n                for chr_file in chr_files:\n                    with open(chr_file) as in_handle:\n                        is_empty = in_handle.readline().startswith(\"track name=empty\")\n                    if not is_empty:\n                        with open(chr_file) as in_handle:\n                            shutil.copyfileobj(in_handle, out_handle)\n    return out_file", "response": "Combine all CNV calls into a single BED file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _prep_sample_cnvs(cnv_file, data):\n    import pybedtools\n    sample_name = tz.get_in([\"rgnames\", \"sample\"], data)\n    def make_names(name):\n        return re.sub(\"[^\\w.]\", '.', name)\n    def matches_sample_name(feat):\n        return (feat.name == sample_name or feat.name == \"X%s\" % sample_name or\n                feat.name == make_names(sample_name))\n    def update_sample_name(feat):\n        feat.name = sample_name\n        return feat\n    sample_file = os.path.join(os.path.dirname(cnv_file), \"%s-cnv.bed\" % sample_name)\n    if not utils.file_exists(sample_file):\n        with file_transaction(data, sample_file) as tx_out_file:\n            with shared.bedtools_tmpdir(data):\n                pybedtools.BedTool(cnv_file).filter(matches_sample_name).each(update_sample_name).saveas(tx_out_file)\n    return sample_file", "response": "Convert a multiple sample CNV file into a single BED file for a sample."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _run_on_chrom(chrom, work_bams, names, work_dir, items):\n    local_sitelib = utils.R_sitelib()\n    batch = sshared.get_cur_batch(items)\n    ext = \"-%s-cnv\" % batch if batch else \"-cnv\"\n    out_file = os.path.join(work_dir, \"%s%s-%s.bed\" % (os.path.splitext(os.path.basename(work_bams[0]))[0],\n                                                       ext, chrom if chrom else \"all\"))\n    if not utils.file_exists(out_file):\n        with file_transaction(items[0], out_file) as tx_out_file:\n            rcode = \"%s-run.R\" % os.path.splitext(out_file)[0]\n            with open(rcode, \"w\") as out_handle:\n                out_handle.write(_script.format(prep_str=_prep_load_script(work_bams, names, chrom, items),\n                                                out_file=tx_out_file,\n                                                local_sitelib=local_sitelib))\n            rscript = utils.Rscript_cmd()\n            try:\n                do.run([rscript, \"--no-environ\", rcode], \"cn.mops CNV detection\", items[0], log_error=False)\n            except subprocess.CalledProcessError as msg:\n                # cn.mops errors out if no CNVs found. Just write an empty file.\n                if _allowed_cnmops_errorstates(str(msg)):\n                    with open(tx_out_file, \"w\") as out_handle:\n                        out_handle.write('track name=empty description=\"No CNVs found\"\\n')\n                else:\n                    logger.exception()\n                    raise\n    return [out_file]", "response": "Run cn. mops on a specific chromosome."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_regional_bed_file(data):\n    variant_regions = bedutils.merge_overlaps(tz.get_in([\"config\", \"algorithm\", \"variant_regions\"], data),\n                                              data)\n    is_genome = data[\"config\"][\"algorithm\"].get(\"coverage_interval\", \"exome\").lower() in [\"genome\"]\n    if variant_regions and utils.file_exists(variant_regions) and not is_genome:\n        return variant_regions", "response": "Get regional file for analysis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _population_load_script(work_bams, names, chrom, pairmode, items):\n    bed_file = _get_regional_bed_file(items[0])\n    if bed_file:\n        return _population_prep_targeted.format(bam_file_str=\",\".join(work_bams), names_str=\",\".join(names),\n                                                chrom=chrom, num_cores=0, pairmode=pairmode, bed_file=bed_file)\n    else:\n        return _population_prep.format(bam_file_str=\",\".join(work_bams), names_str=\",\".join(names),\n                                       chrom=chrom, num_cores=0, pairmode=pairmode)", "response": "Prepare BAMs for assessing CNVs in a population."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _paired_load_script(work_bams, names, chrom, pairmode, items):\n    paired = vcfutils.get_paired_bams(work_bams, items)\n    bed_file = _get_regional_bed_file(items[0])\n    if bed_file:\n        return _paired_prep_targeted.format(case_file=paired.tumor_bam, case_name=paired.tumor_name,\n                                            ctrl_file=paired.normal_bam, ctrl_name=paired.normal_name,\n                                            num_cores=0, chrom=chrom, pairmode=pairmode, bed_file=bed_file)\n    else:\n        return _paired_prep.format(case_file=paired.tumor_bam, case_name=paired.tumor_name,\n                                   ctrl_file=paired.normal_bam, ctrl_name=paired.normal_name,\n                                   num_cores=0, chrom=chrom, pairmode=pairmode)", "response": "Prepare BAMs for assessing CNVs in a paired tumor setup."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tobam_cl(data, out_file, is_paired=False):\n    do_dedup = _check_dedup(data)\n    umi_consensus = dd.get_umi_consensus(data)\n    with file_transaction(data, out_file) as tx_out_file:\n        if not do_dedup:\n            yield (sam_to_sortbam_cl(data, tx_out_file), tx_out_file)\n        elif umi_consensus:\n            yield (_sam_to_grouped_umi_cl(data, umi_consensus, tx_out_file), tx_out_file)\n        elif is_paired and _need_sr_disc_reads(data) and not _too_many_contigs(dd.get_ref_file(data)):\n            sr_file = \"%s-sr.bam\" % os.path.splitext(out_file)[0]\n            disc_file = \"%s-disc.bam\" % os.path.splitext(out_file)[0]\n            with file_transaction(data, sr_file) as tx_sr_file:\n                with file_transaction(data, disc_file) as tx_disc_file:\n                    yield (samblaster_dedup_sort(data, tx_out_file, tx_sr_file, tx_disc_file),\n                           tx_out_file)\n        else:\n            yield (_biobambam_dedup_sort(data, tx_out_file), tx_out_file)", "response": "Prepare command line for producing de - duplicated sorted output."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_cores_memory(data, downscale=2):\n    resources = config_utils.get_resources(\"samtools\", data[\"config\"])\n    num_cores = data[\"config\"][\"algorithm\"].get(\"num_cores\", 1)\n    max_mem = config_utils.adjust_memory(resources.get(\"memory\", \"2G\"),\n                                         downscale, \"decrease\").upper()\n    return num_cores, max_mem", "response": "Retrieve cores and memory using samtools as baseline."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting to sorted BAM output.", "response": "def sam_to_sortbam_cl(data, tx_out_file, name_sort=False):\n    \"\"\"Convert to sorted BAM output.\n\n    Set name_sort to True to sort reads by queryname\n    \"\"\"\n    samtools = config_utils.get_program(\"samtools\", data[\"config\"])\n    cores, mem = _get_cores_memory(data, downscale=2)\n    tmp_file = \"%s-sorttmp\" % utils.splitext_plus(tx_out_file)[0]\n    sort_flag = \"-n\" if name_sort else \"\"\n    return (\"{samtools} sort -@ {cores} -m {mem} {sort_flag} \"\n            \"-T {tmp_file} -o {tx_out_file} /dev/stdin\".format(**locals()))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform streaming deduplication and sorting with biobambam s bamsormadup", "response": "def _biobambam_dedup_sort(data, tx_out_file):\n    \"\"\"Perform streaming deduplication and sorting with biobambam's bamsormadup\n    \"\"\"\n    samtools = config_utils.get_program(\"samtools\", data[\"config\"])\n    cores, mem = _get_cores_memory(data, downscale=2)\n    tmp_file = \"%s-sorttmp\" % utils.splitext_plus(tx_out_file)[0]\n    if data.get(\"align_split\"):\n        sort_opt = \"-n\" if data.get(\"align_split\") and _check_dedup(data) else \"\"\n        cmd = \"{samtools} sort %s -@ {cores} -m {mem} -O bam -T {tmp_file}-namesort -o {tx_out_file} -\" % sort_opt\n    else:\n        # scale core usage to avoid memory issues with larger WGS samples\n        cores = max(1, int(math.ceil(cores * 0.75)))\n        ds_cmd = bam.get_maxcov_downsample_cl(data, \"bamsormadup\")\n        bamsormadup = config_utils.get_program(\"bamsormadup\", data)\n        cmd = (\"{bamsormadup} inputformat=sam threads={cores} tmpfile={tmp_file}-markdup \"\n               \"SO=coordinate %s > {tx_out_file}\" % ds_cmd)\n    return cmd.format(**locals())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _sam_to_grouped_umi_cl(data, umi_consensus, tx_out_file):\n    tmp_file = \"%s-sorttmp\" % utils.splitext_plus(tx_out_file)[0]\n    jvm_opts = _get_fgbio_jvm_opts(data, os.path.dirname(tmp_file), 1)\n    cores, mem = _get_cores_memory(data)\n    bamsormadup = config_utils.get_program(\"bamsormadup\", data)\n    cmd = (\"{bamsormadup} tmpfile={tmp_file}-markdup inputformat=sam threads={cores} outputformat=bam \"\n           \"level=0 SO=coordinate | \")\n    # UMIs in a separate file\n    if os.path.exists(umi_consensus) and os.path.isfile(umi_consensus):\n        cmd += \"fgbio {jvm_opts} AnnotateBamWithUmis -i /dev/stdin -f {umi_consensus} -o {tx_out_file}\"\n    # UMIs embedded in read name\n    else:\n        cmd += (\"%s %s bamtag - | samtools view -b > {tx_out_file}\" %\n                (utils.get_program_python(\"umis\"),\n                 config_utils.get_program(\"umis\", data[\"config\"])))\n    return cmd.format(**locals())", "response": "Mark duplicates on aligner output and convert to grouped UMIs by position."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef umi_consensus(data):\n    align_bam = dd.get_work_bam(data)\n    umi_method, umi_tag = _check_umi_type(align_bam)\n    f1_out = \"%s-cumi-1.fq.gz\" % utils.splitext_plus(align_bam)[0]\n    f2_out = \"%s-cumi-2.fq.gz\" % utils.splitext_plus(align_bam)[0]\n    avg_coverage = coverage.get_average_coverage(\"rawumi\", dd.get_variant_regions(data), data)\n    if not utils.file_uptodate(f1_out, align_bam):\n        with file_transaction(data, f1_out, f2_out) as (tx_f1_out, tx_f2_out):\n            jvm_opts = _get_fgbio_jvm_opts(data, os.path.dirname(tx_f1_out), 2)\n            # Improve speeds by avoiding compression read/write bottlenecks\n            io_opts = \"--async-io=true --compression=0\"\n            est_options = _estimate_fgbio_defaults(avg_coverage)\n            group_opts, cons_opts, filter_opts = _get_fgbio_options(data, est_options, umi_method)\n            cons_method = \"CallDuplexConsensusReads\" if umi_method == \"paired\" else \"CallMolecularConsensusReads\"\n            tempfile = \"%s-bamtofastq-tmp\" % utils.splitext_plus(f1_out)[0]\n            ref_file = dd.get_ref_file(data)\n            cmd = (\"unset JAVA_HOME && \"\n                   \"fgbio {jvm_opts} {io_opts} GroupReadsByUmi {group_opts} -t {umi_tag} -s {umi_method} \"\n                   \"-i {align_bam} | \"\n                   \"fgbio {jvm_opts} {io_opts} {cons_method} {cons_opts} --sort-order=:none: \"\n                   \"-i /dev/stdin -o /dev/stdout | \"\n                   \"fgbio {jvm_opts} {io_opts} FilterConsensusReads {filter_opts} -r {ref_file} \"\n                   \"-i /dev/stdin -o /dev/stdout | \"\n                   \"bamtofastq collate=1 T={tempfile} F={tx_f1_out} F2={tx_f2_out} tags=cD,cM,cE gz=1\")\n            do.run(cmd.format(**locals()), \"UMI consensus fastq generation\")\n    return f1_out, f2_out, avg_coverage", "response": "Convert UMI grouped reads into fastq pair for re - alignment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining the type of UMI from BAM tags standard or paired.", "response": "def _check_umi_type(bam_file):\n    \"\"\"Determine the type of UMI from BAM tags: standard or paired.\n    \"\"\"\n    with pysam.Samfile(bam_file, \"rb\") as in_bam:\n        for read in in_bam:\n            cur_umi = None\n            for tag in [\"RX\", \"XC\"]:\n                try:\n                    cur_umi = read.get_tag(tag)\n                    break\n                except KeyError:\n                    pass\n            if cur_umi:\n                if \"-\" in cur_umi and len(cur_umi.split(\"-\")) == 2:\n                    return \"paired\", tag\n                else:\n                    return \"adjacency\", tag"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets adjustable options through resources or default options for fgbio.", "response": "def _get_fgbio_options(data, estimated_defaults, umi_method):\n    \"\"\"Get adjustable, through resources, or default options for fgbio.\n    \"\"\"\n    group_opts = [\"--edits\", \"--min-map-q\"]\n    cons_opts = [\"--min-input-base-quality\"]\n    if umi_method != \"paired\":\n        cons_opts += [\"--min-reads\", \"--max-reads\"]\n    filter_opts = [\"--min-reads\", \"--min-base-quality\", \"--max-base-error-rate\"]\n    defaults = {\"--min-reads\": \"1\",\n                \"--max-reads\": \"100000\",\n                \"--min-map-q\": \"1\",\n                \"--min-base-quality\": \"13\",\n                \"--max-base-error-rate\": \"0.1\",\n                \"--min-input-base-quality\": \"2\",\n                \"--edits\": \"1\"}\n    defaults.update(estimated_defaults)\n    ropts = config_utils.get_resources(\"fgbio\", data[\"config\"]).get(\"options\", [])\n    assert len(ropts) % 2 == 0, \"Expect even number of options for fgbio\" % ropts\n    ropts = dict(tz.partition(2, ropts))\n    # Back compatibility for older base quality settings\n    if \"--min-consensus-base-quality\" in ropts:\n        ropts[\"--min-base-quality\"] = ropts.pop(\"--min-consensus-base-quality\")\n    defaults.update(ropts)\n    group_out = \" \".join([\"%s=%s\" % (x, defaults[x]) for x in group_opts])\n    cons_out = \" \".join([\"%s=%s\" % (x, defaults[x]) for x in cons_opts])\n    filter_out = \" \".join([\"%s=%s\" % (x, defaults[x]) for x in filter_opts])\n    if umi_method != \"paired\":\n        cons_out += \" --output-per-base-tags=false\"\n    return group_out, cons_out, filter_out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _check_dedup(data):\n    if dd.get_analysis(data).lower() in [\"rna-seq\", \"smallrna-seq\"] or not dd.get_aligner(data):\n        dup_param = utils.get_in(data, (\"config\", \"algorithm\", \"mark_duplicates\"), False)\n    else:\n        dup_param = utils.get_in(data, (\"config\", \"algorithm\", \"mark_duplicates\"), True)\n    if dup_param and isinstance(dup_param, six.string_types):\n        logger.info(\"Warning: bcbio no longer support explicit setting of mark_duplicate algorithm. \"\n                    \"Using best-practice choice based on input data.\")\n        dup_param = True\n    return dup_param", "response": "Check configuration for de - duplication."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dedup_bam(in_bam, data):\n    if _check_dedup(data):\n        out_file = os.path.join(utils.safe_makedir(os.path.join(os.getcwd(), \"align\", dd.get_sample_name(data))),\n                                \"%s-dedup%s\" % utils.splitext_plus(os.path.basename(in_bam)))\n        if not utils.file_exists(out_file):\n            with tx_tmpdir(data) as tmpdir:\n                with file_transaction(data, out_file) as tx_out_file:\n                    bammarkduplicates = config_utils.get_program(\"bammarkduplicates\", data[\"config\"])\n                    base_tmp = os.path.join(tmpdir, os.path.splitext(os.path.basename(tx_out_file))[0])\n                    cores, mem = _get_cores_memory(data, downscale=2)\n                    cmd = (\"{bammarkduplicates} tmpfile={base_tmp}-markdup \"\n                           \"markthreads={cores} I={in_bam} O={tx_out_file}\")\n                    do.run(cmd.format(**locals()), \"De-duplication with biobambam\")\n        bam.index(out_file, data[\"config\"])\n        return out_file\n    else:\n        return in_bam", "response": "Perform non - stream based deduplication of BAM input files using biobambam."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinalizes the output of a single sample from TitanCNA calling optional solution.", "response": "def _finalize_sv(solution_file, data):\n    \"\"\"Add output files from TitanCNA calling optional solution.\n    \"\"\"\n    out = {\"variantcaller\": \"titancna\"}\n    with open(solution_file) as in_handle:\n        solution = dict(zip(in_handle.readline().strip(\"\\r\\n\").split(\"\\t\"),\n                            in_handle.readline().strip(\"\\r\\n\").split(\"\\t\")))\n    if solution.get(\"path\"):\n        out[\"purity\"] = solution[\"purity\"]\n        out[\"ploidy\"] = solution[\"ploidy\"]\n        out[\"cellular_prevalence\"] = [x.strip() for x in solution[\"cellPrev\"].split(\",\")]\n        base = os.path.basename(solution[\"path\"])\n        out[\"plot\"] = dict([(n, solution[\"path\"] + ext) for (n, ext) in [(\"rplots\", \".Rplots.pdf\"),\n                                                                         (\"cf\", \"/%s_CF.pdf\" % base),\n                                                                         (\"cna\", \"/%s_CNA.pdf\" % base),\n                                                                         (\"loh\", \"/%s_LOH.pdf\" % base)]\n                            if os.path.exists(solution[\"path\"] + ext)])\n        out[\"subclones\"] = \"%s.segs.txt\" % solution[\"path\"]\n        out[\"hetsummary\"] = solution_file\n        out[\"vrn_file\"] = to_vcf(out[\"subclones\"], \"TitanCNA\", _get_header, _seg_to_vcf, data)\n        out[\"lohsummary\"] = loh.summary_status(out, data)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if enough input data to proceed with analysis.", "response": "def _should_run(het_file):\n    \"\"\"Check for enough input data to proceed with analysis.\n    \"\"\"\n    has_hets = False\n    with open(het_file) as in_handle:\n        for i, line in enumerate(in_handle):\n            if i > 1:\n                has_hets = True\n                break\n    return has_hets"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nselects optimal cluster from ploidy.", "response": "def _run_select_solution(ploidy_outdirs, work_dir, data):\n    \"\"\"Select optimal\n    \"\"\"\n    out_file = os.path.join(work_dir, \"optimalClusters.txt\")\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            ploidy_inputs = \" \".join([\"--ploidyRun%s=%s\" % (p, d) for p, d in ploidy_outdirs])\n            cmd = \"titanCNA_selectSolution.R {ploidy_inputs} --outFile={tx_out_file}\"\n            do.run(cmd.format(**locals()), \"TitanCNA: select optimal solution\")\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns titanCNA wrapper script on given ploidy and clusters.", "response": "def _run_titancna(cn_file, het_file, ploidy, num_clusters, work_dir, data):\n    \"\"\"Run titanCNA wrapper script on given ploidy and clusters.\n    \"\"\"\n    sample = dd.get_sample_name(data)\n    cores = dd.get_num_cores(data)\n    export_cmd = utils.get_R_exports()\n    ploidy_dir = utils.safe_makedir(os.path.join(work_dir, \"run_ploidy%s\" % ploidy))\n\n    cluster_dir = \"%s_cluster%02d\" % (sample, num_clusters)\n    out_dir = os.path.join(ploidy_dir, cluster_dir)\n    if not utils.file_uptodate(out_dir + \".titan.txt\", cn_file):\n        with tx_tmpdir(data) as tmp_dir:\n            with utils.chdir(tmp_dir):\n                cmd = (\"{export_cmd} && titanCNA.R --id {sample} --hetFile {het_file} --cnFile {cn_file} \"\n                       \"--numClusters {num_clusters} --ploidy {ploidy} --numCores {cores} --outDir {tmp_dir} \"\n                       \"--libdir None\")\n                chroms = [\"'%s'\" % c.name.replace(\"chr\", \"\") for c in ref.file_contigs(dd.get_ref_file(data))\n                          if chromhacks.is_autosomal_or_x(c.name)]\n                if \"'X'\" not in chroms:\n                    chroms += [\"'X'\"]\n                # Use UCSC style naming for human builds to support BSgenome\n                genome_build = (\"hg19\" if dd.get_genome_build(data) in [\"GRCh37\", \"hg19\"]\n                                else dd.get_genome_build(data))\n                cmd += \"\"\" --chrs \"c(%s)\" \"\"\" % \",\".join(chroms)\n                cmd += \" --genomeBuild {genome_build}\"\n                if data[\"genome_build\"] in (\"hg19\", \"hg38\"):\n                    cmd += \" --genomeStyle UCSC\"\n                if data[\"genome_build\"] in [\"hg38\"]:\n                    data_dir = os.path.normpath(os.path.join(\n                        os.path.dirname(os.path.realpath(os.path.join(\n                            os.path.dirname(utils.Rscript_cmd()), \"titanCNA.R\"))),\n                        os.pardir, os.pardir, \"data\"))\n                    cytoband_file = os.path.join(data_dir, \"cytoBand_hg38.txt\")\n                    assert os.path.exists(cytoband_file), cytoband_file\n                    cmd += \" --cytobandFile %s\" % cytoband_file\n                # TitanCNA's model is influenced by the variance in read coverage data\n                # and data type: set reasonable defaults for non-WGS runs\n                # (see https://github.com/gavinha/TitanCNA/tree/master/scripts/R_scripts)\n                if dd.get_coverage_interval(data) != \"genome\":\n                    cmd += \" --alphaK=2500 --alphaKHigh=2500\"\n                do.run(cmd.format(**locals()), \"TitanCNA CNV detection: ploidy %s, cluster %s\" % (ploidy, num_clusters))\n            for fname in glob.glob(os.path.join(tmp_dir, cluster_dir + \"*\")):\n                shutil.move(fname, ploidy_dir)\n            if os.path.exists(os.path.join(tmp_dir, \"Rplots.pdf\")):\n                shutil.move(os.path.join(tmp_dir, \"Rplots.pdf\"),\n                            os.path.join(ploidy_dir, \"%s.Rplots.pdf\" % cluster_dir))\n    return ploidy_dir"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert CNVkit or GATK4 normalized input into TitanCNA ready format.", "response": "def _titan_cn_file(cnr_file, work_dir, data):\n    \"\"\"Convert CNVkit or GATK4 normalized input into TitanCNA ready format.\n    \"\"\"\n    out_file = os.path.join(work_dir, \"%s.cn\" % (utils.splitext_plus(os.path.basename(cnr_file))[0]))\n    support_cols = {\"cnvkit\": [\"chromosome\", \"start\", \"end\", \"log2\"],\n                    \"gatk-cnv\": [\"CONTIG\", \"START\", \"END\", \"LOG2_COPY_RATIO\"]}\n    cols = support_cols[cnvkit.bin_approach(data)]\n    if not utils.file_uptodate(out_file, cnr_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            iterator = pd.read_table(cnr_file, sep=\"\\t\", iterator=True, header=0, comment=\"@\")\n            with open(tx_out_file, \"w\") as handle:\n                for chunk in iterator:\n                    chunk = chunk[cols]\n                    chunk.columns = [\"chrom\", \"start\", \"end\", \"logR\"]\n                    if cnvkit.bin_approach(data) == \"cnvkit\":\n                        chunk['start'] += 1\n                    chunk.to_csv(handle, mode=\"a\", sep=\"\\t\", index=False)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting TitanCNA segs file into bgzipped VCF.", "response": "def to_vcf(in_file, caller, header_fn, vcf_fn, data, sep=\"\\t\"):\n    \"\"\"Convert output TitanCNA segs file into bgzipped VCF.\n    \"\"\"\n    out_file = \"%s.vcf\" % utils.splitext_plus(in_file)[0]\n    if not utils.file_exists(out_file + \".gz\") and not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            with open(in_file) as in_handle:\n                with open(tx_out_file, \"w\") as out_handle:\n                    out_handle.write(_vcf_header.format(caller=caller))\n                    out_handle.write(\"\\t\".join([\"#CHROM\", \"POS\", \"ID\", \"REF\", \"ALT\", \"QUAL\",\n                                                \"FILTER\", \"INFO\", \"FORMAT\", dd.get_sample_name(data)]) + \"\\n\")\n                    header, in_handle = header_fn(in_handle)\n                    for line in in_handle:\n                        out = vcf_fn(dict(zip(header, line.strip().split(sep))))\n                        if out:\n                            out_handle.write(\"\\t\".join(out) + \"\\n\")\n    out_file = vcfutils.bgzip_and_index(out_file, data[\"config\"])\n    effects_vcf, _ = effects.add_to_vcf(out_file, data, \"snpeff\")\n    return effects_vcf or out_file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(items):\n    assert len(items) == 1, \"Expect one input to MetaSV ensemble calling\"\n    data = items[0]\n    work_dir = _sv_workdir(data)\n    out_file = os.path.join(work_dir, \"variants.vcf.gz\")\n    cmd = _get_cmd() + [\"--sample\", dd.get_sample_name(data), \"--reference\", dd.get_ref_file(data),\n                        \"--bam\", dd.get_align_bam(data), \"--outdir\", work_dir]\n    methods = []\n    for call in data.get(\"sv\", []):\n        vcf_file = call.get(\"vcf_file\", call.get(\"vrn_file\", None))\n        if call[\"variantcaller\"] in SUPPORTED and call[\"variantcaller\"] not in methods and vcf_file is not None:\n            methods.append(call[\"variantcaller\"])\n            cmd += [\"--%s_vcf\" % call[\"variantcaller\"], vcf_file]\n    if len(methods) >= MIN_CALLERS:\n        if not utils.file_exists(out_file):\n            tx_work_dir = utils.safe_makedir(os.path.join(work_dir, \"raw\"))\n            ins_stats = shared.calc_paired_insert_stats_save(dd.get_align_bam(data),\n                                                             os.path.join(tx_work_dir, \"insert-stats.yaml\"))\n            cmd += [\"--workdir\", tx_work_dir, \"--num_threads\", str(dd.get_num_cores(data))]\n            cmd += [\"--spades\", utils.which(\"spades.py\"), \"--age\", utils.which(\"age_align\")]\n            cmd += [\"--assembly_max_tools=1\", \"--assembly_pad=500\"]\n            cmd += [\"--boost_sc\", \"--isize_mean\", ins_stats[\"mean\"], \"--isize_sd\", ins_stats[\"std\"]]\n            do.run(cmd, \"Combine variant calls with MetaSV\")\n        filters = (\"(NUM_SVTOOLS = 1 && ABS(SVLEN)>50000) || \"\n                   \"(NUM_SVTOOLS = 1 && ABS(SVLEN)<4000 && BA_FLANK_PERCENT>80) || \"\n                   \"(NUM_SVTOOLS = 1 && ABS(SVLEN)<4000 && BA_NUM_GOOD_REC=0) || \"\n                   \"(ABS(SVLEN)<4000 && BA_NUM_GOOD_REC>2)\")\n        filter_file = vfilter.cutoff_w_expression(out_file, filters,\n                                                  data, name=\"ReassemblyStats\", limit_regions=None)\n        effects_vcf, _ = effects.add_to_vcf(filter_file, data, \"snpeff\")\n        data[\"sv\"].append({\"variantcaller\": \"metasv\",\n                           \"vrn_file\": effects_vcf or filter_file})\n    return [data]", "response": "Run MetaSV on the input set of items."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef combine_count_files(files, out_file=None, ext=\".fpkm\"):\n    files = list(files)\n    if not files:\n        return None\n    assert all([file_exists(x) for x in files]), \\\n        \"Some count files in %s do not exist.\" % files\n    for f in files:\n        assert file_exists(f), \"%s does not exist or is empty.\" % f\n    col_names = [os.path.basename(x.replace(ext, \"\")) for x in files]\n    if not out_file:\n        out_dir = os.path.join(os.path.dirname(files[0]))\n        out_file = os.path.join(out_dir, \"combined.counts\")\n\n    if file_exists(out_file):\n        return out_file\n    logger.info(\"Combining count files into %s.\" % out_file)\n    row_names = []\n    col_vals = defaultdict(list)\n    for i, f in enumerate(files):\n        vals = []\n        if i == 0:\n            with open(f) as in_handle:\n                for line in in_handle:\n                    rname, val = line.strip().split(\"\\t\")\n                    row_names.append(rname)\n                    vals.append(val)\n        else:\n            with open(f) as in_handle:\n                for line in in_handle:\n                    _, val = line.strip().split(\"\\t\")\n                    vals.append(val)\n        col_vals[col_names[i]] = vals\n\n    df = pd.DataFrame(col_vals, index=row_names)\n    df.to_csv(out_file, sep=\"\\t\", index_label=\"id\")\n    return out_file", "response": "Combine a set of count files into a single combined file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a CWL step into a nextflow process.", "response": "def nf_step_to_process(step, out_handle):\n    \"\"\"Convert CWL step into a nextflow process.\n    \"\"\"\n    pprint.pprint(step)\n\n    directives = []\n    for req in step[\"task_definition\"][\"requirements\"]:\n        if req[\"requirement_type\"] == \"docker\":\n            directives.append(\"container '%s'\" % req[\"value\"])\n        elif req[\"requirement_type\"] == \"cpu\":\n            directives.append(\"cpus %s\" % req[\"value\"])\n        elif req[\"requirement_type\"] == \"memory\":\n            directives.append(\"memory '%s'\" % req[\"value\"])\n\n    task_id = step[\"task_id\"]\n    directives = \"\\n  \".join(directives)\n    inputs = \"\\n    \".join(nf_io_to_process(step[\"inputs\"], step[\"task_definition\"][\"inputs\"],\n                                        step[\"scatter\"]))\n    outputs = \"\\n    \".join(nf_io_to_process(step[\"outputs\"], step[\"task_definition\"][\"outputs\"]))\n    commandline = (step[\"task_definition\"][\"baseCommand\"] + \" \" +\n                   \" \".join([nf_input_to_cl(i) for i in step[\"task_definition\"][\"inputs\"]]))\n    out_handle.write(_nf_process_tmpl.format(**locals()))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts CWL input and output into a nextflow process definition.", "response": "def nf_io_to_process(ios, def_ios, scatter_ios=None):\n    \"\"\"Convert CWL input/output into a nextflow process definition.\n\n    Needs to handle scattered/parallel variables.\n    \"\"\"\n    scatter_names = {k: v for k, v in scatter_ios} if scatter_ios else {}\n    var_types = {}\n    for def_io in def_ios:\n        var_types[def_io[\"name\"]] = nf_type(def_io[\"variable_type\"])\n    out = []\n    for io in ios:\n        cur_id = io[\"id\"]\n        if scatter_names:\n            input_id = scatter_names[io[\"value\"]]\n        else:\n            input_id = io.get(\"value\")\n        vtype = var_types[cur_id]\n        if input_id and cur_id != input_id:\n            out.append(\"%s %s as %s\" % (vtype, input_id, cur_id))\n        else:\n            out.append(\"%s %s\" % (vtype, cur_id))\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef nf_input_to_cl(inp):\n    sep = \" \" if inp.get(\"separate\") else \"\"\n    val = \"'%s'\" % inp.get(\"default\") if inp.get(\"default\") else \"$%s\" % inp[\"name\"]\n    return \"%s%s%s\" % (inp[\"prefix\"], sep, val)", "response": "Convert an input description into a command line argument."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a cwl2wdl workflow into cwl2wdl style dictionary.", "response": "def _wf_to_dict(wf):\n    \"\"\"Parse a workflow into cwl2wdl style dictionary.\n    \"\"\"\n    inputs, outputs = _get_wf_inout(wf)\n    out = {\"name\": _id_to_name(wf.tool[\"id\"]).replace(\"-\", \"_\"), \"inputs\": inputs,\n           \"outputs\": outputs, \"steps\": [], \"subworkflows\": [],\n           \"requirements\": []}\n    for step in wf.steps:\n        inputs, outputs = _get_step_inout(step)\n        inputs, scatter = _organize_step_scatter(step, inputs)\n        if isinstance(step.embedded_tool, cwltool.workflow.Workflow):\n            wf_def = _wf_to_dict(step.embedded_tool)\n            out[\"subworkflows\"].append({\"id\": wf_def[\"name\"], \"definition\": wf_def,\n                                        \"inputs\": inputs, \"outputs\": outputs, \"scatter\": scatter})\n        else:\n            task_def = _tool_to_dict(step.embedded_tool)\n            out[\"steps\"].append({\"task_id\": task_def[\"name\"], \"task_definition\": task_def,\n                                 \"inputs\": inputs, \"outputs\": outputs, \"scatter\": scatter})\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_step_inout(step):\n    inputs = []\n    outputs = []\n    assert step.inputs_record_schema[\"type\"] == \"record\"\n    for inp in step.inputs_record_schema[\"fields\"]:\n        source = inp[\"source\"].split(\"#\")[-1].replace(\"/\", \".\")\n        # Check if we're unpacking from a record, and unpack from our object\n        if \"valueFrom\" in inp:\n            attr_access = \"['%s']\" % inp[\"name\"]\n            if inp[\"valueFrom\"].find(attr_access) > 0:\n                source += \".%s\" % inp[\"name\"]\n        inputs.append({\"id\": inp[\"name\"], \"value\": source})\n    assert step.outputs_record_schema[\"type\"] == \"record\"\n    for outp in step.outputs_record_schema[\"fields\"]:\n        outputs.append({\"id\": outp[\"name\"]})\n    return inputs, outputs", "response": "Retrieve set of inputs and outputs connecting steps.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _to_variable_type(x):\n    var_mapping = {\"string\": \"String\", \"File\": \"File\", \"null\": \"String\",\n                   \"long\": \"Float\", \"int\": \"Int\"}\n    if isinstance(x, dict):\n        if x[\"type\"] == \"record\":\n            return \"Object\"\n        else:\n            assert x[\"type\"] == \"array\", x\n            return \"Array[%s]\" % _to_variable_type(x[\"items\"])\n    elif isinstance(x, (list, tuple)):\n        vars = [v for v in x if v != \"null\"]\n        assert len(vars) == 1, vars\n        return var_mapping[vars[0]]\n    else:\n        return var_mapping[x]", "response": "Convert CWL variables to WDL variables handling nested arrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _input_to_dict(i):\n    var_type = _to_variable_type(i[\"type\"])\n    if var_type.startswith(\"Array\") and \"inputBinding\" in i.get(\"type\", {}):\n        ib = i[\"type\"][\"inputBinding\"]\n    elif \"inputBinding\" in i:\n        ib = i[\"inputBinding\"]\n    else:\n        ib = {\"prefix\": None, \"itemSeparator\": None, \"position\": None}\n    return {\"name\": _id_to_localname(i[\"id\"]) if \"id\" in i else i[\"name\"],\n            \"variable_type\": var_type,\n            \"prefix\": ib[\"prefix\"], \"separator\": ib[\"itemSeparator\"],\n            \"position\": ib[\"position\"], \"is_required\": True,\n            \"default\": i.get(\"default\", None), \"separate\": ib.get(\"separate\", True)}", "response": "Convert a cwl2wdl Input object into a dictionary required for a cwl2wdl Input object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a cwl2wdl tool definition into a cwl2wdl style dictionary.", "response": "def _tool_to_dict(tool):\n    \"\"\"Parse a tool definition into a cwl2wdl style dictionary.\n    \"\"\"\n    out = {\"name\": _id_to_name(tool.tool[\"id\"]),\n           \"baseCommand\": \" \".join(tool.tool[\"baseCommand\"]),\n           \"arguments\": [],\n           \"inputs\": [_input_to_dict(i) for i in tool.tool[\"inputs\"]],\n           \"outputs\": [_output_to_dict(o) for o in tool.tool[\"outputs\"]],\n           \"requirements\": _requirements_to_dict(tool.requirements + tool.hints),\n           \"stdin\": None, \"stdout\": None}\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_validate(data):\n    if data.get(\"vrn_file\") and tz.get_in([\"config\", \"algorithm\", \"validate\"], data):\n        return utils.deepish_copy(data)\n    elif \"group_orig\" in data:\n        for sub in multi.get_orig_items(data):\n            if \"validate\" in sub[\"config\"][\"algorithm\"]:\n                sub_val = utils.deepish_copy(sub)\n                sub_val[\"vrn_file\"] = data[\"vrn_file\"]\n                return sub_val\n    return None", "response": "Retrieve items to validate from single samples or combined joint calls."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef normalize_input_path(x, data):\n    if x is None:\n        return None\n    elif os.path.isabs(x):\n        return os.path.normpath(x)\n    else:\n        for d in [data[\"dirs\"].get(\"fastq\"), data[\"dirs\"].get(\"work\")]:\n            if d:\n                cur_x = os.path.normpath(os.path.join(d, x))\n                if os.path.exists(cur_x):\n                    return cur_x\n        raise IOError(\"Could not find validation file %s\" % x)", "response": "Normalize input path for input files handling relative paths in local and fastq directories."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget caller supplement for MuTect.", "response": "def _get_caller_supplement(caller, data):\n    \"\"\"Some callers like MuTect incorporate a second caller for indels.\n    \"\"\"\n    if caller == \"mutect\":\n        icaller = tz.get_in([\"config\", \"algorithm\", \"indelcaller\"], data)\n        if icaller:\n            caller = \"%s/%s\" % (caller, icaller)\n    return caller"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _pick_lead_item(items):\n    paired = vcfutils.get_paired(items)\n    if paired:\n        return paired.tumor_data\n    else:\n        return list(items)[0]", "response": "Choose a lead item for a set of samples."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _normalize_cwl_inputs(items):\n    with_validate = {}\n    vrn_files = []\n    ready_items = []\n    batch_samples = []\n    for data in (cwlutils.normalize_missing(utils.to_single_data(d)) for d in items):\n        batch_samples.append(dd.get_sample_name(data))\n        if tz.get_in([\"config\", \"algorithm\", \"validate\"], data):\n            with_validate[_checksum(tz.get_in([\"config\", \"algorithm\", \"validate\"], data))] = data\n        if data.get(\"vrn_file\"):\n            vrn_files.append(data[\"vrn_file\"])\n        ready_items.append(data)\n    if len(with_validate) == 0:\n        data = _pick_lead_item(ready_items)\n        data[\"batch_samples\"] = batch_samples\n        return data\n    else:\n        assert len(with_validate) == 1, len(with_validate)\n        assert len(set(vrn_files)) == 1, set(vrn_files)\n        data = _pick_lead_item(with_validate.values())\n        data[\"batch_samples\"] = batch_samples\n        data[\"vrn_file\"] = vrn_files[0]\n        return data", "response": "Extract variation and validation data from CWL input list of batched samples."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _checksum(in_file, block_size=65536):\n    cs = hashlib.sha256()\n    with open(in_file, \"rb\") as f:\n        for block in iter(lambda: f.read(block_size), b''):\n            cs.update(block)\n    return cs.hexdigest()", "response": "Calculate the SHA - 256 checksum of a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomparing final variant calls against reference materials of known calls.", "response": "def compare_to_rm(data):\n    \"\"\"Compare final variant calls against reference materials of known calls.\n    \"\"\"\n    if isinstance(data, (list, tuple)) and cwlutils.is_cwl_run(utils.to_single_data(data[0])):\n        data = _normalize_cwl_inputs(data)\n    toval_data = _get_validate(data)\n    toval_data = cwlutils.unpack_tarballs(toval_data, toval_data)\n    if toval_data:\n        caller = _get_caller(toval_data)\n        sample = dd.get_sample_name(toval_data)\n        base_dir = utils.safe_makedir(os.path.join(toval_data[\"dirs\"][\"work\"], \"validate\", sample, caller))\n\n        if isinstance(toval_data[\"vrn_file\"], (list, tuple)):\n            raise NotImplementedError(\"Multiple input files for validation: %s\" % toval_data[\"vrn_file\"])\n        else:\n            vrn_file = os.path.abspath(toval_data[\"vrn_file\"])\n        rm_file = normalize_input_path(toval_data[\"config\"][\"algorithm\"][\"validate\"], toval_data)\n        rm_interval_file = _gunzip(normalize_input_path(toval_data[\"config\"][\"algorithm\"].get(\"validate_regions\"),\n                                                        toval_data),\n                                   toval_data)\n        rm_interval_file = bedutils.clean_file(rm_interval_file, toval_data, prefix=\"validateregions-\",\n                                               bedprep_dir=utils.safe_makedir(os.path.join(base_dir, \"bedprep\")))\n        rm_file = naming.handle_synonyms(rm_file, dd.get_ref_file(toval_data), data.get(\"genome_build\"),\n                                         base_dir, data)\n        rm_interval_file = (naming.handle_synonyms(rm_interval_file, dd.get_ref_file(toval_data),\n                                                   data.get(\"genome_build\"), base_dir, data)\n                            if rm_interval_file else None)\n        vmethod = tz.get_in([\"config\", \"algorithm\", \"validate_method\"], data, \"rtg\")\n        # RTG can fail on totally empty files. Call everything in truth set as false negatives\n        if not vcfutils.vcf_has_variants(vrn_file):\n            eval_files = _setup_call_false(rm_file, rm_interval_file, base_dir, toval_data, \"fn\")\n            data[\"validate\"] = _rtg_add_summary_file(eval_files, base_dir, toval_data)\n        # empty validation file, every call is a false positive\n        elif not vcfutils.vcf_has_variants(rm_file):\n            eval_files = _setup_call_fps(vrn_file, rm_interval_file, base_dir, toval_data, \"fp\")\n            data[\"validate\"] = _rtg_add_summary_file(eval_files, base_dir, toval_data)\n        elif vmethod in [\"rtg\", \"rtg-squash-ploidy\"]:\n            eval_files = _run_rtg_eval(vrn_file, rm_file, rm_interval_file, base_dir, toval_data, vmethod)\n            eval_files = _annotate_validations(eval_files, toval_data)\n            data[\"validate\"] = _rtg_add_summary_file(eval_files, base_dir, toval_data)\n        elif vmethod == \"hap.py\":\n            data[\"validate\"] = _run_happy_eval(vrn_file, rm_file, rm_interval_file, base_dir, toval_data)\n        elif vmethod == \"bcbio.variation\":\n            data[\"validate\"] = _run_bcbio_variation(vrn_file, rm_file, rm_interval_file, base_dir,\n                                                    sample, caller, toval_data)\n    return [[data]]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _annotate_validations(eval_files, data):\n    for key in [\"tp\", \"tp-calls\", \"fp\", \"fn\"]:\n        if eval_files.get(key):\n            eval_files[key] = annotation.add_genome_context(eval_files[key], data)\n    return eval_files", "response": "Add annotations about potential problem regions to validation VCFs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _setup_call_false(vrn_file, rm_bed, base_dir, data, call_type):\n    out_file = os.path.join(base_dir, \"%s.vcf.gz\" % call_type)\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            if not vrn_file.endswith(\".gz\"):\n                vrn_file = vcfutils.bgzip_and_index(vrn_file, out_dir=os.path.dirname(tx_out_file))\n            cmd = (\"bcftools view -R {rm_bed} -f 'PASS,.' {vrn_file} -O z -o {tx_out_file}\")\n            do.run(cmd.format(**locals()), \"Prepare %s with empty reference\" % call_type, data)\n    return {call_type: out_file}", "response": "Create set of false positives or ngatives for inputs with empty truth sets."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses output TP FP and FN files to generate metrics for plotting.", "response": "def _rtg_add_summary_file(eval_files, base_dir, data):\n    \"\"\"Parse output TP FP and FN files to generate metrics for plotting.\n    \"\"\"\n    out_file = os.path.join(base_dir, \"validate-summary.csv\")\n    if not utils.file_uptodate(out_file, eval_files.get(\"tp\", eval_files.get(\"fp\", eval_files[\"fn\"]))):\n        with file_transaction(data, out_file) as tx_out_file:\n            with open(tx_out_file, \"w\") as out_handle:\n                writer = csv.writer(out_handle)\n                writer.writerow([\"sample\", \"caller\", \"vtype\", \"metric\", \"value\"])\n                base = _get_sample_and_caller(data)\n                for metric in [\"tp\", \"fp\", \"fn\"]:\n                    for vtype, bcftools_types in [(\"SNPs\", \"--types snps\"),\n                                                  (\"Indels\", \"--exclude-types snps\")]:\n                        in_file = eval_files.get(metric)\n                        if in_file and os.path.exists(in_file):\n                            cmd = (\"bcftools view {bcftools_types} {in_file} | grep -v ^# | wc -l\")\n                            count = int(subprocess.check_output(cmd.format(**locals()), shell=True))\n                        else:\n                            count = 0\n                        writer.writerow(base + [vtype, metric, count])\n    eval_files[\"summary\"] = out_file\n    return eval_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _prepare_inputs(vrn_file, rm_file, rm_interval_file, base_dir, data):\n    if not rm_file.endswith(\".vcf.gz\") or not os.path.exists(rm_file + \".tbi\"):\n        rm_file = vcfutils.bgzip_and_index(rm_file, data[\"config\"], out_dir=base_dir)\n    if len(vcfutils.get_samples(vrn_file)) > 1:\n        base = utils.splitext_plus(os.path.basename(vrn_file))[0]\n        sample_file = os.path.join(base_dir, \"%s-%s.vcf.gz\" % (base, dd.get_sample_name(data)))\n        vrn_file = vcfutils.select_sample(vrn_file, dd.get_sample_name(data), sample_file, data[\"config\"])\n    # rtg fails on bgzipped VCFs produced by GatherVcfs so we re-prep them\n    else:\n        vrn_file = vcfutils.bgzip_and_index(vrn_file, data[\"config\"], out_dir=base_dir)\n\n    interval_bed = _get_merged_intervals(rm_interval_file, vrn_file, base_dir, data)\n    return vrn_file, rm_file, interval_bed", "response": "Prepare input VCF and BED files for validation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _run_rtg_eval(vrn_file, rm_file, rm_interval_file, base_dir, data, validate_method):\n    out_dir = os.path.join(base_dir, \"rtg\")\n    if not utils.file_exists(os.path.join(out_dir, \"done\")):\n        if os.path.exists(out_dir):\n            shutil.rmtree(out_dir)\n        vrn_file, rm_file, interval_bed = _prepare_inputs(vrn_file, rm_file, rm_interval_file, base_dir, data)\n\n        rtg_ref = tz.get_in([\"reference\", \"rtg\"], data)\n        if isinstance(rtg_ref, dict) and \"base\" in rtg_ref:\n            rtg_ref = os.path.dirname(rtg_ref[\"base\"])\n        assert rtg_ref and os.path.exists(rtg_ref), (\"Did not find rtg indexed reference file for validation:\\n%s\\n\"\n                                                     \"Run bcbio_nextgen.py upgrade --data --aligners rtg\" % rtg_ref)\n        # handle CWL where we have a reference to a single file in the RTG directory\n        if os.path.isfile(rtg_ref):\n            rtg_ref = os.path.dirname(rtg_ref)\n\n        # get core and memory usage from standard configuration\n        threads = min(dd.get_num_cores(data), 6)\n        resources = config_utils.get_resources(\"rtg\", data[\"config\"])\n        memory = config_utils.adjust_opts(resources.get(\"jvm_opts\", [\"-Xms500m\", \"-Xmx1500m\"]),\n                                          {\"algorithm\": {\"memory_adjust\": {\"magnitude\": threads,\n                                                                           \"direction\": \"increase\"}}})\n        jvm_stack = [x for x in memory if x.startswith(\"-Xms\")]\n        jvm_mem = [x for x in memory if x.startswith(\"-Xmx\")]\n        jvm_stack = jvm_stack[0] if len(jvm_stack) > 0 else \"-Xms500m\"\n        jvm_mem = jvm_mem[0].replace(\"-Xmx\", \"\") if len(jvm_mem) > 0 else \"3g\"\n        cmd = [\"rtg\", \"vcfeval\", \"--threads\", str(threads),\n               \"-b\", rm_file, \"--bed-regions\", interval_bed,\n               \"-c\", vrn_file, \"-t\", rtg_ref, \"-o\", out_dir]\n        if validate_method == \"rtg-squash-ploidy\":\n            cmd += [\"--squash-ploidy\"]\n        rm_samples = vcfutils.get_samples(rm_file)\n        if len(rm_samples) > 1 and dd.get_sample_name(data) in rm_samples:\n            cmd += [\"--sample=%s\" % dd.get_sample_name(data)]\n        cmd += [\"--vcf-score-field='%s'\" % (_pick_best_quality_score(vrn_file))]\n        mem_export = \"%s export RTG_JAVA_OPTS='%s' && export RTG_MEM=%s\" % (utils.local_path_export(),\n                                                                            jvm_stack, jvm_mem)\n        cmd = mem_export + \" && \" + \" \".join(cmd)\n        do.run(cmd, \"Validate calls using rtg vcfeval\", data)\n    out = {\"fp\": os.path.join(out_dir, \"fp.vcf.gz\"),\n           \"fn\": os.path.join(out_dir, \"fn.vcf.gz\")}\n    tp_calls = os.path.join(out_dir, \"tp.vcf.gz\")\n    tp_baseline = os.path.join(out_dir, \"tp-baseline.vcf.gz\")\n    if os.path.exists(tp_baseline):\n        out[\"tp\"] = tp_baseline\n        out[\"tp-calls\"] = tp_calls\n    else:\n        out[\"tp\"] = tp_calls\n    return out", "response": "Run evaluation of a caller against the truth set using rtg vcfeval."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _pick_best_quality_score(vrn_file):\n    # pysam fails on checking reference contigs if input is empty\n    if not vcfutils.vcf_has_variants(vrn_file):\n        return \"DP\"\n    to_check = 25\n    scores = collections.defaultdict(int)\n    try:\n        in_handle = VariantFile(vrn_file)\n    except ValueError:\n        raise ValueError(\"Failed to parse input file in preparation for validation: %s\" % vrn_file)\n    with contextlib.closing(in_handle) as val_in:\n        for i, rec in enumerate(val_in):\n            if i > to_check:\n                break\n            if \"VQSLOD\" in rec.info and rec.info.get(\"VQSLOD\") is not None:\n                scores[\"INFO=VQSLOD\"] += 1\n            if \"TLOD\" in rec.info and rec.info.get(\"TLOD\") is not None:\n                scores[\"INFO=TLOD\"] += 1\n            for skey in [\"AVR\", \"GQ\", \"DP\"]:\n                if len(rec.samples) > 0 and rec.samples[0].get(skey) is not None:\n                    scores[skey] += 1\n            if rec.qual:\n                scores[\"QUAL\"] += 1\n    for key in [\"AVR\", \"INFO=VQSLOD\", \"INFO=TLOD\", \"GQ\", \"QUAL\", \"DP\"]:\n        if scores[key] > 0:\n            return key\n    raise ValueError(\"Did not find quality score for validation from %s\" % vrn_file)", "response": "Flexible quality score selection picking the best available."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_merged_intervals(rm_interval_file, vrn_file, base_dir, data):\n    a_intervals = get_analysis_intervals(data, vrn_file, base_dir)\n    if a_intervals:\n        final_intervals = shared.remove_lcr_regions(a_intervals, [data])\n        if rm_interval_file:\n            caller = _get_caller(data)\n            sample = dd.get_sample_name(data)\n            combo_intervals = os.path.join(base_dir, \"%s-%s-%s-wrm.bed\" %\n                                           (utils.splitext_plus(os.path.basename(final_intervals))[0],\n                                            sample, caller))\n            if not utils.file_uptodate(combo_intervals, final_intervals):\n                with file_transaction(data, combo_intervals) as tx_out_file:\n                    with utils.chdir(os.path.dirname(tx_out_file)):\n                        # Copy files locally to avoid issues on shared filesystems\n                        # where BEDtools has trouble accessing the same base\n                        # files from multiple locations\n                        a = os.path.basename(final_intervals)\n                        b = os.path.basename(rm_interval_file)\n                        try:\n                            shutil.copyfile(final_intervals, a)\n                        except IOError:\n                            time.sleep(60)\n                            shutil.copyfile(final_intervals, a)\n                        try:\n                            shutil.copyfile(rm_interval_file, b)\n                        except IOError:\n                            time.sleep(60)\n                            shutil.copyfile(rm_interval_file, b)\n                        cmd = (\"bedtools intersect -nonamecheck -a {a} -b {b} > {tx_out_file}\")\n                        do.run(cmd.format(**locals()), \"Intersect callable intervals for rtg vcfeval\")\n            final_intervals = combo_intervals\n    else:\n        assert rm_interval_file, \"No intervals to subset analysis with for %s\" % vrn_file\n        final_intervals = shared.remove_lcr_regions(rm_interval_file, [data])\n    return final_intervals", "response": "Retrieve intervals to run validation on and merge reference and callable BED files."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves callable regions based on gVCF.", "response": "def _callable_from_gvcf(data, vrn_file, out_dir):\n    \"\"\"Retrieve callable regions based on ref call regions in gVCF.\n\n    Uses https://github.com/lijiayong/gvcf_regions\n    \"\"\"\n    methods = {\"freebayes\": \"freebayes\", \"platypus\": \"platypus\",\n               \"gatk-haplotype\": \"gatk\"}\n    gvcf_type = methods.get(dd.get_variantcaller(data))\n    if gvcf_type:\n        out_file = os.path.join(out_dir, \"%s-gcvf-coverage.bed\" %\n                                utils.splitext_plus(os.path.basename(vrn_file))[0])\n        if not utils.file_uptodate(out_file, vrn_file):\n            with file_transaction(data, out_file) as tx_out_file:\n                cmd = (\"gvcf_regions.py --gvcf_type {gvcf_type} {vrn_file} \"\n                       \"| bedtools merge > {tx_out_file}\")\n                do.run(cmd.format(**locals()), \"Convert gVCF to BED file of callable regions\")\n        return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve analysis regions for the current variant calling pipeline.", "response": "def get_analysis_intervals(data, vrn_file, base_dir):\n    \"\"\"Retrieve analysis regions for the current variant calling pipeline.\n    \"\"\"\n    from bcbio.bam import callable\n    if vrn_file and vcfutils.is_gvcf_file(vrn_file):\n        callable_bed = _callable_from_gvcf(data, vrn_file, base_dir)\n        if callable_bed:\n            return callable_bed\n\n    if data.get(\"ensemble_bed\"):\n        return data[\"ensemble_bed\"]\n    elif dd.get_sample_callable(data):\n        return dd.get_sample_callable(data)\n    elif data.get(\"align_bam\"):\n        return callable.sample_callable_bed(data[\"align_bam\"], dd.get_ref_file(data), data)[0]\n    elif data.get(\"work_bam\"):\n        return callable.sample_callable_bed(data[\"work_bam\"], dd.get_ref_file(data), data)[0]\n    elif data.get(\"work_bam_callable\"):\n        data = utils.deepish_copy(data)\n        data[\"work_bam\"] = data.pop(\"work_bam_callable\")\n        return callable.sample_callable_bed(data[\"work_bam\"], dd.get_ref_file(data), data)[0]\n    elif tz.get_in([\"config\", \"algorithm\", \"callable_regions\"], data):\n        return tz.get_in([\"config\", \"algorithm\", \"callable_regions\"], data)\n    elif tz.get_in([\"config\", \"algorithm\", \"variant_regions\"], data):\n        return tz.get_in([\"config\", \"algorithm\", \"variant_regions\"], data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _run_happy_eval(vrn_file, rm_file, rm_interval_file, base_dir, data):\n    out_dir = utils.safe_makedir(os.path.join(base_dir, \"happy\"))\n    out_prefix = os.path.join(out_dir, \"val\")\n    if not utils.file_exists(out_prefix + \".summary.csv\"):\n        vrn_file, rm_file, interval_bed = _prepare_inputs(vrn_file, rm_file, rm_interval_file, base_dir, data)\n        cmd = [\"hap.py\", \"-V\", \"-f\", interval_bed, \"-r\", dd.get_ref_file(data),\n               \"-l\", \",\".join(_get_location_list(interval_bed)),\n               \"-o\", out_prefix, rm_file, vrn_file]\n        do.run(cmd, \"Validate calls using hap.py\", data)\n    return {\"vcf\": out_prefix + \".vcf.gz\"}", "response": "Validate calls using hap. py."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_location_list(interval_bed):\n    import pybedtools\n    regions = collections.OrderedDict()\n    for region in pybedtools.BedTool(interval_bed):\n        regions[str(region.chrom)] = None\n    return regions.keys()", "response": "Retrieve list of locations to analyze from input BED file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _run_bcbio_variation(vrn_file, rm_file, rm_interval_file, base_dir, sample, caller, data):\n    val_config_file = _create_validate_config_file(vrn_file, rm_file, rm_interval_file,\n                                                   base_dir, data)\n    work_dir = os.path.join(base_dir, \"work\")\n    out = {\"summary\": os.path.join(work_dir, \"validate-summary.csv\"),\n           \"grading\": os.path.join(work_dir, \"validate-grading.yaml\"),\n           \"discordant\": os.path.join(work_dir, \"%s-eval-ref-discordance-annotate.vcf\" % sample)}\n    if not utils.file_exists(out[\"discordant\"]) or not utils.file_exists(out[\"grading\"]):\n        bcbio_variation_comparison(val_config_file, base_dir, data)\n    out[\"concordant\"] = filter(os.path.exists,\n                                [os.path.join(work_dir, \"%s-%s-concordance.vcf\" % (sample, x))\n                                 for x in [\"eval-ref\", \"ref-eval\"]])[0]\n    return out", "response": "Run validation of a caller against the truth set using bcbio. variation. bcbio_variation_comparison."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning a variant comparison using the bcbio. variation toolkit given a configuration file.", "response": "def bcbio_variation_comparison(config_file, base_dir, data):\n    \"\"\"Run a variant comparison using the bcbio.variation toolkit, given an input configuration.\n    \"\"\"\n    tmp_dir = utils.safe_makedir(os.path.join(base_dir, \"tmp\"))\n    resources = config_utils.get_resources(\"bcbio_variation\", data[\"config\"])\n    jvm_opts = resources.get(\"jvm_opts\", [\"-Xms750m\", \"-Xmx2g\"])\n    cmd = [\"bcbio-variation\"] + jvm_opts + broad.get_default_jvm_opts(tmp_dir) + \\\n          [\"variant-compare\", config_file]\n    do.run(cmd, \"Comparing variant calls using bcbio.variation\", data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _create_validate_config(vrn_file, rm_file, rm_interval_file, base_dir, data):\n    ref_call = {\"file\": str(rm_file), \"name\": \"ref\", \"type\": \"grading-ref\",\n                \"fix-sample-header\": True, \"remove-refcalls\": True}\n    a_intervals = get_analysis_intervals(data, vrn_file, base_dir)\n    if a_intervals:\n        a_intervals = shared.remove_lcr_regions(a_intervals, [data])\n    if rm_interval_file:\n        ref_call[\"intervals\"] = rm_interval_file\n    eval_call = {\"file\": vrn_file, \"name\": \"eval\", \"remove-refcalls\": True}\n    exp = {\"sample\": data[\"name\"][-1],\n           \"ref\": dd.get_ref_file(data),\n           \"approach\": \"grade\",\n           \"calls\": [ref_call, eval_call]}\n    if a_intervals:\n        exp[\"intervals\"] = os.path.abspath(a_intervals)\n    if data.get(\"align_bam\"):\n        exp[\"align\"] = data[\"align_bam\"]\n    elif data.get(\"work_bam\"):\n        exp[\"align\"] = data[\"work_bam\"]\n    return {\"dir\": {\"base\": base_dir, \"out\": \"work\", \"prep\": \"work/prep\"},\n            \"experiments\": [exp]}", "response": "Create a bcbio. variation configuration input for validation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprovide summaries of grading results across all samples.", "response": "def summarize_grading(samples, vkey=\"validate\"):\n    \"\"\"Provide summaries of grading results across all samples.\n\n    Handles both traditional pipelines (validation part of variants) and CWL\n    pipelines (validation at top level)\n    \"\"\"\n    samples = list(utils.flatten(samples))\n    if not _has_grading_info(samples, vkey):\n        return [[d] for d in samples]\n    validate_dir = utils.safe_makedir(os.path.join(samples[0][\"dirs\"][\"work\"], vkey))\n    header = [\"sample\", \"caller\", \"variant.type\", \"category\", \"value\"]\n    _summarize_combined(samples, vkey)\n    validated, out = _group_validate_samples(samples, vkey,\n                                             ([\"metadata\", \"validate_batch\"], [\"metadata\", \"batch\"], [\"description\"]))\n    for vname, vitems in validated.items():\n        out_csv = os.path.join(validate_dir, \"grading-summary-%s.csv\" % vname)\n        with open(out_csv, \"w\") as out_handle:\n            writer = csv.writer(out_handle)\n            writer.writerow(header)\n            plot_data = []\n            plot_files = []\n            for data in sorted(vitems, key=lambda x: x.get(\"lane\", dd.get_sample_name(x)) or \"\"):\n                validations = [variant.get(vkey) for variant in data.get(\"variants\", [])\n                               if isinstance(variant, dict)]\n                validations = [v for v in validations if v]\n                if len(validations) == 0 and vkey in data:\n                    validations = [data.get(vkey)]\n                for validate in validations:\n                    if validate:\n                        validate[\"grading_summary\"] = out_csv\n                        if validate.get(\"grading\"):\n                            for row in _get_validate_plotdata_yaml(validate[\"grading\"], data):\n                                writer.writerow(row)\n                                plot_data.append(row)\n                        elif validate.get(\"summary\") and not validate.get(\"summary\") == \"None\":\n                            if isinstance(validate[\"summary\"], (list, tuple)):\n                                plot_files.extend(list(set(validate[\"summary\"])))\n                            else:\n                                plot_files.append(validate[\"summary\"])\n        if plot_files:\n            plots = validateplot.classifyplot_from_plotfiles(plot_files, out_csv)\n        elif plot_data:\n            plots = validateplot.create(plot_data, header, 0, data[\"config\"],\n                                        os.path.splitext(out_csv)[0])\n        else:\n            plots = []\n        for data in vitems:\n            if data.get(vkey):\n                data[vkey][\"grading_plots\"] = plots\n            for variant in data.get(\"variants\", []):\n                if isinstance(variant, dict) and variant.get(vkey):\n                    variant[vkey][\"grading_plots\"] = plots\n            out.append([data])\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _summarize_combined(samples, vkey):\n    validate_dir = utils.safe_makedir(os.path.join(samples[0][\"dirs\"][\"work\"], vkey))\n    combined, _ = _group_validate_samples(samples, vkey, [[\"metadata\", \"validate_combine\"]])\n    for vname, vitems in combined.items():\n        if vname:\n            cur_combined = collections.defaultdict(int)\n            for data in sorted(vitems, key=lambda x: x.get(\"lane\", dd.get_sample_name(x))):\n                validations = [variant.get(vkey) for variant in data.get(\"variants\", [])]\n                validations = [v for v in validations if v]\n                if len(validations) == 0 and vkey in data:\n                    validations = [data.get(vkey)]\n                for validate in validations:\n                    with open(validate[\"summary\"]) as in_handle:\n                        reader = csv.reader(in_handle)\n                        next(reader)  # header\n                        for _, caller, vtype, metric, value in reader:\n                            cur_combined[(caller, vtype, metric)] += int(value)\n            out_csv = os.path.join(validate_dir, \"grading-summary-%s.csv\" % vname)\n            with open(out_csv, \"w\") as out_handle:\n                writer = csv.writer(out_handle)\n                header = [\"sample\", \"caller\", \"vtype\", \"metric\", \"value\"]\n                writer.writerow(header)\n                for (caller, variant_type, category), val in cur_combined.items():\n                    writer.writerow([\"combined-%s\" % vname, caller, variant_type, category, val])\n            plots = validateplot.classifyplot_from_valfile(out_csv)", "response": "Prepare summarized CSV and plot files for samples to combine together."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncombining multiple batch validations into validation outputs.", "response": "def combine_validations(items, vkey=\"validate\"):\n    \"\"\"Combine multiple batch validations into validation outputs.\n    \"\"\"\n    csvs = set([])\n    pngs = set([])\n    for v in [x.get(vkey) for x in items]:\n        if v and v.get(\"grading_summary\"):\n            csvs.add(v.get(\"grading_summary\"))\n        if v and v.get(\"grading_plots\"):\n            pngs |= set(v.get(\"grading_plots\"))\n    if len(csvs) == 1:\n        grading_summary = csvs.pop()\n    else:\n        grading_summary = os.path.join(utils.safe_makedir(os.path.join(dd.get_work_dir(items[0]), vkey)),\n                                       \"grading-summary-combined.csv\")\n        with open(grading_summary, \"w\") as out_handle:\n            for i, csv in enumerate(sorted(list(csvs))):\n                with open(csv) as in_handle:\n                    h = in_handle.readline()\n                    if i == 0:\n                        out_handle.write(h)\n                    for l in in_handle:\n                        out_handle.write(l)\n    return {\"grading_plots\": sorted(list(pngs)), \"grading_summary\": grading_summary}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_validate_plotdata_yaml(grading_file, data):\n    with open(grading_file) as in_handle:\n        grade_stats = yaml.safe_load(in_handle)\n    for sample_stats in grade_stats:\n        sample = sample_stats[\"sample\"]\n        for vtype, cat, val in _flatten_grading(sample_stats):\n            yield [sample, variant.get(\"variantcaller\", \"\"),\n                   vtype, cat, val]", "response": "Retrieve validation plot data from grading YAML file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef freq_summary(val_file, call_file, truth_file, target_name):\n    out_file = \"%s-freqs.csv\" % utils.splitext_plus(val_file)[0]\n    truth_freqs = _read_truth_freqs(truth_file)\n    call_freqs = _read_call_freqs(call_file, target_name)\n    with VariantFile(val_file) as val_in:\n        with open(out_file, \"w\") as out_handle:\n            writer = csv.writer(out_handle)\n            writer.writerow([\"vtype\", \"valclass\", \"freq\"])\n            for rec in val_in:\n                call_type = _classify_rec(rec)\n                val_type = _get_validation_status(rec)\n                key = _get_key(rec)\n                freq = truth_freqs.get(key, call_freqs.get(key, 0.0))\n                writer.writerow([call_type, val_type, freq])\n    return out_file", "response": "Summarize true and false positive calls by variant type and frequency."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nidentifies frequencies for calls in the input file.", "response": "def _read_call_freqs(in_file, sample_name):\n    \"\"\"Identify frequencies for calls in the input file.\n    \"\"\"\n    from bcbio.heterogeneity import bubbletree\n    out = {}\n    with VariantFile(in_file) as call_in:\n        for rec in call_in:\n            if rec.filter.keys() == [\"PASS\"]:\n                for name, sample in rec.samples.items():\n                    if name == sample_name:\n                        alt, depth, freq = bubbletree.sample_alt_and_depth(rec, sample)\n                        if freq is not None:\n                            out[_get_key(rec)] = freq\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read_truth_freqs(in_file):\n    out = {}\n    with VariantFile(in_file) as bcf_in:\n        for rec in bcf_in:\n            freq = float(rec.info.get(\"VAF\", 1.0))\n            out[_get_key(rec)] = freq\n    return out", "response": "Read frequency of calls from truth VCF."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_rec(samples, default_keys=None):\n    recs = samples_to_records([normalize_missing(utils.to_single_data(x)) for x in samples], default_keys)\n    return [[x] for x in recs]", "response": "Convert input samples into CWL records useful for single item parallelization."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_rec_single(samples, default_keys=None):\n    out = []\n    for data in samples:\n        recs = samples_to_records([normalize_missing(utils.to_single_data(data))], default_keys)\n        assert len(recs) == 1\n        out.append(recs[0])\n    return out", "response": "Convert output into a list of single CWL records."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck for cases where we have a combined input nested list.", "response": "def handle_combined_input(args):\n    \"\"\"Check for cases where we have a combined input nested list.\n\n    In these cases the CWL will be double nested:\n\n    [[[rec_a], [rec_b]]]\n\n    and we remove the outer nesting.\n    \"\"\"\n    cur_args = args[:]\n    while len(cur_args) == 1 and isinstance(cur_args[0], (list, tuple)):\n        cur_args = cur_args[0]\n    return cur_args"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nnormalizing missing values to avoid string None inputs.", "response": "def normalize_missing(xs):\n    \"\"\"Normalize missing values to avoid string 'None' inputs.\n    \"\"\"\n    if isinstance(xs, dict):\n        for k, v in xs.items():\n            xs[k] = normalize_missing(v)\n    elif isinstance(xs, (list, tuple)):\n        xs = [normalize_missing(x) for x in xs]\n    elif isinstance(xs, six.string_types):\n        if xs.lower() in [\"none\", \"null\"]:\n            xs = None\n        elif xs.lower() == \"true\":\n            xs = True\n        elif xs.lower() == \"false\":\n            xs = False\n    return xs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nunpacking a workflow tarball into ready to use directories.", "response": "def unpack_tarballs(xs, data, use_subdir=True):\n    \"\"\"Unpack workflow tarballs into ready to use directories.\n    \"\"\"\n    if isinstance(xs, dict):\n        for k, v in xs.items():\n            xs[k] = unpack_tarballs(v, data, use_subdir)\n    elif isinstance(xs, (list, tuple)):\n        xs = [unpack_tarballs(x, data, use_subdir) for x in xs]\n    elif isinstance(xs, six.string_types):\n        if os.path.isfile(xs.encode(\"utf-8\", \"ignore\")) and xs.endswith(\"-wf.tar.gz\"):\n            if use_subdir:\n                tarball_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"wf-inputs\"))\n            else:\n                tarball_dir = dd.get_work_dir(data)\n            out_dir = os.path.join(tarball_dir,\n                                   os.path.basename(xs).replace(\"-wf.tar.gz\", \"\").replace(\"--\", os.path.sep))\n            if not os.path.exists(out_dir):\n                with utils.chdir(tarball_dir):\n                    with tarfile.open(xs, \"r:gz\") as tar:\n                        tar.extractall()\n            assert os.path.exists(out_dir), out_dir\n            # Default to representing output directory\n            xs = out_dir\n            # Look for aligner indices\n            for fname in os.listdir(out_dir):\n                if fname.endswith(DIR_TARGETS):\n                    xs = os.path.join(out_dir, fname)\n                    break\n                elif fname.endswith(BASENAME_TARGETS):\n                    base = os.path.join(out_dir, utils.splitext_plus(os.path.basename(fname))[0])\n                    xs = glob.glob(\"%s*\" % base)\n                    break\n    return xs"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves cwlkeys from inputs handling defaults which can be null.", "response": "def _get_all_cwlkeys(items, default_keys=None):\n    \"\"\"Retrieve cwlkeys from inputs, handling defaults which can be null.\n\n    When inputs are null in some and present in others, this creates unequal\n    keys in each sample, confusing decision making about which are primary and extras.\n    \"\"\"\n    if default_keys:\n        default_keys = set(default_keys)\n    else:\n        default_keys = set([\"metadata__batch\", \"config__algorithm__validate\",\n                            \"config__algorithm__validate_regions\",\n                            \"config__algorithm__validate_regions_merged\",\n                            \"config__algorithm__variant_regions\",\n                            \"validate__summary\",\n                            \"validate__tp\", \"validate__fp\", \"validate__fn\",\n                            \"config__algorithm__coverage\", \"config__algorithm__coverage_merged\",\n                            \"genome_resources__variation__cosmic\", \"genome_resources__variation__dbsnp\",\n                            \"genome_resources__variation__clinvar\"\n        ])\n    all_keys = set([])\n    for data in items:\n        all_keys.update(set(data[\"cwl_keys\"]))\n    all_keys.update(default_keys)\n    return all_keys"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsplit a set of CWL output dictionaries into data samples and CWL items.", "response": "def split_data_cwl_items(items, default_keys=None):\n    \"\"\"Split a set of CWL output dictionaries into data samples and CWL items.\n\n    Handles cases where we're arrayed on multiple things, like a set of regional\n    VCF calls and data objects.\n    \"\"\"\n    key_lens = set([])\n    for data in items:\n        key_lens.add(len(_get_all_cwlkeys([data], default_keys)))\n    extra_key_len = min(list(key_lens)) if len(key_lens) > 1 else None\n    data_out = []\n    extra_out = []\n    for data in items:\n        if extra_key_len and len(_get_all_cwlkeys([data], default_keys)) == extra_key_len:\n            extra_out.append(data)\n        else:\n            data_out.append(data)\n    if len(extra_out) == 0:\n        return data_out, {}\n    else:\n        cwl_keys = extra_out[0][\"cwl_keys\"]\n        for extra in extra_out[1:]:\n            cur_cwl_keys = extra[\"cwl_keys\"]\n            assert cur_cwl_keys == cwl_keys, pprint.pformat(extra_out)\n        cwl_extras = collections.defaultdict(list)\n        for data in items:\n            for key in cwl_keys:\n                cwl_extras[key].append(data[key])\n        data_final = []\n        for data in data_out:\n            for key in cwl_keys:\n                data.pop(key)\n            data_final.append(data)\n        return data_final, dict(cwl_extras)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef samples_to_records(samples, default_keys=None):\n    from bcbio.pipeline import run_info\n    RECORD_CONVERT_TO_LIST = set([\"config__algorithm__tools_on\", \"config__algorithm__tools_off\",\n                                  \"reference__genome_context\"])\n    all_keys = _get_all_cwlkeys(samples, default_keys)\n    out = []\n    for data in samples:\n        for raw_key in sorted(list(all_keys)):\n            key = raw_key.split(\"__\")\n            if tz.get_in(key, data) is None:\n                data = tz.update_in(data, key, lambda x: None)\n            if raw_key not in data[\"cwl_keys\"]:\n                data[\"cwl_keys\"].append(raw_key)\n            if raw_key in RECORD_CONVERT_TO_LIST:\n                val = tz.get_in(key, data)\n                if not val: val = []\n                elif not isinstance(val, (list, tuple)): val = [val]\n                data = tz.update_in(data, key, lambda x: val)\n            # Booleans are problematic for CWL serialization, convert into string representation\n            if isinstance(tz.get_in(key, data), bool):\n                data = tz.update_in(data, key, lambda x: str(tz.get_in(key, data)))\n        data[\"metadata\"] = run_info.add_metadata_defaults(data.get(\"metadata\", {}))\n        out.append(data)\n    return out", "response": "Convert samples into CWL records."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nassign complex inputs like variants and align outputs to samples.", "response": "def assign_complex_to_samples(items):\n    \"\"\"Assign complex inputs like variants and align outputs to samples.\n\n    Handles list inputs to record conversion where we have inputs from multiple\n    locations and need to ensure they are properly assigned to samples in many\n    environments.\n\n    The unpleasant approach here is to use standard file naming to match\n    with samples so this can work in environments where we don't download/stream\n    the input files (for space/time savings).\n    \"\"\"\n    extract_fns = {(\"variants\", \"samples\"): _get_vcf_samples,\n                   (\"align_bam\",): _get_bam_samples}\n    complex = {k: {} for k in extract_fns.keys()}\n    for data in items:\n        for k in complex:\n            v = tz.get_in(k, data)\n            if v is not None:\n                for s in extract_fns[k](v, items):\n                    if s:\n                        complex[k][s] = v\n    out = []\n    for data in items:\n        for k in complex:\n            newv = tz.get_in([k, dd.get_sample_name(data)], complex)\n            if newv:\n                data = tz.update_in(data, k, lambda x: newv)\n        out.append(data)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsummarizing variant calls and validation for multiple samples.", "response": "def summarize_vc(items):\n    \"\"\"CWL target: summarize variant calls and validation for multiple samples.\n    \"\"\"\n    items = [utils.to_single_data(x) for x in utils.flatten(items)]\n    items = [_normalize_vc_input(x) for x in items]\n    items = validate.summarize_grading(items)\n    items = [utils.to_single_data(x) for x in items]\n    out = {\"validate\": validate.combine_validations(items),\n           \"variants\": {\"calls\": [], \"gvcf\": [], \"samples\": []}}\n    added = set([])\n    variants_by_sample = collections.defaultdict(list)\n    sample_order = []\n    for data in items:\n        batch_samples = data.get(\"batch_samples\", [dd.get_sample_name(data)])\n        for s in batch_samples:\n            if s not in sample_order:\n                sample_order.append(s)\n        if data.get(\"vrn_file\"):\n            # Only get batches if we're actually doing variantcalling in bcbio\n            # otherwise we'll be using the original files\n            names = dd.get_batches(data) if dd.get_variantcaller(data) else None\n            if not names:\n                names = [dd.get_sample_name(data)]\n            batch_name = names[0]\n            if data.get(\"vrn_file_joint\") is not None:\n                to_add = [(\"vrn_file\", \"gvcf\", dd.get_sample_name(data)),\n                          (\"vrn_file_joint\", \"calls\", batch_name)]\n            else:\n                to_add = [(\"vrn_file\", \"calls\", batch_name)]\n            for vrn_key, out_key, name in to_add:\n                cur_name = \"%s-%s\" % (name, dd.get_variantcaller(data))\n                out_file = os.path.join(utils.safe_makedir(os.path.join(dd.get_work_dir(data),\n                                                                        \"variants\", out_key)),\n                                        \"%s.vcf.gz\" % cur_name)\n                for s in batch_samples:\n                    variants_by_sample[s].append(out_file)\n                if cur_name not in added:\n                    added.add(cur_name)\n                    # Ideally could symlink here but doesn't appear to work with\n                    # Docker container runs on Toil where PATHs don't get remapped\n                    utils.copy_plus(os.path.realpath(data[vrn_key]), out_file)\n                    vcfutils.bgzip_and_index(out_file, data[\"config\"])\n                    out[\"variants\"][out_key].append(out_file)\n    for sample in sample_order:\n        out[\"variants\"][\"samples\"].append(variants_by_sample[sample])\n    return [out]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _normalize_vc_input(data):\n    if data.get(\"ensemble\"):\n        for k in [\"batch_samples\", \"validate\", \"vrn_file\"]:\n            data[k] = data[\"ensemble\"][k]\n        data[\"config\"][\"algorithm\"][\"variantcaller\"] = \"ensemble\"\n        data[\"metadata\"] = {\"batch\": data[\"ensemble\"][\"batch_id\"]}\n    return data", "response": "Normalize different types of variant calling inputs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nposting - processing of variant calls.", "response": "def postprocess_variants(items):\n    \"\"\"Provide post-processing of variant calls: filtering and effects annotation.\n    \"\"\"\n    vrn_key = \"vrn_file\"\n    if not isinstance(items, dict):\n        items = [utils.to_single_data(x) for x in items]\n        if \"vrn_file_joint\" in items[0]:\n            vrn_key = \"vrn_file_joint\"\n    data, items = _get_batch_representative(items, vrn_key)\n    items = cwlutils.unpack_tarballs(items, data)\n    data = cwlutils.unpack_tarballs(data, data)\n    cur_name = \"%s, %s\" % (dd.get_sample_name(data), get_variantcaller(data, require_bam=False))\n    logger.info(\"Finalizing variant calls: %s\" % cur_name)\n    orig_vrn_file = data.get(vrn_key)\n    data = _symlink_to_workdir(data, [vrn_key])\n    data = _symlink_to_workdir(data, [\"config\", \"algorithm\", \"variant_regions\"])\n    if data.get(vrn_key):\n        logger.info(\"Calculating variation effects for %s\" % cur_name)\n        ann_vrn_file, vrn_stats = effects.add_to_vcf(data[vrn_key], data)\n        if ann_vrn_file:\n            data[vrn_key] = ann_vrn_file\n        if vrn_stats:\n            data[\"vrn_stats\"] = vrn_stats\n        orig_items = _get_orig_items(items)\n        logger.info(\"Annotate VCF file: %s\" % cur_name)\n        data[vrn_key] = annotation.finalize_vcf(data[vrn_key], get_variantcaller(data, require_bam=False),\n                                                orig_items)\n        if cwlutils.is_cwl_run(data):\n            logger.info(\"Annotate with population level variation data\")\n            ann_file = population.run_vcfanno(data[vrn_key], data)\n            if ann_file:\n                data[vrn_key] = ann_file\n        logger.info(\"Filtering for %s\" % cur_name)\n        data[vrn_key] = variant_filtration(data[vrn_key], dd.get_ref_file(data),\n                                           tz.get_in((\"genome_resources\", \"variation\"), data, {}),\n                                           data, orig_items)\n        logger.info(\"Prioritization for %s\" % cur_name)\n        prio_vrn_file = prioritize.handle_vcf_calls(data[vrn_key], data, orig_items)\n        if prio_vrn_file != data[vrn_key]:\n            data[vrn_key] = prio_vrn_file\n            logger.info(\"Germline extraction for %s\" % cur_name)\n            data = germline.extract(data, orig_items)\n\n        if dd.get_align_bam(data):\n            data = damage.run_filter(data[vrn_key], dd.get_align_bam(data), dd.get_ref_file(data),\n                                     data, orig_items)\n    if orig_vrn_file and os.path.samefile(data[vrn_key], orig_vrn_file):\n        data[vrn_key] = orig_vrn_file\n    return [[data]]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve original items in a batch handling CWL and standard cases.", "response": "def _get_orig_items(data):\n    \"\"\"Retrieve original items in a batch, handling CWL and standard cases.\n    \"\"\"\n    if isinstance(data, dict):\n        if dd.get_align_bam(data) and tz.get_in([\"metadata\", \"batch\"], data) and \"group_orig\" in data:\n            return vmulti.get_orig_items(data)\n        else:\n            return [data]\n    else:\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve a representative data item from a batch.", "response": "def _get_batch_representative(items, key):\n    \"\"\"Retrieve a representative data item from a batch.\n\n    Handles standard bcbio cases (a single data item) and CWL cases with\n    batches that have a consistent variant file.\n    \"\"\"\n    if isinstance(items, dict):\n        return items, items\n    else:\n        vals = set([])\n        out = []\n        for data in items:\n            if key in data:\n                vals.add(data[key])\n                out.append(data)\n        if len(vals) != 1:\n            raise ValueError(\"Incorrect values for %s: %s\" % (key, list(vals)))\n        return out[0], items"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_storage_manager(resource):\n    for manager in (AmazonS3, ArvadosKeep, SevenBridges, DNAnexus, AzureBlob, GoogleCloud, RegularServer):\n        if manager.check_resource(resource):\n            return manager()\n\n    raise ValueError(\"Unexpected object store  %(resource)s\" %\n                     {\"resource\": resource})", "response": "Return a storage manager which can process this resource."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef default_region(fname):\n    manager = _get_storage_manager(fname)\n    if hasattr(manager, \"get_region\"):\n        return manager.get_region()\n\n    raise NotImplementedError(\"Unexpected object store %s\" % fname)", "response": "Return the default region for the received resource."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndownload the resource from the storage.", "response": "def download(fname, input_dir, dl_dir=None):\n    \"\"\"Download the resource from the storage.\"\"\"\n    try:\n        manager = _get_storage_manager(fname)\n    except ValueError:\n        return fname\n    return manager.download(fname, input_dir, dl_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cl_input(fname, unpack=True, anonpipe=True):\n    try:\n        manager = _get_storage_manager(fname)\n    except ValueError:\n        return fname\n\n    return manager.cl_input(fname, unpack, anonpipe)", "response": "Return command line input for a file handling streaming\n    remote cases."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn all user - defined metadata standard HTTP properties and system properties for the blob.", "response": "def blob_properties(self):\n        \"\"\"Returns all user-defined metadata, standard HTTP properties,\n        and system properties for the blob.\n        \"\"\"\n        if not self._blob_properties:\n            self._blob_properties = self._blob_service.get_blob_properties(\n                container_name=self._container_name,\n                blob_name=self._blob_name)\n        return self._blob_properties"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _chunk_offsets(self):\n        index = 0\n        blob_size = self.blob_properties.get('content-length')\n        while index < blob_size:\n            yield index\n            index = index + self._chunk_size", "response": "Iterator over chunk offests."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _chunk_iter(self):\n        for chunk_offset in self._chunk_offsets():\n            yield self._download_chunk(chunk_offset=chunk_offset,\n                                       chunk_size=self._chunk_size)", "response": "Iterator over the blob file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _download_chunk_with_retries(self, chunk_offset, chunk_size,\n                                     retries=3, retry_wait=1):\n        \"\"\"Reads or downloads the received blob from the system.\"\"\"\n        import azure\n        while True:\n            try:\n                chunk = self._download_chunk(chunk_offset, chunk_size)\n            except azure.WindowsAzureError:\n                if retries > 0:\n                    retries = retries - 1\n                    time.sleep(retry_wait)\n                else:\n                    raise\n            else:\n                return chunk", "response": "Reads or downloads the received blob from the system."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _download_chunk(self, chunk_offset, chunk_size):\n        range_id = 'bytes={0}-{1}'.format(\n            chunk_offset, chunk_offset + chunk_size - 1)\n\n        return self._blob_service.get_blob(\n            container_name=self._container_name,\n            blob_name=self._blob_name,\n            x_ms_range=range_id)", "response": "Reads or downloads the received blob from the system."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads at most size bytes from the file.", "response": "def read(self, size=sys.maxsize):\n        \"\"\"Read at most size bytes from the file (less if the read hits EOF\n        before obtaining size bytes).\n        \"\"\"\n        blob_size = int(self.blob_properties.get('content-length'))\n        if self._pointer < blob_size:\n            chunk = self._download_chunk_with_retries(\n                chunk_offset=self._pointer, chunk_size=size)\n            self._pointer += size\n            return chunk"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_remote(cls, filename):\n        parts = filename.split(\"//\")[-1].split(\"/\", 1)\n        bucket, key = parts if len(parts) == 2 else (parts[0], None)\n        if bucket.find(\"@\") > 0:\n            bucket, region = bucket.split(\"@\")\n        else:\n            region = None\n\n        return cls._REMOTE_FILE(\"s3\", bucket, key, region)", "response": "Parses a remote filename into bucket and key information."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncommand line required for download using the standard AWS command line interface.", "response": "def _cl_aws_cli(cls, file_info, region):\n        \"\"\"Command line required for download using the standard AWS\n        command line interface.\n        \"\"\"\n        s3file = cls._S3_FILE % {\"bucket\": file_info.bucket,\n                                 \"key\": file_info.key,\n                                 \"region\": \"\"}\n        command = [os.path.join(os.path.dirname(sys.executable), \"aws\"),\n                   \"s3\", \"cp\", \"--region\", region, s3file]\n        return (command, \"awscli\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _cl_gof3r(file_info, region):\n        command = [\"gof3r\", \"get\", \"--no-md5\",\n                   \"-k\", file_info.key,\n                   \"-b\", file_info.bucket]\n        if region != \"us-east-1\":\n            command += [\"--endpoint=s3-%s.amazonaws.com\" % region]\n        return (command, \"gof3r\")", "response": "Command line required for download using gof3r."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _download_cl(cls, filename):\n        file_info = cls.parse_remote(filename)\n        region = cls.get_region(filename)\n        if region in REGIONS_NEWPERMS[\"s3\"]:\n            return cls._cl_aws_cli(file_info, region)\n        else:\n            return cls._cl_gof3r(file_info, region)", "response": "Provide potentially streaming download from S3 using gof3r\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve region from standard environmental variables or file name.", "response": "def get_region(cls, resource=None):\n        \"\"\"Retrieve region from standard environmental variables\n        or file name.\n\n        More information of the following link: http://goo.gl/Vb9Jky\n        \"\"\"\n        if resource:\n            resource_info = cls.parse_remote(resource)\n            if resource_info.region:\n                return resource_info.region\n\n        return os.environ.get(\"AWS_DEFAULT_REGION\", cls._DEFAULT_REGION)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef connect(cls, resource):\n        import boto\n        return boto.s3.connect_to_region(cls.get_region(resource))", "response": "Connect to this Region s endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprovide potentially streaming download from S3 using gof3r or the AWS CLI.", "response": "def download(cls, filename, input_dir, dl_dir=None):\n        \"\"\"Provide potentially streaming download from S3 using gof3r\n        or the AWS CLI.\n        \"\"\"\n        file_info = cls.parse_remote(filename)\n        if not dl_dir:\n            dl_dir = os.path.join(input_dir, file_info.bucket,\n                                  os.path.dirname(file_info.key))\n            utils.safe_makedir(dl_dir)\n\n        out_file = os.path.join(dl_dir, os.path.basename(file_info.key))\n\n        if not utils.file_exists(out_file):\n            with file_transaction({}, out_file) as tx_out_file:\n                command, prog = cls._download_cl(filename)\n                if prog == \"gof3r\":\n                    command.extend([\"-p\", tx_out_file])\n                elif prog == \"awscli\":\n                    command.extend([tx_out_file])\n                else:\n                    raise NotImplementedError(\n                        \"Unexpected download program %s\" % prog)\n                subprocess.check_call(command)\n        return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn command line input for a file handling streaming remote cases.", "response": "def cl_input(cls, filename, unpack=True, anonpipe=True):\n        \"\"\"Return command line input for a file, handling streaming\n        remote cases.\n        \"\"\"\n        command, prog = cls._download_cl(filename)\n        if prog == \"awscli\":\n            command.append(\"-\")\n\n        command = \" \".join(command)\n        if filename.endswith(\".gz\") and unpack:\n            command = \"%(command)s | gunzip -c\" % {\"command\": command}\n        elif filename.endswith(\".bz2\") and unpack:\n            command = \"%(command)s | bunzip2 -c\" % {\"command\": command}\n        if anonpipe:\n            command = \"<(%(command)s)\" % {\"command\": command}\n\n        return command"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list containing the names of the entries in the directory given by path.", "response": "def list(cls, path):\n        \"\"\"Return a list containing the names of the entries in the directory\n        given by path. The list is in arbitrary order.\n        \"\"\"\n        file_info = cls.parse_remote(path)\n        connection = cls.connect(path)\n        bucket = connection.get_bucket(file_info.bucket)\n        region = \"@%s\" % file_info.region if file_info.region else \"\"\n\n        output = []\n        for key in bucket.get_all_keys(prefix=file_info.key):\n            output.append(cls._S3_FILE % {\"bucket\": file_info.bucket,\n                                          \"key\": key.name,\n                                          \"region\": region})\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef open(cls, filename):\n        import boto\n        file_info = cls.parse_remote(filename)\n        connection = cls.connect(filename)\n        try:\n            s3_bucket = connection.get_bucket(file_info.bucket)\n        except boto.exception.S3ResponseError as error:\n            # if we don't have bucket permissions but folder permissions,\n            # try without validation\n            if error.status == 403:\n                s3_bucket = connection.get_bucket(file_info.bucket,\n                                                  validate=False)\n            else:\n                raise\n\n        s3_key = s3_bucket.get_key(file_info.key)\n        if s3_key is None:\n            raise ValueError(\"Did not find S3 key: %s\" % filename)\n        return S3Handle(s3_key)", "response": "Return a handle like object for streaming from S3."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a remote filename into blob information.", "response": "def parse_remote(cls, filename):\n        \"\"\"Parses a remote filename into blob information.\"\"\"\n        blob_file = cls._URL_FORMAT.search(filename)\n        return cls._REMOTE_FILE(\"blob\",\n                                storage=blob_file.group(\"storage\"),\n                                container=blob_file.group(\"container\"),\n                                blob=blob_file.group(\"blob\"))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a connection object pointing to the endpoint associated to the received resource.", "response": "def connect(cls, resource):\n        \"\"\"Returns a connection object pointing to the endpoint\n        associated to the received resource.\n        \"\"\"\n        from azure import storage as azure_storage\n        file_info = cls.parse_remote(resource)\n        return azure_storage.BlobService(file_info.storage)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef download(cls, filename, input_dir, dl_dir=None):\n        file_info = cls.parse_remote(filename)\n        if not dl_dir:\n            dl_dir = os.path.join(input_dir, file_info.container,\n                                  os.path.dirname(file_info.blob))\n            utils.safe_makedir(dl_dir)\n\n        out_file = os.path.join(dl_dir, os.path.basename(file_info.blob))\n\n        if not utils.file_exists(out_file):\n            with file_transaction({}, out_file) as tx_out_file:\n                blob_service = cls.connect(filename)\n                blob_service.get_blob_to_path(\n                    container_name=file_info.container,\n                    blob_name=file_info.blob,\n                    file_path=tx_out_file)\n        return out_file", "response": "Download the resource from the storage."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list(cls, path):\n        import azure\n        from azure import storage as azure_storage\n        output = []\n        path_info = cls.parse_remote(path)\n        blob_service = azure_storage.BlobService(path_info.storage)\n        try:\n            blob_enum = blob_service.list_blobs(path_info.container)\n        except azure.WindowsAzureMissingResourceError:\n            return output\n\n        for item in blob_enum:\n            output.append(cls._BLOB_FILE.format(storage=path_info.storage,\n                                                container=path_info.container,\n                                                blob=item.name))\n        return output", "response": "Return a list containing the names of the entries in the directory\n        given by path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprovide a handle - like object for streaming.", "response": "def open(cls, filename):\n        \"\"\"Provide a handle-like object for streaming.\"\"\"\n        file_info = cls.parse_remote(filename)\n        blob_service = cls.connect(filename)\n        return BlobHandle(blob_service=blob_service,\n                          container=file_info.container,\n                          blob=file_info.blob,\n                          chunk_size=cls._BLOB_CHUNK_DATA_SIZE)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncompresses a BAM file to CRAM.", "response": "def compress(in_bam, data):\n    \"\"\"Compress a BAM file to CRAM, providing indexed CRAM file.\n\n    Does 8 bin compression of quality score and read name removal\n    using bamUtils squeeze if `cram` specified:\n\n    http://genome.sph.umich.edu/wiki/BamUtil:_squeeze\n\n    Otherwise does `cram-lossless` which only converts to CRAM.\n    \"\"\"\n    out_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"archive\"))\n    out_file = os.path.join(out_dir, \"%s.cram\" % os.path.splitext(os.path.basename(in_bam))[0])\n    cores = dd.get_num_cores(data)\n    ref_file = dd.get_ref_file(data)\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            compress_type = dd.get_archive(data)\n            samtools = config_utils.get_program(\"samtools\", data[\"config\"])\n            try:\n                bam_cmd = config_utils.get_program(\"bam\", data[\"config\"])\n            except config_utils.CmdNotFound:\n                bam_cmd = None\n            to_cram = (\"{samtools} view -T {ref_file} -@ {cores} \"\n                       \"-C -x BD -x BI -o {tx_out_file}\")\n            compressed = False\n            if \"cram\" in compress_type and bam_cmd:\n                try:\n                    cmd = (\"{bam_cmd} squeeze --in {in_bam} --out -.ubam --keepDups \"\n                           \"--binQualS=2,10,20,25,30,35,70 --binMid | \" + to_cram)\n                    do.run(cmd.format(**locals()), \"Compress BAM to CRAM: quality score binning\")\n                    compressed = True\n                # Retry failures avoiding using bam squeeze which can cause issues\n                except subprocess.CalledProcessError:\n                    pass\n            if not compressed:\n                cmd = (to_cram + \" {in_bam}\")\n                do.run(cmd.format(**locals()), \"Compress BAM to CRAM: lossless\")\n    index(out_file, data[\"config\"])\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef index(in_cram, config):\n    out_file = in_cram + \".crai\"\n    if not utils.file_uptodate(out_file, in_cram):\n        with file_transaction(config, in_cram + \".crai\") as tx_out_file:\n            tx_in_file = os.path.splitext(tx_out_file)[0]\n            utils.symlink_plus(in_cram, tx_in_file)\n            cmd = \"samtools index {tx_in_file}\"\n            do.run(cmd.format(**locals()), \"Index CRAM file\")\n    return out_file", "response": "Ensure CRAM file has a. crai index file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_bam(in_file, out_file, data):\n    if not utils.file_uptodate(out_file, in_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            cmd = [\"samtools\", \"view\", \"-O\", \"BAM\", \"-o\", tx_out_file, in_file]\n            do.run(cmd, \"Convert CRAM to BAM\")\n    bam.index(out_file, data[\"config\"])\n    return out_file", "response": "Convert CRAM file into BAM."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstarts a single cluster or machines with a given function.", "response": "def start(parallel, items, config, dirs=None, name=None, multiplier=1,\n          max_multicore=None):\n    \"\"\"Start a parallel cluster or machines to be used for running remote\n    functions.\n\n    Returns a function used to process, in parallel items with a given function.\n\n    Allows sharing of a single cluster across multiple functions with\n    identical resource requirements. Uses local execution for non-distributed\n    clusters or completed jobs.\n\n    A checkpoint directory keeps track of finished tasks, avoiding spinning up\n    clusters for sections that have been previous processed.\n\n    multiplier - Number of expected jobs per initial input item. Used to avoid\n    underscheduling cores when an item is split during processing.\n    max_multicore -- The maximum number of cores to use for each process. Can be\n    used to process less multicore usage when jobs run faster on more single\n    cores.\n    \"\"\"\n    if name:\n        checkpoint_dir = utils.safe_makedir(os.path.join(dirs[\"work\"],\n                                                         \"checkpoints_parallel\"))\n        checkpoint_file = os.path.join(checkpoint_dir, \"%s.done\" % name)\n    else:\n        checkpoint_file = None\n    sysinfo = system.get_info(dirs, parallel, config.get(\"resources\", {}))\n    items = [x for x in items if x is not None] if items else []\n    max_multicore = int(max_multicore or sysinfo.get(\"cores\", 1))\n    parallel = resources.calculate(parallel, items, sysinfo, config,\n                                   multiplier=multiplier,\n                                   max_multicore=max_multicore)\n    try:\n        view = None\n        if parallel[\"type\"] == \"ipython\":\n            if checkpoint_file and os.path.exists(checkpoint_file):\n                logger.info(\"Running locally instead of distributed -- checkpoint passed: %s\" % name)\n                parallel[\"cores_per_job\"] = 1\n                parallel[\"num_jobs\"] = 1\n                parallel[\"checkpointed\"] = True\n                yield multi.runner(parallel, config)\n            else:\n                from bcbio.distributed import ipython\n                with ipython.create(parallel, dirs, config) as view:\n                    yield ipython.runner(view, parallel, dirs, config)\n        else:\n            yield multi.runner(parallel, config)\n    except:\n        if view is not None:\n            from bcbio.distributed import ipython\n            ipython.stop(view)\n        raise\n    else:\n        for x in [\"cores_per_job\", \"num_jobs\", \"mem\"]:\n            parallel.pop(x, None)\n        if checkpoint_file:\n            with open(checkpoint_file, \"w\") as out_handle:\n                out_handle.write(\"done\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef recalibrate_quality(bam_file, sam_ref, dbsnp_file, picard_dir):\n    cl = [\"picard_gatk_recalibrate.py\", picard_dir, sam_ref, bam_file]\n    if dbsnp_file:\n        cl.append(dbsnp_file)\n    subprocess.check_call(cl)\n    out_file = glob.glob(\"%s*gatkrecal.bam\" % os.path.splitext(bam_file)[0])[0]\n    return out_file", "response": "Recalibrate alignments with GATK and provide pdf summary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming SNP genotyping and analysis using GATK.", "response": "def run_genotyper(bam_file, ref_file, dbsnp_file, config_file):\n    \"\"\"Perform SNP genotyping and analysis using GATK.\n    \"\"\"\n    cl = [\"gatk_genotyper.py\", config_file, ref_file, bam_file]\n    if dbsnp_file:\n        cl.append(dbsnp_file)\n    subprocess.check_call(cl)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving fastq files for the given lane and fc_name.", "response": "def get_fastq_files(directory, lane, fc_name):\n    \"\"\"Retrieve fastq files for the given lane, ready to process.\n    \"\"\"\n    files = glob.glob(os.path.join(directory, \"%s_*%s*txt*\" % (lane, fc_name)))\n    files.sort()\n    if len(files) > 2 or len(files) == 0:\n        raise ValueError(\"Did not find correct files for %s %s %s %s\" %\n                (directory, lane, fc_name, files))\n    ready_files = []\n    for fname in files:\n        if fname.endswith(\".gz\"):\n            cl = [\"gunzip\", fname]\n            subprocess.check_call(cl)\n            ready_files.append(os.path.splitext(fname)[0])\n        else:\n            ready_files.append(fname)\n    return ready_files[0], (ready_files[1] if len(ready_files) > 1 else None)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves the reference genome file location from the galaxy configuration.", "response": "def get_genome_ref(genome_build, aligner, galaxy_base):\n    \"\"\"Retrieve the reference genome file location from galaxy configuration.\n    \"\"\"\n    ref_files = dict(\n            bowtie = \"bowtie_indices.loc\",\n            bwa = \"bwa_index.loc\",\n            samtools = \"sam_fa_indices.loc\",\n            maq = \"bowtie_indices.loc\")\n    remap_fns = dict(\n            maq = _remap_to_maq\n            )\n    out_info = []\n    for ref_get in [aligner, \"samtools\"]:\n        ref_file = os.path.join(galaxy_base, \"tool-data\", ref_files[ref_get])\n        with open(ref_file) as in_handle:\n            for line in in_handle:\n                if not line.startswith(\"#\"):\n                    parts = line.strip().split()\n                    if parts[0] == \"index\":\n                        parts = parts[1:]\n                    if parts[0] == genome_build:\n                        out_info.append(parts[-1])\n                        break\n        try:\n            out_info[-1] = remap_fns[ref_get](out_info[-1])\n        except KeyError:\n            pass\n        except IndexError:\n            raise IndexError(\"Genome %s not found in %s\" % (genome_build,\n                ref_file))\n\n    if len(out_info) != 2:\n        raise ValueError(\"Did not find genome reference for %s %s\" %\n                (genome_build, aligner))\n    else:\n        return tuple(out_info)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _update_config_w_custom(config, lane_info):\n    config = copy.deepcopy(config)\n    custom = config[\"custom_algorithms\"].get(lane_info.get(\"analysis\", None),\n            None)\n    if custom:\n        for key, val in custom.iteritems():\n            config[\"algorithm\"][key] = val\n    return config", "response": "Update the configuration for this lane if a custom analysis is specified."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(items, background=None):\n    paired = vcfutils.get_paired_bams([x[\"align_bam\"] for x in items], items)\n    if not paired or not paired.normal_bam:\n        logger.warn(\"Battenberg only works on paired tumor/normal inputs, skipping %s\"\n                    % dd.get_sample_name(items[0]))\n        batout = None\n    elif not tz.get_in([\"genome_resources\", \"aliases\", \"human\"], paired.tumor_data):\n        logger.warn(\"Battenberg only works on human data, skipping %s\"\n                    % dd.get_sample_name(items[0]))\n        batout = None\n    else:\n        batout = _do_run(paired)\n        batout[\"variantcaller\"] = \"battenberg\"\n    out = []\n    for data in items:\n        if batout and dd.get_sample_name(data) == paired.tumor_name:\n            if \"sv\" not in data:\n                data[\"sv\"] = []\n            data[\"sv\"].append(batout)\n        out.append(data)\n    return out", "response": "Detect copy number variations from tumor and normal samples using Battenberg."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nperform Battenberg calibration with the paired dataset.", "response": "def _do_run(paired):\n    \"\"\"Perform Battenberg caling with the paired dataset.\n\n    This purposely does not use a temporary directory for the output\n    since Battenberg does smart restarts.\n    \"\"\"\n    work_dir = _sv_workdir(paired.tumor_data)\n    out = _get_battenberg_out(paired, work_dir)\n    ignore_file = os.path.join(work_dir, \"ignore_chromosomes.txt\")\n    if len(_missing_files(out)) > 0:\n        ref_file = dd.get_ref_file(paired.tumor_data)\n        bat_datadir = os.path.normpath(os.path.join(os.path.dirname(ref_file), os.pardir, \"battenberg\"))\n        ignore_file, gl_file = _make_ignore_file(work_dir, ref_file, ignore_file,\n                                                 os.path.join(bat_datadir, \"impute\", \"impute_info.txt\"))\n        tumor_bam = paired.tumor_bam\n        normal_bam = paired.normal_bam\n        platform = dd.get_platform(paired.tumor_data)\n        genome_build = paired.tumor_data[\"genome_build\"]\n        # scale cores to avoid over-using memory during imputation\n        cores = max(1, int(dd.get_num_cores(paired.tumor_data) * 0.5))\n        gender = {\"male\": \"XY\", \"female\": \"XX\", \"unknown\": \"L\"}.get(population.get_gender(paired.tumor_data))\n        if gender == \"L\":\n            gender_str = \"-ge %s -gl %s\" % (gender, gl_file)\n        else:\n            gender_str = \"-ge %s\" % (gender)\n        r_export_cmd = utils.get_R_exports()\n        local_sitelib = utils.R_sitelib()\n        cmd = (\"export R_LIBS_USER={local_sitelib} && {r_export_cmd} && \"\n               \"battenberg.pl -t {cores} -o {work_dir} -r {ref_file}.fai \"\n               \"-tb {tumor_bam} -nb {normal_bam} -e {bat_datadir}/impute/impute_info.txt \"\n               \"-u {bat_datadir}/1000genomesloci -c {bat_datadir}/probloci.txt \"\n               \"-ig {ignore_file} {gender_str} \"\n               \"-assembly {genome_build} -species Human -platform {platform}\")\n        do.run(cmd.format(**locals()), \"Battenberg CNV calling\")\n    assert len(_missing_files(out)) == 0, \"Missing Battenberg output: %s\" % _missing_files(out)\n    out[\"plot\"] = _get_battenberg_out_plots(paired, work_dir)\n    out[\"ignore\"] = ignore_file\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate input files with chromosomes to ignore and gender loci.", "response": "def _make_ignore_file(work_dir, ref_file, ignore_file, impute_file):\n    \"\"\"Create input files with chromosomes to ignore and gender loci.\n    \"\"\"\n    gl_file = os.path.join(work_dir, \"gender_loci.txt\")\n    chroms = set([])\n    with open(impute_file) as in_handle:\n        for line in in_handle:\n            chrom = line.split()[0]\n            chroms.add(chrom)\n            if not chrom.startswith(\"chr\"):\n                chroms.add(\"chr%s\" % chrom)\n    with open(ignore_file, \"w\") as out_handle:\n        for contig in ref.file_contigs(ref_file):\n            if contig.name not in chroms:\n                out_handle.write(\"%s\\n\" % contig.name)\n    with open(gl_file, \"w\") as out_handle:\n        for contig in ref.file_contigs(ref_file):\n            if contig.name in [\"Y\", \"chrY\"]:\n                # From https://github.com/cancerit/cgpBattenberg/blob/dev/perl/share/gender/GRCh37d5_Y.loci\n                positions = [2934912, 4546684, 4549638, 4550107]\n                for pos in positions:\n                    out_handle.write(\"%s\\t%s\\n\" % (contig.name, pos))\n    return ignore_file, gl_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncopies required files for processing using rsync potentially to a local server.", "response": "def copy_flowcell(dname, fastq_dir, sample_cfile, config):\n    \"\"\"Copy required files for processing using rsync, potentially to a remote server.\n    \"\"\"\n    with utils.chdir(dname):\n        reports = reduce(operator.add,\n                         [glob.glob(\"*.xml\"),\n                          glob.glob(\"Data/Intensities/BaseCalls/*.xml\"),\n                          glob.glob(\"Data/Intensities/BaseCalls/*.xsl\"),\n                          glob.glob(\"Data/Intensities/BaseCalls/*.htm\"),\n                          [\"Data/Intensities/BaseCalls/Plots\", \"Data/reports\",\n                           \"Data/Status.htm\", \"Data/Status_Files\", \"InterOp\"]])\n        run_info = reduce(operator.add,\n                          [glob.glob(\"run_info.yaml\"),\n                           glob.glob(\"*.csv\")])\n        fastq = glob.glob(os.path.join(fastq_dir.replace(dname + \"/\", \"\", 1),\n                                       \"*.gz\"))\n        configs = [sample_cfile.replace(dname + \"/\", \"\", 1)]\n    include_file = os.path.join(dname, \"transfer_files.txt\")\n    with open(include_file, \"w\") as out_handle:\n        out_handle.write(\"+ */\\n\")\n        for fname in configs + fastq + run_info + reports:\n            out_handle.write(\"+ %s\\n\" % fname)\n        out_handle.write(\"- *\\n\")\n    # remote transfer\n    if utils.get_in(config, (\"process\", \"host\")):\n        dest = \"%s@%s:%s\" % (utils.get_in(config, (\"process\", \"username\")),\n                             utils.get_in(config, (\"process\", \"host\")),\n                             utils.get_in(config, (\"process\", \"dir\")))\n    # local transfer\n    else:\n        dest = utils.get_in(config, (\"process\", \"dir\"))\n    cmd = [\"rsync\", \"-akmrtv\", \"--include-from=%s\" % include_file, dname, dest]\n    logger.info(\"Copying files to analysis machine\")\n    logger.info(\" \".join(cmd))\n    subprocess.check_call(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn umis command line argument with correct python and locale.", "response": "def _umis_cmd(data):\n    \"\"\"Return umis command line argument, with correct python and locale.\n    \"\"\"\n    return \"%s %s %s\" % (utils.locale_export(),\n                         utils.get_program_python(\"umis\"),\n                         config_utils.get_program(\"umis\", data[\"config\"], default=\"umis\"))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntransforming each read by identifying the barcode and UMI for each read and putting the information in the read name.", "response": "def umi_transform(data):\n    \"\"\"\n    transform each read by identifying the barcode and UMI for each read\n    and putting the information in the read name\n    \"\"\"\n    fqfiles = data[\"files\"]\n    fqfiles.extend(list(repeat(\"\", 4-len(fqfiles))))\n    fq1, fq2, fq3, fq4 = fqfiles\n    umi_dir = os.path.join(dd.get_work_dir(data), \"umis\")\n    safe_makedir(umi_dir)\n    transform = dd.get_umi_type(data)\n    if not transform:\n        logger.info(\"No UMI transform specified, assuming pre-transformed data.\")\n        if is_transformed(fq1):\n            logger.info(\"%s detected as pre-transformed, passing it on unchanged.\" % fq1)\n            data[\"files\"] = [fq1]\n            return [[data]]\n        else:\n            logger.error(\"No UMI transform was specified, but %s does not look \"\n                         \"pre-transformed.\" % fq1)\n            sys.exit(1)\n\n    if file_exists(transform):\n        transform_file = transform\n    else:\n        transform_file = get_transform_file(transform)\n        if not file_exists(transform_file):\n            logger.error(\n                \"The UMI transform can be specified as either a file or a \"\n                \"bcbio-supported transform. Either the file %s does not exist \"\n                \"or the transform is not supported by bcbio. Supported \"\n                \"transforms are %s.\"\n                %(dd.get_umi_type(data), \", \".join(SUPPORTED_TRANSFORMS)))\n            sys.exit(1)\n    out_base = dd.get_sample_name(data) + \".umitransformed.fq.gz\"\n    out_file = os.path.join(umi_dir, out_base)\n    if file_exists(out_file):\n        data[\"files\"] = [out_file]\n        return [[data]]\n    cellular_barcodes = get_cellular_barcodes(data)\n    if len(cellular_barcodes) > 1:\n        split_option = \"--separate_cb\"\n    else:\n        split_option = \"\"\n    if dd.get_demultiplexed(data):\n        demuxed_option = \"--demuxed_cb %s\" % dd.get_sample_name(data)\n        split_option = \"\"\n    else:\n        demuxed_option = \"\"\n    cores = dd.get_num_cores(data)\n    # skip transformation if the file already looks transformed\n    with open_fastq(fq1) as in_handle:\n        read = next(in_handle)\n        if \"UMI_\" in read:\n            data[\"files\"] = [out_file]\n            return [[data]]\n    locale_export = utils.locale_export()\n    umis = _umis_cmd(data)\n    cmd = (\"{umis} fastqtransform {split_option} {transform_file} \"\n           \"--cores {cores} {demuxed_option} \"\n           \"{fq1} {fq2} {fq3} {fq4}\"\n           \"| seqtk seq -L 20 - | gzip > {tx_out_file}\")\n    message = (\"Inserting UMI and barcode information into the read name of %s\"\n               % fq1)\n    with file_transaction(out_file) as tx_out_file:\n        do.run(cmd.format(**locals()), message)\n    data[\"files\"] = [out_file]\n    return [[data]]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef demultiplex_samples(data):\n    work_dir = os.path.join(dd.get_work_dir(data), \"umis\")\n    sample_dir = os.path.join(work_dir, dd.get_sample_name(data))\n    demulti_dir = os.path.join(sample_dir, \"demultiplexed\")\n\n    files = data[\"files\"]\n    if len(files) == 2:\n        logger.error(\"Sample demultiplexing doesn't handle paired-end reads, but \"\n            \"we can add it. Open an issue here https://github.com/bcbio/bcbio-nextgen/issues if you need this and we'll add it.\")\n        sys.exit(1)\n    else:\n        fq1 = files[0]\n    # check if samples need to be demultiplexed\n    with open_fastq(fq1) as in_handle:\n        read = next(in_handle)\n        if \"SAMPLE_\" not in read:\n            return [[data]]\n\n    bcfile = get_sample_barcodes(dd.get_sample_barcodes(data), sample_dir)\n    demultiplexed = glob.glob(os.path.join(demulti_dir, \"*.fq*\"))\n    if demultiplexed:\n        return [split_demultiplexed_sampledata(data, demultiplexed)]\n    umis = _umis_cmd(data)\n    cmd = (\"{umis} demultiplex_samples --nedit 1 --barcodes {bcfile} \"\n           \"--out_dir {tx_dir} {fq1}\")\n    msg = \"Demultiplexing {fq1}.\"\n    with file_transaction(data, demulti_dir) as tx_dir:\n        do.run(cmd.format(**locals()), msg.format(**locals()))\n    demultiplexed = glob.glob(os.path.join(demulti_dir, \"*.fq*\"))\n    return [split_demultiplexed_sampledata(data, demultiplexed)]", "response": "Demultiplex a fastqtransformed FASTQ file into separate sample barcode files"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef split_demultiplexed_sampledata(data, demultiplexed):\n    datadicts = []\n    samplename = dd.get_sample_name(data)\n    for fastq in demultiplexed:\n        barcode = os.path.basename(fastq).split(\".\")[0]\n        datadict = copy.deepcopy(data)\n        datadict = dd.set_sample_name(datadict, samplename + \"-\" + barcode)\n        datadict = dd.set_description(datadict, samplename + \"-\" + barcode)\n        datadict[\"rgnames\"][\"rg\"] = samplename + \"-\" + barcode\n        datadict[\"name\"]= [\"\", samplename + \"-\" + barcode]\n        datadict[\"files\"] = [fastq]\n        datadicts.append(datadict)\n    return datadicts", "response": "splits demultiplexed samples into separate sample data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if a FASTQ file has already been transformed by umis", "response": "def is_transformed(fastq):\n    \"\"\"\n    check the first 100 reads to see if a FASTQ file has already been transformed\n    by umis\n    \"\"\"\n\n    with open_fastq(fastq) as in_handle:\n        for line in islice(in_handle, 400):\n            if \"UMI_\" in line:\n                return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading a sparse matrix loading row and column name files. if rowprefix is specified will add a prefix to the row or column names", "response": "def read(self, filename, rowprefix=None, colprefix=None, delim=\":\"):\n        \"\"\"read a sparse matrix, loading row and column name files. if\n        specified, will add a prefix to the row or column names\"\"\"\n\n        self.matrix = scipy.io.mmread(filename)\n        with open(filename + \".rownames\") as in_handle:\n            self.rownames = [x.strip() for x in in_handle]\n            if rowprefix:\n                self.rownames = [rowprefix + delim + x for x in self.rownames]\n        with open(filename + \".colnames\") as in_handle:\n            self.colnames = [x.strip() for x in in_handle]\n            if colprefix:\n                self.colnames = [colprefix + delim + x for x in self.colnames]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads a sparse matrix loading row and column name files", "response": "def write(self, filename):\n        \"\"\"read a sparse matrix, loading row and column name files\"\"\"\n        if file_exists(filename):\n            return filename\n        out_files = [filename, filename + \".rownames\", filename + \".colnames\"]\n        with file_transaction(out_files) as tx_out_files:\n            with open(tx_out_files[0], \"wb\") as out_handle:\n                scipy.io.mmwrite(out_handle, scipy.sparse.csr_matrix(self.matrix))\n            pd.Series(self.rownames).to_csv(tx_out_files[1], index=False)\n            pd.Series(self.colnames).to_csv(tx_out_files[2], index=False)\n        return filename"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef trim_srna_sample(data):\n    data = umi_transform(data)\n    in_file = data[\"files\"][0]\n    names = data[\"rgnames\"]['sample']\n    work_dir = os.path.join(dd.get_work_dir(data), \"trimmed\")\n    out_dir = os.path.join(work_dir, names)\n    log_out = os.path.join(out_dir, \"%s.log\" % names)\n    utils.safe_makedir(out_dir)\n    out_file = replace_directory(append_stem(in_file, \".clean\"), out_dir)\n    trim_reads = data[\"config\"][\"algorithm\"].get(\"trim_reads\", True)\n    if utils.file_exists(out_file):\n        data[\"files\"][0] = out_file\n        data[\"clean_fastq\"] = out_file\n        data[\"collapse\"] = _collapse(data[\"clean_fastq\"])\n        data[\"size_stats\"] = _summary(data['collapse'])\n        data[\"log_trimming\"] = log_out\n        return [[data]]\n\n    adapter = dd.get_adapters(data)\n    is_4n = any([a == \"4N\" for a in adapter])\n    adapter = [a for a in adapter if re.compile(\"^([NATGC]+)$\").match(a)]\n    if adapter and not trim_reads:\n        trim_reads = True\n        logger.info(\"Adapter is set up in config file, but trim_reads is not true.\"\n                    \"If you want to skip trimming, skip adapter option from config.\")\n    if trim_reads and not adapter and error_dnapi:\n        raise ValueError(error_dnapi)\n    if trim_reads:\n        adapters = adapter if adapter else _dnapi_prediction(in_file, out_dir)\n    times = \"\" if not trim_reads or len(adapters) == 1 else \"--times %s\" % len(adapters)\n    if trim_reads and adapters:\n        adapter_cmd = \" \".join(map(lambda x: \"-a \" + x, adapters))\n        if any([a for a in adapters if re.compile(\"^N+$\").match(a)]):\n            adapter_cmd = \"-N %s\" % adapter_cmd\n        out_noadapter_file = replace_directory(append_stem(in_file, \".fragments\"), out_dir)\n        out_short_file = replace_directory(append_stem(in_file, \".short\"), out_dir)\n        # atropos = _get_atropos()\n        atropos = config_utils.get_program(\"atropos\", data, default=\"atropos\")\n        options = \" \".join(data.get('resources', {}).get('atropos', {}).get(\"options\", \"\"))\n        if options.strip() == \"-u 4 -u -4\":\n            options = \"\"\n            is_4n = \"4N\"\n        cores = (\"--threads %s\" % dd.get_num_cores(data) if dd.get_num_cores(data) > 1 else \"\")\n        if \" \".join(data.get('resources', {}).get('cutadapt', {}).get(\"options\", \"\")):\n            raise ValueError(\"Atropos is now used, but cutadapt options found in YAML file.\"\n                             \"See https://atropos.readthedocs.io/en/latest/\")\n        cmd = _cmd_atropos()\n        if not utils.file_exists(out_file):\n            with file_transaction(out_file) as tx_out_file:\n                do.run(cmd.format(**locals()), \"remove adapter for %s\" % names)\n                if utils.file_exists(log_out):\n                    content = open(log_out).read().replace(out_short_file, names)\n                    open(log_out, 'w').write(content)\n                if is_4n:\n                    options = \"-u 4 -u -4\"\n                    in_file = append_stem(tx_out_file, \".tmp\")\n                    utils.move_safe(tx_out_file, in_file)\n                    cmd = \"{atropos} {cores} {options} -se {in_file} -o {tx_out_file} -m 17\"\n                    do.run(cmd.format(**locals()), \"atropos with this parameters %s for %s\" %(options, names))\n        data[\"log_trimming\"] = log_out\n    else:\n        if not trim_reads:\n            logger.debug(\"Skip trimming for: %s\" % names)\n        elif not adapters:\n            logger.info(\"No adapter founds in %s, this is an issue related\"\n                        \" to no small RNA enrichment in your sample.\" % names)\n        symlink_plus(in_file, out_file)\n    data[\"files\"][0] = out_file\n    data[\"clean_fastq\"] = out_file\n    data[\"collapse\"] = _collapse(data[\"clean_fastq\"])\n    data[\"size_stats\"] = _summary(data['collapse'])\n    return [[data]]", "response": "Remove 3 adapter for smallRNA - seq\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nannotates miRNAs using seqbuster tool", "response": "def sample_annotation(data):\n    \"\"\"\n    Annotate miRNAs using miRBase database with seqbuster tool\n    \"\"\"\n    names = data[\"rgnames\"]['sample']\n    tools = dd.get_expression_caller(data)\n    work_dir = os.path.join(dd.get_work_dir(data), \"mirbase\")\n    out_dir = os.path.join(work_dir, names)\n    utils.safe_makedir(out_dir)\n    out_file = op.join(out_dir, names)\n    if dd.get_mirbase_hairpin(data):\n        mirbase = op.abspath(op.dirname(dd.get_mirbase_hairpin(data)))\n        if utils.file_exists(data[\"collapse\"]):\n            data['transcriptome_bam'] = _align(data[\"collapse\"],\n                                               dd.get_mirbase_hairpin(data),\n                                               out_file,\n                                               data)\n            data['seqbuster'] = _miraligner(data[\"collapse\"], out_file,\n                                            dd.get_species(data),\n                                            mirbase,\n                                            data['config'])\n            data[\"mirtop\"] = _mirtop(data['seqbuster'],\n                                     dd.get_species(data),\n                                     mirbase,\n                                     out_dir,\n                                     data['config'])\n        else:\n            logger.debug(\"Trimmed collapsed file is empty for %s.\" % names)\n    else:\n        logger.debug(\"No annotation file from miRBase.\")\n\n    sps = dd.get_species(data) if dd.get_species(data) else \"None\"\n    logger.debug(\"Looking for mirdeep2 database for %s\" % names)\n    if file_exists(op.join(dd.get_work_dir(data), \"mirdeep2\", \"novel\", \"hairpin.fa\")):\n        data['seqbuster_novel'] = _miraligner(data[\"collapse\"], \"%s_novel\" % out_file, sps,\n                                              op.join(dd.get_work_dir(data),\n                                                      \"mirdeep2\", \"novel\"),\n                                              data['config'])\n\n    if \"trna\" in tools:\n        data['trna'] = _mint_trna_annotation(data)\n\n    data = spikein.counts_spikein(data)\n    return [[data]]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncutting the beginning of the reads to avoid detection of miRNAs", "response": "def _prepare_file(fn, out_dir):\n    \"\"\"Cut the beginning of the reads to avoid detection of miRNAs\"\"\"\n    atropos = _get_atropos()\n    cmd = \"{atropos} trim --max-reads 500000 -u 22 -se {fn} -o {tx_file}\"\n    out_file = os.path.join(out_dir, append_stem(os.path.basename(fn), \"end\"))\n    if file_exists(out_file):\n        return out_file\n    with file_transaction(out_file) as tx_file:\n        do.run(cmd.format(**locals()))\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _collapse(in_file):\n    seqcluster = op.join(utils.get_bcbio_bin(), \"seqcluster\")\n    out_file = \"%s.fastq\" % utils.splitext_plus(append_stem(in_file, \"_trimmed\"))[0]\n    out_dir = os.path.dirname(in_file)\n    if file_exists(out_file):\n        return out_file\n    cmd = (\"{seqcluster} collapse -o {out_dir} -f {in_file} -m 1 --min_size 16\")\n    do.run(cmd.format(**locals()), \"Running seqcluster collapse in %s.\" % in_file)\n    return out_file", "response": "Collpase reads into unique sequences with seqcluster"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _summary(in_file):\n    data = Counter()\n    out_file = in_file + \"_size_stats\"\n    if file_exists(out_file):\n        return out_file\n    with open(in_file) as in_handle:\n        for line in in_handle:\n            counts = int(line.strip().split(\"_x\")[1])\n            line = next(in_handle)\n            l = len(line.strip())\n            next(in_handle)\n            next(in_handle)\n            data[l] += counts\n    with file_transaction(out_file) as tx_out_file:\n        with open(tx_out_file, 'w') as out_handle:\n            for l, c in data.items():\n                out_handle.write(\"%s %s\\n\" % (l, c))\n    return out_file", "response": "Calculate size distribution after adapter removal"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning miraligner tool to align a fastq file with the specified species.", "response": "def _miraligner(fastq_file, out_file, species, db_folder, config):\n    \"\"\"\n    Run miraligner tool (from seqcluster suit) with default\n    parameters.\n    \"\"\"\n    resources = config_utils.get_resources(\"miraligner\", config)\n    miraligner = config_utils.get_program(\"miraligner\", config)\n    jvm_opts =  \"-Xms750m -Xmx4g\"\n    if resources and resources.get(\"jvm_opts\"):\n        jvm_opts = \" \".join(resources.get(\"jvm_opts\"))\n    export = _get_env()\n    cmd = (\"{export} {miraligner} {jvm_opts} -freq -sub 1 -trim 3 -add 3 -minl 16\"\n           \" -s {species} -i {fastq_file} -db {db_folder}  -o {tx_out_file}\")\n    if not file_exists(out_file + \".mirna\"):\n        with file_transaction(out_file) as tx_out_file:\n            do.run(cmd.format(**locals()), \"Do miRNA annotation for %s\" % fastq_file)\n            shutil.move(tx_out_file + \".mirna\", out_file + \".mirna\")\n    return out_file + \".mirna\""}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a miRNA input file to GFF3 format", "response": "def _mirtop(input_fn, sps, db, out_dir, config):\n    \"\"\"\n    Convert to GFF3 standard format\n    \"\"\"\n    hairpin = os.path.join(db, \"hairpin.fa\")\n    gtf = os.path.join(db, \"mirbase.gff3\")\n    if not file_exists(hairpin) or not file_exists(gtf):\n        logger.warning(\"%s or %s are not installed. Skipping.\" % (hairpin, gtf))\n        return None\n    out_gtf_fn = \"%s.gtf\" % utils.splitext_plus(os.path.basename(input_fn))[0]\n    out_gff_fn = \"%s.gff\" % utils.splitext_plus(os.path.basename(input_fn))[0]\n    export = _get_env()\n    cmd = (\"{export} mirtop gff  --sps {sps} --hairpin {hairpin} \"\n           \"--gtf {gtf} --format seqbuster -o {out_tx} {input_fn}\")\n    if not file_exists(os.path.join(out_dir, out_gtf_fn)) and \\\n       not file_exists(os.path.join(out_dir, out_gff_fn)):\n        with tx_tmpdir() as out_tx:\n            do.run(cmd.format(**locals()), \"Do miRNA annotation for %s\" % input_fn)\n            with utils.chdir(out_tx):\n                out_fn = out_gtf_fn if utils.file_exists(out_gtf_fn) \\\n                                    else out_gff_fn\n                if utils.file_exists(out_fn):\n                    shutil.move(os.path.join(out_tx, out_fn),\n                                os.path.join(out_dir, out_fn))\n    out_fn = out_gtf_fn if utils.file_exists(os.path.join(out_dir, out_gtf_fn)) \\\n                        else os.path.join(out_dir, out_gff_fn)\n    if utils.file_exists(os.path.join(out_dir, out_fn)):\n        return os.path.join(out_dir, out_fn)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun TdrMapper on the input fastq file and return the base directory of the tRNAs", "response": "def _trna_annotation(data):\n    \"\"\"\n    use tDRmapper to quantify tRNAs\n    \"\"\"\n    trna_ref = op.join(dd.get_srna_trna_file(data))\n    name = dd.get_sample_name(data)\n    work_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"trna\", name))\n    in_file = op.basename(data[\"clean_fastq\"])\n    tdrmapper = os.path.join(os.path.dirname(sys.executable), \"TdrMappingScripts.pl\")\n    perl_export = utils.get_perl_exports()\n    if not file_exists(trna_ref) or not file_exists(tdrmapper):\n        logger.info(\"There is no tRNA annotation to run TdrMapper.\")\n        return work_dir\n    out_file = op.join(work_dir, in_file + \".hq_cs.mapped\")\n    if not file_exists(out_file):\n        with tx_tmpdir(data) as txdir:\n            with utils.chdir(txdir):\n                utils.symlink_plus(data[\"clean_fastq\"], op.join(txdir, in_file))\n                cmd = (\"{perl_export} && perl {tdrmapper} {trna_ref} {in_file}\").format(**locals())\n                do.run(cmd, \"tRNA for %s\" % name)\n                for filename in glob.glob(\"*mapped*\"):\n                    shutil.move(filename, work_dir)\n    return work_dir"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _mint_trna_annotation(data):\n    trna_lookup = op.join(dd.get_srna_mint_lookup(data))\n    trna_space = op.join(dd.get_srna_mint_space(data))\n    trna_other = op.join(dd.get_srna_mint_other(data))\n    name = dd.get_sample_name(data)\n    work_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"trna_mint\", name))\n    in_file = op.basename(data[\"clean_fastq\"])\n    mintmap = os.path.realpath(os.path.join(os.path.dirname(sys.executable), \"MINTmap.pl\"))\n    perl_export = utils.get_perl_exports()\n    if not file_exists(trna_lookup) or not file_exists(mintmap):\n        logger.info(\"There is no tRNA annotation to run MINTmap.\")\n        return work_dir\n    jar_folder = os.path.join(os.path.dirname(mintmap), \"MINTplates\")\n    out_file = op.join(work_dir, name + \"-MINTmap_v1-exclusive-tRFs.expression.txt\")\n    if not file_exists(out_file):\n        with tx_tmpdir(data) as txdir:\n            with utils.chdir(txdir):\n                utils.symlink_plus(data[\"clean_fastq\"], op.join(txdir, in_file))\n                cmd = (\"{perl_export} && {mintmap} -f {in_file} -p {name} \"\n                       \"-l {trna_lookup} -s {trna_space} -j {jar_folder} \"\n                       \"-o {trna_other}\").format(**locals())\n                do.run(cmd, \"tRNA for %s\" % name)\n                for filename in glob.glob(\"*MINTmap*\"):\n                    shutil.move(filename, work_dir)\n    return work_dir", "response": "run MINTmap to quantify tRNAs"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dict of the lengths of sequences in a fasta file", "response": "def sequence_length(fasta):\n    \"\"\"\n    return a dict of the lengths of sequences in a fasta file\n    \"\"\"\n    sequences = SeqIO.parse(fasta, \"fasta\")\n    records = {record.id: len(record) for record in sequences}\n    return records"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of the sequence names in a FASTA file", "response": "def sequence_names(fasta):\n    \"\"\"\n    return a list of the sequence IDs in a FASTA file\n    \"\"\"\n    sequences = SeqIO.parse(fasta, \"fasta\")\n    records = [record.id for record in sequences]\n    return records"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstrips transcript versions from a FASTA file. these appear like this : A > C > C > E", "response": "def strip_transcript_versions(fasta, out_file):\n    \"\"\"\n    strip transcript versions from a FASTA file. these appear like this:\n    >ENST00000434970.2 cdna chromosome:GRCh38:14:22439007:22439015:1 etc\n    \"\"\"\n    if file_exists(out_file):\n        return out_file\n    with file_transaction(out_file) as tx_out_file:\n        with open(tx_out_file, \"w\") as out_handle:\n            with open(fasta) as in_handle:\n                for line in in_handle:\n                    if line.startswith(\">\"):\n                        out_handle.write(line.split(\" \")[0].split(\".\")[0] + \"\\n\")\n                    else:\n                        out_handle.write(line)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _prepare_inputs(ma_fn, bam_file, out_dir):\n    fixed_fa = os.path.join(out_dir, \"file_reads.fa\")\n    count_name =dict()\n    with file_transaction(fixed_fa) as out_tx:\n        with open(out_tx, 'w') as out_handle:\n            with open(ma_fn) as in_handle:\n                h = next(in_handle)\n                for line in in_handle:\n                    cols = line.split(\"\\t\")\n                    name_with_counts = \"%s_x%s\" % (cols[0], sum(map(int, cols[2:])))\n                    count_name[cols[0]] = name_with_counts\n                    out_handle.write(\">%s\\n%s\\n\" % (name_with_counts, cols[1]))\n    fixed_bam = os.path.join(out_dir, \"align.bam\")\n    bam_handle = pysam.AlignmentFile(bam_file, \"rb\")\n    with pysam.AlignmentFile(fixed_bam, \"wb\", template=bam_handle) as out_handle:\n        for read in bam_handle.fetch():\n            read.query_name = count_name[read.query_name]\n            out_handle.write(read)\n\n    return fixed_fa, fixed_bam", "response": "Convert to fastq with counts\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_novel(csv_file, sps=\"new\"):\n    read = 0\n    seen = set()\n    safe_makedir(\"novel\")\n    with open(\"novel/hairpin.fa\", \"w\") as fa_handle, open(\"novel/miRNA.str\", \"w\") as str_handle:\n        with open(csv_file) as in_handle:\n            for line in in_handle:\n                if line.startswith(\"mature miRBase miRNAs detected by miRDeep2\"):\n                    break\n                if line.startswith(\"novel miRNAs predicted\"):\n                    read = 1\n                    line = next(in_handle)\n                    continue\n                if read and line.strip():\n                    cols = line.strip().split(\"\\t\")\n                    name, start, score = cols[0], cols[16], cols[1]\n                    if float(score) < 1:\n                        continue\n                    m5p, m3p, pre = cols[13], cols[14], cols[15].replace('u', 't').upper()\n                    m5p_start = cols[15].find(m5p) + 1\n                    m3p_start = cols[15].find(m3p) + 1\n                    m5p_end = m5p_start + len(m5p) - 1\n                    m3p_end = m3p_start + len(m3p) - 1\n                    if m5p in seen:\n                        continue\n                    fa_handle.write(\">{sps}-{name} {start}\\n{pre}\\n\".format(**locals()))\n                    str_handle.write(\">{sps}-{name} ({score}) [{sps}-{name}-5p:{m5p_start}-{m5p_end}] [{sps}-{name}-3p:{m3p_start}-{m3p_end}]\\n\".format(**locals()))\n                    seen.add(m5p)\n    return op.abspath(\"novel\")", "response": "Create input of novel miRNAs from miRDeep2"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef align(fastq_file, pair_file, index_dir, names, align_dir, data):\n    umi_ext = \"-cumi\" if \"umi_bam\" in data else \"\"\n    out_file = os.path.join(align_dir, \"{0}-sort{1}.bam\".format(dd.get_sample_name(data), umi_ext))\n    num_cores = data[\"config\"][\"algorithm\"].get(\"num_cores\", 1)\n    rg_info = \"rgid={rg} rgpl={pl} rgpu={pu} rgsm={sample}\".format(**names)\n    pair_file = pair_file if pair_file else \"\"\n    final_file = None\n    if data.get(\"align_split\"):\n        # BBMap does not accept input fastq streams\n        raise ValueError(\"bbmap is not compatible with alignment splitting, set `align_split: false`\")\n    pair_arg = \"in2=%s\" % pair_file if pair_file else \"\"\n    if not utils.file_exists(out_file) and (final_file is None or not utils.file_exists(final_file)):\n        with postalign.tobam_cl(data, out_file, pair_file != \"\") as (tobam_cl, tx_out_file):\n            if index_dir.endswith((\"/ref\", \"/ref/\")):\n                index_dir = os.path.dirname(index_dir)\n            # sam=1.3 required for compatibility with strelka2\n            cmd = (\"bbmap.sh sam=1.3 mdtag=t {rg_info} path={index_dir} in1={fastq_file} \"\n                   \"{pair_arg} out=stdout.sam | \")\n            do.run(cmd.format(**locals()) + tobam_cl, \"bbmap alignment: %s\" % dd.get_sample_name(data))\n    data[\"work_bam\"] = out_file\n    return data", "response": "Perform piped alignment of fastq files generating sorted deduplicated BAM."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns smoove on a set of samples.", "response": "def _run_smoove(full_bams, sr_bams, disc_bams, work_dir, items):\n    \"\"\"Run lumpy-sv using smoove.\n    \"\"\"\n    batch = sshared.get_cur_batch(items)\n    ext = \"-%s-svs\" % batch if batch else \"-svs\"\n    name = \"%s%s\" % (dd.get_sample_name(items[0]), ext)\n    out_file = os.path.join(work_dir, \"%s-smoove.genotyped.vcf.gz\" % name)\n    sv_exclude_bed = sshared.prepare_exclude_file(items, out_file)\n    old_out_file = os.path.join(work_dir, \"%s%s-prep.vcf.gz\"\n                                % (os.path.splitext(os.path.basename(items[0][\"align_bam\"]))[0], ext))\n    if utils.file_exists(old_out_file):\n        return old_out_file, sv_exclude_bed\n    if not utils.file_exists(out_file):\n        with file_transaction(items[0], out_file) as tx_out_file:\n            cores = dd.get_num_cores(items[0])\n            out_dir = os.path.dirname(tx_out_file)\n            ref_file = dd.get_ref_file(items[0])\n            full_bams = \" \".join(_prepare_smoove_bams(full_bams, sr_bams, disc_bams, items,\n                                                      os.path.dirname(tx_out_file)))\n            std_excludes = [\"~^GL\", \"~^HLA\", \"~_random\", \"~^chrUn\", \"~alt\", \"~decoy\"]\n            def _is_std_exclude(n):\n                clean_excludes = [x.replace(\"~\", \"\").replace(\"^\", \"\") for x in std_excludes]\n                return any([n.startswith(x) or n.endswith(x) for x in clean_excludes])\n            exclude_chrs = [c.name for c in ref.file_contigs(ref_file)\n                            if not chromhacks.is_nonalt(c.name) and not _is_std_exclude(c.name)]\n            exclude_chrs = \"--excludechroms '%s'\" % \",\".join(std_excludes + exclude_chrs)\n            exclude_bed = (\"--exclude %s\" % sv_exclude_bed) if utils.file_exists(sv_exclude_bed) else \"\"\n            tempdir = os.path.dirname(tx_out_file)\n            cmd = (\"export TMPDIR={tempdir} && \"\n                   \"smoove call --processes {cores} --genotype --removepr --fasta {ref_file} \"\n                   \"--name {name} --outdir {out_dir} \"\n                   \"{exclude_bed} {exclude_chrs} {full_bams}\")\n            with utils.chdir(tempdir):\n                try:\n                    do.run(cmd.format(**locals()), \"smoove lumpy calling\", items[0])\n                except subprocess.CalledProcessError as msg:\n                    if _allowed_errors(msg):\n                        vcfutils.write_empty_vcf(tx_out_file, config=items[0][\"config\"],\n                                                 samples=[dd.get_sample_name(d) for d in items])\n                    else:\n                        logger.exception()\n                        raise\n    vcfutils.bgzip_and_index(out_file, items[0][\"config\"])\n    return out_file, sv_exclude_bed"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _prepare_smoove_bams(full_bams, sr_bams, disc_bams, items, tx_work_dir):\n    input_dir = utils.safe_makedir(tx_work_dir)\n    out = []\n    for full, sr, disc, data in zip(full_bams, sr_bams, disc_bams, items):\n        if sr and disc:\n            new_full = os.path.join(input_dir, \"%s.bam\" % dd.get_sample_name(data))\n            new_sr = os.path.join(input_dir, \"%s.split.bam\" % dd.get_sample_name(data))\n            new_disc = os.path.join(input_dir, \"%s.disc.bam\" % dd.get_sample_name(data))\n            utils.symlink_plus(full, new_full)\n            utils.symlink_plus(sr, new_sr)\n            utils.symlink_plus(disc, new_disc)\n            out.append(new_full)\n        else:\n            out.append(full)\n    return out", "response": "Prepare BAMs for smoove."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _filter_by_support(in_file, data):\n    rc_filter = (\"FORMAT/SU < 4 || \"\n                 \"(FORMAT/SR == 0 && FORMAT/SU < 15 && ABS(SVLEN)>50000) || \"\n                 \"(FORMAT/SR == 0 && FORMAT/SU < 5 && ABS(SVLEN)<2000) || \"\n                 \"(FORMAT/SR == 0 && FORMAT/SU < 15 && ABS(SVLEN)<300)\")\n    return vfilter.cutoff_w_expression(in_file, rc_filter, data, name=\"ReadCountSupport\",\n                                       limit_regions=None)", "response": "Filter the file based on read count support."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _filter_by_background(base_name, back_samples, gt_vcfs, data):\n    filtname = \"InBackground\"\n    filtdoc = \"Variant also present in background samples with same genotype\"\n    orig_vcf = gt_vcfs[base_name]\n    out_file = \"%s-backfilter.vcf\" % (utils.splitext_plus(orig_vcf)[0])\n    if not utils.file_exists(out_file) and not utils.file_exists(out_file + \".gz\"):\n        with file_transaction(data, out_file) as tx_out_file:\n            with utils.open_gzipsafe(orig_vcf) as in_handle:\n                inp = vcf.Reader(in_handle, orig_vcf)\n                inp.filters[filtname] = vcf.parser._Filter(filtname, filtdoc)\n                with open(tx_out_file, \"w\") as out_handle:\n                    outp = vcf.Writer(out_handle, inp)\n                    for rec in inp:\n                        if _genotype_in_background(rec, base_name, back_samples):\n                            rec.add_filter(filtname)\n                        outp.write_record(rec)\n    if utils.file_exists(out_file + \".gz\"):\n        out_file = out_file + \".gz\"\n    gt_vcfs[base_name] = vcfutils.bgzip_and_index(out_file, data[\"config\"])\n    return gt_vcfs", "response": "Filter base samples by background samples."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _genotype_in_background(rec, base_name, back_samples):\n    def passes(rec):\n        return not rec.FILTER or len(rec.FILTER) == 0\n    return (passes(rec) and\n            any(rec.genotype(base_name).gt_alleles == rec.genotype(back_name).gt_alleles\n                for back_name in back_samples))", "response": "Check if the genotype in the background record is present in the background records."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms structural variations with lumpy.", "response": "def run(items):\n    \"\"\"Perform detection of structural variations with lumpy.\n    \"\"\"\n    paired = vcfutils.get_paired(items)\n    work_dir = _sv_workdir(paired.tumor_data if paired and paired.tumor_data else items[0])\n    previous_evidence = {}\n    full_bams, sr_bams, disc_bams = [], [], []\n    for data in items:\n        full_bams.append(dd.get_align_bam(data))\n        sr_bam, disc_bam = sshared.find_existing_split_discordants(data)\n        sr_bams.append(sr_bam)\n        disc_bams.append(disc_bam)\n        cur_dels, cur_dups = _bedpes_from_cnv_caller(data, work_dir)\n        previous_evidence[dd.get_sample_name(data)] = {}\n        if cur_dels and utils.file_exists(cur_dels):\n            previous_evidence[dd.get_sample_name(data)][\"dels\"] = cur_dels\n        if cur_dups and utils.file_exists(cur_dups):\n            previous_evidence[dd.get_sample_name(data)][\"dups\"] = cur_dups\n    lumpy_vcf, exclude_file = _run_smoove(full_bams, sr_bams, disc_bams, work_dir, items)\n    lumpy_vcf = sshared.annotate_with_depth(lumpy_vcf, items)\n    gt_vcfs = {}\n    # Retain paired samples with tumor/normal genotyped in one file\n    if paired and paired.normal_name:\n        batches = [[paired.tumor_data, paired.normal_data]]\n    else:\n        batches = [[x] for x in items]\n\n    for batch_items in batches:\n        for data in batch_items:\n            gt_vcfs[dd.get_sample_name(data)] = _filter_by_support(lumpy_vcf, data)\n    if paired and paired.normal_name:\n        gt_vcfs = _filter_by_background(paired.tumor_name, [paired.normal_name], gt_vcfs, paired.tumor_data)\n    out = []\n    upload_counts = collections.defaultdict(int)\n    for data in items:\n        if \"sv\" not in data:\n            data[\"sv\"] = []\n        vcf_file = gt_vcfs.get(dd.get_sample_name(data))\n        if vcf_file:\n            effects_vcf, _ = effects.add_to_vcf(vcf_file, data, \"snpeff\")\n            data[\"sv\"].append({\"variantcaller\": \"lumpy\",\n                               \"vrn_file\": effects_vcf or vcf_file,\n                               \"do_upload\": upload_counts[vcf_file] == 0,  # only upload a single file per batch\n                               \"exclude_file\": exclude_file})\n            upload_counts[vcf_file] += 1\n        out.append(data)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving BEDPEs deletion and duplications from CNV callers.", "response": "def _bedpes_from_cnv_caller(data, work_dir):\n    \"\"\"Retrieve BEDPEs deletion and duplications from CNV callers.\n\n    Currently integrates with CNVkit.\n    \"\"\"\n    supported = set([\"cnvkit\"])\n    cns_file = None\n    for sv in data.get(\"sv\", []):\n        if sv[\"variantcaller\"] in supported and \"cns\" in sv and \"lumpy_usecnv\" in dd.get_tools_on(data):\n            cns_file = sv[\"cns\"]\n            break\n    if not cns_file:\n        return None, None\n    else:\n        out_base = os.path.join(work_dir, utils.splitext_plus(os.path.basename(cns_file))[0])\n        out_dels = out_base + \"-dels.bedpe\"\n        out_dups = out_base + \"-dups.bedpe\"\n        if not os.path.exists(out_dels) or not os.path.exists(out_dups):\n            with file_transaction(data, out_dels, out_dups) as (tx_out_dels, tx_out_dups):\n                try:\n                    cnvanator_path = config_utils.get_program(\"cnvanator_to_bedpes.py\", data)\n                except config_utils.CmdNotFound:\n                    return None, None\n                cmd = [cnvanator_path, \"-c\", cns_file, \"--cnvkit\",\n                        \"--del_o=%s\" % tx_out_dels, \"--dup_o=%s\" % tx_out_dups,\n                        \"-b\", \"250\"]  # XXX Uses default piece size for CNVkit. Right approach?\n                do.run(cmd, \"Prepare CNVkit as input for lumpy\", data)\n        return out_dels, out_dups"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving bcbio internal path from first element in PATH.", "response": "def remove_bcbiopath():\n    \"\"\"Remove bcbio internal path from first element in PATH.\n\n    Useful when we need to access remote programs, like Java 7 for older\n    installations.\n    \"\"\"\n    to_remove = os.environ.get(\"BCBIOPATH\", utils.get_bcbio_bin()) + \":\"\n    if os.environ[\"PATH\"].startswith(to_remove):\n        os.environ[\"PATH\"] = os.environ[\"PATH\"][len(to_remove):]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calculate_sv_bins(*items):\n    calcfns = {\"cnvkit\": _calculate_sv_bins_cnvkit, \"gatk-cnv\": _calculate_sv_bins_gatk}\n    from bcbio.structural import cnvkit\n    items = [utils.to_single_data(x) for x in cwlutils.handle_combined_input(items)]\n    if all(not cnvkit.use_general_sv_bins(x) for x in items):\n        return [[d] for d in items]\n    out = []\n    for i, cnv_group in enumerate(_group_by_cnv_method(multi.group_by_batch(items, False))):\n        size_calc_fn = MemoizedSizes(cnv_group.region_file, cnv_group.items).get_target_antitarget_bin_sizes\n        for data in cnv_group.items:\n            if cnvkit.use_general_sv_bins(data):\n                target_bed, anti_bed, gcannotated_tsv = calcfns[cnvkit.bin_approach(data)](data, cnv_group,\n                                                                                           size_calc_fn)\n                if not data.get(\"regions\"):\n                    data[\"regions\"] = {}\n                data[\"regions\"][\"bins\"] = {\"target\": target_bed, \"antitarget\": anti_bed, \"group\": str(i),\n                                           \"gcannotated\": gcannotated_tsv}\n            out.append([data])\n    if not len(out) == len(items):\n        raise AssertionError(\"Inconsistent samples in and out of SV bin calculation:\\nout: %s\\nin : %s\" %\n                             (sorted([dd.get_sample_name(utils.to_single_data(x)) for x in out]),\n                              sorted([dd.get_sample_name(x) for x in items])))\n    return out", "response": "Calculate regional bins for samples across multiple CNV callers."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate structural variant bins using GATK4 CNV callers region or even intervals approach.", "response": "def _calculate_sv_bins_gatk(data, cnv_group, size_calc_fn):\n    \"\"\"Calculate structural variant bins using GATK4 CNV callers region or even intervals approach.\n    \"\"\"\n    if dd.get_background_cnv_reference(data, \"gatk-cnv\"):\n        target_bed = gatkcnv.pon_to_bed(dd.get_background_cnv_reference(data, \"gatk-cnv\"), cnv_group.work_dir, data)\n    else:\n        target_bed = gatkcnv.prepare_intervals(data, cnv_group.region_file, cnv_group.work_dir)\n    gc_annotated_tsv = gatkcnv.annotate_intervals(target_bed, data)\n    return target_bed, None, gc_annotated_tsv"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating structural variant bins using target and anti - target approach from CNVkit.", "response": "def _calculate_sv_bins_cnvkit(data, cnv_group, size_calc_fn):\n    \"\"\"Calculate structural variant bins using target/anti-target approach from CNVkit.\n    \"\"\"\n    from bcbio.structural import cnvkit\n    if dd.get_background_cnv_reference(data, \"cnvkit\"):\n        target_bed, anti_bed = cnvkit.targets_from_background(dd.get_background_cnv_reference(data, \"cnvkit\"),\n                                                              cnv_group.work_dir, data)\n    else:\n        target_bed, anti_bed = cnvkit.targets_w_bins(cnv_group.region_file, cnv_group.access_file,\n                                                     size_calc_fn, cnv_group.work_dir, data)\n    return target_bed, anti_bed, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngroups samples by CNV - based approach.", "response": "def _group_by_cnv_method(batches):\n    \"\"\"Group into batches samples with identical CNV/SV approaches.\n\n    Allows sharing of background samples across multiple batches,\n    using all normals from tumor/normal pairs with the same prep method\n    for background.\n    \"\"\"\n    CnvGroup = collections.namedtuple(\"CnvGroup\", \"items, work_dir, access_file, region_file\")\n    out = []\n    groups = collections.defaultdict(list)\n    for batch, items in batches.items():\n        for data in items:\n            work_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"structural\", \"bins\", batch))\n            cnv_file = get_base_cnv_regions(data, work_dir, \"transcripts100\", include_gene_names=False)\n            if cnv_file:\n                break\n        assert cnv_file, (\"Did not find coverage regions for batch %s: %s\" %\n                          (batch, \" \".join([dd.get_sample_name(d) for d in items])))\n        groups[(cnv_file, dd.get_prep_method(data))].append((items, data, work_dir))\n    for (cnv_file, _), cur_group in groups.items():\n        group_items = reduce(operator.add, [xs[0] for xs in cur_group])\n        access_file = tz.get_in([\"config\", \"algorithm\", \"callable_regions\"], cur_group[0][1])\n        out.append(CnvGroup(group_items, cur_group[0][2], access_file, cnv_file))\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calculate_sv_coverage(data):\n    calcfns = {\"cnvkit\": _calculate_sv_coverage_cnvkit, \"gatk-cnv\": _calculate_sv_coverage_gatk}\n    from bcbio.structural import cnvkit\n    data = utils.to_single_data(data)\n    if not cnvkit.use_general_sv_bins(data):\n        out_target_file, out_anti_file = (None, None)\n    else:\n        work_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"structural\",\n                                                   dd.get_sample_name(data), \"bins\"))\n        out_target_file, out_anti_file = calcfns[cnvkit.bin_approach(data)](data, work_dir)\n        if not os.path.exists(out_target_file):\n            out_target_file, out_anti_file = (None, None)\n    if \"seq2c\" in dd.get_svcaller(data):\n        from bcbio.structural import seq2c\n        seq2c_target = seq2c.precall(data)\n    else:\n        seq2c_target = None\n\n    if not tz.get_in([\"depth\", \"bins\"], data):\n        data = tz.update_in(data, [\"depth\", \"bins\"], lambda x: {})\n    data[\"depth\"][\"bins\"] = {\"target\": out_target_file, \"antitarget\": out_anti_file, \"seq2c\": seq2c_target}\n    return [[data]]", "response": "Calculate coverage within bins for downstream CNV calling."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate coverage in defined regions using GATK tools", "response": "def _calculate_sv_coverage_gatk(data, work_dir):\n    \"\"\"Calculate coverage in defined regions using GATK tools\n\n    TODO: This does double calculations to get GATK4 compatible HDF read counts\n    and then depth and gene annotations. Both are needed for creating heterogeneity inputs.\n    Ideally replace with a single mosdepth coverage calculation, and creat GATK4 TSV format:\n\n    CONTIG  START   END     COUNT\n    chrM    1       1000    13268\n    \"\"\"\n    from bcbio.variation import coverage\n    from bcbio.structural import annotate\n    # GATK compatible\n    target_file = gatkcnv.collect_read_counts(data, work_dir)\n    # heterogeneity compatible\n    target_in = bedutils.clean_file(tz.get_in([\"regions\", \"bins\", \"target\"], data), data, bedprep_dir=work_dir)\n    target_cov = coverage.run_mosdepth(data, \"target-gatk\", target_in)\n    target_cov_genes = annotate.add_genes(target_cov.regions, data, max_distance=0)\n    return target_file, target_cov_genes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate coverage in an CNVkit ready format using mosdepth.", "response": "def _calculate_sv_coverage_cnvkit(data, work_dir):\n    \"\"\"Calculate coverage in an CNVkit ready format using mosdepth.\n    \"\"\"\n    from bcbio.variation import coverage\n    from bcbio.structural import annotate\n    out_target_file = os.path.join(work_dir, \"%s-target-coverage.cnn\" % dd.get_sample_name(data))\n    out_anti_file = os.path.join(work_dir, \"%s-antitarget-coverage.cnn\" % dd.get_sample_name(data))\n    if ((not utils.file_exists(out_target_file) or not utils.file_exists(out_anti_file)) and\n          (dd.get_align_bam(data) or dd.get_work_bam(data))):\n        target_cov = coverage.run_mosdepth(data, \"target\", tz.get_in([\"regions\", \"bins\", \"target\"], data))\n        anti_cov = coverage.run_mosdepth(data, \"antitarget\", tz.get_in([\"regions\", \"bins\", \"antitarget\"], data))\n        target_cov_genes = annotate.add_genes(target_cov.regions, data, max_distance=0)\n        out_target_file = _add_log2_depth(target_cov_genes, out_target_file, data)\n        out_anti_file = _add_log2_depth(anti_cov.regions, out_anti_file, data)\n    return out_target_file, out_anti_file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _add_log2_depth(in_file, out_file, data):\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            with utils.open_gzipsafe(in_file) as in_handle:\n                with open(tx_out_file, \"w\") as out_handle:\n                    out_handle.write(\"chromosome\\tstart\\tend\\tgene\\tlog2\\tdepth\\n\")\n                    for line in in_handle:\n                        parts = line.rstrip().split(\"\\t\")\n                        if len(parts) > 4:\n                            # Handle inputs unannotated with gene names\n                            if len(parts) == 5:\n                                chrom, start, end, orig_name, depth = parts\n                                gene_name = orig_name if (orig_name in [\"Antitarget\", \"Background\"]) else \".\"\n                            else:\n                                assert len(parts) == 6, parts\n                                chrom, start, end, orig_name, depth, gene_name = parts\n                            depth = float(depth)\n                            log2_depth = math.log(float(depth), 2) if depth else -20.0\n                            out_handle.write(\"%s\\t%s\\t%s\\t%s\\t%.3f\\t%.2f\\n\" %\n                                             (chrom, start, end, gene_name, log2_depth, depth))\n    return out_file", "response": "Add log2 depth to a CNVkit cnn file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nnormalize CNV coverage for multiple samples.", "response": "def normalize_sv_coverage(*items):\n    \"\"\"Normalize CNV coverage, providing flexible point for multiple methods.\n    \"\"\"\n    calcfns = {\"cnvkit\": _normalize_sv_coverage_cnvkit, \"gatk-cnv\": _normalize_sv_coverage_gatk}\n    from bcbio.structural import cnvkit\n    from bcbio.structural import shared as sshared\n    items = [utils.to_single_data(x) for x in cwlutils.handle_combined_input(items)]\n    if all(not cnvkit.use_general_sv_bins(x) for x in items):\n        return [[d] for d in items]\n    out_files = {}\n    back_files = {}\n    for group_id, gitems in itertools.groupby(items, lambda x: tz.get_in([\"regions\", \"bins\", \"group\"], x)):\n        # No CNVkit calling for this particular set of samples\n        if group_id is None:\n            continue\n        inputs, backgrounds = sshared.find_case_control(list(gitems))\n        assert inputs, \"Did not find inputs for sample batch: %s\" % (\" \".join(dd.get_sample_name(x) for x in items))\n        work_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(inputs[0]), \"structural\",\n                                                    dd.get_sample_name(inputs[0]), \"bins\"))\n        back_files, out_files = calcfns[cnvkit.bin_approach(inputs[0])](group_id, inputs, backgrounds, work_dir,\n                                                                        back_files, out_files)\n    out = []\n    for data in items:\n        if dd.get_sample_name(data) in out_files:\n            data[\"depth\"][\"bins\"][\"background\"] = back_files[dd.get_sample_name(data)]\n            data[\"depth\"][\"bins\"][\"normalized\"] = out_files[dd.get_sample_name(data)]\n        out.append([data])\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nnormalize CNV coverage using GATK s de - noise approaches.", "response": "def _normalize_sv_coverage_gatk(group_id, inputs, backgrounds, work_dir, back_files, out_files):\n    \"\"\"Normalize CNV coverage using panel of normals with GATK's de-noise approaches.\n    \"\"\"\n    input_backs = set(filter(lambda x: x is not None,\n                             [dd.get_background_cnv_reference(d, \"gatk-cnv\") for d in inputs]))\n    if input_backs:\n        assert len(input_backs) == 1, \"Multiple backgrounds in group: %s\" % list(input_backs)\n        pon = list(input_backs)[0]\n    elif backgrounds:\n        pon = gatkcnv.create_panel_of_normals(backgrounds, group_id, work_dir)\n    else:\n        pon = None\n    for data in inputs:\n        work_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"structural\",\n                                                   dd.get_sample_name(data), \"bins\"))\n        denoise_file = gatkcnv.denoise(data, pon, work_dir)\n        out_files[dd.get_sample_name(data)] = denoise_file\n        back_files[dd.get_sample_name(data)] = pon\n    return back_files, out_files"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nnormalize CNV coverage depths by GC repeats and background using CNVkit.", "response": "def _normalize_sv_coverage_cnvkit(group_id, inputs, backgrounds, work_dir, back_files, out_files):\n    \"\"\"Normalize CNV coverage depths by GC, repeats and background using CNVkit\n\n    - reference: calculates reference backgrounds from normals and pools\n      including GC and repeat information\n    - fix: Uses background to normalize coverage estimations\n    http://cnvkit.readthedocs.io/en/stable/pipeline.html#fix\n    \"\"\"\n    from bcbio.structural import cnvkit\n    cnns = reduce(operator.add, [[tz.get_in([\"depth\", \"bins\", \"target\"], x),\n                                    tz.get_in([\"depth\", \"bins\", \"antitarget\"], x)] for x in backgrounds], [])\n    for d in inputs:\n        if tz.get_in([\"depth\", \"bins\", \"target\"], d):\n            target_bed = tz.get_in([\"depth\", \"bins\", \"target\"], d)\n            antitarget_bed = tz.get_in([\"depth\", \"bins\", \"antitarget\"], d)\n    input_backs = set(filter(lambda x: x is not None,\n                                [dd.get_background_cnv_reference(d, \"cnvkit\") for d in inputs]))\n    if input_backs:\n        assert len(input_backs) == 1, \"Multiple backgrounds in group: %s\" % list(input_backs)\n        back_file = list(input_backs)[0]\n    else:\n        back_file = cnvkit.cnvkit_background(cnns,\n                                             os.path.join(work_dir, \"background-%s-cnvkit.cnn\" % (group_id)),\n                                             backgrounds or inputs, target_bed, antitarget_bed)\n    fix_cmd_inputs = []\n    for data in inputs:\n        work_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"structural\",\n                                                    dd.get_sample_name(data), \"bins\"))\n        if tz.get_in([\"depth\", \"bins\", \"target\"], data):\n            fix_file = os.path.join(work_dir, \"%s-normalized.cnr\" % (dd.get_sample_name(data)))\n            fix_cmd_inputs.append((tz.get_in([\"depth\", \"bins\", \"target\"], data),\n                                    tz.get_in([\"depth\", \"bins\", \"antitarget\"], data),\n                                    back_file, fix_file, data))\n            out_files[dd.get_sample_name(data)] = fix_file\n            back_files[dd.get_sample_name(data)] = back_file\n    parallel = {\"type\": \"local\", \"cores\": dd.get_cores(inputs[0]), \"progs\": [\"cnvkit\"]}\n    run_multicore(cnvkit.run_fix_parallel, fix_cmd_inputs, inputs[0][\"config\"], parallel)\n    return back_files, out_files"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve set of target regions for CNV analysis.", "response": "def get_base_cnv_regions(data, work_dir, genome_default=\"transcripts1e4\", include_gene_names=True):\n    \"\"\"Retrieve set of target regions for CNV analysis.\n\n    Subsets to extended transcript regions for WGS experiments to avoid\n    long runtimes.\n    \"\"\"\n    cov_interval = dd.get_coverage_interval(data)\n    base_regions = get_sv_bed(data, include_gene_names=include_gene_names)\n    # if we don't have a configured BED or regions to use for SV caling\n    if not base_regions:\n        # For genome calls, subset to regions near genes as targets\n        if cov_interval == \"genome\":\n            base_regions = get_sv_bed(data, genome_default, work_dir, include_gene_names=include_gene_names)\n            if base_regions:\n                base_regions = remove_exclude_regions(base_regions, base_regions, [data])\n        # Finally, default to the defined variant regions\n        if not base_regions:\n            base_regions = dd.get_variant_regions(data) or dd.get_sample_callable(data)\n    return bedutils.clean_file(base_regions, data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving centromere and short end regions from an existing BED file of regions to target.", "response": "def remove_exclude_regions(orig_bed, base_file, items, remove_entire_feature=False):\n    \"\"\"Remove centromere and short end regions from an existing BED file of regions to target.\n    \"\"\"\n    from bcbio.structural import shared as sshared\n    out_bed = os.path.join(\"%s-noexclude.bed\" % (utils.splitext_plus(base_file)[0]))\n    if not utils.file_uptodate(out_bed, orig_bed):\n        exclude_bed = sshared.prepare_exclude_file(items, base_file)\n        with file_transaction(items[0], out_bed) as tx_out_bed:\n            pybedtools.BedTool(orig_bed).subtract(pybedtools.BedTool(exclude_bed),\n                                                  A=remove_entire_feature, nonamecheck=True).saveas(tx_out_bed)\n    if utils.file_exists(out_bed):\n        return out_bed\n    else:\n        return orig_bed"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves a BED file of regions for SV and heterogeneity calling using the provided method.", "response": "def get_sv_bed(data, method=None, out_dir=None, include_gene_names=True):\n    \"\"\"Retrieve a BED file of regions for SV and heterogeneity calling using the provided method.\n\n    method choices:\n      - exons: Raw BED file of exon regions\n      - transcripts: Full collapsed regions with the min and max of each transcript.\n      - transcriptsXXXX: Collapsed regions around transcripts with a window size of\n        XXXX.\n      - A custom BED file of regions\n    \"\"\"\n    if method is None:\n        method = (tz.get_in([\"config\", \"algorithm\", \"sv_regions\"], data) or dd.get_variant_regions(data)\n                  or dd.get_sample_callable(data))\n    gene_file = dd.get_gene_bed(data)\n    if method and os.path.isfile(method):\n        return method\n    elif not gene_file or not method:\n        return None\n    elif method == \"exons\":\n        return gene_file\n    elif method.startswith(\"transcripts\"):\n        window = method.split(\"transcripts\")[-1]\n        window = int(float(window)) if window else 0\n        return _collapse_transcripts(gene_file, window, data, out_dir, include_gene_names=include_gene_names)\n    else:\n        raise ValueError(\"Unexpected transcript retrieval method: %s\" % method)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _collapse_transcripts(in_file, window, data, out_dir, include_gene_names=True):\n    if out_dir is None:\n        out_dir = os.path.dirname(in_file)\n    out_file = os.path.join(out_dir,\n                            \"%s-transcripts_w%s.bed\" % (os.path.splitext(os.path.basename(in_file))[0],\n                                                        window))\n    chrom_sizes = {}\n    for contig in ref.file_contigs(dd.get_ref_file(data), data[\"config\"]):\n        chrom_sizes[contig.name] = contig.size\n    if not utils.file_uptodate(out_file, in_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            prep_file = \"%s-sortprep%s\" % os.path.splitext(tx_out_file)\n            sort_cmd = bedutils.get_sort_cmd(os.path.dirname(tx_out_file))\n            cmd = \"{sort_cmd} -k4,4 -k1,1 {in_file} > {prep_file}\"\n            do.run(cmd.format(**locals()), \"Sort BED file by transcript name\")\n            with open(tx_out_file, \"w\") as out_handle:\n                # Work around for segmentation fault issue with groupby\n                # https://github.com/daler/pybedtools/issues/131#issuecomment-89832476\n                x = pybedtools.BedTool(prep_file)\n                def gen():\n                    for r in x:\n                        yield r\n                for name, rs in itertools.groupby(gen(), lambda r: (r.name, r.chrom)):\n                    rs = list(rs)\n                    r = rs[0]\n                    if r.chrom in chrom_sizes:\n                        for gcoords in _group_coords(rs):\n                            min_pos = max(min(gcoords) - window, 0)\n                            max_pos = min(max(gcoords) + window, chrom_sizes[r.chrom])\n                            if include_gene_names:\n                                out_handle.write(\"%s\\t%s\\t%s\\t%s\\n\" % (r.chrom, min_pos, max_pos, r.name))\n                            else:\n                                out_handle.write(\"%s\\t%s\\t%s\\n\" % (r.chrom, min_pos, max_pos))\n    return bedutils.sort_merge(out_file, data)", "response": "Collapse transcripts into min max coordinates and optionally add windows."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _group_coords(rs):\n    max_intron_size = 1e5\n    coords = []\n    for r in rs:\n        coords.append(r.start)\n        coords.append(r.end)\n    coord_groups = []\n    cur_group = []\n    for coord in sorted(coords):\n        if not cur_group or coord - cur_group[-1] < max_intron_size:\n            cur_group.append(coord)\n        else:\n            coord_groups.append(cur_group)\n            cur_group = [coord]\n    if cur_group:\n        coord_groups.append(cur_group)\n    return coord_groups", "response": "Organize coordinate regions into groups for each transcript."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _calc_sizes(self, cnv_file, items):\n        bp_per_bin = 100000  # same target as CNVkit\n        range_map = {\"target\": (100, 250), \"antitarget\": (10000, 1000000)}\n        target_bps = []\n        anti_bps = []\n        checked_beds = set([])\n        for data in items:\n            region_bed = tz.get_in([\"depth\", \"variant_regions\", \"regions\"], data)\n            if region_bed and region_bed not in checked_beds:\n                with utils.open_gzipsafe(region_bed) as in_handle:\n                    for r in pybedtools.BedTool(in_handle).intersect(cnv_file):\n                        if r.stop - r.start > range_map[\"target\"][0]:\n                            target_bps.append(float(r.name))\n                with utils.open_gzipsafe(region_bed) as in_handle:\n                    for r in pybedtools.BedTool(in_handle).intersect(cnv_file, v=True):\n                        if r.stop - r.start > range_map[\"target\"][1]:\n                            anti_bps.append(float(r.name))\n                checked_beds.add(region_bed)\n        def scale_in_boundary(raw, round_interval, range_targets):\n            min_val, max_val = range_targets\n            out = int(math.ceil(raw / float(round_interval)) * round_interval)\n            if out > max_val:\n                return max_val\n            elif out < min_val:\n                return min_val\n            else:\n                return out\n        if target_bps and np.median(target_bps) > 0:\n            raw_target_bin = bp_per_bin / float(np.median(target_bps))\n            target_bin = scale_in_boundary(raw_target_bin, 50, range_map[\"target\"])\n        else:\n            target_bin = range_map[\"target\"][1]\n\n        if anti_bps and np.median(anti_bps) > 0:\n            raw_anti_bin = bp_per_bin / float(np.median(anti_bps))\n            anti_bin = scale_in_boundary(raw_anti_bin, 10000, range_map[\"antitarget\"])\n        else:\n            anti_bin = range_map[\"antitarget\"][1]\n        return target_bin, anti_bin", "response": "Calculate target and antitarget bin sizes based on depth."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsearch a protein sequence against a HMMER sequence database.", "response": "def phmmer(**kwargs):\n    \"\"\"Search a protein sequence against a HMMER sequence database.\n\n    Arguments:\n      seq - The sequence to search -- a Fasta string.\n      seqdb -- Sequence database to search against.\n      range -- A string range of results to return (ie. 1,10 for the first ten)\n      output -- The output format (defaults to JSON).\n    \"\"\"\n    logging.debug(kwargs)\n    args = {'seq' : kwargs.get('seq'),\n            'seqdb' : kwargs.get('seqdb')}\n    args2 = {'output' : kwargs.get('output', 'json'),\n             'range' : kwargs.get('range')}\n    return _hmmer(\"http://hmmer.janelia.org/search/phmmer\", args, args2)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating file same dimension than mtx. colnames with metadata and sample name.", "response": "def scrnaseq_concatenate_metadata(samples):\n    \"\"\"\n    Create file same dimension than mtx.colnames\n    with metadata and sample name to help in the\n    creation of the SC object.\n    \"\"\"\n    barcodes = {}\n    counts =  \"\"\n    metadata = {}\n    has_sample_barcodes = False\n    for sample in dd.sample_data_iterator(samples):\n        if dd.get_sample_barcodes(sample):\n            has_sample_barcodes = True\n            with open(dd.get_sample_barcodes(sample)) as inh:\n                for line in inh:\n                    cols = line.strip().split(\",\")\n                    if len(cols) == 1:\n                        # Assign sample name in case of missing in barcodes\n                        cols.append(\"NaN\")\n                    barcodes[(dd.get_sample_name(sample), cols[0])] = cols[1:]\n        else:\n            barcodes[(dd.get_sample_name(sample), \"NaN\")] = [dd.get_sample_name(sample), \"NaN\"]\n\n        counts = dd.get_combined_counts(sample)\n        meta = map(str, list(sample[\"metadata\"].values()))\n        meta_cols = list(sample[\"metadata\"].keys())\n        meta = [\"NaN\" if not v else v for v in meta]\n        metadata[dd.get_sample_name(sample)] = meta\n\n    metadata_fn = counts + \".metadata\"\n    if file_exists(metadata_fn):\n        return samples\n    with file_transaction(metadata_fn) as tx_metadata_fn:\n        with open(tx_metadata_fn, 'w') as outh:\n            outh.write(\",\".join([\"sample\"] + meta_cols) + '\\n')\n            with open(counts + \".colnames\") as inh:\n                for line in inh:\n                    sample = line.split(\":\")[0]\n                    if has_sample_barcodes:\n                        barcode = sample.split(\"-\")[1]\n                    else:\n                        barcode = \"NaN\"\n                    outh.write(\",\".join(barcodes[(sample, barcode)] + metadata[sample]) + '\\n')\n    return samples"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rnaseq_variant_calling(samples, run_parallel):\n    samples = run_parallel(\"run_rnaseq_variant_calling\", samples)\n    variantcaller = dd.get_variantcaller(to_single_data(samples[0]))\n    if variantcaller and (\"gatk-haplotype\" in variantcaller):\n        out = []\n        for d in joint.square_off(samples, run_parallel):\n            out.extend([[to_single_data(xs)] for xs in multi.split_variants_by_sample(to_single_data(d))])\n        samples = out\n    if variantcaller:\n        samples = run_parallel(\"run_rnaseq_ann_filter\", samples)\n    if variantcaller and (\"gatk-haplotype\" in variantcaller):\n        out = []\n        for data in (to_single_data(xs) for xs in samples):\n            if \"variants\" not in data:\n                data[\"variants\"] = []\n            data[\"variants\"].append({\"variantcaller\": \"gatk-haplotype\", \"vcf\": data[\"vrn_file_orig\"],\n                                     \"population\": {\"vcf\": data[\"vrn_file\"]}})\n            data[\"vrn_file\"] = data.pop(\"vrn_file_orig\")\n            out.append([data])\n        samples = out\n    return samples", "response": "Run RNA - seq variant calling using GATKKKVariant calling."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun RNA - seq variant calling", "response": "def run_rnaseq_variant_calling(data):\n    \"\"\"\n    run RNA-seq variant calling, variation file is stored in `vrn_file`\n    in the datadict\n    \"\"\"\n    variantcaller = dd.get_variantcaller(data)\n    if isinstance(variantcaller, list) and len(variantcaller) > 1:\n        logger.error(\"Only one variantcaller can be run for RNA-seq at \"\n                     \"this time. Post an issue here \"\n                     \"(https://github.com/bcbio/bcbio-nextgen/issues) \"\n                     \"if this is something you need to do.\")\n        sys.exit(1)\n\n    if variantcaller:\n        if \"gatk-haplotype\" in variantcaller:\n            data = variation.rnaseq_gatk_variant_calling(data)\n        if vardict.get_vardict_command(data):\n            data = variation.rnaseq_vardict_variant_calling(data)\n        vrn_file = dd.get_vrn_file(data)\n    return [[data]]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_rnaseq_ann_filter(data):\n    data = to_single_data(data)\n    if dd.get_vrn_file(data):\n        eff_file = effects.add_to_vcf(dd.get_vrn_file(data), data)[0]\n        if eff_file:\n            data = dd.set_vrn_file(data, eff_file)\n        ann_file = population.run_vcfanno(dd.get_vrn_file(data), data)\n        if ann_file:\n            data = dd.set_vrn_file(data, ann_file)\n    variantcaller = dd.get_variantcaller(data)\n    if variantcaller and (\"gatk-haplotype\" in variantcaller):\n        filter_file = variation.gatk_filter_rnaseq(dd.get_vrn_file(data), data)\n        data = dd.set_vrn_file(data, filter_file)\n    # remove variants close to splice junctions\n    vrn_file = dd.get_vrn_file(data)\n    vrn_file = variation.filter_junction_variants(vrn_file, data)\n    data = dd.set_vrn_file(data, vrn_file)\n    return [[data]]", "response": "Run RNA - seq annotation and filtering."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a CWL target for quantitation.", "response": "def quantitate(data):\n    \"\"\"CWL target for quantitation.\n\n    XXX Needs to be split and parallelized by expression caller, with merging\n    of multiple calls.\n    \"\"\"\n    data = to_single_data(to_single_data(data))\n    data = generate_transcript_counts(data)[0][0]\n    data[\"quant\"] = {}\n    if \"sailfish\" in dd.get_expression_caller(data):\n        data = to_single_data(sailfish.run_sailfish(data)[0])\n        data[\"quant\"][\"tsv\"] = data[\"sailfish\"]\n        data[\"quant\"][\"hdf5\"] = os.path.join(os.path.dirname(data[\"sailfish\"]), \"abundance.h5\")\n    if (\"kallisto\" in dd.get_expression_caller(data) or \"pizzly\" in dd.get_fusion_caller(data, [])):\n        data = to_single_data(kallisto.run_kallisto_rnaseq(data)[0])\n        data[\"quant\"][\"tsv\"] = os.path.join(data[\"kallisto_quant\"], \"abundance.tsv\")\n        data[\"quant\"][\"hdf5\"] = os.path.join(data[\"kallisto_quant\"], \"abundance.h5\")\n    if (os.path.exists(os.path.join(data[\"kallisto_quant\"], \"fusion.txt\"))):\n        data[\"quant\"][\"fusion\"] = os.path.join(data[\"kallisto_quant\"], \"fusion.txt\")\n    else:\n        data[\"quant\"][\"fusion\"] = None\n    if \"salmon\" in dd.get_expression_caller(data):\n        data = to_single_data(salmon.run_salmon_reads(data)[0])\n        data[\"quant\"][\"tsv\"] = data[\"salmon\"]\n        data[\"quant\"][\"hdf5\"] = os.path.join(os.path.dirname(data[\"salmon\"]), \"abundance.h5\")\n    return [[data]]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns the expression calling in parallel and return the sequence of samples", "response": "def quantitate_expression_parallel(samples, run_parallel):\n    \"\"\"\n    quantitate expression, all programs run here should be multithreaded to\n    take advantage of the threaded run_parallel environment\n    \"\"\"\n    data = samples[0][0]\n    samples = run_parallel(\"generate_transcript_counts\", samples)\n    if \"cufflinks\" in dd.get_expression_caller(data):\n        samples = run_parallel(\"run_cufflinks\", samples)\n    if \"stringtie\" in dd.get_expression_caller(data):\n        samples = run_parallel(\"run_stringtie_expression\", samples)\n    if (\"kallisto\" in dd.get_expression_caller(data) or\n        dd.get_fusion_mode(data) or\n        \"pizzly\" in dd.get_fusion_caller(data, [])):\n        samples = run_parallel(\"run_kallisto_index\", [samples])\n        samples = run_parallel(\"run_kallisto_rnaseq\", samples)\n    if \"sailfish\" in dd.get_expression_caller(data):\n        samples = run_parallel(\"run_sailfish_index\", [samples])\n        samples = run_parallel(\"run_sailfish\", samples)\n    # always run salmon\n    samples = run_parallel(\"run_salmon_index\", [samples])\n    samples = run_parallel(\"run_salmon_reads\", samples)\n\n    samples = run_parallel(\"detect_fusions\", samples)\n    return samples"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef quantitate_expression_noparallel(samples, run_parallel):\n    data = samples[0][0]\n    if \"express\" in dd.get_expression_caller(data):\n        samples = run_parallel(\"run_express\", samples)\n    if \"dexseq\" in dd.get_expression_caller(data):\n        samples = run_parallel(\"run_dexseq\", samples)\n    return samples", "response": "run transcript quantitation for algorithms that don t run in parallel"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates counts per transcript per exon from an alignment", "response": "def generate_transcript_counts(data):\n    \"\"\"Generate counts per transcript and per exon from an alignment\"\"\"\n    data[\"count_file\"] = featureCounts.count(data)\n\n    if dd.get_fusion_mode(data, False) and not dd.get_fusion_caller(data):\n        oncofuse_file = oncofuse.run(data)\n        if oncofuse_file:\n            data = dd.set_oncofuse_file(data, oncofuse_file)\n\n    if dd.get_transcriptome_align(data):\n        # to create a disambiguated transcriptome file realign with bowtie2\n        if dd.get_disambiguate(data):\n            logger.info(\"Aligning to the transcriptome with bowtie2 using the \"\n                        \"disambiguated reads.\")\n            bam_path = data[\"work_bam\"]\n            fastq_paths = alignprep._bgzip_from_bam(bam_path, data[\"dirs\"], data, is_retry=False, output_infix='-transcriptome')\n            if len(fastq_paths) == 2:\n                file1, file2 = fastq_paths\n            else:\n                file1, file2 = fastq_paths[0], None\n            ref_file = dd.get_ref_file(data)\n            data = bowtie2.align_transcriptome(file1, file2, ref_file, data)\n        else:\n            file1, file2 = dd.get_input_sequence_files(data)\n        if not dd.get_transcriptome_bam(data):\n            ref_file = dd.get_ref_file(data)\n            logger.info(\"Transcriptome alignment was flagged to run, but the \"\n                        \"transcriptome BAM file was not found. Aligning to the \"\n                        \"transcriptome with bowtie2.\")\n            data = bowtie2.align_transcriptome(file1, file2, ref_file, data)\n    data = spikein.counts_spikein(data)\n    return [[data]]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef combine_express(samples, combined):\n    if not combined:\n        return None\n    to_combine = [dd.get_express_counts(x) for x in\n                  dd.sample_data_iterator(samples) if dd.get_express_counts(x)]\n    gtf_file = dd.get_gtf_file(samples[0][0])\n    isoform_to_gene_file = os.path.join(os.path.dirname(combined), \"isoform_to_gene.txt\")\n    isoform_to_gene_file = express.isoform_to_gene_name(\n        gtf_file, isoform_to_gene_file, next(dd.sample_data_iterator(samples)))\n    if len(to_combine) > 0:\n        eff_counts_combined_file = os.path.splitext(combined)[0] + \".isoform.express_counts\"\n        eff_counts_combined = count.combine_count_files(to_combine, eff_counts_combined_file, ext=\".counts\")\n        to_combine = [dd.get_express_tpm(x) for x in\n                      dd.sample_data_iterator(samples) if dd.get_express_tpm(x)]\n        tpm_counts_combined_file = os.path.splitext(combined)[0] + \".isoform.express_tpm\"\n        tpm_counts_combined = count.combine_count_files(to_combine, tpm_counts_combined_file)\n        to_combine = [dd.get_express_fpkm(x) for x in dd.sample_data_iterator(samples)\n                      if dd.get_express_fpkm(x)]\n        fpkm_counts_combined_file = os.path.splitext(combined)[0] + \".isoform.express_fpkm\"\n        fpkm_counts_combined = count.combine_count_files(to_combine, fpkm_counts_combined_file, ext=\".fpkm\")\n        return {'counts': eff_counts_combined, 'tpm': tpm_counts_combined,\n                'fpkm': fpkm_counts_combined, 'isoform_to_gene': isoform_to_gene_file}\n    return {}", "response": "Combine tpm effective counts and fpkm from express results"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef assemble_transcripts(run_parallel, samples):\n    assembler = dd.get_in_samples(samples, dd.get_transcript_assembler)\n    data = samples[0][0]\n    if assembler:\n        if \"cufflinks\" in assembler:\n            samples = run_parallel(\"cufflinks_assemble\", samples)\n        if \"stringtie\" in assembler:\n            samples = run_parallel(\"run_stringtie_expression\", samples)\n        if \"stringtie\" in assembler and stringtie.supports_merge(data):\n            samples = run_parallel(\"stringtie_merge\", [samples])\n        else:\n            samples = run_parallel(\"cufflinks_merge\", [samples])\n    return samples", "response": "Assemble transcripts into a single language file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef combine_files(samples):\n    data = samples[0][0]\n    # prefer the supplied transcriptome gtf file\n    gtf_file = dd.get_transcriptome_gtf(data, None)\n    if not gtf_file:\n        gtf_file = dd.get_gtf_file(data, None)\n    dexseq_gff = dd.get_dexseq_gff(data)\n\n    # combine featureCount files\n    count_files = filter_missing([dd.get_count_file(x[0]) for x in samples])\n    combined = count.combine_count_files(count_files, ext=\".counts\")\n    annotated = count.annotate_combined_count_file(combined, gtf_file)\n\n    # add tx2gene file\n    tx2gene_file = os.path.join(dd.get_work_dir(data), \"annotation\", \"tx2gene.csv\")\n    if gtf_file:\n        tx2gene_file = sailfish.create_combined_tx2gene(data)\n\n    # combine eXpress files\n    express_counts_combined = combine_express(samples, combined)\n\n    # combine Cufflinks files\n    fpkm_files = filter_missing([dd.get_fpkm(x[0]) for x in samples])\n    if fpkm_files and combined:\n        fpkm_combined_file = os.path.splitext(combined)[0] + \".fpkm\"\n        fpkm_combined = count.combine_count_files(fpkm_files, fpkm_combined_file)\n    else:\n        fpkm_combined = None\n    isoform_files = filter_missing([dd.get_fpkm_isoform(x[0]) for x in samples])\n    if isoform_files and combined:\n        fpkm_isoform_combined_file = os.path.splitext(combined)[0] + \".isoform.fpkm\"\n        fpkm_isoform_combined = count.combine_count_files(isoform_files,\n                                                          fpkm_isoform_combined_file,\n                                                          \".isoform.fpkm\")\n    else:\n        fpkm_isoform_combined = None\n    # combine DEXseq files\n    to_combine_dexseq = filter_missing([dd.get_dexseq_counts(data[0]) for data\n                                        in samples])\n    if to_combine_dexseq and combined:\n        dexseq_combined_file = os.path.splitext(combined)[0] + \".dexseq\"\n        dexseq_combined = count.combine_count_files(to_combine_dexseq,\n                                                    dexseq_combined_file, \".dexseq\")\n        if dexseq_combined:\n            dexseq.create_dexseq_annotation(dexseq_gff, dexseq_combined)\n    else:\n        dexseq_combined = None\n    samples = spikein.combine_spikein(samples)\n    updated_samples = []\n    for data in dd.sample_data_iterator(samples):\n        if combined:\n            data = dd.set_combined_counts(data, combined)\n        if annotated:\n            data = dd.set_annotated_combined_counts(data, annotated)\n        if fpkm_combined:\n            data = dd.set_combined_fpkm(data, fpkm_combined)\n        if fpkm_isoform_combined:\n            data = dd.set_combined_fpkm_isoform(data, fpkm_isoform_combined)\n        if express_counts_combined:\n            data = dd.set_express_counts(data, express_counts_combined['counts'])\n            data = dd.set_express_tpm(data, express_counts_combined['tpm'])\n            data = dd.set_express_fpkm(data, express_counts_combined['fpkm'])\n            data = dd.set_isoform_to_gene(data, express_counts_combined['isoform_to_gene'])\n        if dexseq_combined:\n            data = dd.set_dexseq_counts(data, dexseq_combined_file)\n        if gtf_file:\n            data = dd.set_tx2gene(data, tx2gene_file)\n        updated_samples.append([data])\n    return updated_samples", "response": "Combine the counts and FPKM files into a single table with all samples"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _prepare_bam_file(bam_file, tmp_dir, config):\n    sort_mode = _get_sort_order(bam_file, config)\n    if sort_mode != \"queryname\":\n        bam_file = sort(bam_file, config, \"queryname\")\n    return bam_file", "response": "Prepare bam file for use in BAM file processing"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert an isoform to a gene name", "response": "def isoform_to_gene_name(gtf_file, out_file, data):\n    \"\"\"\n    produce a table of isoform -> gene mappings for loading into EBSeq\n    \"\"\"\n    if not out_file:\n         out_file = tempfile.NamedTemporaryFile(delete=False).name\n    if file_exists(out_file):\n        return out_file\n    db = gtf.get_gtf_db(gtf_file)\n    line_format = \"{transcript}\\t{gene}\\n\"\n    with file_transaction(data, out_file) as tx_out_file:\n        with open(tx_out_file, \"w\") as out_handle:\n            for feature in db.features_of_type('transcript'):\n                transcript = feature['transcript_id'][0]\n                gene = feature['gene_id'][0]\n                out_handle.write(line_format.format(**locals()))\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef shared_variantcall(call_fn, name, align_bams, ref_file, items,\n                       assoc_files, region=None, out_file=None):\n    \"\"\"Provide base functionality for prepping and indexing for variant calling.\n    \"\"\"\n    config = items[0][\"config\"]\n    if out_file is None:\n        if vcfutils.is_paired_analysis(align_bams, items):\n            out_file = \"%s-paired-variants.vcf.gz\" % config[\"metdata\"][\"batch\"]\n        else:\n            out_file = \"%s-variants.vcf.gz\" % os.path.splitext(align_bams[0])[0]\n    if not file_exists(out_file):\n        logger.debug(\"Genotyping with {name}: {region} {fname}\".format(\n              name=name, region=region, fname=os.path.basename(align_bams[0])))\n        variant_regions = bedutils.population_variant_regions(items, merged=True)\n        target_regions = subset_variant_regions(variant_regions, region, out_file, items=items)\n        if (variant_regions is not None and isinstance(target_regions, six.string_types)\n              and not os.path.isfile(target_regions)):\n            vcfutils.write_empty_vcf(out_file, config)\n        else:\n            with file_transaction(config, out_file) as tx_out_file:\n                call_fn(align_bams, ref_file, items, target_regions,\n                        tx_out_file)\n    if out_file.endswith(\".gz\"):\n        out_file = vcfutils.bgzip_and_index(out_file, config)\n    return out_file", "response": "Provide base functionality for prepping and indexing for variant calling."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun samtools on the input BAM files.", "response": "def run_samtools(align_bams, items, ref_file, assoc_files, region=None,\n                 out_file=None):\n    \"\"\"Detect SNPs and indels with samtools mpileup and bcftools.\n    \"\"\"\n    return shared_variantcall(_call_variants_samtools, \"samtools\", align_bams, ref_file,\n                              items, assoc_files, region, out_file)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall variants with samtools in target_regions.", "response": "def _call_variants_samtools(align_bams, ref_file, items, target_regions, tx_out_file):\n    \"\"\"Call variants with samtools in target_regions.\n\n    Works around a GATK VCF 4.2 compatibility issue in samtools 1.0\n    by removing addition 4.2-only isms from VCF header lines.\n    \"\"\"\n    config = items[0][\"config\"]\n    mpileup = prep_mpileup(align_bams, ref_file, config,\n                           target_regions=target_regions, want_bcf=True)\n    bcftools = config_utils.get_program(\"bcftools\", config)\n    samtools_version = programs.get_version(\"samtools\", config=config)\n    if samtools_version and LooseVersion(samtools_version) <= LooseVersion(\"0.1.19\"):\n        raise ValueError(\"samtools calling not supported with pre-1.0 samtools\")\n    bcftools_opts = \"call -v -m\"\n    compress_cmd = \"| bgzip -c\" if tx_out_file.endswith(\".gz\") else \"\"\n    fix_ambig_ref = vcfutils.fix_ambiguous_cl()\n    fix_ambig_alt = vcfutils.fix_ambiguous_cl(5)\n    cmd = (\"{mpileup} \"\n           \"| {bcftools} {bcftools_opts} - \"\n           \"| {fix_ambig_ref} | {fix_ambig_alt} \"\n           \"| vt normalize -n -q -r {ref_file} - \"\n           \"| sed 's/VCFv4.2/VCFv4.1/' \"\n           \"| sed 's/,Version=3>/>/' \"\n           \"| sed 's/,Version=\\\"3\\\">/>/' \"\n           \"| sed 's/Number=R/Number=./' \"\n           \"{compress_cmd} > {tx_out_file}\")\n    do.run(cmd.format(**locals()), \"Variant calling with samtools\", items[0])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _convert_fastq(srafn, outdir, single=False):\n    \"convert sra to fastq\"\n    cmd = \"fastq-dump --split-files --gzip {srafn}\"\n    cmd = \"%s %s\" % (utils.local_path_export(), cmd)\n    sraid = os.path.basename(utils.splitext_plus(srafn)[0])\n    if not srafn:\n        return None\n    if not single:\n        out_file = [os.path.join(outdir, \"%s_1.fastq.gz\" % sraid),\n                    os.path.join(outdir, \"%s_2.fastq.gz\" % sraid)]\n        if not utils.file_exists(out_file[0]):\n            with utils.chdir(outdir):\n                do.run(cmd.format(**locals()), \"Covert to fastq %s\" % sraid)\n        if not utils.file_exists(out_file[0]):\n            raise IOError(\"SRA %s didn't convert, something happened.\" % srafn)\n        return [out for out in out_file if utils.file_exists(out)]\n    else:\n        raise ValueError(\"Not supported single-end sra samples for now.\")", "response": "convert sra to fastq"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_and_postprocess(args):\n    with open(args.process_config) as in_handle:\n        config = yaml.safe_load(in_handle)\n    setup_local_logging(config)\n    for dname in _find_unprocessed(config):\n        lane_details = nglims.get_runinfo(config[\"galaxy_url\"], config[\"galaxy_apikey\"], dname,\n                                          utils.get_in(config, (\"process\", \"storedir\")))\n        if isinstance(lane_details, dict) and \"error\" in lane_details:\n            print(\"Flowcell not found in Galaxy: %s\" % lane_details)\n        else:\n            lane_details = _tweak_lane(lane_details, dname)\n            fcid_ss = samplesheet.from_flowcell(dname, lane_details)\n            _update_reported(config[\"msg_db\"], dname)\n            fastq_dir = demultiplex.run_bcl2fastq(dname, fcid_ss, config)\n            bcbio_config, ready_fastq_dir = nglims.prep_samples_and_config(dname, lane_details, fastq_dir, config)\n            transfer.copy_flowcell(dname, ready_fastq_dir, bcbio_config, config)\n            _start_processing(dname, bcbio_config, config)", "response": "Check for newly dumped sequencer output post - processing and transferring."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _tweak_lane(lane_details, dname):\n    tweak_config_file = os.path.join(dname, \"lane_config.yaml\")\n    if os.path.exists(tweak_config_file):\n        with open(tweak_config_file) as in_handle:\n            tweak_config = yaml.safe_load(in_handle)\n        if tweak_config.get(\"uniquify_lanes\"):\n            out = []\n            for ld in lane_details:\n                ld[\"name\"] = \"%s-%s\" % (ld[\"name\"], ld[\"lane\"])\n                out.append(ld)\n            return out\n    return lane_details", "response": "Potentially tweaks lane information to handle custom processing"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _remap_dirname(local, remote):\n    def do(x):\n        return x.replace(local, remote, 1)\n    return do", "response": "Remap local directory names to remote directory names."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitiating processing on a remote server or locally on a cluster.", "response": "def _start_processing(dname, sample_file, config):\n    \"\"\"Initiate processing: on a remote server or locally on a cluster.\n    \"\"\"\n    to_remote = _remap_dirname(dname, os.path.join(utils.get_in(config, (\"process\", \"dir\")),\n                                                   os.path.basename(dname)))\n    args = {\"work_dir\": to_remote(os.path.join(dname, \"analysis\")),\n            \"run_config\": to_remote(sample_file),\n            \"fc_dir\": to_remote(dname)}\n    # call a remote server\n    if utils.get_in(config, (\"process\", \"server\")):\n        print(\"%s/run?args=%s\" % (utils.get_in(config, (\"process\", \"server\")), json.dumps(args)))\n        requests.get(url=\"%s/run\" % utils.get_in(config, (\"process\", \"server\")),\n                     params={\"args\": json.dumps(args)})\n    # submit to a cluster scheduler\n    elif \"submit_cmd\" in config[\"process\"] and \"bcbio_batch\" in config[\"process\"]:\n        with utils.chdir(utils.safe_makedir(args[\"work_dir\"])):\n            batch_script = \"submit_bcbio.sh\"\n            with open(batch_script, \"w\") as out_handle:\n                out_handle.write(config[\"process\"][\"bcbio_batch\"].format(fcdir=args[\"fc_dir\"],\n                                                                         run_config=args[\"run_config\"]))\n            submit_cmd = utils.get_in(config, (\"process\", \"submit_cmd\"))\n            subprocess.check_call(submit_cmd.format(batch_script=batch_script), shell=True)\n    else:\n        raise ValueError(\"Unexpected processing approach: %s\" % config[\"process\"])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind any finished directories that have not been processed.", "response": "def _find_unprocessed(config):\n    \"\"\"Find any finished directories that have not been processed.\n    \"\"\"\n    reported = _read_reported(config[\"msg_db\"])\n    for dname in _get_directories(config):\n        if os.path.isdir(dname) and dname not in reported:\n            if _is_finished_dumping(dname):\n                yield dname"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining if the sequencing directory has all files.", "response": "def _is_finished_dumping(directory):\n    \"\"\"Determine if the sequencing directory has all files.\n\n    The final checkpoint file will differ depending if we are a\n    single or paired end run.\n    \"\"\"\n    #if _is_finished_dumping_checkpoint(directory):\n    #    return True\n    # Check final output files; handles both HiSeq and GAII\n    run_info = os.path.join(directory, \"RunInfo.xml\")\n    hi_seq_checkpoint = \"Basecalling_Netcopy_complete_Read%s.txt\" % \\\n                        _expected_reads(run_info)\n    to_check = [\"Basecalling_Netcopy_complete_SINGLEREAD.txt\",\n                \"Basecalling_Netcopy_complete_READ2.txt\",\n                hi_seq_checkpoint]\n    return reduce(operator.or_,\n                  [os.path.exists(os.path.join(directory, f)) for f in to_check])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the checkpoint is finished.", "response": "def _is_finished_dumping_checkpoint(directory):\n    \"\"\"Recent versions of RTA (1.10 or better), write the complete file.\n\n    This is the most straightforward source but as of 1.10 still does not\n    work correctly as the file will be created at the end of Read 1 even\n    if there are multiple reads.\n    \"\"\"\n    check_file = os.path.join(directory, \"Basecalling_Netcopy_complete.txt\")\n    check_v1, check_v2 = (1, 10)\n    if os.path.exists(check_file):\n        with open(check_file) as in_handle:\n            line = in_handle.readline().strip()\n        if line:\n            version = line.split()[-1]\n            v1, v2 = [float(v) for v in version.split(\".\")[:2]]\n            if ((v1 > check_v1) or (v1 == check_v1 and v2 >= check_v2)):\n                return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _expected_reads(run_info_file):\n    reads = []\n    if os.path.exists(run_info_file):\n        tree = ElementTree()\n        tree.parse(run_info_file)\n        read_elem = tree.find(\"Run/Reads\")\n        reads = read_elem.findall(\"Read\")\n    return len(reads)", "response": "Parse the number of expected reads from the RunInfo. xml file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _read_reported(msg_db):\n    reported = []\n    if os.path.exists(msg_db):\n        with open(msg_db) as in_handle:\n            for line in in_handle:\n                reported.append(line.strip())\n    return reported", "response": "Retrieve a list of directories previous reported."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(bam_file, data, out_dir):\n    source_link = 'https://gdc.cancer.gov/about-data/data-harmonization-and-generation/gdc-reference-files'\n    viral_target = \"gdc-viral\"\n    out = {}\n    if vcfutils.get_paired_phenotype(data):\n        viral_refs = [x for x in dd.get_viral_files(data) if os.path.basename(x) == \"%s.fa\" % viral_target]\n        if viral_refs and utils.file_exists(viral_refs[0]):\n            viral_ref = viral_refs[0]\n            viral_bam = os.path.join(utils.safe_makedir(out_dir),\n                                     \"%s-%s.bam\" % (dd.get_sample_name(data),\n                                                    utils.splitext_plus(os.path.basename(viral_ref))[0]))\n            out_file = \"%s-completeness.txt\" % utils.splitext_plus(viral_bam)[0]\n            cores = dd.get_num_cores(data)\n            if not utils.file_uptodate(out_file, bam_file):\n                if not utils.file_uptodate(viral_bam, bam_file):\n                    with file_transaction(data, viral_bam) as tx_out_file:\n                        tmpfile = \"%s-tmp\" % utils.splitext_plus(tx_out_file)[0]\n                        cmd = (\"samtools view -u -f 4 {bam_file} | \"\n                               \"bamtofastq collate=0 | \"\n                               \"bwa mem -t {cores} {viral_ref} - | \"\n                               \"bamsort tmpfile={tmpfile} inputthreads={cores} outputthreads={cores} \"\n                               \"inputformat=sam index=1 indexfilename={tx_out_file}.bai O={tx_out_file}\")\n                        do.run(cmd.format(**locals()), \"Align unmapped reads to viral genome\")\n                with file_transaction(data, out_file) as tx_out_file:\n                    sample_name = dd.get_sample_name(data)\n                    mosdepth_prefix = os.path.splitext(viral_bam)[0]\n                    cmd = (\"mosdepth -t {cores} {mosdepth_prefix} {viral_bam} -n --thresholds 1,5,25 --by \"\n                           \"<(awk 'BEGIN {{FS=\\\"\\\\t\\\"}}; {{print $1 FS \\\"0\\\" FS $2}}' {viral_ref}.fai) && \"\n                           \"echo '## Viral sequences (from {source_link}) found in unmapped reads' > {tx_out_file} &&\"\n                           \"echo '## Sample: {sample_name}' >> {tx_out_file} && \"\n                           \"echo '#virus\\tsize\\tdepth\\t1x\\t5x\\t25x' >> {tx_out_file} && \"\n                           \"paste <(zcat {mosdepth_prefix}.regions.bed.gz) <(zgrep -v ^# {mosdepth_prefix}.thresholds.bed.gz) | \"\n                           \"awk 'BEGIN {{FS=\\\"\\\\t\\\"}} {{ print $1 FS $3 FS $4 FS $10/$3 FS $11/$3 FS $12/$3}}' | \"\n                           \"sort -n -r -k 5,5 >> {tx_out_file}\")\n                    do.run(cmd.format(**locals()), \"Analyse coverage of viral genomes\")\n            out[\"base\"] = out_file\n            out[\"secondary\"] = []\n    return out", "response": "Run the viral QC analysis on the unmapped reads."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve pre - installed viral reference files.", "response": "def get_files(data):\n    \"\"\"Retrieve pre-installed viral reference files.\n    \"\"\"\n    all_files = glob.glob(os.path.normpath(os.path.join(os.path.dirname(dd.get_ref_file(data)),\n                                                        os.pardir, \"viral\", \"*\")))\n    return sorted(all_files)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef gatk_splitreads(data):\n    broad_runner = broad.runner_from_config(dd.get_config(data))\n    ref_file = dd.get_ref_file(data)\n    deduped_bam = dd.get_deduped_bam(data)\n    base, ext = os.path.splitext(deduped_bam)\n    split_bam = base + \".splitN\" + ext\n    if file_exists(split_bam):\n        data = dd.set_split_bam(data, split_bam)\n        return data\n    gatk_type = broad_runner.gatk_type()\n    with file_transaction(data, split_bam) as tx_split_bam:\n        params = [\"-T\", \"SplitNCigarReads\",\n                  \"-R\", ref_file,\n                  \"-I\", deduped_bam]\n        if gatk_type == \"gatk4\":\n            params += [\"--output\", tx_split_bam]\n        else:\n            params += [\"-rf\", \"ReassignOneMappingQuality\",\n                       \"-RMQF\", \"255\",\n                       \"-RMQT\", \"60\",\n                       \"-rf\", \"UnmappedRead\",\n                       \"-U\", \"ALLOW_N_CIGAR_READS\",\n                       \"-o\", tx_split_bam]\n            if dd.get_quality_format(data) == \"illumina\":\n                params += [\"--fix_misencoded_quality_scores\", \"-fixMisencodedQuals\"]\n        broad_runner.run_gatk(params)\n    bam.index(split_bam, dd.get_config(data))\n    data = dd.set_split_bam(data, split_bam)\n    return data", "response": "Split reads with Ns in the CIGAR string into two sets of reads."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _setup_variant_regions(data, out_dir):\n    vr_file = dd.get_variant_regions(data)\n    if not vr_file:\n        vr_file = regions.get_sv_bed(data, \"transcripts\", out_dir=out_dir)\n    contigs = set([c.name for c in ref.file_contigs(dd.get_ref_file(data))])\n    out_file = os.path.join(utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"bedprep\")),\n                            \"%s-rnaseq_clean.bed\" % utils.splitext_plus(os.path.basename(vr_file))[0])\n    if not utils.file_uptodate(out_file, vr_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            with open(tx_out_file, \"w\") as out_handle:\n                with shared.bedtools_tmpdir(data):\n                    for r in pybedtools.BedTool(vr_file):\n                        if r.chrom in contigs:\n                            if chromhacks.is_nonalt(r.chrom):\n                                out_handle.write(str(r))\n    data = dd.set_variant_regions(data, out_file)\n    return data", "response": "Ensure we have variant regions for calling using transcript."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef gatk_rnaseq_calling(data):\n    from bcbio.bam import callable\n    data = utils.deepish_copy(data)\n    tools_on = dd.get_tools_on(data)\n    if not tools_on:\n        tools_on = []\n    tools_on.append(\"gvcf\")\n    data = dd.set_tools_on(data, tools_on)\n    data = dd.set_jointcaller(data, [\"%s-joint\" % v for v in dd.get_variantcaller(data)])\n    out_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data),\n                                              \"variation\", \"rnaseq\", \"gatk-haplotype\"))\n    data = _setup_variant_regions(data, out_dir)\n    out_file = os.path.join(out_dir, \"%s-gatk-haplotype.vcf.gz\" % dd.get_sample_name(data))\n    if not utils.file_exists(out_file):\n        region_files = []\n        regions = []\n        for cur_region in callable.get_split_regions(dd.get_variant_regions(data), data):\n            str_region = \"_\".join([str(x) for x in cur_region])\n            region_file = os.path.join(utils.safe_makedir(os.path.join(dd.get_work_dir(data),\n                                                                    \"variation\", \"rnaseq\", \"gatk-haplotype\",\n                                                                    \"regions\")),\n                                    \"%s-%s-gatk-haplotype.vcf.gz\" % (dd.get_sample_name(data), str_region))\n            region_file = gatk.haplotype_caller([dd.get_split_bam(data)], [data], dd.get_ref_file(data), {},\n                                                region=cur_region, out_file=region_file)\n            region_files.append(region_file)\n            regions.append(cur_region)\n        out_file = vcfutils.concat_variant_files(region_files, out_file, regions,\n                                                 dd.get_ref_file(data), data[\"config\"])\n    return dd.set_vrn_file(data, out_file)", "response": "Use GATK to perform variant calling on RNA - seq data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfilter RNA - seq variants using GATK4.", "response": "def gatk_filter_rnaseq(vrn_file, data):\n    \"\"\"\n    this incorporates filters listed here, dropping clusters of variants\n    within a 35 nucleotide window, high fischer strand values and low\n    quality by depth\n    https://software.broadinstitute.org/gatk/guide/article?id=3891\n    java -jar GenomeAnalysisTK.jar -T VariantFiltration -R hg_19.fasta -V\n    input.vcf -window 35 -cluster 3 -filterName FS -filter \"FS > 30.0\"\n    -filterName QD -filter \"QD < 2.0\" -o output.vcf\n    \"\"\"\n    out_file = \"%s-filter%s\" % utils.splitext_plus(vrn_file)\n    if not file_exists(out_file):\n        ref_file = dd.get_ref_file(data)\n        with file_transaction(data, out_file) as tx_out_file:\n            params = [\"VariantFiltration\",\n                      \"-R\", ref_file,\n                      \"-V\", vrn_file,\n                      \"--cluster-window-size\", \"35\",\n                      \"--cluster-size\", \"3\",\n                      \"--filter-expression\", \"'FS > 30.0'\",\n                      \"--filter-name\", \"FS\",\n                      \"--filter-expression\", \"'QD < 2.0'\",\n                      \"--filter-name\", \"QD\",\n                      \"--output\", tx_out_file]\n            # Use GATK4 for filtering, tools_off is for variant calling\n            config = utils.deepish_copy(dd.get_config(data))\n            if \"gatk4\" in dd.get_tools_off({\"config\": config}):\n                config[\"algorithm\"][\"tools_off\"].remove(\"gatk4\")\n            jvm_opts = broad.get_gatk_opts(config, os.path.dirname(tx_out_file))\n            do.run(broad.gatk_cmd(\"gatk\", jvm_opts, params, config), \"Filter RNA-seq variants.\")\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfiltering out variants within 10 basepairs of a splice junction.", "response": "def filter_junction_variants(vrn_file, data):\n    \"\"\"\n    filter out variants within 10 basepairs of a splice junction, these are\n    very prone to being false positives with RNA-seq data\n    \"\"\"\n    SJ_BP_MASK = 10\n    vrn_dir = os.path.dirname(vrn_file)\n    splicebed = dd.get_junction_bed(data)\n    if not file_exists(splicebed):\n        logger.info(\"Splice junction BED file not found, skipping filtering of \"\n                    \"variants closed to splice junctions.\")\n        return vrn_file\n    spliceslop = get_padded_bed_file(vrn_dir, splicebed, SJ_BP_MASK, data)\n    out_file = os.path.splitext(vrn_file)[0] + \"-junctionfiltered.vcf.gz\"\n    if file_exists(out_file):\n        return out_file\n    with file_transaction(data, out_file) as tx_out_file:\n        out_base = os.path.splitext(tx_out_file)[0]\n        logger.info(\"Removing variants within %d bases of splice junctions listed in %s from %s. \" % (SJ_BP_MASK, spliceslop, vrn_file))\n        pybedtools.BedTool(vrn_file).intersect(spliceslop, wa=True, header=True, v=True).saveas(out_base)\n        tx_out_file = vcfutils.bgzip_and_index(out_base, dd.get_config(data))\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _clean_args(sys_argv, args):\n    base = [x for x in sys_argv if\n            x.startswith(\"-\") or not args.datadir == os.path.abspath(os.path.expanduser(x))]\n    # Remove installer only options we don't pass on\n    base = [x for x in base if x not in set([\"--minimize-disk\"])]\n    if \"--nodata\" in base:\n        base.remove(\"--nodata\")\n    else:\n        base.append(\"--data\")\n    return base", "response": "Remove data directory from arguments to pass to upgrade function."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef install_anaconda_python(args):\n    anaconda_dir = os.path.join(args.datadir, \"anaconda\")\n    bindir = os.path.join(anaconda_dir, \"bin\")\n    conda = os.path.join(bindir, \"conda\")\n    if not os.path.exists(anaconda_dir) or not os.path.exists(conda):\n        if os.path.exists(anaconda_dir):\n            shutil.rmtree(anaconda_dir)\n        dist = args.distribution if args.distribution else _guess_distribution()\n        url = REMOTES[\"anaconda\"] % (\"MacOSX\" if dist.lower() == \"macosx\" else \"Linux\")\n        if not os.path.exists(os.path.basename(url)):\n            subprocess.check_call([\"wget\", \"--progress=dot:mega\", \"--no-check-certificate\", url])\n        subprocess.check_call(\"bash %s -b -p %s\" %\n                              (os.path.basename(url), anaconda_dir), shell=True)\n    return {\"conda\": conda,\n            \"pip\": os.path.join(bindir, \"pip\"),\n            \"dir\": anaconda_dir}", "response": "Provide isolated installation of Anaconda python for running bcbio - nextgen."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setup_manifest(datadir):\n    manifest_dir = os.path.join(datadir, \"manifest\")\n    if not os.path.exists(manifest_dir):\n        os.makedirs(manifest_dir)", "response": "Create barebones manifest to be filled in during update\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_system_config(base_url, datadir, tooldir):\n    out_file = os.path.join(datadir, \"galaxy\", os.path.basename(base_url))\n    if not os.path.exists(os.path.dirname(out_file)):\n        os.makedirs(os.path.dirname(out_file))\n    if os.path.exists(out_file):\n        # if no tool directory and exists, do not overwrite\n        if tooldir is None:\n            return out_file\n        else:\n            bak_file = out_file + \".bak%s\" % (datetime.datetime.now().strftime(\"%Y%M%d_%H%M\"))\n            shutil.copy(out_file, bak_file)\n    if tooldir:\n        java_basedir = os.path.join(tooldir, \"share\", \"java\")\n    rewrite_ignore = (\"log\",)\n    with contextlib.closing(urllib_request.urlopen(base_url)) as in_handle:\n        with open(out_file, \"w\") as out_handle:\n            in_resources = False\n            in_prog = None\n            for line in (l.decode(\"utf-8\") for l in in_handle):\n                if line[0] != \" \":\n                    in_resources = line.startswith(\"resources\")\n                    in_prog = None\n                elif (in_resources and line[:2] == \"  \" and line[2] != \" \"\n                      and not line.strip().startswith(rewrite_ignore)):\n                    in_prog = line.split(\":\")[0].strip()\n                # Update java directories to point to install directory, avoid special cases\n                elif line.strip().startswith(\"dir:\") and in_prog and in_prog not in [\"log\", \"tmp\"]:\n                    final_dir = os.path.basename(line.split()[-1])\n                    if tooldir:\n                        line = \"%s: %s\\n\" % (line.split(\":\")[0],\n                                             os.path.join(java_basedir, final_dir))\n                    in_prog = None\n                elif line.startswith(\"galaxy\"):\n                    line = \"# %s\" % line\n                out_handle.write(line)\n    return out_file", "response": "Write a bcbio_system. yaml configuration file with tool information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if required tools for installation are present.", "response": "def check_dependencies():\n    \"\"\"Ensure required tools for installation are present.\n    \"\"\"\n    print(\"Checking required dependencies\")\n    for dep, msg in [([\"git\", \"--version\"], \"Git (http://git-scm.com/)\"),\n                     ([\"wget\", \"--version\"], \"wget\"),\n                     ([\"bzip2\", \"-h\"], \"bzip2\")]:\n        try:\n            p = subprocess.Popen(dep, stderr=subprocess.STDOUT, stdout=subprocess.PIPE)\n            out, code = p.communicate()\n        except OSError:\n            out = \"Executable not found\"\n            code = 127\n        if code == 127:\n            raise OSError(\"bcbio-nextgen installer requires %s\\n%s\" % (msg, out))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check_toolplus(x):\n    import argparse\n    Tool = collections.namedtuple(\"Tool\", [\"name\", \"fname\"])\n    std_choices = set([\"data\", \"cadd\", \"dbnsfp\", \"ericscript\"])\n    if x in std_choices:\n        return Tool(x, None)\n    elif \"=\" in x and len(x.split(\"=\")) == 2:\n        name, fname = x.split(\"=\")\n        fname = os.path.normpath(os.path.realpath(fname))\n        if not os.path.exists(fname):\n            raise argparse.ArgumentTypeError(\"Unexpected --toolplus argument for %s. File does not exist: %s\"\n                                             % (name, fname))\n        return Tool(name, fname)\n    else:\n        raise argparse.ArgumentTypeError(\"Unexpected --toolplus argument. Expect toolname=filename.\")", "response": "Parse options for adding non - standard tools like GATK and MuTecT."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process(args):\n    # Set environment to standard to use periods for decimals and avoid localization\n    os.environ[\"LC_ALL\"] = \"C\"\n    os.environ[\"LC\"] = \"C\"\n    os.environ[\"LANG\"] = \"C\"\n    setpath.prepend_bcbiopath()\n    try:\n        fn = getattr(multitasks, args.name)\n    except AttributeError:\n        raise AttributeError(\"Did not find exposed function in bcbio.distributed.multitasks named '%s'\" % args.name)\n    if args.moreargs or args.raw:\n        fnargs = [args.argfile] + args.moreargs\n        work_dir = None\n        argfile = None\n    else:\n        with open(args.argfile) as in_handle:\n            fnargs = yaml.safe_load(in_handle)\n        work_dir = os.path.dirname(args.argfile)\n        fnargs = config_utils.merge_resources(fnargs)\n        argfile = args.outfile if args.outfile else \"%s-out%s\" % os.path.splitext(args.argfile)\n    if not work_dir:\n        work_dir = os.getcwd()\n    if len(fnargs) > 0 and fnargs[0] == \"cwl\":\n        fnargs, parallel, out_keys, input_files = _world_from_cwl(args.name, fnargs[1:], work_dir)\n        # Can remove this awkward Docker merge when we do not need custom GATK3 installs\n        fnargs = config_utils.merge_resources(fnargs)\n        argfile = os.path.join(work_dir, \"cwl.output.json\")\n    else:\n        parallel, out_keys, input_files = None, {}, []\n    with utils.chdir(work_dir):\n        with contextlib.closing(log.setup_local_logging(parallel={\"wrapper\": \"runfn\"})):\n            try:\n                out = fn(*fnargs)\n            except:\n                logger.exception()\n                raise\n            finally:\n                # Clean up any copied and unpacked workflow inputs, avoiding extra disk usage\n                wf_input_dir = os.path.join(work_dir, \"wf-inputs\")\n                if os.path.exists(wf_input_dir) and os.path.isdir(wf_input_dir):\n                    shutil.rmtree(wf_input_dir)\n    if argfile:\n        try:\n            _write_out_argfile(argfile, out, fnargs, parallel, out_keys, input_files, work_dir)\n        except:\n            logger.exception()\n            raise", "response": "Run the function in args. name given arguments in args. argfile."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _cwlvar_to_wdl(var):\n    if isinstance(var, (list, tuple)):\n        return [_cwlvar_to_wdl(x) for x in var]\n    elif isinstance(var, dict):\n        assert var.get(\"class\") == \"File\", var\n        # XXX handle secondary files\n        return var.get(\"path\") or var[\"value\"]\n    else:\n        return var", "response": "Convert a CWL output object into a WDL output."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites out an argument file to a CWL ready JSON or YAML representation of the world.", "response": "def _write_out_argfile(argfile, out, fnargs, parallel, out_keys, input_files, work_dir):\n    \"\"\"Write output argfile, preparing a CWL ready JSON or YAML representation of the world.\n    \"\"\"\n    with open(argfile, \"w\") as out_handle:\n        if argfile.endswith(\".json\"):\n            record_name, record_attrs = _get_record_attrs(out_keys)\n            if record_name:\n                if parallel in [\"multi-batch\"]:\n                    recs = _nested_cwl_record(out, record_attrs, input_files)\n                elif parallel in [\"single-split\", \"multi-combined\", \"multi-parallel\", \"batch-single\",\n                                  \"single-single\"]:\n                    recs = [_collapse_to_cwl_record_single(utils.to_single_data(xs), record_attrs, input_files)\n                            for xs in out]\n                else:\n                    samples = [utils.to_single_data(xs) for xs in out]\n                    recs = [_collapse_to_cwl_record(samples, record_attrs, input_files)]\n                json.dump(_combine_cwl_records(recs, record_name, parallel),\n                            out_handle, sort_keys=True, indent=4, separators=(', ', ': '))\n            elif parallel in [\"single-split\", \"multi-combined\", \"batch-split\"]:\n                json.dump(_convert_to_cwl_json([utils.to_single_data(xs) for xs in out], fnargs, input_files),\n                            out_handle, sort_keys=True, indent=4, separators=(', ', ': '))\n            else:\n                json.dump(_convert_to_cwl_json(utils.to_single_data(utils.to_single_data(out)), fnargs, input_files),\n                            out_handle, sort_keys=True, indent=4, separators=(', ', ': '))\n        else:\n            yaml.safe_dump(out, out_handle, default_flow_style=False, allow_unicode=False)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_record_attrs(out_keys):\n    if len(out_keys) == 1:\n        attr = list(out_keys.keys())[0]\n        if out_keys[attr]:\n            return attr, out_keys[attr]\n    return None, None", "response": "Check for records a single key plus output attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmerges input resources with current CWL runtime parameters.", "response": "def _add_resources(data, runtime):\n    \"\"\"Merge input resources with current CWL runtime parameters.\n    \"\"\"\n    if \"config\" not in data:\n        data[\"config\"] = {}\n    # Convert input resources, which may be a JSON string\n    resources = data.get(\"resources\", {}) or {}\n    if isinstance(resources, six.string_types) and resources.startswith((\"{\", \"[\")):\n        resources = json.loads(resources)\n        data[\"resources\"] = resources\n    assert isinstance(resources, dict), (resources, data)\n    data[\"config\"][\"resources\"] = resources\n    # Add in memory and core usage from CWL\n    memory = int(float(runtime[\"ram\"]) / float(runtime[\"cores\"]))\n    data[\"config\"][\"resources\"].update({\"default\": {\"cores\": int(runtime[\"cores\"]),\n                                                    \"memory\": \"%sM\" % memory,\n                                                    \"jvm_opts\": [\"-Xms%sm\" % min(1000, memory // 2),\n                                                                    \"-Xmx%sm\" % memory]}})\n    data[\"config\"][\"algorithm\"][\"num_cores\"] = int(runtime[\"cores\"])\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _world_from_cwl(fn_name, fnargs, work_dir):\n    parallel = None\n    output_cwl_keys = None\n    runtime = {}\n    out = []\n    data = {}\n    passed_keys = []\n    for fnarg in fnargs:\n        key, val = fnarg.split(\"=\")\n        # extra values pulling in nested indexes\n        if key == \"ignore\":\n            continue\n        if key == \"sentinel_parallel\":\n            parallel = val\n            continue\n        if key == \"sentinel_runtime\":\n            runtime = dict(tz.partition(2, val.split(\",\")))\n            continue\n        if key == \"sentinel_outputs\":\n            output_cwl_keys = _parse_output_keys(val)\n            continue\n        if key == \"sentinel_inputs\":\n            input_order = collections.OrderedDict([x.split(\":\") for x in val.split(\",\")])\n            continue\n        else:\n            assert key not in passed_keys, \"Multiple keys should be handled via JSON records\"\n            passed_keys.append(key)\n            key = key.split(\"__\")\n            data = _update_nested(key, _convert_value(val), data)\n    if data:\n        out.append(_finalize_cwl_in(data, work_dir, passed_keys, output_cwl_keys, runtime))\n\n    # Read inputs from standard files instead of command line\n    assert os.path.exists(os.path.join(work_dir, \"cwl.inputs.json\"))\n    out, input_files = _read_from_cwlinput(os.path.join(work_dir, \"cwl.inputs.json\"), work_dir, runtime, parallel,\n                                           input_order, output_cwl_keys)\n\n    if parallel in [\"single-parallel\", \"single-merge\", \"multi-parallel\", \"multi-combined\", \"multi-batch\",\n                    \"batch-split\", \"batch-parallel\", \"batch-merge\", \"batch-single\"]:\n        out = [out]\n    else:\n        assert len(out) == 1, \"%s\\n%s\" % (pprint.pformat(out), pprint.pformat(fnargs))\n    return out, parallel, output_cwl_keys, input_files", "response": "Reconstitute a bcbio world data object from flattened CWL - compatible inputs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_output_keys(val):\n    out = {}\n    for k in val.split(\",\"):\n        # record output\n        if \":\" in k:\n            name, attrs = k.split(\":\")\n            out[name] = attrs.split(\";\")\n        else:\n            out[k] = None\n    return out", "response": "Parse expected output keys from string handling records."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _find_input_files(var, out):\n    if isinstance(var, (list, tuple)):\n        for x in var:\n            out = _find_input_files(x, out)\n    elif isinstance(var, dict):\n        if var.get(\"class\") == \"File\":\n            out.append(var[\"path\"])\n            out = _find_input_files(var.get(\"secondaryFiles\", []), out)\n        for key, val in var.items():\n            out = _find_input_files(val, out)\n    return out", "response": "Find input files within the given CWL object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread data records from a JSON dump of inputs. Avoids command line flattening of records.", "response": "def _read_from_cwlinput(in_file, work_dir, runtime, parallel, input_order, output_cwl_keys):\n    \"\"\"Read data records from a JSON dump of inputs. Avoids command line flattening of records.\n    \"\"\"\n    with open(in_file) as in_handle:\n        inputs = json.load(in_handle)\n    items_by_key = {}\n    input_files = []\n    passed_keys = set([])\n    for key, input_val in ((k, v) for (k, v) in inputs.items() if not k.startswith((\"sentinel\", \"ignore\"))):\n        if key.endswith(\"_toolinput\"):\n            key = key.replace(\"_toolinput\", \"\")\n        if input_order[key] == \"record\":\n            cur_keys, items = _read_cwl_record(input_val)\n            passed_keys |= cur_keys\n            items_by_key[key] = items\n        else:\n            items_by_key[tuple(key.split(\"__\"))] = _cwlvar_to_wdl(input_val)\n        input_files = _find_input_files(input_val, input_files)\n    prepped = _merge_cwlinputs(items_by_key, input_order, parallel)\n    out = []\n    for data in prepped:\n        if isinstance(data, (list, tuple)):\n            out.append([_finalize_cwl_in(utils.to_single_data(x), work_dir, list(passed_keys),\n                                         output_cwl_keys, runtime) for x in data])\n        else:\n            out.append(_finalize_cwl_in(data, work_dir, list(passed_keys), output_cwl_keys, runtime))\n    return out, input_files"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _maybe_nest_bare_single(items_by_key, parallel):\n    if (parallel == \"multi-parallel\" and\n          (sum([1 for x in items_by_key.values() if not _is_nested_item(x)]) >=\n           sum([1 for x in items_by_key.values() if _is_nested_item(x)]))):\n        out = {}\n        for k, v in items_by_key.items():\n            out[k] = [v]\n        return out\n    else:\n        return items_by_key", "response": "Nest single inputs to avoid confusing single items and lists like files."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks for single nested inputs that match our target count and unnest.", "response": "def _check_for_single_nested(target, items_by_key, input_order):\n    \"\"\"Check for single nested inputs that match our target count and unnest.\n\n    Handles complex var inputs where some have an extra layer of nesting.\n    \"\"\"\n    out = utils.deepish_copy(items_by_key)\n    for (k, t) in input_order.items():\n        if t == \"var\":\n            v = items_by_key[tuple(k.split(\"__\"))]\n            if _is_nested_single(v, target):\n                out[tuple(k.split(\"__\"))] = v[0]\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _concat_records(items_by_key, input_order):\n    all_records = []\n    for (k, t) in input_order.items():\n        if t == \"record\":\n            all_records.append(k)\n    out_items_by_key = utils.deepish_copy(items_by_key)\n    out_input_order = utils.deepish_copy(input_order)\n    if len(all_records) > 1:\n        final_k = all_records[0]\n        final_v = items_by_key[final_k]\n        for k in all_records[1:]:\n            final_v += items_by_key[k]\n            del out_items_by_key[k]\n            del out_input_order[k]\n        out_items_by_key[final_k] = final_v\n    return out_items_by_key, out_input_order", "response": "Concatenate records into a single key to avoid merging."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmerging multiple cwl records and inputs handling multiple data items.", "response": "def _merge_cwlinputs(items_by_key, input_order, parallel):\n    \"\"\"Merge multiple cwl records and inputs, handling multiple data items.\n\n    Special cases:\n    - Single record but multiple variables (merging arrayed jobs). Assign lists\n      of variables to the record.\n    \"\"\"\n    items_by_key = _maybe_nest_bare_single(items_by_key, parallel)\n    if parallel == \"multi-combined\":\n        items_by_key, input_order = _concat_records(items_by_key, input_order)\n    var_items = set([_item_count(items_by_key[tuple(k.split(\"__\"))])\n                     for (k, t) in input_order.items() if t == \"var\"])\n    rec_items = set([_item_count(items_by_key[k]) for (k, t) in input_order.items() if t == \"record\"])\n    if var_items:\n        num_items = var_items\n        if len(num_items) == 2 and 1 in num_items:\n            num_items.remove(1)\n            items_by_key_test = _check_for_single_nested(num_items.pop(), items_by_key, input_order)\n            var_items = set([_item_count(items_by_key_test[tuple(k.split(\"__\"))])\n                             for (k, t) in input_order.items() if t == \"var\"])\n            num_items = var_items\n        assert len(num_items) == 1, \"Non-consistent variable data counts in CWL input:\\n%s\" % \\\n            (pprint.pformat(items_by_key))\n        items_by_key, num_items = _nest_vars_in_rec(var_items, rec_items, input_order, items_by_key, parallel)\n    else:\n        num_items = rec_items\n        assert len(num_items) == 1, \"Non-consistent record data counts in CWL input:\\n%s\" % \\\n            (pprint.pformat(items_by_key))\n    target_items = num_items.pop()\n    out = [{} for _ in range(target_items)]\n    for (cwl_key, cwl_type) in input_order.items():\n        if cwl_type == \"var\":\n            cwl_key = tuple(cwl_key.split(\"__\"))\n        cur_vals = items_by_key[cwl_key]\n        if _is_nested_single(cur_vals, target_items):\n            cur_vals = [[x] for x in cur_vals[0]]\n        for i, cur_val in enumerate(cur_vals):\n            if isinstance(cwl_key, (list, tuple)):\n                # nested batches with records\n                if (parallel.startswith((\"batch\", \"multi-parallel\")) and\n                      isinstance(out[i], (list, tuple))):\n                    for j in range(len(out[i])):\n                        out[i][j] = _update_nested(list(cwl_key), cur_val, out[i][j], allow_overwriting=True)\n                else:\n                    out[i] = _update_nested(list(cwl_key), cur_val, out[i], allow_overwriting=True)\n            elif out[i] == {}:\n                out[i] = cur_val\n            else:\n                # Handle single non-batched records\n                if isinstance(cur_val, (list, tuple)) and len(cur_val) == 1:\n                    cur_val = cur_val[0]\n                assert isinstance(cur_val, dict), (cwl_key, cur_val)\n                for k, v in cur_val.items():\n                    out[i] = _update_nested([k], v, out[i], allow_overwriting=True)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _nest_vars_in_rec(var_items, rec_items, input_order, items_by_key, parallel):\n    num_items = var_items\n    var_items = list(var_items)[0]\n    if rec_items:\n        rec_items = list(rec_items)[0]\n        if ((rec_items == 1 and var_items > 1) or parallel.startswith(\"batch\")):\n            num_items = set([rec_items])\n            for var_key in (k for (k, t) in input_order.items() if t != \"record\"):\n                var_key = tuple(var_key.split(\"__\"))\n                items_by_key[var_key] = [items_by_key[var_key]] * rec_items\n        else:\n            assert var_items == rec_items, (var_items, rec_items)\n    return items_by_key, num_items", "response": "Nest multiple variable inputs into a single record or list of batch records."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexpands record to apply to number of variants.", "response": "def _expand_rec_to_vars(var_items, rec_items, input_order, items_by_key, parallel):\n    \"\"\"Expand record to apply to number of variants.\n\n    Alternative approach to _nest_vars_in_rec\n    to combining a single record with multiple variants.\n    \"\"\"\n    num_items = var_items\n    var_items = list(var_items)[0]\n    if rec_items:\n        for rec_key in (k for (k, t) in input_order.items() if t == \"record\"):\n            rec_vals = items_by_key[rec_key]\n            if len(rec_vals) == 1 and var_items > 1:\n                items_by_key[rec_key] = rec_vals * var_items\n            else:\n                assert var_items == len(rec_vals), (var_items, rec_vals)\n    return items_by_key, num_items"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread CWL records handling multiple nesting and batching cases.", "response": "def _read_cwl_record(rec):\n    \"\"\"Read CWL records, handling multiple nesting and batching cases.\n    \"\"\"\n    keys = set([])\n    out = []\n    if isinstance(rec, dict):\n        is_batched = all([isinstance(v, (list, tuple)) for v in rec.values()])\n        cur = [{} for _ in range(len(rec.values()[0]) if is_batched else 1)]\n        for k in rec.keys():\n            keys.add(k)\n            val = rec[k]\n            val = val if is_batched else [val]\n            for i, v in enumerate(val):\n                v = _cwlvar_to_wdl(v)\n                cur[i] = _update_nested(k.split(\"__\"), v, cur[i])\n        if is_batched:\n            out.append(cur)\n        else:\n            assert len(cur) == 1\n            out.append(cur[0])\n    else:\n        assert isinstance(rec, (list, tuple))\n        for sub_rec in rec:\n            sub_keys, sub_out = _read_cwl_record(sub_rec)\n            keys |= sub_keys\n            out.append(sub_out)\n    return keys, out"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _finalize_cwl_in(data, work_dir, passed_keys, output_cwl_keys, runtime):\n    data[\"dirs\"] = {\"work\": work_dir}\n    if not tz.get_in([\"config\", \"algorithm\"], data):\n        if \"config\" not in data:\n            data[\"config\"] = {}\n        data[\"config\"][\"algorithm\"] = {}\n    if \"rgnames\" not in data and \"description\" in data:\n        data[\"rgnames\"] = {\"sample\": data[\"description\"]}\n    data[\"cwl_keys\"] = passed_keys\n    data[\"output_cwl_keys\"] = output_cwl_keys\n    data = _add_resources(data, runtime)\n    data = cwlutils.normalize_missing(data)\n    data = run_info.normalize_world(data)\n    return data", "response": "Finalize data object with inputs from CWL."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _convert_value(val):\n    def _is_number(x, op):\n        try:\n            op(x)\n            return True\n        except ValueError:\n            return False\n    if isinstance(val, (list, tuple)):\n        return [_convert_value(x) for x in val]\n    elif val is None:\n        return val\n    elif _is_number(val, int):\n        return int(val)\n    elif _is_number(val, float):\n        return float(val)\n    elif val.find(\";;\") >= 0:\n        return [_convert_value(v) for v in val.split(\";;\")]\n    elif val.startswith((\"{\", \"[\")):\n        # Can get ugly JSON output from CWL with unicode and ' instead of \"\n        # This tries to fix it so parsed correctly by json loader\n        return json.loads(val.replace(\"u'\", \"'\").replace(\"'\", '\"'))\n    elif val.lower() == \"true\":\n        return True\n    elif val.lower() == \"false\":\n        return False\n    else:\n        return val", "response": "Convert a value into a list of items."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _convert_to_cwl_json(data, fnargs, input_files):\n    out = {}\n    for outvar in _get_output_cwl_keys(fnargs):\n        keys = []\n        for key in outvar.split(\"__\"):\n            try:\n                key = int(key)\n            except ValueError:\n                pass\n            keys.append(key)\n        if isinstance(data, dict):\n            out[outvar] = _to_cwl(tz.get_in(keys, data), input_files)\n        else:\n            out[outvar] = [_to_cwl(tz.get_in(keys, x), input_files) for x in data]\n    return out", "response": "Convert world data object or list of data objects into CWL output files for CWL ingestion."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves output_cwl_keys from potentially nested input arguments.", "response": "def _get_output_cwl_keys(fnargs):\n    \"\"\"Retrieve output_cwl_keys from potentially nested input arguments.\n    \"\"\"\n    for d in utils.flatten(fnargs):\n        if isinstance(d, dict) and d.get(\"output_cwl_keys\"):\n            return d[\"output_cwl_keys\"]\n    raise ValueError(\"Did not find output_cwl_keys in %s\" % (pprint.pformat(fnargs)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a list of nexted CWL records keyed by output key.", "response": "def _combine_cwl_records(recs, record_name, parallel):\n    \"\"\"Provide a list of nexted CWL records keyed by output key.\n\n    Handles batches, where we return a list of records, and single items\n    where we return one record.\n    \"\"\"\n    if parallel not in [\"multi-batch\", \"single-split\", \"multi-combined\", \"batch-single\"]:\n        assert len(recs) == 1, pprint.pformat(recs)\n        return {record_name: recs[0]}\n    else:\n        return {record_name: recs}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a single sample into a CWL record.", "response": "def _collapse_to_cwl_record_single(data, want_attrs, input_files):\n    \"\"\"Convert a single sample into a CWL record.\n    \"\"\"\n    out = {}\n    for key in want_attrs:\n        key_parts = key.split(\"__\")\n        out[key] = _to_cwl(tz.get_in(key_parts, data), input_files)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _nested_cwl_record(xs, want_attrs, input_files):\n    if isinstance(xs, (list, tuple)):\n        return [_nested_cwl_record(x, want_attrs, input_files) for x in xs]\n    else:\n        assert isinstance(xs, dict), pprint.pformat(xs)\n        return _collapse_to_cwl_record_single(xs, want_attrs, input_files)", "response": "Convert arbitrarily nested samples into a nested list of dictionaries."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts nested samples from batches into a CWL record based on input keys.", "response": "def _collapse_to_cwl_record(samples, want_attrs, input_files):\n    \"\"\"Convert nested samples from batches into a CWL record, based on input keys.\n    \"\"\"\n    input_keys = sorted(list(set().union(*[d[\"cwl_keys\"] for d in samples])), key=lambda x: (-len(x), tuple(x)))\n    out = {}\n    for key in input_keys:\n        if key in want_attrs:\n            key_parts = key.split(\"__\")\n            vals = []\n            cur = []\n            for d in samples:\n                vals.append(_to_cwl(tz.get_in(key_parts, d), input_files))\n                # Remove nested keys to avoid specifying multiple times\n                cur.append(_dissoc_in(d, key_parts) if len(key_parts) > 1 else d)\n            samples = cur\n            out[key] = vals\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _file_and_exists(val, input_files):\n    return ((os.path.exists(val) and os.path.isfile(val)) or\n            val in input_files)", "response": "Check if an input is a file and exists."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a value into CWL formatted JSON handling files and complex things.", "response": "def _to_cwl(val, input_files):\n    \"\"\"Convert a value into CWL formatted JSON, handling files and complex things.\n    \"\"\"\n    if isinstance(val, six.string_types):\n        if _file_and_exists(val, input_files):\n            val = {\"class\": \"File\", \"path\": val}\n            secondary = []\n            for idx in [\".bai\", \".tbi\", \".gbi\", \".fai\", \".crai\", \".db\"]:\n                idx_file = val[\"path\"] + idx\n                if _file_and_exists(idx_file, input_files):\n                    secondary.append({\"class\": \"File\", \"path\": idx_file})\n            for idx in [\".dict\"]:\n                idx_file = os.path.splitext(val[\"path\"])[0] + idx\n                if _file_and_exists(idx_file, input_files):\n                    secondary.append({\"class\": \"File\", \"path\": idx_file})\n            cur_dir, cur_file = os.path.split(val[\"path\"])\n            # Handle relative paths\n            if not cur_dir:\n                cur_dir = os.getcwd()\n            if cur_file.endswith(cwlutils.DIR_TARGETS):\n                if os.path.exists(cur_dir):\n                    for fname in os.listdir(cur_dir):\n                        if fname != cur_file and not os.path.isdir(os.path.join(cur_dir, fname))\\\n                                and fname != 'sbg.worker.log':\n                            secondary.append({\"class\": \"File\", \"path\": os.path.join(cur_dir, fname)})\n                else:\n                    for f in input_files:\n                        if f.startswith(cur_dir) and f != cur_file and not os.path.isdir(f):\n                            secondary.append({\"class\": \"File\", \"path\": f})\n            if secondary:\n                val[\"secondaryFiles\"] = _remove_duplicate_files(secondary)\n    elif isinstance(val, (list, tuple)):\n        val = [_to_cwl(x, input_files) for x in val]\n    elif isinstance(val, dict):\n        # File representation with secondary files\n        if \"base\" in val and \"secondary\" in val:\n            out = {\"class\": \"File\", \"path\": val[\"base\"]}\n            secondary = [{\"class\": \"File\", \"path\": x} for x in val[\"secondary\"] if not os.path.isdir(x)]\n            if secondary:\n                out[\"secondaryFiles\"] = _remove_duplicate_files(secondary)\n            val = out\n        else:\n            val = json.dumps(val, sort_keys=True, separators=(',', ':'))\n    return val"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _remove_duplicate_files(xs):\n    seen = set([])\n    out = []\n    for x in xs:\n        if x[\"path\"] not in seen:\n            out.append(x)\n            seen.add(x[\"path\"])\n    return out", "response": "Remove files specified multiple times in a list."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _update_nested(key, val, data, allow_overwriting=False):\n    if isinstance(val, dict):\n        for sub_key, sub_val in val.items():\n            data = _update_nested(key + [sub_key], sub_val, data, allow_overwriting=allow_overwriting)\n    else:\n        already_there = tz.get_in(key, data) is not None\n        if already_there and val:\n            if not allow_overwriting:\n                raise ValueError(\"Duplicated key %s: %s and %s\" % (key, val, tz.get_in(key, data)))\n            else:\n                already_there = False\n        if val or not already_there:\n            data = tz.update_in(data, key, lambda x: val)\n    return data", "response": "Update the data object with nested dicts."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts the run server with provided command line arguments.", "response": "def start(args):\n    \"\"\"Run server with provided command line arguments.\n    \"\"\"\n    application = tornado.web.Application([(r\"/run\", run.get_handler(args)),\n                                           (r\"/status\", run.StatusHandler)])\n    application.runmonitor = RunMonitor()\n    application.listen(args.port)\n    tornado.ioloop.IOLoop.instance().start()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_subparser(subparsers):\n    parser = subparsers.add_parser(\"server\", help=\"Run a bcbio-nextgen server allowing remote job execution.\")\n    parser.add_argument(\"-c\", \"--config\", help=(\"Global YAML configuration file specifying system details.\"\n                                                \"Defaults to installed bcbio_system.yaml\"))\n    parser.add_argument(\"-p\", \"--port\", help=\"Port to listen on (default 8080)\",\n                        default=8080, type=int)\n    parser.add_argument(\"-n\", \"--cores\", help=\"Cores to use when processing locally when not requested (default 1)\",\n                        default=1, type=int)\n    parser.add_argument(\"-d\", \"--biodata_dir\", help=\"Directory with biological data\",\n                        default=\"/mnt/biodata\", type=str)\n    return parser", "response": "Add command line arguments as server subparser."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_versions(dirs, items):\n    genomes = {}\n    for d in items:\n        genomes[d[\"genome_build\"]] = d.get(\"reference\", {}).get(\"versions\")\n    out_file = _get_out_file(dirs)\n    found_versions = False\n    if genomes and out_file:\n        with open(out_file, \"w\") as out_handle:\n            writer = csv.writer(out_handle)\n            writer.writerow([\"genome\", \"resource\", \"version\"])\n            for genome, version_file in genomes.items():\n                if not version_file:\n                    genome_dir = install.get_genome_dir(genome, dirs.get(\"galaxy\"), items[0])\n                    if genome_dir:\n                        version_file = os.path.join(genome_dir, \"versions.csv\")\n                if version_file and os.path.exists(version_file):\n                    found_versions = True\n                    with open(version_file) as in_handle:\n                        reader = csv.reader(in_handle)\n                        for parts in reader:\n                            if len(parts) >= 2:\n                                resource, version = parts[:2]\n                                writer.writerow([genome, resource, version])\n    if found_versions:\n        return out_file", "response": "Write data versioning for genomes present in the configuration."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbatch together per sample and joint and germline calls for ensemble combination.", "response": "def batch(samples):\n    \"\"\"CWL: batch together per sample, joint and germline calls for ensemble combination.\n\n    Sets up groups of same sample/batch variant calls for ensemble calling, as\n    long as we have more than one caller per group.\n    \"\"\"\n    samples = [utils.to_single_data(x) for x in samples]\n    sample_order = [dd.get_sample_name(x) for x in samples]\n    batch_groups = collections.defaultdict(list)\n    for data in samples:\n        batch_samples = tuple(data.get(\"batch_samples\", [dd.get_sample_name(data)]))\n        batch_groups[(batch_samples, dd.get_phenotype(data))].append(data)\n\n    out = []\n    for (batch_samples, phenotype), gsamples in batch_groups.items():\n        if len(gsamples) > 1:\n            batches = set([])\n            for d in gsamples:\n                batches |= set(dd.get_batches(d))\n            gsamples.sort(key=dd.get_variantcaller_order)\n            cur = copy.deepcopy(gsamples[0])\n            cur.update({\"batch_id\": sorted(list(batches))[0] if batches else \"_\".join(batch_samples),\n                        \"batch_samples\": batch_samples,\n                        \"variants\": {\"variantcallers\": [dd.get_variantcaller(d) for d in gsamples],\n                                     \"calls\": [d.get(\"vrn_file\") for d in gsamples]}})\n            out.append(cur)\n\n    def by_original_order(d):\n        return min([sample_order.index(s) for s in d[\"batch_samples\"] if s in sample_order])\n    return sorted(out, key=by_original_order)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef combine_calls(*args):\n    if len(args) == 3:\n        is_cwl = False\n        batch_id, samples, data = args\n        caller_names, vrn_files = _organize_variants(samples, batch_id)\n    else:\n        is_cwl = True\n        samples = [utils.to_single_data(x) for x in args]\n        samples = [cwlutils.unpack_tarballs(x, x) for x in samples]\n        data = samples[0]\n        batch_id = data[\"batch_id\"]\n        caller_names = data[\"variants\"][\"variantcallers\"]\n        vrn_files = data[\"variants\"][\"calls\"]\n    logger.info(\"Ensemble consensus calls for {0}: {1}\".format(\n        batch_id, \",\".join(caller_names)))\n    edata = copy.deepcopy(data)\n    base_dir = utils.safe_makedir(os.path.join(edata[\"dirs\"][\"work\"], \"ensemble\", batch_id))\n    if any([vcfutils.vcf_has_variants(f) for f in vrn_files]):\n        # Decompose multiallelic variants and normalize\n        passonly = not tz.get_in([\"config\", \"algorithm\", \"ensemble\", \"use_filtered\"], edata, False)\n        vrn_files = [normalize.normalize(f, data, passonly=passonly, rerun_effects=False, remove_oldeffects=True,\n                                         nonrefonly=True,\n                                         work_dir=utils.safe_makedir(os.path.join(base_dir, c)))\n                     for c, f in zip(caller_names, vrn_files)]\n        if \"classifiers\" not in (dd.get_ensemble(edata) or {}):\n            callinfo = _run_ensemble_intersection(batch_id, vrn_files, caller_names, base_dir, edata)\n        else:\n            config_file = _write_config_file(batch_id, caller_names, base_dir, edata)\n            callinfo = _run_ensemble(batch_id, vrn_files, config_file, base_dir,\n                                     dd.get_ref_file(edata), edata)\n            callinfo[\"vrn_file\"] = vcfutils.bgzip_and_index(callinfo[\"vrn_file\"], data[\"config\"])\n        # After decomposing multiallelic variants and normalizing, re-evaluate effects\n        ann_ma_file, _ = effects.add_to_vcf(callinfo[\"vrn_file\"], data)\n        if ann_ma_file:\n            callinfo[\"vrn_file\"] = ann_ma_file\n\n        edata[\"config\"][\"algorithm\"][\"variantcaller\"] = \"ensemble\"\n        edata[\"vrn_file\"] = callinfo[\"vrn_file\"]\n        edata[\"ensemble_bed\"] = callinfo[\"bed_file\"]\n        callinfo[\"validate\"] = validate.compare_to_rm(edata)[0][0].get(\"validate\")\n    else:\n        out_vcf_file = os.path.join(base_dir, \"{0}-ensemble.vcf\".format(batch_id))\n        vcfutils.write_empty_vcf(out_vcf_file, samples=[dd.get_sample_name(d) for d in samples])\n        callinfo = {\"variantcaller\": \"ensemble\",\n                    \"vrn_file\": vcfutils.bgzip_and_index(out_vcf_file, data[\"config\"]),\n                    \"bed_file\": None}\n    if is_cwl:\n        callinfo[\"batch_samples\"] = data[\"batch_samples\"]\n        callinfo[\"batch_id\"] = batch_id\n        return [{\"ensemble\": callinfo}]\n    else:\n        return [[batch_id, callinfo]]", "response": "Combine multiple callsets into a final set of merged calls."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncombines calls using batched Ensemble approach.", "response": "def combine_calls_parallel(samples, run_parallel):\n    \"\"\"Combine calls using batched Ensemble approach.\n    \"\"\"\n    batch_groups, extras = _group_by_batches(samples, _has_ensemble)\n    out = []\n    if batch_groups:\n        processed = run_parallel(\"combine_calls\", ((b, xs, xs[0]) for b, xs in batch_groups.items()))\n        for batch_id, callinfo in processed:\n            for data in batch_groups[batch_id]:\n                data[\"variants\"].insert(0, callinfo)\n                out.append([data])\n    return out + extras"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _group_by_batches(samples, check_fn):\n    batch_groups = collections.defaultdict(list)\n    extras = []\n    for data in [x[0] for x in samples]:\n        if check_fn(data):\n            batch_groups[multi.get_batch_for_key(data)].append(data)\n        else:\n            extras.append([data])\n    return batch_groups, extras", "response": "Group samples by batches."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves variant calls for all samples merging batched samples into single VCF.", "response": "def _organize_variants(samples, batch_id):\n    \"\"\"Retrieve variant calls for all samples, merging batched samples into single VCF.\n    \"\"\"\n    caller_names = [x[\"variantcaller\"] for x in samples[0][\"variants\"]]\n    calls = collections.defaultdict(list)\n    for data in samples:\n        for vrn in data[\"variants\"]:\n            calls[vrn[\"variantcaller\"]].append(vrn[\"vrn_file\"])\n    data = samples[0]\n    vrn_files = []\n    for caller in caller_names:\n        fnames = calls[caller]\n        if len(fnames) == 1:\n            vrn_files.append(fnames[0])\n        else:\n            vrn_files.append(population.get_multisample_vcf(fnames, batch_id, caller, data))\n    return caller_names, vrn_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _handle_somatic_ensemble(vrn_file, data):\n    if tz.get_in([\"metadata\", \"phenotype\"], data, \"\").lower().startswith(\"tumor\"):\n        vrn_file_temp = vrn_file.replace(\".vcf\", \"_tumorOnly_noFilteredCalls.vcf\")\n        # Select tumor sample and keep only PASS and . calls\n        vrn_file = vcfutils.select_sample(in_file=vrn_file, sample=data[\"name\"][1],\n                                          out_file=vrn_file_temp,\n                                          config=data[\"config\"], filters=\"PASS,.\")\n    return vrn_file", "response": "Handle somatic ensemble calls."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun a variant comparison using the bcbio. variation toolkit.", "response": "def _bcbio_variation_ensemble(vrn_files, out_file, ref_file, config_file, base_dir, data):\n    \"\"\"Run a variant comparison using the bcbio.variation toolkit, given an input configuration.\n    \"\"\"\n    vrn_files = [_handle_somatic_ensemble(v, data) for v in vrn_files]\n    tmp_dir = utils.safe_makedir(os.path.join(base_dir, \"tmp\"))\n    resources = config_utils.get_resources(\"bcbio_variation\", data[\"config\"])\n    jvm_opts = resources.get(\"jvm_opts\", [\"-Xms750m\", \"-Xmx2g\"])\n    java_args = [\"-Djava.io.tmpdir=%s\" % tmp_dir]\n    cmd = [\"bcbio-variation\"] + jvm_opts + java_args + \\\n          [\"variant-ensemble\", config_file, ref_file, out_file] + vrn_files\n    with utils.chdir(base_dir):\n        cmd = \"%s %s\" % (utils.local_path_export(), \" \".join(str(x) for x in cmd))\n        do.run(cmd, \"Ensemble calling: %s\" % os.path.basename(base_dir))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns an ensemble call using merging and SVM - based approach in bcbio. variation", "response": "def _run_ensemble(batch_id, vrn_files, config_file, base_dir, ref_file, data):\n    \"\"\"Run an ensemble call using merging and SVM-based approach in bcbio.variation\n    \"\"\"\n    out_vcf_file = os.path.join(base_dir, \"{0}-ensemble.vcf\".format(batch_id))\n    out_bed_file = os.path.join(base_dir, \"{0}-callregions.bed\".format(batch_id))\n    work_dir = \"%s-work\" % os.path.splitext(out_vcf_file)[0]\n    if not utils.file_exists(out_vcf_file):\n        _bcbio_variation_ensemble(vrn_files, out_vcf_file, ref_file, config_file,\n                                  base_dir, data)\n        if not utils.file_exists(out_vcf_file):\n            base_vcf = glob.glob(os.path.join(work_dir, \"prep\", \"*-cfilter.vcf\"))[0]\n            utils.symlink_plus(base_vcf, out_vcf_file)\n    if not utils.file_exists(out_bed_file):\n        multi_beds = glob.glob(os.path.join(work_dir, \"prep\", \"*-multicombine.bed\"))\n        if len(multi_beds) > 0:\n            utils.symlink_plus(multi_beds[0], out_bed_file)\n    return {\"variantcaller\": \"ensemble\",\n            \"vrn_file\": out_vcf_file,\n            \"bed_file\": out_bed_file if os.path.exists(out_bed_file) else None}"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites YAML configuration to generate an ensemble set of combined calls.", "response": "def _write_config_file(batch_id, caller_names, base_dir, data):\n    \"\"\"Write YAML configuration to generate an ensemble set of combined calls.\n    \"\"\"\n    config_dir = utils.safe_makedir(os.path.join(base_dir, \"config\"))\n    config_file = os.path.join(config_dir, \"{0}-ensemble.yaml\".format(batch_id))\n    algorithm = data[\"config\"][\"algorithm\"]\n    econfig = {\"ensemble\": algorithm[\"ensemble\"],\n               \"names\": caller_names,\n               \"prep-inputs\": False}\n    intervals = validate.get_analysis_intervals(data, None, base_dir)\n    if intervals:\n        econfig[\"intervals\"] = os.path.abspath(intervals)\n    with open(config_file, \"w\") as out_handle:\n        yaml.safe_dump(econfig, out_handle, allow_unicode=False, default_flow_style=False)\n    return config_file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_num_pass(data, n):\n    numpass = tz.get_in([\"config\", \"algorithm\", \"ensemble\", \"numpass\"], data)\n    if numpass:\n        return int(numpass)\n    trusted_pct = tz.get_in([\"config\", \"algorithm\", \"ensemble\", \"trusted_pct\"], data)\n    if trusted_pct:\n        return int(math.ceil(float(trusted_pct) * n))\n    return 2", "response": "Calculate the number of samples needed to pass ensemble calling."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _run_ensemble_intersection(batch_id, vrn_files, callers, base_dir, edata):\n    out_vcf_file = os.path.join(base_dir, \"{0}-ensemble.vcf.gz\".format(batch_id))\n    if not utils.file_exists(out_vcf_file):\n        num_pass = _get_num_pass(edata, len(vrn_files))\n        cmd = [\n            config_utils.get_program(\n                \"bcbio-variation-recall\", edata[\"config\"]),\n            \"ensemble\",\n            \"--cores=%s\" % edata[\"config\"][\"algorithm\"].get(\"num_cores\", 1),\n            \"--numpass\", str(num_pass),\n            \"--names\", \",\".join(callers)\n        ]\n        # Remove filtered calls, do not try to rescue, unless configured\n        if not tz.get_in([\"config\", \"algorithm\", \"ensemble\", \"use_filtered\"], edata):\n            cmd += [\"--nofiltered\"]\n\n        with file_transaction(edata, out_vcf_file) as tx_out_file:\n            cmd += [tx_out_file, dd.get_ref_file(edata)] + vrn_files\n            cmd = \"%s && %s\" % (utils.get_java_clprep(), \" \".join(str(x) for x in cmd))\n            do.run(cmd, \"Ensemble intersection calling: %s\" % (batch_id))\n    in_data = utils.deepish_copy(edata)\n    in_data[\"vrn_file\"] = out_vcf_file\n    return {\"variantcaller\": \"ensemble\",\n            \"vrn_file\": out_vcf_file,\n            \"bed_file\": None}", "response": "Run intersection with ensemble method using bcbio. variation. recall."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget dictionary of loc_type loc_file. loc files in the galaxy base", "response": "def get_loc_files(galaxy_base):\n    \"\"\"\n    get dictionary of loc_type: loc_file, .loc files in the galaxy base\n    for example: {\"bwa\": \"/galaxy_base_path/tool-dir/bwa_index.loc\"}\n    \"\"\"\n    return {k: os.path.join(galaxy_base, \"tool-data\", v) for k, v in REF_FILES.items()}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting dictionary of genome location for all genomes of type loc_type in a. loc file", "response": "def get_genome_refs(loc_file, loc_type):\n    \"\"\"\n    get dictionary of genome: location for all genomes of type in a .loc file\n    for example: {'hg19': '/genomedir/Hsapiens/hg19/seq/hg19.fa'}\n    \"\"\"\n    if not file_exists(loc_file):\n        return None\n    refs = {}\n    with open(loc_file) as in_handle:\n        for line in in_handle:\n            if not line.startswith(\"#\"):\n                parts = line.strip().split()\n                if loc_type in [\"bowtie2\", \"samtools\", \"alignseq\"]:\n                    refs[parts[1]] = parts[-1]\n                else:\n                    refs[parts[0]] = parts[-1]\n    return refs"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves regions to parallelize by from callable regions or chromosomes.", "response": "def _get_callable_regions(data):\n    \"\"\"Retrieve regions to parallelize by from callable regions or chromosomes.\n    \"\"\"\n    import pybedtools\n    callable_files = data.get(\"callable_regions\")\n    if callable_files:\n        assert len(callable_files) == 1\n        regions = [(r.chrom, int(r.start), int(r.stop)) for r in pybedtools.BedTool(callable_files[0])]\n    else:\n        work_bam = list(tz.take(1, filter(lambda x: x and x.endswith(\".bam\"), data[\"work_bams\"])))\n        if work_bam:\n            with pysam.Samfile(work_bam[0], \"rb\") as pysam_bam:\n                regions = [(chrom, 0, length) for (chrom, length) in zip(pysam_bam.references,\n                                                                         pysam_bam.lengths)]\n        else:\n            regions = [(r.name, 0, r.size) for r in\n                       ref.file_contigs(dd.get_ref_file(data), data[\"config\"])]\n    return regions"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _split_by_callable_region(data):\n    batch = tz.get_in((\"metadata\", \"batch\"), data)\n    jointcaller = tz.get_in((\"config\", \"algorithm\", \"jointcaller\"), data)\n    name = batch if batch else tz.get_in((\"rgnames\", \"sample\"), data)\n    out_dir = utils.safe_makedir(os.path.join(data[\"dirs\"][\"work\"], \"joint\", jointcaller, name))\n    utils.safe_makedir(os.path.join(out_dir, \"inprep\"))\n    parts = []\n    for feat in _get_callable_regions(data):\n        region_dir = utils.safe_makedir(os.path.join(out_dir, feat[0]))\n        region_prep_dir = os.path.join(region_dir, \"inprep\")\n        if not os.path.exists(region_prep_dir):\n            os.symlink(os.path.join(os.pardir, \"inprep\"), region_prep_dir)\n        region_outfile = os.path.join(region_dir, \"%s-%s.vcf.gz\" % (batch, region.to_safestr(feat)))\n        parts.append((feat, data[\"work_bams\"], data[\"vrn_files\"], region_outfile))\n    out_file = os.path.join(out_dir, \"%s-joint.vcf.gz\" % name)\n    return out_file, parts", "response": "Split by callable or variant regions."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _is_jointcaller_compatible(data):\n    jointcaller = tz.get_in((\"config\", \"algorithm\", \"jointcaller\"), data)\n    variantcaller = tz.get_in((\"config\", \"algorithm\", \"variantcaller\"), data)\n    if isinstance(variantcaller, (list, tuple)) and len(variantcaller) == 1:\n        variantcaller = variantcaller[0]\n    return jointcaller == \"%s-joint\" % variantcaller or not variantcaller", "response": "Match variant caller inputs to compatible joint callers."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef square_off(samples, run_parallel):\n    to_process = []\n    extras = []\n    for data in [utils.to_single_data(x) for x in samples]:\n        added = False\n        if tz.get_in((\"metadata\", \"batch\"), data):\n            for add in genotype.handle_multiple_callers(data, \"jointcaller\", require_bam=False):\n                if _is_jointcaller_compatible(add):\n                    added = True\n                    to_process.append([add])\n        if not added:\n            extras.append([data])\n    processed = grouped_parallel_split_combine(to_process, _split_by_callable_region,\n                                               multi.group_batches_joint, run_parallel,\n                                               \"square_batch_region\", \"concat_variant_files\",\n                                               \"vrn_file\", [\"region\", \"sam_ref\", \"config\"])\n    return _combine_to_jointcaller(processed) + extras", "response": "Perform joint calling at all variants within a batch."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _combine_to_jointcaller(processed):\n    by_vrn_file = collections.OrderedDict()\n    for data in (x[0] for x in processed):\n        key = (tz.get_in((\"config\", \"algorithm\", \"jointcaller\"), data), data[\"vrn_file\"])\n        if key not in by_vrn_file:\n            by_vrn_file[key] = []\n        by_vrn_file[key].append(data)\n    out = []\n    for grouped_data in by_vrn_file.values():\n        cur = grouped_data[0]\n        out.append([cur])\n    return out", "response": "Combine joint calling information to variants while collapsing independent regions."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform squaring of a batch in a supplied region.", "response": "def square_batch_region(data, region, bam_files, vrn_files, out_file):\n    \"\"\"Perform squaring of a batch in a supplied region, with input BAMs\n    \"\"\"\n    from bcbio.variation import sentieon, strelka2\n    if not utils.file_exists(out_file):\n        jointcaller = tz.get_in((\"config\", \"algorithm\", \"jointcaller\"), data)\n        if jointcaller in [\"%s-joint\" % x for x in SUPPORTED[\"general\"]]:\n            _square_batch_bcbio_variation(data, region, bam_files, vrn_files, out_file, \"square\")\n        elif jointcaller in [\"%s-merge\" % x for x in SUPPORTED[\"general\"]]:\n            _square_batch_bcbio_variation(data, region, bam_files, vrn_files, out_file, \"merge\")\n        elif jointcaller in [\"%s-joint\" % x for x in SUPPORTED[\"gatk\"]]:\n            gatkjoint.run_region(data, region, vrn_files, out_file)\n        elif jointcaller in [\"%s-joint\" % x for x in SUPPORTED[\"gvcf\"]]:\n            strelka2.run_gvcfgenotyper(data, region, vrn_files, out_file)\n        elif jointcaller in [\"%s-joint\" % x for x in SUPPORTED[\"sentieon\"]]:\n            sentieon.run_gvcftyper(vrn_files, out_file, region, data)\n        else:\n            raise ValueError(\"Unexpected joint calling approach: %s.\" % jointcaller)\n    if region:\n        data[\"region\"] = region\n    data = _fix_orig_vcf_refs(data)\n    data[\"vrn_file\"] = out_file\n    return [data]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _fix_orig_vcf_refs(data):\n    variantcaller = tz.get_in((\"config\", \"algorithm\", \"variantcaller\"), data)\n    if variantcaller:\n        data[\"vrn_file_orig\"] = data[\"vrn_file\"]\n    for i, sub in enumerate(data.get(\"group_orig\", [])):\n        sub_vrn = sub.pop(\"vrn_file\", None)\n        if sub_vrn:\n            sub[\"vrn_file_orig\"] = sub_vrn\n            data[\"group_orig\"][i] = sub\n    return data", "response": "Supply references to initial variantcalls if run in addition to batching.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _square_batch_bcbio_variation(data, region, bam_files, vrn_files, out_file,\n                                  todo=\"square\"):\n    \"\"\"Run squaring or merging analysis using bcbio.variation.recall.\n    \"\"\"\n    ref_file = tz.get_in((\"reference\", \"fasta\", \"base\"), data)\n    cores = tz.get_in((\"config\", \"algorithm\", \"num_cores\"), data, 1)\n    resources = config_utils.get_resources(\"bcbio-variation-recall\", data[\"config\"])\n    # adjust memory by cores but leave room for run program memory\n    memcores = int(math.ceil(float(cores) / 5.0))\n    jvm_opts = config_utils.adjust_opts(resources.get(\"jvm_opts\", [\"-Xms250m\", \"-Xmx2g\"]),\n                                        {\"algorithm\": {\"memory_adjust\": {\"direction\": \"increase\",\n                                                                         \"magnitude\": memcores}}})\n    # Write unique VCFs and BAMs to input file\n    input_file = \"%s-inputs.txt\" % os.path.splitext(out_file)[0]\n    with open(input_file, \"w\") as out_handle:\n        out_handle.write(\"\\n\".join(sorted(list(set(vrn_files)))) + \"\\n\")\n        if todo == \"square\":\n            out_handle.write(\"\\n\".join(sorted(list(set(bam_files)))) + \"\\n\")\n    variantcaller = tz.get_in((\"config\", \"algorithm\", \"jointcaller\"), data).replace(\"-joint\", \"\")\n    cmd = [\"bcbio-variation-recall\", todo] + jvm_opts + broad.get_default_jvm_opts() + \\\n          [\"-c\", cores, \"-r\", bamprep.region_to_gatk(region)]\n    if todo == \"square\":\n        cmd += [\"--caller\", variantcaller]\n    cmd += [out_file, ref_file, input_file]\n    bcbio_env = utils.get_bcbio_env()\n    cmd = \" \".join(str(x) for x in cmd)\n    do.run(cmd, \"%s in region: %s\" % (cmd, bamprep.region_to_gatk(region)), env=bcbio_env)\n    return out_file", "response": "Perform squaring or merging analysis using bcbio. variation. recall."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload data from collectl files into a pandas DataFrame.", "response": "def load_collectl(pattern, start_time, end_time):\n    \"\"\"Read data from collectl data files into a pandas DataFrame.\n        :pattern: Absolute path to raw collectl files\n    \"\"\"\n    start_tstamp = calendar.timegm(start_time.utctimetuple())\n    end_tstamp = calendar.timegm(end_time.utctimetuple())\n\n    cols = []\n    rows = []\n\n    for path in glob.glob(pattern):\n        hardware, raw = _parse_raw(\n            _CollectlGunzip(path, 'r'), start_tstamp, end_tstamp)\n\n        if not cols:\n            instances = {\n                'disk': set(),\n                'net': set(),\n                'proc': set(),\n            }\n            for tstamp, sample in raw.items():\n                for group, items in sample.items():\n                    if group == 'disk':\n                        instances['disk'] = instances['disk'].union(\n                            items.keys())\n                    elif group == 'net':\n                        instances['net'] = instances['net'].union(\n                            items.keys())\n                    elif group == 'proc':\n                        instances['proc'] = instances['proc'].union(\n                            items.keys())\n\n            cols = ['tstamp']\n            cols.extend([\n                'cpu_{}'.format(var)\n                for var\n                 in ['user', 'nice', 'sys', 'idle', 'wait',\n                     'irq', 'soft', 'steal']\n                 ])\n            for node in instances['disk']:\n                cols.extend([\n                    '{}_{}'.format(node, var)\n                    for var\n                     in ['num_reads', 'reads_merged',\n                         'sectors_read', 'msec_spent_reading',\n                         'num_writes', 'writes_merged',\n                         'sectors_written', 'msec_spent_writing',\n                         'iops_in_progress', 'msec_spent_on_iops',\n                         'weighted_msec_spent_on_iops']\n                     ])\n            cols.extend([\n                'mem_{}'.format(var)\n                for var\n                 in ['total', 'free', 'buffers', 'cached']\n                 ])\n            for iface in instances['net']:\n                cols.extend([\n                    '{}_{}'.format(iface, var)\n                    for var\n                     in ['rbyte', 'rpkt', 'rerr', 'rdrop',\n                         'rfifo', 'rframe', 'rcomp', 'rmulti',\n                         'tbyte', 'tpkt', 'terr', 'tdrop',\n                         'tfifo', 'tcoll', 'tcarrier', 'tcomp']\n                     ])\n            for pid in instances['proc']:\n                cols.extend([\n                    '{}_{}'.format(pid, var)\n                    for var\n                     in ['name', 'read_bytes', 'write_bytes']\n                     ])\n\n        for tstamp, sample in raw.items():\n            if ('cpu' not in sample or\n                'disk' not in sample or\n                'mem' not in sample):\n                # Skip incomplete samples; there might be a truncated\n                # sample on the end of the file.\n                continue\n\n            values = [tstamp]\n            values.extend([\n                sample['cpu']['user'], sample['cpu']['nice'],\n                sample['cpu']['sys'], sample['cpu']['idle'],\n                sample['cpu']['wait'], sample['cpu']['irq'],\n                sample['cpu']['soft'], sample['cpu']['steal'],\n            ])\n            for node in instances['disk']:\n                data = sample['disk'].get(node, {})\n                values.extend([\n                    data.get('num_reads', 0),\n                    data.get('reads_merged', 0),\n                    data.get('sectors_read', 0),\n                    data.get('msec_spent_reading', 0),\n                    data.get('num_writes', 0),\n                    data.get('writes_merged', 0),\n                    data.get('sectors_written', 0),\n                    data.get('msec_spent_writing', 0),\n                    data.get('iops_in_progress', 0),\n                    data.get('msec_spent_on_iops', 0),\n                    data.get('weighted_msec_spent_on_iops', 0),\n                ])\n            values.extend([\n                sample['mem']['total'], sample['mem']['free'],\n                sample['mem']['buffers'], sample['mem']['cached'],\n            ])\n            for iface in instances['net']:\n                data = sample['net'].get(iface, {})\n                values.extend([\n                    data.get('rbyte', 0), data.get('rpkt', 0),\n                    data.get('rerr', 0), data.get('rdrop', 0),\n                    data.get('rfifo', 0), data.get('rframe', 0),\n                    data.get('rcomp', 0), data.get('rmulti', 0),\n                    data.get('tbyte', 0), data.get('tpkt', 0),\n                    data.get('terr', 0), data.get('tdrop', 0),\n                    data.get('tfifo', 0), data.get('tcoll', 0),\n                    data.get('tcarrier', 0), data.get('tcomp', 0),\n                ])\n            if 'proc' in sample:\n                for pid in instances['proc']:\n                    data = sample['proc'].get(pid, {})\n                    values.extend([\n                        data.get('cmd', ''),\n                        data.get('read_bytes', 0),\n                        data.get('write_bytes', 0),\n                    ])\n\n            rows.append(values)\n\n    if len(rows) == 0:\n        return pd.DataFrame(columns=cols), {}\n\n    df = pd.DataFrame(rows, columns=cols)\n    df = df.convert_objects(convert_numeric=True)\n    df['tstamp'] = df['tstamp'].astype('datetime64[s]')\n    df.set_index('tstamp', inplace=True)\n    df = df.tz_localize('UTC')\n\n    return df, hardware"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef precall(data):\n    data = utils.to_single_data(data)\n    bed_file = tz.get_in([\"config\", \"algorithm\", \"seq2c_bed_ready\"], data)\n    if not bed_file:\n        raise ValueError(\"Error: svregions or variant_regions BED file required for Seq2C\")\n    sample_name = dd.get_sample_name(data)\n    work_dir = _sv_workdir(data)\n    return _calculate_coverage(data, work_dir, bed_file, sample_name)", "response": "Perform initial pre - calling steps for seq2c variant coverage calculation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(items):\n    items = [utils.to_single_data(x) for x in items]\n    work_dir = _sv_workdir(items[0])\n\n    input_backs = list(set(filter(lambda x: x is not None,\n                                  [dd.get_background_cnv_reference(d, \"seq2c\") for d in items])))\n    coverage_file = _combine_coverages(items, work_dir, input_backs)\n    read_mapping_file = _calculate_mapping_reads(items, work_dir, input_backs)\n    normal_names = []\n    if input_backs:\n        with open(input_backs[0]) as in_handle:\n            for line in in_handle:\n                if len(line.split()) == 2:\n                    normal_names.append(line.split()[0])\n    normal_names += [dd.get_sample_name(x) for x in items if population.get_affected_status(x) == 1]\n    seq2c_calls_file = _call_cnv(items, work_dir, read_mapping_file, coverage_file, normal_names)\n    items = _split_cnv(items, seq2c_calls_file, read_mapping_file, coverage_file)\n    return items", "response": "This function performs the CNV calling for the cohort and returns the results."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nselects the bed file cleaning and properly annotating for Seq2C", "response": "def prep_seq2c_bed(data):\n    \"\"\"Selecting the bed file, cleaning, and properly annotating for Seq2C\n    \"\"\"\n    if dd.get_background_cnv_reference(data, \"seq2c\"):\n        bed_file = _background_to_bed(dd.get_background_cnv_reference(data, \"seq2c\"), data)\n    else:\n        bed_file = regions.get_sv_bed(data)\n    if bed_file:\n        bed_file = bedutils.clean_file(bed_file, data, prefix=\"svregions-\")\n    else:\n        bed_file = bedutils.clean_file(dd.get_variant_regions(data), data)\n    if not bed_file:\n        return None\n\n    col_num = bt.BedTool(bed_file).field_count()\n    if col_num < 4:\n        annotated_file = annotate.add_genes(bed_file, data, max_distance=0)\n        if annotated_file == bed_file:\n            raise ValueError(\"BED file for Seq2C must be annotated with gene names, \"\n                             \"however the input BED is 3-columns and we have no transcript \"\n                             \"data to annotate with \" + bed_file)\n        annotated_file = annotate.gene_one_per_line(annotated_file, data)\n    else:\n        annotated_file = bed_file\n\n    ready_file = \"%s-seq2cclean.bed\" % (utils.splitext_plus(annotated_file)[0])\n    if not utils.file_uptodate(ready_file, annotated_file):\n        bed = bt.BedTool(annotated_file)\n        if col_num > 4 and col_num != 8:\n            bed = bed.cut(range(4))\n        bed = bed.filter(lambda x: x.name not in [\"\", \".\", \"-\"])\n        with file_transaction(data, ready_file) as tx_out_file:\n            bed.saveas(tx_out_file)\n        logger.debug(\"Saved Seq2C clean annotated ready input BED into \" + ready_file)\n\n    return ready_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a seq2c background file with calls into BED regions for coverage.", "response": "def _background_to_bed(back_file, data):\n    \"\"\"Convert a seq2c background file with calls into BED regions for coverage.\n\n    seq2c background files are a concatenation of mapping and sample_coverages from\n    potentially multiple samples. We use the coverage information from the first\n    sample to translate into BED.\n    \"\"\"\n    out_file = os.path.join(utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"bedprep\")),\n                            \"%s-regions.bed\" % utils.splitext_plus(os.path.basename(back_file))[0])\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            with open(back_file) as in_handle:\n                with open(tx_out_file, \"w\") as out_handle:\n                    sample = in_handle.readline().split(\"\\t\")[0]\n                    for line in in_handle:\n                        if line.startswith(sample) and len(line.split()) >= 5:\n                            _, gene, chrom, start, end = line.split()[:5]\n                            out_handle.write(\"%s\\n\" % (\"\\t\".join([chrom, str(int(start) - 1), end, gene])))\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting adjustable through resources or default options for seq2c.", "response": "def _get_seq2c_options(data):\n    \"\"\"Get adjustable, through resources, or default options for seq2c.\n    \"\"\"\n    cov2lr_possible_opts = [\"-F\"]\n    defaults = {}\n    ropts = config_utils.get_resources(\"seq2c\", data[\"config\"]).get(\"options\", [])\n    assert len(ropts) % 2 == 0, \"Expect even number of options for seq2c\" % ropts\n    defaults.update(dict(tz.partition(2, ropts)))\n    cov2lr_out, lr2gene_out = [], []\n    for k, v in defaults.items():\n        if k in cov2lr_possible_opts:\n            cov2lr_out += [str(k), str(v)]\n        else:\n            lr2gene_out += [str(k), str(v)]\n    return cov2lr_out, lr2gene_out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_vcf(in_tsv, data):\n    call_convert = {\"Amp\": \"DUP\", \"Del\": \"DEL\"}\n    out_file = \"%s.vcf\" % utils.splitext_plus(in_tsv)[0]\n    if not utils.file_uptodate(out_file, in_tsv):\n        with file_transaction(data, out_file) as tx_out_file:\n            with open(in_tsv) as in_handle:\n                with open(tx_out_file, \"w\") as out_handle:\n                    out_handle.write(VCF_HEADER + \"#CHROM\\tPOS\\tID\\tREF\\tALT\\tQUAL\\tFILTER\\tINFO\\tFORMAT\\t%s\\n\"\n                                     % (dd.get_sample_name(data)))\n                    header = in_handle.readline().split(\"\\t\")\n                    for cur in (dict(zip(header, l.split(\"\\t\"))) for l in in_handle):\n                        if cur[\"Amp_Del\"] in call_convert:\n                            svtype = call_convert[cur[\"Amp_Del\"]]\n                            info = \"SVTYPE=%s;END=%s;SVLEN=%s;FOLD_CHANGE_LOG=%s;PROBES=%s;GENE=%s\" % (\n                                svtype, cur[\"End\"], int(cur[\"End\"]) - int(cur[\"Start\"]),\n                                cur[\"Log2ratio\"], cur[\"Ab_Seg\"], cur[\"Gene\"])\n                            out_handle.write(\"\\t\".join([cur[\"Chr\"], cur[\"Start\"], \".\", \"N\", \"<%s>\" % (svtype),\n                                                        \".\", \".\", info, \"GT\", \"1/1\"]) + \"\\n\")\n    return vcfutils.sort_by_ref(out_file, data)", "response": "Convert seq2c output file into BED output."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _combine_coverages(items, work_dir, input_backs=None):\n    out_file = os.path.join(work_dir, \"sample_coverages.txt\")\n    if not utils.file_exists(out_file):\n        with file_transaction(items[0], out_file) as tx_out_file:\n            with open(tx_out_file, 'w') as out_f:\n                for data in items:\n                    cov_file = tz.get_in([\"depth\", \"bins\", \"seq2c\"], data)\n                    with open(cov_file) as cov_f:\n                        out_f.write(cov_f.read())\n                if input_backs:\n                    for input_back in input_backs:\n                        with open(input_back) as in_handle:\n                            for line in in_handle:\n                                if len(line.split()) >= 4:\n                                    out_f.write(line)\n    return out_file", "response": "Combine coverage cnns calculated for individual inputs into single file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _calculate_mapping_reads(items, work_dir, input_backs=None):\n    out_file = os.path.join(work_dir, \"mapping_reads.txt\")\n    if not utils.file_exists(out_file):\n        lines = []\n        for data in items:\n            count = 0\n            for line in subprocess.check_output([\n                \"samtools\", \"idxstats\", dd.get_align_bam(data)]).decode().split(\"\\n\"):\n                if line.strip():\n                    count += int(line.split(\"\\t\")[2])\n            lines.append(\"%s\\t%s\" % (dd.get_sample_name(data), count))\n        with file_transaction(items[0], out_file) as tx_out_file:\n            with open(tx_out_file, \"w\") as out_handle:\n                out_handle.write(\"\\n\".join(lines) + \"\\n\")\n                if input_backs:\n                    for input_back in input_backs:\n                        with open(input_back) as in_handle:\n                            for line in in_handle:\n                                if len(line.split()) == 2:\n                                    out_handle.write(line)\n    return out_file", "response": "Calculate read counts from samtools idxstats for each sample."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve genome information from a genome - references. yaml file.", "response": "def get_resources(genome, ref_file, data):\n    \"\"\"Retrieve genome information from a genome-references.yaml file.\n    \"\"\"\n    base_dir = os.path.normpath(os.path.dirname(ref_file))\n    resource_file = os.path.join(base_dir, \"%s-resources.yaml\" % genome.replace(\"-test\", \"\"))\n    if not os.path.exists(resource_file):\n        raise IOError(\"Did not find resource file for %s: %s\\n\"\n                      \"To update bcbio_nextgen.py with genome resources for standard builds, run:\\n\"\n                      \"bcbio_nextgen.py upgrade -u skip\"\n                      % (genome, resource_file))\n    with open(resource_file) as in_handle:\n        resources = yaml.safe_load(in_handle)\n\n    def resource_file_path(x):\n        if isinstance(x, six.string_types) and os.path.exists(os.path.join(base_dir, x)):\n            return os.path.normpath(os.path.join(base_dir, x))\n        return x\n    cleaned = utils.dictapply(resources, resource_file_path)\n    return ensure_annotations(cleaned, data)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_required_resources(resources):\n    required = [[\"variation\", \"cosmic\"], [\"variation\", \"clinvar\"], [\"variation\", \"dbsnp\"],\n                [\"variation\", \"lcr\"], [\"variation\", \"polyx\"],\n                [\"variation\", \"encode_blacklist\"], [\"variation\", \"gc_profile\"],\n                [\"variation\", \"germline_het_pon\"],\n                [\"variation\", \"train_hapmap\"], [\"variation\", \"train_indels\"],\n                [\"variation\", \"editing\"], [\"variation\", \"exac\"], [\"variation\", \"esp\"],\n                [\"variation\", \"gnomad_exome\"],\n                [\"variation\", \"1000g\"], [\"aliases\", \"human\"]]\n    for key in required:\n        if not tz.get_in(key, resources):\n            resources = tz.update_in(resources, key, lambda x: None)\n    return resources", "response": "Add default or empty values for required resources referenced in CWL\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npreparing any potentially missing annotations for downstream processing.", "response": "def ensure_annotations(resources, data):\n    \"\"\"Prepare any potentially missing annotations for downstream processing in a local directory.\n    \"\"\"\n    transcript_gff = tz.get_in([\"rnaseq\", \"transcripts\"], resources)\n    if transcript_gff and utils.file_exists(transcript_gff):\n        out_dir = os.path.join(tz.get_in([\"dirs\", \"work\"], data),\n                               \"inputs\", \"data\", \"annotations\")\n        resources[\"rnaseq\"][\"gene_bed\"] = gtf.gtf_to_bed(transcript_gff, out_dir)\n    return resources"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nnormalizing any file paths found in a subdirectory of configuration input.", "response": "def abs_file_paths(xs, base_dir=None, ignore_keys=None, fileonly_keys=None, cur_key=None,\n                   do_download=True):\n    \"\"\"Normalize any file paths found in a subdirectory of configuration input.\n\n    base_dir -- directory to normalize relative paths to\n    ignore_keys -- algorithm key names to ignore normalize for (keywords, not files/directories)\n    fileonly_keys -- algorithm key names to only expand files (not directories)\n    cur_key -- current key when calling recursively\n    \"\"\"\n    ignore_keys = set([]) if ignore_keys is None else set(ignore_keys)\n    fileonly_keys = set([]) if fileonly_keys is None else set(fileonly_keys)\n    if base_dir is None:\n        base_dir = os.getcwd()\n    orig_dir = os.getcwd()\n    os.chdir(base_dir)\n    input_dir = os.path.join(base_dir, \"inputs\")\n    if isinstance(xs, dict):\n        out = {}\n        for k, v in xs.items():\n            if k not in ignore_keys and v and isinstance(v, six.string_types):\n                if v.lower() == \"none\":\n                    out[k] = None\n                else:\n                    out[k] = abs_file_paths(v, base_dir, ignore_keys, fileonly_keys, k, do_download=do_download)\n            elif isinstance(v, (list, tuple)):\n                out[k] = [abs_file_paths(x, base_dir, ignore_keys, fileonly_keys, k, do_download=do_download)\n                          for x in v]\n            else:\n                out[k] = v\n    elif isinstance(xs, six.string_types):\n        if os.path.exists(xs) or (do_download and objectstore.is_remote(xs)):\n            dl = objectstore.download(xs, input_dir)\n            if dl and cur_key not in ignore_keys and not (cur_key in fileonly_keys and not os.path.isfile(dl)):\n                out = os.path.normpath(os.path.join(base_dir, dl))\n            else:\n                out = xs\n        else:\n            out = xs\n    else:\n        out = xs\n    os.chdir(orig_dir)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_galaxy_loc_file(name, galaxy_dt, ref_dir, galaxy_base):\n    if \"file\" in galaxy_dt and os.path.exists(os.path.join(galaxy_base, galaxy_dt[\"file\"])):\n        loc_file = os.path.join(galaxy_base, galaxy_dt[\"file\"])\n        need_remap = False\n    elif alignment.TOOLS[name].galaxy_loc_file is None:\n        loc_file = os.path.join(ref_dir, alignment.BASE_LOCATION_FILE)\n        need_remap = True\n    else:\n        loc_file = os.path.join(ref_dir, alignment.TOOLS[name].galaxy_loc_file)\n        need_remap = False\n    if not os.path.exists(loc_file):\n        loc_file = os.path.join(ref_dir, alignment.BASE_LOCATION_FILE)\n        need_remap = True\n    return loc_file, need_remap", "response": "Retrieve the Galaxy loc file for the given reference and alignment name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving reference genome file from Galaxy *. loc file.", "response": "def _get_ref_from_galaxy_loc(name, genome_build, loc_file, galaxy_dt, need_remap,\n                             galaxy_config, data):\n    \"\"\"Retrieve reference genome file from Galaxy *.loc file.\n\n    Reads from tool_data_table_conf.xml information for the index if it\n    exists, otherwise uses heuristics to find line based on most common setups.\n    \"\"\"\n    refs = [ref for dbkey, ref in _galaxy_loc_iter(loc_file, galaxy_dt, need_remap)\n            if dbkey == genome_build]\n    remap_fn = alignment.TOOLS[name].remap_index_fn\n    need_remap = remap_fn is not None\n    if len(refs) == 0:\n        raise ValueError(\"Did not find genome build %s in bcbio installation: %s\" %\n                         (genome_build, os.path.normpath(loc_file)))\n    else:\n        cur_ref = refs[-1]\n    # Find genome directory and check for packed wf tarballs\n    cur_ref_norm = os.path.normpath(utils.add_full_path(cur_ref, galaxy_config[\"tool_data_path\"]))\n    base_dir_i = cur_ref_norm.find(\"/%s/\" % genome_build)\n    base_dir = os.path.join(cur_ref_norm[:base_dir_i], genome_build)\n    for tarball in glob.glob(os.path.join(base_dir, \"*-wf.tar.gz\")):\n        cwlutils.unpack_tarballs(tarball, {\"dirs\": {\"work\": base_dir}}, use_subdir=False)\n    if need_remap:\n        assert remap_fn is not None, \"%s requires remapping function from base location file\" % name\n        cur_ref = os.path.normpath(utils.add_full_path(cur_ref, galaxy_config[\"tool_data_path\"]))\n        cur_ref = remap_fn(os.path.abspath(cur_ref))\n    return cur_ref"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving Galaxy tool - data information from defaults or galaxy config file.", "response": "def _get_galaxy_tool_info(galaxy_base):\n    \"\"\"Retrieve Galaxy tool-data information from defaults or galaxy config file.\n    \"\"\"\n    ini_file = os.path.join(galaxy_base, \"universe_wsgi.ini\")\n    info = {\"tool_data_table_config_path\": os.path.join(galaxy_base, \"tool_data_table_conf.xml\"),\n            \"tool_data_path\": os.path.join(galaxy_base, \"tool-data\")}\n    config = configparser.ConfigParser()\n    config.read(ini_file)\n    if \"app:main\" in config.sections():\n        for option in config.options(\"app:main\"):\n            if option in info:\n                info[option] = os.path.join(galaxy_base, config.get(\"app:main\", option))\n    return info"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse data table config file for details on tool location and columns.", "response": "def _get_galaxy_data_table(name, dt_config_file):\n    \"\"\"Parse data table config file for details on tool *.loc location and columns.\n    \"\"\"\n    out = {}\n    if os.path.exists(dt_config_file):\n        tdtc = ElementTree.parse(dt_config_file)\n        for t in tdtc.getiterator(\"table\"):\n            if t.attrib.get(\"name\", \"\") in [name, \"%s_indexes\" % name]:\n                out[\"column\"] = [x.strip() for x in t.find(\"columns\").text.split(\",\")]\n                out[\"file\"] = t.find(\"file\").attrib.get(\"path\", \"\")\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve the reference genome file location from the galaxy configuration.", "response": "def get_refs(genome_build, aligner, galaxy_base, data):\n    \"\"\"Retrieve the reference genome file location from galaxy configuration.\n    \"\"\"\n    out = {}\n    name_remap = {\"samtools\": \"fasta\"}\n    if genome_build:\n        galaxy_config = _get_galaxy_tool_info(galaxy_base)\n        for name in [x for x in (\"samtools\", aligner) if x]:\n            galaxy_dt = _get_galaxy_data_table(name, galaxy_config[\"tool_data_table_config_path\"])\n            loc_file, need_remap = _get_galaxy_loc_file(name, galaxy_dt, galaxy_config[\"tool_data_path\"],\n                                                        galaxy_base)\n            cur_ref = _get_ref_from_galaxy_loc(name, genome_build, loc_file, galaxy_dt, need_remap,\n                                               galaxy_config, data)\n            base = os.path.normpath(utils.add_full_path(cur_ref, galaxy_config[\"tool_data_path\"]))\n            # Expand directories unless we are an aligner like minimap2 that uses the seq directory\n            if os.path.isdir(base) and not (need_remap and os.path.basename(base) == \"seq\"):\n                indexes = sorted(glob.glob(os.path.join(base, \"*\")))\n            elif name != \"samtools\":\n                indexes = sorted(glob.glob(\"%s*\" % utils.splitext_plus(base)[0]))\n            else:\n                indexes = []\n            name = name_remap.get(name, name)\n            out[name] = {}\n            if os.path.exists(base) and os.path.isfile(base):\n                out[name][\"base\"] = base\n            if indexes:\n                out[name][\"indexes\"] = indexes\n            # For references, add compressed inputs and indexes if they exist\n            if name == \"fasta\" and \"base\" in out[name] and os.path.exists(out[name][\"base\"] + \".gz\"):\n                indexes = [out[name][\"base\"] + \".gz.fai\", out[name][\"base\"] + \".gz.gzi\",\n                           utils.splitext_plus(out[name][\"base\"])[0] + \".dict\"]\n                out[name + \"gz\"] = {\"base\": out[name][\"base\"] + \".gz\",\n                                    \"indexes\": [x for x in indexes if os.path.exists(x)]}\n        # add additional indices relative to the base\n        if tz.get_in([\"fasta\", \"base\"], out):\n            ref_dir, ref_filebase = os.path.split(out[\"fasta\"][\"base\"])\n            rtg_dir = os.path.normpath(os.path.join(ref_dir, os.path.pardir, \"rtg\",\n                                                    \"%s.sdf\" % (os.path.splitext(ref_filebase)[0])))\n            out[\"rtg\"] = {\"base\": os.path.join(rtg_dir, \"mainIndex\"),\n                          \"indexes\": [x for x in glob.glob(os.path.join(rtg_dir, \"*\"))\n                                      if not x.endswith(\"/mainIndex\")]}\n            twobit = os.path.normpath(os.path.join(ref_dir, os.path.pardir, \"ucsc\",\n                                                   \"%s.2bit\" % (os.path.splitext(ref_filebase)[0])))\n            if os.path.exists(twobit):\n                out[\"twobit\"] = twobit\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_builds(galaxy_base):\n    name = \"samtools\"\n    galaxy_config = _get_galaxy_tool_info(galaxy_base)\n    galaxy_dt = _get_galaxy_data_table(name, galaxy_config[\"tool_data_table_config_path\"])\n    loc_file, need_remap = _get_galaxy_loc_file(name, galaxy_dt, galaxy_config[\"tool_data_path\"],\n                                                galaxy_base)\n    assert not need_remap, \"Should not need to remap reference files\"\n    fnames = {}\n    for dbkey, fname in _galaxy_loc_iter(loc_file, galaxy_dt):\n        fnames[dbkey] = fname\n    out = []\n    for dbkey in sorted(fnames.keys()):\n        out.append((dbkey, fnames[dbkey]))\n    return out", "response": "Retrieve configured genome builds and reference files using Galaxy configuration files."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve common options for running VarScan.", "response": "def _get_jvm_opts(config, tmp_dir):\n    \"\"\"Retrieve common options for running VarScan.\n    Handles jvm_opts, setting user and country to English to avoid issues\n    with different locales producing non-compliant VCF.\n    \"\"\"\n    resources = config_utils.get_resources(\"varscan\", config)\n    jvm_opts = resources.get(\"jvm_opts\", [\"-Xmx750m\", \"-Xmx2g\"])\n    jvm_opts = config_utils.adjust_opts(jvm_opts,\n                                        {\"algorithm\": {\"memory_adjust\":\n                                                       {\"magnitude\": 1.1, \"direction\": \"decrease\"}}})\n    jvm_opts += [\"-Duser.language=en\", \"-Duser.country=US\"]\n    jvm_opts += broad.get_default_jvm_opts(tmp_dir)\n    return \" \".join(jvm_opts)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves additional options for VarScan from the configuration.", "response": "def _varscan_options_from_config(config):\n    \"\"\"Retrieve additional options for VarScan from the configuration.\n    \"\"\"\n    opts = [\"--min-coverage 5\", \"--p-value 0.98\", \"--strand-filter 1\"]\n    resources = config_utils.get_resources(\"varscan\", config)\n    if resources.get(\"options\"):\n        opts += [str(x) for x in resources[\"options\"]]\n    return opts"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfilters VarScan calls based on the SPV value and frequency.", "response": "def spv_freq_filter(line, tumor_index):\n    \"\"\"Filter VarScan calls based on the SPV value and frequency.\n\n    Removes calls with SPV < 0.05 and a tumor FREQ > 0.35.\n\n    False positives dominate these higher frequency, low SPV calls. They appear\n    to be primarily non-somatic/germline variants not removed by other filters.\n    \"\"\"\n    if line.startswith(\"#CHROM\"):\n        headers = [('##FILTER=<ID=SpvFreq,Description=\"High frequency (tumor FREQ > 0.35) '\n                    'and low p-value for somatic (SPV < 0.05)\">')]\n        return \"\\n\".join(headers) + \"\\n\" + line\n    elif line.startswith(\"#\"):\n        return line\n    else:\n        parts = line.split(\"\\t\")\n        sample_ft = {a: v for (a, v) in zip(parts[8].split(\":\"), parts[9 + tumor_index].split(\":\"))}\n        freq = utils.safe_to_float(sample_ft.get(\"FREQ\"))\n        spvs = [x for x in parts[7].split(\";\") if x.startswith(\"SPV=\")]\n        spv = utils.safe_to_float(spvs[0].split(\"=\")[-1] if spvs else None)\n        fname = None\n        if spv is not None and freq is not None:\n            if spv < 0.05 and freq > 0.35:\n                fname = \"SpvFreq\"\n        if fname:\n            if parts[6] in set([\".\", \"PASS\"]):\n                parts[6] = fname\n            else:\n                parts[6] += \";%s\" % fname\n        line = \"\\t\".join(parts)\n        return line"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun VarScan on a set of paired BAM files.", "response": "def _varscan_paired(align_bams, ref_file, items, target_regions, out_file):\n\n    \"\"\"Run a paired VarScan analysis, also known as \"somatic\". \"\"\"\n\n    max_read_depth = \"1000\"\n    config = items[0][\"config\"]\n    paired = get_paired_bams(align_bams, items)\n    if not paired.normal_bam:\n        affected_batch = items[0][\"metadata\"][\"batch\"]\n        message = (\"Batch {} requires both tumor and normal BAM files for\"\n                   \" VarScan cancer calling\").format(affected_batch)\n        raise ValueError(message)\n\n    if not utils.file_exists(out_file):\n        assert out_file.endswith(\".vcf.gz\"), \"Expect bgzipped output to VarScan\"\n        normal_mpileup_cl = samtools.prep_mpileup([paired.normal_bam], ref_file,\n                                                  config, max_read_depth,\n                                                  target_regions=target_regions,\n                                                  want_bcf=False)\n        tumor_mpileup_cl = samtools.prep_mpileup([paired.tumor_bam], ref_file,\n                                                 config, max_read_depth,\n                                                 target_regions=target_regions,\n                                                 want_bcf=False)\n        base, ext = utils.splitext_plus(out_file)\n        indel_file = base + \"-indel.vcf\"\n        snp_file = base + \"-snp.vcf\"\n        with file_transaction(config, indel_file, snp_file) as (tx_indel, tx_snp):\n            with tx_tmpdir(items[0]) as tmp_dir:\n                jvm_opts = _get_jvm_opts(config, tmp_dir)\n                opts = \" \".join(_varscan_options_from_config(config))\n                remove_zerocoverage = r\"{ ifne grep -v -P '\\t0\\t\\t$' || true; }\"\n                export = utils.local_path_export()\n                varscan_cmd = (\"{export} varscan {jvm_opts} somatic \"\n                               \"<({normal_mpileup_cl} | {remove_zerocoverage}) \"\n                               \"<({tumor_mpileup_cl} | {remove_zerocoverage}) \"\n                               \"--output-snp {tx_snp} --output-indel {tx_indel} \"\n                               \"--output-vcf {opts} \")\n                # add minimum AF\n                min_af = float(utils.get_in(paired.tumor_config, (\"algorithm\",\n                                                                  \"min_allele_fraction\"), 10)) / 100.0\n                varscan_cmd += \"--min-var-freq {min_af} \"\n                do.run(varscan_cmd.format(**locals()), \"Varscan\", None, None)\n\n        to_combine = []\n        for fname in [snp_file, indel_file]:\n            if utils.file_exists(fname):\n                fix_file = \"%s-fix.vcf.gz\" % (utils.splitext_plus(fname)[0])\n                with file_transaction(config, fix_file) as tx_fix_file:\n                    fix_ambig_ref = vcfutils.fix_ambiguous_cl()\n                    fix_ambig_alt = vcfutils.fix_ambiguous_cl(5)\n                    py_cl = os.path.join(os.path.dirname(sys.executable), \"py\")\n                    normal_name = paired.normal_name\n                    tumor_name = paired.tumor_name\n                    cmd = (\"cat {fname} | \"\n                           \"{py_cl} -x 'bcbio.variation.varscan.fix_varscan_output(x,\"\n                            \"\"\" \"{normal_name}\", \"{tumor_name}\")' | \"\"\"\n                           \"{fix_ambig_ref} | {fix_ambig_alt} | ifne vcfuniqalleles | \"\n                           \"\"\"{py_cl} -x 'bcbio.variation.vcfutils.add_contig_to_header(x, \"{ref_file}\")' | \"\"\"\n                           \"\"\"bcftools filter -m + -s REJECT -e \"SS != '.' && SS != '2'\" 2> /dev/null | \"\"\"\n                           \"bgzip -c > {tx_fix_file}\")\n                    do.run(cmd.format(**locals()), \"Varscan paired fix\")\n                to_combine.append(fix_file)\n\n        if not to_combine:\n            out_file = write_empty_vcf(out_file, config)\n        else:\n            out_file = combine_variant_files(to_combine,\n                                             out_file, ref_file, config,\n                                             region=target_regions)\n        if os.path.getsize(out_file) == 0:\n            write_empty_vcf(out_file)\n        if out_file.endswith(\".gz\"):\n            out_file = bgzip_and_index(out_file, config)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfixing a varscan VCF line.", "response": "def fix_varscan_output(line, normal_name=\"\", tumor_name=\"\"):\n    \"\"\"Fix a varscan VCF line.\n\n    Fixes the ALT column and also fixes floating point values\n    output as strings to by Floats: FREQ, SSC.\n\n    This function was contributed by Sean Davis <sdavis2@mail.nih.gov>,\n    with minor modifications by Luca Beltrame <luca.beltrame@marionegri.it>.\n    \"\"\"\n    line = line.strip()\n\n    tofix = (\"##INFO=<ID=SSC\", \"##FORMAT=<ID=FREQ\")\n    if(line.startswith(\"##\")):\n        if line.startswith(tofix):\n            line = line.replace('Number=1,Type=String',\n                                'Number=1,Type=Float')\n        return line\n    line = line.split(\"\\t\")\n\n    if line[0].startswith(\"#CHROM\"):\n        if tumor_name and normal_name:\n            mapping = {\"NORMAL\": normal_name, \"TUMOR\": tumor_name}\n            base_header = line[:9]\n            old_samples = line[9:]\n\n            if len(old_samples) == 0:\n                return \"\\t\".join(line)\n\n            samples = [mapping[sample_name] for sample_name in old_samples]\n\n            assert len(old_samples) == len(samples)\n            return \"\\t\".join(base_header + samples)\n        else:\n            return \"\\t\".join(line)\n\n    try:\n        REF, ALT = line[3:5]\n    except ValueError:\n        return \"\\t\".join(line)\n\n    def _normalize_freq(line, sample_i):\n        \"\"\"Ensure FREQ genotype value is float as defined in header.\n        \"\"\"\n        ft_parts = line[8].split(\":\")\n        dat = line[sample_i].split(\":\")\n        # Non-conforming no-call sample, don't try to fix FREQ\n        if len(dat) != len(ft_parts):\n            return line\n        freq_i = ft_parts.index(\"FREQ\")\n        try:\n            dat[freq_i] = str(float(dat[freq_i].rstrip(\"%\")) / 100)\n        except ValueError:  # illegal binary characters -- set frequency to zero\n            dat[freq_i] = \"0.0\"\n        line[sample_i] = \":\".join(dat)\n        return line\n\n    if len(line) > 9:\n        line = _normalize_freq(line, 9)\n        if len(line) > 10:\n            line = _normalize_freq(line, 10)\n            # HACK: The position of the SS= changes, so we just search for it\n            ss_vals = [item for item in line[7].split(\";\") if item.startswith(\"SS=\")]\n            if len(ss_vals) > 0:\n                somatic_status = int(ss_vals[0].split(\"=\")[1])  # Get the number\n            else:\n                somatic_status = None\n            if somatic_status == 5:\n                # \"Unknown\" states are broken in current versions of VarScan\n                # so we just bail out here for now\n                return\n            # fix FREQ for any additional samples -- multi-sample VarScan calling\n            if len(line) > 11:\n                for i in range(11, len(line)):\n                    line = _normalize_freq(line, i)\n\n    #FIXME: VarScan also produces invalid REF records (e.g. CAA/A)\n    # This is not handled yet.\n\n    if \"+\" in ALT or \"-\" in ALT:\n        if \"/\" not in ALT:\n            if ALT[0] == \"+\":\n                R = REF\n                A = REF + ALT[1:]\n            elif ALT[0] == \"-\":\n                R = REF + ALT[1:]\n                A = REF\n        else:\n            Ins = [p[1:] for p in ALT.split(\"/\") if p[0] == \"+\"]\n            Del = [p[1:] for p in ALT.split(\"/\") if p[0] == \"-\"]\n\n            if len(Del):\n                REF += sorted(Del, key=lambda x: len(x))[-1]\n\n            A = \",\".join([REF[::-1].replace(p[::-1], \"\", 1)[::-1]\n                          for p in Del] + [REF + p for p in Ins])\n            R = REF\n\n        REF = R\n        ALT = A\n    else:\n        ALT = ALT.replace('/', ',')\n\n    line[3] = REF\n    line[4] = ALT\n    return \"\\t\".join(line)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate sample list from input BAMs and create input sample list.", "response": "def _create_sample_list(in_bams, vcf_file):\n    \"\"\"Pull sample names from input BAMs and create input sample list.\n    \"\"\"\n    out_file = \"%s-sample_list.txt\" % os.path.splitext(vcf_file)[0]\n    with open(out_file, \"w\") as out_handle:\n        for in_bam in in_bams:\n            with pysam.Samfile(in_bam, \"rb\") as work_bam:\n                for rg in work_bam.header.get(\"RG\", []):\n                    out_handle.write(\"%s\\n\" % rg[\"SM\"])\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _varscan_work(align_bams, ref_file, items, target_regions, out_file):\n    config = items[0][\"config\"]\n\n    orig_out_file = out_file\n    out_file = orig_out_file.replace(\".vcf.gz\", \".vcf\")\n\n    max_read_depth = \"1000\"\n    sample_list = _create_sample_list(align_bams, out_file)\n    mpileup = samtools.prep_mpileup(align_bams, ref_file, config, max_read_depth,\n                                    target_regions=target_regions, want_bcf=False)\n    # VarScan fails to generate a header on files that start with\n    # zerocoverage calls; strip these with grep, we're not going to\n    # call on them\n    remove_zerocoverage = r\"{ ifne grep -v -P '\\t0\\t\\t$' || true; }\"\n    # we use ifne from moreutils to ensure we process only on files with input, skipping otherwise\n    # http://manpages.ubuntu.com/manpages/natty/man1/ifne.1.html\n    with tx_tmpdir(items[0]) as tmp_dir:\n        jvm_opts = _get_jvm_opts(config, tmp_dir)\n        opts = \" \".join(_varscan_options_from_config(config))\n        min_af = float(utils.get_in(config, (\"algorithm\", \"min_allele_fraction\"), 10)) / 100.0\n        fix_ambig_ref = vcfutils.fix_ambiguous_cl()\n        fix_ambig_alt = vcfutils.fix_ambiguous_cl(5)\n        py_cl = os.path.join(os.path.dirname(sys.executable), \"py\")\n        export = utils.local_path_export()\n        cmd = (\"{export} {mpileup} | {remove_zerocoverage} | \"\n               \"ifne varscan {jvm_opts} mpileup2cns {opts} \"\n               \"--vcf-sample-list {sample_list} --min-var-freq {min_af} --output-vcf --variants | \"\n               \"\"\"{py_cl} -x 'bcbio.variation.vcfutils.add_contig_to_header(x, \"{ref_file}\")' | \"\"\"\n               \"{py_cl} -x 'bcbio.variation.varscan.fix_varscan_output(x)' | \"\n               \"{fix_ambig_ref} | {fix_ambig_alt} | ifne vcfuniqalleles > {out_file}\")\n        do.run(cmd.format(**locals()), \"Varscan\", None,\n                [do.file_exists(out_file)])\n    os.remove(sample_list)\n    # VarScan can create completely empty files in regions without\n    # variants, so we create a correctly formatted empty file\n    if os.path.getsize(out_file) == 0:\n        write_empty_vcf(out_file)\n\n    if orig_out_file.endswith(\".gz\"):\n        vcfutils.bgzip_and_index(out_file, config)", "response": "Perform SNP and indel genotyping with VarScan."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd gene annotations to a BED file from pre - prepared RNA - seq data.", "response": "def add_genes(in_file, data, max_distance=10000, work_dir=None):\n    \"\"\"Add gene annotations to a BED file from pre-prepared RNA-seq data.\n\n    max_distance -- only keep annotations within this distance of event\n    \"\"\"\n    gene_file = regions.get_sv_bed(data, \"exons\", out_dir=os.path.dirname(in_file))\n    if gene_file and utils.file_exists(in_file):\n        out_file = \"%s-annotated.bed\" % utils.splitext_plus(in_file)[0]\n        if work_dir:\n            out_file = os.path.join(work_dir, os.path.basename(out_file))\n        if not utils.file_uptodate(out_file, in_file):\n            fai_file = ref.fasta_idx(dd.get_ref_file(data))\n            with file_transaction(data, out_file) as tx_out_file:\n                _add_genes_to_bed(in_file, gene_file, fai_file, tx_out_file, data, max_distance)\n        return out_file\n    else:\n        return in_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd genes to a BED file.", "response": "def _add_genes_to_bed(in_file, gene_file, fai_file, out_file, data, max_distance=10000):\n    \"\"\"Re-usable subcomponent that annotates BED file genes from another BED\n    \"\"\"\n    try:\n        input_rec = next(iter(pybedtools.BedTool(in_file)))\n    except StopIteration:  # empty file\n        utils.copy_plus(in_file, out_file)\n        return\n    # keep everything after standard chrom/start/end, 1-based\n    extra_fields = list(range(4, len(input_rec.fields) + 1))\n    # keep the new gene annotation\n    gene_index = len(input_rec.fields) + 4\n    extra_fields.append(gene_index)\n    columns = \",\".join([str(x) for x in extra_fields])\n    max_column = max(extra_fields) + 1\n    ops = \",\".join([\"distinct\"] * len(extra_fields))\n    # swap over gene name to '.' if beyond maximum distance\n    # cut removes the last distance column which can cause issues\n    # with bedtools merge: 'ERROR: illegal character '.' found in integer conversion of string'\n    distance_filter = (r\"\"\"awk -F$'\\t' -v OFS='\\t' '{if ($NF > %s || $NF < -%s) $%s = \".\"} {print}'\"\"\" %\n                       (max_distance, max_distance, gene_index))\n    sort_cmd = bedutils.get_sort_cmd(os.path.dirname(out_file))\n    cat_cmd = \"zcat\" if in_file.endswith(\".gz\") else \"cat\"\n    # Ensure gene transcripts match reference genome\n    ready_gene_file = os.path.join(os.path.dirname(out_file), \"%s-genomeonly.bed\" %\n                                   (utils.splitext_plus(os.path.basename(gene_file))[0]))\n    ready_gene_file = bedutils.subset_to_genome(gene_file, ready_gene_file, data)\n    exports = \"export TMPDIR=%s && %s\" % (os.path.dirname(out_file), utils.local_path_export())\n    bcbio_py = sys.executable\n    gsort = config_utils.get_program(\"gsort\", data)\n    cmd = (\"{exports}{cat_cmd} {in_file} | grep -v ^track | grep -v ^browser | grep -v ^# | \"\n           \"{bcbio_py} -c 'from bcbio.variation import bedutils; bedutils.remove_bad()' | \"\n           \"{gsort} - {fai_file} | \"\n            \"bedtools closest -g {fai_file} \"\n            \"-D ref -t first -a - -b <({gsort} {ready_gene_file} {fai_file}) | \"\n            \"{distance_filter} | cut -f 1-{max_column} | \"\n            \"bedtools merge -i - -c {columns} -o {ops} -delim ',' -d -10 > {out_file}\")\n    do.run(cmd.format(**locals()), \"Annotate BED file with gene info\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsplitting comma - separated gene annotations into separate lines. Leads to duplicated records.", "response": "def gene_one_per_line(in_file, data):\n    \"\"\"Split comma-separated gene annotations (after add_genes). Leads to duplicated records.\n       Input:\n          chr1 100 200 F1,F2\n       Output:\n          chr1 100 200 F1\n          chr1 100 200 F2\n    \"\"\"\n    if in_file:\n        # Report all duplicated annotations one-per-line\n        one_per_line_file = \"%s-opl.bed\" % utils.splitext_plus(in_file)[0]\n        if not utils.file_uptodate(one_per_line_file, in_file):\n            with file_transaction(data, one_per_line_file) as tx_out_file:\n                with open(tx_out_file, 'w') as out:\n                    for r in pybedtools.BedTool(in_file):\n                        for g in r.name.split(','):\n                            out.write('\\t'.join(map(str, [r.chrom, r.start, r.end, g])) + '\\n')\n        return one_per_line_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a queue based on the provided arguments.", "response": "def create(parallel):\n    \"\"\"Create a queue based on the provided parallel arguments.\n\n    TODO Startup/tear-down. Currently using default queue for testing\n    \"\"\"\n    queue = {k: v for k, v in parallel.items() if k in [\"queue\", \"cores_per_job\", \"mem\"]}\n    yield queue"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a function that runs individual jobs on an existing queue.", "response": "def runner(queue, parallel):\n    \"\"\"Run individual jobs on an existing queue.\n    \"\"\"\n    def run(fn_name, items):\n        logger.info(\"clusterk: %s\" % fn_name)\n        assert \"wrapper\" in parallel, \"Clusterk requires bcbio-nextgen-vm wrapper\"\n        fn = getattr(__import__(\"{base}.clusterktasks\".format(base=parallel[\"module\"]),\n                                fromlist=[\"clusterktasks\"]),\n                     parallel[\"wrapper\"])\n        wrap_parallel = {k: v for k, v in parallel.items() if k in set([\"fresources\", \"pack\"])}\n        out = []\n        for data in [fn(fn_name, queue, parallel.get(\"wrapper_args\"), wrap_parallel, x) for x in items]:\n            if data:\n                out.extend(data)\n        return out\n    return run"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the file is only a list of genes", "response": "def is_gene_list(bed_file):\n    \"\"\"Check if the file is only a list of genes, not a BED\n    \"\"\"\n    with utils.open_gzipsafe(bed_file) as in_handle:\n        for line in in_handle:\n            if not line.startswith(\"#\"):\n                if len(line.split()) == 1:\n                    return True\n                else:\n                    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _find_gene_list_from_bed(bed_file, base_file, data):\n    # Check for a gene list, we can just return that.\n    if is_gene_list(bed_file):\n        return bed_file\n    out_file = \"%s-genes.txt\" % utils.splitext_plus(base_file)[0]\n    if not os.path.exists(out_file):\n        genes = set([])\n        import pybedtools\n        with utils.open_gzipsafe(bed_file) as in_handle:\n            for r in pybedtools.BedTool(in_handle):\n                if r.name:\n                    if not r.name.startswith(\"{\"):\n                        genes.add(r.name)\n        with file_transaction(data, out_file) as tx_out_file:\n            with open(tx_out_file, \"w\") as out_handle:\n                if len(genes) > 0:\n                    out_handle.write(\"\\n\".join(sorted(list(genes))) + \"\\n\")\n    if utils.file_exists(out_file):\n        return out_file", "response": "Retrieve list of gene names from input BED file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprovide prioritized tab delimited output for a single caller.", "response": "def _prioritize_vcf(caller, vcf_file, prioritize_by, post_prior_fn, work_dir, data):\n    \"\"\"Provide prioritized tab delimited output for a single caller.\n    \"\"\"\n    sample = dd.get_sample_name(data)\n    out_file = os.path.join(work_dir, \"%s-%s-prioritize.tsv\" % (sample, caller))\n    simple_vcf = os.path.join(work_dir, \"%s-%s-simple.vcf.gz\" % (sample, caller))\n    if not utils.file_exists(simple_vcf):\n        gene_list = _find_gene_list_from_bed(prioritize_by, out_file, data)\n        # If we have a standard gene list we can skip BED based prioritization\n        priority_vcf = \"%s.vcf.gz\" % utils.splitext_plus(out_file)[0]\n        if gene_list:\n            if vcf_file.endswith(\".vcf.gz\"):\n                utils.symlink_plus(vcf_file, priority_vcf)\n            else:\n                assert vcf_file.endswith(\".vcf\")\n                utils.symlink_plus(vcf_file, priority_vcf.replace(\".vcf.gz\", \".vcf\"))\n                vcfutils.bgzip_and_index(priority_vcf.replace(\".vcf.gz\", \".vcf\"),\n                                         data[\"config\"], remove_orig=False)\n        # otherwise prioritize based on BED and proceed\n        else:\n            if not utils.file_exists(priority_vcf):\n                with file_transaction(data, priority_vcf) as tx_out_file:\n                    resources = config_utils.get_resources(\"bcbio_prioritize\", data[\"config\"])\n                    jvm_opts = resources.get(\"jvm_opts\", [\"-Xms1g\", \"-Xmx4g\"])\n                    jvm_opts = config_utils.adjust_opts(jvm_opts, {\"algorithm\": {\"memory_adjust\":\n                                                                                 {\"direction\": \"increase\",\n                                                                                  \"maximum\": \"30000M\",\n                                                                                  \"magnitude\": dd.get_cores(data)}}})\n                    jvm_opts = \" \".join(jvm_opts)\n                    export = utils.local_path_export()\n                    cmd = (\"{export} bcbio-prioritize {jvm_opts} known -i {vcf_file} -o {tx_out_file} \"\n                           \" -k {prioritize_by}\")\n                    do.run(cmd.format(**locals()), \"Prioritize: select in known regions of interest\")\n\n        data_dir = os.path.dirname(os.path.realpath(utils.which(\"simple_sv_annotation.py\")))\n        with file_transaction(data, simple_vcf) as tx_out_file:\n            fusion_file = os.path.join(data_dir, \"fusion_pairs.txt\")\n            opts = \"\"\n            if os.path.exists(fusion_file):\n                opts += \" --known_fusion_pairs %s\" % fusion_file\n            if not gene_list:\n                opts += \" --gene_list %s\" % os.path.join(data_dir, \"az-cancer-panel.txt\")\n            else:\n                opts += \" --gene_list %s\" % gene_list\n            cmd = \"simple_sv_annotation.py {opts} -o - {priority_vcf} | bgzip -c > {tx_out_file}\"\n            do.run(cmd.format(**locals()), \"Prioritize: simplified annotation output\")\n    simple_vcf = vcfutils.bgzip_and_index(vcfutils.sort_by_ref(simple_vcf, data), data[\"config\"])\n    if post_prior_fn:\n        simple_vcf = post_prior_fn(simple_vcf, work_dir, data)\n    if not utils.file_uptodate(out_file, simple_vcf):\n        with file_transaction(data, out_file) as tx_out_file:\n            export = utils.local_path_export(env_cmd=\"vawk\")\n            cmd = (\"{export} zcat {simple_vcf} | vawk -v SNAME={sample} -v CALLER={caller} \"\n                   \"\"\"'{{if (($7 == \"PASS\" || $7 == \".\") && (S${sample}$GT != \"0/0\")) \"\"\"\n                   \"print CALLER,SNAME,$1,$2,I$END,\"\n                   \"\"\"I$SVTYPE==\"BND\" ? I$SVTYPE\":\"$3\":\"I$MATEID : I$SVTYPE,\"\"\"\n                   \"I$LOF,I$SIMPLE_ANN,\"\n                   \"S${sample}$SR,S${sample}$PE,S${sample}$PR}}' > {tx_out_file}\")\n            do.run(cmd.format(**locals()), \"Prioritize: convert to tab delimited\")\n    return out_file, simple_vcf"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncombine multiple priority tsv files into a final sorted output.", "response": "def _combine_files(tsv_files, work_dir, data):\n    \"\"\"Combine multiple priority tsv files into a final sorted output.\n    \"\"\"\n    header = \"\\t\".join([\"caller\", \"sample\", \"chrom\", \"start\", \"end\", \"svtype\",\n                        \"lof\", \"annotation\", \"split_read_support\", \"paired_support_PE\", \"paired_support_PR\"])\n    sample = dd.get_sample_name(data)\n    out_file = os.path.join(work_dir, \"%s-prioritize.tsv\" % (sample))\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            tmpdir = os.path.dirname(tx_out_file)\n            input_files = \" \".join(tsv_files)\n            sort_cmd = bedutils.get_sort_cmd(tmpdir)\n            cmd = \"{{ echo '{header}'; cat {input_files} | {sort_cmd} -k3,3 -k4,4n; }} > {tx_out_file}\"\n            do.run(cmd.format(**locals()), \"Combine prioritized from multiple callers\")\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsummarizing non - diploid calls with copy numbers and confidence intervals.", "response": "def _cnvkit_prioritize(sample, genes, allele_file, metrics_file):\n    \"\"\"Summarize non-diploid calls with copy numbers and confidence intervals.\n    \"\"\"\n    mdf = pd.read_table(metrics_file)\n    mdf.columns = [x.lower() for x in mdf.columns]\n    if len(genes) > 0:\n        mdf = mdf[mdf[\"gene\"].str.contains(\"|\".join(genes))]\n    mdf = mdf[[\"chromosome\", \"start\", \"end\", \"gene\", \"log2\", \"ci_hi\", \"ci_lo\"]]\n    adf = pd.read_table(allele_file)\n    if len(genes) > 0:\n        adf = adf[adf[\"gene\"].str.contains(\"|\".join(genes))]\n    if \"cn1\" in adf.columns and \"cn2\" in adf.columns:\n        adf = adf[[\"chromosome\", \"start\", \"end\", \"cn\", \"cn1\", \"cn2\"]]\n    else:\n        adf = adf[[\"chromosome\", \"start\", \"end\", \"cn\"]]\n    df = pd.merge(mdf, adf, on=[\"chromosome\", \"start\", \"end\"])\n    df = df[df[\"cn\"] != 2]\n    if len(df) > 0:\n        def passes(row):\n            spread = abs(row[\"ci_hi\"] - row[\"ci_lo\"])\n            return spread < 0.25\n        df[\"passes\"] = df.apply(passes, axis=1)\n    df.insert(0, \"sample\", [sample] * len(df))\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _cnv_prioritize(data):\n    supported = {\"cnvkit\": {\"inputs\": [\"call_file\", \"segmetrics\"], \"fn\": _cnvkit_prioritize}}\n    pcall = None\n    priority_files = None\n    for call in data.get(\"sv\", []):\n        if call[\"variantcaller\"] in supported:\n            priority_files = [call.get(x) for x in supported[call[\"variantcaller\"]][\"inputs\"]]\n            priority_files = [x for x in priority_files if x is not None and utils.file_exists(x)]\n            if len(priority_files) == len(supported[call[\"variantcaller\"]][\"inputs\"]):\n                pcall = call\n                break\n    prioritize_by = tz.get_in([\"config\", \"algorithm\", \"svprioritize\"], data)\n    if pcall and prioritize_by:\n        out_file = \"%s-prioritize.tsv\" % utils.splitext_plus(priority_files[0])[0]\n        gene_list = _find_gene_list_from_bed(prioritize_by, out_file, data)\n        if gene_list:\n            with open(gene_list) as in_handle:\n                genes = [x.strip() for x in in_handle]\n            args = [dd.get_sample_name(data), genes] + priority_files\n            df = supported[pcall[\"variantcaller\"]][\"fn\"](*args)\n            with file_transaction(data, out_file) as tx_out_file:\n                df.to_csv(tx_out_file, sep=\"\\t\", index=False)\n            pcall[\"priority\"] = out_file\n    return data", "response": "Perform confidence interval based prioritization for CNVs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_default_jvm_opts(tmp_dir=None, parallel_gc=False):\n    opts = [\"-XX:+UseSerialGC\"] if not parallel_gc else []\n    if tmp_dir:\n        opts.append(\"-Djava.io.tmpdir=%s\" % tmp_dir)\n    return opts", "response": "Retrieve default JVM tuning options."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_gatk_opts(config, names, tmp_dir=None, memscale=None, include_gatk=True, parallel_gc=False):\n    if include_gatk and \"gatk4\" in dd.get_tools_off({\"config\": config}):\n        opts = [\"-U\", \"LENIENT_VCF_PROCESSING\", \"--read_filter\",\n                \"BadCigar\", \"--read_filter\", \"NotPrimaryAlignment\"]\n    else:\n        opts = []\n    jvm_opts = [\"-Xms750m\", \"-Xmx2g\"]\n    for n in names:\n        resources = config_utils.get_resources(n, config)\n        if resources and resources.get(\"jvm_opts\"):\n            jvm_opts = resources.get(\"jvm_opts\")\n            break\n    if memscale:\n        jvm_opts = config_utils.adjust_opts(jvm_opts, {\"algorithm\": {\"memory_adjust\": memscale}})\n    jvm_opts += get_default_jvm_opts(tmp_dir, parallel_gc=parallel_gc)\n    return jvm_opts + opts", "response": "Retrieve GATK memory specifications moving down a list of potential specifications."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove extra environmental information reported in java when querying for versions.", "response": "def _clean_java_out(version_str):\n    \"\"\"Remove extra environmental information reported in java when querying for versions.\n\n    Java will report information like _JAVA_OPTIONS environmental variables in the output.\n    \"\"\"\n    out = []\n    for line in version_str.decode().split(\"\\n\"):\n        if line.startswith(\"Picked up\"):\n            pass\n        if line.find(\"setlocale\") > 0:\n            pass\n        else:\n            out.append(line)\n    return \"\\n\".join(out)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve version from input jar name since there is not an easy way to get MuTect version.", "response": "def get_mutect_version(mutect_jar):\n    \"\"\"Retrieves version from input jar name since there is not an easy way to get MuTect version.\n    Check mutect jar for SomaticIndelDetector, which is an Appistry feature\n    \"\"\"\n    cl = [\"java\", \"-Xms128m\", \"-Xmx256m\"] + get_default_jvm_opts() + [\"-jar\", mutect_jar, \"-h\"]\n    with closing(subprocess.Popen(cl, stdout=subprocess.PIPE, stderr=subprocess.STDOUT).stdout) as stdout:\n        if \"SomaticIndelDetector\" in stdout.read().strip():\n            mutect_type = \"-appistry\"\n        else:\n            mutect_type = \"\"\n    version = os.path.basename(mutect_jar).lower()\n    for to_remove in [\".jar\", \"-standalone\", \"mutect\"]:\n        version = version.replace(to_remove, \"\")\n    if version.startswith((\"-\", \".\")):\n        version = version[1:]\n    if not version:\n        raise ValueError(\"Unable to determine MuTect version from jar file. \"\n                         \"Need to have version contained in jar (ie. muTect-1.1.5.jar): %s\" % mutect_jar)\n    _check_for_bad_version(version, \"MuTect\")\n    return version + mutect_type"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadjust the SPARK_USER and GATK_USER parameters if current username missing.", "response": "def fix_missing_spark_user(cl, prog, params):\n    \"\"\"Adjust /etc/passwd and GATK parameters if current username missing.\n\n    Set Spark user to avoid lookup errors on environments like Docker where\n    we run as a user id that is not present in /etc/passwd\n\n    https://stackoverflow.com/questions/45198252/apache-spark-standalone-for-anonymous-uid-without-user-name/45361221#45361221\n    https://github.com/jaceklaskowski/mastering-apache-spark-book/blob/master/spark-sparkcontext-creating-instance-internals.adoc#-utilsgetcurrentusername\n    https://blog.openshift.com/jupyter-on-openshift-part-6-running-as-an-assigned-user-id/\n    \"\"\"\n    if prog.find(\"Spark\") >= 0 or \"--spark-master\" in params:\n        user = None\n        try:\n            user = getpass.getuser()\n        except KeyError:\n            if os.access(\"/etc/passwd\", os.W_OK):\n                with open(\"/etc/passwd\", \"a\") as out_handle:\n                    out_handle.write(\"sparkanon:x:{uid}:{uid}:sparkanon:/nonexistent:/usr/sbin/nologin\\n\"\n                                        .format(uid=os.getuid()))\n                try:\n                    user = getpass.getuser()\n                except KeyError:\n                    pass\n        if user:\n            cl = \"export SPARK_USER=%s && \" % (user) + cl\n    return cl"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_picard_ref(config):\n    try:\n        picard = config_utils.get_program(\"picard\", config, default=\"notfound\")\n    except config_utils.CmdNotFound:\n        picard = \"notfound\"\n    if picard == \"notfound\" or os.path.isdir(picard):\n        picard = config_utils.get_program(\"picard\", config, \"dir\")\n    return picard", "response": "Handle retrieval of Picard for running."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef gatk_cmd(name, jvm_opts, params, config=None):\n    if name == \"gatk\":\n        if isinstance(config, dict) and \"config\" not in config:\n            data = {\"config\": config}\n        else:\n            data = config\n        if not data or \"gatk4\" not in dd.get_tools_off(data):\n            return _gatk4_cmd(jvm_opts, params, data)\n        else:\n            name = \"gatk3\"\n    gatk_cmd = utils.which(os.path.join(os.path.dirname(os.path.realpath(sys.executable)), name))\n    # if we can't find via the local executable, fallback to being in the path\n    if not gatk_cmd:\n        gatk_cmd = utils.which(name)\n    if gatk_cmd:\n        return \"%s && export PATH=%s:\\\"$PATH\\\" && %s %s %s\" % \\\n            (utils.clear_java_home(), utils.get_java_binpath(gatk_cmd), gatk_cmd,\n             \" \".join(jvm_opts), \" \".join([str(x) for x in params]))", "response": "Retrieve PATH to gatk using locally installed java."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _gatk4_cmd(jvm_opts, params, data):\n    gatk_cmd = utils.which(os.path.join(os.path.dirname(os.path.realpath(sys.executable)), \"gatk\"))\n    return \"%s && export PATH=%s:\\\"$PATH\\\" && gatk --java-options '%s' %s\" % \\\n        (utils.clear_java_home(), utils.get_java_binpath(gatk_cmd),\n         \" \".join(jvm_opts), \" \".join([str(x) for x in params]))", "response": "Retrieve unified command for GATK4 using gatk."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving pre - computed version information for expensive to retrieve versions.", "response": "def _set_default_versions(self, config):\n        \"\"\"Retrieve pre-computed version information for expensive to retrieve versions.\n        Starting up GATK takes a lot of resources so we do it once at start of analysis.\n        \"\"\"\n        out = []\n        for name in [\"gatk\", \"gatk4\", \"picard\", \"mutect\"]:\n            v = tz.get_in([\"resources\", name, \"version\"], config)\n            if not v:\n                try:\n                    v = programs.get_version(name, config=config)\n                except KeyError:\n                    v = None\n            out.append(v)\n        self._gatk_version, self._gatk4_version, self._picard_version, self._mutect_version = out"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset new resource usage for the given program.", "response": "def new_resources(self, program):\n        \"\"\"Set new resource usage for the given program.\n        This allows customization of memory usage for particular sub-programs\n        of GATK like HaplotypeCaller.\n        \"\"\"\n        resources = config_utils.get_resources(program, self._config)\n        if resources.get(\"jvm_opts\"):\n            self._jvm_opts = resources.get(\"jvm_opts\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_fn(self, name, *args, **kwds):\n        fn = None\n        to_check = [picardrun]\n        for ns in to_check:\n            try:\n                fn = getattr(ns, name)\n                break\n            except AttributeError:\n                pass\n        assert fn is not None, \"Could not find function %s in %s\" % (name, to_check)\n        return fn(self, *args, **kwds)", "response": "Run pre - built functionality that used Broad tools by name."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprepares a Picard commandline.", "response": "def cl_picard(self, command, options, memscale=None):\n        \"\"\"Prepare a Picard commandline.\n        \"\"\"\n        options = [\"%s=%s\" % (x, y) for x, y in options]\n        options.append(\"VALIDATION_STRINGENCY=SILENT\")\n        return self._get_picard_cmd(command, memscale=memscale) + options"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns a Picard command with the provided option pairs.", "response": "def run(self, command, options, pipe=False, get_stdout=False, memscale=None):\n        \"\"\"Run a Picard command with the provided option pairs.\n        \"\"\"\n        cl = self.cl_picard(command, options, memscale=memscale)\n        if pipe:\n            subprocess.Popen(cl)\n        elif get_stdout:\n            p = subprocess.Popen(cl, stdout=subprocess.PIPE)\n            stdout = p.stdout.read()\n            p.wait()\n            p.stdout.close()\n            return stdout\n        else:\n            do.run(cl, \"Picard {0}\".format(command), None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndefines parameters to run the mutect paired algorithm.", "response": "def cl_mutect(self, params, tmp_dir):\n        \"\"\"Define parameters to run the mutect paired algorithm.\n        \"\"\"\n        gatk_jar = self._get_jar(\"muTect\", [\"mutect\"])\n        # Decrease memory slightly from configuration to avoid memory allocation errors\n        jvm_opts = config_utils.adjust_opts(self._jvm_opts,\n                                            {\"algorithm\": {\"memory_adjust\":\n                                                           {\"magnitude\": 1.1, \"direction\": \"decrease\"}}})\n        return [\"java\"] + jvm_opts + get_default_jvm_opts(tmp_dir) + \\\n               [\"-jar\", gatk_jar] + [str(x) for x in params]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntop level interface to running a GATK command. ld_preload injects required libraries for Java JNI calls: https://gatkforums.broadinstitute.org/gatk/discussion/8810/something-about-create-pon-workflow", "response": "def run_gatk(self, params, tmp_dir=None, log_error=True,\n                 data=None, region=None, memscale=None, parallel_gc=False, ld_preload=False):\n        \"\"\"Top level interface to running a GATK command.\n\n        ld_preload injects required libraries for Java JNI calls:\n        https://gatkforums.broadinstitute.org/gatk/discussion/8810/something-about-create-pon-workflow\n        \"\"\"\n        needs_java7 = LooseVersion(self.get_gatk_version()) < LooseVersion(\"3.6\")\n        # For old Java requirements use global java 7\n        if needs_java7:\n            setpath.remove_bcbiopath()\n        with tx_tmpdir(self._config) as local_tmp_dir:\n            if tmp_dir is None:\n                tmp_dir = local_tmp_dir\n            cl = self.cl_gatk(params, tmp_dir, memscale=memscale, parallel_gc=parallel_gc)\n            atype_index = params.index(\"-T\") if params.count(\"-T\") > 0 \\\n                          else params.index(\"--analysis_type\")\n            prog = params[atype_index + 1]\n            cl = fix_missing_spark_user(cl, prog, params)\n            if ld_preload:\n                cl = \"export LD_PRELOAD=%s/lib/libopenblas.so && %s\" % (os.path.dirname(utils.get_bcbio_bin()), cl)\n            do.run(cl, \"GATK: {0}\".format(prog), data, region=region,\n                   log_error=log_error)\n        if needs_java7:\n            setpath.prepend_bcbiopath()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_gatk_version(self):\n        if self._gatk_version is None:\n            self._set_default_versions(self._config)\n\n        if \"gatk4\" not in dd.get_tools_off({\"config\": self._config}):\n            # In cases whwere we don't have manifest versions. Not possible to get\n            # version from commandline with GATK4 alpha version\n            if self._gatk4_version is None:\n                self._gatk4_version = \"4.0\"\n            return self._gatk4_version\n        elif self._gatk_version is not None:\n            return self._gatk_version\n        else:\n            if self._has_gatk_conda_wrapper():\n                gatk_jar = None\n            else:\n                gatk_jar = self._get_jar(\"GenomeAnalysisTK\", [\"GenomeAnalysisTKLite\"], allow_missing=True)\n            self._gatk_version = get_gatk_version(gatk_jar, config=self._config)\n            return self._gatk_version", "response": "Retrieve GATK version handling locally and config cached versions."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves the Mutect version.", "response": "def get_mutect_version(self):\n        \"\"\"Retrieve the Mutect version.\n        \"\"\"\n        if self._mutect_version is None:\n            mutect_jar = self._get_jar(\"muTect\", [\"mutect\"])\n            self._mutect_version = get_mutect_version(mutect_jar)\n        return self._mutect_version"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving type of GATK jar allowing support for older GATK lite.", "response": "def gatk_type(self):\n        \"\"\"Retrieve type of GATK jar, allowing support for older GATK lite.\n        Returns either `lite` (targeting GATK-lite 2.3.9) or `restricted`,\n        the latest 2.4+ restricted version of GATK.\n        \"\"\"\n        if LooseVersion(self.gatk_major_version()) > LooseVersion(\"3.9\"):\n            return \"gatk4\"\n        elif LooseVersion(self.gatk_major_version()) > LooseVersion(\"2.3\"):\n            return \"restricted\"\n        else:\n            return \"lite\""}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving the GATK major version of the current GATK instance.", "response": "def gatk_major_version(self):\n        \"\"\"Retrieve the GATK major version, handling multiple GATK distributions.\n\n        Has special cases for GATK nightly builds, Appistry releases and\n        GATK prior to 2.3.\n        \"\"\"\n        full_version = self.get_gatk_version()\n        # Working with a recent version if using nightlies\n        if full_version.startswith(\"nightly-\"):\n            return \"3.6\"\n        parts = full_version.split(\"-\")\n        if len(parts) == 4:\n            appistry_release, version, subversion, githash = parts\n        elif len(parts) == 3:\n            version, subversion, githash = parts\n        elif len(parts) == 2:\n            version, subversion = parts\n        elif len(parts) == 1:\n            version = parts[0]\n        # version was not properly implemented in earlier GATKs\n        else:\n            version = \"2.3\"\n        if version.startswith(\"v\"):\n            version = version[1:]\n        return version"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the base Picard command handling both shell scripts and directory of jars.", "response": "def _get_picard_cmd(self, command, memscale=None):\n        \"\"\"Retrieve the base Picard command, handling both shell scripts and directory of jars.\n        \"\"\"\n        resources = config_utils.get_resources(\"picard\", self._config)\n        if memscale:\n            jvm_opts = get_picard_opts(self._config, memscale=memscale)\n        elif resources.get(\"jvm_opts\"):\n            jvm_opts = resources.get(\"jvm_opts\")\n        else:\n            jvm_opts = self._jvm_opts\n        if os.path.isdir(self._picard_ref):\n            dist_file = self._get_jar(command)\n            return [\"java\"] + jvm_opts + get_default_jvm_opts() + [\"-jar\", dist_file]\n        else:\n            # XXX Cannot currently set JVM opts with picard-tools script\n            return [self._picard_ref, command]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the jar for running the specified command.", "response": "def _get_jar(self, command, alts=None, allow_missing=False):\n        \"\"\"Retrieve the jar for running the specified command.\n        \"\"\"\n        dirs = []\n        for bdir in [self._gatk_dir, self._picard_ref]:\n            dirs.extend([bdir,\n                         os.path.join(bdir, os.pardir, \"gatk\")])\n        if alts is None: alts = []\n        for check_cmd in [command] + alts:\n            for dir_check in dirs:\n                try:\n                    check_file = config_utils.get_jar(check_cmd, dir_check)\n                    return check_file\n                except ValueError as msg:\n                    if str(msg).find(\"multiple\") > 0:\n                        raise\n                    else:\n                        pass\n        if allow_missing:\n            return None\n        else:\n            raise ValueError(\"Could not find jar %s in %s:%s\" % (command, self._picard_ref, self._gatk_dir))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cpmap(cores=1):\n    if int(cores) == 1:\n        yield itertools.imap\n    else:\n        if futures is None:\n            raise ImportError(\"concurrent.futures not available\")\n        pool = futures.ProcessPoolExecutor(cores)\n        yield pool.map\n        pool.shutdown()", "response": "Configurable parallel map context manager."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping standard function to easily pass into map processing.", "response": "def map_wrap(f):\n    \"\"\"Wrap standard function to easily pass into 'map' processing.\n    \"\"\"\n    @functools.wraps(f)\n    def wrapper(*args, **kwargs):\n        return f(*args, **kwargs)\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_single_data(input):\n    if (isinstance(input, (list, tuple)) and len(input) == 1):\n        return input[0]\n    else:\n        assert isinstance(input, dict), input\n        return input", "response": "Convert an input to a single bcbio data object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unpack_worlds(items):\n    # Unpack nested lists of samples grouped together (old IPython style)\n    if isinstance(items[0], (list, tuple)) and len(items[0]) == 1:\n        out = []\n        for d in items:\n            assert len(d) == 1 and isinstance(d[0], dict), len(d)\n            out.append(d[0])\n    # Unpack a single argument with multiple samples (CWL style)\n    elif isinstance(items, (list, tuple)) and len(items) == 1 and isinstance(items[0], (list, tuple)):\n        out = items[0]\n    else:\n        out = items\n    return out", "response": "Unpacks a list of dicts with single or multiple samples."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef safe_makedir(dname):\n    if not dname:\n        return dname\n    num_tries = 0\n    max_tries = 5\n    while not os.path.exists(dname):\n        # we could get an error here if multiple processes are creating\n        # the directory at the same time. Grr, concurrency.\n        try:\n            os.makedirs(dname)\n        except OSError:\n            if num_tries > max_tries:\n                raise\n            num_tries += 1\n            time.sleep(2)\n    return dname", "response": "Make a directory if it doesn t exist handling concurrent race conditions."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tmpfile(*args, **kwargs):\n    (fd, fname) = tempfile.mkstemp(*args, **kwargs)\n    try:\n        yield fname\n    finally:\n        os.close(fd)\n        if os.path.exists(fname):\n            os.remove(fname)", "response": "Create a tempfile and clean up file descriptors on completion."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef file_exists(fname):\n    try:\n        return fname and os.path.exists(fname) and os.path.getsize(fname) > 0\n    except OSError:\n        return False", "response": "Check if a file exists and is non - empty."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_size(path):\n    if os.path.isfile(path):\n        return os.path.getsize(path)\n    return sum(get_size(os.path.join(path, f)) for f in os.listdir(path))", "response": "Returns the size in bytes of the file or directory at path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef file_uptodate(fname, cmp_fname):\n    try:\n        return (file_exists(fname) and file_exists(cmp_fname) and\n                os.path.getmtime(fname) >= os.path.getmtime(cmp_fname))\n    except OSError:\n        return False", "response": "Check if a file exists is non - empty and is more recent than cmp_fname."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_diskspace(fname, reason, config):\n    if config[\"algorithm\"].get(\"save_diskspace\", False):\n        for ext in [\"\", \".bai\"]:\n            if os.path.exists(fname + ext):\n                with open(fname + ext, \"w\") as out_handle:\n                    out_handle.write(\"File removed to save disk space: %s\" % reason)", "response": "Overwrite a file in place with a short message to save disk."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_galaxy_amqp_config(galaxy_config, base_dir):\n    galaxy_config = add_full_path(galaxy_config, base_dir)\n    config = six.moves.configparser.ConfigParser()\n    config.read(galaxy_config)\n    amqp_config = {}\n    for option in config.options(\"galaxy_amqp\"):\n        amqp_config[option] = config.get(\"galaxy_amqp\", option)\n    return amqp_config", "response": "Read connection information on the RabbitMQ server from Galaxy config."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsplitting on file extensions allowing for zipped extensions.", "response": "def splitext_plus(f):\n    \"\"\"Split on file extensions, allowing for zipped extensions.\n    \"\"\"\n    base, ext = os.path.splitext(f)\n    if ext in [\".gz\", \".bz2\", \".zip\"]:\n        base, ext2 = os.path.splitext(base)\n        ext = ext2 + ext\n    return base, ext"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmove file from origin to target.", "response": "def move_safe(origin, target):\n    \"\"\"\n    Move file, skip if exists\n    \"\"\"\n    if origin == target:\n        return origin\n    if file_exists(target):\n        return target\n    shutil.move(origin, target)\n    return target"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a file name into the file plus required indexes.", "response": "def file_plus_index(fname):\n    \"\"\"Convert a file name into the file plus required indexes.\n    \"\"\"\n    exts = {\".vcf\": \".idx\", \".bam\": \".bai\", \".vcf.gz\": \".tbi\", \".bed.gz\": \".tbi\",\n            \".fq.gz\": \".gbi\"}\n    ext = splitext_plus(fname)[-1]\n    if ext in exts:\n        return [fname, fname + exts[ext]]\n    else:\n        return [fname]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_plus(orig):\n    for ext in [\"\", \".idx\", \".gbi\", \".tbi\", \".bai\"]:\n        if os.path.exists(orig + ext):\n            remove_safe(orig + ext)", "response": "Remove a file with the same extension and its subdirectories."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncopies a file with the same extension and its subdirectories.", "response": "def copy_plus(orig, new):\n    \"\"\"Copy a fils, including biological index files.\n    \"\"\"\n    for ext in [\"\", \".idx\", \".gbi\", \".tbi\", \".bai\"]:\n        if os.path.exists(orig + ext) and (not os.path.lexists(new + ext) or not os.path.exists(new + ext)):\n            shutil.copyfile(orig + ext, new + ext)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates relative symlinks and handle associated biological index files.", "response": "def symlink_plus(orig, new):\n    \"\"\"Create relative symlinks and handle associated biological index files.\n    \"\"\"\n    orig = os.path.abspath(orig)\n    if not os.path.exists(orig):\n        raise RuntimeError(\"File not found: %s\" % orig)\n    for ext in [\"\", \".idx\", \".gbi\", \".tbi\", \".bai\", \".fai\"]:\n        if os.path.exists(orig + ext) and (not os.path.lexists(new + ext) or not os.path.exists(new + ext)):\n            with chdir(os.path.dirname(new)):\n                remove_safe(new + ext)\n                # Work around symlink issues on some filesystems. Randomly\n                # fail to symlink.\n                try:\n                    os.symlink(os.path.relpath(orig + ext), os.path.basename(new + ext))\n                except OSError:\n                    if not os.path.exists(new + ext) or not os.path.lexists(new + ext):\n                        remove_safe(new + ext)\n                        shutil.copyfile(orig + ext, new + ext)\n    orig_noext = splitext_plus(orig)[0]\n    new_noext = splitext_plus(new)[0]\n    for sub_ext in [\".bai\", \".dict\"]:\n        if os.path.exists(orig_noext + sub_ext) and not os.path.lexists(new_noext + sub_ext):\n            with chdir(os.path.dirname(new_noext)):\n                os.symlink(os.path.relpath(orig_noext + sub_ext), os.path.basename(new_noext + sub_ext))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrenaming a filename or list of filenames with word appended to the stem", "response": "def append_stem(to_transform, word):\n    \"\"\"\n    renames a filename or list of filenames with 'word' appended to the stem\n    of each one:\n    example: append_stem(\"/path/to/test.sam\", \"_filtered\") ->\n    \"/path/to/test_filtered.sam\"\n\n    \"\"\"\n    if is_sequence(to_transform):\n        return [append_stem(f, word) for f in to_transform]\n    elif is_string(to_transform):\n        (base, ext) = splitext_plus(to_transform)\n        return \"\".join([base, word, ext])\n    else:\n        raise ValueError(\"append_stem takes a single filename as a string or \"\n                         \"a list of filenames to transform.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef replace_suffix(to_transform, suffix):\n    if is_sequence(to_transform):\n        transformed = []\n        for f in to_transform:\n            (base, _) = os.path.splitext(f)\n            transformed.append(base + suffix)\n        return transformed\n    elif is_string(to_transform):\n        (base, _) = os.path.splitext(to_transform)\n        return base + suffix\n    else:\n        raise ValueError(\"replace_suffix takes a single filename as a string or \"\n                         \"a list of filenames to transform.\")", "response": "replace_suffix takes a single filename or a list of filenames to transform and returns the new filename."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef partition_all(n, iterable):\n    it = iter(iterable)\n    while True:\n        chunk = list(itertools.islice(it, n))\n        if not chunk:\n            break\n        yield chunk", "response": "Partition a list into equally sized pieces including last smaller parts\n    http://stackoverflow. com/questions / 5129102 / python - equivalent - to - clojures - partition - all\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreplacing partition_all with a more robust version. Workaround for a segfault in pybedtools when using a BedTool as an iterator: https://github.com/daler/pybedtools/issues/88 for the discussion", "response": "def robust_partition_all(n, iterable):\n    \"\"\"\n    replaces partition_all with a more robust version.\n    Workaround for a segfault in pybedtools when using a BedTool as an iterator:\n    https://github.com/daler/pybedtools/issues/88 for the discussion\n    \"\"\"\n    it = iter(iterable)\n    while True:\n        x = []\n        for _ in range(n):\n            try:\n                x.append(next(it))\n            except StopIteration:\n                yield x\n                # Omitting this StopIteration results in a segfault!\n                raise StopIteration\n        yield x"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef partition(pred, iterable, tolist=False):\n    'Use a predicate to partition entries into false entries and true entries'\n    # partition(is_odd, range(10)) --> 0 2 4 6 8   and  1 3 5 7 9\n    t1, t2 = itertools.tee(iterable)\n    ifalse = six.moves.filterfalse(pred, t1)\n    itrue = six.moves.filter(pred, t2)\n    if tolist:\n        return list(ifalse), list(itrue)\n    else:\n        return ifalse, itrue", "response": "Use a predicate to partition entries into false entries and true entries"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef merge_config_files(fnames):\n    def _load_yaml(fname):\n        with open(fname) as in_handle:\n            config = yaml.safe_load(in_handle)\n        return config\n    out = _load_yaml(fnames[0])\n    for fname in fnames[1:]:\n        cur = _load_yaml(fname)\n        for k, v in cur.items():\n            if k in out and isinstance(out[k], dict):\n                out[k].update(v)\n            else:\n                out[k] = v\n    return out", "response": "Merge configuration files preferring definitions in latter files."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef deepish_copy(org):\n    out = dict().fromkeys(org)\n    for k, v in org.items():\n        if isinstance(v, dict):\n            out[k] = deepish_copy(v)\n        else:\n            try:\n                out[k] = v.copy()   # dicts, sets\n            except AttributeError:\n                try:\n                    out[k] = v[:]   # lists, tuples, strings, unicode\n                except TypeError:\n                    out[k] = v      # ints\n    return out", "response": "Improved speed deep copy for dictionaries of simple python types."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef replace_directory(out_files, dest_dir):\n    if is_sequence(out_files):\n        filenames = map(os.path.basename, out_files)\n        return [os.path.join(dest_dir, x) for x in filenames]\n    elif is_string(out_files):\n        return os.path.join(dest_dir, os.path.basename(out_files))\n    else:\n        raise ValueError(\"in_files must either be a sequence of filenames \"\n                         \"or a string\")", "response": "change the output directory to dest_dir\n    can take a string or a list of files"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsamples num_items from the stream keeping each with equal probability Apps", "response": "def reservoir_sample(stream, num_items, item_parser=lambda x: x):\n    \"\"\"\n    samples num_items from the stream keeping each with equal probability\n    \"\"\"\n    kept = []\n    for index, item in enumerate(stream):\n        if index < num_items:\n            kept.append(item_parser(item))\n        else:\n            r = random.randint(0, index)\n            if r < num_items:\n                kept[r] = item_parser(item)\n    return kept"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dictapply(d, fn):\n    for k, v in d.items():\n        if isinstance(v, dict):\n            v = dictapply(v, fn)\n        else:\n            d[k] = fn(v)\n    return d", "response": "apply a function to all non - dict values in a dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving path to locally installed Rscript or first in PATH.", "response": "def Rscript_cmd():\n    \"\"\"Retrieve path to locally installed Rscript or first in PATH.\n\n    Prefers Rscript version installed via conda to a system version.\n    \"\"\"\n    rscript = which(os.path.join(get_bcbio_bin(), \"Rscript\"))\n    if rscript:\n        return rscript\n    else:\n        return which(\"Rscript\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef R_package_path(package):\n    local_sitelib = R_sitelib()\n    rscript = Rscript_cmd()\n    cmd = \"\"\"{rscript} --no-environ -e '.libPaths(c(\"{local_sitelib}\")); find.package(\"{package}\")'\"\"\"\n    try:\n        output = subprocess.check_output(cmd.format(**locals()), shell=True)\n    except subprocess.CalledProcessError as e:\n        return None\n    for line in output.decode().split(\"\\n\"):\n        if \"[1]\" not in line:\n            continue\n        dirname = line.split(\"[1]\")[1].replace(\"\\\"\", \"\").strip()\n        if os.path.exists(dirname):\n            return dirname\n    return None", "response": "Returns the path to an installed R package"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef R_package_resource(package, resource):\n    package_path = R_package_path(package)\n    if not package_path:\n        return None\n    package_resource = os.path.join(package_path, resource)\n    if not file_exists(package_resource):\n        return None\n    else:\n        return package_resource", "response": "returns a path to an R package resource if it is available"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve path for java to use handling custom BCBIO_JAVA_HOME defaults to the dirname of cmd or local anaconda directory", "response": "def get_java_binpath(cmd=None):\n    \"\"\"Retrieve path for java to use, handling custom BCBIO_JAVA_HOME\n\n    Defaults to the dirname of cmd, or local anaconda directory\n    \"\"\"\n    if os.environ.get(\"BCBIO_JAVA_HOME\"):\n        test_cmd = os.path.join(os.environ[\"BCBIO_JAVA_HOME\"], \"bin\", \"java\")\n        if os.path.exists(test_cmd):\n            cmd = test_cmd\n    if not cmd:\n        cmd = Rscript_cmd()\n    return os.path.dirname(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclear JAVA_HOME environment or reset to BCBIO_JAVA_HOME command.", "response": "def clear_java_home():\n    \"\"\"Clear JAVA_HOME environment or reset to BCBIO_JAVA_HOME.\n\n    Avoids accidental java injection but respects custom BCBIO_JAVA_HOME\n    command.\n    \"\"\"\n    if os.environ.get(\"BCBIO_JAVA_HOME\"):\n        test_cmd = os.path.join(os.environ[\"BCBIO_JAVA_HOME\"], \"bin\", \"java\")\n        if os.path.exists(test_cmd):\n            return \"export JAVA_HOME=%s\" % os.environ[\"BCBIO_JAVA_HOME\"]\n    return \"unset JAVA_HOME\""}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving path to locally installed conda Perl or first in PATH.", "response": "def perl_cmd():\n    \"\"\"Retrieve path to locally installed conda Perl or first in PATH.\n    \"\"\"\n    perl = which(os.path.join(get_bcbio_bin(), \"perl\"))\n    if perl:\n        return perl\n    else:\n        return which(\"perl\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the environmental exports to use conda installed perl.", "response": "def get_perl_exports(tmpdir=None):\n    \"\"\"Environmental exports to use conda installed perl.\n    \"\"\"\n    perl_path = os.path.dirname(perl_cmd())\n    out = \"unset PERL5LIB && export PATH=%s:\\\"$PATH\\\"\" % (perl_path)\n    if tmpdir:\n        out += \" && export TMPDIR=%s\" % (tmpdir)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_all_conda_bins():\n    bcbio_bin = get_bcbio_bin()\n    conda_dir = os.path.dirname(bcbio_bin)\n    if os.path.join(\"anaconda\", \"envs\") in conda_dir:\n        conda_dir = os.path.join(conda_dir[:conda_dir.rfind(os.path.join(\"anaconda\", \"envs\"))], \"anaconda\")\n    return [bcbio_bin] + list(glob.glob(os.path.join(conda_dir, \"envs\", \"*\", \"bin\")))", "response": "Retrieve all possible conda bin directories including environments."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the full path to a python version linked to the command.", "response": "def get_program_python(cmd):\n    \"\"\"Get the full path to a python version linked to the command.\n\n    Allows finding python based programs in python 2 versus python 3\n    environments.\n    \"\"\"\n    full_cmd = os.path.realpath(which(cmd))\n    cmd_python = os.path.join(os.path.dirname(full_cmd), \"python\")\n    env_python = None\n    if \"envs\" in cmd_python:\n        parts = cmd_python.split(os.sep)\n        env_python = os.path.join(os.sep.join(parts[:parts.index(\"envs\") + 2]), \"bin\", \"python\")\n    if os.path.exists(cmd_python):\n        return cmd_python\n    elif env_python and os.path.exists(env_python):\n        return env_python\n    else:\n        return os.path.realpath(sys.executable)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving paths to local install also including environment paths if env_cmd included.", "response": "def local_path_export(at_start=True, env_cmd=None):\n    \"\"\"Retrieve paths to local install, also including environment paths if env_cmd included.\n    \"\"\"\n    paths = [get_bcbio_bin()]\n    if env_cmd:\n        env_path = os.path.dirname(get_program_python(env_cmd))\n        if env_path not in paths:\n            paths.insert(0, env_path)\n    if at_start:\n        return \"export PATH=%s:\\\"$PATH\\\" && \" % (\":\".join(paths))\n    else:\n        return \"export PATH=\\\"$PATH\\\":%s && \" % (\":\".join(paths))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef locale_export():\n    locale_to_use = \"C.UTF-8\"\n    try:\n        locales = subprocess.check_output([\"locale\", \"-a\"]).decode(errors=\"ignore\").split(\"\\n\")\n    except subprocess.CalledProcessError:\n        locales = []\n    for locale in locales:\n        if locale.lower().endswith((\"utf-8\", \"utf8\")):\n            locale_to_use = locale\n            break\n    return \"export LC_ALL=%s && export LANG=%s && \" % (locale_to_use, locale_to_use)", "response": "Exports for dealing with Click - based programs and ASCII or Unicode errors."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nact like rbind for pandas dataframes", "response": "def rbind(dfs):\n    \"\"\"\n    acts like rbind for pandas dataframes\n    \"\"\"\n    if len(dfs) == 1:\n        return dfs[0]\n    df = dfs[0]\n    for d in dfs[1:]:\n        df = df.append(d)\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the maximum length of the command line in bytes defaulting to a conservative number", "response": "def max_command_length():\n    \"\"\"\n    get the maximum length of the command line, in bytes, defaulting\n    to a conservative number if not set\n    http://www.in-ulm.de/~mascheck/various/argmax/\n    \"\"\"\n    DEFAULT_MAX_LENGTH = 150000 # lowest seen so far is 200k\n    try:\n        arg_max = os.sysconf('SC_ARG_MAX')\n        env_lines = len(os.environ) * 4\n        env_chars = sum([len(x) + len(y) for x, y in os.environ.items()])\n        arg_length = arg_max - env_lines - 2048\n    except ValueError:\n        arg_length = DEFAULT_MAX_LENGTH\n    return arg_length if arg_length > 0 else DEFAULT_MAX_LENGTH"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsorts a list of files by filename only ignoring the directory names", "response": "def sort_filenames(filenames):\n    \"\"\"\n    sort a list of files by filename only, ignoring the directory names\n    \"\"\"\n    basenames = [os.path.basename(x) for x in filenames]\n    indexes = [i[0] for i in sorted(enumerate(basenames), key=lambda x:x[1])]\n    return [filenames[x] for x in indexes]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwalking over a parsed JSON nested structure d apply func to each leaf element and replace it with result", "response": "def walk_json(d, func):\n    \"\"\" Walk over a parsed JSON nested structure `d`, apply `func` to each leaf element and replace it with result\n    \"\"\"\n    if isinstance(d, Mapping):\n        return OrderedDict((k, walk_json(v, func)) for k, v in d.items())\n    elif isinstance(d, list):\n        return [walk_json(v, func) for v in d]\n    else:\n        return func(d)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prepare_sample(data):\n    data = utils.to_single_data(data)\n    logger.debug(\"Preparing %s\" % data[\"rgnames\"][\"sample\"])\n    data[\"files\"] = get_fastq_files(data)\n    # get_fastq_files swaps over quality scores to standard, unless trimming\n    if not(dd.get_trim_reads(data)):\n        data = dd.set_quality_format(data, \"standard\")\n    return [[data]]", "response": "Prepare a sample to be run from BAM to\n    FASTQ and downsampling the number of reads for a test run"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef trim_sample(data):\n    data = utils.to_single_data(data)\n    trim_reads = dd.get_trim_reads(data)\n    # this block is to maintain legacy configuration files\n    if not trim_reads:\n        logger.info(\"Skipping trimming of %s.\" % dd.get_sample_name(data))\n    else:\n        if \"skewer\" in dd.get_tools_on(data) or trim_reads == \"skewer\":\n            trim_adapters = skewer.trim_adapters\n        else:\n            trim_adapters = trim.trim_adapters\n        out_files = trim_adapters(data)\n        data[\"files\"] = out_files\n    return [[data]]", "response": "Trim from a sample with the provided trimming method."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprovide symlinks of BAM file and existing indexes if needed.", "response": "def _link_bam_file(in_file, new_dir, data):\n    \"\"\"Provide symlinks of BAM file and existing indexes if needed.\n    \"\"\"\n    new_dir = utils.safe_makedir(new_dir)\n    out_file = os.path.join(new_dir, os.path.basename(in_file))\n    if not utils.file_exists(out_file):\n        out_file = os.path.join(new_dir, \"%s-prealign.bam\" % dd.get_sample_name(data))\n    if data.get(\"cwl_keys\"):\n        # Has indexes, we're okay to go with the original file\n        if utils.file_exists(in_file + \".bai\"):\n            out_file = in_file\n        else:\n            utils.copy_plus(in_file, out_file)\n    else:\n        utils.symlink_plus(in_file, out_file)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_supplemental_bams(data):\n    file_key = \"work_bam\"\n    if data.get(file_key):\n        for supext in [\"disc\", \"sr\"]:\n            base, ext = os.path.splitext(data[file_key])\n            test_file = \"%s-%s%s\" % (base, supext, ext)\n            if os.path.exists(test_file):\n                sup_key = file_key + \"_plus\"\n                if sup_key not in data:\n                    data[sup_key] = {}\n                data[sup_key][supext] = test_file\n    return data", "response": "Add supplemental files produced by alignment to the data dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds extracted fastq files of HLA alleles for typing.", "response": "def _add_hla_files(data):\n    \"\"\"Add extracted fastq files of HLA alleles for typing.\n    \"\"\"\n    if \"hla\" not in data:\n        data[\"hla\"] = {}\n    align_file = dd.get_align_bam(data)\n    hla_dir = os.path.join(os.path.dirname(align_file), \"hla\")\n    if not os.path.exists(hla_dir):\n        hla_files = None\n    else:\n        hla_files = sorted(list(glob.glob(os.path.join(hla_dir, \"%s.*.fq\" % os.path.basename(align_file)))))\n    data[\"hla\"][\"fastq\"] = hla_files\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprocess alignment of fastq files and return a sorted BAM file.", "response": "def process_alignment(data, alt_input=None):\n    \"\"\"Do an alignment of fastq files, preparing a sorted BAM output file.\n    \"\"\"\n    data = cwlutils.normalize_missing(utils.to_single_data(data))\n    data = cwlutils.unpack_tarballs(data, data)\n    fastq1, fastq2 = dd.get_input_sequence_files(data)\n    if alt_input:\n        fastq1, fastq2 = alt_input\n    config = data[\"config\"]\n    aligner = config[\"algorithm\"].get(\"aligner\", None)\n    if fastq1 and objectstore.file_exists_or_remote(fastq1) and aligner:\n        logger.info(\"Aligning lane %s with %s aligner\" % (data[\"rgnames\"][\"lane\"], aligner))\n        data = align_to_sort_bam(fastq1, fastq2, aligner, data)\n        if dd.get_umi_consensus(data):\n            data[\"umi_bam\"] = dd.get_work_bam(data)\n            if fastq2:\n                f1, f2, avg_cov = postalign.umi_consensus(data)\n                data[\"config\"][\"algorithm\"][\"rawumi_avg_cov\"] = avg_cov\n                del data[\"config\"][\"algorithm\"][\"umi_type\"]\n                data[\"config\"][\"algorithm\"][\"mark_duplicates\"] = False\n                data = align_to_sort_bam(f1, f2, aligner, data)\n            else:\n                raise ValueError(\"Single fastq input for UMI processing; fgbio needs paired reads: %s\" %\n                                 dd.get_sample_name(data))\n        data = _add_supplemental_bams(data)\n    elif fastq1 and objectstore.file_exists_or_remote(fastq1) and fastq1.endswith(\".bam\"):\n        sort_method = config[\"algorithm\"].get(\"bam_sort\")\n        bamclean = config[\"algorithm\"].get(\"bam_clean\")\n        if bamclean is True or bamclean == \"picard\":\n            if sort_method and sort_method != \"coordinate\":\n                raise ValueError(\"Cannot specify `bam_clean: picard` with `bam_sort` other than coordinate: %s\"\n                                 % sort_method)\n            out_bam = cleanbam.picard_prep(fastq1, data[\"rgnames\"], dd.get_ref_file(data), data[\"dirs\"],\n                                           data)\n        elif bamclean == \"fixrg\":\n            out_bam = cleanbam.fixrg(fastq1, data[\"rgnames\"], dd.get_ref_file(data), data[\"dirs\"], data)\n        elif bamclean == \"remove_extracontigs\":\n            out_bam = cleanbam.remove_extracontigs(fastq1, data)\n        elif sort_method:\n            runner = broad.runner_from_path(\"picard\", config)\n            out_file = os.path.join(data[\"dirs\"][\"work\"], \"{}-sort.bam\".format(\n                os.path.splitext(os.path.basename(fastq1))[0]))\n            if not utils.file_exists(out_file):\n                work_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"bamclean\",\n                                                           dd.get_sample_name(data)))\n                out_file = os.path.join(work_dir, \"{}-sort.bam\".format(dd.get_sample_name(data)))\n            out_bam = runner.run_fn(\"picard_sort\", fastq1, sort_method, out_file)\n        else:\n            out_bam = _link_bam_file(fastq1, os.path.join(dd.get_work_dir(data), \"prealign\",\n                                                          dd.get_sample_name(data)), data)\n        bam.index(out_bam, data[\"config\"])\n        bam.check_header(out_bam, data[\"rgnames\"], dd.get_ref_file(data), data[\"config\"])\n        dedup_bam = postalign.dedup_bam(out_bam, data)\n        bam.index(dedup_bam, data[\"config\"])\n        data[\"work_bam\"] = dedup_bam\n    elif fastq1 and objectstore.file_exists_or_remote(fastq1) and fastq1.endswith(\".cram\"):\n        data[\"work_bam\"] = fastq1\n    elif fastq1 is None and not dd.get_aligner(data):\n        data[\"config\"][\"algorithm\"][\"variantcaller\"] = False\n        data[\"work_bam\"] = None\n    elif not fastq1:\n        raise ValueError(\"No 'files' specified for input sample: %s\" % dd.get_sample_name(data))\n    elif \"kraken\" in config[\"algorithm\"]:  # kraken doesn's need bam\n        pass\n    else:\n        raise ValueError(\"Could not process input file from sample configuration. \\n\" +\n                         fastq1 +\n                         \"\\nIs the path to the file correct or is empty?\\n\" +\n                         \"If it is a fastq file (not pre-aligned BAM or CRAM), \"\n                         \"is an aligner specified in the input configuration?\")\n    if data.get(\"work_bam\"):\n        # Add stable 'align_bam' target to use for retrieving raw alignment\n        data[\"align_bam\"] = data[\"work_bam\"]\n        data = _add_hla_files(data)\n    return [[data]]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhandle any global preparatory steps for samples with potentially shared data.", "response": "def prep_samples(*items):\n    \"\"\"Handle any global preparatory steps for samples with potentially shared data.\n\n    Avoids race conditions in postprocess alignment when performing prep tasks\n    on shared files between multiple similar samples.\n\n    Cleans input BED files to avoid issues with overlapping input segments.\n    \"\"\"\n    out = []\n    for data in (utils.to_single_data(x) for x in items):\n        data = cwlutils.normalize_missing(data)\n        data = cwlutils.unpack_tarballs(data, data)\n        data = clean_inputs(data)\n        out.append([data])\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncleans input files to avoid overlapping segments that cause downstream issues.", "response": "def clean_inputs(data):\n    \"\"\"Clean BED input files to avoid overlapping segments that cause downstream issues.\n\n    Per-merges inputs to avoid needing to call multiple times during later parallel steps.\n    \"\"\"\n    if not utils.get_in(data, (\"config\", \"algorithm\", \"variant_regions_orig\")):\n        data[\"config\"][\"algorithm\"][\"variant_regions_orig\"] = dd.get_variant_regions(data)\n    clean_vr = clean_file(dd.get_variant_regions(data), data, prefix=\"cleaned-\")\n    merged_vr = merge_overlaps(clean_vr, data)\n    data[\"config\"][\"algorithm\"][\"variant_regions\"] = clean_vr\n    data[\"config\"][\"algorithm\"][\"variant_regions_merged\"] = merged_vr\n\n    if dd.get_coverage(data):\n        if not utils.get_in(data, (\"config\", \"algorithm\", \"coverage_orig\")):\n            data[\"config\"][\"algorithm\"][\"coverage_orig\"] = dd.get_coverage(data)\n        clean_cov_bed = clean_file(dd.get_coverage(data), data, prefix=\"cov-\", simple=True)\n        merged_cov_bed = merge_overlaps(clean_cov_bed, data)\n        data[\"config\"][\"algorithm\"][\"coverage\"] = clean_cov_bed\n        data[\"config\"][\"algorithm\"][\"coverage_merged\"] = merged_cov_bed\n\n    if 'seq2c' in get_svcallers(data):\n        seq2c_ready_bed = prep_seq2c_bed(data)\n        if not seq2c_ready_bed:\n            logger.warning(\"Can't run Seq2C without a svregions or variant_regions BED file\")\n        else:\n            data[\"config\"][\"algorithm\"][\"seq2c_bed_ready\"] = seq2c_ready_bed\n    elif regions.get_sv_bed(data):\n        dd.set_sv_regions(data, clean_file(regions.get_sv_bed(data), data, prefix=\"svregions-\"))\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming post - processing steps required on full BAM files.", "response": "def postprocess_alignment(data):\n    \"\"\"Perform post-processing steps required on full BAM files.\n    Prepares list of callable genome regions allowing subsequent parallelization.\n    \"\"\"\n    data = cwlutils.normalize_missing(utils.to_single_data(data))\n    data = cwlutils.unpack_tarballs(data, data)\n    bam_file = data.get(\"align_bam\") or data.get(\"work_bam\")\n    ref_file = dd.get_ref_file(data)\n    if vmulti.bam_needs_processing(data) and bam_file and bam_file.endswith(\".bam\"):\n        out_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"align\",\n                                                  dd.get_sample_name(data)))\n        bam_file_ready = os.path.join(out_dir, os.path.basename(bam_file))\n        if not utils.file_exists(bam_file_ready):\n            utils.symlink_plus(bam_file, bam_file_ready)\n        bam.index(bam_file_ready, data[\"config\"])\n        covinfo = callable.sample_callable_bed(bam_file_ready, ref_file, data)\n        callable_region_bed, nblock_bed = \\\n            callable.block_regions(covinfo.raw_callable, bam_file_ready, ref_file, data)\n        data[\"regions\"] = {\"nblock\": nblock_bed,\n                           \"callable\": covinfo.raw_callable,\n                           \"sample_callable\": covinfo.callable,\n                           \"mapped_stats\": readstats.get_cache_file(data)}\n        data[\"depth\"] = covinfo.depth_files\n        data = coverage.assign_interval(data)\n        data = samtools.run_and_save(data)\n        data = recalibrate.prep_recal(data)\n        data = recalibrate.apply_recal(data)\n    elif dd.get_variant_regions(data):\n        callable_region_bed, nblock_bed = \\\n            callable.block_regions(dd.get_variant_regions(data), bam_file, ref_file, data)\n        data[\"regions\"] = {\"nblock\": nblock_bed, \"callable\": dd.get_variant_regions(data),\n                           \"sample_callable\": dd.get_variant_regions(data)}\n    return [[data]]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _merge_out_from_infiles(in_files):\n    fname = os.path.commonprefix([os.path.basename(f) for f in in_files])\n    while fname.endswith((\"-\", \"_\", \".\")):\n        fname = fname[:-1]\n    ext = os.path.splitext(in_files[0])[-1]\n    dirname = os.path.dirname(in_files[0])\n    while dirname.endswith((\"split\", \"merge\")):\n        dirname = os.path.dirname(dirname)\n    return os.path.join(dirname, \"%s%s\" % (fname, ext))", "response": "Generate output merged file name from set of input files."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delayed_bam_merge(data):\n    if data.get(\"combine\"):\n        assert len(data[\"combine\"].keys()) == 1\n        file_key = list(data[\"combine\"].keys())[0]\n        extras = []\n        for x in data[\"combine\"][file_key].get(\"extras\", []):\n            if isinstance(x, (list, tuple)):\n                extras.extend(x)\n            else:\n                extras.append(x)\n        if file_key in data:\n            extras.append(data[file_key])\n        in_files = sorted(list(set(extras)))\n        out_file = tz.get_in([\"combine\", file_key, \"out\"], data, _merge_out_from_infiles(in_files))\n        sup_exts = data.get(file_key + \"_plus\", {}).keys()\n        for ext in list(sup_exts) + [\"\"]:\n            merged_file = None\n            if os.path.exists(utils.append_stem(out_file, \"-\" + ext)):\n                cur_out_file, cur_in_files = out_file, []\n            if ext:\n                cur_in_files = list(filter(os.path.exists, (utils.append_stem(f, \"-\" + ext) for f in in_files)))\n                cur_out_file = utils.append_stem(out_file, \"-\" + ext) if len(cur_in_files) > 0 else None\n            else:\n                cur_in_files, cur_out_file = in_files, out_file\n            if cur_out_file:\n                config = copy.deepcopy(data[\"config\"])\n                if len(cur_in_files) > 0:\n                    merged_file = merge_bam_files(cur_in_files, os.path.dirname(cur_out_file), data,\n                                                  out_file=cur_out_file)\n                else:\n                    assert os.path.exists(cur_out_file)\n                    merged_file = cur_out_file\n            if merged_file:\n                if ext:\n                    data[file_key + \"_plus\"][ext] = merged_file\n                else:\n                    data[file_key] = merged_file\n        data.pop(\"region\", None)\n        data.pop(\"combine\", None)\n    return [[data]]", "response": "Perform a merge on previously prepped files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef merge_split_alignments(data):\n    data = utils.to_single_data(data)\n    data = _merge_align_bams(data)\n    data = _merge_hla_fastq_inputs(data)\n    return [[data]]", "response": "Merge split BAM inputs generated by common workflow language runs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmerging multiple alignment BAMs including split and discordant reads.", "response": "def _merge_align_bams(data):\n    \"\"\"Merge multiple alignment BAMs, including split and discordant reads.\n    \"\"\"\n    for key in ([\"work_bam\"], [\"work_bam_plus\", \"disc\"], [\"work_bam_plus\", \"sr\"], [\"umi_bam\"]):\n        in_files = tz.get_in(key, data, [])\n        if not isinstance(in_files, (list, tuple)):\n            in_files = [in_files]\n        in_files = [x for x in in_files if x and x != \"None\"]\n        if in_files:\n            ext = \"-%s\" % key[-1] if len(key) > 1 else \"\"\n            out_file = os.path.join(dd.get_work_dir(data), \"align\", dd.get_sample_name(data),\n                                    \"%s-sort%s.bam\" % (dd.get_sample_name(data), ext))\n            merged_file = merge_bam_files(in_files, utils.safe_makedir(os.path.dirname(out_file)),\n                                          data, out_file=out_file)\n            data = tz.update_in(data, key, lambda x: merged_file)\n        else:\n            data = tz.update_in(data, key, lambda x: None)\n    if \"align_bam\" in data and \"work_bam\" in data:\n        data[\"align_bam\"] = data[\"work_bam\"]\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _merge_hla_fastq_inputs(data):\n    hla_key = [\"hla\", \"fastq\"]\n    hla_sample_files = [x for x in (tz.get_in(hla_key, data) or []) if x and x != \"None\"]\n    merged_hlas = None\n    if hla_sample_files:\n        out_files = collections.defaultdict(list)\n        for hla_file in utils.flatten(hla_sample_files):\n            rehla = re.search(r\".hla.(?P<hlatype>[\\w-]+).fq\", hla_file)\n            if rehla:\n                hlatype = rehla.group(\"hlatype\")\n                out_files[hlatype].append(hla_file)\n        if len(out_files) > 0:\n            hla_outdir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"align\",\n                                                         dd.get_sample_name(data), \"hla\"))\n            merged_hlas = []\n            for hlatype, files in out_files.items():\n                out_file = os.path.join(hla_outdir, \"%s-%s.fq\" % (dd.get_sample_name(data), hlatype))\n                optitype.combine_hla_fqs([(hlatype, f) for f in files], out_file, data)\n                merged_hlas.append(out_file)\n    data = tz.update_in(data, hla_key, lambda x: merged_hlas)\n    return data", "response": "Merge HLA inputs from a split initial alignment."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prepare_bcbio_samples(sample):\n    logger.info(\"Preparing %s files %s to merge into %s.\" % (sample['name'], sample['files'], sample['out_file']))\n    if sample['fn'] == \"fq_merge\":\n        out_file = fq_merge(sample['files'], sample['out_file'], sample['config'])\n    elif sample['fn'] == \"bam_merge\":\n        out_file = bam_merge(sample['files'], sample['out_file'], sample['config'])\n    elif sample['fn'] == \"query_gsm\":\n        out_file = query_gsm(sample['files'], sample['out_file'], sample['config'])\n    elif sample['fn'] == \"query_srr\":\n        out_file = query_srr(sample['files'], sample['out_file'], sample['config'])\n    sample['out_file'] = out_file\n    return [sample]", "response": "Function that will use specific function to merge input files into a single BCBIO sample"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves calls organized by name to use for heterogeneity analysis.", "response": "def _get_calls(data, cnv_only=False):\n    \"\"\"Retrieve calls, organized by name, to use for heterogeneity analysis.\n    \"\"\"\n    cnvs_supported = set([\"cnvkit\", \"battenberg\"])\n    out = {}\n    for sv in data.get(\"sv\", []):\n        if not cnv_only or sv[\"variantcaller\"] in cnvs_supported:\n            out[sv[\"variantcaller\"]] = sv\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving set of variants for heterogeneity analysis.", "response": "def get_variants(data, include_germline=False):\n    \"\"\"Retrieve set of variant calls to use for heterogeneity analysis.\n    \"\"\"\n    data = utils.deepish_copy(data)\n    supported = [\"precalled\", \"vardict\", \"vardict-java\", \"vardict-perl\",\n                 \"freebayes\", \"octopus\", \"strelka2\"]\n    # Right now mutect2 and mutect do not provide heterozygous germline calls\n    # to be useful https://github.com/bcbio/bcbio-nextgen/issues/2464\n    # supported += [\"mutect2\", \"mutect\"]\n    if include_germline:\n        supported.insert(1, \"gatk-haplotype\")\n    out = []\n    # CWL based input\n    if isinstance(data.get(\"variants\"), dict) and \"samples\" in data[\"variants\"]:\n        cur_vs = []\n        # Unpack single sample list of files\n        if (isinstance(data[\"variants\"][\"samples\"], (list, tuple)) and\n              len(data[\"variants\"][\"samples\"]) == 1 and isinstance(data[\"variants\"][\"samples\"][0], (list, tuple))):\n            data[\"variants\"][\"samples\"] = data[\"variants\"][\"samples\"][0]\n        for fname in data[\"variants\"][\"samples\"]:\n            variantcaller = utils.splitext_plus(os.path.basename(fname))[0]\n            variantcaller = variantcaller.replace(dd.get_sample_name(data) + \"-\", \"\")\n            for batch in dd.get_batches(data):\n                variantcaller = variantcaller.replace(batch + \"-\", \"\")\n            cur_vs.append({\"vrn_file\": fname, \"variantcaller\": variantcaller})\n        data[\"variants\"] = cur_vs\n    for v in data.get(\"variants\", []):\n        if v[\"variantcaller\"] in supported and v.get(\"vrn_file\"):\n            out.append((supported.index(v[\"variantcaller\"]), v))\n    out.sort()\n    return [xs[1] for xs in out]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ready_for_het_analysis(items):\n    paired = vcfutils.get_paired_bams([dd.get_align_bam(d) for d in items], items)\n    has_het = any(dd.get_hetcaller(d) for d in items)\n    if has_het and paired:\n        return get_variants(paired.tumor_data) and _get_calls(paired.tumor_data, cnv_only=True)", "response": "Check if a sample has input information for heterogeneity analysis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef estimate(items, batch, config):\n    hetcallers = {\"theta\": theta.run,\n                  \"phylowgs\": phylowgs.run,\n                  \"bubbletree\": bubbletree.run}\n    paired = vcfutils.get_paired_bams([dd.get_align_bam(d) for d in items], items)\n    calls = _get_calls(paired.tumor_data)\n    variants = get_variants(paired.tumor_data)\n    het_info = []\n    for hetcaller in _get_hetcallers(items):\n        try:\n            hetfn = hetcallers[hetcaller]\n        except KeyError:\n            hetfn = None\n            print(\"%s not yet implemented\" % hetcaller)\n        if hetfn:\n            hetout = hetfn(variants[0], calls, paired)\n            if hetout:\n                het_info.append(hetout)\n    out = []\n    for data in items:\n        if batch == _get_batches(data)[0]:\n            if dd.get_sample_name(data) == paired.tumor_name:\n                if het_info:\n                    data[\"heterogeneity\"] = het_info\n            out.append([data])\n    return out", "response": "Estimate heterogeneity for a pair of tumor and normal samples. Run in parallel."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(items, run_parallel):\n    to_process = []\n    extras = []\n    for batch, cur_items in _group_by_batches(items).items():\n        if _ready_for_het_analysis(cur_items):\n            to_process.append((batch, cur_items))\n        else:\n            for data in cur_items:\n                extras.append([data])\n    processed = run_parallel(\"heterogeneity_estimate\", ([xs, b, xs[0][\"config\"]] for b, xs in to_process))\n    return _group_by_sample_and_batch(extras + processed)", "response": "Top level entry point for calculating heterogeneity."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nindexes input reads and prepare groups of reads to process concurrently.", "response": "def create_inputs(data):\n    \"\"\"Index input reads and prepare groups of reads to process concurrently.\n\n    Allows parallelization of alignment beyond processors available on a single\n    machine. Prepares a bgzip and grabix indexed file for retrieving sections\n    of files.\n    \"\"\"\n    from bcbio.pipeline import sample\n    data = cwlutils.normalize_missing(data)\n    aligner = tz.get_in((\"config\", \"algorithm\", \"aligner\"), data)\n    # CRAM files must be converted to bgzipped fastq, unless not aligning.\n    # Also need to prep and download remote files.\n    if not (\"files\" in data and data[\"files\"] and aligner and (_is_cram_input(data[\"files\"]) or\n                                                               objectstore.is_remote(data[\"files\"][0]))):\n        # skip indexing on samples without input files or not doing alignment\n        if (\"files\" not in data or not data[\"files\"] or data[\"files\"][0] is None or not aligner):\n            return [[data]]\n    data[\"files_orig\"] = data[\"files\"]\n    data[\"files\"] = prep_fastq_inputs(data[\"files\"], data)\n    # preparation converts illumina into sanger format\n    data[\"config\"][\"algorithm\"][\"quality_format\"] = \"standard\"\n    # Handle any necessary trimming\n    data = utils.to_single_data(sample.trim_sample(data)[0])\n    _prep_grabix_indexes(data[\"files\"], data)\n    data = _set_align_split_size(data)\n    out = []\n    if tz.get_in([\"config\", \"algorithm\", \"align_split_size\"], data):\n        splits = _find_read_splits(data[\"files\"][0], int(data[\"config\"][\"algorithm\"][\"align_split_size\"]))\n        for split in splits:\n            cur_data = copy.deepcopy(data)\n            cur_data[\"align_split\"] = split\n            out.append([cur_data])\n    else:\n        out.append([data])\n    if \"output_cwl_keys\" in data:\n        out = cwlutils.samples_to_records([utils.to_single_data(x) for x in out],\n                                          [\"files\", \"align_split\", \"config__algorithm__quality_format\"])\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting useful align_split_size for CWL run and UMI calculations.", "response": "def _set_align_split_size(data):\n    \"\"\"Set useful align_split_size, generating an estimate if it doesn't exist.\n\n    We try to split on larger inputs and avoid too many pieces, aiming for size\n    chunks of 5Gb or at most 50 maximum splits.\n\n    The size estimate used in calculations is 20 million reads for ~5Gb.\n\n    For UMI calculations we skip splitting since we're going to align and\n    re-align after consensus.\n\n    For CWL runs, we pick larger split sizes to avoid overhead of staging each chunk.\n    \"\"\"\n    if cwlutils.is_cwl_run(data):\n        target_size = 20  # Gb\n        target_size_reads = 80  # million reads\n    else:\n        target_size = 5  # Gb\n        target_size_reads = 20  # million reads\n    max_splits = 100  # Avoid too many pieces, causing merge memory problems\n    val = dd.get_align_split_size(data)\n    umi_consensus = dd.get_umi_consensus(data)\n    if val is None:\n        if not umi_consensus:\n            total_size = 0  # Gb\n            # Use original files if we might have reduced the size of our prepped files\n            input_files = data.get(\"files_orig\", []) if dd.get_save_diskspace(data) else data.get(\"files\", [])\n            for fname in input_files:\n                if os.path.exists(fname):\n                    total_size += os.path.getsize(fname) / (1024.0 * 1024.0 * 1024.0)\n            # Only set if we have files and are bigger than the target size\n            if total_size > target_size:\n                data[\"config\"][\"algorithm\"][\"align_split_size\"] = \\\n                  int(1e6 * _pick_align_split_size(total_size, target_size,\n                                                   target_size_reads, max_splits))\n    elif val:\n        assert not umi_consensus, \"Cannot set align_split_size to %s with UMI conensus specified\" % val\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _pick_align_split_size(total_size, target_size, target_size_reads, max_splits):\n    # Too many pieces, increase our target size to get max_splits pieces\n    if total_size // target_size > max_splits:\n        piece_size = total_size // max_splits\n        return int(piece_size * target_size_reads / target_size)\n    else:\n        return int(target_size_reads)", "response": "Picks the split size for the given criteria."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a commandline suitable for use as a named pipe with reads in a given region.", "response": "def split_namedpipe_cls(pair1_file, pair2_file, data):\n    \"\"\"Create a commandline suitable for use as a named pipe with reads in a given region.\n    \"\"\"\n    if \"align_split\" in data:\n        start, end = [int(x) for x in data[\"align_split\"].split(\"-\")]\n    else:\n        start, end = None, None\n    if pair1_file.endswith(\".sdf\"):\n        assert not pair2_file, pair2_file\n        return rtg.to_fastq_apipe_cl(pair1_file, start, end)\n    else:\n        out = []\n        for in_file in pair1_file, pair2_file:\n            if in_file:\n                assert _get_grabix_index(in_file), \"Need grabix index for %s\" % in_file\n                out.append(\"<(grabix grab {in_file} {start} {end})\".format(**locals()))\n            else:\n                out.append(None)\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprovide a commandline for prep of fastq inputs with seqtk.", "response": "def _seqtk_fastq_prep_cl(data, in_file=None, read_num=0):\n    \"\"\"Provide a commandline for prep of fastq inputs with seqtk.\n\n    Handles fast conversion of fastq quality scores and trimming.\n    \"\"\"\n    needs_convert = dd.get_quality_format(data).lower() == \"illumina\"\n    trim_ends = dd.get_trim_ends(data)\n    seqtk = config_utils.get_program(\"seqtk\", data[\"config\"])\n    if in_file:\n        in_file = objectstore.cl_input(in_file)\n    else:\n        in_file = \"/dev/stdin\"\n    cmd = \"\"\n    if needs_convert:\n        cmd += \"{seqtk} seq -Q64 -V {in_file}\".format(**locals())\n    if trim_ends:\n        left_trim, right_trim = trim_ends[0:2] if data.get(\"read_num\", read_num) == 0 else trim_ends[2:4]\n        if left_trim or right_trim:\n            trim_infile = \"/dev/stdin\" if needs_convert else in_file\n            pipe = \" | \" if needs_convert else \"\"\n            cmd += \"{pipe}{seqtk} trimfq -b {left_trim} -e {right_trim} {trim_infile}\".format(**locals())\n    return cmd"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a fastq file to an anonymous pipe.", "response": "def fastq_convert_pipe_cl(in_file, data):\n    \"\"\"Create an anonymous pipe converting Illumina 1.3-1.7 to Sanger.\n\n    Uses seqtk: https://github.com/lh3/seqt\n    \"\"\"\n    cmd = _seqtk_fastq_prep_cl(data, in_file)\n    if not cmd:\n        cat_cmd = \"zcat\" if in_file.endswith(\".gz\") else \"cat\"\n        cmd = cat_cmd + \" \" + in_file\n    return \"<(%s)\" % cmd"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parallel_multiplier(items):\n    multiplier = 1\n    for data in (x[0] for x in items):\n        if (tz.get_in([\"config\", \"algorithm\", \"align_split_size\"], data) is not False and\n              tz.get_in([\"algorithm\", \"align_split_size\"], data) is not False):\n            multiplier += 50\n    return multiplier", "response": "Determine if we will be parallelizing items during processing."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmanage merging split alignments into a final working BAM file.", "response": "def merge_split_alignments(samples, run_parallel):\n    \"\"\"Manage merging split alignments back into a final working BAM file.\n\n    Perform de-duplication on the final merged file.\n    \"\"\"\n    ready = []\n    file_key = \"work_bam\"\n    to_merge = collections.defaultdict(list)\n    for data in (xs[0] for xs in samples):\n        if data.get(\"combine\"):\n            out_key = tz.get_in([\"combine\", file_key, \"out\"], data)\n            if not out_key:\n                out_key = data[\"rgnames\"][\"lane\"]\n            to_merge[out_key].append(data)\n        else:\n            ready.append([data])\n    ready_merge = []\n    hla_merges = []\n    for mgroup in to_merge.values():\n        cur_data = mgroup[0]\n        del cur_data[\"align_split\"]\n        for x in mgroup[1:]:\n            cur_data[\"combine\"][file_key][\"extras\"].append(x[file_key])\n        ready_merge.append([cur_data])\n        cur_hla = None\n        for d in mgroup:\n            hla_files = tz.get_in([\"hla\", \"fastq\"], d)\n            if hla_files:\n                if not cur_hla:\n                    cur_hla = {\"rgnames\": {\"sample\": dd.get_sample_name(cur_data)},\n                               \"config\": cur_data[\"config\"], \"dirs\": cur_data[\"dirs\"],\n                               \"hla\": {\"fastq\": []}}\n                cur_hla[\"hla\"][\"fastq\"].append(hla_files)\n        if cur_hla:\n            hla_merges.append([cur_hla])\n    if not tz.get_in([\"config\", \"algorithm\", \"kraken\"], data):\n        # kraken requires fasta filenames from data['files'] as input.\n        # We don't want to remove those files if kraken qc is required.\n        _save_fastq_space(samples)\n    merged = run_parallel(\"delayed_bam_merge\", ready_merge)\n    hla_merge_raw = run_parallel(\"merge_split_alignments\", hla_merges)\n    hla_merges = {}\n    for hla_merge in [x[0] for x in hla_merge_raw]:\n        hla_merges[dd.get_sample_name(hla_merge)] = tz.get_in([\"hla\", \"fastq\"], hla_merge)\n\n    # Add stable 'align_bam' target to use for retrieving raw alignment\n    out = []\n    for data in [x[0] for x in merged + ready]:\n        if data.get(\"work_bam\"):\n            data[\"align_bam\"] = data[\"work_bam\"]\n        if dd.get_sample_name(data) in hla_merges:\n            data[\"hla\"][\"fastq\"] = hla_merges[dd.get_sample_name(data)]\n        else:\n            hla_files = glob.glob(os.path.join(dd.get_work_dir(data), \"align\",\n                                               dd.get_sample_name(data), \"hla\", \"*.fq\"))\n            if hla_files:\n                data[\"hla\"][\"fastq\"] = hla_files\n        out.append([data])\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef total_reads_from_grabix(in_file):\n    gbi_file = _get_grabix_index(in_file)\n    if gbi_file:\n        with open(gbi_file) as in_handle:\n            next(in_handle)  # throw away\n            num_lines = int(next(in_handle).strip())\n        assert num_lines % 4 == 0, \"Expected lines to be multiple of 4\"\n        return num_lines // 4\n    else:\n        return 0", "response": "Retrieve total reads in a fastq file from grabix index."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind the number of reads in a given number of splits.", "response": "def _find_read_splits(in_file, split_size):\n    \"\"\"Determine sections of fastq files to process in splits.\n\n    Assumes a 4 line order to input files (name, read, name, quality).\n    grabix is 1-based inclusive, so return coordinates in that format.\n    \"\"\"\n    num_lines = total_reads_from_grabix(in_file) * 4\n    assert num_lines and num_lines > 0, \"Did not find grabix index reads: %s %s\" % (in_file, num_lines)\n    split_lines = split_size * 4\n    chunks = []\n    last = 1\n    for chunki in range(num_lines // split_lines + min(1, num_lines % split_lines)):\n        new = last + split_lines - 1\n        chunks.append((last, min(new, num_lines)))\n        last = new + 1\n    return [\"%s-%s\" % (s, e) for s, e in chunks]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if we have gzipped fastq and need to convert or split.", "response": "def _ready_gzip_fastq(in_files, data, require_bgzip=False):\n    \"\"\"Check if we have gzipped fastq and don't need format conversion or splitting.\n\n    Avoid forcing bgzip if we don't need indexed files.\n    \"\"\"\n    all_gzipped = all([not x or x.endswith(\".gz\") for x in in_files])\n    if require_bgzip and all_gzipped:\n        all_gzipped = all([not x or not _check_gzipped_input(x, data)[0] for x in in_files])\n    needs_convert = dd.get_quality_format(data).lower() == \"illumina\"\n    needs_trim = dd.get_trim_ends(data)\n    do_splitting = dd.get_align_split_size(data) is not False\n    return (all_gzipped and not needs_convert and not do_splitting and\n            not objectstore.is_remote(in_files[0]) and not needs_trim and not get_downsample_params(data))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prep_fastq_inputs(in_files, data):\n    if len(in_files) == 1 and _is_bam_input(in_files):\n        out = _bgzip_from_bam(in_files[0], data[\"dirs\"], data)\n    elif len(in_files) == 1 and _is_cram_input(in_files):\n        out = _bgzip_from_cram(in_files[0], data[\"dirs\"], data)\n    elif len(in_files) in [1, 2] and _ready_gzip_fastq(in_files, data):\n        out = _symlink_in_files(in_files, data)\n    else:\n        if len(in_files) > 2:\n            fpairs = fastq.combine_pairs(in_files)\n            pair_types = set([len(xs) for xs in fpairs])\n            assert len(pair_types) == 1\n            fpairs.sort(key=lambda x: os.path.basename(x[0]))\n            organized = [[xs[0] for xs in fpairs]]\n            if len(fpairs[0]) > 1:\n                organized.append([xs[1] for xs in fpairs])\n            in_files = organized\n        parallel = {\"type\": \"local\", \"num_jobs\": len(in_files),\n                    \"cores_per_job\": max(1, data[\"config\"][\"algorithm\"][\"num_cores\"] // len(in_files))}\n        inputs = [{\"in_file\": x, \"read_num\": i, \"dirs\": data[\"dirs\"], \"config\": data[\"config\"],\n                   \"is_cwl\": \"cwl_keys\" in data,\n                   \"rgnames\": data[\"rgnames\"]}\n                  for i, x in enumerate(in_files) if x]\n        out = run_multicore(_bgzip_from_fastq_parallel, [[d] for d in inputs], data[\"config\"], parallel)\n    return out", "response": "Prepare bgzipped fastq inputs\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _prep_grabix_indexes(in_files, data):\n    # if we have gzipped but not bgzipped, add a fake index for CWL support\n    # Also skips bgzip indexing if we don't need alignment splitting\n    if _ready_gzip_fastq(in_files, data) and (not _ready_gzip_fastq(in_files, data, require_bgzip=True) or\n                                              dd.get_align_split_size(data) is False):\n        for in_file in in_files:\n            if not utils.file_exists(in_file + \".gbi\"):\n                with file_transaction(data, in_file + \".gbi\") as tx_gbi_file:\n                    with open(tx_gbi_file, \"w\") as out_handle:\n                        out_handle.write(\"Not grabix indexed; index added for compatibility.\\n\")\n    else:\n        items = [[{\"bgzip_file\": x, \"config\": copy.deepcopy(data[\"config\"])}] for x in in_files if x]\n        run_multicore(_grabix_index, items, data[\"config\"])\n    return data", "response": "Parallel preparation of grabix indexes for files."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _bgzip_from_cram(cram_file, dirs, data):\n    import pybedtools\n    region_file = (tz.get_in([\"config\", \"algorithm\", \"variant_regions\"], data)\n                   if tz.get_in([\"config\", \"algorithm\", \"coverage_interval\"], data)\n                     in [\"regional\", \"exome\", \"amplicon\"]\n                   else None)\n    if region_file:\n        regions = [\"%s:%s-%s\" % tuple(r[:3]) for r in pybedtools.BedTool(region_file)]\n    else:\n        regions = [None]\n    work_dir = utils.safe_makedir(os.path.join(dirs[\"work\"], \"align_prep\"))\n    out_s, out_p1, out_p2 = [os.path.join(work_dir, \"%s-%s.fq.gz\" %\n                                          (utils.splitext_plus(os.path.basename(cram_file))[0], fext))\n                             for fext in [\"s1\", \"p1\", \"p2\"]]\n    if (not utils.file_exists(out_s) and\n          (not utils.file_exists(out_p1) or not utils.file_exists(out_p2))):\n        cram.index(cram_file, data[\"config\"])\n        fastqs, part_dir = _cram_to_fastq_regions(regions, cram_file, dirs, data)\n        if len(fastqs[0]) == 1:\n            with file_transaction(data, out_s) as tx_out_file:\n                _merge_and_bgzip([xs[0] for xs in fastqs], tx_out_file, out_s)\n        else:\n            for i, out_file in enumerate([out_p1, out_p2]):\n                if not utils.file_exists(out_file):\n                    ext = \"/%s\" % (i + 1)\n                    with file_transaction(data, out_file) as tx_out_file:\n                        _merge_and_bgzip([xs[i] for xs in fastqs], tx_out_file, out_file, ext)\n        shutil.rmtree(part_dir)\n    if utils.file_exists(out_p1):\n        return [out_p1, out_p2]\n    else:\n        assert utils.file_exists(out_s)\n        return [out_s]", "response": "Create bgzipped fastq files from a CRAM file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _bgzip_from_cram_sambamba(cram_file, dirs, data):\n    raise NotImplementedError(\"sambamba doesn't yet support retrieval from CRAM by BED file\")\n    region_file = (tz.get_in([\"config\", \"algorithm\", \"variant_regions\"], data)\n                   if tz.get_in([\"config\", \"algorithm\", \"coverage_interval\"], data) in [\"regional\", \"exome\"]\n                   else None)\n    base_name = utils.splitext_plus(os.path.basename(cram_file))[0]\n    work_dir = utils.safe_makedir(os.path.join(dirs[\"work\"], \"align_prep\",\n                                               \"%s-parts\" % base_name))\n    f1, f2, o1, o2, si = [os.path.join(work_dir, \"%s.fq\" % x) for x in [\"match1\", \"match2\", \"unmatch1\", \"unmatch2\",\n                                                                        \"single\"]]\n    ref_file = dd.get_ref_file(data)\n    region = \"-L %s\" % region_file if region_file else \"\"\n    cmd = (\"sambamba view -f bam -l 0 -C {cram_file} -T {ref_file} {region} | \"\n           \"bamtofastq F={f1} F2={f2} S={si} O={o1} O2={o2}\")\n    do.run(cmd.format(**locals()), \"Convert CRAM to fastq in regions\")", "response": "Extract BED files from CRAM using sambamba."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _merge_and_bgzip(orig_files, out_file, base_file, ext=\"\"):\n    assert out_file.endswith(\".gz\")\n    full_file = out_file.replace(\".gz\", \"\")\n    run_file = \"%s-merge.bash\" % utils.splitext_plus(base_file)[0]\n\n    cmds = [\"set -e\\n\"]\n    for i, fname in enumerate(orig_files):\n        cmd = (\"\"\"zcat %s | awk '{print (NR%%4 == 1) ? \"@%s_\" ++i \"%s\" : $0}' >> %s\\n\"\"\"\n               % (fname, i, ext, full_file))\n        cmds.append(cmd)\n    cmds.append(\"bgzip -f %s\\n\" % full_file)\n\n    with open(run_file, \"w\") as out_handle:\n        out_handle.write(\"\".join(\"\".join(cmds)))\n    do.run([do.find_bash(), run_file], \"Rename, merge and bgzip CRAM fastq output\")\n    assert os.path.exists(out_file) and not _is_gzip_empty(out_file)", "response": "Merge a group of gzipped input files into a final bgzipped output."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting CRAM files to fastq potentially within sub regions.", "response": "def _cram_to_fastq_regions(regions, cram_file, dirs, data):\n    \"\"\"Convert CRAM files to fastq, potentially within sub regions.\n\n    Returns multiple fastq files that can be merged back together.\n    \"\"\"\n    base_name = utils.splitext_plus(os.path.basename(cram_file))[0]\n    work_dir = utils.safe_makedir(os.path.join(dirs[\"work\"], \"align_prep\",\n                                               \"%s-parts\" % base_name))\n    fnames = run_multicore(_cram_to_fastq_region,\n                           [(cram_file, work_dir, base_name, region, data) for region in regions],\n                           data[\"config\"])\n    # check if we have paired or single end data\n    if any(not _is_gzip_empty(p1) for p1, p2, s in fnames):\n        out = [[p1, p2] for p1, p2, s in fnames]\n    else:\n        out = [[s] for p1, p2, s in fnames]\n    return out, work_dir"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _cram_to_fastq_region(cram_file, work_dir, base_name, region, data):\n    ref_file = tz.get_in([\"reference\", \"fasta\", \"base\"], data)\n    resources = config_utils.get_resources(\"bamtofastq\", data[\"config\"])\n    cores = tz.get_in([\"config\", \"algorithm\", \"num_cores\"], data, 1)\n    max_mem = config_utils.convert_to_bytes(resources.get(\"memory\", \"1G\")) * cores\n    rext = \"-%s\" % region.replace(\":\", \"_\").replace(\"-\", \"_\") if region else \"full\"\n    out_s, out_p1, out_p2, out_o1, out_o2 = [os.path.join(work_dir, \"%s%s-%s.fq.gz\" %\n                                                          (base_name, rext, fext))\n                                             for fext in [\"s1\", \"p1\", \"p2\", \"o1\", \"o2\"]]\n    if not utils.file_exists(out_p1):\n        with file_transaction(data, out_s, out_p1, out_p2, out_o1, out_o2) as \\\n             (tx_out_s, tx_out_p1, tx_out_p2, tx_out_o1, tx_out_o2):\n            cram_file = objectstore.cl_input(cram_file)\n            sortprefix = \"%s-sort\" % utils.splitext_plus(tx_out_s)[0]\n            cmd = (\"bamtofastq filename={cram_file} inputformat=cram T={sortprefix} \"\n                   \"gz=1 collate=1 colsbs={max_mem} exclude=SECONDARY,SUPPLEMENTARY \"\n                   \"F={tx_out_p1} F2={tx_out_p2} S={tx_out_s} O={tx_out_o1} O2={tx_out_o2} \"\n                   \"reference={ref_file}\")\n            if region:\n                cmd += \" ranges='{region}'\"\n            do.run(cmd.format(**locals()), \"CRAM to fastq %s\" % region if region else \"\")\n    return [[out_p1, out_p2, out_s]]", "response": "Convert CRAM to fastq in a specified region."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates bgzipped fastq files from a BAM file.", "response": "def _bgzip_from_bam(bam_file, dirs, data, is_retry=False, output_infix=''):\n    \"\"\"Create bgzipped fastq files from an input BAM file.\n    \"\"\"\n    # tools\n    config = data[\"config\"]\n    bamtofastq = config_utils.get_program(\"bamtofastq\", config)\n    resources = config_utils.get_resources(\"bamtofastq\", config)\n    cores = config[\"algorithm\"].get(\"num_cores\", 1)\n    max_mem = config_utils.convert_to_bytes(resources.get(\"memory\", \"1G\")) * cores\n    bgzip = tools.get_bgzip_cmd(config, is_retry)\n    # files\n    work_dir = utils.safe_makedir(os.path.join(dirs[\"work\"], \"align_prep\"))\n    out_file_1 = os.path.join(work_dir, \"%s%s-1.fq.gz\" % (os.path.splitext(os.path.basename(bam_file))[0], output_infix))\n    out_file_2 = out_file_1.replace(\"-1.fq.gz\", \"-2.fq.gz\")\n    needs_retry = False\n    if is_retry or not utils.file_exists(out_file_1):\n        if not bam.is_paired(bam_file):\n            out_file_2 = None\n        with file_transaction(config, out_file_1) as tx_out_file:\n            for f in [tx_out_file, out_file_1, out_file_2]:\n                if f and os.path.exists(f):\n                    os.remove(f)\n            fq1_bgzip_cmd = \"%s -c /dev/stdin > %s\" % (bgzip, tx_out_file)\n            prep_cmd = _seqtk_fastq_prep_cl(data, read_num=0)\n            if prep_cmd:\n                fq1_bgzip_cmd = prep_cmd + \" | \" + fq1_bgzip_cmd\n            sortprefix = \"%s-sort\" % os.path.splitext(tx_out_file)[0]\n            if bam.is_paired(bam_file):\n                prep_cmd = _seqtk_fastq_prep_cl(data, read_num=1)\n                fq2_bgzip_cmd = \"%s -c /dev/stdin > %s\" % (bgzip, out_file_2)\n                if prep_cmd:\n                    fq2_bgzip_cmd = prep_cmd + \" | \" + fq2_bgzip_cmd\n                out_str = (\"F=>({fq1_bgzip_cmd}) F2=>({fq2_bgzip_cmd}) S=/dev/null O=/dev/null \"\n                           \"O2=/dev/null collate=1 colsbs={max_mem}\")\n            else:\n                out_str = \"S=>({fq1_bgzip_cmd})\"\n            bam_file = objectstore.cl_input(bam_file)\n            extra_opts = \" \".join([str(x) for x in resources.get(\"options\", [])])\n            cmd = \"{bamtofastq} filename={bam_file} T={sortprefix} {extra_opts} \" + out_str\n            try:\n                do.run(cmd.format(**locals()), \"BAM to bgzipped fastq\",\n                       checks=[do.file_reasonable_size(tx_out_file, bam_file)],\n                       log_error=False)\n            except subprocess.CalledProcessError as msg:\n                if not is_retry and \"deflate failed\" in str(msg):\n                    logger.info(\"bamtofastq deflate IO failure preparing %s. Retrying with single core.\"\n                                % (bam_file))\n                    needs_retry = True\n                else:\n                    logger.exception()\n                    raise\n    if needs_retry:\n        return _bgzip_from_bam(bam_file, dirs, data, is_retry=True)\n    else:\n        return [x for x in [out_file_1, out_file_2] if x is not None and utils.file_exists(x)]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _grabix_index(data):\n    in_file = data[\"bgzip_file\"]\n    config = data[\"config\"]\n    grabix = config_utils.get_program(\"grabix\", config)\n    gbi_file = _get_grabix_index(in_file)\n    # We always build grabix input so we can use it for counting reads and doing downsampling\n    if not gbi_file or _is_partial_index(gbi_file):\n        if gbi_file:\n            utils.remove_safe(gbi_file)\n        else:\n            gbi_file = in_file + \".gbi\"\n        with file_transaction(data, gbi_file) as tx_gbi_file:\n            tx_in_file = os.path.splitext(tx_gbi_file)[0]\n            utils.symlink_plus(in_file, tx_in_file)\n            do.run([grabix, \"index\", tx_in_file], \"Index input with grabix: %s\" % os.path.basename(in_file))\n    assert utils.file_exists(gbi_file)\n    return [gbi_file]", "response": "Create grabix index of bgzip input file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _is_partial_index(gbi_file):\n    with open(gbi_file) as in_handle:\n        for i, _ in enumerate(in_handle):\n            if i > 2:\n                return False\n    return True", "response": "Check if the file is truncated since grabix doesn t write to a transactional directory."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _bgzip_from_fastq(data):\n    in_file = data[\"in_file\"]\n    if isinstance(in_file, (list, tuple)):\n        in_file = in_file[0]\n    needs_convert = dd.get_quality_format(data).lower() == \"illumina\"\n    # special case, empty files that have been cleaned\n    if not objectstore.is_remote(in_file) and os.path.getsize(in_file) == 0:\n        needs_bgzip, needs_gunzip = False, False\n    elif in_file.endswith(\".gz\") and not objectstore.is_remote(in_file):\n        if needs_convert or dd.get_trim_ends(data):\n            needs_bgzip, needs_gunzip = True, True\n        else:\n            needs_bgzip, needs_gunzip = _check_gzipped_input(in_file, data)\n    elif in_file.endswith(\".bz2\"):\n        needs_bgzip, needs_gunzip = True, True\n    elif objectstore.is_remote(in_file) and not tz.get_in([\"config\", \"algorithm\", \"align_split_size\"], data):\n        needs_bgzip, needs_gunzip = False, False\n    else:\n        needs_bgzip, needs_gunzip = True, False\n    work_dir = utils.safe_makedir(os.path.join(data[\"dirs\"][\"work\"], \"align_prep\"))\n    if (needs_bgzip or needs_gunzip or needs_convert or dd.get_trim_ends(data) or\n          objectstore.is_remote(in_file) or\n          (isinstance(data[\"in_file\"], (tuple, list)) and len(data[\"in_file\"]) > 1)):\n        out_file = _bgzip_file(data[\"in_file\"], data[\"config\"], work_dir,\n                               needs_bgzip, needs_gunzip, needs_convert, data)\n    else:\n        out_file = os.path.join(work_dir, \"%s_%s\" % (dd.get_sample_name(data), os.path.basename(in_file)))\n        out_file = _symlink_or_copy_grabix(in_file, out_file, data)\n    return out_file", "response": "Prepare a bgzipped file from a fastq file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle bgzip of input file potentially gunzipping an existing file.", "response": "def _bgzip_file(finput, config, work_dir, needs_bgzip, needs_gunzip, needs_convert, data):\n    \"\"\"Handle bgzip of input file, potentially gunzipping an existing file.\n\n    Handles cases where finput might be multiple files and need to be concatenated.\n    \"\"\"\n    if isinstance(finput, six.string_types):\n        in_file = finput\n    else:\n        assert not needs_convert, \"Do not yet handle quality conversion with multiple inputs\"\n        return _bgzip_multiple_files(finput, work_dir, data)\n    out_file = os.path.join(work_dir, os.path.basename(in_file).replace(\".bz2\", \"\") +\n                            (\".gz\" if not in_file.endswith(\".gz\") else \"\"))\n    if not utils.file_exists(out_file):\n        with file_transaction(config, out_file) as tx_out_file:\n            bgzip = tools.get_bgzip_cmd(config)\n            is_remote = objectstore.is_remote(in_file)\n            in_file = objectstore.cl_input(in_file, unpack=needs_gunzip or needs_convert or\n                                           needs_bgzip or dd.get_trim_ends(data))\n            if needs_convert or dd.get_trim_ends(data):\n                in_file = fastq_convert_pipe_cl(in_file, data)\n            if needs_gunzip and not (needs_convert or dd.get_trim_ends(data)):\n                if in_file.endswith(\".bz2\"):\n                    gunzip_cmd = \"bunzip2 -c {in_file} |\".format(**locals())\n                else:\n                    gunzip_cmd = \"gunzip -c {in_file} |\".format(**locals())\n                bgzip_in = \"/dev/stdin\"\n            else:\n                gunzip_cmd = \"\"\n                bgzip_in = in_file\n            if needs_bgzip:\n                do.run(\"{gunzip_cmd} {bgzip} -c {bgzip_in} > {tx_out_file}\".format(**locals()),\n                       \"bgzip input file\")\n            elif is_remote:\n                bgzip = \"| bgzip -c\" if (needs_convert or dd.get_trim_ends(data)) else \"\"\n                do.run(\"cat {in_file} {bgzip} > {tx_out_file}\".format(**locals()), \"Get remote input\")\n            else:\n                raise ValueError(\"Unexpected inputs: %s %s %s %s\" % (in_file, needs_bgzip,\n                                                                     needs_gunzip, needs_convert))\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_gzipped_input(in_file, data):\n    grabix = config_utils.get_program(\"grabix\", data[\"config\"])\n    is_bgzip = subprocess.check_output([grabix, \"check\", in_file])\n    if is_bgzip.strip() == \"yes\":\n        return False, False\n    else:\n        return True, True", "response": "Determine if a gzipped input file is blocked gzip or standard."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun fastqc and return a report.", "response": "def run(bam_file, data, fastqc_out):\n    \"\"\"Run fastqc, generating report in specified directory and parsing metrics.\n\n    Downsamples to 10 million reads to avoid excessive processing times with large\n    files, unless we're running a Standard/smallRNA-seq/QC pipeline.\n\n    Handles fastqc 0.11+, which use a single HTML file and older versions that use\n    a directory of files + images. The goal is to eventually move to only 0.11+\n    \"\"\"\n    sentry_file = os.path.join(fastqc_out, \"fastqc_report.html\")\n    if not os.path.exists(sentry_file):\n        work_dir = os.path.dirname(fastqc_out)\n        utils.safe_makedir(work_dir)\n        ds_file = (bam.downsample(bam_file, data, 1e7, work_dir=work_dir)\n                   if data.get(\"analysis\", \"\").lower() not in [\"standard\", \"smallrna-seq\"]\n                   else None)\n        if ds_file is not None:\n            bam_file = ds_file\n        frmt = \"bam\" if bam_file.endswith(\"bam\") else \"fastq\"\n        fastqc_name = utils.splitext_plus(os.path.basename(bam_file))[0]\n        fastqc_clean_name = dd.get_sample_name(data)\n        num_cores = data[\"config\"][\"algorithm\"].get(\"num_cores\", 1)\n        with tx_tmpdir(data, work_dir) as tx_tmp_dir:\n            with utils.chdir(tx_tmp_dir):\n                cl = [config_utils.get_program(\"fastqc\", data[\"config\"]),\n                      \"-d\", tx_tmp_dir,\n                      \"-t\", str(num_cores), \"--extract\", \"-o\", tx_tmp_dir, \"-f\", frmt, bam_file]\n                cl = \"%s %s %s\" % (utils.java_freetype_fix(),\n                                   utils.local_path_export(), \" \".join([str(x) for x in cl]))\n                do.run(cl, \"FastQC: %s\" % dd.get_sample_name(data))\n                tx_fastqc_out = os.path.join(tx_tmp_dir, \"%s_fastqc\" % fastqc_name)\n                tx_combo_file = os.path.join(tx_tmp_dir, \"%s_fastqc.html\" % fastqc_name)\n                if not os.path.exists(sentry_file) and os.path.exists(tx_combo_file):\n                    utils.safe_makedir(fastqc_out)\n                    # Use sample name for reports instead of bam file name\n                    with open(os.path.join(tx_fastqc_out, \"fastqc_data.txt\"), 'r') as fastqc_bam_name, \\\n                            open(os.path.join(tx_fastqc_out, \"_fastqc_data.txt\"), 'w') as fastqc_sample_name:\n                        for line in fastqc_bam_name:\n                            fastqc_sample_name.write(line.replace(os.path.basename(bam_file), fastqc_clean_name))\n                    shutil.move(os.path.join(tx_fastqc_out, \"_fastqc_data.txt\"), os.path.join(fastqc_out, 'fastqc_data.txt'))\n                    shutil.move(tx_combo_file, sentry_file)\n                    if os.path.exists(\"%s.zip\" % tx_fastqc_out):\n                        shutil.move(\"%s.zip\" % tx_fastqc_out, os.path.join(fastqc_out, \"%s.zip\" % fastqc_clean_name))\n                elif not os.path.exists(sentry_file):\n                    raise ValueError(\"FastQC failed to produce output HTML file: %s\" % os.listdir(tx_tmp_dir))\n    logger.info(\"Produced HTML report %s\" % sentry_file)\n    parser = FastQCParser(fastqc_out, dd.get_sample_name(data))\n    stats = parser.get_fastqc_summary()\n    parser.save_sections_into_file()\n    return stats"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_module(self, parser, module):\n        dt = []\n        lines = parser.clean_data(module)\n        header = lines[0]\n        for data in lines[1:]:\n            if data[0].startswith(\"#\"):  # some modules have two headers\n                header = data\n                continue\n            if data[0].find(\"-\") > -1:  # expand positions 1-3 to 1, 2, 3\n                f, s = map(int, data[0].split(\"-\"))\n                for pos in range(f, s):\n                    dt.append([str(pos)] + data[1:])\n            else:\n                dt.append(data)\n        dt = pd.DataFrame(dt)\n        dt.columns = [h.replace(\" \", \"_\") for h in header]\n        dt['sample'] = self.sample\n        return dt", "response": "Get module using fadapa package"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nspawn the task. Throws RuntimeError if the task was already started.", "response": "def start(self):\n        \"\"\"Spawn the task.\n\n        Throws RuntimeError if the task was already started.\"\"\"\n        if not self.pipe is None:\n            raise RuntimeError(\"Cannot start task twice\")\n\n        self.ioloop = tornado.ioloop.IOLoop.instance()\n        if self.timeout > 0:\n            self.expiration = self.ioloop.add_timeout( time.time() + self.timeout, self.on_timeout )\n        self.pipe = subprocess.Popen(**self.args)\n\n        self.streams = [ (self.pipe.stdout.fileno(), []),\n                         (self.pipe.stderr.fileno(), []) ]\n        for fd, d in self.streams:\n            flags = fcntl.fcntl(fd, fcntl.F_GETFL)| os.O_NDELAY\n            fcntl.fcntl( fd, fcntl.F_SETFL, flags)\n            self.ioloop.add_handler( fd,\n                                     self.stat,\n                                     self.ioloop.READ|self.ioloop.ERROR)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stat( self, *args ):\n        '''Check process completion and consume pending I/O data'''\n        self.pipe.poll()\n        if not self.pipe.returncode is None:\n            '''cleanup handlers and timeouts'''\n            if not self.expiration is None:\n                self.ioloop.remove_timeout(self.expiration)\n            for fd, dest in  self.streams:\n                self.ioloop.remove_handler(fd)\n            '''schedulle callback (first try to read all pending data)'''\n            self.ioloop.add_callback(self.on_finish)\n        for fd, dest in  self.streams:\n            while True:\n                try:\n                    data = os.read(fd, 4096)\n                    if len(data) == 0:\n                        break\n                    print(data.rstrip())\n                except:\n                    break", "response": "Check process completion and consume pending I/O data and print out the status of the I/O."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_fc_date(out_config_file):\n    if os.path.exists(out_config_file):\n        with open(out_config_file) as in_handle:\n            old_config = yaml.safe_load(in_handle)\n            fc_date = old_config[\"fc_date\"]\n    else:\n        fc_date = datetime.datetime.now().strftime(\"%y%m%d\")\n    return fc_date", "response": "Retrieve flowcell date reusing older dates if refreshing a present workflow."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef draw_quality_plot(db_file, plot_file, position_select, title):\n    robjects.r.assign('db.file', db_file)\n    robjects.r.assign('plot.file', plot_file)\n    robjects.r.assign('position.select', position_select)\n    robjects.r.assign('title', title)\n    robjects.r('''\n      library(sqldf)\n      library(plyr)\n      library(ggplot2)\n      sql <- paste(\"select * from data WHERE position\", position.select, sep=\" \")\n      exp.data <- sqldf(sql, dbname=db.file)\n      remap.data <- ddply(exp.data, c(\"orig\", \"remap\"), transform, count=sum(count))\n      p <- ggplot(remap.data, aes(orig, remap)) +\n           geom_tile(aes(fill = count)) +\n           scale_fill_gradient(low = \"white\", high = \"steelblue\", trans=\"log\") +\n           opts(panel.background = theme_rect(fill = \"white\"),\n                title=title) +\n           geom_abline(intercept=0, slope=1)\n      ggsave(plot.file, p, width=6, height=6)\n    ''')", "response": "Draw a heatmap of remapped qualities at each position in the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines how to sub - divide recalibration analysis based on read length.", "response": "def _positions_to_examine(db_file):\n    \"\"\"Determine how to sub-divide recalibration analysis based on read length.\n    \"\"\"\n    conn = sqlite3.connect(db_file)\n    cursor = conn.cursor()\n    cursor.execute(\"\"\"SELECT MAX(position) FROM data\"\"\")\n    position = cursor.fetchone()[0]\n    if position is not None:\n        position = int(position)\n    cursor.close()\n    split_at = 50\n    if position is None:\n        return []\n    elif position < split_at:\n        return [(\"<= %s\" % position, \"lt%s\" % position)]\n    else:\n        return [(\"< %s\" % split_at, \"lt%s\" % split_at),\n                (\">= %s\" % split_at, \"gt%s\" % split_at)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread two CSV files of qualities organizing values by position.", "response": "def _organize_by_position(orig_file, cmp_file, chunk_size):\n    \"\"\"Read two CSV files of qualities, organizing values by position.\n    \"\"\"\n    with open(orig_file) as in_handle:\n        reader1 = csv.reader(in_handle)\n        positions = len(next(reader1)) - 1\n    for positions in _chunks(range(positions), chunk_size):\n        with open(orig_file) as orig_handle:\n            with open(cmp_file) as cmp_handle:\n                orig_reader = csv.reader(orig_handle)\n                cmp_reader = csv.reader(cmp_handle)\n                for item in _counts_at_position(positions,\n                        orig_reader, cmp_reader):\n                    yield item"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _chunks(l, n):\n    for i in xrange(0, len(l), n):\n        yield l[i:i+n]", "response": "Yield successive n - sized chunks from l."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _counts_at_position(positions, orig_reader, cmp_reader):\n    pos_counts = collections.defaultdict(lambda:\n                 collections.defaultdict(lambda:\n                 collections.defaultdict(int)))\n    for orig_parts in orig_reader:\n        cmp_parts = next(cmp_reader)\n        for pos in positions:\n            try:\n                pos_counts[pos][int(orig_parts[pos+1])][int(cmp_parts[pos+1])] += 1\n            except IndexError:\n                pass\n    for pos, count_dict in pos_counts.iteritems():\n        for orig_val, cmp_dict in count_dict.iteritems():\n            for cmp_val, count in cmp_dict.iteritems():\n                yield pos+1, orig_val, cmp_val, count", "response": "Combine orignal and new qualities at each position generating counts."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsorts a CSV file by read name allowing direct comparison.", "response": "def sort_csv(in_file):\n    \"\"\"Sort a CSV file by read name, allowing direct comparison.\n    \"\"\"\n    out_file = \"%s.sort\" % in_file\n    if not (os.path.exists(out_file) and os.path.getsize(out_file) > 0):\n        cl = [\"sort\", \"-k\", \"1,1\", in_file]\n        with open(out_file, \"w\") as out_handle:\n            child = subprocess.Popen(cl, stdout=out_handle)\n            child.wait()\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fastq_to_csv(in_file, fastq_format, work_dir):\n    out_file = \"%s.csv\" % (os.path.splitext(os.path.basename(in_file))[0])\n    out_file = os.path.join(work_dir, out_file)\n    if not (os.path.exists(out_file) and os.path.getsize(out_file) > 0):\n        with open(in_file) as in_handle:\n            with open(out_file, \"w\") as out_handle:\n                writer = csv.writer(out_handle)\n                for rec in SeqIO.parse(in_handle, fastq_format):\n                    writer.writerow([rec.id] + rec.letter_annotations[\"phred_quality\"])\n    return out_file", "response": "Convert a fastq file into a CSV of phred quality scores."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a BAM file to fastq files.", "response": "def bam_to_fastq(bam_file, is_paired):\n    \"\"\"Convert a BAM file to fastq files.\n    \"\"\"\n    out_files, out_handles = _get_fastq_handles(bam_file,\n            is_paired)\n    if len(out_handles) > 0:\n        in_bam = pysam.Samfile(bam_file, mode='rb')\n        for read in in_bam:\n            num = 1 if (not read.is_paired or read.is_read1) else 2\n            # reverse the sequence and quality if mapped to opposite strand\n            if read.is_reverse:\n                seq = str(Seq.reverse_complement(Seq.Seq(read.seq)))\n                qual = \"\".join(reversed(read.qual))\n            else:\n                seq = read.seq\n                qual = read.qual\n            out_handles[num].write(\"@%s\\n%s\\n+\\n%s\\n\" % (read.qname,\n                seq, qual))\n    [h.close() for h in out_handles.values()]\n    return out_files"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_latex_report(base, report_dir, section_info):\n    out_name = \"%s_recal_plots.tex\" % base\n    out = os.path.join(report_dir, out_name)\n    with open(out, \"w\") as out_handle:\n        out_tmpl = Template(out_template)\n        out_handle.write(out_tmpl.render(sections=section_info))\n    start_dir = os.getcwd()\n    try:\n        os.chdir(report_dir)\n        cl = [\"pdflatex\", out_name]\n        child = subprocess.Popen(cl)\n        child.wait()\n    finally:\n        os.chdir(start_dir)", "response": "Generate a pdf report with plots using latex."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngrouping a set of sample items by batch or singleton name.", "response": "def group_by_batch(items, require_bam=True):\n    \"\"\"Group a set of sample items by batch (or singleton) name.\n\n    Items in multiple batches cause two batches to be merged together.\n    \"\"\"\n    out = collections.defaultdict(list)\n    batch_groups = _get_representative_batch(_merge_batches(_find_all_groups(items, require_bam)))\n    for data in items:\n        batches = _get_batches(data, require_bam)\n        # take first batch as representative\n        batch = batch_groups[batches[0]]\n        out[batch].append(utils.deepish_copy(data))\n    return dict(out)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if a work input needs processing for parallelization.", "response": "def bam_needs_processing(data):\n    \"\"\"Check if a work input needs processing for parallelization.\n    \"\"\"\n    return ((data.get(\"work_bam\") or data.get(\"align_bam\")) and\n            (any(tz.get_in([\"config\", \"algorithm\", x], data) for x in\n                 [\"variantcaller\", \"mark_duplicates\", \"recalibrate\", \"realign\", \"svcaller\",\n                  \"jointcaller\", \"variant_regions\"])\n             or any(k in data for k in [\"cwl_keys\", \"output_cwl_keys\"])))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_batch_for_key(data):\n    batches = _get_batches(data, require_bam=False)\n    if len(batches) == 1:\n        return batches[0]\n    else:\n        return tuple(batches)", "response": "Retrieve batch information useful as a unique key for the sample."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds all groups in a list of items", "response": "def _find_all_groups(items, require_bam=True):\n    \"\"\"Find all groups\n    \"\"\"\n    all_groups = []\n    for data in items:\n        batches = _get_batches(data, require_bam)\n        all_groups.append(batches)\n    return all_groups"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _merge_batches(all_groups):\n    merged = []\n    while len(all_groups) > 0:\n        first, rest = all_groups[0], all_groups[1:]\n        first = set(first)\n        lf = -1\n        while len(first) > lf:\n            lf = len(first)\n\n            rest2 = []\n            for r in rest:\n                if len(first.intersection(set(r))) > 0:\n                    first |= set(r)\n                else:\n                    rest2.append(r)\n            rest = rest2\n        merged.append(first)\n        all_groups = rest\n    return merged", "response": "Merge batches with overlapping groups. Uses merge approach from :\n    http://stackoverflow. com/a/4842897/252589\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprepares dictionary matching batch items to a representative within a group.", "response": "def _get_representative_batch(merged):\n    \"\"\"Prepare dictionary matching batch items to a representative within a group.\n    \"\"\"\n    out = {}\n    for mgroup in merged:\n        mgroup = sorted(list(mgroup))\n        for x in mgroup:\n            out[x] = mgroup[0]\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nshares functionality for grouping by batches for variant calling and joint calling.", "response": "def _group_batches_shared(xs, caller_batch_fn, prep_data_fn):\n    \"\"\"Shared functionality for grouping by batches for variant calling and joint calling.\n    \"\"\"\n    singles = []\n    batch_groups = collections.defaultdict(list)\n    for args in xs:\n        data = utils.to_single_data(args)\n        caller, batch = caller_batch_fn(data)\n        region = _list_to_tuple(data[\"region\"]) if \"region\" in data else ()\n        if batch is not None:\n            batches = batch if isinstance(batch, (list, tuple)) else [batch]\n            for b in batches:\n                batch_groups[(b, region, caller)].append(utils.deepish_copy(data))\n        else:\n            data = prep_data_fn(data, [data])\n            singles.append(data)\n    batches = []\n    for batch, items in batch_groups.items():\n        batch_data = utils.deepish_copy(_pick_lead_item(items))\n        # For nested primary batches, split permanently by batch\n        if tz.get_in([\"metadata\", \"batch\"], batch_data):\n            batch_name = batch[0]\n            batch_data[\"metadata\"][\"batch\"] = batch_name\n        batch_data = prep_data_fn(batch_data, items)\n        batch_data[\"group_orig\"] = _collapse_subitems(batch_data, items)\n        batch_data[\"group\"] = batch\n        batches.append(batch_data)\n    return singles + batches"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef group_batches(xs):\n    def _caller_batches(data):\n        caller = tz.get_in((\"config\", \"algorithm\", \"variantcaller\"), data)\n        jointcaller = tz.get_in((\"config\", \"algorithm\", \"jointcaller\"), data)\n        batch = tz.get_in((\"metadata\", \"batch\"), data) if not jointcaller else None\n        return caller, batch\n    def _prep_data(data, items):\n        data[\"region_bams\"] = [x[\"region_bams\"] for x in items]\n        return data\n    return _group_batches_shared(xs, _caller_batches, _prep_data)", "response": "Group samples into batches for simultaneous variant calling."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms grouping by batches for joint calling and squaring off.", "response": "def group_batches_joint(samples):\n    \"\"\"Perform grouping by batches for joint calling/squaring off.\n    \"\"\"\n    def _caller_batches(data):\n        jointcaller = tz.get_in((\"config\", \"algorithm\", \"jointcaller\"), data)\n        batch = tz.get_in((\"metadata\", \"batch\"), data) if jointcaller else None\n        return jointcaller, batch\n    def _prep_data(data, items):\n        for r in [\"callable_regions\", \"variant_regions\"]:\n            data[r] = list(set(filter(lambda x: x is not None,\n                                      [tz.get_in((\"config\", \"algorithm\", r), d) for d in items])))\n        data[\"work_bams\"] = [dd.get_align_bam(x) or dd.get_work_bam(x) for x in items]\n        data[\"vrn_files\"] = [x[\"vrn_file\"] for x in items]\n        return data\n    return _group_batches_shared(samples, _caller_batches, _prep_data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _collapse_subitems(base, items):\n    out = []\n    for d in items:\n        newd = _diff_dict(base, d)\n        out.append(newd)\n    return out", "response": "Collapse full data representations relative to a standard base."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _pick_lead_item(items):\n    if vcfutils.is_paired_analysis([dd.get_align_bam(x) for x in items], items):\n        for data in items:\n            if vcfutils.get_paired_phenotype(data) == \"tumor\":\n                return data\n        raise ValueError(\"Did not find tumor sample in paired tumor/normal calling\")\n    else:\n        return items[0]", "response": "Pick lead item for batch calling"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves original items from a diffed set of nested samples.", "response": "def get_orig_items(base):\n    \"\"\"Retrieve original items from a diffed set of nested samples.\n    \"\"\"\n    assert \"group_orig\" in base\n    out = []\n    for data_diff in base[\"group_orig\"]:\n        new = utils.deepish_copy(base)\n        new.pop(\"group_orig\")\n        out.append(_patch_dict(data_diff, new))\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npatch a dictionary substituting in changed items from the nested diff.", "response": "def _patch_dict(diff, base):\n    \"\"\"Patch a dictionary, substituting in changed items from the nested diff.\n    \"\"\"\n    for k, v in diff.items():\n        if isinstance(v, dict):\n            base[k] = _patch_dict(v, base.get(k, {}))\n        elif not v:\n            base.pop(k, None)\n        else:\n            base[k] = v\n    return base"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef split_variants_by_sample(data):\n    # not split, do nothing\n    if \"group_orig\" not in data:\n        return [[data]]\n    # cancer tumor/normal\n    elif (vcfutils.get_paired_phenotype(data)\n            and \"tumor\" in [vcfutils.get_paired_phenotype(d) for d in get_orig_items(data)]):\n        out = []\n        for i, sub_data in enumerate(get_orig_items(data)):\n            if vcfutils.get_paired_phenotype(sub_data) == \"tumor\":\n                cur_batch = tz.get_in([\"metadata\", \"batch\"], data)\n                if cur_batch:\n                    sub_data[\"metadata\"][\"batch\"] = cur_batch\n                sub_data[\"vrn_file\"] = data[\"vrn_file\"]\n            else:\n                sub_data.pop(\"vrn_file\", None)\n            out.append([sub_data])\n        return out\n    # joint calling or population runs, do not split back up and keep in batches\n    else:\n        out = []\n        for sub_data in get_orig_items(data):\n            cur_batch = tz.get_in([\"metadata\", \"batch\"], data)\n            if cur_batch:\n                sub_data[\"metadata\"][\"batch\"] = cur_batch\n            sub_data[\"vrn_file_batch\"] = data[\"vrn_file\"]\n            sub_data[\"vrn_file\"] = data[\"vrn_file\"]\n            out.append([sub_data])\n        return out", "response": "Split a multi - sample call file into inputs for individual samples."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning filtering on the input call file handling SNPs and indels separately.", "response": "def run(call_file, ref_file, vrn_files, data):\n    \"\"\"Run filtering on the input call file, handling SNPs and indels separately.\n    \"\"\"\n    algs = [data[\"config\"][\"algorithm\"]] * len(data.get(\"vrn_files\", [1]))\n    if includes_missingalt(data):\n        logger.info(\"Removing variants with missing alts from %s.\" % call_file)\n        call_file = gatk_remove_missingalt(call_file, data)\n\n    if \"gatkcnn\" in dd.get_tools_on(data):\n        return _cnn_filter(call_file, vrn_files, data)\n    elif config_utils.use_vqsr(algs, call_file):\n        if vcfutils.is_gvcf_file(call_file):\n            raise ValueError(\"Cannot force gVCF output with joint calling using tools_on: [gvcf] and use VQSR. \"\n                             \"Try using cutoff-based soft filtering with tools_off: [vqsr]\")\n        snp_file, indel_file = vcfutils.split_snps_indels(call_file, ref_file, data[\"config\"])\n        snp_filter_file = _variant_filtration(snp_file, ref_file, vrn_files, data, \"SNP\",\n                                              vfilter.gatk_snp_cutoff)\n        indel_filter_file = _variant_filtration(indel_file, ref_file, vrn_files, data, \"INDEL\",\n                                                vfilter.gatk_indel_cutoff)\n        orig_files = [snp_filter_file, indel_filter_file]\n        out_file = \"%scombined.vcf.gz\" % os.path.commonprefix(orig_files)\n        combined_file = vcfutils.combine_variant_files(orig_files, out_file, ref_file, data[\"config\"])\n        return combined_file\n    else:\n        snp_filter = vfilter.gatk_snp_cutoff(call_file, data)\n        indel_filter = vfilter.gatk_indel_cutoff(snp_filter, data)\n        return indel_filter"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nperform CNN filtering on input VCF using pre - trained models.", "response": "def _cnn_filter(in_file, vrn_files, data):\n    \"\"\"Perform CNN filtering on input VCF using pre-trained models.\n    \"\"\"\n    #tensor_type = \"reference\"  # 1D, reference sequence\n    tensor_type = \"read_tensor\"  # 2D, reads, flags, mapping quality\n    score_file = _cnn_score_variants(in_file, tensor_type, data)\n    return _cnn_tranch_filtering(score_file, vrn_files, tensor_type, data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfiltering CNN scored VCFs in tranches using standard SNP and Indel truth sets.", "response": "def _cnn_tranch_filtering(in_file, vrn_files, tensor_type, data):\n    \"\"\"Filter CNN scored VCFs in tranches using standard SNP and Indel truth sets.\n    \"\"\"\n    out_file = \"%s-filter.vcf.gz\" % utils.splitext_plus(in_file)[0]\n    if not utils.file_uptodate(out_file, in_file):\n        runner = broad.runner_from_config(data[\"config\"])\n        gatk_type = runner.gatk_type()\n        assert gatk_type == \"gatk4\", \"CNN filtering requires GATK4\"\n        if \"train_hapmap\" not in vrn_files:\n            raise ValueError(\"CNN filtering requires HapMap training inputs: %s\" % vrn_files)\n        with file_transaction(data, out_file) as tx_out_file:\n            params = [\"-T\", \"FilterVariantTranches\", \"--variant\", in_file,\n                      \"--output\", tx_out_file,\n                      \"--snp-truth-vcf\", vrn_files[\"train_hapmap\"],\n                      \"--indel-truth-vcf\", vrn_files[\"train_indels\"]]\n            if tensor_type == \"reference\":\n                params += [\"--info-key\", \"CNN_1D\", \"--tranche\", \"99\"]\n            else:\n                assert tensor_type == \"read_tensor\"\n                params += [\"--info-key\", \"CNN_2D\", \"--tranche\", \"99\"]\n            runner.run_gatk(params)\n    return vcfutils.bgzip_and_index(out_file, data[\"config\"])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _cnn_score_variants(in_file, tensor_type, data):\n    out_file = \"%s-cnnscore.vcf.gz\" % utils.splitext_plus(in_file)[0]\n    if not utils.file_uptodate(out_file, in_file):\n        runner = broad.runner_from_config(data[\"config\"])\n        gatk_type = runner.gatk_type()\n        assert gatk_type == \"gatk4\", \"CNN filtering requires GATK4\"\n        with file_transaction(data, out_file) as tx_out_file:\n            params = [\"-T\", \"CNNScoreVariants\", \"--variant\", in_file, \"--reference\", dd.get_ref_file(data),\n                    \"--output\", tx_out_file, \"--input\", dd.get_align_bam(data)]\n            params += [\"--tensor-type\", tensor_type]\n            runner.run_gatk(params)\n    return vcfutils.bgzip_and_index(out_file, data[\"config\"])", "response": "Score variants with pre - trained CNN models."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _apply_vqsr(in_file, ref_file, recal_file, tranch_file,\n                sensitivity_cutoff, filter_type, data):\n    \"\"\"Apply VQSR based on the specified tranche, returning a filtered VCF file.\n    \"\"\"\n    base, ext = utils.splitext_plus(in_file)\n    out_file = \"{base}-{filter}filter{ext}\".format(base=base, ext=ext,\n                                                   filter=filter_type)\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            broad_runner = broad.runner_from_config(data[\"config\"])\n            gatk_type = broad_runner.gatk_type()\n            if gatk_type == \"gatk4\":\n                params = [\"-T\", \"ApplyVQSR\",\n                          \"--variant\", in_file,\n                          \"--output\", tx_out_file,\n                          \"--recal-file\", recal_file,\n                          \"--tranches-file\", tranch_file]\n            else:\n                params = [\"-T\", \"ApplyRecalibration\",\n                          \"--input\", in_file,\n                          \"--out\", tx_out_file,\n                          \"--recal_file\", recal_file,\n                          \"--tranches_file\", tranch_file]\n            params += [\"-R\", ref_file,\n                       \"--mode\", filter_type]\n            resources = config_utils.get_resources(\"gatk_apply_recalibration\", data[\"config\"])\n            opts = resources.get(\"options\", [])\n            if not opts:\n                if gatk_type == \"gatk4\":\n                    opts += [\"--truth-sensitivity-filter-level\", sensitivity_cutoff]\n                else:\n                    opts += [\"--ts_filter_level\", sensitivity_cutoff]\n            params += opts\n            broad_runner.run_gatk(params)\n    return out_file", "response": "Apply VQSR based on the specified tranche returning a filtered VCF file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving training data returning an empty set of information if not available.", "response": "def _get_training_data(vrn_files):\n    \"\"\"Retrieve training data, returning an empty set of information if not available.\n    \"\"\"\n    out = {\"SNP\": [], \"INDEL\": []}\n    # SNPs\n    for name, train_info in [(\"train_hapmap\", \"known=false,training=true,truth=true,prior=15.0\"),\n                             (\"train_omni\", \"known=false,training=true,truth=true,prior=12.0\"),\n                             (\"train_1000g\", \"known=false,training=true,truth=false,prior=10.0\"),\n                             (\"dbsnp\", \"known=true,training=false,truth=false,prior=2.0\")]:\n        if name not in vrn_files:\n            return {}\n        else:\n            out[\"SNP\"].append((name.replace(\"train_\", \"\"), train_info, vrn_files[name]))\n    # Indels\n    if \"train_indels\" in vrn_files:\n        out[\"INDEL\"].append((\"mills\", \"known=true,training=true,truth=true,prior=12.0\",\n                             vrn_files[\"train_indels\"]))\n    else:\n        return {}\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_vqsr_training(filter_type, vrn_files, gatk_type):\n    params = []\n    for name, train_info, fname in _get_training_data(vrn_files)[filter_type]:\n        if gatk_type == \"gatk4\":\n            params.extend([\"--resource:%s,%s\" % (name, train_info), fname])\n            if filter_type == \"INDEL\":\n                params.extend([\"--max-gaussians\", \"4\"])\n        else:\n            params.extend([\"-resource:%s,VCF,%s\" % (name, train_info), fname])\n            if filter_type == \"INDEL\":\n                params.extend([\"--maxGaussians\", \"4\"])\n    return params", "response": "Return parameters for VQSR training handling SNPs and Indels."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves appropriate annotations to use for VQSR based on filter type.", "response": "def _get_vqsr_annotations(filter_type, data):\n    \"\"\"Retrieve appropriate annotations to use for VQSR based on filter type.\n\n    Issues reported with MQ and bwa-mem quality distribution, results in intermittent\n    failures to use VQSR:\n    http://gatkforums.broadinstitute.org/discussion/4425/variant-recalibration-failing\n    http://gatkforums.broadinstitute.org/discussion/4248/variantrecalibrator-removing-all-snps-from-the-training-set\n    \"\"\"\n    if filter_type == \"SNP\":\n        # MQ, MQRankSum\n        anns = [\"QD\", \"FS\", \"ReadPosRankSum\", \"SOR\"]\n    else:\n        assert filter_type == \"INDEL\"\n        # MQRankSum\n        anns = [\"QD\", \"FS\", \"ReadPosRankSum\", \"SOR\"]\n    if dd.get_coverage_interval(data) == \"genome\":\n        anns += [\"DP\"]\n    return anns"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns variant quality score recalibration.", "response": "def _run_vqsr(in_file, ref_file, vrn_files, sensitivity_cutoff, filter_type, data):\n    \"\"\"Run variant quality score recalibration.\n    \"\"\"\n    cutoffs = [\"100.0\", \"99.99\", \"99.98\", \"99.97\", \"99.96\", \"99.95\", \"99.94\", \"99.93\", \"99.92\", \"99.91\",\n               \"99.9\", \"99.8\", \"99.7\", \"99.6\", \"99.5\", \"99.0\", \"98.0\", \"90.0\"]\n    if sensitivity_cutoff not in cutoffs:\n        cutoffs.append(sensitivity_cutoff)\n        cutoffs.sort()\n    broad_runner = broad.runner_from_config(data[\"config\"])\n    gatk_type = broad_runner.gatk_type()\n    base = utils.splitext_plus(in_file)[0]\n    recal_file = (\"%s-vqsrrecal.vcf.gz\" % base) if gatk_type == \"gatk4\" else (\"%s.recal\" % base)\n    tranches_file = \"%s.tranches\" % base\n    plot_file = \"%s-plots.R\" % base\n    if not utils.file_exists(recal_file):\n        with file_transaction(data, recal_file, tranches_file, plot_file) as (tx_recal, tx_tranches, tx_plot_file):\n            params = [\"-T\", \"VariantRecalibrator\",\n                      \"-R\", ref_file,\n                      \"--mode\", filter_type]\n            if gatk_type == \"gatk4\":\n                params += [\"--variant\", in_file, \"--output\", tx_recal,\n                           \"--tranches-file\", tx_tranches, \"--rscript-file\", tx_plot_file]\n            else:\n                params += [\"--input\", in_file, \"--recal_file\", tx_recal,\n                           \"--tranches_file\", tx_tranches, \"--rscript_file\", tx_plot_file]\n            params += _get_vqsr_training(filter_type, vrn_files, gatk_type)\n            resources = config_utils.get_resources(\"gatk_variant_recalibrator\", data[\"config\"])\n            opts = resources.get(\"options\", [])\n            if not opts:\n                for cutoff in cutoffs:\n                    opts += [\"-tranche\", str(cutoff)]\n                for a in _get_vqsr_annotations(filter_type, data):\n                    opts += [\"-an\", a]\n            params += opts\n            cores = dd.get_cores(data)\n            memscale = {\"magnitude\": 0.9 * cores, \"direction\": \"increase\"} if cores > 1 else None\n            try:\n                broad_runner.new_resources(\"gatk-vqsr\")\n                broad_runner.run_gatk(params, log_error=False, memscale=memscale, parallel_gc=True)\n            except:  # Can fail to run if not enough values are present to train.\n                return None, None\n    if gatk_type == \"gatk4\":\n        vcfutils.bgzip_and_index(recal_file, data[\"config\"])\n    return recal_file, tranches_file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _already_cutoff_filtered(in_file, filter_type):\n    filter_file = \"%s-filter%s.vcf.gz\" % (utils.splitext_plus(in_file)[0], filter_type)\n    return utils.file_exists(filter_file)", "response": "Check if we have a cutoff - based filter file from previous VQSR failure."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _variant_filtration(in_file, ref_file, vrn_files, data, filter_type,\n                        hard_filter_fn):\n    \"\"\"Filter SNP and indel variant calls using GATK best practice recommendations.\n\n    Use cutoff-based soft filters if configuration indicates too little data or\n    already finished a cutoff-based filtering step, otherwise try VQSR.\n    \"\"\"\n    # Algorithms multiplied by number of input files to check for large enough sample sizes\n    algs = [data[\"config\"][\"algorithm\"]] * len(data.get(\"vrn_files\", [1]))\n    if (not config_utils.use_vqsr(algs, in_file) or\n          _already_cutoff_filtered(in_file, filter_type)):\n        logger.info(\"Skipping VQSR, using cutoff-based filers: we don't have whole genome input data\")\n        return hard_filter_fn(in_file, data)\n    elif not _have_training_data(vrn_files):\n        logger.info(\"Skipping VQSR, using cutoff-based filers: genome build does not have sufficient training data\")\n        return hard_filter_fn(in_file, data)\n    else:\n        sensitivities = {\"INDEL\": \"98.0\", \"SNP\": \"99.97\"}\n        recal_file, tranches_file = _run_vqsr(in_file, ref_file, vrn_files,\n                                              sensitivities[filter_type], filter_type, data)\n        if recal_file is None:  # VQSR failed\n            logger.info(\"VQSR failed due to lack of training data. Using cutoff-based soft filtering.\")\n            return hard_filter_fn(in_file, data)\n        else:\n            return _apply_vqsr(in_file, ref_file, recal_file, tranches_file,\n                               sensitivities[filter_type], filter_type, data)", "response": "Filter SNP and indel variant calls using GATK best practice recommendations."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef includes_missingalt(data):\n    MISSINGALT_VERSION = LooseVersion(\"4.1.0.0\")\n    version = LooseVersion(broad.get_gatk_version(config=dd.get_config(data)))\n    return version >= MISSINGALT_VERSION", "response": "Returns True if the data contains a missing alternative variant."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove missing ALTs from a GATK input file.", "response": "def gatk_remove_missingalt(in_file, data):\n    \"\"\"\n    GATK 4.1.0.0 outputs variants that have missing ALTs, which breaks downstream\n    tools, this filters those out.\n    \"\"\"\n    base = in_file.split('.vcf.gz')[0]\n    out_file = \"%s-nomissingalt%s\" % (base, '.vcf.gz')\n    if utils.file_exists(out_file):\n        return out_file\n    no_gzip_out = out_file.replace(\".vcf.gz\", \".vcf\")\n    with file_transaction(no_gzip_out) as tx_out_file:\n        with utils.open_gzipsafe(in_file) as in_handle, open(tx_out_file, \"w\") as out_handle:\n            for line in in_handle:\n                line = remove_missingalt(line)\n                if line:\n                    out_handle.write(line)\n    return vcfutils.bgzip_and_index(no_gzip_out, data[\"config\"])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gene_tracking_to_fpkm(tracking_file, out_file):\n    if file_exists(out_file):\n        return out_file\n    df = pd.io.parsers.read_table(tracking_file, sep=\"\\t\", header=0)\n    df = df[['tracking_id', 'FPKM']]\n    df = df.groupby(['tracking_id']).sum()\n    df.to_csv(out_file, sep=\"\\t\", header=False, index_label=False)\n    return out_file", "response": "Takes a gene - level tracking file from Cufflinks and outputs a two column\n    table with the first column as IDs and the second column as FPKM for the sample."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclean the likely garbage transcripts from the GTF file.", "response": "def clean_assembly(gtf_file, clean=None, dirty=None):\n    \"\"\"\n    clean the likely garbage transcripts from the GTF file including:\n    1. any novel single-exon transcripts\n    2. any features with an unknown strand\n    \"\"\"\n    base, ext = os.path.splitext(gtf_file)\n    db = gtf.get_gtf_db(gtf_file, in_memory=True)\n    clean = clean if clean else base + \".clean\" + ext\n    dirty = dirty if dirty else base + \".dirty\" + ext\n    if file_exists(clean):\n        return clean, dirty\n    logger.info(\"Cleaning features with an unknown strand from the assembly.\")\n    with open(clean, \"w\") as clean_handle, open(dirty, \"w\") as dirty_handle:\n        for gene in db.features_of_type('gene'):\n            for transcript in db.children(gene, level=1):\n                if is_likely_noise(db, transcript):\n                    write_transcript(db, dirty_handle, transcript)\n                else:\n                    write_transcript(db, clean_handle, transcript)\n    return clean, dirty"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the transcript is unknown.", "response": "def strand_unknown(db, transcript):\n    \"\"\"\n    for unstranded data with novel transcripts single exon genes\n    will have no strand information. single exon novel genes are also\n    a source of noise in the Cufflinks assembly so this removes them\n    \"\"\"\n    features = list(db.children(transcript))\n    strand = features[0].strand\n    if strand == \".\":\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fix_cufflinks_attributes(ref_gtf, merged_gtf, data, out_file=None):\n    base, ext = os.path.splitext(merged_gtf)\n    fixed = out_file if out_file else base + \".clean.fixed\" + ext\n    if file_exists(fixed):\n        return fixed\n    ref_db = gtf.get_gtf_db(ref_gtf)\n    merged_db = gtf.get_gtf_db(merged_gtf, in_memory=True)\n\n    ref_tid_to_gid = {}\n    for gene in ref_db.features_of_type('gene'):\n        for transcript in ref_db.children(gene, level=1):\n            ref_tid_to_gid[transcript.id] = gene.id\n\n    ctid_to_cgid = {}\n    ctid_to_oid = {}\n    for gene in merged_db.features_of_type('gene'):\n        for transcript in merged_db.children(gene, level=1):\n            ctid_to_cgid[transcript.id] = gene.id\n            feature = list(merged_db.children(transcript))[0]\n            oid = feature.attributes.get(\"oId\", [None])[0]\n            if oid:\n                ctid_to_oid[transcript.id] = oid\n    cgid_to_gid = {}\n    for ctid, oid in ctid_to_oid.items():\n        cgid = ctid_to_cgid.get(ctid, None)\n        oid = ctid_to_oid.get(ctid, None)\n        gid = ref_tid_to_gid.get(oid, None) if oid else None\n        if cgid and gid:\n            cgid_to_gid[cgid] = gid\n\n    with file_transaction(data, fixed) as tmp_fixed_file:\n        with open(tmp_fixed_file, \"w\") as out_handle:\n            for gene in merged_db.features_of_type('gene'):\n                for transcript in merged_db.children(gene, level=1):\n                    for feature in merged_db.children(transcript):\n                        cgid = feature.attributes.get(\"gene_id\", [None])[0]\n                        gid = cgid_to_gid.get(cgid, None)\n                        ctid = None\n                        if gid:\n                            feature.attributes[\"gene_id\"][0] = gid\n                            ctid = feature.attributes.get(\"transcript_id\",\n                                                          [None])[0]\n                        tid = ctid_to_oid.get(ctid, None)\n                        if tid:\n                            feature.attributes[\"transcript_id\"][0] = tid\n                        if \"nearest_ref\" in feature.attributes:\n                            del feature.attributes[\"nearest_ref\"]\n                        if \"oId\" in feature.attributes:\n                            del feature.attributes[\"oId\"]\n                        out_handle.write(str(feature) + \"\\n\")\n    return fixed", "response": "Replace the cufflinks gene_id and transcript_id with the cufflinks gene_id and transcript_id from merged_gtf where available\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns cuffmerge on a set of assembled GTF files", "response": "def merge(assembled_gtfs, ref_file, gtf_file, num_cores, data):\n    \"\"\"\n    run cuffmerge on a set of assembled GTF files\n    \"\"\"\n    assembled_file = tempfile.NamedTemporaryFile(delete=False).name\n    with open(assembled_file, \"w\") as temp_handle:\n        for assembled in assembled_gtfs:\n            temp_handle.write(assembled + \"\\n\")\n    out_dir = os.path.join(\"assembly\", \"cuffmerge\")\n    merged_file = os.path.join(out_dir, \"merged.gtf\")\n    out_file = os.path.join(out_dir, \"assembled.gtf\")\n    if file_exists(out_file):\n        return out_file\n    if not file_exists(merged_file):\n        with file_transaction(data, out_dir) as tmp_out_dir:\n            cmd = (\"cuffmerge -o {tmp_out_dir} --ref-gtf {gtf_file} \"\n                   \"--num-threads {num_cores} --ref-sequence {ref_file} \"\n                   \"{assembled_file}\")\n            cmd = cmd.format(**locals())\n            message = (\"Merging the following transcript assemblies with \"\n                       \"Cuffmerge: %s\" % \", \".join(assembled_gtfs))\n            do.run(cmd, message)\n    clean, _ = clean_assembly(merged_file)\n    fixed = fix_cufflinks_attributes(gtf_file, clean, data)\n    classified = annotate_gtf.annotate_novel_coding(fixed, gtf_file, ref_file,\n                                                    data)\n    filtered = annotate_gtf.cleanup_transcripts(classified, gtf_file, ref_file)\n    shutil.move(filtered, out_file)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn breakend information line with mate and imprecise location.", "response": "def _vcf_info(start, end, mate_id, info=None):\n    \"\"\"Return breakend information line with mate and imprecise location.\n    \"\"\"\n    out = \"SVTYPE=BND;MATEID={mate};IMPRECISE;CIPOS=0,{size}\".format(\n        mate=mate_id, size=end-start)\n    if info is not None:\n        extra_info = \";\".join(\"{0}={1}\".format(k, v) for k, v in info.iteritems())\n        out = \"{0};{1}\".format(out, extra_info)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate ALT line in VCF 4. 1 format associating with other paired end.", "response": "def _vcf_alt(base, other_chr, other_pos, isrc, is_first):\n    \"\"\"Create ALT allele line in VCF 4.1 format associating with other paired end.\n    \"\"\"\n    if is_first:\n        pipe = \"[\" if isrc else \"]\"\n        out_str = \"{base}{pipe}{chr}:{pos}{pipe}\"\n    else:\n        pipe = \"]\" if isrc else \"[\"\n        out_str = \"{pipe}{chr}:{pos}{pipe}{base}\"\n    return out_str.format(pipe=pipe, chr=other_chr, pos=other_pos + 1,\n                          base=base)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting BEDPE strand representation of breakpoints into VCF.", "response": "def _breakend_orientation(strand1, strand2):\n    \"\"\"Convert BEDPE strand representation of breakpoints into VCF.\n\n    | strand1  |  strand2 |     VCF      |\n    +----------+----------+--------------+\n    |   +      |     -    | t[p[ ]p]t    |\n    |   +      |     +    | t]p] t]p]    |\n    |   -      |     -    | [p[t [p[t    |\n    |   -      |     +    | ]p]t t[p[    |\n    \"\"\"\n    EndOrientation = namedtuple(\"EndOrientation\",\n                                [\"is_first1\", \"is_rc1\", \"is_first2\", \"is_rc2\"])\n    if strand1 == \"+\" and strand2 == \"-\":\n        return EndOrientation(True, True, False, True)\n    elif strand1 == \"+\" and strand2 == \"+\":\n        return EndOrientation(True, False, True, False)\n    elif strand1 == \"-\" and strand2 == \"-\":\n        return EndOrientation(False, False, False, False)\n    elif strand1 == \"-\" and strand2 == \"+\":\n        return EndOrientation(False, True, True, True)\n    else:\n        raise ValueError(\"Unexpected strand pairing: {0} {1}\".format(\n            strand1, strand2))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts BedPe feature information into VCF parts.", "response": "def build_vcf_parts(feature, genome_2bit, info=None):\n    \"\"\"Convert BedPe feature information into VCF part representation.\n\n    Each feature will have two VCF lines for each side of the breakpoint.\n    \"\"\"\n    base1 = genome_2bit[feature.chrom1].get(\n        feature.start1, feature.start1 + 1).upper()\n    id1 = \"hydra{0}a\".format(feature.name)\n    base2 = genome_2bit[feature.chrom2].get(\n        feature.start2, feature.start2 + 1).upper()\n    id2 = \"hydra{0}b\".format(feature.name)\n    orientation = _breakend_orientation(feature.strand1, feature.strand2)\n    return (VcfLine(feature.chrom1, feature.start1, id1, base1,\n                    _vcf_alt(base1, feature.chrom2, feature.start2,\n                             orientation.is_rc1, orientation.is_first1),\n                    _vcf_info(feature.start1, feature.end1, id2, info)),\n            VcfLine(feature.chrom2, feature.start2, id2, base2,\n                    _vcf_alt(base2, feature.chrom1, feature.start1,\n                             orientation.is_rc2, orientation.is_first2),\n                    _vcf_info(feature.start2, feature.end2, id1, info)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a VcfLine for deletion from BedPE breakpoints.", "response": "def build_vcf_deletion(x, genome_2bit):\n    \"\"\"Provide representation of deletion from BedPE breakpoints.\n    \"\"\"\n    base1 = genome_2bit[x.chrom1].get(x.start1, x.start1 + 1).upper()\n    id1 = \"hydra{0}\".format(x.name)\n    return VcfLine(x.chrom1, x.start1, id1, base1, \"<DEL>\",\n                   _vcf_single_end_info(x, \"DEL\", True))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_vcf_inversion(x1, x2, genome_2bit):\n    id1 = \"hydra{0}\".format(x1.name)\n    start_coords = sorted([x1.start1, x1.end1, x2.start1, x2.end1])\n    end_coords = sorted([x1.start2, x1.end2, x2.start2, x2.start2])\n    start_pos = (start_coords[1] + start_coords[2]) // 2\n    end_pos = (end_coords[1] + end_coords[2]) // 2\n    base1 = genome_2bit[x1.chrom1].get(start_pos, start_pos + 1).upper()\n    info = \"SVTYPE=INV;IMPRECISE;CIPOS={cip1},{cip2};CIEND={cie1},{cie2};\" \\\n           \"END={end};SVLEN={length}\".format(cip1=start_pos - start_coords[0],\n                                             cip2=start_coords[-1] - start_pos,\n                                             cie1=end_pos - end_coords[0],\n                                             cie2=end_coords[-1] - end_pos,\n                                             end=end_pos,\n                                             length=end_pos-start_pos)\n    return VcfLine(x1.chrom1, start_pos, id1, base1, \"<INV>\", info)", "response": "Build a VCFLine from two inversions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hydra_parser(in_file, options=None):\n    if options is None: options = {}\n    BedPe = namedtuple('BedPe', [\"chrom1\", \"start1\", \"end1\",\n                                 \"chrom2\", \"start2\", \"end2\",\n                                 \"name\", \"strand1\", \"strand2\",\n                                 \"support\"])\n    with open(in_file) as in_handle:\n        reader = csv.reader(in_handle, dialect=\"excel-tab\")\n        for line in reader:\n            cur = BedPe(line[0], int(line[1]), int(line[2]),\n                        line[3], int(line[4]), int(line[5]),\n                        line[6], line[8], line[9],\n                        float(line[18]))\n            if cur.support >= options.get(\"min_support\", 0):\n                yield cur", "response": "Parse a hydra input file into a list of namedtuple of values."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclustering breakends by specified attributes.", "response": "def _cluster_by(end_iter, attr1, attr2, cluster_distance):\n    \"\"\"Cluster breakends by specified attributes.\n    \"\"\"\n    ClusterInfo = namedtuple(\"ClusterInfo\", [\"chroms\", \"clusters\", \"lookup\"])\n    chr_clusters = {}\n    chroms = []\n    brends_by_id = {}\n    for brend in end_iter:\n        if not chr_clusters.has_key(brend.chrom1):\n            chroms.append(brend.chrom1)\n            chr_clusters[brend.chrom1] = ClusterTree(cluster_distance, 1)\n        brends_by_id[int(brend.name)] = brend\n        chr_clusters[brend.chrom1].insert(getattr(brend, attr1),\n                                          getattr(brend, attr2),\n                                          int(brend.name))\n    return ClusterInfo(chroms, chr_clusters, brends_by_id)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _calculate_cluster_distance(end_iter):\n    out = []\n    sizes = []\n    for x in end_iter:\n        out.append(x)\n        sizes.append(x.end1 - x.start1)\n        sizes.append(x.end2 - x.start2)\n    distance = sum(sizes) // len(sizes)\n    return distance, out", "response": "Compute allowed distance for clustering based on end confidence intervals."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngroup together hydra breakends with overlapping ends.", "response": "def group_hydra_breakends(end_iter):\n    \"\"\"Group together hydra breakends with overlapping ends.\n\n    This provides a way to identify inversions, translocations\n    and insertions present in hydra break point ends. We cluster together the\n    endpoints and return together any items with closely oriented pairs.\n    This helps in describing more complex rearrangement events.\n    \"\"\"\n    cluster_distance, all_ends = _calculate_cluster_distance(end_iter)\n    first_cluster = _cluster_by(all_ends, \"start1\", \"end1\", cluster_distance)\n    for chrom in first_cluster.chroms:\n        for _, _, brends in first_cluster.clusters[chrom].getregions():\n            if len(brends) == 1:\n                yield [first_cluster.lookup[brends[0]]]\n            else:\n                second_cluster = _cluster_by([first_cluster.lookup[x] for x in brends],\n                                             \"start2\", \"end2\", cluster_distance)\n                for chrom2 in second_cluster.chroms:\n                    for _, _, brends in second_cluster.clusters[chrom].getregions():\n                        yield [second_cluster.lookup[x] for x in brends]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites the VCF header information for Hydra structural variant.", "response": "def _write_vcf_header(out_handle):\n    \"\"\"Write VCF header information for Hydra structural variant.\n    \"\"\"\n    def w(line):\n        out_handle.write(\"{0}\\n\".format(line))\n    w('##fileformat=VCFv4.1')\n    w('##INFO=<ID=IMPRECISE,Number=0,Type=Flag,Description=\"Imprecise structural variation\">')\n    w('##INFO=<ID=END,Number=1,Type=Integer,'\n      'Description=\"End position of the variant described in this record\">')\n    w('##INFO=<ID=CIPOS,Number=2,Type=Integer,'\n      'Description=\"Confidence interval around POS for imprecise variants\">')\n    w('##INFO=<ID=CIEND,Number=2,Type=Integer,'\n      'Description=\"Confidence interval around END for imprecise variants\">')\n    w('##INFO=<ID=SVLEN,Number=.,Type=Integer,'\n      'Description=\"Difference in length between REF and ALT alleles\">')\n    w('##INFO=<ID=SVTYPE,Number=1,Type=String,Description=\"Type of structural variant\">')\n    w('##INFO=<ID=MATEID,Number=.,Type=String,Description=\"ID of mate breakends\">')\n    w('##INFO=<ID=EVENT,Number=1,Type=String,Description=\"ID of event associated to breakend\">')\n    w('##ALT=<ID=DEL,Description=\"Deletion\">')\n    w('##ALT=<ID=INV,Description=\"Inversion\">')\n    w('##ALT=<ID=DUP,Description=\"Duplication\">')\n    w('##ALT=<ID=DUP:TANDEM,Description=\"Tandem Duplication\">')\n    w('##source=hydra')\n    w(\"#\" + \"\\t\".join([\"CHROM\", \"POS\", \"ID\", \"REF\", \"ALT\", \"QUAL\", \"FILTER\", \"INFO\"]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _write_vcf_breakend(brend, out_handle):\n    out_handle.write(\"{0}\\n\".format(\"\\t\".join(str(x) for x in\n        [brend.chrom, brend.pos + 1, brend.id, brend.ref, brend.alt,\n         \".\", \"PASS\", brend.info])))", "response": "Write out a single VCF line with breakpoint information."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing BEDPE input and yield VCF ready breakends.", "response": "def _get_vcf_breakends(hydra_file, genome_2bit, options=None):\n    \"\"\"Parse BEDPE input, yielding VCF ready breakends.\n    \"\"\"\n    if options is None: options = {}\n    for features in group_hydra_breakends(hydra_parser(hydra_file, options)):\n        if len(features) == 1 and is_deletion(features[0], options):\n            yield build_vcf_deletion(features[0], genome_2bit)\n        elif len(features) == 1 and is_tandem_dup(features[0], options):\n            yield build_tandem_deletion(features[0], genome_2bit)\n        elif len(features) == 2 and is_inversion(*features):\n            yield build_vcf_inversion(features[0], features[1], genome_2bit)\n        elif len(features) == 2 and is_translocation(*features):\n            info = get_translocation_info(features[0], features[1])\n            for feature in features:\n                for brend in build_vcf_parts(feature, genome_2bit, info):\n                    yield brend\n        else:\n            for feature in features:\n                for brend in build_vcf_parts(feature, genome_2bit):\n                    yield brend"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hydra_to_vcf_writer(hydra_file, genome_2bit, options, out_handle):\n    _write_vcf_header(out_handle)\n    brends = list(_get_vcf_breakends(hydra_file, genome_2bit, options))\n    brends.sort(key=attrgetter(\"chrom\", \"pos\"))\n    for brend in brends:\n        _write_vcf_breakend(brend, out_handle)", "response": "Write a sorted VCF file from a hydra file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert kallisto output to a count table where the rows are equivalence classes and the columns are cells", "response": "def kallisto_table(kallisto_dir, index):\n    \"\"\"\n    convert kallisto output to a count table where the rows are\n    equivalence classes and the columns are cells\n    \"\"\"\n    quant_dir = os.path.join(kallisto_dir, \"quant\")\n    out_file = os.path.join(quant_dir, \"matrix.csv\")\n    if file_exists(out_file):\n        return out_file\n    tsvfile = os.path.join(quant_dir, \"matrix.tsv\")\n    ecfile = os.path.join(quant_dir, \"matrix.ec\")\n    cellsfile = os.path.join(quant_dir, \"matrix.cells\")\n    fastafile = os.path.splitext(index)[0] + \".fa\"\n    fasta_names = fasta.sequence_names(fastafile)\n    ec_names = get_ec_names(ecfile, fasta_names)\n    df = pd.read_table(tsvfile, header=None, names=[\"ec\", \"cell\", \"count\"])\n    df[\"ec\"] = [ec_names[x] for x in df[\"ec\"]]\n    df = df.pivot(index='ec', columns='cell', values='count')\n    cellnames = get_cell_names(cellsfile)\n    colnames = [cellnames[x] for x in df.columns]\n    df.columns = colnames\n    df.to_csv(out_file)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_ec_names(ecfile, fasta_names):\n    df = pd.read_table(ecfile, header=None, names=[\"ec\", \"transcripts\"])\n    transcript_groups = [x.split(\",\") for x in df[\"transcripts\"]]\n    transcripts = []\n    for group in transcript_groups:\n        transcripts.append(\":\".join([fasta_names[int(x)] for x in group]))\n    return transcripts", "response": "convert equivalence classes to their set of transcripts\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_dirname(fc_dir):\n    (_, fc_dir) = os.path.split(fc_dir)\n    parts = fc_dir.split(\"_\")\n    name = None\n    date = None\n    for p in parts:\n        if p.endswith((\"XX\", \"xx\", \"XY\", \"X2\")):\n            name = p\n        elif len(p) == 6:\n            try:\n                int(p)\n                date = p\n            except ValueError:\n                pass\n    if name is None or date is None:\n        raise ValueError(\"Did not find flowcell name: %s\" % fc_dir)\n    return name, date", "response": "Parse the flow cell ID and date from a flow cell directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves the qseq directory within Solexa flowcell output.", "response": "def get_qseq_dir(fc_dir):\n    \"\"\"Retrieve the qseq directory within Solexa flowcell output.\n    \"\"\"\n    machine_bc = os.path.join(fc_dir, \"Data\", \"Intensities\", \"BaseCalls\")\n    if os.path.exists(machine_bc):\n        return machine_bc\n    # otherwise assume we are in the qseq directory\n    # XXX What other cases can we end up with here?\n    else:\n        return fc_dir"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves the fastq directory within Solexa flowcell output.", "response": "def get_fastq_dir(fc_dir):\n    \"\"\"Retrieve the fastq directory within Solexa flowcell output.\n    \"\"\"\n    full_goat_bc = glob.glob(os.path.join(fc_dir, \"Data\", \"*Firecrest*\", \"Bustard*\"))\n    bustard_bc = glob.glob(os.path.join(fc_dir, \"Data\", \"Intensities\", \"*Bustard*\"))\n    machine_bc = os.path.join(fc_dir, \"Data\", \"Intensities\", \"BaseCalls\")\n    if os.path.exists(machine_bc):\n        return os.path.join(machine_bc, \"fastq\")\n    elif len(full_goat_bc) > 0:\n        return os.path.join(full_goat_bc[0], \"fastq\")\n    elif len(bustard_bc) > 0:\n        return os.path.join(bustard_bc[0], \"fastq\")\n    # otherwise assume we are in the fastq directory\n    # XXX What other cases can we end up with here?\n    else:\n        return fc_dir"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_details(self, run):\n        run_data = dict(run=run)\n        req = urllib.request.Request(\"%s/nglims/api_run_details\" % self._base_url,\n                urllib.parse.urlencode(run_data))\n        response = urllib.request.urlopen(req)\n        info = json.loads(response.read())\n        if \"error\" in info:\n            raise ValueError(\"Problem retrieving info: %s\" % info[\"error\"])\n        else:\n            return info[\"details\"]", "response": "Retrieve sequencing run details as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert fastq inputs into internal Mosaik representation.", "response": "def _convert_fastq(fastq_file, pair_file, rg_name, out_file, config):\n    \"\"\"Convert fastq inputs into internal Mosaik representation.\n    \"\"\"\n    out_file = \"{0}-fq.mkb\".format(os.path.splitext(out_file)[0])\n    if not file_exists(out_file):\n        with file_transaction(config, out_file) as tx_out_file:\n            cl = [config_utils.get_program(\"mosaik\", config,\n                                           default=\"MosaikAligner\").replace(\"Aligner\", \"Build\")]\n            cl += [\"-q\", fastq_file,\n                   \"-out\", tx_out_file,\n                   \"-st\", config[\"algorithm\"].get(\"platform\", \"illumina\").lower()]\n            if pair_file:\n                cl += [\"-q2\", pair_file]\n            if rg_name:\n                cl += [\"-id\", rg_name]\n            env_set = \"export MOSAIK_TMP={0}\".format(os.path.dirname(tx_out_file))\n            subprocess.check_call(env_set + \" && \" + \" \".join(cl), shell=True)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve default neural network files from GitHub to pass to Mosaik.", "response": "def _get_mosaik_nn_args(out_file):\n    \"\"\"Retrieve default neural network files from GitHub to pass to Mosaik.\n    \"\"\"\n    base_nn_url = \"https://raw.github.com/wanpinglee/MOSAIK/master/src/networkFile/\"\n    out = []\n    for arg, fname in [(\"-annse\", \"2.1.26.se.100.005.ann\"),\n                       (\"-annpe\", \"2.1.26.pe.100.0065.ann\")]:\n        arg_fname = os.path.join(os.path.dirname(out_file), fname)\n        if not file_exists(arg_fname):\n            subprocess.check_call([\"wget\", \"-O\", arg_fname, base_nn_url + fname])\n        out += [arg, arg_fname]\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef align(fastq_file, pair_file, ref_file, names, align_dir, data,\n          extra_args=None):\n    \"\"\"Alignment with MosaikAligner.\n    \"\"\"\n    config = data[\"config\"]\n    rg_name = names.get(\"rg\", None) if names else None\n    out_file = os.path.join(align_dir, \"%s-align.bam\" % names[\"lane\"])\n    if not file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            built_fastq = _convert_fastq(fastq_file, pair_file, rg_name,\n                                         out_file, config)\n            cl = [config_utils.get_program(\"mosaik\", config, default=\"MosaikAligner\")]\n            cl += _mosaik_args_from_config(config)\n            cl += extra_args if extra_args is not None else []\n            cl += [\"-ia\", ref_file,\n                   \"-in\", built_fastq,\n                   \"-out\", os.path.splitext(tx_out_file)[0]]\n            jump_base = os.path.splitext(ref_file)[0]\n            key_file = \"{0}_keys.jmp\".format(jump_base)\n            if file_exists(key_file):\n                cl += [\"-j\", jump_base]\n                # XXX hacky way to guess key size which needs to match\n                # Can I get hash size directly\n                jump_size_gb = os.path.getsize(key_file) / 1073741824.0\n                if jump_size_gb < 1.0:\n                    cl += [\"-hs\", \"13\"]\n            cl += _get_mosaik_nn_args(out_file)\n            env_set = \"export MOSAIK_TMP={0}\".format(os.path.dirname(tx_out_file))\n            subprocess.check_call(env_set + \" && \"+\n                                  \" \".join([str(x) for x in cl]), shell=True)\n            os.remove(built_fastq)\n    return out_file", "response": "Align a fastq file with MosaikAligner."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfetch the local nodes that contain collectl files from the bcbio log file.", "response": "def get_bcbio_nodes(path):\n    \"\"\"Fetch the local nodes (-c local) that contain collectl files from\n       the bcbio log file.\n\n       :returns: A list with unique (non-FQDN) local hostnames\n                 where collectl raw logs can be found.\n    \"\"\"\n    with open(path, 'r') as file_handle:\n        hosts = collections.defaultdict(dict)\n        for line in file_handle:\n            matches = re.search(r'\\]\\s([^:]+):', line)\n            if not matches:\n                continue\n            # Format of the record will be \"[Date] host: Timing: Step\" if distributed,\n            # otherwise the host will be missing and it means its a local run, we can stop\n            elif 'Timing: ' in line and line.split(': ')[1] != 'Timing':\n                hosts = collections.defaultdict(dict, {socket.gethostname() : {}})\n                break\n\n            hosts[matches.group(1)]\n\n    return hosts"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfetches timing information from a bcbio log file.", "response": "def get_bcbio_timings(path):\n    \"\"\"Fetch timing information from a bcbio log file.\"\"\"\n    with open(path, 'r') as file_handle:\n        steps = {}\n        for line in file_handle:\n            matches = re.search(r'^\\[([^\\]]+)\\] ([^:]+: .*)', line)\n            if not matches:\n                continue\n\n            tstamp = matches.group(1)\n            msg = matches.group(2)\n\n            # XXX: new special logs do not have this\n            #if not msg.find('Timing: ') >= 0:\n            #    continue\n\n            when = datetime.strptime(tstamp, '%Y-%m-%dT%H:%MZ').replace(\n                tzinfo=pytz.timezone('UTC'))\n\n            step = msg.split(\":\")[-1].strip()\n            steps[when] = step\n\n        return steps"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwalking an iterable returning the current and previous items as a two - tuple.", "response": "def this_and_prev(iterable):\n    \"\"\"Walk an iterable, returning the current and previous items\n    as a two-tuple.\"\"\"\n    try:\n        item = next(iterable)\n        while True:\n            next_item = next(iterable)\n            yield item, next_item\n            item = next_item\n    except StopIteration:\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the deltas for all of collectl s data values.", "response": "def calc_deltas(data_frame, series=None):\n    \"\"\"Many of collectl's data values are cumulative (monotonically\n    increasing), so subtract the previous value to determine the value\n    for the current interval.\n    \"\"\"\n    series = series or []\n    data_frame = data_frame.sort_index(ascending=True)\n\n    for s in series:\n        prev_values = iter(data_frame[s])\n        # Burn the first value, so the first row we call delta_from_prev()\n        # for gets its previous value from the second row in the series,\n        # and so on.\n        next(prev_values)\n        data_frame[s] = data_frame[s].apply(functools.partial(\n            delta_from_prev, iter(prev_values),\n            this_and_prev(iter(data_frame.index))))\n\n    return data_frame"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove outliers from a series.", "response": "def remove_outliers(series, stddev):\n    \"\"\"Remove the outliers from a series.\"\"\"\n    return series[(series - series.mean()).abs() < stddev * series.std()]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprepare a dataframe for graphing by calculating deltas resampling and removing outliers.", "response": "def prep_for_graph(data_frame, series=None, delta_series=None, smoothing=None,\n                   outlier_stddev=None):\n    \"\"\"Prepare a dataframe for graphing by calculating deltas for\n    series that need them, resampling, and removing outliers.\n    \"\"\"\n    series = series or []\n    delta_series = delta_series or []\n    graph = calc_deltas(data_frame, delta_series)\n\n    for s in series + delta_series:\n        if smoothing:\n            graph[s] = graph[s].resample(smoothing)\n        if outlier_stddev:\n            graph[s] = remove_outliers(graph[s], outlier_stddev)\n\n    return graph[series + delta_series]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_common_plot_features(plot, steps):\n    _setup_matplotlib()\n    plot.yaxis.set_tick_params(labelright=True)\n    plot.set_xlabel('')\n\n    ymax = plot.get_ylim()[1]\n    ticks = {}\n    for tstamp, step in steps.items():\n        if step == 'finished':\n            continue\n        plot.vlines(tstamp, 0, ymax, linestyles='dashed')\n        tstamp = mpl.dates.num2epoch(mpl.dates.date2num(tstamp))\n        ticks[tstamp] = step\n    tick_kvs = sorted(ticks.items())\n    top_axis = plot.twiny()\n    top_axis.set_xlim(*plot.get_xlim())\n    top_axis.set_xticks([k for k, v in tick_kvs])\n    top_axis.set_xticklabels([v for k, v in tick_kvs],\n                             rotation=45, ha='left', size=pylab.rcParams['font.size'])\n\n    plot.set_ylim(0)\n\n    return plot", "response": "Add common plot features to all plots such as bcbio step\n    information."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rawfile_within_timeframe(rawfile, timeframe):\n    matches = re.search(r'-(\\d{8})-', rawfile)\n    if matches:\n        ftime = datetime.strptime(matches.group(1), \"%Y%m%d\")\n        ftime = pytz.utc.localize(ftime)\n\n    return ftime.date() >= timeframe[0].date() and ftime.date() <= timeframe[1].date()", "response": "Checks whether the given raw filename timestamp falls within the given timeframe."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating system statistics from the run data files.", "response": "def resource_usage(bcbio_log, cluster, rawdir, verbose):\n    \"\"\"Generate system statistics from bcbio runs.\n\n    Parse the obtained files and put the information in\n    a :class pandas.DataFrame:.\n\n    :param bcbio_log:   local path to bcbio log file written by the run\n    :param cluster:\n    :param rawdir:      directory to put raw data files\n    :param verbose:     increase verbosity\n\n    :return: a tuple with three dictionaries, the first one contains\n             an instance of :pandas.DataFrame: for each host, the second one\n             contains information regarding the hardware configuration and\n             the last one contains information regarding timing.\n    :type return: tuple\n    \"\"\"\n    data_frames = {}\n    hardware_info = {}\n    time_frame = log_time_frame(bcbio_log)\n\n    for collectl_file in sorted(os.listdir(rawdir)):\n        if not collectl_file.endswith('.raw.gz'):\n            continue\n\n        # Only load filenames within sampling timerange (gathered from bcbio_log time_frame)\n        if rawfile_within_timeframe(collectl_file, time_frame):\n\n            collectl_path = os.path.join(rawdir, collectl_file)\n            data, hardware = load_collectl(\n                collectl_path, time_frame.start, time_frame.end)\n\n            if len(data) == 0:\n                #raise ValueError(\"No data present in collectl file %s, mismatch in timestamps between raw collectl and log file?\", collectl_path)\n                continue\n\n            host = re.sub(r'-\\d{8}-\\d{6}\\.raw\\.gz$', '', collectl_file)\n            hardware_info[host] = hardware\n            if host not in data_frames:\n                data_frames[host] = data\n            else:\n                data_frames[host] = pd.concat([data_frames[host], data])\n\n    return (data_frames, hardware_info, time_frame.steps)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_graphs(data_frames, hardware_info, steps, outdir,\n                    verbose=False):\n    \"\"\"Generate all graphs for a bcbio run.\"\"\"\n    _setup_matplotlib()\n    # Hash of hosts containing (data, hardware, steps) tuple\n    collectl_info = collections.defaultdict(dict)\n\n    for host, data_frame in data_frames.items():\n        if verbose:\n            print('Generating CPU graph for {}...'.format(host))\n        graph, data_cpu = graph_cpu(data_frame, steps, hardware_info[host]['num_cpus'])\n\n        graph.get_figure().savefig(\n            os.path.join(outdir, '{}_cpu.png'.format(host)),\n            bbox_inches='tight', pad_inches=0.25)\n        pylab.close()\n\n        ifaces = set([series.split('_')[0]\n                      for series in data_frame.keys()\n                      if series.startswith(('eth', 'ib'))])\n\n        if verbose:\n            print('Generating network graphs for {}...'.format(host))\n        graph, data_net_bytes = graph_net_bytes(data_frame, steps, ifaces)\n        graph.get_figure().savefig(\n            os.path.join(outdir, '{}_net_bytes.png'.format(host)),\n            bbox_inches='tight', pad_inches=0.25)\n        pylab.close()\n\n        graph, data_net_pkts = graph_net_pkts(data_frame, steps, ifaces)\n        graph.get_figure().savefig(\n            os.path.join(outdir, '{}_net_pkts.png'.format(host)),\n            bbox_inches='tight', pad_inches=0.25)\n        pylab.close()\n\n        if verbose:\n            print('Generating memory graph for {}...'.format(host))\n        graph, data_mem = graph_memory(data_frame, steps, hardware_info[host][\"memory\"])\n        graph.get_figure().savefig(\n            os.path.join(outdir, '{}_memory.png'.format(host)),\n            bbox_inches='tight', pad_inches=0.25)\n        pylab.close()\n\n        if verbose:\n            print('Generating storage I/O graph for {}...'.format(host))\n        drives = set([\n            series.split('_')[0]\n            for series in data_frame.keys()\n            if series.startswith(('sd', 'vd', 'hd', 'xvd'))\n        ])\n        graph, data_disk = graph_disk_io(data_frame, steps, drives)\n        graph.get_figure().savefig(\n            os.path.join(outdir, '{}_disk_io.png'.format(host)),\n            bbox_inches='tight', pad_inches=0.25)\n        pylab.close()\n\n        print('Serializing output to pickle object for node {}...'.format(host))\n        # \"Clean\" dataframes ready to be plotted\n        collectl_info[host] = { \"hardware\": hardware_info,\n                                \"steps\": steps, \"cpu\": data_cpu, \"mem\": data_mem,\n                                \"disk\": data_disk, \"net_bytes\": data_net_bytes,\n                                \"net_pkts\": data_net_pkts\n                              }\n    return collectl_info", "response": "Generate all graphs for a bcbio run."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving a ploidy of a region handling special cases.", "response": "def get_ploidy(items, region=None):\n    \"\"\"Retrieve ploidy of a region, handling special cases.\n    \"\"\"\n    chrom = chromosome_special_cases(region[0] if isinstance(region, (list, tuple))\n                                     else None)\n    ploidy = _configured_ploidy(items)\n    sexes = _configured_genders(items)\n    if chrom == \"mitochondrial\":\n        # For now, do haploid calling. Could also do pooled calling\n        # but not entirely clear what the best default would be.\n        return ploidy.get(\"mitochondrial\", 1)\n    elif chrom == \"X\":\n        # Do standard diploid calling if we have any females or unspecified.\n        if \"female\" in sexes or \"f\" in sexes:\n            return ploidy.get(\"female\", ploidy[\"default\"])\n        elif \"male\" in sexes or \"m\" in sexes:\n            return ploidy.get(\"male\", 1)\n        else:\n            return ploidy.get(\"female\", ploidy[\"default\"])\n    elif chrom == \"Y\":\n        # Always call Y single. If female, filter_vcf_by_sex removes Y regions.\n        return 1\n    else:\n        return ploidy[\"default\"]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfilters a single sample VCF by sex chromosomes.", "response": "def filter_vcf_by_sex(vcf_file, items):\n    \"\"\"Post-filter a single sample VCF, handling sex chromosomes.\n\n    Removes Y chromosomes from batches with all female samples.\n    \"\"\"\n    out_file = \"%s-ploidyfix%s\" % utils.splitext_plus(vcf_file)\n    if not utils.file_exists(out_file):\n        genders = list(_configured_genders(items))\n        is_female = len(genders) == 1 and genders[0] and genders[0] in [\"female\", \"f\"]\n        if is_female:\n            orig_out_file = out_file\n            out_file = orig_out_file.replace(\".vcf.gz\", \".vcf\")\n            with file_transaction(items[0], out_file) as tx_out_file:\n                with open(tx_out_file, \"w\") as out_handle:\n                    with utils.open_gzipsafe(vcf_file) as in_handle:\n                        for line in in_handle:\n                            if line.startswith(\"#\"):\n                                out_handle.write(line)\n                            else:\n                                chrom = chromosome_special_cases(line.split(\"\\t\"))\n                                if chrom != \"Y\":\n                                    out_handle.write(line)\n            if orig_out_file.endswith(\".gz\"):\n                out_file = vcfutils.bgzip_and_index(out_file, items[0][\"config\"])\n        else:\n            out_file = vcf_file\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef variant_filtration(call_file, ref_file, vrn_files, data, items):\n    caller = data[\"config\"][\"algorithm\"].get(\"variantcaller\")\n    if \"gvcf\" not in dd.get_tools_on(data):\n        call_file = ploidy.filter_vcf_by_sex(call_file, items)\n    if caller in [\"freebayes\"]:\n        return vfilter.freebayes(call_file, ref_file, vrn_files, data)\n    elif caller in [\"platypus\"]:\n        return vfilter.platypus(call_file, data)\n    elif caller in [\"samtools\"]:\n        return vfilter.samtools(call_file, data)\n    elif caller in [\"gatk\", \"gatk-haplotype\", \"haplotyper\"]:\n        if dd.get_analysis(data).lower().find(\"rna-seq\") >= 0:\n            from bcbio.rnaseq import variation as rnaseq_variation\n            return rnaseq_variation.gatk_filter_rnaseq(call_file, data)\n        else:\n            return gatkfilter.run(call_file, ref_file, vrn_files, data)\n    # no additional filtration for callers that filter as part of call process\n    else:\n        return call_file", "response": "Filter variant calls using variant quality score recalibration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncombining multiple variant calls into a single data item with multiple calls.", "response": "def combine_multiple_callers(samples):\n    \"\"\"Collapse together variant calls from multiple approaches into single data item with `variants`.\n    \"\"\"\n    by_bam = collections.OrderedDict()\n    for data in (x[0] for x in samples):\n        work_bam = tz.get_in((\"combine\", \"work_bam\", \"out\"), data, data.get(\"align_bam\"))\n        # For pre-computed VCF inputs, we don't have BAM files\n        if not work_bam:\n            work_bam = dd.get_sample_name(data)\n        jointcaller = tz.get_in((\"config\", \"algorithm\", \"jointcaller\"), data)\n        variantcaller = get_variantcaller(data)\n        key = (multi.get_batch_for_key(data), work_bam)\n        if key not in by_bam:\n            by_bam[key] = []\n        by_bam[key].append((variantcaller, jointcaller, data))\n    out = []\n    for callgroup in by_bam.values():\n        ready_calls = []\n        for variantcaller, jointcaller, data in callgroup:\n            if variantcaller:\n                cur = data.get(\"vrn_file_plus\", {})\n                cur.update({\"variantcaller\": variantcaller,\n                            \"vrn_file\": data.get(\"vrn_file_orig\") if jointcaller else data.get(\"vrn_file\"),\n                            \"vrn_file_batch\": data.get(\"vrn_file_batch\") if not jointcaller else None,\n                            \"vrn_stats\": data.get(\"vrn_stats\"),\n                            \"validate\": data.get(\"validate\") if not jointcaller else None})\n                if jointcaller:\n                    cur[\"population\"] = False\n                ready_calls.append(cur)\n            if jointcaller:\n                cur = {\"variantcaller\": jointcaller,\n                       \"vrn_file\": data.get(\"vrn_file\"),\n                       \"vrn_file_batch\": data.get(\"vrn_file_batch\"),\n                       \"validate\": data.get(\"validate\"),\n                       \"do_upload\": False}\n                if not variantcaller:\n                    cur[\"population\"] = {\"vcf\": data.get(\"vrn_file\")}\n                ready_calls.append(cur)\n            if not jointcaller and not variantcaller:\n                ready_calls.append({\"variantcaller\": \"precalled\",\n                                    \"vrn_file\": data.get(\"vrn_file\"),\n                                    \"validate\": data.get(\"validate\"),\n                                    \"do_upload\": False})\n        final = callgroup[0][-1]\n        def orig_variantcaller_order(x):\n            try:\n                return final[\"config\"][\"algorithm\"][\"orig_variantcaller\"].index(x[\"variantcaller\"])\n            except ValueError:\n                return final[\"config\"][\"algorithm\"][\"orig_jointcaller\"].index(x[\"variantcaller\"])\n        if len(ready_calls) > 1 and \"orig_variantcaller\" in final[\"config\"][\"algorithm\"]:\n            final[\"variants\"] = sorted(ready_calls, key=orig_variantcaller_order)\n            final[\"config\"][\"algorithm\"][\"variantcaller\"] = final[\"config\"][\"algorithm\"].pop(\"orig_variantcaller\")\n            if \"orig_jointcaller\" in final[\"config\"][\"algorithm\"]:\n                final[\"config\"][\"algorithm\"][\"jointcaller\"] = final[\"config\"][\"algorithm\"].pop(\"orig_jointcaller\")\n        else:\n            final[\"variants\"] = ready_calls\n        final.pop(\"vrn_file_batch\", None)\n        final.pop(\"vrn_file_orig\", None)\n        final.pop(\"vrn_file_plus\", None)\n        final.pop(\"vrn_stats\", None)\n        out.append([final])\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _split_by_ready_regions(ext, file_key, dir_ext_fn):\n    def _sort_by_size(region_w_bams):\n        region, _ = region_w_bams\n        _, start, end = region\n        return end - start\n    def _assign_bams_to_regions(data):\n        \"\"\"Ensure BAMs aligned with input regions, either global or individual.\n        \"\"\"\n        for i, region in enumerate(data[\"region\"]):\n            work_bams = []\n            for xs in data[\"region_bams\"]:\n                if len(xs) == 1:\n                    work_bams.append(xs[0])\n                else:\n                    work_bams.append(xs[i])\n            for work_bam in work_bams:\n                assert os.path.exists(work_bam), work_bam\n            yield region, work_bams\n    def _do_work(data):\n        if \"region\" in data:\n            name = data[\"group\"][0] if \"group\" in data else data[\"description\"]\n            out_dir = os.path.join(data[\"dirs\"][\"work\"], dir_ext_fn(data))\n            out_file = os.path.join(out_dir, \"%s%s\" % (name, ext))\n            assert isinstance(data[\"region\"], (list, tuple))\n            out_parts = []\n            for r, work_bams in sorted(_assign_bams_to_regions(data), key=_sort_by_size, reverse=True):\n                out_region_dir = os.path.join(out_dir, r[0])\n                out_region_file = os.path.join(out_region_dir,\n                                               \"%s-%s%s\" % (name, pregion.to_safestr(r), ext))\n                out_parts.append((r, work_bams, out_region_file))\n            return out_file, out_parts\n        else:\n            return None, []\n    return _do_work", "response": "Split by regions generated by parallel_prep_region."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _collapse_by_bam_variantcaller(samples):\n    by_bam = collections.OrderedDict()\n    for data in (x[0] for x in samples):\n        work_bam = utils.get_in(data, (\"combine\", \"work_bam\", \"out\"), data.get(\"align_bam\"))\n        variantcaller = get_variantcaller(data)\n        if isinstance(work_bam, list):\n            work_bam = tuple(work_bam)\n        key = (multi.get_batch_for_key(data), work_bam, variantcaller)\n        try:\n            by_bam[key].append(data)\n        except KeyError:\n            by_bam[key] = [data]\n    out = []\n    for grouped_data in by_bam.values():\n        cur = grouped_data[0]\n        cur.pop(\"region\", None)\n        region_bams = cur.pop(\"region_bams\", None)\n        if region_bams and len(region_bams[0]) > 1:\n            cur.pop(\"work_bam\", None)\n        out.append([cur])\n    return out", "response": "Collapse regions to a single representative by BAM input variant caller and batch."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprepare samples by variant callers duplicating any with multiple callers.", "response": "def _dup_samples_by_variantcaller(samples, require_bam=True):\n    \"\"\"Prepare samples by variant callers, duplicating any with multiple callers.\n    \"\"\"\n    samples = [utils.to_single_data(x) for x in samples]\n    samples = germline.split_somatic(samples)\n    to_process = []\n    extras = []\n    for data in samples:\n        added = False\n        for i, add in enumerate(handle_multiple_callers(data, \"variantcaller\", require_bam=require_bam)):\n            added = True\n            add = dd.set_variantcaller_order(add, i)\n            to_process.append([add])\n        if not added:\n            data = _handle_precalled(data)\n            data = dd.set_variantcaller_order(data, 0)\n            extras.append([data])\n    return to_process, extras"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nperform variant calling on samples by region.", "response": "def parallel_variantcall_region(samples, run_parallel):\n    \"\"\"Perform variant calling and post-analysis on samples by region.\n    \"\"\"\n    to_process, extras = _dup_samples_by_variantcaller(samples)\n    split_fn = _split_by_ready_regions(\".vcf.gz\", \"work_bam\", get_variantcaller)\n    samples = _collapse_by_bam_variantcaller(\n        grouped_parallel_split_combine(to_process, split_fn,\n                                       multi.group_batches, run_parallel,\n                                       \"variantcall_sample\", \"concat_variant_files\",\n                                       \"vrn_file\", [\"region\", \"sam_ref\", \"config\"]))\n    return extras + samples"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npreparing output record from variant calling to feed into downstream analysis.", "response": "def vc_output_record(samples):\n    \"\"\"Prepare output record from variant calling to feed into downstream analysis.\n\n    Prep work handles reformatting so we return generated dictionaries.\n\n    For any shared keys that are calculated only once for a batch, like variant calls\n    for the batch, we assign to every sample.\n    \"\"\"\n    shared_keys = [[\"vrn_file\"], [\"validate\", \"summary\"],\n                   [\"validate\", \"tp\"], [\"validate\", \"fp\"], [\"validate\", \"fn\"]]\n    raw = cwlutils.samples_to_records([utils.to_single_data(x) for x in samples])\n    shared = {}\n    for key in shared_keys:\n        cur = list(set([x for x in [tz.get_in(key, d) for d in raw] if x]))\n        if len(cur) > 0:\n            assert len(cur) == 1, (key, cur)\n            shared[tuple(key)] = cur[0]\n        else:\n            shared[tuple(key)] = None\n    out = []\n    for d in raw:\n        for key, val in shared.items():\n            d = tz.update_in(d, key, lambda x: val)\n        out.append([d])\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprepares a set of samples for parallel variant calling.", "response": "def batch_for_variantcall(samples):\n    \"\"\"Prepare a set of samples for parallel variant calling.\n\n    CWL input target that groups samples into batches and variant callers\n    for parallel processing.\n\n    If doing joint calling, with `tools_on: [gvcf]`, split the sample into\n    individuals instead of combining into a batch.\n    \"\"\"\n    sample_order = [dd.get_sample_name(utils.to_single_data(x)) for x in samples]\n    to_process, extras = _dup_samples_by_variantcaller(samples, require_bam=False)\n    batch_groups = collections.defaultdict(list)\n    to_process = [utils.to_single_data(x) for x in to_process]\n    for data in cwlutils.samples_to_records(to_process):\n        vc = get_variantcaller(data, require_bam=False)\n        batches = dd.get_batches(data) or dd.get_sample_name(data)\n        if not isinstance(batches, (list, tuple)):\n            batches = [batches]\n        for b in batches:\n            batch_groups[(b, vc)].append(utils.deepish_copy(data))\n    batches = []\n    for cur_group in batch_groups.values():\n        joint_calling = any([is_joint(d) for d in cur_group])\n        if joint_calling:\n            for d in cur_group:\n                batches.append([d])\n        else:\n            batches.append(cur_group)\n    def by_original_order(xs):\n        return (min([sample_order.index(dd.get_sample_name(x)) for x in xs]),\n                min([dd.get_variantcaller_order(x) for x in xs]))\n    return sorted(batches + extras, key=by_original_order)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncopy in external pre - called variants fed into analysis.", "response": "def _handle_precalled(data):\n    \"\"\"Copy in external pre-called variants fed into analysis.\n\n    Symlinks for non-CWL runs where we want to ensure VCF present\n    in a local directory.\n    \"\"\"\n    if data.get(\"vrn_file\") and not cwlutils.is_cwl_run(data):\n        vrn_file = data[\"vrn_file\"]\n        if isinstance(vrn_file, (list, tuple)):\n            assert len(vrn_file) == 1\n            vrn_file = vrn_file[0]\n        precalled_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"precalled\"))\n        ext = utils.splitext_plus(vrn_file)[-1]\n        orig_file = os.path.abspath(vrn_file)\n        our_vrn_file = os.path.join(precalled_dir, \"%s-precalled%s\" % (dd.get_sample_name(data), ext))\n        utils.copy_plus(orig_file, our_vrn_file)\n        data[\"vrn_file\"] = our_vrn_file\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle_multiple_callers(data, key, default=None, require_bam=True):\n    callers = get_variantcaller(data, key, default, require_bam=require_bam)\n    if isinstance(callers, six.string_types):\n        return [data]\n    elif not callers:\n        return []\n    else:\n        out = []\n        for caller in callers:\n            base = copy.deepcopy(data)\n            if not base[\"config\"][\"algorithm\"].get(\"orig_%s\" % key):\n                base[\"config\"][\"algorithm\"][\"orig_%s\" % key] = \\\n                  base[\"config\"][\"algorithm\"][key]\n            base[\"config\"][\"algorithm\"][key] = caller\n            # if splitting by variant caller, also split by jointcaller\n            if key == \"variantcaller\":\n                jcallers = get_variantcaller(data, \"jointcaller\", [])\n                if isinstance(jcallers, six.string_types):\n                    jcallers = [jcallers]\n                if jcallers:\n                    base[\"config\"][\"algorithm\"][\"orig_jointcaller\"] = jcallers\n                    jcallers = [x for x in jcallers if x.startswith(caller)]\n                    if jcallers:\n                        base[\"config\"][\"algorithm\"][\"jointcaller\"] = jcallers[0]\n                    else:\n                        base[\"config\"][\"algorithm\"][\"jointcaller\"] = False\n            out.append(base)\n        return out", "response": "Split samples that potentially require multiple variant calling approaches."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming variant calling of a sample.", "response": "def variantcall_sample(data, region=None, align_bams=None, out_file=None):\n    \"\"\"Parallel entry point for doing genotyping of a region of a sample.\n    \"\"\"\n    if out_file is None or not os.path.exists(out_file) or not os.path.lexists(out_file):\n        utils.safe_makedir(os.path.dirname(out_file))\n        ref_file = dd.get_ref_file(data)\n        config = data[\"config\"]\n        caller_fns = get_variantcallers()\n        caller_fn = caller_fns[config[\"algorithm\"].get(\"variantcaller\")]\n        if len(align_bams) == 1:\n            items = [data]\n        else:\n            items = multi.get_orig_items(data)\n            assert len(items) == len(align_bams)\n        assoc_files = tz.get_in((\"genome_resources\", \"variation\"), data, {})\n        if not assoc_files: assoc_files = {}\n        for bam_file in align_bams:\n            bam.index(bam_file, data[\"config\"], check_timestamp=False)\n        out_file = caller_fn(align_bams, items, ref_file, assoc_files, region, out_file)\n    if region:\n        data[\"region\"] = region\n    data[\"vrn_file\"] = out_file\n    return [data]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef concat_batch_variantcalls(items, region_block=True, skip_jointcheck=False):\n    items = [utils.to_single_data(x) for x in items]\n    batch_name = _get_batch_name(items, skip_jointcheck)\n    variantcaller = _get_batch_variantcaller(items)\n    # Pre-called input variant files\n    if not variantcaller and all(d.get(\"vrn_file\") for d in items):\n        return {\"vrn_file\": items[0][\"vrn_file\"]}\n    out_file = os.path.join(dd.get_work_dir(items[0]), variantcaller, \"%s.vcf.gz\" % (batch_name))\n    utils.safe_makedir(os.path.dirname(out_file))\n    if region_block:\n        regions = [_region_to_coords(rs[0]) for rs in items[0][\"region_block\"]]\n    else:\n        regions = [_region_to_coords(r) for r in items[0][\"region\"]]\n    vrn_file_regions = items[0][\"vrn_file_region\"]\n    out_file = vcfutils.concat_variant_files(vrn_file_regions, out_file, regions,\n                                             dd.get_ref_file(items[0]), items[0][\"config\"])\n    return {\"vrn_file\": out_file}", "response": "Combine multiple variant calls into a single VCF file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsplits GATK region specification into chrom start and end tuples", "response": "def _region_to_coords(region):\n    \"\"\"Split GATK region specification (chr1:1-10) into a tuple of chrom, start, end\n    \"\"\"\n    chrom, coords = region.split(\":\")\n    start, end = coords.split(\"-\")\n    return (chrom, int(start), int(end))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_batch_name(items, skip_jointcheck=False):\n    batch_names = collections.defaultdict(int)\n    has_joint = any([is_joint(d) for d in items])\n    for data in items:\n        if has_joint and not skip_jointcheck:\n            batches = dd.get_sample_name(data)\n        else:\n            batches = dd.get_batches(data) or dd.get_sample_name(data)\n        if not isinstance(batches, (list, tuple)):\n            batches = [batches]\n        for b in batches:\n            batch_names[b] += 1\n    return sorted(batch_names.items(), key=lambda x: x[-1], reverse=True)[0][0]", "response": "Retrieve the shared batch name for a group of items."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning variant calling on a batch of items using multiple cores.", "response": "def _run_variantcall_batch_multicore(items, regions, final_file):\n    \"\"\"Run variant calling on a batch of items using multiple cores.\n    \"\"\"\n    batch_name = _get_batch_name(items)\n    variantcaller = _get_batch_variantcaller(items)\n    work_bams = [dd.get_work_bam(d) or dd.get_align_bam(d) for d in items]\n    def split_fn(data):\n        out = []\n        for region in regions:\n            region = _region_to_coords(region)\n            chrom, start, end = region\n            region_str = \"_\".join(str(x) for x in region)\n            out_file = os.path.join(dd.get_work_dir(items[0]), variantcaller, chrom,\n                                    \"%s-%s.vcf.gz\" % (batch_name, region_str))\n            out.append((region, work_bams, out_file))\n        return final_file, out\n    parallel = {\"type\": \"local\", \"num_jobs\": dd.get_num_cores(items[0]), \"cores_per_job\": 1}\n    run_parallel = dmulti.runner(parallel, items[0][\"config\"])\n    to_run = copy.deepcopy(items[0])\n    to_run[\"sam_ref\"] = dd.get_ref_file(to_run)\n    to_run[\"group_orig\"] = items\n    parallel_split_combine([[to_run]], split_fn, run_parallel,\n                           \"variantcall_sample\", \"concat_variant_files\",\n                           \"vrn_file\", [\"region\", \"sam_ref\", \"config\"])\n    return final_file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a cluster based on the provided parallel arguments.", "response": "def create(parallel, dirs, config):\n    \"\"\"Create a cluster based on the provided parallel arguments.\n\n    Returns an IPython view on the cluster, enabling processing on jobs.\n\n    Adds a mincores specification if he have machines with a larger\n    number of cores to allow jobs to be batched together for shared\n    memory usage.\n    \"\"\"\n    profile_dir = utils.safe_makedir(os.path.join(dirs[\"work\"], get_log_dir(config), \"ipython\"))\n    has_mincores = any(x.startswith(\"mincores=\") for x in parallel[\"resources\"])\n    cores = min(_get_common_cores(config[\"resources\"]), parallel[\"system_cores\"])\n    if cores > 1 and not has_mincores:\n        adj_cores = max(1, int(math.floor(cores * float(parallel.get(\"mem_pct\", 1.0)))))\n        # if we have less scheduled cores than per machine, use the scheduled count\n        if cores > parallel[\"cores\"]:\n            cores = parallel[\"cores\"]\n        # if we have less total cores required for the entire process, use that\n        elif adj_cores > parallel[\"num_jobs\"] * parallel[\"cores_per_job\"]:\n            cores = parallel[\"num_jobs\"] * parallel[\"cores_per_job\"]\n        else:\n            cores = adj_cores\n            cores = per_machine_target_cores(cores, parallel[\"num_jobs\"])\n        parallel[\"resources\"].append(\"mincores=%s\" % cores)\n    return ipython_cluster.cluster_view(parallel[\"scheduler\"].lower(), parallel[\"queue\"],\n                                        parallel[\"num_jobs\"], parallel[\"cores_per_job\"],\n                                        profile=profile_dir, start_wait=parallel[\"timeout\"],\n                                        extra_params={\"resources\": parallel[\"resources\"],\n                                                      \"mem\": parallel[\"mem\"],\n                                                      \"tag\": parallel.get(\"tag\"),\n                                                      \"run_local\": parallel.get(\"run_local\"),\n                                                      \"local_controller\": parallel.get(\"local_controller\")},\n                                        retries=parallel.get(\"retries\"))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef per_machine_target_cores(cores, num_jobs):\n    if cores >= 32 and num_jobs == 1:\n        cores = cores - 2\n    elif cores >= 16 and num_jobs in [1, 2]:\n        cores = cores - 1\n    return cores", "response": "Select target cores on larger machines to leave room for batch script and controller."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_common_cores(resources):\n    all_cores = []\n    for vs in resources.values():\n        cores = vs.get(\"cores\")\n        if cores:\n            all_cores.append(int(vs[\"cores\"]))\n    return collections.Counter(all_cores).most_common(1)[0][0]", "response": "Retrieve the most common number of cores in the input file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncompressing arguments using msgpack.", "response": "def zip_args(args, config=None):\n    \"\"\"Compress arguments using msgpack.\n    \"\"\"\n    if msgpack:\n        return [msgpack.packb(x, use_single_float=True, use_bin_type=True) for x in args]\n    else:\n        return args"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef runner(view, parallel, dirs, config):\n    def run(fn_name, items):\n        setpath.prepend_bcbiopath()\n        out = []\n        fn, fn_name = (fn_name, fn_name.__name__) if callable(fn_name) else (_get_ipython_fn(fn_name, parallel), fn_name)\n        items = [x for x in items if x is not None]\n        items = diagnostics.track_parallel(items, fn_name)\n        logger.info(\"ipython: %s\" % fn_name)\n        if len(items) > 0:\n            items = [config_utils.add_cores_to_config(x, parallel[\"cores_per_job\"], parallel) for x in items]\n            if \"wrapper\" in parallel:\n                wrap_parallel = {k: v for k, v in parallel.items() if k in set([\"fresources\"])}\n                items = [[fn_name] + parallel.get(\"wrapper_args\", []) + [wrap_parallel] + list(x) for x in items]\n            items = zip_args([args for args in items])\n            for data in view.map_sync(fn, items, track=False):\n                if data:\n                    out.extend(unzip_args(data))\n        return out\n    return run", "response": "A function that runs a task on an ipython parallel cluster allowing alternative queue types."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprepares the data for peak calling", "response": "def peakcall_prepare(data, run_parallel):\n    \"\"\"Entry point for doing peak calling\"\"\"\n    caller_fns = get_callers()\n    to_process = []\n    for sample in data:\n        mimic = copy.copy(sample[0])\n        callers = dd.get_peakcaller(sample[0])\n        if not isinstance(callers, list):\n            callers = [callers]\n        for caller in callers:\n            if caller in caller_fns:\n                mimic[\"peak_fn\"] = caller\n                name = dd.get_sample_name(mimic)\n                mimic = _check(mimic, data)\n                if mimic:\n                    to_process.append(mimic)\n                else:\n                    logger.info(\"Skipping peak calling. No input sample for %s\" % name)\n    if to_process:\n        after_process = run_parallel(\"peakcalling\", to_process)\n        data = _sync(data, after_process)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _sync(original, processed):\n    for original_sample in original:\n        original_sample[0][\"peaks_files\"] = {}\n        for process_sample in processed:\n            if dd.get_sample_name(original_sample[0]) == dd.get_sample_name(process_sample[0]):\n                for key in [\"peaks_files\"]:\n                    if process_sample[0].get(key):\n                        original_sample[0][key] = process_sample[0][key]\n    return original", "response": "Synchronize the original data with the processed data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _check(sample, data):\n    if dd.get_chip_method(sample).lower() == \"atac\":\n        return [sample]\n    if dd.get_phenotype(sample) == \"input\":\n        return None\n    for origin in data:\n        if dd.get_batch(sample) in (dd.get_batches(origin[0]) or []) and dd.get_phenotype(origin[0]) == \"input\":\n            sample[\"work_bam_input\"] = origin[0].get(\"work_bam\")\n            return [sample]\n    return [sample]", "response": "Check if input sample for each chip bam file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting multiplier to get jobs only for samples that have input", "response": "def _get_multiplier(samples):\n    \"\"\"Get multiplier to get jobs\n       only for samples that have input\n    \"\"\"\n    to_process = 1.0\n    to_skip = 0\n    for sample in samples:\n        if dd.get_phenotype(sample[0]) == \"chip\":\n            to_process += 1.0\n        elif dd.get_chip_method(sample[0]).lower() == \"atac\":\n            to_process += 1.0\n        else:\n            to_skip += 1.0\n    mult = (to_process - to_skip) / len(samples)\n    if mult <= 0:\n        mult = 1 / len(samples)\n    return max(mult, 1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun ChIP - seq greylisting on the input BAM file.", "response": "def greylisting(data):\n    \"\"\"\n    Run ChIP-seq greylisting\n    \"\"\"\n    input_bam = data.get(\"work_bam_input\", None)\n    if not input_bam:\n        logger.info(\"No input BAM file detected, skipping greylisting.\")\n        return None\n    try:\n        greylister = config_utils.get_program(\"chipseq-greylist\", data)\n    except config_utils.CmdNotFound:\n        logger.info(\"No greylister found, skipping greylisting.\")\n        return None\n    greylistdir = os.path.join(os.path.dirname(input_bam), \"greylist\")\n    if os.path.exists(greylistdir):\n        return greylistdir\n    cmd = \"{greylister} --outdir {txgreylistdir} {input_bam}\"\n    message = \"Running greylisting on %s.\" % input_bam\n    with file_transaction(greylistdir) as txgreylistdir:\n        utils.safe_makedir(txgreylistdir)\n        try:\n            do.run(cmd.format(**locals()), message)\n        except subprocess.CalledProcessError as msg:\n            if str(msg).find(\"Cannot take a larger sample than population when 'replace=False'\") >= 0:\n                logger.info(\"Skipping chipseq greylisting because of small sample size: %s\"\n                            % dd.get_sample_name(data))\n                return None\n    return greylistdir"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_parallel(args, module=\"bcbio.distributed\"):\n    ptype, cores = _get_cores_and_type(args.numcores,\n                                       getattr(args, \"paralleltype\", None),\n                                       args.scheduler)\n    local_controller = getattr(args, \"local_controller\", False)\n    parallel = {\"type\": ptype, \"cores\": cores,\n                \"scheduler\": args.scheduler, \"queue\": args.queue,\n                \"tag\": args.tag, \"module\": module,\n                \"resources\": args.resources, \"timeout\": args.timeout,\n                \"retries\": args.retries,\n                \"run_local\": args.queue == \"localrun\",\n                \"local_controller\": local_controller}\n    return parallel", "response": "Convert input arguments into a dictionary for passing to processing."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_cores_and_type(numcores, paralleltype, scheduler):\n    if scheduler is not None:\n        paralleltype = \"ipython\"\n    if paralleltype is None:\n        paralleltype = \"local\"\n    if not numcores or int(numcores) < 1:\n        numcores = 1\n    return paralleltype, int(numcores)", "response": "Return core and parallelization approach from command line providing sane defaults."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tophat_align(fastq_file, pair_file, ref_file, out_base, align_dir, data,\n                 names=None):\n    \"\"\"\n    run alignment using Tophat v2\n    \"\"\"\n    config = data[\"config\"]\n    options = get_in(config, (\"resources\", \"tophat\", \"options\"), {})\n    options = _set_fusion_mode(options, data)\n    options = _set_quality_flag(options, data)\n    options = _set_transcriptome_option(options, data, ref_file)\n    options = _set_cores(options, config)\n    options = _set_rg_options(options, names)\n    options = _set_stranded_flag(options, config)\n\n    ref_file, runner = _determine_aligner_and_reference(ref_file, data)\n\n    # fusion search does not work properly with Bowtie2\n    if options.get(\"fusion-search\", False):\n        ref_file = ref_file.replace(\"/bowtie2\", \"/bowtie\")\n\n    if _tophat_major_version(config) == 1:\n        raise NotImplementedError(\"Tophat versions < 2.0 are not supported, please \"\n                                  \"download the newest version of Tophat here: \"\n                                  \"http://tophat.cbcb.umd.edu\")\n\n    if _ref_version(ref_file) == 1 or options.get(\"fusion-search\", False):\n        options[\"bowtie1\"] = True\n\n    out_dir = os.path.join(align_dir, \"%s_tophat\" % out_base)\n    final_out = os.path.join(out_dir, \"{0}.bam\".format(names[\"sample\"]))\n    if file_exists(final_out):\n        return final_out\n\n    out_file = os.path.join(out_dir, \"accepted_hits.bam\")\n    unmapped = os.path.join(out_dir, \"unmapped.bam\")\n    files = [ref_file, fastq_file]\n    if not file_exists(out_file):\n        with file_transaction(config, out_dir) as tx_out_dir:\n            safe_makedir(tx_out_dir)\n            if pair_file and not options.get(\"mate-inner-dist\", None):\n                d, d_stdev = _estimate_paired_innerdist(fastq_file, pair_file,\n                                                        ref_file, out_base,\n                                                        tx_out_dir, data)\n                options[\"mate-inner-dist\"] = d\n                options[\"mate-std-dev\"] = d_stdev\n                files.append(pair_file)\n            options[\"output-dir\"] = tx_out_dir\n            options[\"no-coverage-search\"] = True\n            options[\"no-mixed\"] = True\n            cmd = [utils.get_program_python(\"tophat\"), config_utils.get_program(\"tophat\", config)]\n            for k, v in options.items():\n                if v is True:\n                    cmd.append(\"--%s\" % k)\n                else:\n                    assert not isinstance(v, bool)\n                    cmd.append(\"--%s=%s\" % (k, v))\n            # tophat requires options before arguments, otherwise it silently ignores them\n            cmd += files\n            do.run(cmd, \"Running Tophat on %s and %s.\" % (fastq_file, pair_file))\n    if pair_file and _has_alignments(out_file):\n        fixed = _fix_mates(out_file, os.path.join(out_dir, \"%s-align.bam\" % out_base),\n                           ref_file, config)\n    else:\n        fixed = out_file\n    fixed_unmapped = _fix_unmapped(fixed, unmapped, data)\n    fixed = merge_unmapped(fixed, fixed_unmapped, config)\n    fixed = _add_rg(fixed, config, names)\n    fixed = bam.sort(fixed, config)\n    picard = broad.runner_from_path(\"picard\", config)\n    # set the contig order to match the reference file so GATK works\n    fixed = picard.run_fn(\"picard_reorder\", fixed, data[\"sam_ref\"],\n                          os.path.splitext(fixed)[0] + \".picard.bam\")\n    fixed = fix_insert_size(fixed, config)\n    if not file_exists(final_out):\n        symlink_plus(fixed, final_out)\n    return final_out", "response": "Run alignment using Tophat v2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfixing problematic unmapped mate pairs in TopHat output.", "response": "def _fix_mates(orig_file, out_file, ref_file, config):\n    \"\"\"Fix problematic unmapped mate pairs in TopHat output.\n\n    TopHat 2.0.9 appears to have issues with secondary reads:\n    https://groups.google.com/forum/#!topic/tuxedo-tools-users/puLfDNbN9bo\n    This cleans the input file to only keep properly mapped pairs,\n    providing a general fix that will handle correctly mapped secondary\n    reads as well.\n    \"\"\"\n    if not file_exists(out_file):\n        with file_transaction(config, out_file) as tx_out_file:\n            samtools = config_utils.get_program(\"samtools\", config)\n            cmd = \"{samtools} view -bS -h -t {ref_file}.fai -F 8 {orig_file} > {tx_out_file}\"\n            do.run(cmd.format(**locals()), \"Fix mate pairs in TopHat output\", {})\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds the missing RG header.", "response": "def _add_rg(unmapped_file, config, names):\n    \"\"\"Add the missing RG header.\"\"\"\n    picard = broad.runner_from_path(\"picard\", config)\n    rg_fixed = picard.run_fn(\"picard_fix_rgs\", unmapped_file, names)\n    return rg_fixed"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfixes unmapped reads with TopHat - Recondition.", "response": "def _fix_unmapped(mapped_file, unmapped_file, data):\n    \"\"\"\n    The unmapped.bam file up until at least Tophat 2.1.1 is broken in various\n    ways, see https://github.com/cbrueffer/tophat-recondition for details.\n    Run TopHat-Recondition to fix these issues.\n    \"\"\"\n    out_file = os.path.splitext(unmapped_file)[0] + \"_fixup.bam\"\n    if file_exists(out_file):\n        return out_file\n\n    assert os.path.dirname(mapped_file) == os.path.dirname(unmapped_file)\n\n    cmd = config_utils.get_program(\"tophat-recondition\", data)\n    cmd += \" -q\"\n    tophat_out_dir = os.path.dirname(mapped_file)\n    tophat_logfile = os.path.join(tophat_out_dir, 'tophat-recondition.log')\n\n    with file_transaction(data, tophat_logfile) as tx_logfile:\n        cmd += ' --logfile %s' % tx_logfile\n        cmd += \" -m %s\" % mapped_file\n        cmd += \" -u %s\" % unmapped_file\n        cmd += \" %s\" % tophat_out_dir\n        do.run(cmd, \"Fixing unmapped reads with Tophat-Recondition.\", None)\n\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _estimate_paired_innerdist(fastq_file, pair_file, ref_file, out_base,\n                               out_dir, data):\n    \"\"\"Use Bowtie to estimate the inner distance of paired reads.\n    \"\"\"\n    mean, stdev = _bowtie_for_innerdist(\"100000\", fastq_file, pair_file, ref_file,\n                                        out_base, out_dir, data, True)\n    if not mean or not stdev:\n        mean, stdev = _bowtie_for_innerdist(\"1\", fastq_file, pair_file, ref_file,\n                                            out_base, out_dir, data, True)\n    # No reads aligning so no data to process, set some default values\n    if not mean or not stdev:\n        mean, stdev = 200, 50\n\n    return mean, stdev", "response": "Use Bowtie to estimate the inner distance of paired reads."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _bowtie_major_version(stdout):\n    version_line = stdout.split(\"\\n\")[0]\n    version_string = version_line.strip().split()[2]\n    major_version = int(version_string.split(\".\")[0])\n    # bowtie version 1 has a leading character of 0 or 1\n    if major_version == 0 or major_version == 1:\n        major_version = 1\n    return major_version", "response": "Return the major version of bowtie."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfix the insert size of the Tophat RG in the input BAM file.", "response": "def fix_insert_size(in_bam, config):\n    \"\"\"\n    Tophat sets PI in the RG to be the inner distance size, but the SAM spec\n    states should be the insert size. This fixes the RG in the alignment\n    file generated by Tophat header to match the spec\n    \"\"\"\n    fixed_file = os.path.splitext(in_bam)[0] + \".pi_fixed.bam\"\n    if file_exists(fixed_file):\n        return fixed_file\n    header_file = os.path.splitext(in_bam)[0] + \".header.sam\"\n    read_length = bam.estimate_read_length(in_bam)\n    bam_handle= bam.open_samfile(in_bam)\n    header = bam_handle.header.copy()\n    rg_dict = header['RG'][0]\n    if 'PI' not in rg_dict:\n        return in_bam\n    PI = int(rg_dict.get('PI'))\n    PI = PI + 2*read_length\n    rg_dict['PI'] = PI\n    header['RG'][0] = rg_dict\n    with pysam.Samfile(header_file, \"wb\", header=header) as out_handle:\n        with bam.open_samfile(in_bam) as in_handle:\n            for record in in_handle:\n                out_handle.write(record)\n    shutil.move(header_file, fixed_file)\n    return fixed_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run_filter(vrn_file, align_bam, ref_file, data, items):\n    if not should_filter(items) or not vcfutils.vcf_has_variants(vrn_file):\n        return data\n    else:\n        raw_file = \"%s-damage.vcf\" % utils.splitext_plus(vrn_file)[0]\n        out_plot_files = [\"%s%s\" % (utils.splitext_plus(raw_file)[0], ext)\n                          for ext in [\"_seq_bias_simplified.pdf\", \"_pcr_bias_simplified.pdf\"]]\n        if not utils.file_uptodate(raw_file, vrn_file) and not utils.file_uptodate(raw_file + \".gz\", vrn_file):\n            with file_transaction(items[0], raw_file) as tx_out_file:\n                # Does not apply --qcSummary plotting due to slow runtimes\n                cmd = [\"dkfzbiasfilter.py\", \"--filterCycles\", \"1\", \"--passOnly\",\n                       \"--tempFolder\", os.path.dirname(tx_out_file),\n                       vrn_file, align_bam, ref_file, tx_out_file]\n                do.run(cmd, \"Filter low frequency variants for DNA damage and strand bias\")\n                for out_plot in out_plot_files:\n                    tx_plot_file = os.path.join(\"%s_qcSummary\" % utils.splitext_plus(tx_out_file)[0], \"plots\",\n                                                os.path.basename(out_plot))\n                    if utils.file_exists(tx_plot_file):\n                        shutil.move(tx_plot_file, out_plot)\n        raw_file = vcfutils.bgzip_and_index(raw_file, items[0][\"config\"])\n        data[\"vrn_file\"] = _filter_to_info(raw_file, items[0])\n        out_plot_files = [x for x in out_plot_files if utils.file_exists(x)]\n        data[\"damage_plots\"] = out_plot_files\n        return data", "response": "Filter and annotate somatic VCFs with damage and bias artifacts on low frequency variants."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmoving DKFZ filter information into INFO field.", "response": "def _filter_to_info(in_file, data):\n    \"\"\"Move DKFZ filter information into INFO field.\n    \"\"\"\n    header = (\"\"\"##INFO=<ID=DKFZBias,Number=.,Type=String,\"\"\"\n              \"\"\"Description=\"Bias estimation based on unequal read support from DKFZBiasFilterVariant Depth\">\\n\"\"\")\n    out_file = \"%s-ann.vcf\" % utils.splitext_plus(in_file)[0]\n    if not utils.file_uptodate(out_file, in_file) and not utils.file_uptodate(out_file + \".gz\", in_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            with utils.open_gzipsafe(in_file) as in_handle:\n                with open(tx_out_file, \"w\") as out_handle:\n                    for line in in_handle:\n                        if line.startswith(\"#CHROM\"):\n                            out_handle.write(header + line)\n                        elif line.startswith(\"#\"):\n                            out_handle.write(line)\n                        else:\n                            out_handle.write(_rec_filter_to_info(line))\n    return vcfutils.bgzip_and_index(out_file, data[\"config\"])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmoving a DKFZBias filter to the INFO field for a record.", "response": "def _rec_filter_to_info(line):\n    \"\"\"Move a DKFZBias filter to the INFO field, for a record.\n    \"\"\"\n    parts = line.rstrip().split(\"\\t\")\n    move_filters = {\"bSeq\": \"strand\", \"bPcr\": \"damage\"}\n    new_filters = []\n    bias_info = []\n    for f in parts[6].split(\";\"):\n        if f in move_filters:\n            bias_info.append(move_filters[f])\n        elif f not in [\".\"]:\n            new_filters.append(f)\n    if bias_info:\n        parts[7] += \";DKFZBias=%s\" % \",\".join(bias_info)\n    parts[6] = \";\".join(new_filters or [\"PASS\"])\n    return \"\\t\".join(parts) + \"\\n\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef should_filter(items):\n    return (vcfutils.get_paired(items) is not None and\n            any(\"damage_filter\" in dd.get_tools_on(d) for d in items))", "response": "Check if we should do damage filtering on somatic calling with low frequency events."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretaining details about starting a command returning a command identifier.", "response": "def start_cmd(cmd, descr, data):\n    \"\"\"Retain details about starting a command, returning a command identifier.\n    \"\"\"\n    if data and \"provenance\" in data:\n        entity_id = tz.get_in([\"provenance\", \"entity\"], data)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef initialize(dirs):\n    if biolite and dirs.get(\"work\"):\n        base_dir = utils.safe_makedir(os.path.join(dirs[\"work\"], \"provenance\"))\n        p_db = os.path.join(base_dir, \"biolite.db\")\n        biolite.config.resources[\"database\"] = p_db\n        biolite.database.connect()", "response": "Initialize the biolite database to load provenance information."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates entity identifiers to trace the given items in parallel execution.", "response": "def track_parallel(items, sub_type):\n    \"\"\"Create entity identifiers to trace the given items in sub-commands.\n\n    Helps handle nesting in parallel program execution:\n\n    run id => sub-section id => parallel ids\n    \"\"\"\n    out = []\n    for i, args in enumerate(items):\n        item_i, item = _get_provitem_from_args(args)\n        if item:\n            sub_entity = \"%s.%s.%s\" % (item[\"provenance\"][\"entity\"], sub_type, i)\n            item[\"provenance\"][\"entity\"] = sub_entity\n            args = list(args)\n            args[item_i] = item\n        out.append(args)\n    # TODO: store mapping of entity to sub identifiers\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_provitem_from_args(xs):\n    for i, x in enumerate(xs):\n        if _has_provenance(x):\n            return i, x\n    return -1, None", "response": "Retrieve processed item from list of input arguments."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing pindel options. Add region to cmd. catalyck.", "response": "def _pindel_options(items, config, out_file, region, tmp_path):\n    \"\"\"parse pindel options. Add region to cmd.\n    :param items: (dict) information from yaml\n    :param config: (dict) information from yaml (items[0]['config'])\n    :param region: (str or tupple) region to analyze\n    :param tmp_path: (str) temporal folder\n    :returns: (list) options for pindel\n    \"\"\"\n    variant_regions = utils.get_in(config, (\"algorithm\", \"variant_regions\"))\n    target = subset_variant_regions(variant_regions, region, out_file, items)\n    opts = \"\"\n    if target:\n        if isinstance(target, six.string_types) and os.path.isfile(target):\n            target_bed = target\n        else:\n            target_bed = os.path.join(tmp_path, \"tmp.bed\")\n            with file_transaction(config, target_bed) as tx_tmp_bed:\n                if not isinstance(region, (list, tuple)):\n                    message = (\"Region must be a tuple - something odd just happened\")\n                    raise ValueError(message)\n                chrom, start, end = region\n                with open(tx_tmp_bed, \"w\") as out_handle:\n                    print(\"%s\\t%s\\t%s\" % (chrom, start, end), file=out_handle)\n        opts = \"-j \" + remove_lcr_regions(target_bed, items)\n    return opts"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if pindel installation on machine.", "response": "def is_installed(config):\n    \"\"\"Check for pindel installation on machine.\n    :param config: (dict) information from yaml(items[0]['config'])\n    :returns: (boolean) if pindel is installed\n    \"\"\"\n    try:\n        config_utils.get_program(\"pindel2vcf\", config)\n        config_utils.get_program(\"pindel\", config)\n        return True\n    except config_utils.CmdNotFound:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetecting indels with pindel in tumor.", "response": "def _run_tumor_pindel_caller(align_bams, items, ref_file, assoc_files,\n                             region=None, out_file=None):\n    \"\"\"Detect indels with pindel in tumor/[normal] analysis.\n    Only attempts to detect small insertion/deletions and not larger structural events.\n    :param align_bam: (list) bam files\n    :param items: (dict) information from yaml\n    :param ref_file: (str) genome in fasta format\n    :param assoc_file: (dict) files for annotation\n    :param region: (str or tupple) region to analyze\n    :param out_file: (str) final vcf file\n    :returns: (str) final vcf file\n    \"\"\"\n    config = items[0][\"config\"]\n    paired = get_paired_bams(align_bams, items)\n    if out_file is None:\n        out_file = \"%s-indels.vcf\" % os.path.splitext(align_bams[0])[0]\n    paired_bam = [paired.tumor_bam]\n    paired_name = [paired.tumor_name]\n    if paired.normal_bam:\n        paired_bam.append(paired.normal_bam)\n        paired_name.append(paired.normal_name)\n    if not utils.file_exists(out_file):\n        with tx_tmpdir(config) as tmp_path:\n            for align_bam in align_bams:\n                bam.index(align_bam, config)\n            root_pindel = os.path.join(tmp_path, \"pindelroot\")\n            pindel = config_utils.get_program(\"pindel\", config)\n            opts = _pindel_options(items, config, out_file, region, tmp_path)\n            tmp_input = _create_tmp_input(paired_bam, paired_name, tmp_path, config)\n            cmd = (\"{pindel} -f {ref_file} -i {tmp_input} -o {root_pindel} \" +\n                   \"{opts} --max_range_index 2 --IndelCorrection \"\n                   \"--report_breakpoints false --report_interchromosomal_events false\")\n            do.run(cmd.format(**locals()), \"Genotyping with pindel\", {})\n            out_file = _create_vcf(root_pindel, out_file, ref_file,\n                                   items, paired)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _create_tmp_input(input_bams, names, tmp_path, config):\n    tmp_input = os.path.join(tmp_path, \"pindel.txt\")\n    with open(tmp_input, 'w') as out_handle:\n        for bam_file, name in zip(input_bams, names):\n            print(\"%s\\t%s\\t%s\\n\" % (bam_file, 250, name), file=out_handle)\n    return tmp_input", "response": "Create input file for pindel. tab file for pindel. txt"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating vcf file from pindel format", "response": "def _create_vcf(root_file, out_file, reference, items, paired=None):\n    \"\"\"use pindel2vcf to create vcf file from pindel format\n    :param root_file: (str) prefix for pindel\n    :param out_file: (str) final vcf file\n    :param reference (str) genome in fasta format\n    :param items: (dics) information fro yaml file\n    :param paired: (tupple) bam files and name of tumor/normal samples\n    :returns: (str) final vcf file\n    \"\"\"\n    config = items[0][\"config\"]\n    name_ref = items[0][\"genome_build\"]\n    date = time.strftime(\"%Y%m%d\")\n    if not utils.file_exists(out_file):\n        pindel2vcf = config_utils.get_program(\"pindel2vcf\", config)\n        vcf_file = out_file.replace(\".gz\", \"\")\n        with file_transaction(items[0], vcf_file) as tx_out_file:\n            cmd = (\"{pindel2vcf} --gatk_compatible -P {root_file} -r {reference} -R {name_ref} \"\n                   \"-d {date} -v {tx_out_file} --compact_output_limit 15\")\n            do.run(cmd.format(**locals()), \"Converting to vcf\", {})\n            if paired.normal_name:\n                _filter_paired(paired.tumor_name, paired.normal_name,\n                               tx_out_file, reference, items[0])\n        out_file = bgzip_and_index(vcf_file, config)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _filter_paired(tumor, normal, out_file, reference, data):\n    in_file = utils.splitext_plus(out_file)[0] + \"-tmp.vcf\"\n    shutil.move(out_file, in_file)\n    config = data[\"config\"]\n    with file_transaction(data, out_file) as tx_out_file:\n        params = [\"-T\", \"SomaticPindelFilter\", \"-V\", in_file, \"-o\",\n                  tx_out_file, \"-TID\", tumor, \"-NID\", normal, \"-R\", reference]\n        jvm_opts = broad.get_gatk_opts(config)\n        do.run(broad.gatk_cmd(\"gatk-framework\", jvm_opts, params), \"Filter pindel variants\")\n    return out_file", "response": "filter paired vcf file with GATKKKK   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_vcf_calls(vcf_file, data, orig_items):\n    if not _do_prioritize(orig_items):\n        return vcf_file\n    else:\n        ann_vcf = population.run_vcfanno(vcf_file, data)\n        if ann_vcf:\n            priority_file = _prep_priority_filter_vcfanno(ann_vcf, data)\n            return _apply_priority_filter(ann_vcf, priority_file, data)\n        # No data available for filtering, return original file\n        else:\n            return vcf_file", "response": "Prioritize VCF calls based on external annotations supplied through GEMINI."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nannotating variants with priority information and use to apply filters.", "response": "def _apply_priority_filter(in_file, priority_file, data):\n    \"\"\"Annotate variants with priority information and use to apply filters.\n    \"\"\"\n    out_file = \"%s-priority%s\" % utils.splitext_plus(in_file)\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            header = ('##INFO=<ID=EPR,Number=.,Type=String,'\n                      'Description=\"Somatic prioritization based on external annotations, '\n                      'identify as likely germline\">')\n            header_file = \"%s-repeatheader.txt\" % utils.splitext_plus(tx_out_file)[0]\n            with open(header_file, \"w\") as out_handle:\n                out_handle.write(header)\n            if \"tumoronly_germline_filter\" in dd.get_tools_on(data):\n                filter_cmd = (\"bcftools filter -m '+' -s 'LowPriority' \"\n                              \"\"\"-e \"EPR[0] != 'pass'\" |\"\"\")\n            else:\n                filter_cmd = \"\"\n            cmd = (\"bcftools annotate -a {priority_file} -h {header_file} \"\n                   \"-c CHROM,FROM,TO,REF,ALT,INFO/EPR {in_file} | \"\n                   \"{filter_cmd} bgzip -c > {tx_out_file}\")\n            do.run(cmd.format(**locals()), \"Run external annotation based prioritization filtering\")\n    vcfutils.bgzip_and_index(out_file, data[\"config\"])\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _prep_priority_filter_vcfanno(in_vcf, data):\n    pops = ['af_adj_exac_afr', 'af_adj_exac_amr', 'af_adj_exac_eas',\n            'af_adj_exac_fin', 'af_adj_exac_nfe', 'af_adj_exac_oth', 'af_adj_exac_sas',\n            'af_exac_all', 'max_aaf_all',\n            \"af_esp_ea\", \"af_esp_aa\", \"af_esp_all\", \"af_1kg_amr\", \"af_1kg_eas\",\n            \"af_1kg_sas\", \"af_1kg_afr\", \"af_1kg_eur\", \"af_1kg_all\"]\n    known = [\"cosmic_ids\", \"cosmic_id\", \"clinvar_sig\"]\n    out_file = \"%s-priority.tsv\" % utils.splitext_plus(in_vcf)[0]\n    if not utils.file_exists(out_file) and not utils.file_exists(out_file + \".gz\"):\n        with file_transaction(data, out_file) as tx_out_file:\n            with open(tx_out_file, \"w\") as out_handle:\n                writer = csv.writer(out_handle, dialect=\"excel-tab\")\n                header = [\"#chrom\", \"start\", \"end\", \"ref\", \"alt\", \"filter\"]\n                writer.writerow(header)\n                vcf_reader = cyvcf2.VCF(in_vcf)\n                impact_info = _get_impact_info(vcf_reader)\n                for rec in vcf_reader:\n                    row = _prepare_vcf_rec(rec, pops, known, impact_info)\n                    cur_filter = _calc_priority_filter(row, pops)\n                    writer.writerow([rec.CHROM, rec.start, rec.end, rec.REF, \",\".join(rec.ALT), cur_filter])\n    return vcfutils.bgzip_and_index(out_file, data[\"config\"],\n                                    tabix_args=\"-0 -c '#' -s 1 -b 2 -e 3\")", "response": "Prepare tabix file with priority filters based on vcfanno annotations."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving impact parsing information from INFO header.", "response": "def _get_impact_info(vcf_reader):\n    \"\"\"Retrieve impact parsing information from INFO header.\n    \"\"\"\n    ImpactInfo = collections.namedtuple(\"ImpactInfo\", \"header, gclass, id\")\n    KEY_2_CLASS = {\n        'CSQ': geneimpacts.VEP,\n        'ANN': geneimpacts.SnpEff,\n        'BCSQ': geneimpacts.BCFT}\n    for l in (x.strip() for x in _from_bytes(vcf_reader.raw_header).split(\"\\n\")):\n        if l.startswith(\"##INFO\"):\n            patt = re.compile(\"(\\w+)=(\\\"[^\\\"]+\\\"|[^,]+)\")\n            stub = l.split(\"=<\")[1].rstrip(\">\")\n            d = dict(patt.findall(_from_bytes(stub)))\n            if d[\"ID\"] in KEY_2_CLASS:\n                return ImpactInfo(_parse_impact_header(d), KEY_2_CLASS[d[\"ID\"]], d[\"ID\"])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse fields for impact taken from vcf2db", "response": "def _parse_impact_header(hdr_dict):\n    \"\"\"Parse fields for impact, taken from vcf2db\n    \"\"\"\n    desc = hdr_dict[\"Description\"]\n    if hdr_dict[\"ID\"] == \"ANN\":\n        parts = [x.strip(\"\\\"'\") for x in re.split(\"\\s*\\|\\s*\", desc.split(\":\", 1)[1].strip('\" '))]\n    elif hdr_dict[\"ID\"] == \"EFF\":\n        parts = [x.strip(\" [])'(\\\"\") for x in re.split(\"\\||\\(\", desc.split(\":\", 1)[1].strip())]\n    elif hdr_dict[\"ID\"] == \"CSQ\":\n        parts = [x.strip(\" [])'(\\\"\") for x in re.split(\"\\||\\(\", desc.split(\":\", 1)[1].strip())]\n    elif hdr_dict[\"ID\"] == \"BCSQ\":\n        parts = desc.split(']', 1)[1].split(']')[0].replace('[','').split(\"|\")\n    else:\n        raise Exception(\"don't know how to use %s as annotation\" % hdr_dict[\"ID\"])\n    return parts"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _prepare_vcf_rec(rec, pops, known, impact_info):\n    out = {}\n    for k in pops + known:\n        out[k] = rec.INFO.get(k)\n    if impact_info:\n        cur_info = rec.INFO.get(impact_info.id)\n        if cur_info:\n            cur_impacts = [impact_info.gclass(e, impact_info.header) for e in _from_bytes(cur_info).split(\",\")]\n            top = geneimpacts.Effect.top_severity(cur_impacts)\n            if isinstance(top, list):\n                top = top[0]\n            out[\"impact_severity\"] = top.effect_severity\n    return out", "response": "Parse a vcfanno output into a dictionary of useful attributes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _calc_priority_filter(row, pops):\n    filters = []\n    passes = []\n    passes.extend(_find_known(row))\n    filters.extend(_known_populations(row, pops))\n    if len(filters) == 0 or (len(passes) > 0 and len(filters) < 2):\n        passes.insert(0, \"pass\")\n    return \",\".join(passes + filters)", "response": "Calculate the priority filter based on external data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _known_populations(row, pops):\n    cutoff = 0.01\n    out = set([])\n    for pop, base in [(\"esp\", \"af_esp_all\"), (\"1000g\", \"af_1kg_all\"),\n                      (\"exac\", \"af_exac_all\"), (\"anypop\", \"max_aaf_all\")]:\n        for key in [x for x in pops if x.startswith(base)]:\n            val = row[key]\n            if val and val > cutoff:\n                out.add(pop)\n    return sorted(list(out))", "response": "Find variants present in substantial frequency in population databases."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _find_known(row):\n    out = []\n    clinvar_no = set([\"unknown\", \"untested\", \"non-pathogenic\", \"probable-non-pathogenic\",\n                      \"uncertain_significance\", \"uncertain_significance\", \"not_provided\",\n                      \"benign\", \"likely_benign\"])\n    if row[\"cosmic_ids\"] or row[\"cosmic_id\"]:\n        out.append(\"cosmic\")\n    if row[\"clinvar_sig\"] and not row[\"clinvar_sig\"].lower() in clinvar_no:\n        out.append(\"clinvar\")\n    return out", "response": "Find all known pathogenic databases."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _do_prioritize(items):\n    if not any(\"tumoronly-prioritization\" in dd.get_tools_off(d) for d in items):\n        if vcfutils.get_paired_phenotype(items[0]):\n            has_tumor = False\n            has_normal = False\n            for sub_data in items:\n                if vcfutils.get_paired_phenotype(sub_data) == \"tumor\":\n                    has_tumor = True\n                elif vcfutils.get_paired_phenotype(sub_data) == \"normal\":\n                    has_normal = True\n            return has_tumor and not has_normal", "response": "Determine if we should perform prioritization."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_cortex(align_bams, items, ref_file, assoc_files, region=None,\n               out_file=None):\n    \"\"\"Top level entry to regional de-novo based variant calling with cortex_var.\n    \"\"\"\n    raise NotImplementedError(\"Cortex currently out of date and needs reworking.\")\n    if len(align_bams) == 1:\n        align_bam = align_bams[0]\n        config = items[0][\"config\"]\n    else:\n        raise NotImplementedError(\"Need to add multisample calling for cortex_var\")\n    if out_file is None:\n        out_file = \"%s-cortex.vcf\" % os.path.splitext(align_bam)[0]\n    if region is not None:\n        work_dir = safe_makedir(os.path.join(os.path.dirname(out_file),\n                                             region.replace(\".\", \"_\")))\n    else:\n        work_dir = os.path.dirname(out_file)\n    if not file_exists(out_file):\n        bam.index(align_bam, config)\n        variant_regions = config[\"algorithm\"].get(\"variant_regions\", None)\n        if not variant_regions:\n            raise ValueError(\"Only support regional variant calling with cortex_var: set variant_regions\")\n        target_regions = subset_variant_regions(variant_regions, region, out_file)\n        if os.path.isfile(target_regions):\n            with open(target_regions) as in_handle:\n                regional_vcfs = [_run_cortex_on_region(x.strip().split(\"\\t\")[:3], align_bam,\n                                                       ref_file, work_dir, out_file, config)\n                                 for x in in_handle]\n\n            combine_file = \"{0}-raw{1}\".format(*os.path.splitext(out_file))\n            _combine_variants(regional_vcfs, combine_file, ref_file, config)\n            _select_final_variants(combine_file, out_file, config)\n        else:\n            vcfutils.write_empty_vcf(out_file)\n    return out_file", "response": "Run cortex_var on a set of variants and return a single VCF file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _passes_cortex_depth(line, min_depth):\n    parts = line.split(\"\\t\")\n    cov_index = parts[8].split(\":\").index(\"COV\")\n    passes_depth = False\n    for gt in parts[9:]:\n        cur_cov = gt.split(\":\")[cov_index]\n        cur_depth = sum(int(x) for x in cur_cov.split(\",\"))\n        if cur_depth >= min_depth:\n            passes_depth = True\n    return passes_depth", "response": "Check if the line passes the minimum depth requirement?"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfilters input file removing items with low depth of support.", "response": "def _select_final_variants(base_vcf, out_vcf, config):\n    \"\"\"Filter input file, removing items with low depth of support.\n\n    cortex_var calls are tricky to filter by depth. Count information is in\n    the COV FORMAT field grouped by alleles, so we need to sum up values and\n    compare.\n    \"\"\"\n    min_depth = int(config[\"algorithm\"].get(\"min_depth\", 4))\n    with file_transaction(out_vcf) as tx_out_file:\n        with open(base_vcf) as in_handle:\n            with open(tx_out_file, \"w\") as out_handle:\n                for line in in_handle:\n                    if line.startswith(\"#\"):\n                        passes = True\n                    else:\n                        passes = _passes_cortex_depth(line, min_depth)\n                    if passes:\n                        out_handle.write(line)\n    return out_vcf"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncombines the variants in a single VCF file.", "response": "def _combine_variants(in_vcfs, out_file, ref_file, config):\n    \"\"\"Combine variant files, writing the header from the first non-empty input.\n\n    in_vcfs is a list with each item starting with the chromosome regions,\n    and ending with the input file.\n    We sort by these regions to ensure the output file is in the expected order.\n    \"\"\"\n    in_vcfs.sort()\n    wrote_header = False\n    with open(out_file, \"w\") as out_handle:\n        for in_vcf in (x[-1] for x in in_vcfs):\n            with open(in_vcf) as in_handle:\n                header = list(itertools.takewhile(lambda x: x.startswith(\"#\"),\n                                                  in_handle))\n                if not header[0].startswith(\"##fileformat=VCFv4\"):\n                    raise ValueError(\"Unexpected VCF file: %s\" % in_vcf)\n                for line in in_handle:\n                    if not wrote_header:\n                        wrote_header = True\n                        out_handle.write(\"\".join(header))\n                    out_handle.write(line)\n        if not wrote_header:\n            out_handle.write(\"\".join(header))\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns cortex on a specified chromosome start and end region.", "response": "def _run_cortex_on_region(region, align_bam, ref_file, work_dir, out_file_base, config):\n    \"\"\"Run cortex on a specified chromosome start/end region.\n    \"\"\"\n    kmers = [31, 51, 71]\n    min_reads = 1750\n    cortex_dir = config_utils.get_program(\"cortex\", config, \"dir\")\n    stampy_dir = config_utils.get_program(\"stampy\", config, \"dir\")\n    vcftools_dir = config_utils.get_program(\"vcftools\", config, \"dir\")\n    if cortex_dir is None or stampy_dir is None:\n        raise ValueError(\"cortex_var requires path to pre-built cortex and stampy\")\n    region_str = \"{0}-{1}-{2}\".format(*region)\n    base_dir = safe_makedir(os.path.join(work_dir, region_str))\n    try:\n        out_vcf_base = os.path.join(base_dir, \"{0}-{1}\".format(\n                    os.path.splitext(os.path.basename(out_file_base))[0], region_str))\n        out_file = os.path.join(work_dir, os.path.basename(\"{0}.vcf\".format(out_vcf_base)))\n        if not file_exists(out_file):\n            fastq = _get_fastq_in_region(region, align_bam, out_vcf_base)\n            if _count_fastq_reads(fastq, min_reads) < min_reads:\n                vcfutils.write_empty_vcf(out_file)\n            else:\n                local_ref, genome_size = _get_local_ref(region, ref_file, out_vcf_base)\n                indexes = _index_local_ref(local_ref, cortex_dir, stampy_dir, kmers)\n                cortex_out = _run_cortex(fastq, indexes, {\"kmers\": kmers, \"genome_size\": genome_size,\n                                                          \"sample\": get_sample_name(align_bam)},\n                                         out_vcf_base, {\"cortex\": cortex_dir, \"stampy\": stampy_dir,\n                                                        \"vcftools\": vcftools_dir},\n                                         config)\n                if cortex_out:\n                    _remap_cortex_out(cortex_out, region, out_file)\n                else:\n                    vcfutils.write_empty_vcf(out_file)\n    finally:\n        if os.path.exists(base_dir):\n            shutil.rmtree(base_dir)\n    return [region[0], int(region[1]), int(region[2]), out_file]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _run_cortex(fastq, indexes, params, out_base, dirs, config):\n    print(out_base)\n    fastaq_index = \"{0}.fastaq_index\".format(out_base)\n    se_fastq_index = \"{0}.se_fastq\".format(out_base)\n    pe_fastq_index = \"{0}.pe_fastq\".format(out_base)\n    reffasta_index = \"{0}.list_ref_fasta\".format(out_base)\n    with open(se_fastq_index, \"w\") as out_handle:\n        out_handle.write(fastq + \"\\n\")\n    with open(pe_fastq_index, \"w\") as out_handle:\n        out_handle.write(\"\")\n    with open(fastaq_index, \"w\") as out_handle:\n        out_handle.write(\"{0}\\t{1}\\t{2}\\t{2}\\n\".format(params[\"sample\"], se_fastq_index,\n                                                       pe_fastq_index))\n    with open(reffasta_index, \"w\") as out_handle:\n        for x in indexes[\"fasta\"]:\n            out_handle.write(x + \"\\n\")\n    os.environ[\"PERL5LIB\"] = \"{0}:{1}:{2}\".format(\n        os.path.join(dirs[\"cortex\"], \"scripts/calling\"),\n        os.path.join(dirs[\"cortex\"], \"scripts/analyse_variants/bioinf-perl/lib\"),\n        os.environ.get(\"PERL5LIB\", \"\"))\n    kmers = sorted(params[\"kmers\"])\n    kmer_info = [\"--first_kmer\", str(kmers[0])]\n    if len(kmers) > 1:\n        kmer_info += [\"--last_kmer\", str(kmers[-1]),\n                      \"--kmer_step\", str(kmers[1] - kmers[0])]\n    subprocess.check_call([\"perl\", os.path.join(dirs[\"cortex\"], \"scripts\", \"calling\", \"run_calls.pl\"),\n                           \"--fastaq_index\", fastaq_index,\n                           \"--auto_cleaning\", \"yes\", \"--bc\", \"yes\", \"--pd\", \"yes\",\n                           \"--outdir\", os.path.dirname(out_base), \"--outvcf\", os.path.basename(out_base),\n                           \"--ploidy\", str(config[\"algorithm\"].get(\"ploidy\", 2)),\n                           \"--stampy_hash\", indexes[\"stampy\"],\n                           \"--stampy_bin\", os.path.join(dirs[\"stampy\"], \"stampy.py\"),\n                           \"--refbindir\", os.path.dirname(indexes[\"cortex\"][0]),\n                           \"--list_ref_fasta\",  reffasta_index,\n                           \"--genome_size\", str(params[\"genome_size\"]),\n                           \"--max_read_len\", \"30000\",\n                           #\"--max_var_len\", \"4000\",\n                           \"--format\", \"FASTQ\", \"--qthresh\", \"5\", \"--do_union\", \"yes\",\n                           \"--mem_height\", \"17\", \"--mem_width\", \"100\",\n                           \"--ref\", \"CoordinatesAndInCalling\", \"--workflow\", \"independent\",\n                           \"--vcftools_dir\", dirs[\"vcftools\"],\n                           \"--logfile\", \"{0}.logfile,f\".format(out_base)]\n                          + kmer_info)\n    final = glob.glob(os.path.join(os.path.dirname(out_base), \"vcfs\",\n                                   \"{0}*FINALcombined_BC*decomp.vcf\".format(os.path.basename(out_base))))\n    # No calls, need to setup an empty file\n    if len(final) != 1:\n        print(\"Did not find output VCF file for {0}\".format(out_base))\n        return None\n    else:\n        return final[0]", "response": "Run cortex_var run_calls. pl producing a VCF variant file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _index_local_ref(fasta_file, cortex_dir, stampy_dir, kmers):\n    base_out = os.path.splitext(fasta_file)[0]\n    cindexes = []\n    for kmer in kmers:\n        out_file = \"{0}.k{1}.ctx\".format(base_out, kmer)\n        if not file_exists(out_file):\n            file_list = \"{0}.se_list\".format(base_out)\n            with open(file_list, \"w\") as out_handle:\n                out_handle.write(fasta_file + \"\\n\")\n            subprocess.check_call([_get_cortex_binary(kmer, cortex_dir),\n                                   \"--kmer_size\", str(kmer), \"--mem_height\", \"17\",\n                                   \"--se_list\", file_list, \"--format\", \"FASTA\",\n                                   \"--max_read_len\", \"30000\",\n\t\t\t           \"--sample_id\", base_out,\n                                   \"--dump_binary\", out_file])\n        cindexes.append(out_file)\n    if not file_exists(\"{0}.stidx\".format(base_out)):\n        subprocess.check_call([os.path.join(stampy_dir, \"stampy.py\"), \"-G\",\n                               base_out, fasta_file])\n        subprocess.check_call([os.path.join(stampy_dir, \"stampy.py\"), \"-g\",\n                               base_out, \"-H\", base_out])\n    return {\"stampy\": base_out,\n            \"cortex\": cindexes,\n            \"fasta\": [fasta_file]}", "response": "Pre - index a generated local reference sequence with cortex_var and stampy."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve a local FASTA file corresponding to the specified region.", "response": "def _get_local_ref(region, ref_file, out_vcf_base):\n    \"\"\"Retrieve a local FASTA file corresponding to the specified region.\n    \"\"\"\n    out_file = \"{0}.fa\".format(out_vcf_base)\n    if not file_exists(out_file):\n        with pysam.Fastafile(ref_file) as in_pysam:\n            contig, start, end = region\n            seq = in_pysam.fetch(contig, int(start), int(end))\n            with open(out_file, \"w\") as out_handle:\n                out_handle.write(\">{0}-{1}-{2}\\n{3}\".format(contig, start, end,\n                                                              str(seq)))\n    with open(out_file) as in_handle:\n        in_handle.readline()\n        size = len(in_handle.readline().strip())\n    return out_file, size"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_fastq_in_region(region, align_bam, out_base):\n    out_file = \"{0}.fastq\".format(out_base)\n    if not file_exists(out_file):\n        with pysam.Samfile(align_bam, \"rb\") as in_pysam:\n            with file_transaction(out_file) as tx_out_file:\n                with open(tx_out_file, \"w\") as out_handle:\n                    contig, start, end = region\n                    for read in in_pysam.fetch(contig, int(start), int(end)):\n                        seq = Seq.Seq(read.seq)\n                        qual = list(read.qual)\n                        if read.is_reverse:\n                            seq = seq.reverse_complement()\n                            qual.reverse()\n                        out_handle.write(\"@{name}\\n{seq}\\n+\\n{qual}\\n\".format(\n                                name=read.qname, seq=str(seq), qual=\"\".join(qual)))\n    return out_file", "response": "Retrieve fastq files in a single end region."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _count_fastq_reads(in_fastq, min_reads):\n    with open(in_fastq) as in_handle:\n        items = list(itertools.takewhile(lambda i : i <= min_reads,\n                                         (i for i, _ in enumerate(FastqGeneralIterator(in_handle)))))\n    return len(items)", "response": "Count the number of fastq reads in a file stopping after reaching min_reads."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tx_tmpdir(data=None, base_dir=None, remove=True):\n    base_dir = base_dir or os.getcwd()\n    tmpdir_base = utils.get_abspath(_get_base_tmpdir(data, base_dir))\n    utils.safe_makedir(tmpdir_base)\n    tmp_dir = tempfile.mkdtemp(dir=tmpdir_base)\n    #logger.debug(\"Created tmp dir %s \" % tmp_dir)\n    try:\n        yield tmp_dir\n    finally:\n        if remove:\n            utils.remove_safe(tmp_dir)", "response": "Context manager to create and remove a transactional temporary directory."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef file_transaction(*data_and_files):\n    with _flatten_plus_safe(data_and_files) as (safe_names, orig_names):\n        # remove any half-finished transactions\n        map(utils.remove_safe, safe_names)\n        # no need for try except block here,\n        # because exceptions and tmp dir removal\n        # are handled by tx_tmpdir contextmanager\n        if len(safe_names) == 1:\n            yield safe_names[0]\n        else:\n            yield tuple(safe_names)\n\n        for safe, orig in zip(safe_names, orig_names):\n            if os.path.exists(safe):\n                _move_tmp_files(safe, orig)", "response": "Wrap file generation in a transaction."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _move_file_with_sizecheck(tx_file, final_file):\n\n    #logger.debug(\"Moving %s to %s\" % (tx_file, final_file))\n\n    tmp_file = final_file + \".bcbiotmp\"\n    open(tmp_file, 'wb').close()\n\n    want_size = utils.get_size(tx_file)\n    shutil.move(tx_file, final_file)\n    transfer_size = utils.get_size(final_file)\n\n    assert want_size == transfer_size, (\n        'distributed.transaction.file_transaction: File copy error: '\n        'file or directory on temporary storage ({}) size {} bytes '\n        'does not equal size of file or directory after transfer to '\n        'shared storage ({}) size {} bytes'.format(\n            tx_file, want_size, final_file, transfer_size)\n    )\n    utils.remove_safe(tmp_file)", "response": "Move transaction file to final location with size check."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _flatten_plus_safe(data_and_files):\n    data, rollback_files = _normalize_args(data_and_files)\n    with tx_tmpdir(data) as tmpdir:\n        tx_files = [os.path.join(tmpdir, os.path.basename(f))\n                    for f in rollback_files]\n        yield tx_files, rollback_files", "response": "Flatten names of files and create temporary file names."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nusing GATK to extract reads from full BAM file.", "response": "def _gatk_extract_reads_cl(data, region, prep_params, tmp_dir):\n    \"\"\"Use GATK to extract reads from full BAM file.\n    \"\"\"\n    args = [\"PrintReads\",\n            \"-L\", region_to_gatk(region),\n            \"-R\", dd.get_ref_file(data),\n            \"-I\", data[\"work_bam\"]]\n    # GATK3 back compatibility, need to specify analysis type\n    if \"gatk4\" in dd.get_tools_off(data):\n        args = [\"--analysis_type\"] + args\n    runner = broad.runner_from_config(data[\"config\"])\n    return runner.cl_gatk(args, tmp_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _piped_input_cl(data, region, tmp_dir, out_base_file, prep_params):\n    return data[\"work_bam\"], _gatk_extract_reads_cl(data, region, prep_params, tmp_dir)", "response": "Retrieve the commandline for streaming input into preparation step."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _piped_realign_gatk(data, region, cl, out_base_file, tmp_dir, prep_params):\n    broad_runner = broad.runner_from_config(data[\"config\"])\n    pa_bam = \"%s-prealign%s\" % os.path.splitext(out_base_file)\n    if not utils.file_exists(pa_bam):\n        with file_transaction(data, pa_bam) as tx_out_file:\n            cmd = \"{cl} -o {tx_out_file}\".format(**locals())\n            do.run(cmd, \"GATK re-alignment {0}\".format(region), data)\n    bam.index(pa_bam, data[\"config\"])\n    realn_file = realign.gatk_realigner_targets(broad_runner, pa_bam, dd.get_ref_file(data), data[\"config\"],\n                                                region=region_to_gatk(region),\n                                                known_vrns=dd.get_variation_resources(data))\n    realn_cl = realign.gatk_indel_realignment_cl(broad_runner, pa_bam, dd.get_ref_file(data),\n                                                 realn_file, tmp_dir, region=region_to_gatk(region),\n                                                 known_vrns=dd.get_variation_resources(data))\n    return pa_bam, realn_cl", "response": "Perform realignment with GATK using input commandline."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms semi - piped BAM preparation using GATK s GATK tool.", "response": "def _piped_bamprep_region_gatk(data, region, prep_params, out_file, tmp_dir):\n    \"\"\"Perform semi-piped BAM preparation using Picard/GATK tools.\n    \"\"\"\n    broad_runner = broad.runner_from_config(data[\"config\"])\n    cur_bam, cl = _piped_input_cl(data, region, tmp_dir, out_file, prep_params)\n    if not prep_params[\"realign\"]:\n        prerecal_bam = None\n    elif prep_params[\"realign\"] == \"gatk\":\n        prerecal_bam, cl = _piped_realign_gatk(data, region, cl, out_file, tmp_dir,\n                                               prep_params)\n    else:\n        raise NotImplementedError(\"Realignment method: %s\" % prep_params[\"realign\"])\n    with file_transaction(data, out_file) as tx_out_file:\n        out_flag = (\"-o\" if (prep_params[\"realign\"] == \"gatk\"\n                             or not prep_params[\"realign\"])\n                    else \">\")\n        cmd = \"{cl} {out_flag} {tx_out_file}\".format(**locals())\n        do.run(cmd, \"GATK: realign {0}\".format(region), data)\n        _cleanup_tempfiles(data, [cur_bam, prerecal_bam])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve configuration parameters with defaults for preparing BAM files.", "response": "def _get_prep_params(data):\n    \"\"\"Retrieve configuration parameters with defaults for preparing BAM files.\n    \"\"\"\n    realign_param = dd.get_realign(data)\n    realign_param = \"gatk\" if realign_param is True else realign_param\n    return {\"realign\": realign_param}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming BAM preprocessing on the selected region.", "response": "def _piped_bamprep_region(data, region, out_file, tmp_dir):\n    \"\"\"Do work of preparing BAM input file on the selected region.\n    \"\"\"\n    if _need_prep(data):\n        prep_params = _get_prep_params(data)\n        _piped_bamprep_region_gatk(data, region, prep_params, out_file, tmp_dir)\n    else:\n        raise ValueError(\"No realignment specified\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef piped_bamprep(data, region=None, out_file=None):\n    data[\"region\"] = region\n    if not _need_prep(data):\n        return [data]\n    else:\n        utils.safe_makedir(os.path.dirname(out_file))\n        if region[0] == \"nochrom\":\n            prep_bam = shared.write_nochr_reads(data[\"work_bam\"], out_file, data[\"config\"])\n        elif region[0] == \"noanalysis\":\n            prep_bam = shared.write_noanalysis_reads(data[\"work_bam\"], region[1], out_file,\n                                                     data[\"config\"])\n        else:\n            if not utils.file_exists(out_file):\n                with tx_tmpdir(data) as tmp_dir:\n                    _piped_bamprep_region(data, region, out_file, tmp_dir)\n            prep_bam = out_file\n        bam.index(prep_bam, data[\"config\"])\n        data[\"work_bam\"] = prep_bam\n        return [data]", "response": "Perform full BAM preparation using pipes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_file(finfo, sample_info, config):\n    if GalaxyInstance is None:\n        raise ImportError(\"Could not import bioblend.galaxy\")\n    if \"dir\" not in config:\n        raise ValueError(\"Galaxy upload requires `dir` parameter in config specifying the \"\n                         \"shared filesystem path to move files to.\")\n    if \"outputs\" in config:\n        _galaxy_tool_copy(finfo, config[\"outputs\"])\n    else:\n        _galaxy_library_upload(finfo, sample_info, config)", "response": "Update file in Galaxy data libraries."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncopying information directly to pre - defined outputs from a Galaxy tool.", "response": "def _galaxy_tool_copy(finfo, outputs):\n    \"\"\"Copy information directly to pre-defined outputs from a Galaxy tool.\n\n    XXX Needs generalization\n    \"\"\"\n    tool_map = {\"align\": \"bam\", \"variants\": \"vcf.gz\"}\n    for galaxy_key, finfo_type in tool_map.items():\n        if galaxy_key in outputs and finfo.get(\"type\") == finfo_type:\n            shutil.copy(finfo[\"path\"], outputs[galaxy_key])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nuploading results to the galaxy library.", "response": "def _galaxy_library_upload(finfo, sample_info, config):\n    \"\"\"Upload results to galaxy library.\n    \"\"\"\n    folder_name = \"%s_%s\" % (config[\"fc_date\"], config[\"fc_name\"])\n    storage_dir = utils.safe_makedir(os.path.join(config[\"dir\"], folder_name))\n    if finfo.get(\"type\") == \"directory\":\n        storage_file = None\n        if finfo.get(\"ext\") == \"qc\":\n            pdf_file = qcsummary.prep_pdf(finfo[\"path\"], config)\n            if pdf_file:\n                finfo[\"path\"] = pdf_file\n                finfo[\"type\"] = \"pdf\"\n                storage_file = filesystem.copy_finfo(finfo, storage_dir, pass_uptodate=True)\n    else:\n        storage_file = filesystem.copy_finfo(finfo, storage_dir, pass_uptodate=True)\n    if \"galaxy_url\" in config and \"galaxy_api_key\" in config:\n        galaxy_url = config[\"galaxy_url\"]\n        if not galaxy_url.endswith(\"/\"):\n            galaxy_url += \"/\"\n        gi = GalaxyInstance(galaxy_url, config[\"galaxy_api_key\"])\n    else:\n        raise ValueError(\"Galaxy upload requires `galaxy_url` and `galaxy_api_key` in config\")\n    if storage_file and sample_info and not finfo.get(\"index\", False) and not finfo.get(\"plus\", False):\n        _to_datalibrary_safe(storage_file, gi, folder_name, sample_info, config)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nuploading a file to a datalibrary folder.", "response": "def _to_datalibrary_safe(fname, gi, folder_name, sample_info, config):\n    \"\"\"Upload with retries for intermittent JSON failures.\n    \"\"\"\n    num_tries = 0\n    max_tries = 5\n    while 1:\n        try:\n            _to_datalibrary(fname, gi, folder_name, sample_info, config)\n            break\n        except (simplejson.scanner.JSONDecodeError, bioblend.galaxy.client.ConnectionError) as e:\n            num_tries += 1\n            if num_tries > max_tries:\n                raise\n            print(\"Retrying upload, failed with:\", str(e))\n            time.sleep(5)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuploads a file to a Galaxy data library in a project specific folder.", "response": "def _to_datalibrary(fname, gi, folder_name, sample_info, config):\n    \"\"\"Upload a file to a Galaxy data library in a project specific folder.\n    \"\"\"\n    library = _get_library(gi, sample_info, config)\n    libitems = gi.libraries.show_library(library.id, contents=True)\n    folder = _get_folder(gi, folder_name, library, libitems)\n    _file_to_folder(gi, fname, sample_info, libitems, library, folder)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nuploads file to folder if not exists on Galaxy", "response": "def _file_to_folder(gi, fname, sample_info, libitems, library, folder):\n    \"\"\"Check if file exists on Galaxy, if not upload to specified folder.\n    \"\"\"\n    full_name = os.path.join(folder[\"name\"], os.path.basename(fname))\n\n    # Handle VCF: Galaxy reports VCF files without the gzip extension\n    file_type = \"vcf_bgzip\" if full_name.endswith(\".vcf.gz\") else \"auto\"\n    if full_name.endswith(\".vcf.gz\"):\n        full_name = full_name.replace(\".vcf.gz\", \".vcf\")\n\n    for item in libitems:\n        if item[\"name\"] == full_name:\n            return item\n    logger.info(\"Uploading to Galaxy library '%s': %s\" % (library.name, full_name))\n    return gi.libraries.upload_from_galaxy_filesystem(str(library.id), fname, folder_id=str(folder[\"id\"]),\n                                                      link_data_only=\"link_to_files\",\n                                                      dbkey=sample_info[\"genome_build\"],\n                                                      file_type=file_type,\n                                                      roles=str(library.roles) if library.roles else None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_folder(gi, folder_name, library, libitems):\n    for item in libitems:\n        if item[\"type\"] == \"folder\" and item[\"name\"] == \"/%s\" % folder_name:\n            return item\n    return gi.libraries.create_folder(library.id, folder_name)[0]", "response": "Retrieve or create a folder inside the library with the specified name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the appropriate data library for the current user.", "response": "def _get_library(gi, sample_info, config):\n    \"\"\"Retrieve the appropriate data library for the current user.\n    \"\"\"\n    galaxy_lib = sample_info.get(\"galaxy_library\",\n                                 config.get(\"galaxy_library\"))\n    role = sample_info.get(\"galaxy_role\",\n                           config.get(\"galaxy_role\"))\n    if galaxy_lib:\n        return _get_library_from_name(gi, galaxy_lib, role, sample_info, create=True)\n    elif config.get(\"private_libs\") or config.get(\"lab_association\") or config.get(\"researcher\"):\n        return _library_from_nglims(gi, sample_info, config)\n    else:\n        raise ValueError(\"No Galaxy library specified for sample: %s\" %\n                         sample_info[\"description\"])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _library_from_nglims(gi, sample_info, config):\n    names = [config.get(x, \"\").strip() for x in [\"lab_association\", \"researcher\"]\n             if config.get(x)]\n    for name in names:\n        for ext in [\"sequencing\", \"lab\"]:\n            check_name = \"%s %s\" % (name.split()[0], ext)\n            try:\n                return _get_library_from_name(gi, check_name, None, sample_info)\n            except ValueError:\n                pass\n    check_names = set([x.lower() for x in names])\n    for libname, role in config[\"private_libs\"]:\n        # Try to find library for lab or rsearcher\n        if libname.lower() in check_names:\n            return _get_library_from_name(gi, libname, role, sample_info)\n    # default to first private library if available\n    if len(config.get(\"private_libs\", [])) > 0:\n        libname, role = config[\"private_libs\"][0]\n        return _get_library_from_name(gi, libname, role, sample_info)\n    # otherwise use the lab association or researcher name\n    elif len(names) > 0:\n        return _get_library_from_name(gi, names[0], None, sample_info, create=True)\n    else:\n        raise ValueError(\"Could not find Galaxy library for sample %s\" % sample_info[\"description\"])", "response": "Retrieve upload library from nglims specified user libraries."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prepare_input_data(config):\n\n    if not dd.get_disambiguate(config):\n        return dd.get_input_sequence_files(config)\n\n    work_bam = dd.get_work_bam(config)\n    logger.info(\"Converting disambiguated reads to fastq...\")\n    fq_files = convert_bam_to_fastq(\n        work_bam, dd.get_work_dir(config), None, None, config\n    )\n    return fq_files", "response": "Prepare input data for EricScript."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_run_command(self, tx_output_dir, input_files):\n        logger.debug(\"Input data: %s\" % ', '.join(input_files))\n        cmd = [\n            self.EXECUTABLE,\n            '-db', self._db_location,\n            '-name', self._sample_name,\n            '-o', tx_output_dir,\n        ] + list(input_files)\n        return \"export PATH=%s:%s:\\\"$PATH\\\"; %s;\" % (self._get_samtools0_path(), self._get_ericscript_path(), \" \".join(cmd))", "response": "Constructs a command to run EricScript via do. run function."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_ericscript_path(self):\n        es = utils.which(os.path.join(utils.get_bcbio_bin(), self.EXECUTABLE))\n        return os.path.dirname(os.path.realpath(es))", "response": "Retrieve PATH to the isolated eriscript anaconda environment."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_samtools0_path(self):\n        samtools_path = os.path.realpath(os.path.join(self._get_ericscript_path(),\"..\", \"..\", \"bin\"))\n        return samtools_path", "response": "Retrieve the path to the samtools bin directory for the eriscript."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef output_dir(self):\n        if self._output_dir is None:\n            self._output_dir = self._get_output_dir()\n        return self._output_dir", "response": "Absolute path to permanent location in working directory where EricScript output will be stored."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sample_out_dir(self):\n        if self._sample_out_dir is None:\n            self._sample_out_dir = os.path.join(\n                self.output_dir, self._sample_name\n            )\n        return self._sample_out_dir", "response": "Absolute path to permanent location in working directory where EricScript output for the current sample will be stored."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reference_index(self):\n        if self._db_location:\n            ref_indices = glob.glob(os.path.join(self._db_location, \"*\", self._REF_INDEX))\n            if ref_indices:\n                return ref_indices[0]", "response": "Absolute path to the BWA index for EricScript reference data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reference_fasta(self):\n        if self._db_location:\n            ref_files = glob.glob(os.path.join(self._db_location, \"*\", self._REF_FASTA))\n            if ref_files:\n                return ref_files[0]", "response": "Absolute path to the fasta file with EricScript reference data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve input args depending on genome build.", "response": "def _get_input_args(bam_file, data, out_base, background):\n    \"\"\"Retrieve input args, depending on genome build.\n\n    VerifyBamID2 only handles GRCh37 (1, 2, 3) not hg19, so need to generate\n    a pileup for hg19 and fix chromosome naming.\n    \"\"\"\n    if dd.get_genome_build(data) in [\"hg19\"]:\n        return [\"--PileupFile\", _create_pileup(bam_file, data, out_base, background)]\n    else:\n        return [\"--BamFile\", bam_file]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate pileup calls for hg19 - > GRCh37 chromosome mapping.", "response": "def _create_pileup(bam_file, data, out_base, background):\n    \"\"\"Create pileup calls in the regions of interest for hg19 -> GRCh37 chromosome mapping.\n    \"\"\"\n    out_file = \"%s-mpileup.txt\" % out_base\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            background_bed = os.path.normpath(os.path.join(\n                os.path.dirname(os.path.realpath(utils.which(\"verifybamid2\"))),\n                \"resource\", \"%s.%s.%s.vcf.gz.dat.bed\" % (background[\"dataset\"],\n                                                         background[\"nvars\"], background[\"build\"])))\n            local_bed = os.path.join(os.path.dirname(out_base),\n                                     \"%s.%s-hg19.bed\" % (background[\"dataset\"], background[\"nvars\"]))\n            if not utils.file_exists(local_bed):\n                with file_transaction(data, local_bed) as tx_local_bed:\n                    with open(background_bed) as in_handle:\n                        with open(tx_local_bed, \"w\") as out_handle:\n                            for line in in_handle:\n                                out_handle.write(\"chr%s\" % line)\n            mpileup_cl = samtools.prep_mpileup([bam_file], dd.get_ref_file(data), data[\"config\"], want_bcf=False,\n                                                target_regions=local_bed)\n            cl = (\"{mpileup_cl} | sed 's/^chr//' > {tx_out_file}\")\n            do.run(cl.format(**locals()), \"Create pileup from BAM input\")\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert cn_mops CNV based bed files into flattened BED", "response": "def _cnvbed_to_bed(in_file, caller, out_file):\n    \"\"\"Convert cn_mops CNV based bed files into flattened BED\n    \"\"\"\n    with open(out_file, \"w\") as out_handle:\n        for feat in pybedtools.BedTool(in_file):\n            out_handle.write(\"\\t\".join([feat.chrom, str(feat.start), str(feat.end),\n                                        \"cnv%s_%s\" % (feat.score, caller)])\n                             + \"\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_bed(call, sample, work_dir, calls, data):\n    out_file = os.path.join(work_dir, \"%s-%s-flat.bed\" % (sample, call[\"variantcaller\"]))\n    if call.get(\"vrn_file\") and not utils.file_uptodate(out_file, call[\"vrn_file\"]):\n        with file_transaction(data, out_file) as tx_out_file:\n            convert_fn = CALLER_TO_BED.get(call[\"variantcaller\"])\n            if convert_fn:\n                vrn_file = call[\"vrn_file\"]\n                if call[\"variantcaller\"] in SUBSET_BY_SUPPORT:\n                    ecalls = [x for x in calls if x[\"variantcaller\"] in SUBSET_BY_SUPPORT[call[\"variantcaller\"]]]\n                    if len(ecalls) > 0:\n                        vrn_file = _subset_by_support(call[\"vrn_file\"], ecalls, data)\n                convert_fn(vrn_file, call[\"variantcaller\"], tx_out_file)\n    if utils.file_exists(out_file):\n        return out_file", "response": "Create a simplified BED file from caller specific input."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(bam_file, data, out_dir):\n    out = dict()\n\n    out_dir = utils.safe_makedir(out_dir)\n    if dd.get_coverage(data) and dd.get_coverage(data) not in [\"None\"]:\n        merged_bed_file = bedutils.clean_file(dd.get_coverage_merged(data), data, prefix=\"cov-\", simple=True)\n        target_name = \"coverage\"\n    elif dd.get_coverage_interval(data) != \"genome\":\n        merged_bed_file = dd.get_variant_regions_merged(data) or dd.get_sample_callable(data)\n        target_name = \"variant_regions\"\n    else:\n        merged_bed_file = None\n        target_name = \"genome\"\n\n    avg_depth = cov.get_average_coverage(target_name, merged_bed_file, data)\n    if target_name == \"coverage\":\n        out_files = cov.coverage_region_detailed_stats(target_name, merged_bed_file, data, out_dir)\n    else:\n        out_files = []\n\n    out['Avg_coverage'] = avg_depth\n\n    samtools_stats_dir = os.path.join(out_dir, os.path.pardir, 'samtools')\n    from bcbio.qc import samtools\n    samtools_stats = samtools.run(bam_file, data, samtools_stats_dir)[\"metrics\"]\n\n    out[\"Total_reads\"] = total_reads = int(samtools_stats[\"Total_reads\"])\n    out[\"Mapped_reads\"] = mapped = int(samtools_stats[\"Mapped_reads\"])\n    out[\"Mapped_paired_reads\"] = int(samtools_stats[\"Mapped_paired_reads\"])\n    out['Duplicates'] = dups = int(samtools_stats[\"Duplicates\"])\n\n    if total_reads:\n        out[\"Mapped_reads_pct\"] = 100.0 * mapped / total_reads\n    if mapped:\n        out['Duplicates_pct'] = 100.0 * dups / mapped\n\n    if dd.get_coverage_interval(data) == \"genome\":\n        mapped_unique = mapped - dups\n    else:\n        mapped_unique = readstats.number_of_mapped_reads(data, bam_file, keep_dups=False)\n    out['Mapped_unique_reads'] = mapped_unique\n\n    if merged_bed_file:\n        ontarget = readstats.number_of_mapped_reads(\n            data, bam_file, keep_dups=False, bed_file=merged_bed_file, target_name=target_name)\n        out[\"Ontarget_unique_reads\"] = ontarget\n        if mapped_unique:\n            out[\"Ontarget_pct\"] = 100.0 * ontarget / mapped_unique\n            out['Offtarget_pct'] = 100.0 * (mapped_unique - ontarget) / mapped_unique\n            if dd.get_coverage_interval(data) != \"genome\":\n                # Skip padded calculation for WGS even if the \"coverage\" file is specified\n                # the padded statistic makes only sense for exomes and panels\n                padded_bed_file = bedutils.get_padded_bed_file(out_dir, merged_bed_file, 200, data)\n                ontarget_padded = readstats.number_of_mapped_reads(\n                    data, bam_file, keep_dups=False, bed_file=padded_bed_file, target_name=target_name + \"_padded\")\n                out[\"Ontarget_padded_pct\"] = 100.0 * ontarget_padded / mapped_unique\n        if total_reads:\n            out['Usable_pct'] = 100.0 * ontarget / total_reads\n\n    indexcov_files = _goleft_indexcov(bam_file, data, out_dir)\n    out_files += [x for x in indexcov_files if x and utils.file_exists(x)]\n    out = {\"metrics\": out}\n    if len(out_files) > 0:\n        out[\"base\"] = out_files[0]\n        out[\"secondary\"] = out_files[1:]\n    return out", "response": "Run coverage analysis on a single sample."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nuse goleft indexcov to estimate coverage distributions using BAM index.", "response": "def _goleft_indexcov(bam_file, data, out_dir):\n    \"\"\"Use goleft indexcov to estimate coverage distributions using BAM index.\n\n    Only used for whole genome runs as captures typically don't have enough data\n    to be useful for index-only summaries.\n    \"\"\"\n    if not dd.get_coverage_interval(data) == \"genome\":\n        return []\n    out_dir = utils.safe_makedir(os.path.join(out_dir, \"indexcov\"))\n    out_files = [os.path.join(out_dir, \"%s-indexcov.%s\" % (dd.get_sample_name(data), ext))\n                 for ext in [\"roc\", \"ped\", \"bed.gz\"]]\n    if not utils.file_uptodate(out_files[-1], bam_file):\n        with transaction.tx_tmpdir(data) as tmp_dir:\n            tmp_dir = utils.safe_makedir(os.path.join(tmp_dir, dd.get_sample_name(data)))\n            gender_chroms = [x.name for x in ref.file_contigs(dd.get_ref_file(data)) if chromhacks.is_sex(x.name)]\n            gender_args = \"--sex %s\" % (\",\".join(gender_chroms)) if gender_chroms else \"\"\n            cmd = \"goleft indexcov --directory {tmp_dir} {gender_args} -- {bam_file}\"\n            try:\n                do.run(cmd.format(**locals()), \"QC: goleft indexcov\")\n            except subprocess.CalledProcessError as msg:\n                if not (\"indexcov: no usable\" in str(msg) or\n                        (\"indexcov: expected\" in str(msg) and \"sex chromosomes, found:\" in str(msg))):\n                    raise\n            for out_file in out_files:\n                orig_file = os.path.join(tmp_dir, os.path.basename(out_file))\n                if utils.file_exists(orig_file):\n                    utils.copy_plus(orig_file, out_file)\n    # MultiQC needs non-gzipped/BED inputs so unpack the file\n    out_bed = out_files[-1].replace(\".bed.gz\", \".tsv\")\n    if utils.file_exists(out_files[-1]) and not utils.file_exists(out_bed):\n        with transaction.file_transaction(data, out_bed) as tx_out_bed:\n            cmd = \"gunzip -c %s > %s\" % (out_files[-1], tx_out_bed)\n            do.run(cmd, \"Unpack indexcov BED file\")\n    out_files[-1] = out_bed\n    return [x for x in out_files if utils.file_exists(x)]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef picard_sort(picard, align_bam, sort_order=\"coordinate\",\n                out_file=None, compression_level=None, pipe=False):\n    \"\"\"Sort a BAM file by coordinates.\n    \"\"\"\n    base, ext = os.path.splitext(align_bam)\n    if out_file is None:\n        out_file = \"%s-sort%s\" % (base, ext)\n    if not file_exists(out_file):\n        with tx_tmpdir(picard._config) as tmp_dir:\n            with file_transaction(picard._config, out_file) as tx_out_file:\n                opts = [(\"INPUT\", align_bam),\n                        (\"OUTPUT\", out_file if pipe else tx_out_file),\n                        (\"TMP_DIR\", tmp_dir),\n                        (\"SORT_ORDER\", sort_order)]\n                if compression_level:\n                    opts.append((\"COMPRESSION_LEVEL\", compression_level))\n                picard.run(\"SortSam\", opts, pipe=pipe)\n    return out_file", "response": "Sort a BAM file by coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef picard_merge(picard, in_files, out_file=None,\n                 merge_seq_dicts=False):\n    \"\"\"Merge multiple BAM files together with Picard.\n    \"\"\"\n    if out_file is None:\n        out_file = \"%smerge.bam\" % os.path.commonprefix(in_files)\n    if not file_exists(out_file):\n        with tx_tmpdir(picard._config) as tmp_dir:\n            with file_transaction(picard._config, out_file) as tx_out_file:\n                opts = [(\"OUTPUT\", tx_out_file),\n                        (\"SORT_ORDER\", \"coordinate\"),\n                        (\"MERGE_SEQUENCE_DICTIONARIES\",\n                         \"true\" if merge_seq_dicts else \"false\"),\n                        (\"USE_THREADING\", \"true\"),\n                        (\"TMP_DIR\", tmp_dir)]\n                for in_file in in_files:\n                    opts.append((\"INPUT\", in_file))\n                picard.run(\"MergeSamFiles\", opts)\n    return out_file", "response": "Merge multiple BAM files together with Picard.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreordering BAM file to match reference file ordering.", "response": "def picard_reorder(picard, in_bam, ref_file, out_file):\n    \"\"\"Reorder BAM file to match reference file ordering.\n    \"\"\"\n    if not file_exists(out_file):\n        with tx_tmpdir(picard._config) as tmp_dir:\n            with file_transaction(picard._config, out_file) as tx_out_file:\n                opts = [(\"INPUT\", in_bam),\n                        (\"OUTPUT\", tx_out_file),\n                        (\"REFERENCE\", ref_file),\n                        (\"ALLOW_INCOMPLETE_DICT_CONCORDANCE\", \"true\"),\n                        (\"TMP_DIR\", tmp_dir)]\n                picard.run(\"ReorderSam\", opts)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds read group information to BAM files and coordinate sort.", "response": "def picard_fix_rgs(picard, in_bam, names):\n    \"\"\"Add read group information to BAM files and coordinate sort.\n    \"\"\"\n    out_file = \"%s-fixrgs.bam\" % os.path.splitext(in_bam)[0]\n    if not file_exists(out_file):\n        with tx_tmpdir(picard._config) as tmp_dir:\n            with file_transaction(picard._config, out_file) as tx_out_file:\n                opts = [(\"INPUT\", in_bam),\n                        (\"OUTPUT\", tx_out_file),\n                        (\"SORT_ORDER\", \"coordinate\"),\n                        (\"RGID\", names[\"rg\"]),\n                        (\"RGLB\", names.get(\"lb\", \"unknown\")),\n                        (\"RGPL\", names[\"pl\"]),\n                        (\"RGPU\", names[\"pu\"]),\n                        (\"RGSM\", names[\"sample\"]),\n                        (\"TMP_DIR\", tmp_dir)]\n                picard.run(\"AddOrReplaceReadGroups\", opts)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef picard_index_ref(picard, ref_file):\n    dict_file = \"%s.dict\" % os.path.splitext(ref_file)[0]\n    if not file_exists(dict_file):\n        with file_transaction(picard._config, dict_file) as tx_dict_file:\n            opts = [(\"REFERENCE\", ref_file),\n                    (\"OUTPUT\", tx_dict_file)]\n            picard.run(\"CreateSequenceDictionary\", opts)\n    return dict_file", "response": "Provide a Picard style dict index file for a reference genome."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef picard_fastq_to_bam(picard, fastq_one, fastq_two, out_dir, names, order=\"queryname\"):\n    out_bam = os.path.join(out_dir, \"%s-fastq.bam\" %\n                           os.path.splitext(os.path.basename(fastq_one))[0])\n    if not file_exists(out_bam):\n        with tx_tmpdir(picard._config) as tmp_dir:\n            with file_transaction(picard._config, out_bam) as tx_out_bam:\n                opts = [(\"FASTQ\", fastq_one),\n                        (\"READ_GROUP_NAME\", names[\"rg\"]),\n                        (\"SAMPLE_NAME\", names[\"sample\"]),\n                        (\"PLATFORM_UNIT\", names[\"pu\"]),\n                        (\"PLATFORM\", names[\"pl\"]),\n                        (\"TMP_DIR\", tmp_dir),\n                        (\"OUTPUT\", tx_out_bam),\n                        (\"SORT_ORDER\", order)]\n                if fastq_two:\n                    opts.append((\"FASTQ2\", fastq_two))\n                picard.run(\"FastqToSam\", opts)\n    return out_bam", "response": "Convert fastq files to BAM."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts BAM file to fastq.", "response": "def picard_bam_to_fastq(picard, in_bam, fastq_one, fastq_two=None):\n    \"\"\"Convert BAM file to fastq.\n    \"\"\"\n    if not file_exists(fastq_one):\n        with tx_tmpdir(picard._config) as tmp_dir:\n            with file_transaction(picard._config, fastq_one) as tx_out1:\n                opts = [(\"INPUT\", in_bam),\n                        (\"FASTQ\", tx_out1),\n                        (\"TMP_DIR\", tmp_dir)]\n                if fastq_two is not None:\n                    opts += [(\"SECOND_END_FASTQ\", fastq_two)]\n                picard.run(\"SamToFastq\", opts)\n    return (fastq_one, fastq_two)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef picard_sam_to_bam(picard, align_sam, fastq_bam, ref_file,\n                      is_paired=False):\n    \"\"\"Convert SAM to BAM, including unmapped reads from fastq BAM file.\n    \"\"\"\n    to_retain = [\"XS\", \"XG\", \"XM\", \"XN\", \"XO\", \"YT\"]\n    if align_sam.endswith(\".sam\"):\n        out_bam = \"%s.bam\" % os.path.splitext(align_sam)[0]\n    elif align_sam.endswith(\"-align.bam\"):\n        out_bam = \"%s.bam\" % align_sam.replace(\"-align.bam\", \"\")\n    else:\n        raise NotImplementedError(\"Input format not recognized\")\n    if not file_exists(out_bam):\n        with tx_tmpdir(picard._config) as tmp_dir:\n            with file_transaction(picard._config, out_bam) as tx_out_bam:\n                opts = [(\"UNMAPPED\", fastq_bam),\n                        (\"ALIGNED\", align_sam),\n                        (\"OUTPUT\", tx_out_bam),\n                        (\"REFERENCE_SEQUENCE\", ref_file),\n                        (\"TMP_DIR\", tmp_dir),\n                        (\"PAIRED_RUN\", (\"true\" if is_paired else \"false\")),\n                        ]\n                opts += [(\"ATTRIBUTES_TO_RETAIN\", x) for x in to_retain]\n                picard.run(\"MergeBamAlignment\", opts)\n    return out_bam", "response": "Convert SAM to BAM."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts aligned SAM file to BAM format.", "response": "def picard_formatconverter(picard, align_sam):\n    \"\"\"Convert aligned SAM file to BAM format.\n    \"\"\"\n    out_bam = \"%s.bam\" % os.path.splitext(align_sam)[0]\n    if not file_exists(out_bam):\n        with tx_tmpdir(picard._config) as tmp_dir:\n            with file_transaction(picard._config, out_bam) as tx_out_bam:\n                opts = [(\"INPUT\", align_sam),\n                        (\"OUTPUT\", tx_out_bam),\n                        (\"TMP_DIR\", tmp_dir)]\n                picard.run(\"SamFormatConverter\", opts)\n    return out_bam"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef picard_fixmate(picard, align_bam):\n    base, ext = os.path.splitext(align_bam)\n    out_file = \"%s-sort%s\" % (base, ext)\n    if not file_exists(out_file):\n        with tx_tmpdir(picard._config) as tmp_dir:\n            with file_transaction(picard._config, out_file) as tx_out_file:\n                opts = [(\"INPUT\", align_bam),\n                        (\"OUTPUT\", tx_out_file),\n                        (\"TMP_DIR\", tmp_dir),\n                        (\"SORT_ORDER\", \"coordinate\")]\n                picard.run(\"FixMateInformation\", opts)\n    return out_file", "response": "Run Picard s FixMateInformation generating an aligned output file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef picard_idxstats(picard, align_bam):\n    opts = [(\"INPUT\", align_bam)]\n    stdout = picard.run(\"BamIndexStats\", opts, get_stdout=True)\n    out = []\n    AlignInfo = collections.namedtuple(\"AlignInfo\", [\"contig\", \"length\", \"aligned\", \"unaligned\"])\n    for line in stdout.split(\"\\n\"):\n        if line:\n            parts = line.split()\n            if len(parts) == 2:\n                _, unaligned = parts\n                out.append(AlignInfo(\"nocontig\", 0, 0, int(unaligned)))\n            elif len(parts) == 7:\n                contig, _, length, _, aligned, _, unaligned = parts\n                out.append(AlignInfo(contig, int(length), int(aligned), int(unaligned)))\n            else:\n                raise ValueError(\"Unexpected output from BamIndexStats: %s\" % line)\n    return out", "response": "Retrieve alignment stats from picard using BamIndexStats.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a bed file to an interval file for use with some of the Picard tools.", "response": "def bed2interval(align_file, bed, out_file=None):\n    \"\"\"Converts a bed file to an interval file for use with some of the\n    Picard tools by grabbing the header from the alignment file, reording\n    the bed file columns and gluing them together.\n\n    align_file can be in BAM or SAM format.\n    bed needs to be in bed12 format:\n    http://genome.ucsc.edu/FAQ/FAQformat.html#format1.5\n\n    \"\"\"\n    import pysam\n    base, ext = os.path.splitext(align_file)\n    if out_file is None:\n        out_file = base + \".interval\"\n\n    with pysam.Samfile(align_file, \"r\" if ext.endswith(\".sam\") else \"rb\") as in_bam:\n        header = in_bam.text\n\n    def reorder_line(line):\n        splitline = line.strip().split(\"\\t\")\n        reordered = \"\\t\".join([splitline[0], str(int(splitline[1]) + 1), splitline[2],\n                               splitline[5], splitline[3]])\n        return reordered + \"\\n\"\n\n    with file_transaction(out_file) as tx_out_file:\n        with open(bed) as bed_handle:\n            with open(tx_out_file, \"w\") as out_handle:\n                out_handle.write(header)\n                for line in bed_handle:\n                    out_handle.write(reorder_line(line))\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd frequency options to the options list", "response": "def _add_freq_options(config, opts, var2vcf_opts):\n    \"\"\" Setting -f option for vardict and var2vcf_valid\n        Prioritizing settings in resources/vardict/options, then algorithm/min_allele_fraction:\n    min_allele_fraction   \"-f\" in opts  var2vcfopts   ->   vardict -f            var2vcf -f\n    yes                           yes   yes                opts                  var2vcfopts\n    yes                           yes   -                  opts                  -\n    yes                           -     yes                min_allele_fraction   var2vcfopts\n    yes                           -     -                  min_allele_fraction   min_allele_fraction\n    default                       yes   yes                opts                  var2vcfopts\n    default                       yes   -                  opts                  -\n    default                       -     yes                min_allele_fraction   var2vcfopts\n    default                       -     -                  min_allele_fraction   min_allele_fraction\n    \"\"\"\n    if \"-f\" not in opts:\n        freq = float(utils.get_in(config, (\"algorithm\", \"min_allele_fraction\"), 10)) / 100.0\n        opts.extend([\"-f\", str(freq)])\n        if \"-f\" not in var2vcf_opts:\n            var2vcf_opts.extend([\"-f\", str(freq)])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nensure we don t have any chunks in the same size as the input file.", "response": "def _enforce_max_region_size(in_file, data):\n    \"\"\"Ensure we don't have any chunks in the region greater than 20kb.\n\n    VarDict memory usage depends on size of individual windows in the input\n    file. This breaks regions into 20kb chunks with 250bp overlaps. 20kb gives\n    ~1Gb/core memory usage and the overlaps avoid missing indels spanning a\n    gap. Downstream VarDict merging sorts out any variants across windows.\n\n    https://github.com/AstraZeneca-NGS/VarDictJava/issues/64\n    \"\"\"\n    max_size = 20000\n    overlap_size = 250\n    def _has_larger_regions(f):\n        return any(r.stop - r.start > max_size for r in pybedtools.BedTool(f))\n    out_file = \"%s-regionlimit%s\" % utils.splitext_plus(in_file)\n    if not utils.file_exists(out_file):\n        if _has_larger_regions(in_file):\n            with file_transaction(data, out_file) as tx_out_file:\n                pybedtools.BedTool().window_maker(w=max_size,\n                                                  s=max_size - overlap_size,\n                                                  b=pybedtools.BedTool(in_file)).saveas(tx_out_file)\n        else:\n            utils.symlink_plus(in_file, out_file)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run_vardict(align_bams, items, ref_file, assoc_files, region=None,\n                out_file=None):\n    \"\"\"Run VarDict variant calling.\n    \"\"\"\n    items = shared.add_highdepth_genome_exclusion(items)\n    if vcfutils.is_paired_analysis(align_bams, items):\n        call_file = _run_vardict_paired(align_bams, items, ref_file,\n                                        assoc_files, region, out_file)\n    else:\n        vcfutils.check_paired_problems(items)\n        call_file = _run_vardict_caller(align_bams, items, ref_file,\n                                        assoc_files, region, out_file)\n    return call_file", "response": "Run VarDict variant calling."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_jvm_opts(data, out_file):\n    if get_vardict_command(data) == \"vardict-java\":\n        resources = config_utils.get_resources(\"vardict\", data[\"config\"])\n        jvm_opts = resources.get(\"jvm_opts\", [\"-Xms750m\", \"-Xmx4g\"])\n        jvm_opts += broad.get_default_jvm_opts(os.path.dirname(out_file))\n        return \"export VAR_DICT_OPTS='%s' && \" % \" \".join(jvm_opts)\n    else:\n        return \"\"", "response": "Retrieve JVM options when running the Java version of VarDict."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns VarDict caller on the given set of BAM files.", "response": "def _run_vardict_caller(align_bams, items, ref_file, assoc_files,\n                          region=None, out_file=None):\n    \"\"\"Detect SNPs and indels with VarDict.\n\n    var2vcf_valid uses -A flag which reports all alleles and improves sensitivity:\n    https://github.com/AstraZeneca-NGS/VarDict/issues/35#issuecomment-276738191\n    \"\"\"\n    config = items[0][\"config\"]\n    if out_file is None:\n        out_file = \"%s-variants.vcf.gz\" % os.path.splitext(align_bams[0])[0]\n    if not utils.file_exists(out_file):\n        with file_transaction(items[0], out_file) as tx_out_file:\n            vrs = bedutils.population_variant_regions(items)\n            target = shared.subset_variant_regions(\n                vrs, region, out_file, items=items, do_merge=False)\n            num_bams = len(align_bams)\n            sample_vcf_names = []  # for individual sample names, given batch calling may be required\n            for bamfile, item in zip(align_bams, items):\n                # prepare commands\n                sample = dd.get_sample_name(item)\n                vardict = get_vardict_command(items[0])\n                opts, var2vcf_opts = _vardict_options_from_config(items, config, out_file, target)\n                vcfstreamsort = config_utils.get_program(\"vcfstreamsort\", config)\n                compress_cmd = \"| bgzip -c\" if tx_out_file.endswith(\"gz\") else \"\"\n                fix_ambig_ref = vcfutils.fix_ambiguous_cl()\n                fix_ambig_alt = vcfutils.fix_ambiguous_cl(5)\n                remove_dup = vcfutils.remove_dup_cl()\n                py_cl = os.path.join(utils.get_bcbio_bin(), \"py\")\n                jvm_opts = _get_jvm_opts(items[0], tx_out_file)\n                setup = (\"%s && unset JAVA_HOME &&\" % utils.get_R_exports())\n                contig_cl = vcfutils.add_contig_to_header_cl(ref_file, tx_out_file)\n                lowfreq_filter = _lowfreq_linear_filter(0, False)\n                cmd = (\"{setup}{jvm_opts}{vardict} -G {ref_file} \"\n                       \"-N {sample} -b {bamfile} {opts} \"\n                       \"| teststrandbias.R \"\n                       \"| var2vcf_valid.pl -A -N {sample} -E {var2vcf_opts} \"\n                       \"| {contig_cl} | bcftools filter -i 'QUAL >= 0' | {lowfreq_filter} \"\n                       \"| {fix_ambig_ref} | {fix_ambig_alt} | {remove_dup} | {vcfstreamsort} {compress_cmd}\")\n                if num_bams > 1:\n                    temp_file_prefix = out_file.replace(\".gz\", \"\").replace(\".vcf\", \"\") + item[\"name\"][1]\n                    tmp_out = temp_file_prefix + \".temp.vcf\"\n                    tmp_out += \".gz\" if out_file.endswith(\"gz\") else \"\"\n                    sample_vcf_names.append(tmp_out)\n                    with file_transaction(item, tmp_out) as tx_tmp_file:\n                        if not _is_bed_file(target):\n                            vcfutils.write_empty_vcf(tx_tmp_file, config, samples=[sample])\n                        else:\n                            cmd += \" > {tx_tmp_file}\"\n                            do.run(cmd.format(**locals()), \"Genotyping with VarDict: Inference\", {})\n                else:\n                    if not _is_bed_file(target):\n                        vcfutils.write_empty_vcf(tx_out_file, config, samples=[sample])\n                    else:\n                        cmd += \" > {tx_out_file}\"\n                        do.run(cmd.format(**locals()), \"Genotyping with VarDict: Inference\", {})\n            if num_bams > 1:\n                # N.B. merge_variant_files wants region in 1-based end-inclusive\n                # coordinates. Thus use bamprep.region_to_gatk\n                vcfutils.merge_variant_files(orig_files=sample_vcf_names,\n                                             out_file=tx_out_file, ref_file=ref_file,\n                                             config=config, region=bamprep.region_to_gatk(region))\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _lowfreq_linear_filter(tumor_index, is_paired):\n    if is_paired:\n        sbf = \"FORMAT/SBF[%s]\" % tumor_index\n        nm = \"FORMAT/NM[%s]\" % tumor_index\n    else:\n        sbf = \"INFO/SBF\"\n        nm = \"INFO/NM\"\n    cmd = (\"\"\"bcftools filter --soft-filter 'LowFreqBias' --mode '+' \"\"\"\n           \"\"\"-e  'FORMAT/AF[{tumor_index}] < 0.02 && FORMAT/VD[{tumor_index}] < 30 \"\"\"\n           \"\"\"&& {sbf} < 0.1 && {nm} >= 2.0'\"\"\")\n    return cmd.format(**locals())", "response": "Linear classifier for removing low frequency false positives."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a DB flag for Germline filters allowing downstream compatibility with PureCN.", "response": "def add_db_germline_flag(line):\n    \"\"\"Adds a DB flag for Germline filters, allowing downstream compatibility with PureCN.\n    \"\"\"\n    if line.startswith(\"#CHROM\"):\n        headers = ['##INFO=<ID=DB,Number=0,Type=Flag,Description=\"Likely germline variant\">']\n        return \"\\n\".join(headers) + \"\\n\" + line\n    elif line.startswith(\"#\"):\n        return line\n    else:\n        parts = line.split(\"\\t\")\n        if parts[7].find(\"STATUS=Germline\") >= 0:\n            parts[7] += \";DB\"\n        return \"\\t\".join(parts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef depth_freq_filter(line, tumor_index, aligner):\n    if line.startswith(\"#CHROM\"):\n        headers = [('##FILTER=<ID=LowAlleleDepth,Description=\"Low depth per allele frequency '\n                    'along with poor depth, quality, mapping quality and read mismatches.\">'),\n                   ('##FILTER=<ID=LowFreqQuality,Description=\"Low frequency read with '\n                    'poor quality and p-value (SSF).\">')]\n        return \"\\n\".join(headers) + \"\\n\" + line\n    elif line.startswith(\"#\"):\n        return line\n    else:\n        parts = line.split(\"\\t\")\n        sample_ft = {a: v for (a, v) in zip(parts[8].split(\":\"), parts[9 + tumor_index].split(\":\"))}\n        qual = utils.safe_to_float(parts[5])\n        dp = utils.safe_to_float(sample_ft.get(\"DP\"))\n        af = utils.safe_to_float(sample_ft.get(\"AF\"))\n        nm = utils.safe_to_float(sample_ft.get(\"NM\"))\n        mq = utils.safe_to_float(sample_ft.get(\"MQ\"))\n        ssfs = [x for x in parts[7].split(\";\") if x.startswith(\"SSF=\")]\n        pval = utils.safe_to_float(ssfs[0].split(\"=\")[-1] if ssfs else None)\n        fname = None\n        if not chromhacks.is_sex(parts[0]) and dp is not None and af is not None:\n            if dp * af < 6:\n                if aligner == \"bwa\" and nm is not None and mq is not None:\n                    if (mq < 55.0 and nm > 1.0) or (mq < 60.0 and nm > 2.0):\n                        fname = \"LowAlleleDepth\"\n                if dp < 10:\n                    fname = \"LowAlleleDepth\"\n                if qual is not None and qual < 45:\n                    fname = \"LowAlleleDepth\"\n        if af is not None and qual is not None and pval is not None:\n            if af < 0.2 and qual < 45 and pval > 0.06:\n                fname = \"LowFreqQuality\"\n        if fname:\n            if parts[6] in set([\".\", \"PASS\"]):\n                parts[6] = fname\n            else:\n                parts[6] += \";%s\" % fname\n        line = \"\\t\".join(parts)\n        return line", "response": "Filter VarDict calls based on depth frequency and quality."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _run_vardict_paired(align_bams, items, ref_file, assoc_files,\n                          region=None, out_file=None):\n    \"\"\"Detect variants with Vardict.\n\n    This is used for paired tumor / normal samples.\n    \"\"\"\n    config = items[0][\"config\"]\n    if out_file is None:\n        out_file = \"%s-paired-variants.vcf.gz\" % os.path.splitext(align_bams[0])[0]\n    if not utils.file_exists(out_file):\n        with file_transaction(items[0], out_file) as tx_out_file:\n            vrs = bedutils.population_variant_regions(items)\n            target = shared.subset_variant_regions(vrs, region,\n                                                   out_file, items=items, do_merge=True)\n            paired = vcfutils.get_paired_bams(align_bams, items)\n            if not _is_bed_file(target):\n                vcfutils.write_empty_vcf(tx_out_file, config,\n                                         samples=[x for x in [paired.tumor_name, paired.normal_name] if x])\n            else:\n                if not paired.normal_bam:\n                    ann_file = _run_vardict_caller(align_bams, items, ref_file,\n                                                   assoc_files, region, out_file)\n                    return ann_file\n                vardict = get_vardict_command(items[0])\n                vcfstreamsort = config_utils.get_program(\"vcfstreamsort\", config)\n                compress_cmd = \"| bgzip -c\" if out_file.endswith(\"gz\") else \"\"\n                freq = float(utils.get_in(config, (\"algorithm\", \"min_allele_fraction\"), 10)) / 100.0\n                # merge bed file regions as amplicon VarDict is only supported in single sample mode\n                opts, var2vcf_opts = _vardict_options_from_config(items, config, out_file, target)\n                fix_ambig_ref = vcfutils.fix_ambiguous_cl()\n                fix_ambig_alt = vcfutils.fix_ambiguous_cl(5)\n                remove_dup = vcfutils.remove_dup_cl()\n                if any(\"vardict_somatic_filter\" in tz.get_in((\"config\", \"algorithm\", \"tools_off\"), data, [])\n                       for data in items):\n                    somatic_filter = \"\"\n                    freq_filter = \"\"\n                else:\n                    var2vcf_opts += \" -M \"  # this makes VarDict soft filter non-differential variants\n                    somatic_filter = (\"| sed 's/\\\\\\\\.*Somatic\\\\\\\\/Somatic/' \"\n                                      \"| sed 's/REJECT,Description=\\\".*\\\">/REJECT,Description=\\\"Not Somatic via VarDict\\\">/' \"\n                                      \"\"\"| %s -c 'from bcbio.variation import freebayes; \"\"\"\n                                      \"\"\"freebayes.call_somatic(\"%s\", \"%s\")' \"\"\"\n                                      % (sys.executable, paired.tumor_name, paired.normal_name))\n                    freq_filter = (\"| bcftools filter -m '+' -s 'REJECT' -e 'STATUS !~ \\\".*Somatic\\\"' 2> /dev/null \"\n                                   \"| %s -x 'bcbio.variation.vardict.add_db_germline_flag(x)' \"\n                                   \"| %s \"\n                                   \"| %s -x 'bcbio.variation.vardict.depth_freq_filter(x, %s, \\\"%s\\\")'\" %\n                                   (os.path.join(os.path.dirname(sys.executable), \"py\"),\n                                    _lowfreq_linear_filter(0, True),\n                                    os.path.join(os.path.dirname(sys.executable), \"py\"),\n                                    0, bam.aligner_from_header(paired.tumor_bam)))\n                jvm_opts = _get_jvm_opts(items[0], tx_out_file)\n                py_cl = os.path.join(utils.get_bcbio_bin(), \"py\")\n                setup = (\"%s && unset JAVA_HOME &&\" % utils.get_R_exports())\n                contig_cl = vcfutils.add_contig_to_header_cl(ref_file, tx_out_file)\n                cmd = (\"{setup}{jvm_opts}{vardict} -G {ref_file} \"\n                       \"-N {paired.tumor_name} -b \\\"{paired.tumor_bam}|{paired.normal_bam}\\\" {opts} \"\n                       \"| awk 'NF>=48' | testsomatic.R \"\n                       \"| var2vcf_paired.pl -P 0.9 -m 4.25 {var2vcf_opts} \"\n                       \"-N \\\"{paired.tumor_name}|{paired.normal_name}\\\" \"\n                       \"| {contig_cl} {freq_filter} \"\n                       \"| bcftools filter -i 'QUAL >= 0' \"\n                       \"{somatic_filter} | {fix_ambig_ref} | {fix_ambig_alt} | {remove_dup} | {vcfstreamsort} \"\n                       \"{compress_cmd} > {tx_out_file}\")\n                do.run(cmd.format(**locals()), \"Genotyping with VarDict: Inference\", {})\n    return out_file", "response": "Detect variants with Vardict."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts variantcaller specification to proper vardict command handling string or list specification", "response": "def get_vardict_command(data):\n    \"\"\"\n    convert variantcaller specification to proper vardict command, handling\n    string or list specification\n    \"\"\"\n    vcaller = dd.get_variantcaller(data)\n    if isinstance(vcaller, list):\n        vardict = [x for x in vcaller if \"vardict\" in x]\n        if not vardict:\n            return None\n        vardict = vardict[0]\n    elif not vcaller:\n        return None\n    else:\n        vardict = vcaller\n    vardict = \"vardict-java\" if not vardict.endswith(\"-perl\") else \"vardict\"\n    return vardict"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prepare_rsem_reference(gtf, multifasta, build):\n    if not utils.which(\"rsem-prepare-reference\"):\n        logger.info(\"Skipping prepping RSEM reference because \"\n                    \"rsem-prepare-reference could not be found.\")\n        return None\n\n    command = PREPARE_REFERENCE.format(gtf=gtf, multifasta=multifasta,\n                                       build=build)\n    with transaction.tx_tmpdir(remove=False) as rsem_genome_dir:\n        with utils.chdir(rsem_genome_dir):\n            message = \"Preparing rsem reference from %s\" % gtf\n            do.run(command, message)\n    return rsem_genome_dir", "response": "Prepares the rsem reference for the given GTF and multifasta."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rsem_calculate_expression(bam_file, rsem_genome_dir, samplename,\n                              build, out_dir, cores=1):\n    \"\"\"\n    works only in unstranded mode for now (--forward-prob 0.5)\n    \"\"\"\n    if not utils.which(\"rsem-calculate-expression\"):\n        logger.info(\"Skipping RSEM because rsem-calculate-expression could \"\n                    \"not be found.\")\n        return None\n\n    sentinel_file = os.path.join(out_dir, samplename + \"Test.genes.results\")\n    if utils.file_exists(sentinel_file):\n        return out_dir\n\n    paired_flag = \"--paired\" if bam.is_paired(bam_file) else \"\"\n    core_flag = \"-p {cores}\".format(cores=cores)\n    command = CALCULATE_EXP.format(\n        core_flag=core_flag, paired_flag=paired_flag, bam_file=bam_file,\n        rsem_genome_dir=rsem_genome_dir, build=build, samplename=samplename)\n    message = \"Calculating transcript expression of {bam_file} using RSEM.\"\n\n    with transaction.file_transaction(out_dir) as tx_out_dir:\n        utils.safe_makedir(tx_out_dir)\n        with utils.chdir(tx_out_dir):\n            do.run(command, message.format(bam_file=bam_file))\n    return out_dir", "response": "Calculate expression of a single transcript using RSEM."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning BubbleTree given variant calls CNVs and somatic_info.", "response": "def run(vrn_info, calls_by_name, somatic_info, do_plots=True, handle_failures=True):\n    \"\"\"Run BubbleTree given variant calls, CNVs and somatic\n    \"\"\"\n    if \"seq2c\" in calls_by_name:\n        cnv_info = calls_by_name[\"seq2c\"]\n    elif \"cnvkit\" in calls_by_name:\n        cnv_info = calls_by_name[\"cnvkit\"]\n    else:\n        raise ValueError(\"BubbleTree only currently support CNVkit and Seq2c: %s\" % \", \".join(calls_by_name.keys()))\n    work_dir = _cur_workdir(somatic_info.tumor_data)\n    class OutWriter:\n        def __init__(self, out_handle):\n            self.writer = csv.writer(out_handle)\n        def write_header(self):\n            self.writer.writerow([\"chrom\", \"start\", \"end\", \"freq\"])\n        def write_row(self, rec, stats):\n            self.writer.writerow([_to_ucsc_style(rec.chrom), rec.start, rec.stop, stats[\"tumor\"][\"freq\"]])\n    vcf_csv = prep_vrn_file(vrn_info[\"vrn_file\"], vrn_info[\"variantcaller\"],\n                            work_dir, somatic_info, OutWriter, cnv_info[\"cns\"])\n    cnv_csv = _prep_cnv_file(cnv_info[\"cns\"], cnv_info[\"variantcaller\"], work_dir,\n                             somatic_info.tumor_data)\n    wide_lrr = cnv_info[\"variantcaller\"] == \"cnvkit\" and somatic_info.normal_bam is None\n    return _run_bubbletree(vcf_csv, cnv_csv, somatic_info.tumor_data, wide_lrr, do_plots,\n                           handle_failures)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun BubbleTree on input vcf file.", "response": "def _run_bubbletree(vcf_csv, cnv_csv, data, wide_lrr=False, do_plots=True,\n                    handle_failures=True):\n    \"\"\"Create R script and run on input data\n\n    BubbleTree has some internal hardcoded paramters that assume a smaller\n    distribution of log2 scores. This is not true for tumor-only calls, so if\n    we specify wide_lrr we scale the calculations to actually get calls. Need a\n    better long term solution with flexible parameters.\n    \"\"\"\n    lrr_scale = 10.0 if wide_lrr else 1.0\n    local_sitelib = utils.R_sitelib()\n    base = utils.splitext_plus(vcf_csv)[0]\n    r_file = \"%s-run.R\" % base\n    bubbleplot_out = \"%s-bubbleplot.pdf\" % base\n    trackplot_out = \"%s-trackplot.pdf\" % base\n    calls_out = \"%s-calls.rds\" % base\n    freqs_out = \"%s-bubbletree_prevalence.txt\" % base\n    sample = dd.get_sample_name(data)\n    do_plots = \"yes\" if do_plots else \"no\"\n    with open(r_file, \"w\") as out_handle:\n        out_handle.write(_script.format(**locals()))\n    if not utils.file_exists(freqs_out):\n        cmd = \"%s && %s --no-environ %s\" % (utils.get_R_exports(), utils.Rscript_cmd(), r_file)\n        try:\n            do.run(cmd, \"Assess heterogeneity with BubbleTree\")\n        except subprocess.CalledProcessError as msg:\n            if handle_failures and _allowed_bubbletree_errorstates(str(msg)):\n                with open(freqs_out, \"w\") as out_handle:\n                    out_handle.write('bubbletree failed:\\n %s\"\\n' % (str(msg)))\n            else:\n                logger.exception()\n                raise\n    return {\"caller\": \"bubbletree\",\n            \"report\": freqs_out,\n            \"plot\": {\"bubble\": bubbleplot_out, \"track\": trackplot_out}}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _prep_cnv_file(cns_file, svcaller, work_dir, data):\n    in_file = cns_file\n    out_file = os.path.join(work_dir, \"%s-%s-prep.csv\" % (utils.splitext_plus(os.path.basename(in_file))[0],\n                                                          svcaller))\n    if not utils.file_uptodate(out_file, in_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            with open(in_file) as in_handle:\n                with open(tx_out_file, \"w\") as out_handle:\n                    reader = csv.reader(in_handle, dialect=\"excel-tab\")\n                    writer = csv.writer(out_handle)\n                    writer.writerow([\"chrom\", \"start\", \"end\", \"num.mark\", \"seg.mean\"])\n                    header = next(reader)\n                    for line in reader:\n                        cur = dict(zip(header, line))\n                        if chromhacks.is_autosomal(cur[\"chromosome\"]):\n                            writer.writerow([_to_ucsc_style(cur[\"chromosome\"]), cur[\"start\"],\n                                             cur[\"end\"], cur[\"probes\"], cur[\"log2\"]])\n    return out_file", "response": "Create a CSV file of CNV calls with log2 and number of marks."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prep_vrn_file(in_file, vcaller, work_dir, somatic_info, writer_class, seg_file=None, params=None):\n    data = somatic_info.tumor_data\n    if not params:\n        params = PARAMS\n    out_file = os.path.join(work_dir, \"%s-%s-prep.csv\" % (utils.splitext_plus(os.path.basename(in_file))[0],\n                                                          vcaller))\n    if not utils.file_uptodate(out_file, in_file):\n        # ready_bed = _identify_heterogeneity_blocks_seg(in_file, seg_file, params, work_dir, somatic_info)\n        ready_bed = None\n        if ready_bed and utils.file_exists(ready_bed):\n            sub_file = _create_subset_file(in_file, ready_bed, work_dir, data)\n        else:\n            sub_file = in_file\n        max_depth = max_normal_germline_depth(sub_file, params, somatic_info)\n        with file_transaction(data, out_file) as tx_out_file:\n            with open(tx_out_file, \"w\") as out_handle:\n                writer = writer_class(out_handle)\n                writer.write_header()\n                bcf_in = pysam.VariantFile(sub_file)\n                for rec in bcf_in:\n                    stats = _is_possible_loh(rec, bcf_in, params, somatic_info, max_normal_depth=max_depth)\n                    if chromhacks.is_autosomal(rec.chrom) and stats is not None:\n                        writer.write_row(rec, stats)\n    return out_file", "response": "Prepare a single VCF file for heterozygous variant calling."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef max_normal_germline_depth(in_file, params, somatic_info):\n    bcf_in = pysam.VariantFile(in_file)\n    depths = []\n    for rec in bcf_in:\n        stats = _is_possible_loh(rec, bcf_in, params, somatic_info)\n        if tz.get_in([\"normal\", \"depth\"], stats):\n            depths.append(tz.get_in([\"normal\", \"depth\"], stats))\n    if depths:\n        return np.median(depths) * NORMAL_FILTER_PARAMS[\"max_depth_percent\"]", "response": "Calculate the maximum germline depth for a single normal variant."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _identify_heterogeneity_blocks_seg(in_file, seg_file, params, work_dir, somatic_info):\n    def _segment_by_cns(target_chrom, freqs, coords):\n        with open(seg_file) as in_handle:\n            reader = csv.reader(in_handle, dialect=\"excel-tab\")\n            next(reader)  # header\n            for cur_chrom, start, end in (xs[:3] for xs in reader):\n                if cur_chrom == target_chrom:\n                    block_freqs = []\n                    for i, (freq, coord) in enumerate(zip(freqs, coords)):\n                        if coord >= int(start) and coord < int(end):\n                            block_freqs.append(freq)\n                        elif coord >= int(end):\n                            break\n                    coords = coords[max(0, i - 1):]\n                    freqs = freqs[max(0, i - 1):]\n                    if len(block_freqs) > params[\"hetblock\"][\"min_alleles\"]:\n                        yield start, end\n    return _identify_heterogeneity_blocks_shared(in_file, _segment_by_cns, params, work_dir, somatic_info)", "response": "Identify heterogeneity blocks corresponding to segmentation from CNV input file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nidentifies blocks of heterogeneity in a HMM.", "response": "def _identify_heterogeneity_blocks_hmm(in_file, params, work_dir, somatic_info):\n    \"\"\"Use a HMM to identify blocks of heterogeneity to use for calculating allele frequencies.\n\n    The goal is to subset the genome to a more reasonable section that contains potential\n    loss of heterogeneity or other allele frequency adjustment based on selection.\n    \"\"\"\n    def _segment_by_hmm(chrom, freqs, coords):\n        cur_coords = []\n        for j, state in enumerate(_predict_states(freqs)):\n            if state == 0:  # heterozygote region\n                if len(cur_coords) == 0:\n                    num_misses = 0\n                cur_coords.append(coords[j])\n            else:\n                num_misses += 1\n            if num_misses > params[\"hetblock\"][\"allowed_misses\"]:\n                if len(cur_coords) >= params[\"hetblock\"][\"min_alleles\"]:\n                    yield min(cur_coords), max(cur_coords)\n                cur_coords = []\n        if len(cur_coords) >= params[\"hetblock\"][\"min_alleles\"]:\n            yield min(cur_coords), max(cur_coords)\n    return _identify_heterogeneity_blocks_shared(in_file, _segment_by_hmm, params, work_dir, somatic_info)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nidentifying heterogeneity blocks corresponding to segmentation from CNV input file.", "response": "def _identify_heterogeneity_blocks_shared(in_file, segment_fn, params, work_dir, somatic_info):\n    \"\"\"Identify heterogeneity blocks corresponding to segmentation from CNV input file.\n    \"\"\"\n    out_file = os.path.join(work_dir, \"%s-hetblocks.bed\" % utils.splitext_plus(os.path.basename(in_file))[0])\n    if not utils.file_uptodate(out_file, in_file):\n        with file_transaction(somatic_info.tumor_data, out_file) as tx_out_file:\n            with open(tx_out_file, \"w\") as out_handle:\n                for chrom, freqs, coords in _freqs_by_chromosome(in_file, params, somatic_info):\n                    for start, end in segment_fn(chrom, freqs, coords):\n                        out_handle.write(\"%s\\t%s\\t%s\\n\" % (chrom, start, end))\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _predict_states(freqs):\n    from hmmlearn import hmm\n    freqs = np.column_stack([np.array(freqs)])\n    model = hmm.GaussianHMM(2, covariance_type=\"full\")\n    model.fit(freqs)\n    states = model.predict(freqs)\n    freqs_by_state = collections.defaultdict(list)\n    for i, state in enumerate(states):\n        freqs_by_state[state].append(freqs[i])\n    if np.median(freqs_by_state[0]) > np.median(freqs_by_state[1]):\n        states = [0 if s == 1 else 1 for s in states]\n    return states", "response": "Use frequencies to predict states across a chromosome."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _freqs_by_chromosome(in_file, params, somatic_info):\n    freqs = []\n    coords = []\n    cur_chrom = None\n    with pysam.VariantFile(in_file) as bcf_in:\n        for rec in bcf_in:\n            if _is_biallelic_snp(rec) and _passes_plus_germline(rec) and chromhacks.is_autosomal(rec.chrom):\n                if cur_chrom is None or rec.chrom != cur_chrom:\n                    if cur_chrom and len(freqs) > 0:\n                        yield cur_chrom, freqs, coords\n                    cur_chrom = rec.chrom\n                    freqs = []\n                    coords = []\n                stats = _tumor_normal_stats(rec, somatic_info)\n                if tz.get_in([\"tumor\", \"depth\"], stats, 0) > params[\"min_depth\"]:\n                    # not a ref only call\n                    if len(rec.samples) == 0 or sum(rec.samples[somatic_info.tumor_name].allele_indices) > 0:\n                        freqs.append(tz.get_in([\"tumor\", \"freq\"], stats))\n                        coords.append(rec.start)\n        if cur_chrom and len(freqs) > 0:\n            yield cur_chrom, freqs, coords", "response": "Retrieve frequencies across each chromosome as inputs to HMM."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_subset_file(in_file, het_region_bed, work_dir, data):\n    cnv_regions = shared.get_base_cnv_regions(data, work_dir)\n    region_bed = bedutils.intersect_two(het_region_bed, cnv_regions, work_dir, data)\n    out_file = os.path.join(work_dir, \"%s-origsubset.bcf\" % utils.splitext_plus(os.path.basename(in_file))[0])\n    if not utils.file_uptodate(out_file, in_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            regions = (\"-R %s\" % region_bed) if utils.file_exists(region_bed) else \"\"\n            cmd = \"bcftools view {regions} -o {tx_out_file} -O b {in_file}\"\n            do.run(cmd.format(**locals()), \"Extract regions for BubbleTree frequency determination\")\n    return out_file", "response": "Subset the VCF to a set of pre - calculated smaller regions."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if a variant record is germline based on INFO attributes.", "response": "def is_info_germline(rec):\n    \"\"\"Check if a variant record is germline based on INFO attributes.\n\n    Works with VarDict's annotation of STATUS.\n    \"\"\"\n    if hasattr(rec, \"INFO\"):\n        status = rec.INFO.get(\"STATUS\", \"\").lower()\n    else:\n        status = rec.info.get(\"STATUS\", \"\").lower()\n    return status == \"germline\" or status.find(\"loh\") >= 0"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if a record passes filters plus germline.", "response": "def _passes_plus_germline(rec, use_status=False):\n    \"\"\"Check if a record passes filters (but might be germline -- labelled with REJECT).\n    \"\"\"\n    if use_status and is_info_germline(rec):\n        return True\n    allowed = set([\"PASS\", \"REJECT\", \".\"])\n    if hasattr(rec, \"FILTER\"):\n        if not rec.FILTER:\n            filters = []\n        else:\n            filters = [x for x in rec.FILTER.split(\";\") if x not in allowed]\n    else:\n        filters = [x for x in rec.filter.keys() if x not in allowed]\n    return len(filters) == 0"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving depth and frequency of tumor and normal samples.", "response": "def _tumor_normal_stats(rec, somatic_info, vcf_rec):\n    \"\"\"Retrieve depth and frequency of tumor and normal samples.\n    \"\"\"\n    out = {\"normal\": {\"alt\": None, \"depth\": None, \"freq\": None},\n           \"tumor\": {\"alt\": 0, \"depth\": 0, \"freq\": None}}\n    if hasattr(vcf_rec, \"samples\"):\n        samples = [(s, {}) for s in vcf_rec.samples]\n        for fkey in [\"AD\", \"AO\", \"RO\", \"AF\", \"DP\"]:\n            try:\n                for i, v in enumerate(rec.format(fkey)):\n                    samples[i][1][fkey] = v\n            except KeyError:\n                pass\n    # Handle INFO only inputs\n    elif len(rec.samples) == 0:\n        samples = [(somatic_info.tumor_name, None)]\n    else:\n        samples = rec.samples.items()\n    for name, sample in samples:\n        alt, depth, freq = sample_alt_and_depth(rec, sample)\n        if depth is not None and freq is not None:\n            if name == somatic_info.normal_name:\n                key = \"normal\"\n            elif name == somatic_info.tumor_name:\n                key = \"tumor\"\n            out[key][\"freq\"] = freq\n            out[key][\"depth\"] = depth\n            out[key][\"alt\"] = alt\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _is_possible_loh(rec, vcf_rec, params, somatic_info, use_status=False, max_normal_depth=None):\n    if _is_biallelic_snp(rec) and _passes_plus_germline(rec, use_status=use_status):\n        stats = _tumor_normal_stats(rec, somatic_info, vcf_rec)\n        depths = [tz.get_in([x, \"depth\"], stats) for x in [\"normal\", \"tumor\"]]\n        depths = [d for d in depths if d is not None]\n        normal_freq = tz.get_in([\"normal\", \"freq\"], stats)\n        tumor_freq = tz.get_in([\"tumor\", \"freq\"], stats)\n        if all([d > params[\"min_depth\"] for d in depths]):\n            if max_normal_depth and tz.get_in([\"normal\", \"depth\"], stats, 0) > max_normal_depth:\n                return None\n            if normal_freq is not None:\n                if normal_freq >= params[\"min_freq\"] and normal_freq <= params[\"max_freq\"]:\n                    return stats\n            elif (tumor_freq >= params[\"tumor_only\"][\"min_freq\"] and\n                    tumor_freq <= params[\"tumor_only\"][\"max_freq\"]):\n                if (vcf_rec and not _has_population_germline(vcf_rec)) or is_population_germline(rec):\n                    return stats", "response": "Check if the VCF record is a het in the normal with sufficient support."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _has_population_germline(rec):\n    for k in population_keys:\n        if k in rec.header.info:\n            return True\n    return False", "response": "Check if the header defines population annotated germline samples for tumor only."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nidentifying a germline call based on annoations with ExAC or other population databases.", "response": "def is_population_germline(rec):\n    \"\"\"Identify a germline calls based on annoations with ExAC or other population databases.\n    \"\"\"\n    min_count = 50\n    for k in population_keys:\n        if k in rec.info:\n            val = rec.info.get(k)\n            if \",\" in val:\n                val = val.split(\",\")[0]\n            if isinstance(val, (list, tuple)):\n                val = max(val)\n            if int(val) > min_count:\n                return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sample_alt_and_depth(rec, sample):\n    if sample and \"AD\" in sample:\n        all_counts = [int(x) for x in sample[\"AD\"]]\n        alt_counts = sum(all_counts[1:])\n        depth = sum(all_counts)\n    elif sample and \"AO\" in sample and sample.get(\"RO\") is not None:\n        alts = sample[\"AO\"]\n        if not isinstance(alts, (list, tuple)):\n            alts = [alts]\n        alt_counts = sum([int(x) for x in alts])\n        depth = alt_counts + int(sample[\"RO\"])\n    elif \"DP\" in rec.info and \"AF\" in rec.info:\n        af = rec.info[\"AF\"][0] if isinstance(rec.info[\"AF\"], (tuple, list)) else rec.info[\"AF\"]\n        return None, rec.info[\"DP\"], af\n    else:\n        alt_counts = None\n    if alt_counts is None or depth is None or depth == 0:\n        return None, None, None\n    else:\n        freq = float(alt_counts) / float(depth)\n        return alt_counts, depth, freq", "response": "Flexibly get ALT allele and depth counts handling FreeBayes MuTect and other cases."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve samtools style fasta index.", "response": "def fasta_idx(in_file, config=None):\n    \"\"\"Retrieve samtools style fasta index.\n    \"\"\"\n    fasta_index = in_file + \".fai\"\n    if not utils.file_exists(fasta_index):\n        samtools = config_utils.get_program(\"samtools\", config) if config else \"samtools\"\n        cmd = \"{samtools} faidx {in_file}\"\n        do.run(cmd.format(**locals()), \"samtools faidx\")\n    return fasta_index"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef file_contigs(ref_file, config=None):\n    ContigInfo = collections.namedtuple(\"ContigInfo\", \"name size\")\n    with open(fasta_idx(ref_file, config)) as in_handle:\n        for line in (l for l in in_handle if l.strip()):\n            name, size = line.split()[:2]\n            yield ContigInfo(name, int(size))", "response": "Iterator of reference contigs and lengths from a reference file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun smCounter2 calling on the input BAMs.", "response": "def run(align_bams, items, ref_file, assoc_files, region=None, out_file=None):\n    \"\"\"Run tumor only smCounter2 calling.\n    \"\"\"\n    paired = vcfutils.get_paired_bams(align_bams, items)\n    assert paired and not paired.normal_bam, (\"smCounter2 supports tumor-only variant calling: %s\" %\n                                              (\",\".join([dd.get_sample_name(d) for d in items])))\n    vrs = bedutils.population_variant_regions(items)\n    target = shared.subset_variant_regions(vrs, region,\n                                            out_file, items=items, do_merge=True)\n    out_file = out_file.replace(\".vcf.gz\", \".vcf\")\n    out_prefix = utils.splitext_plus(os.path.basename(out_file))[0]\n    if not utils.file_exists(out_file) and not utils.file_exists(out_file + \".gz\"):\n        with file_transaction(paired.tumor_data, out_file) as tx_out_file:\n            cmd = [\"smCounter2\", \"--runPath\", os.path.dirname(tx_out_file),\n                   \"--outPrefix\", out_prefix,\n                   \"--bedTarget\", target, \"--refGenome\", ref_file,\n                   \"--bamFile\", paired.tumor_bam, \"--bamType\", \"consensus\",\n                   \"--nCPU\", dd.get_num_cores(paired.tumor_data)]\n            do.run(cmd, \"smcounter2 variant calling\")\n            for fname in glob.glob(os.path.join(os.path.dirname(tx_out_file), \"*.smCounter*\")):\n                shutil.move(fname, os.path.join(os.path.dirname(out_file), os.path.basename(fname)))\n            utils.symlink_plus(os.path.join(os.path.dirname(out_file),\n                                            \"%s.smCounter.cut.vcf\" % out_prefix),\n                               out_file)\n    return vcfutils.bgzip_and_index(out_file, paired.tumor_data[\"config\"], remove_orig=False,\n                                    prep_cmd=\"sed 's#FORMAT\\t%s#FORMAT\\t%s#' | %s\" %\n                                    (out_prefix, dd.get_sample_name(paired.tumor_data),\n                                     vcfutils.add_contig_to_header_cl(dd.get_ref_file(paired.tumor_data), out_file)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _backcompatible_cache_file(query_flags, bed_file, target_name, data):\n    cmd_id = \"num_\" + \" and \".join(query_flags).replace(\" \", \"_\")\n    if bed_file is not None:\n        target_name = target_name or os.path.basename(bed_file)\n        cmd_id += \"_on_\" + target_name\n    work_dir = os.path.join(dd.get_work_dir(data), \"coverage\", dd.get_sample_name(data), \"sambamba\")\n    output_file = os.path.join(work_dir, cmd_id)\n    if utils.file_exists(output_file):\n        return output_file", "response": "Retrieve back - compatible cache file from previous location."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef number_of_mapped_reads(data, bam_file, keep_dups=True, bed_file=None, target_name=None):\n    # Flag explainer https://broadinstitute.github.io/picard/explain-flags.html\n    callable_flags = [\"not unmapped\", \"not mate_is_unmapped\", \"not secondary_alignment\",\n                      \"not failed_quality_control\"]\n    if keep_dups:\n        query_flags = callable_flags\n        flag = 780  # not (read unmapped or mate unmapped or fails QC or secondary alignment)\n    else:\n        query_flags = callable_flags + [\"not duplicate\"]\n        flag = 1804  # as above plus not duplicate\n\n    # Back compatible cache\n    oldcache_file = _backcompatible_cache_file(query_flags, bed_file, target_name, data)\n    if oldcache_file:\n        with open(oldcache_file) as f:\n            return int(f.read().strip())\n\n    # New cache\n    key = json.dumps({\"flags\": sorted(query_flags),\n                      \"region\": os.path.basename(bed_file) if bed_file else \"\",\n                      \"sample\": dd.get_sample_name(data)},\n                     separators=(\",\", \":\"), sort_keys=True)\n    cache_file = get_cache_file(data)\n    if utils.file_exists(cache_file):\n        with open(cache_file) as in_handle:\n            for cur_key, cur_val in (l.strip().split(\"\\t\") for l in in_handle):\n                if cur_key == key:\n                    return int(cur_val)\n\n    # Calculate stats\n    count_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"coverage\",\n                                                dd.get_sample_name(data), \"counts\"))\n    if not bed_file:\n        bed_file = os.path.join(count_dir, \"fullgenome.bed\")\n        if not utils.file_exists(bed_file):\n            with file_transaction(data, bed_file) as tx_out_file:\n                with open(tx_out_file, \"w\") as out_handle:\n                    for c in ref.file_contigs(dd.get_ref_file(data), data[\"config\"]):\n                        out_handle.write(\"%s\\t%s\\t%s\\n\" % (c.name, 0, c.size))\n    count_file = os.path.join(count_dir,\n                              \"%s-%s-counts.txt\" % (os.path.splitext(os.path.basename(bed_file))[0], flag))\n    if not utils.file_exists(count_file):\n        bam.index(bam_file, data[\"config\"], check_timestamp=False)\n        num_cores = dd.get_num_cores(data)\n        with file_transaction(data, count_file) as tx_out_file:\n            cmd = (\"hts_nim_tools count-reads -t {num_cores} -F {flag} {bed_file} {bam_file} > {tx_out_file}\")\n            do.run(cmd.format(**locals()), \"Count mapped reads: %s\" % (dd.get_sample_name(data)))\n    count = 0\n    with open(count_file) as in_handle:\n        for line in in_handle:\n            count += int(line.rstrip().split()[-1])\n\n    with _simple_lock(cache_file):\n        with open(cache_file, \"a\") as out_handle:\n            out_handle.write(\"%s\\t%s\\n\" % (key, count))\n    return count", "response": "Count mapped reads in a single BED file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve number of regions that can be processed in parallel from current samples.", "response": "def get_max_counts(samples):\n    \"\"\"Retrieve number of regions that can be processed in parallel from current samples.\n    \"\"\"\n    counts = []\n    for data in (x[0] for x in samples):\n        count = tz.get_in([\"config\", \"algorithm\", \"callable_count\"], data, 1)\n        vcs = tz.get_in([\"config\", \"algorithm\", \"variantcaller\"], data, [])\n        if isinstance(vcs, six.string_types):\n            vcs = [vcs]\n        if vcs:\n            count *= len(vcs)\n        counts.append(count)\n    return max(counts)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsplitting a BAM file into chromosomal regions.", "response": "def _split_by_regions(dirname, out_ext, in_key):\n    \"\"\"Split a BAM file data analysis into chromosomal regions.\n    \"\"\"\n    def _do_work(data):\n        # XXX Need to move retrieval of regions into preparation to avoid\n        # need for files when running in non-shared filesystems\n        regions = _get_parallel_regions(data)\n        def _sort_by_size(region):\n            _, start, end = region\n            return end - start\n        regions.sort(key=_sort_by_size, reverse=True)\n        bam_file = data[in_key]\n        if bam_file is None:\n            return None, []\n        part_info = []\n        base_out = os.path.splitext(os.path.basename(bam_file))[0]\n        nowork = [[\"nochrom\"], [\"noanalysis\", data[\"config\"][\"algorithm\"][\"non_callable_regions\"]]]\n        for region in regions + nowork:\n            out_dir = os.path.join(data[\"dirs\"][\"work\"], dirname, data[\"name\"][-1], region[0])\n            region_outfile = os.path.join(out_dir, \"%s-%s%s\" %\n                                          (base_out, to_safestr(region), out_ext))\n            part_info.append((region, region_outfile))\n        out_file = os.path.join(data[\"dirs\"][\"work\"], dirname, data[\"name\"][-1],\n                                \"%s%s\" % (base_out, out_ext))\n        return out_file, part_info\n    return _do_work"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves regions to run in parallel", "response": "def _get_parallel_regions(data):\n    \"\"\"Retrieve regions to run in parallel, putting longest intervals first.\n    \"\"\"\n    callable_regions = tz.get_in([\"config\", \"algorithm\", \"callable_regions\"], data)\n    if not callable_regions:\n        raise ValueError(\"Did not find any callable regions for sample: %s\\n\"\n                            \"Check 'align/%s/*-callableblocks.bed' and 'regions' to examine callable regions\"\n                            % (dd.get_sample_name(data), dd.get_sample_name(data)))\n    with open(callable_regions) as in_handle:\n        regions = [(xs[0], int(xs[1]), int(xs[2])) for xs in\n                    (l.rstrip().split(\"\\t\") for l in in_handle) if (len(xs) >= 3 and\n                                                                    not xs[0].startswith((\"track\", \"browser\",)))]\n    return regions"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_parallel_regions(batch):\n    samples = [utils.to_single_data(d) for d in batch]\n    regions = _get_parallel_regions(samples[0])\n    return [{\"region\": \"%s:%s-%s\" % (c, s, e)} for c, s, e in regions]", "response": "CWL target to retrieve a list of callable regions for parallelization.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds information for later combining work.", "response": "def _add_combine_info(output, combine_map, file_key):\n    \"\"\"Do not actually combine, but add details for later combining work.\n\n    Each sample will contain information on the out file and additional files\n    to merge, enabling other splits and recombines without losing information.\n    \"\"\"\n    files_per_output = collections.defaultdict(list)\n    for part_file, out_file in combine_map.items():\n        files_per_output[out_file].append(part_file)\n    out_by_file = collections.defaultdict(list)\n    out = []\n    for data in output:\n        # Do not pass along nochrom, noanalysis regions\n        if data[\"region\"][0] not in [\"nochrom\", \"noanalysis\"]:\n            cur_file = data[file_key]\n            # If we didn't process, no need to add combine information\n            if cur_file in combine_map:\n                out_file = combine_map[cur_file]\n                if \"combine\" not in data:\n                    data[\"combine\"] = {}\n                data[\"combine\"][file_key] = {\"out\": out_file,\n                                             \"extras\": files_per_output.get(out_file, [])}\n                out_by_file[out_file].append(data)\n            elif cur_file:\n                out_by_file[cur_file].append(data)\n            else:\n                out.append([data])\n    for samples in out_by_file.values():\n        regions = [x[\"region\"] for x in samples]\n        region_bams = [x[\"work_bam\"] for x in samples]\n        assert len(regions) == len(region_bams)\n        if len(set(region_bams)) == 1:\n            region_bams = [region_bams[0]]\n        data = samples[0]\n        data[\"region_bams\"] = region_bams\n        data[\"region\"] = regions\n        data = dd.set_mark_duplicates(data, data[\"config\"][\"algorithm\"][\"orig_markduplicates\"])\n        del data[\"config\"][\"algorithm\"][\"orig_markduplicates\"]\n        out.append([data])\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nperforming full pre - variant calling BAM prep work on regions.", "response": "def parallel_prep_region(samples, run_parallel):\n    \"\"\"Perform full pre-variant calling BAM prep work on regions.\n    \"\"\"\n    file_key = \"work_bam\"\n    split_fn = _split_by_regions(\"bamprep\", \"-prep.bam\", file_key)\n    # identify samples that do not need preparation -- no recalibration or realignment\n    extras = []\n    torun = []\n    for data in [x[0] for x in samples]:\n        if data.get(\"work_bam\"):\n            data[\"align_bam\"] = data[\"work_bam\"]\n        if (not dd.get_realign(data) and not dd.get_variantcaller(data)):\n            extras.append([data])\n        elif not data.get(file_key):\n            extras.append([data])\n        else:\n            # Do not want to re-run duplicate marking after realignment\n            data[\"config\"][\"algorithm\"][\"orig_markduplicates\"] = dd.get_mark_duplicates(data)\n            data = dd.set_mark_duplicates(data, False)\n            torun.append([data])\n    return extras + parallel_split_combine(torun, split_fn, run_parallel,\n                                           \"piped_bamprep\", _add_combine_info, file_key, [\"config\"])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delayed_bamprep_merge(samples, run_parallel):\n    if any(\"combine\" in data[0] for data in samples):\n        return run_parallel(\"delayed_bam_merge\", samples)\n    else:\n        return samples", "response": "Perform a delayed merge on regional prepared BAM files."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clean_sample_data(samples):\n    out = []\n    for data in (utils.to_single_data(x) for x in samples):\n        if \"dirs\" in data:\n            data[\"dirs\"] = {\"work\": data[\"dirs\"][\"work\"], \"galaxy\": data[\"dirs\"][\"galaxy\"],\n                            \"fastq\": data[\"dirs\"].get(\"fastq\")}\n        data[\"config\"] = {\"algorithm\": data[\"config\"][\"algorithm\"],\n                          \"resources\": data[\"config\"][\"resources\"]}\n        for remove_attr in [\"config_file\", \"algorithm\"]:\n            data.pop(remove_attr, None)\n        out.append([data])\n    return out", "response": "Clean unnecessary information from sample data reducing size for message passing."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding splice junction databases to the command line", "response": "def _add_sj_index_commands(fq1, ref_file, gtf_file):\n    \"\"\"\n    newer versions of STAR can generate splice junction databases on thephfly\n    this is preferable since we can tailor it to the read lengths\n    \"\"\"\n    if _has_sj_index(ref_file):\n        return \"\"\n    else:\n        rlength = fastq.estimate_maximum_read_length(fq1)\n        cmd = \" --sjdbGTFfile %s \" % gtf_file\n        cmd += \" --sjdbOverhang %s \" % str(rlength - 1)\n        return cmd"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _has_sj_index(ref_file):\n    return (file_exists(os.path.join(ref_file, \"sjdbInfo.txt\")) and\n            (file_exists(os.path.join(ref_file, \"transcriptInfo.tab\"))))", "response": "check if the file exists and if we can do splice junction indexing"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmaps sequence references to equivalent star indexes", "response": "def remap_index_fn(ref_file):\n    \"\"\"Map sequence references to equivalent star indexes\n    \"\"\"\n    return os.path.join(os.path.dirname(os.path.dirname(ref_file)), \"star\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a STAR index in the defined reference directory.", "response": "def index(ref_file, out_dir, data):\n    \"\"\"Create a STAR index in the defined reference directory.\n    \"\"\"\n    (ref_dir, local_file) = os.path.split(ref_file)\n    gtf_file = dd.get_gtf_file(data)\n    if not utils.file_exists(gtf_file):\n        raise ValueError(\"%s not found, could not create a star index.\" % (gtf_file))\n    if not utils.file_exists(out_dir):\n        with tx_tmpdir(data, os.path.dirname(out_dir)) as tx_out_dir:\n            num_cores = dd.get_cores(data)\n            cmd = (\"STAR --genomeDir {tx_out_dir} --genomeFastaFiles {ref_file} \"\n                   \"--runThreadN {num_cores} \"\n                   \"--runMode genomeGenerate --sjdbOverhang 99 --sjdbGTFfile {gtf_file}\")\n            do.run(cmd.format(**locals()), \"Index STAR\")\n            if os.path.exists(out_dir):\n                shutil.rmtree(out_dir)\n            shutil.move(tx_out_dir, out_dir)\n    return out_dir"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_splicejunction_file(out_dir, data):\n    samplename = dd.get_sample_name(data)\n    sjfile = os.path.join(out_dir, os.pardir, \"{0}SJ.out.tab\").format(samplename)\n    if file_exists(sjfile):\n        return sjfile\n    else:\n        return None", "response": "locate the splicejunction file starting from the alignment directory"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef junction2bed(junction_file):\n    base, _ = os.path.splitext(junction_file)\n    out_file = base + \"-minimized.bed\"\n    if file_exists(out_file):\n        return out_file\n    if not file_exists(junction_file):\n        return None\n    with file_transaction(out_file) as tx_out_file:\n        with open(junction_file) as in_handle:\n            with open(tx_out_file, \"w\") as out_handle:\n                 for line in in_handle:\n                    tokens = line.split()\n                    chrom, sj1, sj2 = tokens[0:3]\n                    if int(sj1) > int(sj2):\n                        tmp = sj1\n                        sj1 = sj2\n                        sj2 = tmp\n                    out_handle.write(\"\\t\".join([chrom, sj1, sj1]) + \"\\n\")\n                    out_handle.write(\"\\t\".join([chrom, sj2, sj2]) + \"\\n\")\n        minimize = bed.minimize(tx_out_file)\n        minimize.saveas(tx_out_file)\n    return out_file", "response": "reformat the STAR junction file to BED3 format one end of the splice junction per line\n countryCode returns the file name of the new file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing HLA files and combine them into one set of fastqs.", "response": "def run(data):\n    \"\"\"HLA typing with OptiType, parsing output from called genotype files.\n    \"\"\"\n    hlas = []\n    for hla_fq in tz.get_in([\"hla\", \"fastq\"], data, []):\n        hla_type = re.search(\"[.-](?P<hlatype>HLA-[\\w-]+).fq\", hla_fq).group(\"hlatype\")\n        if hla_type in SUPPORTED_HLAS:\n            if utils.file_exists(hla_fq):\n                hlas.append((hla_type, hla_fq))\n    if len(hlas) > 0:\n        out_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"align\",\n                                                  dd.get_sample_name(data), \"hla\",\n                                                  \"OptiType-HLA-A_B_C\"))\n        # When running UMIs and hla typing we want to pick the original fastqs\n        if len(hlas) > len(SUPPORTED_HLAS):\n            hlas = [x for x in hlas if os.path.basename(x[1]).find(\"-cumi\") == -1]\n        if len(hlas) == len(SUPPORTED_HLAS):\n            hla_fq = combine_hla_fqs(hlas, out_dir + \"-input.fq\", data)\n            if utils.file_exists(hla_fq):\n                out_file = glob.glob(os.path.join(out_dir, \"*\", \"*_result.tsv\"))\n                if len(out_file) > 0:\n                    out_file = out_file[0]\n                else:\n                    out_file = _call_hla(hla_fq, out_dir, data)\n                out_file = _prepare_calls(out_file, os.path.dirname(out_dir), data)\n                data[\"hla\"].update({\"call_file\": out_file,\n                                    \"hlacaller\": \"optitype\"})\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncombining all HLAs with single HLA.", "response": "def combine_hla_fqs(hlas, out_file, data):\n    \"\"\"OptiType performs best on a combination of all extracted HLAs.\n    \"\"\"\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            with open(tx_out_file, \"w\") as out_handle:\n                for hla_type, hla_fq in hlas:\n                    if utils.file_exists(hla_fq):\n                        with open(hla_fq) as in_handle:\n                            shutil.copyfileobj(in_handle, out_handle)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprepares calls to HLA typing by allele.", "response": "def _prepare_calls(result_file, out_dir, data):\n    \"\"\"Write summary file of results of HLA typing by allele.\n    \"\"\"\n    sample = dd.get_sample_name(data)\n    out_file = os.path.join(out_dir, \"%s-optitype.csv\" % (sample))\n    if not utils.file_uptodate(out_file, result_file):\n        hla_truth = bwakit.get_hla_truthset(data)\n        with file_transaction(data, out_file) as tx_out_file:\n            with open(tx_out_file, \"w\") as out_handle:\n                writer = csv.writer(out_handle)\n                allele_info = _parse_result_file(result_file)\n                if len(allele_info) == 1:\n                    writer.writerow([\"sample\", \"locus\", \"alleles\", \"expected\", \"validates\"])\n                else:\n                    writer.writerow([\"sample\", \"local\", \"index\", \"alleles\", \"score\"])\n                for j, (alleles, score) in enumerate(allele_info):\n                    for hla_locus, call_alleles in alleles:\n                        truth_alleles = tz.get_in([sample, hla_locus], hla_truth, [])\n                        if len(allele_info) == 1:\n                            writer.writerow([sample, hla_locus,\n                                             \";\".join(call_alleles), \";\".join(truth_alleles),\n                                             bwakit.matches_truth(call_alleles, truth_alleles, data)])\n                        else:\n                            writer.writerow([sample, hla_locus, j, \";\".join(call_alleles), score])\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning OptiType HLA calling for a specific fastq input.", "response": "def _call_hla(hla_fq, out_dir, data):\n    \"\"\"Run OptiType HLA calling for a specific fastq input.\n    \"\"\"\n    bin_dir = os.path.dirname(os.path.realpath(sys.executable))\n    out_dir = utils.safe_makedir(out_dir)\n    with tx_tmpdir(data, os.path.dirname(out_dir)) as tx_out_dir:\n        config_file = os.path.join(tx_out_dir, \"config.ini\")\n        with open(config_file, \"w\") as out_handle:\n            razers3 = os.path.join(bin_dir, \"razers3\")\n            if not os.path.exists(razers3):\n                raise ValueError(\"Could not find razers3 executable at %s\" % (razers3))\n            out_handle.write(CONFIG_TMPL.format(razers3=razers3, cores=dd.get_cores(data)))\n        resources = config_utils.get_resources(\"optitype\", data[\"config\"])\n        if resources.get(\"options\"):\n            opts = \" \".join([str(x) for x in resources[\"options\"]])\n        else:\n            opts = \"\"\n        cmd = (\"OptiTypePipeline.py -v --dna {opts} -o {tx_out_dir} \"\n                \"-i {hla_fq} -c {config_file}\")\n        do.run(cmd.format(**locals()), \"HLA typing with OptiType\")\n        for outf in os.listdir(tx_out_dir):\n            shutil.move(os.path.join(tx_out_dir, outf), os.path.join(out_dir, outf))\n    out_file = glob.glob(os.path.join(out_dir, \"*\", \"*_result.tsv\"))\n    assert len(out_file) == 1, \"Expected one result file for OptiType, found %s\" % out_file\n    return out_file[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_autosomal(chrom):\n    try:\n        int(chrom)\n        return True\n    except ValueError:\n        try:\n            int(str(chrom.lower().replace(\"chr\", \"\").replace(\"_\", \"\").replace(\"-\", \"\")))\n            return True\n        except ValueError:\n            return False", "response": "Keep chromosomes that are a digit 1 - 22 or chr prefixed digit chr1 - chr22."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun the Variant Collection QC analysis.", "response": "def run(_, data, out_dir):\n    \"\"\"Prepare variants QC analysis: bcftools stats and snpEff output.\n    \"\"\"\n    out = []\n    vcinfo = get_active_vcinfo(data)\n    if vcinfo:\n        if dd.get_phenotype(data) == \"normal\" and \"germline\" in vcinfo:\n            out.append(_bcftools_stats(data, out_dir, \"germline\", germline=True))\n        elif dd.get_phenotype(data) != \"germline\":\n            out.append(_bcftools_stats(data, out_dir))\n            if \"germline\" in vcinfo:\n                out.append(_bcftools_stats(data, out_dir, \"germline\", germline=True))\n        else:\n            out.append(_bcftools_stats(data, out_dir, germline=True))\n    out.append(_snpeff_stats(data, out_dir))\n\n    out = [item for item in out if item]\n    if out:\n        return {\"base\": out[0], \"secondary\": out[1:]}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_filename_details(full_f):\n    out = {\"vrn_file\": full_f}\n    f = os.path.basename(full_f)\n    for vc in list(genotype.get_variantcallers().keys()) + [\"ensemble\"]:\n        if f.find(\"-%s.vcf\" % vc) > 0:\n            out[\"variantcaller\"] = vc\n    if f.find(\"-germline-\") >= 0:\n        out[\"germline\"] = full_f\n    return out", "response": "Add variant callers and germline information standard CWL filenames."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_variants(data):\n    active_vs = []\n    if \"variants\" in data:\n        variants = data[\"variants\"]\n        # CWL based list of variants\n        if isinstance(variants, dict) and \"samples\" in variants:\n            variants = variants[\"samples\"]\n        for v in variants:\n            # CWL -- a single variant file\n            if isinstance(v, six.string_types) and os.path.exists(v):\n                active_vs.append(_add_filename_details(v))\n            elif (isinstance(v, (list, tuple)) and len(v) > 0 and\n                  isinstance(v[0], six.string_types) and os.path.exists(v[0])):\n                for subv in v:\n                    active_vs.append(_add_filename_details(subv))\n            elif isinstance(v, dict):\n                if v.get(\"vrn_file\"):\n                    active_vs.append(v)\n                elif v.get(\"population\"):\n                    vrnfile = v.get(\"population\").get(\"vcf\")\n                    active_vs.append(_add_filename_details(vrnfile))\n                elif v.get(\"vcf\"):\n                    active_vs.append(_add_filename_details(v.get(\"vcf\")))\n    return active_vs", "response": "Retrieve variants from CWL and standard inputs for organizing variants."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the active VariantSet from the data.", "response": "def get_active_vcinfo(data, use_ensemble=True):\n    \"\"\"Use first caller if ensemble is not active\n    \"\"\"\n    active_vs = _get_variants(data)\n    if len(active_vs) > 0:\n        e_active_vs = []\n        if use_ensemble:\n            e_active_vs = [v for v in active_vs if v.get(\"variantcaller\") == \"ensemble\"]\n        if len(e_active_vs) == 0:\n            e_active_vs = [v for v in active_vs if v.get(\"variantcaller\") != \"ensemble\"]\n        if len(e_active_vs) > 0:\n            return e_active_vs[0]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extract_germline_vcinfo(data, out_dir):\n    supported_germline = set([\"vardict\", \"octopus\", \"freebayes\"])\n    if dd.get_phenotype(data) in [\"tumor\"]:\n        for v in _get_variants(data):\n            if v.get(\"variantcaller\") in supported_germline:\n                if v.get(\"germline\"):\n                    return v\n                else:\n                    d = utils.deepish_copy(data)\n                    d[\"vrn_file\"] = v[\"vrn_file\"]\n                    gd = germline.extract(d, [d], out_dir)\n                    v[\"germline\"] = gd[\"vrn_file_plus\"][\"germline\"]\n                    return v", "response": "Extract germline VCFs from existing tumor inputs."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmerges multiple BAM files into a single BAM file.", "response": "def merge_bam_files(bam_files, work_dir, data, out_file=None, batch=None):\n    \"\"\"Merge multiple BAM files from a sample into a single BAM for processing.\n\n    Checks system open file limit and merges in batches if necessary to avoid\n    file handle limits.\n    \"\"\"\n    out_file = _merge_outfile_fname(out_file, bam_files, work_dir, batch)\n    if not utils.file_exists(out_file):\n        if len(bam_files) == 1 and bam.bam_already_sorted(bam_files[0], data[\"config\"], \"coordinate\"):\n            with file_transaction(data, out_file) as tx_out_file:\n                _create_merge_filelist(bam_files, tx_out_file, data[\"config\"])\n            out_file = bam_files[0]\n            samtools = config_utils.get_program(\"samtools\", data[\"config\"])\n            do.run('{} quickcheck -v {}'.format(samtools, out_file),\n                   \"Check for valid merged BAM after transfer\")\n        else:\n            with tx_tmpdir(data) as tmpdir:\n                with utils.chdir(tmpdir):\n                    with file_transaction(data, out_file) as tx_out_file:\n                        tx_bam_file_list = _create_merge_filelist(bam_files, tx_out_file, data[\"config\"])\n                        samtools = config_utils.get_program(\"samtools\", data[\"config\"])\n                        resources = config_utils.get_resources(\"samtools\", data[\"config\"])\n                        num_cores = dd.get_num_cores(data)\n                        # Aim for 3.5Gb/core memory for BAM merging\n                        num_cores = config_utils.adjust_cores_to_mb_target(\n                            3500, resources.get(\"memory\", \"2G\"), num_cores)\n                        max_mem = config_utils.adjust_memory(resources.get(\"memory\", \"1G\"),\n                                                             2, \"decrease\").upper()\n                        if dd.get_mark_duplicates(data):\n                            cmd = _biobambam_merge_dedup_maxcov(data)\n                        else:\n                            cmd = _biobambam_merge_maxcov(data)\n                        do.run(cmd.format(**locals()), \"Merge bam files to %s\" % os.path.basename(out_file),\n                                None)\n                        do.run('{} quickcheck -v {}'.format(samtools, tx_out_file),\n                               \"Check for valid merged BAM\")\n            do.run('{} quickcheck -v {}'.format(samtools, out_file),\n                   \"Check for valid merged BAM after transfer\")\n            _finalize_merge(out_file, bam_files, data[\"config\"])\n    bam.index(out_file, data[\"config\"])\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _create_merge_filelist(bam_files, base_file, config):\n    bam_file_list = \"%s.list\" % os.path.splitext(base_file)[0]\n    samtools = config_utils.get_program(\"samtools\", config)\n    with open(bam_file_list, \"w\") as out_handle:\n        for f in sorted(bam_files):\n            do.run('{} quickcheck -v {}'.format(samtools, f),\n                   \"Ensure integrity of input merge BAM files\")\n            out_handle.write(\"%s\\n\" % f)\n    return bam_file_list", "response": "Create list of input files for merge."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nderive correct name of BAM file based on batching.", "response": "def _merge_outfile_fname(out_file, bam_files, work_dir, batch):\n    \"\"\"Derive correct name of BAM file based on batching.\n    \"\"\"\n    if out_file is None:\n        out_file = os.path.join(work_dir, os.path.basename(sorted(bam_files)[0]))\n    if batch is not None:\n        base, ext = os.path.splitext(out_file)\n        out_file = \"%s-b%s%s\" % (base, batch, ext)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nhandle indexes and cleanups of merged BAM and input files.", "response": "def _finalize_merge(out_file, bam_files, config):\n    \"\"\"Handle indexes and cleanups of merged BAM and input files.\n    \"\"\"\n    # Ensure timestamps are up to date on output file and index\n    # Works around issues on systems with inconsistent times\n    for ext in [\"\", \".bai\"]:\n        if os.path.exists(out_file + ext):\n            subprocess.check_call([\"touch\", out_file + ext])\n    for b in bam_files:\n        utils.save_diskspace(b, \"BAM merged to %s\" % out_file, config)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _cwl_workflow_template(inputs, top_level=False):\n    ready_inputs = []\n    for inp in inputs:\n        cur_inp = copy.deepcopy(inp)\n        for attr in [\"source\", \"valueFrom\", \"wf_duplicate\"]:\n            cur_inp.pop(attr, None)\n        if top_level:\n            cur_inp = workflow._flatten_nested_input(cur_inp)\n        cur_inp = _clean_record(cur_inp)\n        ready_inputs.append(cur_inp)\n    return {\"class\": \"Workflow\",\n            \"cwlVersion\": \"v1.0\",\n            \"hints\": [],\n            \"requirements\": [{\"class\": \"EnvVarRequirement\",\n                              \"envDef\": [{\"envName\": \"MPLCONFIGDIR\", \"envValue\": \".\"}]},\n                             {\"class\": \"ScatterFeatureRequirement\"},\n                             {\"class\": \"SubworkflowFeatureRequirement\"}],\n            \"inputs\": ready_inputs,\n            \"outputs\": [],\n            \"steps\": []}", "response": "Return a CWL template for the given list of inputs."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves disk usage estimates as CWL ResourceRequirement and hint.", "response": "def _get_disk_estimates(name, parallel, inputs, file_estimates, samples, disk,\n                        cur_remotes, no_files):\n    \"\"\"Retrieve disk usage estimates as CWL ResourceRequirement and hint.\n\n    Disk specification for temporary files and outputs.\n\n    Also optionally includes disk input estimates as a custom hint for\n    platforms which need to stage these and don't pre-estimate these when\n    allocating machine sizes.\n    \"\"\"\n    tmp_disk, out_disk, in_disk = 0, 0, 0\n    if file_estimates:\n        if disk:\n            for key, multiplier in disk.items():\n                if key in file_estimates:\n                    out_disk += int(multiplier * file_estimates[key])\n        for inp in inputs:\n            scale = 2.0 if inp.get(\"type\") == \"array\" else 1.0\n            # Allocating all samples, could remove for `to_rec` when we ensure we\n            # don't have to stage. Currently dnanexus stages everything so need to consider\n            if parallel in [\"multi-combined\", \"multi-batch\"] and \"dnanexus\" in cur_remotes:\n                scale *= (len(samples))\n            if workflow.is_cwl_record(inp):\n                for f in _get_record_fields(inp):\n                    if f[\"name\"] in file_estimates:\n                        in_disk += file_estimates[f[\"name\"]] * scale\n            elif inp[\"id\"] in file_estimates:\n                in_disk += file_estimates[inp[\"id\"]] * scale\n        # Round total estimates to integer, assign extra half to temp space\n        # It's not entirely clear how different runners interpret this\n        tmp_disk = int(math.ceil(out_disk * 0.5))\n        out_disk = int(math.ceil(out_disk))\n\n    bcbio_docker_disk = (10 if cur_remotes else 1) * 1024  # Minimum requirements for bcbio Docker image\n    disk_hint = {\"outdirMin\": bcbio_docker_disk + out_disk, \"tmpdirMin\": tmp_disk}\n    # Skip input disk for steps which require only transformation (and thus no staging)\n    if no_files:\n        in_disk = 0\n    # Avoid accidentally flagging as no staging if we don't know sizes of expected inputs\n    elif in_disk == 0:\n        in_disk = 1\n    input_hint = {\"class\": \"dx:InputResourceRequirement\", \"indirMin\": int(math.ceil(in_disk))}\n    return disk_hint, input_hint"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _add_current_quay_tag(repo, container_tags):\n    if ':' in repo:\n        return repo, container_tags\n    try:\n        latest_tag = container_tags[repo]\n    except KeyError:\n        repo_id = repo[repo.find('/') + 1:]\n        tags = requests.request(\"GET\", \"https://quay.io/api/v1/repository/\" + repo_id).json()[\"tags\"]\n        latest_tag = None\n        latest_modified = None\n        for tag, info in tags.items():\n            if latest_tag:\n                if (dateutil.parser.parse(info['last_modified']) > dateutil.parser.parse(latest_modified)\n                      and tag != 'latest'):\n                    latest_modified = info['last_modified']\n                    latest_tag = tag\n            else:\n                latest_modified = info['last_modified']\n                latest_tag = tag\n        container_tags[repo] = str(latest_tag)\n    latest_pull = repo + ':' + str(latest_tag)\n    return latest_pull, container_tags", "response": "Lookup the current quay tag for the repository adding to repo string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates an ExpressionTool output for the given inputs and outputs", "response": "def _write_expressiontool(step_dir, name, inputs, outputs, expression, parallel):\n    \"\"\"Create an ExpressionTool output for the given inputs\n    \"\"\"\n    out_file = os.path.join(step_dir, \"%s.cwl\" % name)\n    out = {\"class\": \"ExpressionTool\",\n           \"cwlVersion\": \"v1.0\",\n           \"requirements\": [{\"class\": \"InlineJavascriptRequirement\"}],\n           \"inputs\": [],\n           \"outputs\": [],\n           \"expression\": expression}\n    out = _add_inputs_to_tool(inputs, out, parallel)\n    out = _add_outputs_to_tool(outputs, out)\n    _tool_to_file(out, out_file)\n    return os.path.join(\"steps\", os.path.basename(out_file))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _clean_record(rec):\n    if workflow.is_cwl_record(rec):\n        def _clean_fields(d):\n            if isinstance(d, dict):\n                if \"fields\" in d:\n                    out = []\n                    for f in d[\"fields\"]:\n                        f = utils.deepish_copy(f)\n                        f.pop(\"secondaryFiles\", None)\n                        out.append(f)\n                    d[\"fields\"] = out\n                    return d\n                else:\n                    out = {}\n                    for k, v in d.items():\n                        out[k] = _clean_fields(v)\n                    return out\n            else:\n                return d\n        return _clean_fields(rec)\n    else:\n        return rec", "response": "Remove secondary files from record fields which are currently not supported."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets field names from a potentially nested record.", "response": "def _get_record_fields(d):\n    \"\"\"Get field names from a potentially nested record.\n    \"\"\"\n    if isinstance(d, dict):\n        if \"fields\" in d:\n            return d[\"fields\"]\n        else:\n            for v in d.values():\n                fields = _get_record_fields(v)\n                if fields:\n                    return fields"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves expected sentinel value for an output expanding records.", "response": "def _get_sentinel_val(v):\n    \"\"\"Retrieve expected sentinel value for an output, expanding records.\n    \"\"\"\n    out = workflow.get_base_id(v[\"id\"])\n    if workflow.is_cwl_record(v):\n        out += \":%s\" % \";\".join([x[\"name\"] for x in _get_record_fields(v)])\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _place_input_binding(inp_tool, inp_binding, parallel):\n    if (parallel in [\"multi-combined\", \"multi-batch\", \"batch-split\", \"batch-parallel\",\n                     \"batch-merge\", \"batch-single\"] and\n          tz.get_in([\"type\", \"type\"], inp_tool) == \"array\"):\n        inp_tool[\"type\"][\"inputBinding\"] = inp_binding\n    else:\n        inp_tool[\"inputBinding\"] = inp_binding\n    return inp_tool", "response": "Place the input binding in the input tool."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplacing secondary files at the level of the File item to ensure indexes get passed.", "response": "def _place_secondary_files(inp_tool, inp_binding=None):\n    \"\"\"Put secondaryFiles at the level of the File item to ensure indexes get passed.\n    \"\"\"\n    def _is_file(val):\n        return (val == \"File\" or (isinstance(val, (list, tuple)) and\n                                  (\"File\" in val or any(isinstance(x, dict) and _is_file(val)) for x in val)))\n    secondary_files = inp_tool.pop(\"secondaryFiles\", None)\n    if secondary_files:\n        key = []\n        while (not _is_file(tz.get_in(key + [\"type\"], inp_tool))\n               and not _is_file(tz.get_in(key + [\"items\"], inp_tool))\n               and not _is_file(tz.get_in(key + [\"items\", \"items\"], inp_tool))):\n            key.append(\"type\")\n        if tz.get_in(key, inp_tool):\n            inp_tool[\"secondaryFiles\"] = secondary_files\n        elif inp_binding:\n            nested_inp_binding = copy.deepcopy(inp_binding)\n            nested_inp_binding[\"prefix\"] = \"ignore=\"\n            nested_inp_binding[\"secondaryFiles\"] = secondary_files\n            inp_tool = tz.update_in(inp_tool, key, lambda x: nested_inp_binding)\n    return inp_tool"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntemplate function for writing a single step.", "response": "def _step_template(name, run_file, inputs, outputs, parallel, step_parallelism, scatter=None):\n    \"\"\"Templating function for writing a step to avoid repeating namespaces.\n    \"\"\"\n    scatter_inputs = []\n    sinputs = []\n    for inp in inputs:\n        step_inp = {\"id\": workflow.get_base_id(inp[\"id\"]), \"source\": inp[\"id\"]}\n        if inp.get(\"wf_duplicate\"):\n            step_inp[\"id\"] += \"_toolinput\"\n        for attr in [\"source\", \"valueFrom\"]:\n            if attr in inp:\n                step_inp[attr] = inp[attr]\n        sinputs.append(step_inp)\n        # An initial parallel scatter and multiple chained parallel sample scatters\n        if (parallel == \"multi-parallel\" and\n              (not step_parallelism or\n               step_parallelism.get(workflow.get_step_prefix(inp[\"id\"])) == \"multi-parallel\")):\n            scatter_inputs.append(step_inp[\"id\"])\n        # scatter on inputs from previous processes that have been arrayed\n        elif (_is_scatter_parallel(parallel) and (_do_scatter_var(inp, parallel)\n                                                  or (scatter and inp[\"id\"] in scatter))):\n            scatter_inputs.append(step_inp[\"id\"])\n    out = {\"run\": run_file,\n           \"id\": name,\n           \"in\": sinputs,\n           \"out\": [{\"id\": workflow.get_base_id(output[\"id\"])} for output in outputs]}\n    if _is_scatter_parallel(parallel):\n        assert scatter_inputs, \"Did not find items to scatter on: %s\" % name\n        out.update({\"scatterMethod\": \"dotproduct\",\n                    \"scatter\": scatter_inputs})\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_cur_remotes(path):\n    cur_remotes = set([])\n    if isinstance(path, (list, tuple)):\n        for v in path:\n            cur_remotes |= _get_cur_remotes(v)\n    elif isinstance(path, dict):\n        for v in path.values():\n            cur_remotes |= _get_cur_remotes(v)\n    elif path and isinstance(path, six.string_types):\n        if path.startswith(tuple(INTEGRATION_MAP.keys())):\n            cur_remotes.add(INTEGRATION_MAP.get(path.split(\":\")[0] + \":\"))\n    return cur_remotes", "response": "Retrieve remote references defined in the CWL.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprepare a CWL description with sub - workflows and steps.", "response": "def prep_cwl(samples, workflow_fn, out_dir, out_file, integrations=None,\n             add_container_tag=None):\n    \"\"\"Output a CWL description with sub-workflows and steps.\n    \"\"\"\n    if add_container_tag is None:\n        container_tags = None\n    elif add_container_tag.lower() == \"quay_lookup\":\n        container_tags = {}\n    else:\n        container_tags = collections.defaultdict(lambda: add_container_tag)\n    step_dir = utils.safe_makedir(os.path.join(out_dir, \"steps\"))\n    get_retriever = GetRetriever(integrations, samples)\n    variables, keyvals = _flatten_samples(samples, out_file, get_retriever)\n    cur_remotes = _get_cur_remotes(keyvals)\n    file_estimates = _calc_input_estimates(keyvals, get_retriever)\n    out = _cwl_workflow_template(variables)\n    parent_wfs = []\n    step_parallelism = {}\n    steps, wfoutputs = workflow_fn(samples)\n    used_inputs = set([])\n    for cur in workflow.generate(variables, steps, wfoutputs):\n        if cur[0] == \"step\":\n            _, name, parallel, inputs, outputs, image, programs, disk, cores, no_files = cur\n            step_file = _write_tool(step_dir, name, inputs, outputs, parallel, image, programs,\n                                    file_estimates, disk, cores, samples, cur_remotes, no_files, container_tags)\n            out[\"steps\"].append(_step_template(name, step_file, inputs, outputs, parallel, step_parallelism))\n            used_inputs |= set(x[\"id\"] for x in inputs)\n        elif cur[0] == \"expressiontool\":\n            _, name, inputs, outputs, expression, parallel = cur\n            step_file = _write_expressiontool(step_dir, name, inputs, outputs, expression, parallel)\n            out[\"steps\"].append(_step_template(name, step_file, inputs, outputs, parallel, step_parallelism))\n            used_inputs |= set(x[\"id\"] for x in inputs)\n        elif cur[0] == \"upload\":\n            for output in cur[1]:\n                wf_output = copy.deepcopy(output)\n                if \"outputSource\" not in wf_output:\n                    wf_output[\"outputSource\"] = wf_output.pop(\"source\")\n                wf_output = _clean_record(wf_output)\n                # Avoid input/output naming clashes\n                if wf_output[\"id\"] in used_inputs:\n                    wf_output[\"id\"] = \"%s_out\" % wf_output[\"id\"]\n                out[\"outputs\"].append(wf_output)\n        elif cur[0] == \"wf_start\":\n            parent_wfs.append(out)\n            out = _cwl_workflow_template(cur[1])\n        elif cur[0] == \"wf_finish\":\n            _, name, parallel, inputs, outputs, scatter = cur\n            wf_out_file = \"wf-%s.cwl\" % name\n            with open(os.path.join(out_dir, wf_out_file), \"w\") as out_handle:\n                yaml.safe_dump(out, out_handle, default_flow_style=False, allow_unicode=False)\n            out = parent_wfs.pop(-1)\n            out[\"steps\"].append(_step_template(name, wf_out_file, inputs, outputs, parallel,\n                                               step_parallelism, scatter))\n            used_inputs |= set(x[\"id\"] for x in inputs)\n        else:\n            raise ValueError(\"Unexpected workflow value %s\" % str(cur))\n        step_parallelism[name] = parallel\n\n    with open(out_file, \"w\") as out_handle:\n        out[\"inputs\"] = [x for x in out[\"inputs\"] if x[\"id\"] in used_inputs]\n        yaml.safe_dump(out, out_handle, default_flow_style=False, allow_unicode=False)\n    sample_json = \"%s-samples.json\" % utils.splitext_plus(out_file)[0]\n    out_clean = _clean_final_outputs(copy.deepcopy({k: v for k, v in keyvals.items() if k in used_inputs}),\n                                     get_retriever)\n    with open(sample_json, \"w\") as out_handle:\n        json.dump(out_clean, out_handle, sort_keys=True, indent=4, separators=(',', ': '))\n    return out_file, sample_json"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a flattened JSON representation of samples from the bcbio world map.", "response": "def _flatten_samples(samples, base_file, get_retriever):\n    \"\"\"Create a flattened JSON representation of data from the bcbio world map.\n    \"\"\"\n    flat_data = []\n    for data in samples:\n        data[\"reference\"] = _indexes_to_secondary_files(data[\"reference\"], data[\"genome_build\"])\n        cur_flat = {}\n        for key_path in [[\"analysis\"], [\"description\"], [\"rgnames\"], [\"config\", \"algorithm\"],\n                         [\"metadata\"], [\"genome_build\"], [\"resources\"],\n                         [\"files\"], [\"reference\"], [\"genome_resources\"], [\"vrn_file\"]]:\n            cur_key = \"__\".join(key_path)\n            for flat_key, flat_val in _to_cwldata(cur_key, tz.get_in(key_path, data), get_retriever):\n                cur_flat[flat_key] = flat_val\n        flat_data.append(cur_flat)\n    out = {}\n    for key in sorted(list(set(reduce(operator.add, [list(d.keys()) for d in flat_data])))):\n        # Periods in keys cause issues with WDL and some CWL implementations\n        clean_key = key.replace(\".\", \"_\")\n        out[clean_key] = []\n        for cur_flat in flat_data:\n            out[clean_key].append(cur_flat.get(key))\n    # special case for back-compatibility with fasta specifications -- yuck\n    if \"reference__fasta__base\" not in out and \"reference__fasta\" in out:\n        out[\"reference__fasta__base\"] = out[\"reference__fasta\"]\n        del out[\"reference__fasta\"]\n    return _samplejson_to_inputs(out), out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a list of genome indexes into a single file plus secondary files.", "response": "def _indexes_to_secondary_files(gresources, genome_build):\n    \"\"\"Convert a list of genome indexes into a single file plus secondary files.\n\n    This ensures that all indices are staged together in a single directory.\n    \"\"\"\n    out = {}\n    for refname, val in gresources.items():\n        if isinstance(val, dict) and \"indexes\" in val:\n            # list of indexes -- aligners\n            if len(val.keys()) == 1:\n                indexes = sorted(val[\"indexes\"])\n                if len(indexes) == 0:\n                    if refname not in alignment.allow_noindices():\n                        raise ValueError(\"Did not find indexes for %s: %s\" % (refname, val))\n                elif len(indexes) == 1:\n                    val = {\"indexes\": indexes[0]}\n                else:\n                    val = {\"indexes\": {\"base\": indexes[0], \"indexes\": indexes[1:]}}\n            # directory plus indexes -- snpEff\n            elif \"base\" in val and os.path.isdir(val[\"base\"]) and len(val[\"indexes\"]) > 0:\n                indexes = val[\"indexes\"]\n                val = {\"base\": indexes[0], \"indexes\": indexes[1:]}\n        elif isinstance(val, dict) and genome_build in val:\n            val = _indexes_to_secondary_files(val, genome_build)\n        out[refname] = val\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding supplementary information to inputs from file values.", "response": "def _add_suppl_info(inp, val):\n    \"\"\"Add supplementary information to inputs from file values.\n    \"\"\"\n    inp[\"type\"] = _get_avro_type(val)\n    secondary = _get_secondary_files(val)\n    if secondary:\n        inp[\"secondaryFiles\"] = secondary\n    return inp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving associated secondary files.", "response": "def _get_secondary_files(val):\n    \"\"\"Retrieve associated secondary files.\n\n    Normalizes input values into definitions of available secondary files.\n    Requires indices to be present in all files, since declared CWL secondary\n    files are not optional. So if we have a mix of BAM (bai) and fastq (gbi) we\n    ignore the existing indices and will have to regenerate during processing.\n    \"\"\"\n    out = []\n    if isinstance(val, (tuple, list)):\n        s_counts = collections.defaultdict(int)\n        for x in val:\n            for s in _get_secondary_files(x):\n                s_counts[s] += 1\n        for s, count in s_counts.items():\n            if s and s not in out and count == len([x for x in val if x]):\n                out.append(s)\n    elif isinstance(val, dict) and (val.get(\"class\") == \"File\" or \"File\" in val.get(\"class\")):\n        if \"secondaryFiles\" in val:\n            for sf in [x[\"path\"] for x in val[\"secondaryFiles\"]]:\n                rext = _get_relative_ext(val[\"path\"], sf)\n                if rext and rext not in out:\n                    out.append(rext)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving relative extension given the original and secondary files.", "response": "def _get_relative_ext(of, sf):\n    \"\"\"Retrieve relative extension given the original and secondary files.\n    \"\"\"\n    def half_finished_trim(orig, prefix):\n        return (os.path.basename(prefix).count(\".\") > 0 and\n                os.path.basename(orig).count(\".\") == os.path.basename(prefix).count(\".\"))\n    # Handle remote files\n    if of.find(\":\") > 0:\n        of = os.path.basename(of.split(\":\")[-1])\n    if sf.find(\":\") > 0:\n        sf = os.path.basename(sf.split(\":\")[-1])\n    prefix = os.path.commonprefix([sf, of])\n    while prefix.endswith(\".\") or (half_finished_trim(sf, prefix) and half_finished_trim(of, prefix)):\n        prefix = prefix[:-1]\n    exts_to_remove = of.replace(prefix, \"\")\n    ext_to_add = sf.replace(prefix, \"\")\n    # Return extensions relative to original\n    if not exts_to_remove or exts_to_remove.startswith(\".\"):\n        return str(\"^\" * exts_to_remove.count(\".\") + ext_to_add)\n    else:\n        raise ValueError(\"No cross platform way to reference complex extension: %s %s\" % (sf, of))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninferring avro type for the current input.", "response": "def _get_avro_type(val):\n    \"\"\"Infer avro type for the current input.\n    \"\"\"\n    if isinstance(val, dict):\n        assert val.get(\"class\") == \"File\" or \"File\" in val.get(\"class\")\n        return \"File\"\n    elif isinstance(val, (tuple, list)):\n        types = []\n        for ctype in [_get_avro_type(v) for v in val]:\n            if isinstance(ctype, dict):\n                nested_types = [x[\"items\"] for x in types if isinstance(x, dict)]\n                if ctype[\"items\"] not in nested_types:\n                    if isinstance(ctype[\"items\"], (list, tuple)):\n                        for t in ctype[\"items\"]:\n                            if t not in types:\n                                types.append(t)\n                    else:\n                        if ctype not in types:\n                            types.append(ctype)\n            elif isinstance(ctype, (list, tuple)):\n                for x in ctype:\n                    if x not in types:\n                        types.append(x)\n            elif ctype not in types:\n                types.append(ctype)\n        # handle empty types, allow null\n        if len(types) == 0:\n            types = [\"null\"]\n            # empty lists\n            if isinstance(val, (list, tuple)) and len(val) == 0:\n                types.append({\"type\": \"array\", \"items\": [\"null\"]})\n        types = _avoid_duplicate_arrays(types)\n        # Avoid empty null only arrays which confuse some runners\n        if len(types) == 1 and types[0] == \"null\":\n            types.append(\"string\")\n        return {\"type\": \"array\", \"items\": (types[0] if len(types) == 1 else types)}\n    elif val is None:\n        return [\"null\"]\n    # encode booleans as string True/False and unencode on other side\n    elif isinstance(val, bool) or isinstance(val, six.string_types) and val.lower() in [\"true\", \"false\", \"none\"]:\n        return [\"string\", \"null\", \"boolean\"]\n    elif isinstance(val, int):\n        return \"long\"\n    elif isinstance(val, float):\n        return \"double\"\n    else:\n        return \"string\""}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _avoid_duplicate_arrays(types):\n    arrays = [t for t in types if isinstance(t, dict) and t[\"type\"] == \"array\"]\n    others = [t for t in types if not (isinstance(t, dict) and t[\"type\"] == \"array\")]\n    if arrays:\n        items = set([])\n        for t in arrays:\n            if isinstance(t[\"items\"], (list, tuple)):\n                items |= set(t[\"items\"])\n            else:\n                items.add(t[\"items\"])\n        if len(items) == 1:\n            items = items.pop()\n        else:\n            items = sorted(list(items))\n        arrays = [{\"type\": \"array\", \"items\": items}]\n    return others + arrays", "response": "Collapse arrays when we have multiple types."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts sample output into inputs for CWL configuration files with types.", "response": "def _samplejson_to_inputs(svals):\n    \"\"\"Convert sample output into inputs for CWL configuration files, with types.\n    \"\"\"\n    out = []\n    for key, val in svals.items():\n        out.append(_add_suppl_info({\"id\": \"%s\" % key}, val))\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts nested dictionary into CWL data flatening and marking up files.", "response": "def _to_cwldata(key, val, get_retriever):\n    \"\"\"Convert nested dictionary into CWL data, flatening and marking up files.\n\n    Moves file objects to the top level, enabling insertion in CWL inputs/outputs.\n    \"\"\"\n    out = []\n    if isinstance(val, dict):\n        if len(val) == 2 and \"base\" in val and \"indexes\" in val:\n            if len(val[\"indexes\"]) > 0 and val[\"base\"] == val[\"indexes\"][0]:\n                out.append((\"%s__indexes\" % key, _item_to_cwldata(val[\"base\"], get_retriever)))\n            else:\n                out.append((key, _to_cwlfile_with_indexes(val, get_retriever)))\n        # Dump shared nested keys like resources as a JSON string\n        elif key in workflow.ALWAYS_AVAILABLE or key in workflow.STRING_DICT:\n            out.append((key, _item_to_cwldata(json.dumps(val), get_retriever)))\n        elif key in workflow.FLAT_DICT:\n            flat = []\n            for k, vs in val.items():\n                if not isinstance(vs, (list, tuple)):\n                    vs = [vs]\n                for v in vs:\n                    flat.append(\"%s:%s\" % (k, v))\n            out.append((key, _item_to_cwldata(flat, get_retriever)))\n        else:\n            remain_val = {}\n            for nkey, nval in val.items():\n                cur_nkey = \"%s__%s\" % (key, nkey)\n                cwl_nval = _item_to_cwldata(nval, get_retriever)\n                if isinstance(cwl_nval, dict):\n                    out.extend(_to_cwldata(cur_nkey, nval, get_retriever))\n                elif key in workflow.ALWAYS_AVAILABLE:\n                    remain_val[nkey] = nval\n                else:\n                    out.append((cur_nkey, cwl_nval))\n            if remain_val:\n                out.append((key, json.dumps(remain_val, sort_keys=True, separators=(',', ':'))))\n    else:\n        out.append((key, _item_to_cwldata(val, get_retriever)))\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _to_cwlfile_with_indexes(val, get_retriever):\n    val[\"indexes\"] = _index_blacklist(val[\"indexes\"])\n    tval = {\"base\": _remove_remote_prefix(val[\"base\"]),\n            \"indexes\": [_remove_remote_prefix(f) for f in val[\"indexes\"]]}\n    # Standard named set of indices, like bwa\n    # Do not include snpEff, which we need to isolate inside a nested directory\n    # hisat2 indices do also not localize cleanly due to compilicated naming\n    cp_dir, cp_base = os.path.split(os.path.commonprefix([tval[\"base\"]] + tval[\"indexes\"]))\n    if (cp_base and cp_dir == os.path.dirname(tval[\"base\"]) and\n            not (\"/snpeff/\" in cp_dir or \"/hisat2\" in cp_dir)):\n        return _item_to_cwldata(val[\"base\"], get_retriever, val[\"indexes\"])\n    else:\n        dirname = os.path.dirname(tval[\"base\"])\n        assert all([x.startswith(dirname) for x in tval[\"indexes\"]])\n        return {\"class\": \"File\", \"path\": directory_tarball(dirname)}", "response": "Convert a read with ready to go indexes into a CWL file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_secondary_if_exists(secondary, out, get_retriever):\n    secondary = [_file_local_or_remote(y, get_retriever) for y in secondary]\n    secondary = [z for z in secondary if z]\n    if secondary:\n        out[\"secondaryFiles\"] = [{\"class\": \"File\", \"path\": f} for f in secondary]\n    return out", "response": "Add secondary files only if present locally or remotely."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if a local or remote file exists.", "response": "def _file_local_or_remote(f, get_retriever):\n    \"\"\"Check for presence of a local or remote file.\n    \"\"\"\n    if os.path.exists(f):\n        return f\n    integration, config = get_retriever.integration_and_config(f)\n    if integration:\n        return integration.file_exists(f, config)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef directory_tarball(dirname):\n    assert os.path.isdir(dirname), dirname\n    base_dir, tarball_dir = os.path.split(dirname)\n    while not os.path.exists(os.path.join(base_dir, \"seq\")) and base_dir and base_dir != \"/\":\n        base_dir, extra_tarball = os.path.split(base_dir)\n        tarball_dir = os.path.join(extra_tarball, tarball_dir)\n    if base_dir == \"/\" and not os.path.exists(os.path.join(base_dir, \"seq\")):\n        raise ValueError(\"Did not find relative directory to create tarball for %s\" % dirname)\n    tarball = os.path.join(base_dir, \"%s-wf.tar.gz\" % (tarball_dir.replace(os.path.sep, \"--\")))\n    if not utils.file_exists(tarball):\n        print(\"Preparing CWL input tarball: %s\" % tarball)\n        with file_transaction({}, tarball) as tx_tarball:\n            with utils.chdir(base_dir):\n                with tarfile.open(tx_tarball, \"w:gz\") as tar:\n                    tar.add(tarball_dir)\n    return tarball", "response": "Create a tarball of a directory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwalking over key - value tuples applying adjust_fn to files.", "response": "def _adjust_files(xs, adjust_fn):\n    \"\"\"Walk over key/value, tuples applying adjust_fn to files.\n    \"\"\"\n    if isinstance(xs, dict):\n        if \"path\" in xs:\n            out = {}\n            out[\"path\"] = adjust_fn(xs[\"path\"])\n            for k, vs in xs.items():\n                if k != \"path\":\n                    out[k] = _adjust_files(vs, adjust_fn)\n            return out\n        else:\n            out = {}\n            for k, vs in xs.items():\n                out[k] = _adjust_files(vs, adjust_fn)\n            return out\n    elif isinstance(xs, (list, tuple)):\n        return [_adjust_files(x, adjust_fn) for x in xs]\n    else:\n        return xs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate estimations of input file sizes for disk usage approximation.", "response": "def _calc_input_estimates(keyvals, get_retriever):\n    \"\"\"Calculate estimations of input file sizes for disk usage approximation.\n\n    These are current dominated by fastq/BAM sizes, so estimate based on that.\n    \"\"\"\n    out = {}\n    for key, val in keyvals.items():\n        size = _calc_file_size(val, 0, get_retriever)\n        if size:\n            out[key] = size\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the size of a file in megabytes including querying remote integrations", "response": "def _get_file_size(path, get_retriever):\n    \"\"\"Return file size in megabytes, including querying remote integrations\n    \"\"\"\n    integration, config = get_retriever.integration_and_config(path)\n    if integration:\n        return integration.file_size(path, config)\n    elif os.path.exists(path):\n        return os.path.getsize(path) / (1024.0 * 1024.0)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a retriever and configuration for the given file path.", "response": "def integration_and_config(self, path):\n        \"\"\"Get a retriever and configuration for the given file path.\n        \"\"\"\n        if path.startswith(tuple(INTEGRATION_MAP.keys())):\n            key = INTEGRATION_MAP[path.split(\":\")[0] + \":\"]\n            integration = self._integrations.get(key)\n            config = {}\n            for sample in self._samples:\n                config = tz.get_in([\"config\", key], sample)\n                if config:\n                    break\n            return integration, config\n\n        return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the initial se. rda object using sinclecell - experiment", "response": "def make_scrnaseq_object(samples):\n    \"\"\"\n    load the initial se.rda object using sinclecell-experiment\n    \"\"\"\n    local_sitelib = R_sitelib()\n    counts_dir = os.path.dirname(dd.get_in_samples(samples, dd.get_combined_counts))\n    gtf_file = dd.get_in_samples(samples, dd.get_transcriptome_gtf)\n    if not gtf_file:\n        gtf_file = dd.get_in_samples(samples, dd.get_gtf_file)\n    rda_file = os.path.join(counts_dir, \"se.rda\")\n    if not file_exists(rda_file):\n        with file_transaction(rda_file) as tx_out_file:\n            rcode = \"%s-run.R\" % os.path.splitext(rda_file)[0]\n            rrna_file = \"%s-rrna.txt\" % os.path.splitext(rda_file)[0]\n            rrna_file = _find_rRNA_genes(gtf_file, rrna_file)\n            with open(rcode, \"w\") as out_handle:\n                out_handle.write(_script.format(**locals()))\n            rscript = Rscript_cmd()\n            try:\n                # do.run([rscript, \"--no-environ\", rcode],\n                #        \"SingleCellExperiment\",\n                #        log_error=False)\n                rda_file = rcode\n            except subprocess.CalledProcessError as msg:\n                logger.exception()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef runner(parallel, config):\n    def run_parallel(fn_name, items):\n        items = [x for x in items if x is not None]\n        if len(items) == 0:\n            return []\n        items = diagnostics.track_parallel(items, fn_name)\n        fn, fn_name = (fn_name, fn_name.__name__) if callable(fn_name) else (get_fn(fn_name, parallel), fn_name)\n        logger.info(\"multiprocessing: %s\" % fn_name)\n        if \"wrapper\" in parallel:\n            wrap_parallel = {k: v for k, v in parallel.items() if k in set([\"fresources\", \"checkpointed\"])}\n            items = [[fn_name] + parallel.get(\"wrapper_args\", []) + [wrap_parallel] + list(x) for x in items]\n        return run_multicore(fn, items, config, parallel=parallel)\n    return run_parallel", "response": "Runs functions on multiple cores on the current machine."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef zeromq_aware_logging(f):\n    @functools.wraps(f)\n    def wrapper(*args, **kwargs):\n        config = None\n        for arg in args:\n            if config_utils.is_std_config_arg(arg):\n                config = arg\n                break\n            elif config_utils.is_nested_config_arg(arg):\n                config = arg[\"config\"]\n            elif isinstance(arg, (list, tuple)) and config_utils.is_nested_config_arg(arg[0]):\n                config = arg[0][\"config\"]\n                break\n        assert config, \"Could not find config dictionary in function arguments.\"\n        if config.get(\"parallel\", {}).get(\"log_queue\") and not config.get(\"parallel\", {}).get(\"wrapper\"):\n            handler = setup_local_logging(config, config[\"parallel\"])\n        else:\n            handler = None\n        try:\n            out = f(*args, **kwargs)\n        finally:\n            if handler and hasattr(handler, \"close\"):\n                handler.close()\n        return out\n    return wrapper", "response": "Decorator that ensures multiprocessing logging uses ZeroMQ queues. This is a wrapper function that does not work with local logging."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns the function using multiple cores on the given items.", "response": "def run_multicore(fn, items, config, parallel=None):\n    \"\"\"Run the function using multiple cores on the given items to process.\n    \"\"\"\n    if len(items) == 0:\n        return []\n    if parallel is None or \"num_jobs\" not in parallel:\n        if parallel is None:\n            parallel = {\"type\": \"local\", \"cores\": config[\"algorithm\"].get(\"num_cores\", 1)}\n        sysinfo = system.get_info({}, parallel)\n        parallel = resources.calculate(parallel, items, sysinfo, config,\n                                       parallel.get(\"multiplier\", 1),\n                                       max_multicore=int(parallel.get(\"max_multicore\", sysinfo[\"cores\"])))\n    items = [config_utils.add_cores_to_config(x, parallel[\"cores_per_job\"]) for x in items]\n    if joblib is None:\n        raise ImportError(\"Need joblib for multiprocessing parallelization\")\n    out = []\n    for data in joblib.Parallel(parallel[\"num_jobs\"], batch_size=1, backend=\"multiprocessing\")(joblib.delayed(fn)(*x) for x in items):\n        if data:\n            out.extend(data)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_umis_with_fastp(read_fq, umi_fq, out_fq, cores):\n    with utils.open_gzipsafe(umi_fq) as in_handle:\n        in_handle.readline()  # name\n        umi_size = len(in_handle.readline().strip())\n    cmd = (\"fastp -Q -A -L -G -w 1 --in1 {read_fq} --in2 {umi_fq} \"\n           \"--umi --umi_prefix UMI --umi_loc read2 --umi_len {umi_size} \"\n           \"--out1 >(bgzip --threads {cores} -c > {out_fq}) --out2 /dev/null \"\n           \"-j /dev/null -h /dev/null\")\n    do.run(cmd.format(**locals()), \"Add UMIs to fastq file with fastp\")", "response": "Add UMIs to reads from separate UMI file using fastp."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding UMI file using different naming schemes.", "response": "def _find_umi(files):\n    \"\"\"Find UMI file using different naming schemes.\n\n    R1/R2/R3 => R1/R3 with R2 UMI\n    R1/R2/I1 => R1/R2 with I1 UMI\n    \"\"\"\n    base = os.path.basename(_commonprefix(files))\n\n    def _file_ext(f):\n        exts = utils.splitext_plus(os.path.basename(f).replace(base, \"\"))[0].split(\"_\")\n        exts = [x for x in exts if x]\n        return exts[0]\n\n    exts = dict([(_file_ext(f), f) for f in files])\n    if \"I1\" in exts:\n        return exts[\"R1\"], exts[\"R2\"], exts[\"I1\"]\n    else:\n        assert \"R3\" in exts, exts\n        return exts[\"R1\"], exts[\"R3\"], exts[\"R2\"]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _commonprefix(files):\n    out = os.path.commonprefix(files)\n    out = out.rstrip(\"_R\")\n    out = out.rstrip(\"_I\")\n    out = out.rstrip(\"_\")\n    return out", "response": "Retrieve a common prefix for a list of files."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cutoff_w_expression(vcf_file, expression, data, name=\"+\", filterext=\"\",\n                      extra_cmd=\"\", limit_regions=\"variant_regions\"):\n    \"\"\"Perform cutoff-based soft filtering using bcftools expressions like %QUAL < 20 || DP < 4.\n    \"\"\"\n    base, ext = utils.splitext_plus(vcf_file)\n    out_file = \"{base}-filter{filterext}{ext}\".format(**locals())\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            if vcfutils.vcf_has_variants(vcf_file):\n                bcftools = config_utils.get_program(\"bcftools\", data[\"config\"])\n                bgzip_cmd = \"| bgzip -c\" if out_file.endswith(\".gz\") else \"\"\n                intervals = \"\"\n                if limit_regions == \"variant_regions\":\n                    variant_regions = dd.get_variant_regions(data)\n                    if variant_regions:\n                        intervals = \"-T %s\" % vcfutils.bgzip_and_index(variant_regions, data[\"config\"])\n                cmd = (\"{bcftools} filter -O v {intervals} --soft-filter '{name}' \"\n                       \"-e '{expression}' -m '+' {vcf_file} {extra_cmd} {bgzip_cmd} > {tx_out_file}\")\n                do.run(cmd.format(**locals()),\n                       \"Cutoff-based soft filtering %s with %s\" % (vcf_file, expression), data)\n            else:\n                shutil.copy(vcf_file, out_file)\n    if out_file.endswith(\".vcf.gz\"):\n        out_file = vcfutils.bgzip_and_index(out_file, data[\"config\"])\n    return out_file", "response": "Perform cutoff - based soft filtering using bcftools."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfilters FreeBayes results using cutoff based on Meynert et al s work modeling sensitivity and high depth heterozygote calling.", "response": "def _freebayes_cutoff(in_file, data):\n    \"\"\"Perform filtering of FreeBayes results, flagging low confidence calls.\n\n    Filters using cutoffs on low depth based on Meynert et al's work modeling sensitivity\n    of homozygote and heterozygote calling on depth:\n\n    http://www.ncbi.nlm.nih.gov/pubmed/23773188\n\n    and high depth heterozygote SNP filtering based on Heng Li's work\n    evaluating variant calling artifacts:\n\n    http://arxiv.org/abs/1404.0929\n\n    Tuned based on NA12878 call comparisons to Genome in a Bottle reference genome.\n    \"\"\"\n    if not vcfutils.vcf_has_variants(in_file):\n        base, ext = utils.splitext_plus(in_file)\n        out_file = \"{base}-filter{ext}\".format(**locals())\n        if not utils.file_exists(out_file):\n            shutil.copy(in_file, out_file)\n        if out_file.endswith(\".vcf.gz\"):\n            out_file = vcfutils.bgzip_and_index(out_file, data[\"config\"])\n        return out_file\n\n    depth_thresh, qual_thresh = None, None\n    if _do_high_depth_filter(data):\n        stats = _calc_vcf_stats(in_file)\n        if stats[\"avg_depth\"] > 0:\n            depth_thresh = int(math.ceil(stats[\"avg_depth\"] + 3 * math.pow(stats[\"avg_depth\"], 0.5)))\n            qual_thresh = depth_thresh * 2.0  # Multiplier from default GATK QD cutoff filter\n    filters = ('(AF[0] <= 0.5 && (max(FORMAT/DP) < 4 || (max(FORMAT/DP) < 13 && %QUAL < 10))) || '\n               '(AF[0] > 0.5 && (max(FORMAT/DP) < 4 && %QUAL < 50))')\n    if depth_thresh:\n        filters += ' || (%QUAL < {qual_thresh} && max(FORMAT/DP) > {depth_thresh} && AF[0] <= 0.5)'.format(**locals())\n    return cutoff_w_expression(in_file, filters, data, name=\"FBQualDepth\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetermines if we should do high depth filtering on non - regional calls.", "response": "def _do_high_depth_filter(data):\n    \"\"\"Check if we should do high depth filtering -- only on germline non-regional calls.\n    \"\"\"\n    is_genome = tz.get_in([\"config\", \"algorithm\", \"coverage_interval\"], data, \"\").lower() == \"genome\"\n    is_paired = vcfutils.get_paired_phenotype(data)\n    return is_genome and not is_paired"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating statistics on VCF for filtering saving to a file for quick re - runs.", "response": "def _calc_vcf_stats(in_file):\n    \"\"\"Calculate statistics on VCF for filtering, saving to a file for quick re-runs.\n    \"\"\"\n    out_file = \"%s-stats.yaml\" % utils.splitext_plus(in_file)[0]\n    if not utils.file_exists(out_file):\n        stats = {\"avg_depth\": _average_called_depth(in_file)}\n        with open(out_file, \"w\") as out_handle:\n            yaml.safe_dump(stats, out_handle, default_flow_style=False, allow_unicode=False)\n        return stats\n    else:\n        with open(out_file) as in_handle:\n            stats = yaml.safe_load(in_handle)\n        return stats"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve the average depth of called reads in the provided VCF.", "response": "def _average_called_depth(in_file):\n    \"\"\"Retrieve the average depth of called reads in the provided VCF.\n    \"\"\"\n    import cyvcf2\n    depths = []\n    for rec in cyvcf2.VCF(str(in_file)):\n        d = rec.INFO.get(\"DP\")\n        if d is not None:\n            depths.append(int(d))\n    if len(depths) > 0:\n        return int(math.ceil(numpy.mean(depths)))\n    else:\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfilter Platypus calls removing Q20 filter replacing with quality based filter.", "response": "def platypus(in_file, data):\n    \"\"\"Filter Platypus calls, removing Q20 filter and replacing with depth and quality based filter.\n\n    Platypus uses its own VCF nomenclature: TC == DP, FR == AF\n\n    Platypus gVCF output appears to have an 0/1 index problem so the reference block\n    regions are 1 base outside regions of interest. We avoid limiting regions during\n    filtering when using it.\n    \"\"\"\n    filters = ('(FR[0] <= 0.5 && TC < 4 && %QUAL < 20) || '\n               '(TC < 13 && %QUAL < 10) || '\n               '(FR[0] > 0.5 && TC < 4 && %QUAL < 50)')\n    limit_regions = \"variant_regions\" if not vcfutils.is_gvcf_file(in_file) else None\n    return cutoff_w_expression(in_file, filters, data, name=\"PlatQualDepth\",\n                               extra_cmd=\"| sed 's/\\\\tQ20\\\\t/\\\\tPASS\\\\t/'\", limit_regions=limit_regions)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nperforming cutoff - based soft filtering on GATK SNPs using GATK s default soft filtering.", "response": "def gatk_snp_cutoff(in_file, data):\n    \"\"\"Perform cutoff-based soft filtering on GATK SNPs using best-practice recommendations.\n\n    We have a more lenient mapping quality (MQ) filter compared to GATK defaults.\n    The recommended filter (MQ < 40) is too stringent, so we adjust to 30:\n    http://imgur.com/a/oHRVB\n\n    QD and FS are not calculated when generating gVCF output:\n    https://github.com/broadgsa/gatk-protected/blob/e91472ddc7d58ace52db0cab4d70a072a918d64c/protected/gatk-tools-protected/src/main/java/org/broadinstitute/gatk/tools/walkers/haplotypecaller/HaplotypeCaller.java#L300\n\n    The extra command removes escaped quotes in the VCF output which\n    pyVCF fails on.\n\n    Does not use the GATK best practice recommend SOR filter (SOR > 3.0) as it\n    has a negative impact on sensitivity relative to precision:\n\n    https://github.com/bcbio/bcbio_validations/tree/master/gatk4#na12878-hg38\n    \"\"\"\n    filters = [\"MQRankSum < -12.5\", \"ReadPosRankSum < -8.0\"]\n    # GATK Haplotype caller (v2.2) appears to have much larger HaplotypeScores\n    # resulting in excessive filtering, so avoid this metric\n    variantcaller = utils.get_in(data, (\"config\", \"algorithm\", \"variantcaller\"))\n    if variantcaller not in [\"gatk-haplotype\", \"haplotyper\"]:\n        filters.append(\"HaplotypeScore > 13.0\")\n    # Additional filter metrics, unless using raw GATK HaplotypeCaller or Sentieon gVCFs\n    if not (vcfutils.is_gvcf_file(in_file) and variantcaller in [\"gatk-haplotype\", \"haplotyper\"]):\n        filters += [\"QD < 2.0\"]\n        filters += [\"FS > 60.0\"]\n        filters += _gatk_general()\n        filters += [\"MQ < 30.0\"]\n    return cutoff_w_expression(in_file, 'TYPE=\"snp\" && (%s)' % \" || \".join(filters), data, \"GATKCutoffSNP\", \"SNP\",\n                               extra_cmd=r\"\"\"| sed 's/\\\\\"//g'\"\"\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef random_regions(base, n, size):\n    spread = size // 2\n    base_info = collections.defaultdict(list)\n    for space, start, end in base:\n        base_info[space].append(start + spread)\n        base_info[space].append(end - spread)\n    regions = []\n    for _ in range(n):\n        space = random.choice(base_info.keys())\n        pos = random.randint(min(base_info[space]), max(base_info[space]))\n        regions.append([space, pos-spread, pos+spread])\n    return regions", "response": "Generate n random regions of size in the provided base spread."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef all_regions(self):\n        regions = []\n        for sq in self._bam.header[\"SQ\"]:\n            regions.append((sq[\"SN\"], 1, int(sq[\"LN\"])))\n        return regions", "response": "Get a tuple of all chromosome start and end regions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve the normalized read count in the provided region.", "response": "def read_count(self, space, start, end):\n        \"\"\"Retrieve the normalized read count in the provided region.\n        \"\"\"\n        read_counts = 0\n        for read in self._bam.fetch(space, start, end):\n            read_counts += 1\n        return self._normalize(read_counts, self._total)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef coverage_pileup(self, space, start, end):\n        return ((col.pos, self._normalize(col.n, self._total))\n                for col in self._bam.pileup(space, start, end))", "response": "Retrieve pileup coverage across a specified region."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun PhyloWGS given variant calls CNVs and tumor information.", "response": "def run(vrn_info, cnvs_by_name, somatic_info):\n    \"\"\"Run PhyloWGS given variant calls, CNVs and tumor/normal information.\n    \"\"\"\n    config = {\"sample_size\": 5000}\n    work_dir = _cur_workdir(somatic_info.tumor_data)\n    if \"battenberg\" not in cnvs_by_name:\n        logger.warn(\"PhyloWGS requires Battenberg CNV calls, skipping %s\"\n                    % dd.get_sample_name(somatic_info.tumor_data))\n    else:\n        ssm_file, cnv_file = _prep_inputs(vrn_info, cnvs_by_name[\"battenberg\"], somatic_info, work_dir, config)\n        evolve_file = _run_evolve(ssm_file, cnv_file, work_dir, somatic_info.tumor_data)\n        summary_file = _prepare_summary(evolve_file, ssm_file, cnv_file, work_dir, somatic_info)\n        print(summary_file)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npreparing a summary of PhyloWGS predictions.", "response": "def _prepare_summary(evolve_file, ssm_file, cnv_file, work_dir, somatic_info):\n    \"\"\"Prepare a summary with gene-labelled heterogeneity from PhyloWGS predictions.\n    \"\"\"\n    out_file = os.path.join(work_dir, \"%s-phylowgs.txt\" % somatic_info.tumor_name)\n    if not utils.file_uptodate(out_file, evolve_file):\n        with file_transaction(somatic_info.tumor_data, out_file) as tx_out_file:\n            with open(tx_out_file, \"w\") as out_handle:\n                ssm_locs = _read_ssm_locs(ssm_file)\n                cnv_ssms = _read_cnv_ssms(cnv_file)\n                for i, (ids, tree) in enumerate(_evolve_reader(evolve_file)):\n                    out_handle.write(\"* Tree %s\\n\" % (i + 1))\n                    out_handle.write(\"\\n\" + \"\\n\".join(tree) + \"\\n\\n\")\n                    for nid, freq, gids in ids:\n                        genes = _gids_to_genes(gids, ssm_locs, cnv_ssms, somatic_info.tumor_data)\n                        out_handle.write(\"%s\\t%s\\t%s\\n\" % (nid, freq, \",\".join(genes)))\n                    out_handle.write(\"\\n\")\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts support ids for SNPs and SSMs into associated genes.", "response": "def _gids_to_genes(gids, ssm_locs, cnv_ssms, data):\n    \"\"\"Convert support ids for SNPs and SSMs into associated genes.\n    \"\"\"\n    locs = collections.defaultdict(set)\n    for gid in gids:\n        cur_locs = []\n        try:\n            cur_locs.append(ssm_locs[gid])\n        except KeyError:\n            for ssm_loc in cnv_ssms.get(gid, []):\n                cur_locs.append(ssm_locs[ssm_loc])\n        for chrom, pos in cur_locs:\n            locs[chrom].add(pos)\n    genes = set([])\n    with tx_tmpdir(data) as tmpdir:\n        chrom_prefix = \"chr\" if next(ref.file_contigs(dd.get_ref_file(data))).name.startswith(\"chr\") else \"\"\n        loc_file = os.path.join(tmpdir, \"battenberg_find_genes.bed\")\n        with open(loc_file, \"w\") as out_handle:\n            for chrom in sorted(locs.keys()):\n                for loc in sorted(list(locs[chrom])):\n                    out_handle.write(\"%s%s\\t%s\\t%s\\n\" % (chrom_prefix, chrom, loc - 1, loc))\n        ann_file = annotate.add_genes(loc_file, data, max_distance=10000)\n        for r in pybedtools.BedTool(ann_file):\n            for gene in r.name.split(\",\"):\n                if gene != \".\":\n                    genes.add(gene)\n    return sorted(list(genes))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _evolve_reader(in_file):\n    cur_id_list = None\n    cur_tree = None\n    with open(in_file) as in_handle:\n        for line in in_handle:\n            if line.startswith(\"id,\"):\n                if cur_id_list:\n                    yield cur_id_list, cur_tree\n                cur_id_list = []\n                cur_tree = None\n            elif cur_tree is not None:\n                if line.strip() and not line.startswith(\"Number of non-empty\"):\n                    cur_tree.append(line.rstrip())\n            elif not line.strip() and cur_id_list and len(cur_id_list) > 0:\n                cur_tree = []\n            elif line.strip():\n                parts = []\n                for part in line.strip().split(\"\\t\"):\n                    if part.endswith(\",\"):\n                        part = part[:-1]\n                    parts.append(part)\n                if len(parts) > 4:\n                    nid, freq, _, _, support = parts\n                    cur_id_list.append((nid, freq, support.split(\"; \")))\n    if cur_id_list:\n        yield cur_id_list, cur_tree", "response": "Generator that yields a list of region IDs and trees from a top_k_trees evolve. py file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_cnv_ssms(in_file):\n    out = {}\n    with open(in_file) as in_handle:\n        in_handle.readline()  # header\n        for line in in_handle:\n            parts = line.strip().split()\n            if len(parts) > 3:\n                cnvid, _, _, ssms = parts\n                out[cnvid] = [x.split(\",\")[0] for x in ssms.split(\";\")]\n    return out", "response": "Map CNVs to associated SSMs\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmap SSMs to chromosomal locations.", "response": "def _read_ssm_locs(in_file):\n    \"\"\"Map SSMs to chromosomal locations.\n    \"\"\"\n    out = {}\n    with open(in_file) as in_handle:\n        in_handle.readline()  # header\n        for line in in_handle:\n            sid, loc = line.split()[:2]\n            chrom, pos = loc.split(\"_\")\n            out[sid] = (chrom, int(pos))\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun evolve. py to infer subclonal composition.", "response": "def _run_evolve(ssm_file, cnv_file, work_dir, data):\n    \"\"\"Run evolve.py to infer subclonal composition.\n    \"\"\"\n    exe = os.path.join(os.path.dirname(sys.executable), \"evolve.py\")\n    assert os.path.exists(exe), \"Could not find evolve script for PhyloWGS runs.\"\n    out_dir = os.path.join(work_dir, \"evolve\")\n    out_file = os.path.join(out_dir, \"top_k_trees\")\n    if not utils.file_uptodate(out_file, cnv_file):\n        with file_transaction(data, out_dir) as tx_out_dir:\n            with utils.chdir(tx_out_dir):\n                cmd = [sys.executable, exe, \"-r\", \"42\", ssm_file, cnv_file]\n                do.run(cmd, \"Run PhyloWGS evolution\")\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _prep_inputs(vrn_info, cnv_info, somatic_info, work_dir, config):\n    exe = os.path.join(os.path.dirname(sys.executable), \"create_phylowgs_inputs.py\")\n    assert os.path.exists(exe), \"Could not find input prep script for PhyloWGS runs.\"\n    ssm_file = os.path.join(work_dir, \"ssm_data.txt\")\n    cnv_file = os.path.join(work_dir, \"cnv_data.txt\")\n    if not utils.file_exists(ssm_file) or not utils.file_exists(cnv_file):\n        with file_transaction(somatic_info.tumor_data, ssm_file, cnv_file) as (tx_ssm_file, tx_cnv_file):\n            variant_type, input_vcf_file = _prep_vrn_file(vrn_info[\"vrn_file\"], vrn_info[\"variantcaller\"],\n                                                          work_dir, somatic_info, cnv_info[\"ignore\"], config)\n            input_cnv_file = _prep_cnv_file(cnv_info[\"subclones\"], work_dir, somatic_info)\n            cmd = [sys.executable, exe,\n                   \"--sample-size\", str(config[\"sample_size\"]), \"--tumor-sample\", somatic_info.tumor_name,\n                   \"--battenberg\", input_cnv_file, \"--cellularity\", _read_contam(cnv_info[\"contamination\"]),\n                   \"--output-cnvs\", tx_cnv_file, \"--output-variants\", tx_ssm_file,\n                   \"--variant-type\", variant_type, input_vcf_file]\n            do.run(cmd, \"Prepare PhyloWGS inputs.\")\n    return ssm_file, cnv_file", "response": "Prepare inputs for running PhyloWGS from variant and CNV calls."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprepare Battenberg CNV file for ingest by PhyloWGS.", "response": "def _prep_cnv_file(in_file, work_dir, somatic_info):\n    \"\"\"Prepare Battenberg CNV file for ingest by PhyloWGS.\n\n    The PhyloWGS preparation script does not handle 'chr' prefixed chromosomes (hg19 style)\n    correctly. This converts them over to GRCh37 (no 'chr') style to match preparation\n    work in _prep_vrn_file.\n    \"\"\"\n    out_file = os.path.join(work_dir, \"%s-prep%s\" % utils.splitext_plus(os.path.basename(in_file)))\n    if not utils.file_uptodate(out_file, in_file):\n        with file_transaction(somatic_info.tumor_data, out_file) as tx_out_file:\n            with open(in_file) as in_handle:\n                with open(tx_out_file, \"w\") as out_handle:\n                    out_handle.write(in_handle.readline())  # header\n                    for line in in_handle:\n                        parts = line.split(\"\\t\")\n                        parts[1] = _phylowgs_compatible_chroms(parts[1])\n                        out_handle.write(\"\\t\".join(parts))\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _prep_vrn_file(in_file, vcaller, work_dir, somatic_info, ignore_file, config):\n    if vcaller.startswith(\"vardict\"):\n        variant_type = \"vardict\"\n    elif vcaller == \"mutect\":\n        variant_type = \"mutect-smchet\"\n    else:\n        raise ValueError(\"Unexpected variant caller for PhyloWGS prep: %s\" % vcaller)\n    out_file = os.path.join(work_dir, \"%s-%s-prep.vcf\" % (utils.splitext_plus(os.path.basename(in_file))[0],\n                                                          vcaller))\n    if not utils.file_uptodate(out_file, in_file):\n        check_fn = _min_sample_pass(ignore_file)\n        with file_transaction(somatic_info.tumor_data, out_file) as tx_out_file:\n            tx_out_file_raw = \"%s-raw%s\" % utils.splitext_plus(tx_out_file)\n            # Filter inputs\n            with VariantFile(in_file) as bcf_in:\n                depths = [_sample_depth(rec, somatic_info.tumor_name) for rec in\n                          filter(check_fn, bcf_in)]\n                depths.sort(reverse=True)\n                depth_thresh = depths[:config[\"sample_size\"]][-1] if depths else 0\n            with VariantFile(in_file) as bcf_in:\n                with VariantFile(tx_out_file_raw, \"w\", header=bcf_in.header) as bcf_out:\n                    for rec in bcf_in:\n                        if (check_fn(rec) and\n                              (depth_thresh < 5 or _sample_depth(rec, somatic_info.tumor_name) >= depth_thresh)):\n                            bcf_out.write(rec)\n            # Fix potential chromosome issues\n            with open(tx_out_file_raw) as in_handle:\n                with open(tx_out_file, \"w\") as out_handle:\n                    for line in in_handle:\n                        if not line.startswith(\"#\"):\n                            parts = line.split(\"\\t\")\n                            parts[0] = _phylowgs_compatible_chroms(parts[0])\n                            line = \"\\t\".join(parts)\n                        out_handle.write(line)\n    return variant_type, out_file", "response": "Prepare a single variant file for PhyloWGS pre - processing."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun seqcluster prepare to merge all samples in one file", "response": "def run_prepare(*data):\n    \"\"\"\n    Run seqcluster prepare to merge all samples in one file\n    \"\"\"\n    out_dir = os.path.join(dd.get_work_dir(data[0][0]), \"seqcluster\", \"prepare\")\n    out_dir = os.path.abspath(safe_makedir(out_dir))\n    prepare_dir = os.path.join(out_dir, \"prepare\")\n    tools = dd.get_expression_caller(data[0][0])\n    if len(tools) == 0:\n        logger.info(\"You didn't specify any other expression caller tool.\"\n                       \"You can add to the YAML file:\"\n                       \"expression_caller:[trna, seqcluster, mirdeep2]\")\n    fn = []\n    for sample in data:\n        name = sample[0][\"rgnames\"]['sample']\n        fn.append(\"%s\\t%s\" % (sample[0]['collapse'], name))\n    args = namedtuple('args', 'debug print_debug minc minl maxl out')\n    args = args(False, False, 2, 17, 40, out_dir)\n    ma_out = op.join(out_dir, \"seqs.ma\")\n    seq_out = op.join(out_dir, \"seqs.fastq\")\n    min_shared = max(int(len(fn) / 10.0), 1)\n    if not file_exists(ma_out):\n        seq_l, sample_l = prepare._read_fastq_files(fn, args)\n        with file_transaction(ma_out) as ma_tx:\n            with open(ma_tx, 'w') as ma_handle:\n                with open(seq_out, 'w') as seq_handle:\n                    logger.info(\"Prepare seqs.fastq with -minl 17 -maxl 40 -minc 2 --min_shared 0.1\")\n                    prepare._create_matrix_uniq_seq(sample_l, seq_l, ma_handle, seq_handle, min_shared)\n\n    for sample in data:\n        sample[0][\"seqcluster_prepare_ma\"] = ma_out\n        sample[0][\"seqcluster_prepare_fastq\"] = seq_out\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_align(*data):\n    work_dir = dd.get_work_dir(data[0][0])\n    out_dir = op.join(work_dir, \"seqcluster\", \"prepare\")\n    seq_out = op.join(out_dir, \"seqs.fastq\")\n    bam_dir = op.join(work_dir, \"align\")\n    new_bam_file = op.join(bam_dir, \"seqs.bam\")\n    tools = dd.get_expression_caller(data[0][0])\n    if not file_exists(new_bam_file):\n        sample = process_alignment(data[0][0], [seq_out, None])\n        bam_file = dd.get_work_bam(sample[0][0])\n        shutil.move(bam_file, new_bam_file)\n        shutil.move(bam_file + \".bai\", new_bam_file + \".bai\")\n        shutil.rmtree(op.join(bam_dir, sample[0][0][\"rgnames\"]['sample']))\n    for sample in data:\n        # sample[0][\"align_bam\"] = sample[0][\"clean_fastq\"]\n        sample[0][\"cluster_bam\"] = new_bam_file\n\n    if \"mirdeep2\" in tools:\n        novel_db = mirdeep.run(data)\n    return data", "response": "Run alignment step only once for each project\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning seqcluster cluster to detect smallRNA clusters", "response": "def run_cluster(*data):\n    \"\"\"\n    Run seqcluster cluster to detect smallRNA clusters\n    \"\"\"\n    sample = data[0][0]\n    tools = dd.get_expression_caller(data[0][0])\n    work_dir = dd.get_work_dir(sample)\n    out_dir = op.join(work_dir, \"seqcluster\", \"cluster\")\n    out_dir = op.abspath(safe_makedir(out_dir))\n    prepare_dir = op.join(work_dir, \"seqcluster\", \"prepare\")\n    bam_file = data[0][0][\"cluster_bam\"]\n    if \"seqcluster\" in tools:\n        gtf_file = dd.get_transcriptome_gtf(sample) if dd.get_transcriptome_gtf(sample) else dd.get_srna_gtf_file(sample)\n        sample[\"seqcluster\"] = _cluster(bam_file, data[0][0][\"seqcluster_prepare_ma\"],\n                                        out_dir, dd.get_ref_file(sample),\n                                        gtf_file)\n        sample[\"report\"] = _report(sample, dd.get_ref_file(sample))\n\n    if \"mirge\" in tools:\n        sample[\"mirge\"] = mirge.run(data)\n\n    out_mirna = _make_isomir_counts(data, out_dir=op.join(work_dir, \"mirbase\"))\n    if out_mirna:\n        sample = dd.set_mirna_counts(sample, out_mirna[0])\n        sample = dd.set_isomir_counts(sample, out_mirna[1])\n\n    out_novel = _make_isomir_counts(data, \"seqbuster_novel\", op.join(work_dir, \"mirdeep2\"), \"_novel\")\n    if out_novel:\n        sample = dd.set_novel_mirna_counts(sample, out_novel[0])\n        sample = dd.set_novel_isomir_counts(sample, out_novel[1])\n    data[0][0] = sample\n    data = spikein.combine_spikein(data)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncluster the sequence using seqcluster", "response": "def _cluster(bam_file, ma_file, out_dir, reference, annotation_file=None):\n    \"\"\"\n    Connect to seqcluster to run cluster with python directly\n    \"\"\"\n    seqcluster = op.join(get_bcbio_bin(), \"seqcluster\")\n    # cl = [\"cluster\", \"-o\", out_dir, \"-m\", ma_file, \"-a\", bam_file, \"-r\", reference]\n    if annotation_file:\n        annotation_file = \"-g \" + annotation_file\n    else:\n        annotation_file = \"\"\n\n    if not file_exists(op.join(out_dir, \"counts.tsv\")):\n        cmd = (\"{seqcluster} cluster -o {out_dir} -m {ma_file} -a {bam_file} -r {reference} {annotation_file}\")\n        do.run(cmd.format(**locals()), \"Running seqcluster.\")\n    counts = op.join(out_dir, \"counts.tsv\")\n    stats = op.join(out_dir, \"read_stats.tsv\")\n    json = op.join(out_dir, \"seqcluster.json\")\n    return {'out_dir': out_dir, 'count_file': counts, 'stat_file': stats, 'json': json}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _report(data, reference):\n    seqcluster = op.join(get_bcbio_bin(), \"seqcluster\")\n    work_dir = dd.get_work_dir(data)\n    out_dir = safe_makedir(os.path.join(work_dir, \"seqcluster\", \"report\"))\n    out_file = op.join(out_dir, \"seqcluster.db\")\n    json = op.join(work_dir, \"seqcluster\", \"cluster\", \"seqcluster.json\")\n    cmd = (\"{seqcluster} report -o {out_dir} -r {reference} -j {json}\")\n    if not file_exists(out_file):\n        do.run(cmd.format(**locals()), \"Run report on clusters\")\n    return out_file", "response": "Run seqcluster report to get browser options for results\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a Rmd report for small RNAseq analysis", "response": "def report(data):\n    \"\"\"Create a Rmd report for small RNAseq analysis\"\"\"\n    work_dir = dd.get_work_dir(data[0][0])\n    out_dir = op.join(work_dir, \"report\")\n    safe_makedir(out_dir)\n    summary_file = op.join(out_dir, \"summary.csv\")\n    with file_transaction(summary_file) as out_tx:\n        with open(out_tx, 'w') as out_handle:\n            out_handle.write(\"sample_id,%s\\n\" % _guess_header(data[0][0]))\n            for sample in data:\n                info = sample[0]\n                group = _guess_group(info)\n                files = info[\"seqbuster\"] if \"seqbuster\" in info else \"None\"\n                out_handle.write(\",\".join([dd.get_sample_name(info),\n                                           group]) + \"\\n\")\n    _modify_report(work_dir, out_dir)\n    return summary_file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nguessing the header for the first group", "response": "def _guess_header(info):\n    \"\"\"Add the first group to get report with some factor\"\"\"\n    value = \"group\"\n    if \"metadata\" in info:\n        if info[\"metadata\"]:\n            return \",\".join(map(str, info[\"metadata\"].keys()))\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nguess the group of the report with some factor", "response": "def _guess_group(info):\n    \"\"\"Add the first group to get report with some factor\"\"\"\n    value = \"fake\"\n    if \"metadata\" in info:\n        if info[\"metadata\"]:\n            return \",\".join(map(str, info[\"metadata\"].values()))\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _modify_report(summary_path, out_dir):\n    summary_path = op.abspath(summary_path)\n    template = op.normpath(op.join(op.dirname(op.realpath(template_seqcluster.__file__)), \"report.rmd\"))\n    content = open(template).read()\n    out_content = string.Template(content).safe_substitute({'path_abs': summary_path})\n    out_file = op.join(out_dir, \"srna_report.rmd\")\n    with open(out_file, 'w') as out_handle:\n        out_handle.write(out_content)\n    return out_file", "response": "Read Rmd template and dump with project path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating counts matrix for isomir.", "response": "def _make_isomir_counts(data, srna_type=\"seqbuster\", out_dir=None, stem=\"\"):\n    \"\"\"\n    Parse miraligner files to create count matrix.\n    \"\"\"\n    work_dir = dd.get_work_dir(data[0][0])\n    if not out_dir:\n        out_dir = op.join(work_dir, \"mirbase\")\n    out_novel_isomir = append_stem(op.join(out_dir, \"counts.tsv\"), stem)\n    out_novel_mirna = append_stem(op.join(out_dir, \"counts_mirna.tsv\"), stem)\n    logger.debug(\"Create %s count data at %s.\" % (srna_type, out_dir))\n    if file_exists(out_novel_mirna):\n        return [out_novel_mirna, out_novel_isomir]\n    out_dts = []\n    for sample in data:\n        if sample[0].get(srna_type):\n            miraligner_fn = sample[0][srna_type]\n            reads = _read_miraligner(miraligner_fn)\n            if reads:\n                out_file, dt, dt_pre = _tab_output(reads, miraligner_fn + \".back\", dd.get_sample_name(sample[0]))\n                out_dts.append(dt)\n            else:\n                logger.debug(\"WARNING::%s has NOT miRNA annotated for %s. Check if fasta files is small or species value.\" % (dd.get_sample_name(sample[0]), srna_type))\n    if out_dts:\n        out_files = _create_counts(out_dts, out_dir)\n        out_files = [move_safe(out_files[0], out_novel_isomir), move_safe(out_files[1], out_novel_mirna)]\n        return out_files\n    else:\n        logger.debug(\"WARNING::any samples have miRNA annotated for %s. Check if fasta files is small or species value.\" % srna_type)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _calc_regional_coverage(in_bam, chrom, start, end, samplename, work_dir, data):\n    region_bt = pybedtools.BedTool(\"%s\\t%s\\t%s\\n\" % (chrom, start, end), from_string=True).saveas()\n    region_file = region_bt.fn\n    coords = \"%s:%s-%s\" % (chrom, start, end)\n    tx_tmp_file = os.path.join(work_dir, \"coverage-%s-%s.txt\" % (samplename, coords.replace(\":\", \"_\")))\n    samtools = config_utils.get_program(\"samtools\", data)\n    bedtools = config_utils.get_program(\"bedtools\", data)\n    cmd = (\"{samtools} view -b {in_bam} {coords} | \"\n           \"{bedtools} coverage -a {region_file} -b - -d > {tx_tmp_file}\")\n    do.run(cmd.format(**locals()), \"Plotting coverage for %s %s\" % (samplename, coords))\n    names = [\"chom\", \"start\", \"end\", \"offset\", \"coverage\"]\n    df = pd.io.parsers.read_table(tx_tmp_file, sep=\"\\t\", header=None,\n                                  names=names).dropna()\n    os.remove(tx_tmp_file)\n    df[\"sample\"] = samplename\n    df[\"chrom\"] = chrom\n    df[\"position\"] = df[\"start\"] + df[\"offset\"] - 1\n    return df[[\"chrom\", \"position\", \"coverage\", \"sample\"]]", "response": "Calculate the coverage for each base in a region."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _combine_regional_coverage(in_bams, samplenames, chrom, start, end, work_dir, data):\n    dfs = [_calc_regional_coverage(bam, chrom, start, end, sample, work_dir, data) for bam, sample\n           in zip(in_bams, samplenames)]\n    return rbind(dfs)", "response": "Combine the regional coverage of each sample in a list of bams and a region."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _split_regions(chrom, start, end):\n    window_size = 1e5\n    if end - start < window_size * 5:\n        return [(chrom, start, end)]\n    else:\n        out = []\n        for r in pybedtools.BedTool().window_maker(w=window_size,\n                                                   b=pybedtools.BedTool(\"%s\\t%s\\t%s\" % (chrom, start, end),\n                                                                        from_string=True)):\n            out.append((r.chrom, r.start, r.end))\n        return out", "response": "Split regions longer than 100kb into smaller sections."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot_multiple_regions_coverage(samples, out_file, data, region_bed=None, stem_bed=None):\n    mpl.use('Agg', force=True)\n    PAD = 100\n    if file_exists(out_file):\n        return out_file\n    in_bams = [dd.get_align_bam(x) for x in samples]\n    samplenames = [dd.get_sample_name(x) for x in samples]\n    if isinstance(region_bed, six.string_types):\n        region_bed = pybedtools.BedTool(region_bed)\n    if isinstance(stem_bed, six.string_types):\n        stem_bed = pybedtools.BedTool(stem_bed)\n    if stem_bed is not None:  # tabix indexed bedtools eval to false\n        stem_bed = stem_bed.tabix()\n    plt.clf()\n    plt.cla()\n    with file_transaction(out_file) as tx_out_file:\n        with backend_pdf.PdfPages(tx_out_file) as pdf_out:\n            sns.despine()\n            for line in region_bed:\n                for chrom, start, end in _split_regions(line.chrom, max(line.start - PAD, 0),\n                                                        line.end + PAD):\n                    df = _combine_regional_coverage(in_bams, samplenames, chrom,\n                                                    start, end, os.path.dirname(tx_out_file), data)\n                    plot = sns.tsplot(df, time=\"position\", unit=\"chrom\",\n                                      value=\"coverage\", condition=\"sample\")\n                    if stem_bed is not None:  # tabix indexed bedtools eval to false\n                        interval = pybedtools.Interval(chrom, start, end)\n                        _add_stems_to_plot(interval, stem_bed, samples, plot)\n                    plt.title(\"{chrom}:{start}-{end}\".format(**locals()))\n                    pdf_out.savefig(plot.get_figure())\n                    plt.close()\n    return out_file", "response": "Plots a single region coverage for a set of samples."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _config_params(base_config, assoc_files, region, out_file, items):\n    params = []\n    dbsnp = assoc_files.get(\"dbsnp\")\n    if dbsnp:\n        params += [\"--dbsnp\", dbsnp]\n    cosmic = assoc_files.get(\"cosmic\")\n    if cosmic:\n        params += [\"--cosmic\", cosmic]\n    variant_regions = bedutils.population_variant_regions(items)\n    region = subset_variant_regions(variant_regions, region, out_file, items)\n    if region:\n        params += [\"-L\", bamprep.region_to_gatk(region), \"--interval_set_rule\",\n                   \"INTERSECTION\"]\n    # set low frequency calling parameter if adjusted\n    # to set other MuTect parameters on contamination, pass options to resources for mutect\n    # --fraction_contamination --minimum_normal_allele_fraction\n    min_af = tz.get_in([\"algorithm\", \"min_allele_fraction\"], base_config)\n    if min_af:\n        params += [\"--minimum_mutation_cell_fraction\", \"%.2f\" % (min_af / 100.0)]\n    resources = config_utils.get_resources(\"mutect\", base_config)\n    if resources.get(\"options\") is not None:\n        params += [str(x) for x in resources.get(\"options\", [])]\n    # Output quality scores\n    if \"--enable_qscore_output\" not in params:\n        params.append(\"--enable_qscore_output\")\n    # drf not currently supported in MuTect to turn off duplicateread filter\n    # params += gatk.standard_cl_params(items)\n    return params", "response": "Add parameters based on configuration variables associated files and genomic regions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npreparing MuTect call for MuTect.", "response": "def _mutect_call_prep(align_bams, items, ref_file, assoc_files,\n                       region=None, out_file=None):\n    \"\"\"Preparation work for MuTect.\n    \"\"\"\n    base_config = items[0][\"config\"]\n    broad_runner = broad.runner_from_path(\"picard\", base_config)\n    broad_runner.run_fn(\"picard_index_ref\", ref_file)\n\n    broad_runner = broad.runner_from_config(base_config, \"mutect\")\n    _check_mutect_version(broad_runner)\n    for x in align_bams:\n        bam.index(x, base_config)\n\n    paired = vcfutils.get_paired_bams(align_bams, items)\n    if not paired:\n        raise ValueError(\"Specified MuTect calling but 'tumor' phenotype not present in batch\\n\"\n                         \"https://bcbio-nextgen.readthedocs.org/en/latest/contents/\"\n                         \"pipelines.html#cancer-variant-calling\\n\"\n                         \"for samples: %s\" % \", \" .join([dd.get_sample_name(x) for x in items]))\n    params = [\"-R\", ref_file, \"-T\", \"MuTect\", \"-U\", \"ALLOW_N_CIGAR_READS\"]\n    params += [\"--read_filter\", \"NotPrimaryAlignment\"]\n    params += [\"-I:tumor\", paired.tumor_bam]\n    params += [\"--tumor_sample_name\", paired.tumor_name]\n    if paired.normal_bam is not None:\n        params += [\"-I:normal\", paired.normal_bam]\n        params += [\"--normal_sample_name\", paired.normal_name]\n    if paired.normal_panel is not None:\n        params += [\"--normal_panel\", paired.normal_panel]\n    params += _config_params(base_config, assoc_files, region, out_file, items)\n    return broad_runner, params"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mutect_caller(align_bams, items, ref_file, assoc_files, region=None,\n                  out_file=None):\n    \"\"\"Run the MuTect paired analysis algorithm.\n    \"\"\"\n    config = items[0][\"config\"]\n    if out_file is None:\n        out_file = \"%s-paired-variants.vcf.gz\" % os.path.splitext(align_bams[0])[0]\n    if not file_exists(out_file):\n        base_config = items[0][\"config\"]\n        broad_runner = broad.runner_from_config(base_config, \"mutect\")\n        out_file_mutect = (out_file.replace(\".vcf\", \"-mutect.vcf\")\n                           if \"vcf\" in out_file else out_file + \"-mutect.vcf\")\n        broad_runner, params = \\\n            _mutect_call_prep(align_bams, items, ref_file, assoc_files,\n                                   region, out_file_mutect)\n        if (not isinstance(region, (list, tuple)) and\n              not all(has_aligned_reads(x, region) for x in align_bams)):\n            paired = vcfutils.get_paired(items)\n            vcfutils.write_empty_vcf(out_file, samples=[x for x in (paired.tumor_name, paired.normal_name) if x])\n            return\n        out_file_orig = \"%s-orig%s\" % utils.splitext_plus(out_file_mutect)\n        if not file_exists(out_file_orig):\n            with file_transaction(config, out_file_orig) as tx_out_file:\n                # Rationale: MuTect writes another table to stdout, which we don't need\n                params += [\"--vcf\", tx_out_file, \"-o\", os.devnull]\n                broad_runner.run_mutect(params)\n        is_paired = \"-I:normal\" in params\n        if not utils.file_uptodate(out_file_mutect, out_file_orig):\n            out_file_mutect = _fix_mutect_output(out_file_orig, config, out_file_mutect, is_paired)\n        indelcaller = vcfutils.get_indelcaller(base_config)\n        if (\"scalpel\" in indelcaller.lower() and region and isinstance(region, (tuple, list))\n              and chromhacks.is_autosomal_or_sex(region[0])):\n            # Scalpel InDels\n            out_file_indels = (out_file.replace(\".vcf\", \"-somaticIndels.vcf\")\n                               if \"vcf\" in out_file else out_file + \"-somaticIndels.vcf\")\n            if scalpel.is_installed(items[0][\"config\"]):\n                if not is_paired:\n                    vcfutils.check_paired_problems(items)\n                    scalpel._run_scalpel_caller(align_bams, items, ref_file, assoc_files,\n                                                region=region, out_file=out_file_indels)\n                else:\n                    scalpel._run_scalpel_paired(align_bams, items, ref_file, assoc_files,\n                                                region=region, out_file=out_file_indels)\n                out_file = vcfutils.combine_variant_files(orig_files=[out_file_mutect, out_file_indels],\n                                                          out_file=out_file,\n                                                          ref_file=items[0][\"sam_ref\"],\n                                                          config=items[0][\"config\"],\n                                                          region=region)\n            else:\n                utils.symlink_plus(out_file_mutect, out_file)\n        elif \"pindel\" in indelcaller.lower():\n            from bcbio.structural import pindel\n            out_file_indels = (out_file.replace(\".vcf\", \"-somaticIndels.vcf\")\n                               if \"vcf\" in out_file else out_file + \"-somaticIndels.vcf\")\n            if pindel.is_installed(items[0][\"config\"]):\n                pindel._run_tumor_pindel_caller(align_bams, items, ref_file, assoc_files, region=region,\n                                          out_file=out_file_indels)\n                out_file = vcfutils.combine_variant_files(orig_files=[out_file_mutect, out_file_indels],\n                                                          out_file=out_file,\n                                                          ref_file=ref_file,\n                                                          config=items[0][\"config\"],\n                                                          region=region)\n            else:\n                utils.symlink_plus(out_file_mutect, out_file)\n        elif ((\"somaticindeldetector\" in indelcaller.lower() or \"sid\" in indelcaller.lower())\n              and \"appistry\" in broad_runner.get_mutect_version()):\n            # SomaticIndelDetector InDels\n            out_file_indels = (out_file.replace(\".vcf\", \"-somaticIndels.vcf\")\n                               if \"vcf\" in out_file else out_file + \"-somaticIndels.vcf\")\n            params_indels = _SID_call_prep(align_bams, items, ref_file, assoc_files,\n                                           region, out_file_indels)\n            with file_transaction(config, out_file_indels) as tx_out_file:\n                params_indels += [\"-o\", tx_out_file]\n                broad_runner.run_mutect(params_indels)\n            out_file = vcfutils.combine_variant_files(orig_files=[out_file_mutect, out_file_indels],\n                                                      out_file=out_file,\n                                                      ref_file=items[0][\"sam_ref\"],\n                                                      config=items[0][\"config\"],\n                                                      region=region)\n        else:\n            utils.symlink_plus(out_file_mutect, out_file)\n    return out_file", "response": "Run MuTect paired analysis."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _SID_call_prep(align_bams, items, ref_file, assoc_files, region=None, out_file=None):\n    base_config = items[0][\"config\"]\n    for x in align_bams:\n        bam.index(x, base_config)\n\n    params = [\"-R\", ref_file, \"-T\", \"SomaticIndelDetector\", \"-U\", \"ALLOW_N_CIGAR_READS\"]\n    # Limit per base read start count to between 200-10000, i.e. from any base\n    # can no more 10000 new reads begin.\n    # Further, limit maxNumberOfReads accordingly, otherwise SID discards\n    # windows for high coverage panels.\n    paired = vcfutils.get_paired_bams(align_bams, items)\n    params += [\"--read_filter\", \"NotPrimaryAlignment\"]\n    params += [\"-I:tumor\", paired.tumor_bam]\n    min_af = float(get_in(paired.tumor_config, (\"algorithm\", \"min_allele_fraction\"), 10)) / 100.0\n    if paired.normal_bam is not None:\n        params += [\"-I:normal\", paired.normal_bam]\n        # notice there must be at least 4 reads of coverage in normal\n        params += [\"--filter_expressions\", \"T_COV<6||N_COV<4||T_INDEL_F<%s||T_INDEL_CF<0.7\" % min_af]\n    else:\n        params += [\"--unpaired\"]\n        params += [\"--filter_expressions\", \"COV<6||INDEL_F<%s||INDEL_CF<0.7\" % min_af]\n    if region:\n        params += [\"-L\", bamprep.region_to_gatk(region), \"--interval_set_rule\",\n                   \"INTERSECTION\"]\n    return params", "response": "Prepare SID call for SomaticIndelDetector."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadjusts MuTect output to match other callers.", "response": "def _fix_mutect_output(orig_file, config, out_file, is_paired):\n    \"\"\"Adjust MuTect output to match other callers.\n\n    - Rename allelic fraction field in mutect output from FA to FREQ to standarize with other tools\n    - Remove extra 'none' samples introduced when calling tumor-only samples\n    \"\"\"\n    out_file_noc = out_file.replace(\".vcf.gz\", \".vcf\")\n    none_index = -1\n    with file_transaction(config, out_file_noc) as tx_out_file:\n        with open_gzipsafe(orig_file) as in_handle:\n            with open(tx_out_file, 'w') as out_handle:\n                for line in in_handle:\n                    if not is_paired and line.startswith(\"#CHROM\"):\n                        parts = line.rstrip().split(\"\\t\")\n                        none_index = parts.index(\"none\")\n                        del parts[none_index]\n                        line = \"\\t\".join(parts) + \"\\n\"\n                    elif line.startswith(\"##FORMAT=<ID=FA\"):\n                        line = line.replace(\"=FA\", \"=FREQ\")\n                    elif not line.startswith(\"#\"):\n                        if none_index > 0:\n                            parts = line.rstrip().split(\"\\t\")\n                            del parts[none_index]\n                            line = \"\\t\".join(parts) + \"\\n\"\n                        line = line.replace(\"FA\", \"FREQ\")\n                    out_handle.write(line)\n    return bgzip_and_index(out_file_noc, config)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prep_gemini_db(fnames, call_info, samples, extras):\n    data = samples[0]\n    name, caller, is_batch = call_info\n    build_type = _get_build_type(fnames, samples, caller)\n    out_dir = utils.safe_makedir(os.path.join(data[\"dirs\"][\"work\"], \"gemini\"))\n    gemini_vcf = get_multisample_vcf(fnames, name, caller, data)\n    # If we're building a gemini database, normalize the inputs\n    if build_type:\n        passonly = all(\"gemini_allvariants\" not in dd.get_tools_on(d) for d in samples)\n        gemini_vcf = normalize.normalize(gemini_vcf, data, passonly=passonly)\n        decomposed = True\n    else:\n        decomposed = False\n    ann_vcf = run_vcfanno(gemini_vcf, data, decomposed)\n    gemini_db = os.path.join(out_dir, \"%s-%s.db\" % (name, caller))\n    if ann_vcf and build_type and not utils.file_exists(gemini_db):\n        ped_file = create_ped_file(samples + extras, gemini_vcf)\n        # Original approach for hg19/GRCh37\n        if vcfanno.is_human(data, builds=[\"37\"]) and \"gemini_orig\" in build_type:\n            gemini_db = create_gemini_db_orig(gemini_vcf, data, gemini_db, ped_file)\n        else:\n            gemini_db = create_gemini_db(ann_vcf, data, gemini_db, ped_file)\n    # only pass along gemini_vcf_downstream if uniquely created here\n    if os.path.islink(gemini_vcf):\n        gemini_vcf = None\n    return [[(name, caller), {\"db\": gemini_db if utils.file_exists(gemini_db) else None,\n                              \"vcf\": ann_vcf or gemini_vcf,\n                              \"decomposed\": decomposed}]]", "response": "Prepare a gemini database from VCF inputs prepared with snpEff."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _back_compatible_gemini(conf_files, data):\n    if vcfanno.is_human(data, builds=[\"37\"]):\n        for f in conf_files:\n            if f and os.path.basename(f) == \"gemini.conf\" and os.path.exists(f):\n                with open(f) as in_handle:\n                    for line in in_handle:\n                        if line.startswith(\"file\"):\n                            fname = line.strip().split(\"=\")[-1].replace('\"', '').strip()\n                            if fname.find(\".tidy.\") > 0:\n                                return install.get_gemini_dir(data)\n    return None", "response": "Provide old install directory for configuration with GEMINI supplied tidy VCFs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun vcfanno on a single input file.", "response": "def run_vcfanno(vcf_file, data, decomposed=False):\n    \"\"\"Run vcfanno, providing annotations from external databases if needed.\n\n    Puts together lua and conf files from multiple inputs by file names.\n    \"\"\"\n    conf_files = dd.get_vcfanno(data)\n    if conf_files:\n        with_basepaths = collections.defaultdict(list)\n        gemini_basepath = _back_compatible_gemini(conf_files, data)\n        for f in conf_files:\n            name = os.path.splitext(os.path.basename(f))[0]\n            if f.endswith(\".lua\"):\n                conf_file = None\n                lua_file = f\n            else:\n                conf_file = f\n                lua_file = \"%s.lua\" % utils.splitext_plus(conf_file)[0]\n            if lua_file and not os.path.exists(lua_file):\n                lua_file = None\n            data_basepath = gemini_basepath if name == \"gemini\" else None\n            if conf_file and os.path.exists(conf_file):\n                with_basepaths[(data_basepath, name)].append(conf_file)\n            if lua_file and os.path.exists(lua_file):\n                with_basepaths[(data_basepath, name)].append(lua_file)\n        conf_files = with_basepaths.items()\n    out_file = None\n    if conf_files:\n        VcfannoIn = collections.namedtuple(\"VcfannoIn\", [\"conf\", \"lua\"])\n        bp_files = collections.defaultdict(list)\n        for (data_basepath, name), anno_files in conf_files:\n            anno_files = list(set(anno_files))\n            if len(anno_files) == 1:\n                cur = VcfannoIn(anno_files[0], None)\n            elif len(anno_files) == 2:\n                lua_files = [x for x in anno_files if x.endswith(\".lua\")]\n                assert len(lua_files) == 1, anno_files\n                lua_file = lua_files[0]\n                anno_files.remove(lua_file)\n                cur = VcfannoIn(anno_files[0], lua_file)\n            else:\n                raise ValueError(\"Unexpected annotation group %s\" % anno_files)\n            bp_files[data_basepath].append(cur)\n        for data_basepath, anno_files in bp_files.items():\n            ann_file = vcfanno.run(vcf_file, [x.conf for x in anno_files],\n                                   [x.lua for x in anno_files], data,\n                                   basepath=data_basepath,\n                                   decomposed=decomposed)\n            if ann_file:\n                out_file = ann_file\n                vcf_file = ann_file\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating GEMINI database for loading variants into a GEMINI database.", "response": "def create_gemini_db(gemini_vcf, data, gemini_db=None, ped_file=None):\n    \"\"\"Generalized vcfanno/vcf2db workflow for loading variants into a GEMINI database.\n    \"\"\"\n    if not gemini_db:\n        gemini_db = \"%s.db\" % utils.splitext_plus(gemini_vcf)[0]\n    if not vcfutils.vcf_has_variants(gemini_vcf):\n        return None\n    if not utils.file_exists(gemini_db):\n        with file_transaction(data, gemini_db) as tx_gemini_db:\n            vcf2db = config_utils.get_program(\"vcf2db.py\", data)\n            if \"vcf2db_expand\" in dd.get_tools_on(data):\n                vcf2db_args = [\"--expand\", \"gt_types\", \"--expand\", \"gt_ref_depths\", \"--expand\", \"gt_alt_depths\"]\n            else:\n                vcf2db_args = []\n            cmd = [vcf2db, gemini_vcf, ped_file, tx_gemini_db] + vcf2db_args\n            do.run(cmd, \"GEMINI: create database with vcf2db\")\n    return gemini_db"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_gemini_db_orig(gemini_vcf, data, gemini_db=None, ped_file=None):\n    if not gemini_db:\n        gemini_db = \"%s.db\" % utils.splitext_plus(gemini_vcf)[0]\n    if not utils.file_exists(gemini_db):\n        if not vcfutils.vcf_has_variants(gemini_vcf):\n            return None\n        with file_transaction(data, gemini_db) as tx_gemini_db:\n            gemini = config_utils.get_program(\"gemini\", data[\"config\"])\n            load_opts = \"\"\n            if \"gemini_allvariants\" not in dd.get_tools_on(data):\n                load_opts += \" --passonly\"\n            # For small test files, skip gene table loading which takes a long time\n            if _is_small_vcf(gemini_vcf):\n                load_opts += \" --skip-gene-tables\"\n            if \"/test_automated_output/\" in gemini_vcf:\n                load_opts += \" --test-mode\"\n            # Skip CADD or gerp-bp if neither are loaded\n            gemini_dir = install.get_gemini_dir(data)\n            for skip_cmd, check_file in [(\"--skip-cadd\", \"whole_genome_SNVs.tsv.compressed.gz\")]:\n                if not os.path.exists(os.path.join(gemini_dir, check_file)):\n                    load_opts += \" %s\" % skip_cmd\n            # skip gerp-bp which slows down loading\n            load_opts += \" --skip-gerp-bp \"\n            num_cores = data[\"config\"][\"algorithm\"].get(\"num_cores\", 1)\n            tmpdir = os.path.dirname(tx_gemini_db)\n            eanns = _get_effects_flag(data)\n            # Apply custom resource specifications, allowing use of alternative annotation_dir\n            resources = config_utils.get_resources(\"gemini\", data[\"config\"])\n            gemini_opts = \" \".join([str(x) for x in resources[\"options\"]]) if resources.get(\"options\") else \"\"\n            exports = utils.local_path_export()\n            cmd = (\"{exports} {gemini} {gemini_opts} load {load_opts} \"\n                   \"-v {gemini_vcf} {eanns} --cores {num_cores} \"\n                   \"--tempdir {tmpdir} {tx_gemini_db}\")\n            cmd = cmd.format(**locals())\n            do.run(cmd, \"Create gemini database for %s\" % gemini_vcf, data)\n            if ped_file:\n                cmd = [gemini, \"amend\", \"--sample\", ped_file, tx_gemini_db]\n                do.run(cmd, \"Add PED file to gemini database\", data)\n    return gemini_db", "response": "Create GEMINI specific data loader."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve the affected status of sample.", "response": "def get_affected_status(data):\n    \"\"\"Retrieve the affected/unaffected status of sample.\n\n    Uses unaffected (1), affected (2), unknown (0) coding from PED files:\n\n    http://pngu.mgh.harvard.edu/~purcell/plink/data.shtml#ped\n    \"\"\"\n    affected = set([\"tumor\", \"affected\", \"2\"])\n    unaffected = set([\"normal\", \"unaffected\", \"1\"])\n    phenotype = str(tz.get_in([\"metadata\", \"phenotype\"], data, \"\")).lower()\n    if dd.get_svclass(data) == \"control\":\n        return 1\n    elif phenotype in affected:\n        return 2\n    elif phenotype in unaffected:\n        return 1\n    else:\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve gender from metadata codified as male or female.", "response": "def get_gender(data):\n    \"\"\"Retrieve gender from metadata, codified as male/female/unknown.\n    \"\"\"\n    g = str(dd.get_gender(data))\n    if g and str(g).lower() in [\"male\", \"m\", \"1\"]:\n        return \"male\"\n    elif g and str(g).lower() in [\"female\", \"f\", \"2\"]:\n        return \"female\"\n    else:\n        return \"unknown\""}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_ped_info(data, samples):\n    family_id = tz.get_in([\"metadata\", \"family_id\"], data, None)\n    if not family_id:\n        family_id = _find_shared_batch(samples)\n    return {\n        \"gender\": {\"male\": 1, \"female\": 2, \"unknown\": 0}.get(get_gender(data)),\n        \"individual_id\": dd.get_sample_name(data),\n        \"family_id\": family_id,\n        \"maternal_id\": tz.get_in([\"metadata\", \"maternal_id\"], data, -9),\n        \"paternal_id\": tz.get_in([\"metadata\", \"paternal_id\"], data, -9),\n        \"affected\": get_affected_status(data),\n        \"ethnicity\": tz.get_in([\"metadata\", \"ethnicity\"], data, -9)\n    }", "response": "Retrieve all PED info from metadata\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_ped_file(samples, base_vcf, out_dir=None):\n    out_file = \"%s.ped\" % utils.splitext_plus(base_vcf)[0]\n    if out_dir:\n        out_file = os.path.join(out_dir, os.path.basename(out_file))\n    sample_ped_lines = {}\n    header = [\"#Family_ID\", \"Individual_ID\", \"Paternal_ID\", \"Maternal_ID\", \"Sex\", \"Phenotype\", \"Ethnicity\"]\n    for md_ped in list(set([x for x in [tz.get_in([\"metadata\", \"ped\"], data)\n                                        for data in samples] if x is not None])):\n        with open(md_ped) as in_handle:\n            reader = csv.reader(in_handle, dialect=\"excel-tab\")\n            for parts in reader:\n                if parts[0].startswith(\"#\") and len(parts) > len(header):\n                    header = header + parts[len(header):]\n                else:\n                    sample_ped_lines[parts[1]] = parts\n    if not utils.file_exists(out_file):\n        with file_transaction(samples[0], out_file) as tx_out_file:\n            with open(tx_out_file, \"w\") as out_handle:\n                want_samples = set(vcfutils.get_samples(base_vcf))\n                writer = csv.writer(out_handle, dialect=\"excel-tab\")\n                writer.writerow(header)\n                for data in samples:\n                    ped_info = get_ped_info(data, samples)\n                    sname = ped_info[\"individual_id\"]\n                    if sname in want_samples:\n                        want_samples.remove(sname)\n                        if sname in sample_ped_lines:\n                            writer.writerow(sample_ped_lines[sname])\n                        else:\n                            writer.writerow([ped_info[\"family_id\"], ped_info[\"individual_id\"],\n                                            ped_info[\"paternal_id\"], ped_info[\"maternal_id\"],\n                                            ped_info[\"gender\"], ped_info[\"affected\"],\n                                            ped_info[\"ethnicity\"]])\n    return out_file", "response": "Create GEMINI - compatible PED file for a set of samples."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _is_small_vcf(vcf_file):\n    count = 0\n    small_thresh = 250\n    with utils.open_gzipsafe(vcf_file) as in_handle:\n        for line in in_handle:\n            if not line.startswith(\"#\"):\n                count += 1\n            if count > small_thresh:\n                return False\n    return True", "response": "Check if a VCF file is small."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_multisample_vcf(fnames, name, caller, data):\n    unique_fnames = []\n    for f in fnames:\n        if f not in unique_fnames:\n            unique_fnames.append(f)\n    out_dir = utils.safe_makedir(os.path.join(data[\"dirs\"][\"work\"], \"gemini\"))\n    if len(unique_fnames) > 1:\n        gemini_vcf = os.path.join(out_dir, \"%s-%s.vcf.gz\" % (name, caller))\n        vrn_file_batch = None\n        for variant in data.get(\"variants\", []):\n            if variant[\"variantcaller\"] == caller and variant.get(\"vrn_file_batch\"):\n                vrn_file_batch = variant[\"vrn_file_batch\"]\n        if vrn_file_batch:\n            utils.symlink_plus(vrn_file_batch, gemini_vcf)\n            return gemini_vcf\n        else:\n            return vcfutils.merge_variant_files(unique_fnames, gemini_vcf, dd.get_ref_file(data),\n                                                data[\"config\"])\n    else:\n        gemini_vcf = os.path.join(out_dir, \"%s-%s%s\" % (name, caller, utils.splitext_plus(unique_fnames[0])[1]))\n        utils.symlink_plus(unique_fnames[0], gemini_vcf)\n        return gemini_vcf", "response": "Retrieve a multiple sample VCF file from a set of input files."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetermines if we should build a gemini database based on the input files and samples.", "response": "def _get_build_type(fnames, samples, caller):\n    \"\"\"Confirm we should build a gemini database: need gemini in tools_on.\n\n    Checks for valid conditions for running a database and gemini or gemini_orig\n    configured in tools on.\n    \"\"\"\n    build_type = set()\n    if any(vcfutils.vcf_has_variants(f) for f in fnames) and caller not in NO_DB_CALLERS:\n        for data in samples:\n            if any([x in dd.get_tools_on(data)\n                    for x in [\"gemini\", \"gemini_orig\", \"gemini_allvariants\", \"vcf2db_expand\"]]):\n                if vcfanno.annotate_gemini(data):\n                    build_type.add(\"gemini_orig\" if \"gemini_orig\" in dd.get_tools_on(data) else \"gemini\")\n                else:\n                    logger.info(\"Not running gemini, input data not found: %s\" % dd.get_sample_name(data))\n            else:\n                logger.info(\"Not running gemini, not configured in tools_on: %s\" % dd.get_sample_name(data))\n    else:\n        logger.info(\"Not running gemini, no samples with variants found: %s\" %\n                    (\", \".join([dd.get_sample_name(d) for d in samples])))\n    return build_type"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nenumerating available gemini data files in a standard installation.", "response": "def get_gemini_files(data):\n    \"\"\"Enumerate available gemini data files in a standard installation.\n    \"\"\"\n    try:\n        from gemini import annotations, config\n    except ImportError:\n        return {}\n    return {\"base\": config.read_gemini_config()[\"annotation_dir\"],\n            \"files\": annotations.get_anno_files().values()}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _group_by_batches(samples, check_fn):\n    batch_groups = collections.defaultdict(list)\n    singles = []\n    out_retrieve = []\n    extras = []\n    for data in [x[0] for x in samples]:\n        if check_fn(data):\n            batch = tz.get_in([\"metadata\", \"batch\"], data)\n            name = str(dd.get_sample_name(data))\n            if batch:\n                out_retrieve.append((str(batch), data))\n            else:\n                out_retrieve.append((name, data))\n            for vrn in data[\"variants\"]:\n                if vrn.get(\"population\", True):\n                    if batch:\n                        batch_groups[(str(batch), vrn[\"variantcaller\"])].append((vrn[\"vrn_file\"], data))\n                    else:\n                        singles.append((name, vrn[\"variantcaller\"], data, vrn[\"vrn_file\"]))\n        else:\n            extras.append(data)\n    return batch_groups, singles, out_retrieve, extras", "response": "Group data items into batches providing details to retrieve results."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npreparing gemini databases in parallel handling jointly called populations.", "response": "def prep_db_parallel(samples, parallel_fn):\n    \"\"\"Prepares gemini databases in parallel, handling jointly called populations.\n    \"\"\"\n    batch_groups, singles, out_retrieve, extras = _group_by_batches(samples, _has_variant_calls)\n    to_process = []\n    has_batches = False\n    for (name, caller), info in batch_groups.items():\n        fnames = [x[0] for x in info]\n        to_process.append([fnames, (str(name), caller, True), [x[1] for x in info], extras])\n        has_batches = True\n    for name, caller, data, fname in singles:\n        to_process.append([[fname], (str(name), caller, False), [data], extras])\n    output = parallel_fn(\"prep_gemini_db\", to_process)\n    out_fetch = {}\n    for batch_id, out_file in output:\n        out_fetch[tuple(batch_id)] = out_file\n    out = []\n    for batch_name, data in out_retrieve:\n        out_variants = []\n        for vrn in data[\"variants\"]:\n            use_population = vrn.pop(\"population\", True)\n            if use_population:\n                vrn[\"population\"] = out_fetch[(batch_name, vrn[\"variantcaller\"])]\n            out_variants.append(vrn)\n        data[\"variants\"] = out_variants\n        out.append([data])\n    for x in extras:\n        out.append([x])\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_s3_files(local_dir, file_info, params):\n    assert len(file_info) == 1\n    files = file_info.values()[0]\n    fnames = []\n    for k in [\"1\", \"2\"]:\n        if files[k] not in fnames:\n            fnames.append(files[k])\n    out = []\n    for fname in fnames:\n        bucket, key = fname.replace(\"s3://\", \"\").split(\"/\", 1)\n        if params[\"access_key_id\"] == \"TEST\":\n            out.append(os.path.join(local_dir, os.path.basename(key)))\n        else:\n            out.append(s3.get_file(local_dir, bucket, key, params))\n    return out", "response": "Retrieve s3 files to local directory handling STORMSeq inputs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_fastq_files(data):\n    assert \"files\" in data, \"Did not find `files` in input; nothing to process\"\n    ready_files = []\n    should_gzip = True\n\n    # Bowtie does not accept gzipped fastq\n    if 'bowtie' in data['reference'].keys():\n        should_gzip = False\n    for fname in data[\"files\"]:\n        if fname.endswith(\".bam\"):\n            if _pipeline_needs_fastq(data[\"config\"], data):\n                ready_files = convert_bam_to_fastq(fname, data[\"dirs\"][\"work\"],\n                                                   data, data[\"dirs\"], data[\"config\"])\n            else:\n                ready_files = [fname]\n        elif objectstore.is_remote(fname):\n            ready_files.append(fname)\n        # Trimming does quality conversion, so if not doing that, do an explicit conversion\n        elif not(dd.get_trim_reads(data)) and dd.get_quality_format(data) != \"standard\":\n            out_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"fastq_convert\"))\n            ready_files.append(fastq.groom(fname, data, out_dir=out_dir))\n        else:\n            ready_files.append(fname)\n    ready_files = [x for x in ready_files if x is not None]\n    if should_gzip:\n        out_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"fastq\"))\n        ready_files = [_gzip_fastq(x, out_dir) for x in ready_files]\n    for in_file in ready_files:\n        if not objectstore.is_remote(in_file):\n            assert os.path.exists(in_file), \"%s does not exist.\" % in_file\n    return ready_files", "response": "Retrieve fastq files for the given lane."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _gzip_fastq(in_file, out_dir=None):\n    if fastq.is_fastq(in_file) and not objectstore.is_remote(in_file):\n        if utils.is_bzipped(in_file):\n            return _bzip_gzip(in_file, out_dir)\n        elif not utils.is_gzipped(in_file):\n            if out_dir:\n                gzipped_file = os.path.join(out_dir, os.path.basename(in_file) + \".gz\")\n            else:\n                gzipped_file = in_file + \".gz\"\n            if file_exists(gzipped_file):\n                return gzipped_file\n            message = \"gzipping {in_file} to {gzipped_file}.\".format(\n                in_file=in_file, gzipped_file=gzipped_file)\n            with file_transaction(gzipped_file) as tx_gzipped_file:\n                do.run(\"gzip -c {in_file} > {tx_gzipped_file}\".format(**locals()),\n                       message)\n            return gzipped_file\n    return in_file", "response": "gzip a fastq file if it is not already gzipped"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _pipeline_needs_fastq(config, data):\n    aligner = config[\"algorithm\"].get(\"aligner\")\n    support_bam = aligner in alignment.metadata.get(\"support_bam\", [])\n    return aligner and not support_bam", "response": "Determine if the pipeline needs fastq conversion."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert_bam_to_fastq(in_file, work_dir, data, dirs, config):\n    return alignprep.prep_fastq_inputs([in_file], data)", "response": "Convert BAM input file into FASTQ files."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmerges smartly fastq files. It recognizes paired fastq files. It recognizes paired fastq files. It recognizes paired fastq files. It recognizes paired fastq files. It recognizes paired fastq files.", "response": "def merge(files, out_file, config):\n    \"\"\"merge smartly fastq files. It recognizes paired fastq files.\"\"\"\n    pair1 = [fastq_file[0] for fastq_file in files]\n    if len(files[0]) > 1:\n        path = splitext_plus(out_file)\n        pair1_out_file = path[0] + \"_R1\" + path[1]\n        pair2 = [fastq_file[1] for fastq_file in files]\n        pair2_out_file = path[0] + \"_R2\" + path[1]\n        _merge_list_fastqs(pair1, pair1_out_file, config)\n        _merge_list_fastqs(pair2, pair2_out_file, config)\n        return [pair1_out_file, pair2_out_file]\n    else:\n        return _merge_list_fastqs(pair1, out_file, config)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmerging list of fastq files into one", "response": "def _merge_list_fastqs(files, out_file, config):\n    \"\"\"merge list of fastq files into one\"\"\"\n    if not all(map(fastq.is_fastq, files)):\n        raise ValueError(\"Not all of the files to merge are fastq files: %s \" % (files))\n    assert all(map(utils.file_exists, files)), (\"Not all of the files to merge \"\n                                                \"exist: %s\" % (files))\n    if not file_exists(out_file):\n        files = [_gzip_fastq(fn) for fn in files]\n        if len(files) == 1:\n            if \"remove_source\" in config and config[\"remove_source\"]:\n                shutil.move(files[0], out_file)\n            else:\n                os.symlink(files[0], out_file)\n            return out_file\n        with file_transaction(out_file) as file_txt_out:\n            files_str = \" \".join(list(files))\n            cmd = \"cat {files_str} > {file_txt_out}\".format(**locals())\n            do.run(cmd, \"merge fastq files %s\" % files)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncleaning a BED file and remove any uncommented entries.", "response": "def decomment(bed_file, out_file):\n    \"\"\"\n    clean a BED file\n    \"\"\"\n    if file_exists(out_file):\n        return out_file\n\n    with utils.open_gzipsafe(bed_file) as in_handle, open(out_file, \"w\") as out_handle:\n        for line in in_handle:\n            if line.startswith(\"#\") or line.startswith(\"browser\") or line.startswith(\"track\"):\n                continue\n            else:\n                out_handle.write(line)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmerge a list of BED files or a list of BED files return a bedtools object", "response": "def merge(bedfiles):\n    \"\"\"\n    given a BED file or list of BED files merge them an return a bedtools object\n    \"\"\"\n    if isinstance(bedfiles, list):\n        catted = concat(bedfiles)\n    else:\n        catted = concat([bedfiles])\n    if catted:\n        return concat(bedfiles).sort().merge()\n    else:\n        return catted"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef minimize(bed_file):\n    if not bed_file:\n        return bed_file\n    else:\n        sorted_bed = bt.BedTool(bed_file).cut(range(3)).sort()\n        if not sorted_bed.fn.endswith(\".bed\"):\n            return sorted_bed.moveto(sorted_bed.fn + \".bed\")\n        else:\n            return sorted_bed", "response": "minimize a BED file to a three necessary columns"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving unaligned read pairs from input alignment BAM as two fastq files.", "response": "def select_unaligned_read_pairs(in_bam, extra, out_dir, config):\n    \"\"\"Retrieve unaligned read pairs from input alignment BAM, as two fastq files.\n    \"\"\"\n    runner = broad.runner_from_config(config)\n    base, ext = os.path.splitext(os.path.basename(in_bam))\n    nomap_bam = os.path.join(out_dir, \"{}-{}{}\".format(base, extra, ext))\n    if not utils.file_exists(nomap_bam):\n        with file_transaction(nomap_bam) as tx_out:\n            runner.run(\"FilterSamReads\", [(\"INPUT\", in_bam),\n                                          (\"OUTPUT\", tx_out),\n                                          (\"EXCLUDE_ALIGNED\", \"true\"),\n                                          (\"WRITE_READS_FILES\", \"false\"),\n                                          (\"SORT_ORDER\", \"queryname\")])\n    has_reads = False\n    with pysam.Samfile(nomap_bam, \"rb\") as in_pysam:\n        for read in in_pysam:\n            if read.is_paired:\n                has_reads = True\n                break\n    if has_reads:\n        out_fq1, out_fq2 = [\"{}-{}.fq\".format(os.path.splitext(nomap_bam)[0], i) for i in [1, 2]]\n        runner.run_fn(\"picard_bam_to_fastq\", nomap_bam, out_fq1, out_fq2)\n        return out_fq1, out_fq2\n    else:\n        return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves any reads without both pairs present in the file.", "response": "def remove_nopairs(in_bam, out_dir, config):\n    \"\"\"Remove any reads without both pairs present in the file.\n    \"\"\"\n    runner = broad.runner_from_config(config)\n    out_bam = os.path.join(out_dir, \"{}-safepair{}\".format(*os.path.splitext(os.path.basename(in_bam))))\n    if not utils.file_exists(out_bam):\n        read_counts = collections.defaultdict(int)\n        with pysam.Samfile(in_bam, \"rb\") as in_pysam:\n            for read in in_pysam:\n                if read.is_paired:\n                    read_counts[read.qname] += 1\n        with pysam.Samfile(in_bam, \"rb\") as in_pysam:\n            with file_transaction(out_bam) as tx_out_bam:\n                with pysam.Samfile(tx_out_bam, \"wb\", template=in_pysam) as out_pysam:\n                    for read in in_pysam:\n                        if read_counts[read.qname] == 2:\n                            out_pysam.write(read)\n    return runner.run_fn(\"picard_sort\", out_bam, \"queryname\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms the alignment of non - mapped reads from previous tier.", "response": "def tiered_alignment(in_bam, tier_num, multi_mappers, extra_args,\n                     genome_build, pair_stats,\n                     work_dir, dirs, config):\n    \"\"\"Perform the alignment of non-mapped reads from previous tier.\n    \"\"\"\n    nomap_fq1, nomap_fq2 = select_unaligned_read_pairs(in_bam, \"tier{}\".format(tier_num),\n                                                       work_dir, config)\n    if nomap_fq1 is not None:\n        base_name = \"{}-tier{}out\".format(os.path.splitext(os.path.basename(in_bam))[0],\n                                          tier_num)\n        config = copy.deepcopy(config)\n        dirs = copy.deepcopy(dirs)\n        config[\"algorithm\"][\"bam_sort\"] = \"queryname\"\n        config[\"algorithm\"][\"multiple_mappers\"] = multi_mappers\n        config[\"algorithm\"][\"extra_align_args\"] = [\"-i\", int(pair_stats[\"mean\"]),\n                                               int(pair_stats[\"std\"])] + extra_args\n        out_bam, ref_file = align_to_sort_bam(nomap_fq1, nomap_fq2,\n                                              lane.rg_names(base_name, base_name, config),\n                                              genome_build, \"novoalign\",\n                                              dirs, config,\n                                              dir_ext=os.path.join(\"hydra\", os.path.split(nomap_fq1)[0]))\n        return out_bam\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting BAM file using BEDTools.", "response": "def convert_bam_to_bed(in_bam, out_file):\n    \"\"\"Convert BAM to bed file using BEDTools.\n    \"\"\"\n    with file_transaction(out_file) as tx_out_file:\n        with open(tx_out_file, \"w\") as out_handle:\n            subprocess.check_call([\"bamToBed\", \"-i\", in_bam, \"-tag\", \"NM\"],\n                                  stdout=out_handle)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetecting structural variation breakpoints with hydra.", "response": "def hydra_breakpoints(in_bam, pair_stats):\n    \"\"\"Detect structural variation breakpoints with hydra.\n    \"\"\"\n    in_bed = convert_bam_to_bed(in_bam)\n    if os.path.getsize(in_bed) > 0:\n        pair_bed = pair_discordants(in_bed, pair_stats)\n        dedup_bed = dedup_discordants(pair_bed)\n        return run_hydra(dedup_bed, pair_stats)\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef detect_sv(align_bam, genome_build, dirs, config):\n    work_dir = utils.safe_makedir(os.path.join(dirs[\"work\"], \"structural\"))\n    pair_stats = shared.calc_paired_insert_stats(align_bam)\n    fix_bam = remove_nopairs(align_bam, work_dir, config)\n    tier2_align = tiered_alignment(fix_bam, \"2\", True, [],\n                                   genome_build, pair_stats,\n                                   work_dir, dirs, config)\n    if tier2_align:\n        tier3_align = tiered_alignment(tier2_align, \"3\", \"Ex 1100\", [\"-t\", \"300\"],\n                                       genome_build, pair_stats,\n                                       work_dir, dirs, config)\n        if tier3_align:\n            hydra_bps = hydra_breakpoints(tier3_align, pair_stats)", "response": "Detect structural variation from discordant aligned pairs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncleaning up a GTF file of assembled transcripts 1) if a known gene is known to code for a protein, remove any *novel* isoforms of the that do not also code for a protein. 2) if a new gene has been annotated and none of its isoforms are protein coding and it is > 200 bp, mark it as a lincRNA. < 200 bp mark it as ncRNA", "response": "def cleanup_transcripts(assembled_gtf, ref_gtf, ref_fasta, out_file=None):\n    \"\"\"\n    Clean up a GTF file of assembled transcripts\n    1) if a known gene is known to code for a protein, remove any *novel*\n    isoforms of the that do not also code for a protein.\n    2) if a new gene has been annotated and none of its isoforms are protein\n    coding and it is > 200 bp, mark it as a lincRNA. < 200 bp mark it as ncRNA\n    \"\"\"\n\n    if not out_file:\n        out_file = os.path.splitext(assembled_gtf)[0] + \".cleaned.gtf\"\n    if file_exists(out_file):\n        return out_file\n    ref_db = gtf.get_gtf_db(ref_gtf)\n    known_transcript = {feature['transcript_id'][0]: feature.source for feature\n                        in gtf.complete_features(ref_db)}\n    ref_gene_to_source = gtf.get_gene_source_set(ref_gtf)\n    assembled_db = gtf.get_gtf_db(assembled_gtf)\n    assembled_fasta = gtf.gtf_to_fasta(assembled_gtf, ref_fasta)\n    lengths = fasta.sequence_length(assembled_fasta)\n    with file_transaction(out_file) as tx_out_file:\n        with open(tx_out_file, 'w') as out_handle:\n            for feature in gtf.complete_features(assembled_db):\n                transcript_id = feature['transcript_id'][0]\n                gene_id = feature['gene_id'][0]\n                if transcript_id in known_transcript:\n                    out_handle.write(str(feature) + \"\\n\")\n                    continue\n                known_coding = \"protein_coding\" in ref_gene_to_source.get(gene_id, [None])\n                if known_coding and feature.source != \"protein_coding\":\n                    continue\n                if feature.source != \"protein_coding\":\n                    if lengths[transcript_id] > 200:\n                        feature.source = \"lincRNA\"\n                    else:\n                        feature.source = \"ncRNA\"\n                out_handle.write(str(feature) + \"\\n\")\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nincreases default sockets for zmq gc. Avoids scaling issues.", "response": "def _increase_gc_sockets():\n    \"\"\"Increase default sockets for zmq gc. Avoids scaling issues.\n    https://github.com/zeromq/pyzmq/issues/471\n    \"\"\"\n    ctx = zmq.Context()\n    ctx.max_sockets = MAX_SOCKETS\n    gc.context = ctx"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef recv(self, timeout=None):\n        if timeout:\n            try:\n                testsock = self._zmq.select([self.socket], [], [], timeout)[0]\n            except zmq.ZMQError as e:\n                if e.errno == errno.EINTR:\n                    testsock = None\n                else:\n                    raise\n            if not testsock:\n                return\n            rv = self.socket.recv(self._zmq.NOBLOCK)\n            return LogRecord.from_dict(json.loads(rv))\n        else:\n            return super(ZeroMQPullSubscriber, self).recv(timeout)", "response": "Overwrite standard recv for timeout calls to catch interrupt errors."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning tumor only pisces calling on the given set of items.", "response": "def run(align_bams, items, ref_file, assoc_files, region=None, out_file=None):\n    \"\"\"Run tumor only pisces calling\n\n    Handles bgzipping output file and fixing VCF sample naming to match BAM sample.\n    \"\"\"\n    paired = vcfutils.get_paired_bams(align_bams, items)\n    assert paired and not paired.normal_bam, (\"Pisces supports tumor-only variant calling: %s\" %\n                                              (\",\".join([dd.get_sample_name(d) for d in items])))\n    vrs = bedutils.population_variant_regions(items)\n    target = shared.subset_variant_regions(vrs, region,\n                                            out_file, items=items, do_merge=True)\n    min_af = float(dd.get_min_allele_fraction(paired.tumor_data)) / 100.0\n    if not utils.file_exists(out_file):\n        base_out_name = utils.splitext_plus(os.path.basename(paired.tumor_bam))[0]\n        raw_file = \"%s.vcf\" % utils.splitext_plus(out_file)[0]\n        with file_transaction(paired.tumor_data, raw_file) as tx_out_file:\n            ref_dir = _prep_genome(os.path.dirname(tx_out_file), paired.tumor_data)\n            out_dir = os.path.dirname(tx_out_file)\n            cores = dd.get_num_cores(paired.tumor_data)\n            emit_min_af = min_af / 10.0\n            cmd = (\"pisces --bampaths {paired.tumor_bam} --genomepaths {ref_dir} --intervalpaths {target} \"\n                   \"--maxthreads {cores} --minvf {emit_min_af} --vffilter {min_af} \"\n                   \"--ploidy somatic --gvcf false -o {out_dir}\")\n            # Recommended filtering for low frequency indels\n            # https://github.com/bcbio/bcbio-nextgen/commit/49d0cbb1f6dcbea629c63749e2f9813bd06dcee3#commitcomment-29765373\n            cmd += \" -RMxNFilter 5,9,0.35\"\n            # For low frequency UMI tagged variants, set higher variant thresholds\n            # https://github.com/Illumina/Pisces/issues/14#issuecomment-399756862\n            if min_af < (1.0 / 100.0):\n                cmd += \" --minbasecallquality 30\"\n            do.run(cmd.format(**locals()), \"Pisces tumor-only somatic calling\")\n            shutil.move(os.path.join(out_dir, \"%s.vcf\" % base_out_name),\n                        tx_out_file)\n        vcfutils.bgzip_and_index(raw_file, paired.tumor_data[\"config\"],\n                                 prep_cmd=\"sed 's#%s.bam#%s#' | %s\" %\n                                 (base_out_name, dd.get_sample_name(paired.tumor_data),\n                                  vcfutils.add_contig_to_header_cl(dd.get_ref_file(paired.tumor_data), out_file)))\n    return vcfutils.bgzip_and_index(out_file, paired.tumor_data[\"config\"])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _prep_genome(out_dir, data):\n    genome_name = utils.splitext_plus(os.path.basename(dd.get_ref_file(data)))[0]\n    out_dir = utils.safe_makedir(os.path.join(out_dir, genome_name))\n    ref_file = dd.get_ref_file(data)\n    utils.symlink_plus(ref_file, os.path.join(out_dir, os.path.basename(ref_file)))\n    with open(os.path.join(out_dir, \"GenomeSize.xml\"), \"w\") as out_handle:\n        out_handle.write('<sequenceSizes genomeName=\"%s\">' % genome_name)\n        for c in pysam.AlignmentFile(\"%s.dict\" % utils.splitext_plus(ref_file)[0]).header[\"SQ\"]:\n            cur_ploidy = ploidy.get_ploidy([data], region=[c[\"SN\"]])\n            out_handle.write('<chromosome fileName=\"%s\" contigName=\"%s\" totalBases=\"%s\" knownBases=\"%s\" '\n                             'isCircular=\"false\" ploidy=\"%s\" md5=\"%s\"/>' %\n                             (os.path.basename(ref_file), c[\"SN\"], c[\"LN\"], c[\"LN\"], cur_ploidy, c[\"M5\"]))\n        out_handle.write('</sequenceSizes>')\n    return out_dir", "response": "Create prepped reference directory for pisces."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning DeepVariant calling on germline samples.", "response": "def run(align_bams, items, ref_file, assoc_files, region, out_file):\n    \"\"\"Return DeepVariant calling on germline samples.\n\n    region can be a single region or list of multiple regions for multicore calling.\n    \"\"\"\n    assert not vcfutils.is_paired_analysis(align_bams, items), \\\n        (\"DeepVariant currently only supports germline calling: %s\" %\n         (\", \".join([dd.get_sample_name(d) for d in items])))\n    assert len(items) == 1, \\\n        (\"DeepVariant currently only supports single sample calling: %s\" %\n         (\", \".join([dd.get_sample_name(d) for d in items])))\n    out_file = _run_germline(align_bams[0], items[0], ref_file,\n                             region, out_file)\n    return vcfutils.bgzip_and_index(out_file, items[0][\"config\"])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun single sample germline variant calling.", "response": "def _run_germline(bam_file, data, ref_file, region, out_file):\n    \"\"\"Single sample germline variant calling.\n    \"\"\"\n    work_dir = utils.safe_makedir(\"%s-work\" % utils.splitext_plus(out_file)[0])\n    region_bed = strelka2.get_region_bed(region, [data], out_file, want_gzip=False)\n    example_dir = _make_examples(bam_file, data, ref_file, region_bed, out_file, work_dir)\n    if _has_candidate_variants(example_dir):\n        tfrecord_file = _call_variants(example_dir, region_bed, data, out_file)\n        return _postprocess_variants(tfrecord_file, data, ref_file, out_file)\n    else:\n        return vcfutils.write_empty_vcf(out_file, data[\"config\"], [dd.get_sample_name(data)])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _make_examples(bam_file, data, ref_file, region_bed, out_file, work_dir):\n    log_dir = utils.safe_makedir(os.path.join(work_dir, \"log\"))\n    example_dir = utils.safe_makedir(os.path.join(work_dir, \"examples\"))\n    if len(glob.glob(os.path.join(example_dir, \"%s.tfrecord*.gz\" % dd.get_sample_name(data)))) == 0:\n        with tx_tmpdir(data) as tx_example_dir:\n            cmd = [\"dv_make_examples.py\", \"--cores\", dd.get_num_cores(data), \"--ref\", ref_file,\n                   \"--reads\", bam_file, \"--regions\", region_bed, \"--logdir\", log_dir,\n                   \"--examples\", tx_example_dir, \"--sample\", dd.get_sample_name(data)]\n            do.run(cmd, \"DeepVariant make_examples %s\" % dd.get_sample_name(data))\n            for fname in glob.glob(os.path.join(tx_example_dir, \"%s.tfrecord*.gz\" % dd.get_sample_name(data))):\n                utils.copy_plus(fname, os.path.join(example_dir, os.path.basename(fname)))\n    return example_dir", "response": "Create example pileup images to feed into variant calling."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _call_variants(example_dir, region_bed, data, out_file):\n    tf_out_file = \"%s-tfrecord.gz\" % utils.splitext_plus(out_file)[0]\n    if not utils.file_exists(tf_out_file):\n        with file_transaction(data, tf_out_file) as tx_out_file:\n            model = \"wes\" if strelka2.coverage_interval_from_bed(region_bed) == \"targeted\" else \"wgs\"\n            cmd = [\"dv_call_variants.py\", \"--cores\", dd.get_num_cores(data),\n                   \"--outfile\", tx_out_file, \"--examples\", example_dir,\n                   \"--sample\", dd.get_sample_name(data), \"--model\", model]\n            do.run(cmd, \"DeepVariant call_variants %s\" % dd.get_sample_name(data))\n    return tf_out_file", "response": "Call variants from prepared pileup examples creating tensorflow record file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nposting - process variants converting into standard VCF file.", "response": "def _postprocess_variants(record_file, data, ref_file, out_file):\n    \"\"\"Post-process variants, converting into standard VCF file.\n    \"\"\"\n    if not utils.file_uptodate(out_file, record_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            cmd = [\"dv_postprocess_variants.py\", \"--ref\", ref_file,\n                   \"--infile\", record_file, \"--outfile\", tx_out_file]\n            do.run(cmd, \"DeepVariant postprocess_variants %s\" % dd.get_sample_name(data))\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(bam_file, data, out_dir):\n    qsig = config_utils.get_program(\"qsignature\", data[\"config\"])\n    res_qsig = config_utils.get_resources(\"qsignature\", data[\"config\"])\n    jvm_opts = \" \".join(res_qsig.get(\"jvm_opts\", [\"-Xms750m\", \"-Xmx8g\"]))\n    if not qsig:\n        logger.info(\"There is no qsignature tool. Skipping...\")\n        return None\n\n    position = dd.get_qsig_file(data)\n    mixup_check = dd.get_mixup_check(data)\n    if mixup_check and mixup_check.startswith(\"qsignature\"):\n        utils.safe_makedir(out_dir)\n        if not position:\n            logger.info(\"There is no qsignature for this species: %s\"\n                        % tz.get_in(['genome_build'], data))\n            return None\n        if mixup_check == \"qsignature_full\":\n            down_bam = bam_file\n        else:\n            down_bam = _slice_bam_chr21(bam_file, data)\n            position = _slice_vcf_chr21(position, out_dir)\n\n        out_name = os.path.basename(down_bam).replace(\"bam\", \"qsig.vcf\")\n        out_file = os.path.join(out_dir, out_name)\n        log_file = os.path.join(out_dir, \"qsig.log\")\n        cores = dd.get_cores(data)\n        base_cmd = (\"{qsig} {jvm_opts} \"\n                    \"org.qcmg.sig.SignatureGenerator \"\n                    \"--noOfThreads {cores} \"\n                    \"-log {log_file} -i {position} \"\n                    \"-i {down_bam} \")\n        if not os.path.exists(out_file):\n            file_qsign_out = \"{0}.qsig.vcf\".format(down_bam)\n            do.run(base_cmd.format(**locals()), \"qsignature vcf generation: %s\" % dd.get_sample_name(data))\n            if os.path.exists(file_qsign_out):\n                with file_transaction(data, out_file) as file_txt_out:\n                    shutil.move(file_qsign_out, file_txt_out)\n            else:\n                raise IOError(\"File doesn't exist %s\" % file_qsign_out)\n        return out_file\n    return None", "response": "Run SignatureGenerator to create normalize vcf that later will be input of qsignature_summary\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun SignatureCompareRelatedSimple module from qsignature tool.", "response": "def summary(*samples):\n    \"\"\"Run SignatureCompareRelatedSimple module from qsignature tool.\n\n    Creates a matrix of pairwise comparison among samples. The\n    function will not run if the output exists\n\n    :param samples: list with only one element containing all samples information\n    :returns: (dict) with the path of the output to be joined to summary\n    \"\"\"\n    warnings, similar = [], []\n    qsig = config_utils.get_program(\"qsignature\", samples[0][0][\"config\"])\n    if not qsig:\n        return [[]]\n    res_qsig = config_utils.get_resources(\"qsignature\", samples[0][0][\"config\"])\n    jvm_opts = \" \".join(res_qsig.get(\"jvm_opts\", [\"-Xms750m\", \"-Xmx8g\"]))\n    work_dir = samples[0][0][\"dirs\"][\"work\"]\n    count = 0\n    for data in samples:\n        data = data[0]\n        vcf = tz.get_in([\"summary\", \"qc\", \"qsignature\", \"base\"], data)\n        if vcf:\n            count += 1\n            vcf_name = dd.get_sample_name(data) + \".qsig.vcf\"\n            out_dir = utils.safe_makedir(os.path.join(work_dir, \"qsignature\"))\n            if not os.path.lexists(os.path.join(out_dir, vcf_name)):\n                os.symlink(vcf, os.path.join(out_dir, vcf_name))\n    if count > 0:\n        qc_out_dir = utils.safe_makedir(os.path.join(work_dir, \"qc\", \"qsignature\"))\n        out_file = os.path.join(qc_out_dir, \"qsignature.xml\")\n        out_ma_file = os.path.join(qc_out_dir, \"qsignature.ma\")\n        out_warn_file = os.path.join(qc_out_dir, \"qsignature.warnings\")\n        log = os.path.join(work_dir, \"qsignature\", \"qsig-summary.log\")\n        if not os.path.exists(out_file):\n            with file_transaction(samples[0][0], out_file) as file_txt_out:\n                base_cmd = (\"{qsig} {jvm_opts} \"\n                            \"org.qcmg.sig.SignatureCompareRelatedSimple \"\n                            \"-log {log} -dir {out_dir} \"\n                            \"-o {file_txt_out} \")\n                do.run(base_cmd.format(**locals()), \"qsignature score calculation\")\n        error, warnings, similar = _parse_qsignature_output(out_file, out_ma_file,\n                                                            out_warn_file, samples[0][0])\n        return [{'total samples': count,\n                 'similar samples pairs': len(similar),\n                 'warnings samples pairs': len(warnings),\n                 'error samples': list(error),\n                 'out_dir': qc_out_dir}]\n    else:\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_qsignature_output(in_file, out_file, warning_file, data):\n    name = {}\n    error, warnings, similar = set(), set(), set()\n    same, replicate, related = 0, 0.1, 0.18\n    mixup_check = dd.get_mixup_check(data)\n    if mixup_check == \"qsignature_full\":\n        same, replicate, related = 0, 0.01, 0.061\n    with open(in_file, 'r') as in_handle:\n        with file_transaction(data, out_file) as out_tx_file:\n            with file_transaction(data, warning_file) as warn_tx_file:\n                with open(out_tx_file, 'w') as out_handle:\n                    with open(warn_tx_file, 'w') as warn_handle:\n                        et = ET.parse(in_handle)\n                        for i in list(et.iter('file')):\n                            name[i.attrib['id']] = os.path.basename(i.attrib['name']).replace(\".qsig.vcf\", \"\")\n                        for i in list(et.iter('comparison')):\n                            msg = None\n                            pair = \"-\".join([name[i.attrib['file1']], name[i.attrib['file2']]])\n                            out_handle.write(\"%s\\t%s\\t%s\\n\" %\n                                             (name[i.attrib['file1']], name[i.attrib['file2']], i.attrib['score']))\n                            if float(i.attrib['score']) == same:\n                                msg = 'qsignature ERROR: read same samples:%s\\n'\n                                error.add(pair)\n                            elif float(i.attrib['score']) < replicate:\n                                msg = 'qsignature WARNING: read similar/replicate samples:%s\\n'\n                                warnings.add(pair)\n                            elif float(i.attrib['score']) < related:\n                                msg = 'qsignature NOTE: read relative samples:%s\\n'\n                                similar.add(pair)\n                            if msg:\n                                logger.info(msg % pair)\n                                warn_handle.write(msg % pair)\n    return error, warnings, similar", "response": "Parse the qsignature output file produced by qsignature."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _slice_bam_chr21(in_bam, data):\n    sambamba = config_utils.get_program(\"sambamba\", data[\"config\"])\n    out_file = \"%s-chr%s\" % os.path.splitext(in_bam)\n    if not utils.file_exists(out_file):\n        bam.index(in_bam, data['config'])\n        with pysam.Samfile(in_bam, \"rb\") as bamfile:\n            bam_contigs = [c[\"SN\"] for c in bamfile.header[\"SQ\"]]\n        chromosome = \"21\"\n        if \"chr21\" in bam_contigs:\n            chromosome = \"chr21\"\n        with file_transaction(data, out_file) as tx_out_file:\n            cmd = (\"{sambamba} slice -o {tx_out_file} {in_bam} {chromosome}\").format(**locals())\n            out = subprocess.check_output(cmd, shell=True)\n    return out_file", "response": "Slice BAM file with only chromosome 21"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _slice_vcf_chr21(vcf_file, out_dir):\n    tmp_file = os.path.join(out_dir, \"chr21_qsignature.vcf\")\n    if not utils.file_exists(tmp_file):\n        cmd = (\"grep chr21 {vcf_file} > {tmp_file}\").format(**locals())\n        out = subprocess.check_output(cmd, shell=True)\n    return tmp_file", "response": "Slice chr21 of qsignature SNPs to reduce computation time\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(vcf, conf_fns, lua_fns, data, basepath=None, decomposed=False):\n    conf_fns.sort(key=lambda x: os.path.basename(x) if x else \"\")\n    lua_fns.sort(key=lambda x: os.path.basename(x) if x else \"\")\n    ext = \"-annotated-%s\" % utils.splitext_plus(os.path.basename(conf_fns[0]))[0]\n    if vcf.find(ext) > 0:\n        out_file = vcf\n    else:\n        out_file = \"%s%s.vcf.gz\" % (utils.splitext_plus(vcf)[0], ext)\n    if not utils.file_exists(out_file):\n        vcfanno = config_utils.get_program(\"vcfanno\", data)\n        with file_transaction(out_file) as tx_out_file:\n            conffn = _combine_files(conf_fns, out_file, data, basepath is None)\n            luafn = _combine_files(lua_fns, out_file, data, False)\n            luaflag = \"-lua {0}\".format(luafn) if luafn and utils.file_exists(luafn) else \"\"\n            basepathflag = \"-base-path {0}\".format(basepath) if basepath else \"\"\n            cores = dd.get_num_cores(data)\n            post_ann = \"sed -e 's/Number=A/Number=1/g' |\" if decomposed else \"\"\n            cmd = (\"{vcfanno} -p {cores} {luaflag} {basepathflag} {conffn} {vcf} \"\n                   \"| {post_ann} bgzip -c > {tx_out_file}\")\n            message = \"Annotating {vcf} with vcfanno, using {conffn}\".format(**locals())\n            do.run(cmd.format(**locals()), message)\n    return vcfutils.bgzip_and_index(out_file, data[\"config\"])", "response": "Annotate a VCF file using vcfanno."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncombines multiple input files into one.", "response": "def _combine_files(orig_files, base_out_file, data, fill_paths=True):\n    \"\"\"Combine multiple input files, fixing file paths if needed.\n\n    We fill in full paths from files in the data dictionary if we're\n    not using basepath (old style GEMINI).\n    \"\"\"\n    orig_files = [x for x in orig_files if x and utils.file_exists(x)]\n    if not orig_files:\n        return None\n    out_file = \"%s-combine%s\" % (utils.splitext_plus(base_out_file)[0],\n                                    utils.splitext_plus(orig_files[0])[-1])\n    with open(out_file, \"w\") as out_handle:\n        for orig_file in orig_files:\n            with open(orig_file) as in_handle:\n                for line in in_handle:\n                    if fill_paths and line.startswith(\"file\"):\n                        line = _fill_file_path(line, data)\n                    out_handle.write(line)\n            out_handle.write(\"\\n\\n\")\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _fill_file_path(line, data):\n    def _find_file(xs, target):\n        if isinstance(xs, dict):\n            for v in xs.values():\n                f = _find_file(v, target)\n                if f:\n                    return f\n        elif isinstance(xs, (list, tuple)):\n            for x in xs:\n                f = _find_file(x, target)\n                if f:\n                    return f\n        elif isinstance(xs, six.string_types) and os.path.exists(xs) and xs.endswith(\"/%s\" % target):\n            return xs\n    orig_file = line.split(\"=\")[-1].replace('\"', '').strip()\n    full_file = _find_file(data, os.path.basename(orig_file))\n    if not full_file and os.path.exists(os.path.abspath(orig_file)):\n        full_file = os.path.abspath(orig_file)\n    assert full_file, \"Did not find vcfanno input file %s\" % (orig_file)\n    return 'file=\"%s\"\\n' % full_file", "response": "Fill in a full file path in the configuration file from the input line."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_annotations(data, retriever=None):\n    conf_files = dd.get_vcfanno(data)\n    if not isinstance(conf_files, (list, tuple)):\n        conf_files = [conf_files]\n    for c in _default_conf_files(data, retriever):\n        if c not in conf_files:\n            conf_files.append(c)\n    conf_checkers = {\"gemini\": annotate_gemini, \"somatic\": _annotate_somatic}\n    out = []\n    annodir = os.path.normpath(os.path.join(os.path.dirname(dd.get_ref_file(data)), os.pardir, \"config\", \"vcfanno\"))\n    if not retriever:\n        annodir = os.path.abspath(annodir)\n    for conf_file in conf_files:\n        if objectstore.is_remote(conf_file) or (os.path.exists(conf_file) and os.path.isfile(conf_file)):\n            conffn = conf_file\n        elif not retriever:\n            conffn = os.path.join(annodir, conf_file + \".conf\")\n        else:\n            conffn = conf_file + \".conf\"\n        luafn = \"%s.lua\" % utils.splitext_plus(conffn)[0]\n        if retriever:\n            conffn, luafn = [(x if objectstore.is_remote(x) else None)\n                             for x in retriever.add_remotes([conffn, luafn], data[\"config\"])]\n        if not conffn:\n            pass\n        elif conf_file in conf_checkers and not conf_checkers[conf_file](data, retriever):\n            logger.warn(\"Skipping vcfanno configuration: %s. Not all input files found.\" % conf_file)\n        elif not objectstore.file_exists_or_remote(conffn):\n            build = dd.get_genome_build(data)\n            CONF_NOT_FOUND = (\n                \"The vcfanno configuration {conffn} was not found for {build}, skipping.\")\n            logger.warn(CONF_NOT_FOUND.format(**locals()))\n        else:\n            out.append(conffn)\n            if luafn and objectstore.file_exists_or_remote(luafn):\n                out.append(luafn)\n    return out", "response": "Find annotations for vcfanno using pre - installed inputs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef annotate_gemini(data, retriever=None):\n    r = dd.get_variation_resources(data)\n    return all([r.get(k) and objectstore.file_exists_or_remote(r[k]) for k in [\"exac\", \"gnomad_exome\"]])", "response": "Annotate with population calls if have data installed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nannotating somatic calls if we have cosmic data installed.", "response": "def _annotate_somatic(data, retriever=None):\n    \"\"\"Annotate somatic calls if we have cosmic data installed.\n    \"\"\"\n    if is_human(data):\n        paired = vcfutils.get_paired([data])\n        if paired:\n            r = dd.get_variation_resources(data)\n            if r.get(\"cosmic\") and objectstore.file_exists_or_remote(r[\"cosmic\"]):\n                return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the genome resource is human.", "response": "def is_human(data, builds=None):\n    \"\"\"Check if human, optionally with build number, search by name or extra GL contigs.\n    \"\"\"\n    def has_build37_contigs(data):\n        for contig in ref.file_contigs(dd.get_ref_file(data)):\n            if contig.name.startswith(\"GL\") or contig.name.find(\"_gl\") >= 0:\n                if contig.name in naming.GMAP[\"hg19\"] or contig.name in naming.GMAP[\"GRCh37\"]:\n                    return True\n        return False\n    if not builds and tz.get_in([\"genome_resources\", \"aliases\", \"human\"], data):\n        return True\n    if not builds or \"37\" in builds:\n        target_builds = [\"hg19\", \"GRCh37\"]\n        if any([dd.get_genome_build(data).startswith(b) for b in target_builds]):\n            return True\n        elif has_build37_contigs(data):\n            return True\n    if not builds or \"38\" in builds:\n        target_builds = [\"hg38\"]\n        if any([dd.get_genome_build(data).startswith(b) for b in target_builds]):\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_resource_programs(progs, algs):\n    checks = {\"gatk-vqsr\": config_utils.use_vqsr,\n              \"snpeff\": config_utils.use_snpeff,\n              \"bcbio-variation-recall\": config_utils.use_bcbio_variation_recall}\n    parent_child = {\"vardict\": _parent_prefix(\"vardict\")}\n    out = set([])\n    for p in progs:\n        if p == \"aligner\":\n            for alg in algs:\n                aligner = alg.get(\"aligner\")\n                if aligner and not isinstance(aligner, bool):\n                    out.add(aligner)\n        elif p in [\"variantcaller\", \"svcaller\", \"peakcaller\"]:\n            if p == \"variantcaller\":\n                for key, fn in parent_child.items():\n                    if fn(algs):\n                        out.add(key)\n            for alg in algs:\n                callers = alg.get(p)\n                if callers and not isinstance(callers, bool):\n                    if isinstance(callers, dict):\n                        callers = reduce(operator.add, callers.values())\n                    if isinstance(callers, (list, tuple)):\n                        for x in callers:\n                            out.add(x)\n                    else:\n                        out.add(callers)\n        elif p in checks:\n            if checks[p](algs):\n                out.add(p)\n        else:\n            out.add(p)\n    return sorted(list(out))", "response": "Retrieve programs used in analysis based on algorithm configurations."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parent_prefix(prefix):\n    def run(algs):\n        for alg in algs:\n            vcs = alg.get(\"variantcaller\")\n            if vcs:\n                if isinstance(vcs, dict):\n                    vcs = reduce(operator.add, vcs.values())\n                if not isinstance(vcs, (list, tuple)):\n                    vcs = [vcs]\n                return any(vc.startswith(prefix) for vc in vcs if vc)\n    return run", "response": "Identify a parent prefix we should add to resources if present in a caller name."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nensures that the minimum resources required for used programs are set to the minimum resources required for used programs.", "response": "def _ensure_min_resources(progs, cores, memory, min_memory):\n    \"\"\"Ensure setting match minimum resources required for used programs.\n    \"\"\"\n    for p in progs:\n        if p in min_memory:\n            if not memory or cores * memory < min_memory[p]:\n                memory = float(min_memory[p]) / cores\n    return cores, memory"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_prog_memory(resources, cores_per_job):\n    out = None\n    for jvm_opt in resources.get(\"jvm_opts\", []):\n        if jvm_opt.startswith(\"-Xmx\"):\n            out = _str_memory_to_gb(jvm_opt[4:])\n    memory = resources.get(\"memory\")\n    if memory:\n        out = _str_memory_to_gb(memory)\n    prog_cores = resources.get(\"cores\")\n    # if a single core with memory is requested for the job\n    # and we run multiple cores, scale down to avoid overscheduling\n    if out and prog_cores and int(prog_cores) == 1 and cores_per_job > int(prog_cores):\n        out = out / float(cores_per_job)\n    return out", "response": "Get expected memory usage in Gb per core for a program from resource specification."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nscales multicore usage to avoid excessive memory usage based on system information.", "response": "def _scale_cores_to_memory(cores, mem_per_core, sysinfo, system_memory):\n    \"\"\"Scale multicore usage to avoid excessive memory usage based on system information.\n    \"\"\"\n    total_mem = \"%.2f\" % (cores * mem_per_core + system_memory)\n    if \"cores\" not in sysinfo:\n        return cores, total_mem, 1.0\n\n    total_mem = min(float(total_mem), float(sysinfo[\"memory\"]) - system_memory)\n    cores = min(cores, int(sysinfo[\"cores\"]))\n    mem_cores = int(math.floor(float(total_mem) / mem_per_core))  # cores based on available memory\n    if mem_cores < 1:\n        out_cores = 1\n    elif mem_cores < cores:\n        out_cores = mem_cores\n    else:\n        out_cores = cores\n    mem_pct = float(out_cores) / float(cores)\n    return out_cores, total_mem, mem_pct"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _scale_jobs_to_memory(jobs, mem_per_core, sysinfo):\n    if \"cores\" not in sysinfo:\n        return jobs, 1.0\n    sys_mem_per_core = float(sysinfo[\"memory\"]) / float(sysinfo[\"cores\"])\n    if sys_mem_per_core < mem_per_core:\n        pct = sys_mem_per_core / float(mem_per_core)\n        target_jobs = int(math.floor(jobs * pct))\n        return max(target_jobs, 1), pct\n    else:\n        return jobs, 1.0", "response": "Scale jobs to memory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve CPU and memory usage per job for all programs in the job.", "response": "def cpu_and_memory(programs, items):\n    \"\"\"Retrieve CPU and memory/core specified in configuration input.\n    \"\"\"\n    assert len(items) > 0, \"Finding job resources but no items to process\"\n    config = items[0][\"config\"]\n    all_cores = []\n    all_memory = []\n    algs = [config_utils.get_algorithm_config(x) for x in items]\n    progs = _get_resource_programs(programs, algs)\n    # Calculate cores\n    for prog in progs:\n        resources = config_utils.get_resources(prog, config)\n        all_cores.append(resources.get(\"cores\", 1))\n    if len(all_cores) == 0:\n        all_cores.append(1)\n    cores_per_job = max(all_cores)\n    # Calculate memory. Use 1Gb memory usage per core as min baseline if not specified\n    for prog in progs:\n        resources = config_utils.get_resources(prog, config)\n        memory = _get_prog_memory(resources, cores_per_job)\n        if memory:\n            all_memory.append(memory)\n    if len(all_memory) == 0:\n        all_memory.append(1)\n    memory_per_core = max(all_memory)\n    return cores_per_job, memory_per_core"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calculate(parallel, items, sysinfo, config, multiplier=1,\n              max_multicore=None):\n    \"\"\"Determine cores and workers to use for this stage based on used programs.\n    multiplier specifies the number of regions items will be split into during\n    processing.\n    max_multicore specifies an optional limit on the maximum cores. Can use to\n    force single core processing during specific tasks.\n    sysinfo specifies cores and memory on processing nodes, allowing us to tailor\n    jobs for available resources.\n    \"\"\"\n    assert len(items) > 0, \"Finding job resources but no items to process\"\n    all_cores = []\n    all_memory = []\n    # Provide 100Mb of additional memory for the system\n    system_memory = 0.10\n    algs = [config_utils.get_algorithm_config(x) for x in items]\n    progs = _get_resource_programs(parallel.get(\"progs\", []), algs)\n    # Calculate cores\n    for prog in progs:\n        resources = config_utils.get_resources(prog, config)\n        all_cores.append(resources.get(\"cores\", 1))\n    if len(all_cores) == 0:\n        all_cores.append(1)\n    cores_per_job = max(all_cores)\n    if max_multicore:\n        cores_per_job = min(cores_per_job, max_multicore)\n    if \"cores\" in sysinfo:\n        cores_per_job = min(cores_per_job, int(sysinfo[\"cores\"]))\n    total = parallel[\"cores\"]\n    if total > cores_per_job:\n        num_jobs = total // cores_per_job\n    else:\n        num_jobs, cores_per_job = 1, total\n\n    # Calculate memory. Use 1Gb memory usage per core as min baseline if not specified\n    for prog in progs:\n        resources = config_utils.get_resources(prog, config)\n        memory = _get_prog_memory(resources, cores_per_job)\n        if memory:\n            all_memory.append(memory)\n    if len(all_memory) == 0:\n        all_memory.append(1)\n    memory_per_core = max(all_memory)\n\n    logger.debug(\"Resource requests: {progs}; memory: {memory}; cores: {cores}\".format(\n        progs=\", \".join(progs), memory=\", \".join(\"%.2f\" % x for x in all_memory),\n        cores=\", \".join(str(x) for x in all_cores)))\n\n    cores_per_job, memory_per_core = _ensure_min_resources(progs, cores_per_job, memory_per_core,\n                                                           min_memory=parallel.get(\"ensure_mem\", {}))\n    if cores_per_job == 1:\n        memory_per_job = \"%.2f\" % memory_per_core\n        num_jobs, mem_pct = _scale_jobs_to_memory(num_jobs, memory_per_core, sysinfo)\n        # For single core jobs, avoid overscheduling maximum cores_per_job\n        num_jobs = min(num_jobs, total)\n    else:\n        cores_per_job, memory_per_job, mem_pct = _scale_cores_to_memory(cores_per_job,\n                                                                        memory_per_core, sysinfo,\n                                                                        system_memory)\n        # For local runs with multiple jobs and multiple cores, potentially scale jobs down\n        if num_jobs > 1 and parallel.get(\"type\") == \"local\":\n            memory_per_core = float(memory_per_job) / cores_per_job\n            num_jobs, _ = _scale_jobs_to_memory(num_jobs, memory_per_core, sysinfo)\n\n    # do not overschedule if we don't have extra items to process\n    num_jobs = int(min(num_jobs, len(items) * multiplier))\n    logger.debug(\"Configuring %d jobs to run, using %d cores each with %sg of \"\n                 \"memory reserved for each job\" % (num_jobs, cores_per_job,\n                                                   str(memory_per_job)))\n    parallel = copy.deepcopy(parallel)\n    parallel[\"cores_per_job\"] = cores_per_job\n    parallel[\"num_jobs\"] = num_jobs\n    parallel[\"mem\"] = str(memory_per_job)\n    parallel[\"mem_pct\"] = \"%.2f\" % mem_pct\n    parallel[\"system_cores\"] = sysinfo.get(\"cores\", 1)\n    return parallel", "response": "Calculate cores and memory usage for the items in the items list."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a HISAT2 splicesites file for the given GTF file.", "response": "def create_splicesites_file(gtf_file, align_dir, data):\n    \"\"\"\n    if not pre-created, make a splicesites file to use with hisat2\n    \"\"\"\n    out_file = os.path.join(align_dir, \"ref-transcripts-splicesites.txt\")\n    if file_exists(out_file):\n        return out_file\n    safe_makedir(align_dir)\n    hisat2_ss = config_utils.get_program(\"hisat2_extract_splice_sites.py\", data)\n    cmd = \"{hisat2_ss} {gtf_file} > {tx_out_file}\"\n    message = \"Creating hisat2 splicesites file from %s.\" % gtf_file\n    with file_transaction(out_file) as tx_out_file:\n        do.run(cmd.format(**locals()), message)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlocates the splice junction file from hisat2. hisat2 outputs a novel splicesites file to go along with the provided file", "response": "def get_splicejunction_file(align_dir, data):\n    \"\"\"\n    locate the splice junction file from hisat2. hisat2 outputs a novel\n    splicesites file to go along with the provided file, if available.\n    this combines the two together and outputs a combined file of all\n    of the known and novel splice junctions\n    \"\"\"\n    samplename = dd.get_sample_name(data)\n    align_dir = os.path.dirname(dd.get_work_bam(data))\n    knownfile = get_known_splicesites_file(align_dir, data)\n    novelfile = os.path.join(align_dir, \"%s-novelsplicesites.bed\" % samplename)\n    bed_files = [x for x in [knownfile, novelfile] if file_exists(x)]\n    splicejunction = bed.concat(bed_files)\n    splicejunctionfile = os.path.join(align_dir,\n                                      \"%s-splicejunctions.bed\" % samplename)\n    if splicejunction:\n        splicejunction.saveas(splicejunctionfile)\n        return splicejunctionfile\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_info(dirs, parallel, config):\n    if parallel[\"type\"] in [\"ipython\"] and not parallel.get(\"run_local\"):\n        out_file = _get_cache_file(dirs, parallel)\n        if not utils.file_exists(out_file):\n            sys_config = copy.deepcopy(config)\n            minfos = _get_machine_info(parallel, sys_config, dirs, config)\n            with open(out_file, \"w\") as out_handle:\n                yaml.safe_dump(minfos, out_handle, default_flow_style=False, allow_unicode=False)", "response": "Write cluster or local filesystem resources spinning up cluster if not present."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_machine_info(parallel, sys_config, dirs, config):\n    if parallel.get(\"queue\") and parallel.get(\"scheduler\"):\n        # dictionary as switch statement; can add new scheduler implementation functions as (lowercase) keys\n        sched_info_dict = {\n                            \"slurm\": _slurm_info,\n                            \"torque\": _torque_info,\n                            \"sge\": _sge_info\n                          }\n        if parallel[\"scheduler\"].lower() in sched_info_dict:\n            try:\n                return sched_info_dict[parallel[\"scheduler\"].lower()](parallel.get(\"queue\", \"\"))\n            except:\n                # If something goes wrong, just hit the queue\n                logger.exception(\"Couldn't get machine information from resource query function for queue \"\n                                 \"'{0}' on scheduler \\\"{1}\\\"; \"\n                                 \"submitting job to queue\".format(parallel.get(\"queue\", \"\"), parallel[\"scheduler\"]))\n        else:\n            logger.info(\"Resource query function not implemented for scheduler \\\"{0}\\\"; \"\n                         \"submitting job to queue\".format(parallel[\"scheduler\"]))\n    from bcbio.distributed import prun\n    with prun.start(parallel, [[sys_config]], config, dirs) as run_parallel:\n        return run_parallel(\"machine_info\", [[sys_config]])", "response": "Get machine resource information from the job scheduler via either the command line or the queue."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the machine information for a slurm job scheduler.", "response": "def _slurm_info(queue):\n    \"\"\"Returns machine information for a slurm job scheduler.\n    \"\"\"\n    cl = \"sinfo -h -p {} --format '%c %m %D'\".format(queue)\n    num_cpus, mem, num_nodes = subprocess.check_output(shlex.split(cl)).decode().split()\n    # if the queue contains multiple memory configurations, the minimum value is printed with a trailing '+'\n    mem = float(mem.replace('+', ''))\n    num_cpus = int(num_cpus.replace('+', ''))\n    # handle small clusters where we need to allocate memory for bcbio and the controller\n    # This will typically be on cloud AWS machines\n    bcbio_mem = 2000\n    controller_mem = 4000\n    if int(num_nodes) < 3 and mem > (bcbio_mem + controller_mem) * 2:\n        mem = mem - bcbio_mem - controller_mem\n    return [{\"cores\": int(num_cpus), \"memory\": mem / 1024.0, \"name\": \"slurm_machine\"}]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _torque_info(queue):\n    nodes = _torque_queue_nodes(queue)\n    pbs_out = subprocess.check_output([\"pbsnodes\"]).decode()\n    info = {}\n    for i, line in enumerate(pbs_out.split(\"\\n\")):\n        if i == 0 and len(nodes) == 0:\n            info[\"name\"] = line.strip()\n        elif line.startswith(nodes):\n            info[\"name\"] = line.strip()\n        elif info.get(\"name\"):\n            if line.strip().startswith(\"np = \"):\n                info[\"cores\"] = int(line.replace(\"np = \", \"\").strip())\n            elif line.strip().startswith(\"status = \"):\n                mem = [x for x in pbs_out.split(\",\") if x.startswith(\"physmem=\")][0]\n                info[\"memory\"] = float(mem.split(\"=\")[1].rstrip(\"kb\")) / 1048576.0\n                return [info]", "response": "Return information about a torque job scheduler using pbsnodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _torque_queue_nodes(queue):\n    qstat_out = subprocess.check_output([\"qstat\", \"-Qf\", queue]).decode()\n    hosts = []\n    in_hosts = False\n    for line in qstat_out.split(\"\\n\"):\n        if line.strip().startswith(\"acl_hosts = \"):\n            hosts.extend(line.replace(\"acl_hosts = \", \"\").strip().split(\",\"))\n            in_hosts = True\n        elif in_hosts:\n            if line.find(\" = \") > 0:\n                break\n            else:\n                hosts.extend(line.strip().split(\",\"))\n    return tuple([h.split(\".\")[0].strip() for h in hosts if h.strip()])", "response": "Retrieve the nodes available for a queue."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _sge_info(queue):\n    qhost_out = subprocess.check_output([\"qhost\", \"-q\", \"-xml\"]).decode()\n    qstat_queue = [\"-q\", queue] if queue and \",\" not in queue else []\n    qstat_out = subprocess.check_output([\"qstat\", \"-f\", \"-xml\"] + qstat_queue).decode()\n    slot_info = _sge_get_slots(qstat_out)\n    mem_info = _sge_get_mem(qhost_out, queue)\n    machine_keys = slot_info.keys()\n    #num_cpus_vec = [slot_info[x][\"slots_total\"] for x in machine_keys]\n    #mem_vec = [mem_info[x][\"mem_total\"] for x in machine_keys]\n    mem_per_slot = [mem_info[x][\"mem_total\"] / float(slot_info[x][\"slots_total\"]) for x in machine_keys]\n    min_ratio_index = mem_per_slot.index(median_left(mem_per_slot))\n    mem_info[machine_keys[min_ratio_index]][\"mem_total\"]\n    return [{\"cores\": slot_info[machine_keys[min_ratio_index]][\"slots_total\"],\n             \"memory\": mem_info[machine_keys[min_ratio_index]][\"mem_total\"],\n             \"name\": \"sge_machine\"}]", "response": "Returns the machine information for an sge job scheduler."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets slot information from qstat", "response": "def _sge_get_slots(xmlstring):\n    \"\"\" Get slot information from qstat\n    \"\"\"\n    rootxml = ET.fromstring(xmlstring)\n    my_machine_dict = {}\n    for queue_list in rootxml.iter(\"Queue-List\"):\n        # find all hosts supporting queues\n        my_hostname = queue_list.find(\"name\").text.rsplit(\"@\")[-1]\n        my_slots = queue_list.find(\"slots_total\").text\n        my_machine_dict[my_hostname] = {}\n        my_machine_dict[my_hostname][\"slots_total\"] = int(my_slots)\n    return my_machine_dict"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget memory information from qhost xmlstring", "response": "def _sge_get_mem(xmlstring, queue_name):\n    \"\"\" Get memory information from qhost\n    \"\"\"\n    rootxml = ET.fromstring(xmlstring)\n    my_machine_dict = {}\n    # on some machines rootxml.tag looks like \"{...}qhost\" where the \"{...}\" gets prepended to all attributes\n    rootTag = rootxml.tag.rstrip(\"qhost\")\n    for host in rootxml.findall(rootTag + 'host'):\n        # find all hosts supporting queues\n        for queues in host.findall(rootTag + 'queue'):\n            # if the user specified queue matches that in the xml:\n            if not queue_name or any(q in queues.attrib['name'] for q in queue_name.split(\",\")):\n                my_machine_dict[host.attrib['name']] = {}\n                # values from xml for number of processors and mem_total on each machine\n                for hostvalues in host.findall(rootTag + 'hostvalue'):\n                    if('mem_total' == hostvalues.attrib['name']):\n                        if hostvalues.text.lower().endswith('g'):\n                            multip = 1\n                        elif hostvalues.text.lower().endswith('m'):\n                            multip = 1 / float(1024)\n                        elif hostvalues.text.lower().endswith('t'):\n                            multip = 1024\n                        else:\n                            raise Exception(\"Unrecognized suffix in mem_tot from SGE\")\n                        my_machine_dict[host.attrib['name']]['mem_total'] = \\\n                                float(hostvalues.text[:-1]) * float(multip)\n                break\n    return my_machine_dict"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving cluster or local filesystem resources from pre - retrieved information.", "response": "def get_info(dirs, parallel, resources=None):\n    \"\"\"Retrieve cluster or local filesystem resources from pre-retrieved information.\n    \"\"\"\n    # Allow custom specification of cores/memory in resources\n    if resources and isinstance(resources, dict) and \"machine\" in resources:\n        minfo = resources[\"machine\"]\n        assert \"memory\" in minfo, \"Require memory specification (Gb) in machine resources: %s\" % minfo\n        assert \"cores\" in minfo, \"Require core specification in machine resources: %s\" % minfo\n        return minfo\n    if parallel[\"type\"] in [\"ipython\"] and not parallel[\"queue\"] == \"localrun\":\n        cache_file = _get_cache_file(dirs, parallel)\n        if utils.file_exists(cache_file):\n            with open(cache_file) as in_handle:\n                minfo = yaml.safe_load(in_handle)\n            return _combine_machine_info(minfo)\n        else:\n            return {}\n    else:\n        return _combine_machine_info(machine_info())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve core and memory information for the current machine.", "response": "def machine_info():\n    \"\"\"Retrieve core and memory information for the current machine.\n    \"\"\"\n    import psutil\n    BYTES_IN_GIG = 1073741824.0\n    free_bytes = psutil.virtual_memory().total\n    return [{\"memory\": float(\"%.1f\" % (free_bytes / BYTES_IN_GIG)), \"cores\": multiprocessing.cpu_count(),\n             \"name\": socket.gethostname()}]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_count(bam_file, dexseq_gff, stranded, out_file, data):\n    assert file_exists(bam_file), \"%s does not exist.\" % bam_file\n    sort_order = bam._get_sort_order(bam_file, {})\n    assert sort_order, \"Cannot determine sort order of %s.\" % bam_file\n    strand_flag = _strand_flag(stranded)\n    assert strand_flag, \"%s is not a valid strandedness value.\" % stranded\n    if not dexseq_gff:\n        logger.info(\"No DEXSeq GFF file was found, skipping exon-level counting.\")\n        return None\n    elif not file_exists(dexseq_gff):\n        logger.info(\"%s was not found, so exon-level counting is being \"\n                    \"skipped.\" % dexseq_gff)\n        return None\n\n    dexseq_count = _dexseq_count_path()\n    if not dexseq_count:\n        logger.info(\"DEXseq is not installed, skipping exon-level counting.\")\n        return None\n\n    if dd.get_aligner(data) == \"bwa\":\n        logger.info(\"Can't use DEXSeq with bwa alignments, skipping exon-level counting.\")\n        return None\n\n    sort_flag = \"name\" if sort_order == \"queryname\" else \"pos\"\n    is_paired = bam.is_paired(bam_file)\n    paired_flag = \"yes\" if is_paired else \"no\"\n    bcbio_python = sys.executable\n\n    if file_exists(out_file):\n        return out_file\n    cmd = (\"{bcbio_python} {dexseq_count} -f bam -r {sort_flag} -p {paired_flag} \"\n           \"-s {strand_flag} {dexseq_gff} {bam_file} {tx_out_file}\")\n    message = \"Counting exon-level counts with %s and %s.\" % (bam_file, dexseq_gff)\n    with file_transaction(data, out_file) as tx_out_file:\n        do.run(cmd.format(**locals()), message)\n    return out_file", "response": "run dexseq_count on a BAM file and return the ID of the new record."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates an easy data frame to allow easy annotation during differential expression analysis", "response": "def create_dexseq_annotation(gff, count_file):\n    \"\"\"\n    Create an easy data frame to allow easy annotation\n    during differential expression analysis i.e\n    gene:exon_id chr start end strand\n    \"\"\"\n    out_file = count_file + \".ann\"\n    if file_exists(out_file):\n        return out_file\n    with file_transaction(out_file) as tx_out:\n        with open(tx_out, 'w') as out_handle:\n            with open(gff) as in_handle:\n                for line in in_handle:\n                    cols = line.strip().split(\"\\t\")\n                    if cols[2] == \"exonic_part\":\n                        exon = [f for f in cols[8].split(\";\") if f.strip().startswith(\"exonic_part_number\")]\n                        gene = [f for f in cols[8].split(\";\") if f.strip().startswith(\"gene_id\")]\n                        exon = exon[0].replace(\"\\\"\", \"\").split()[1]\n                        gene = gene[0].replace(\"\\\"\", \"\").split()[1]\n                        length = int(cols[4]) - int(cols[3]) + 1\n                        line = \"%s:%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\" % (gene, exon, gene,\n                                                                    cols[0], cols[3],\n                                                                    cols[4],\n                                                                    length, cols[6])\n                        out_handle.write(line)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntrim the adapters in the input file", "response": "def _trim_adapters(fastq_files, out_dir, data):\n    \"\"\"\n    for small insert sizes, the read length can be longer than the insert\n    resulting in the reverse complement of the 3' adapter being sequenced.\n    this takes adapter sequences and trims the only the reverse complement\n    of the adapter\n\n    MYSEQUENCEAAAARETPADA -> MYSEQUENCEAAAA (no polyA trim)\n    \"\"\"\n    to_trim = _get_sequences_to_trim(data[\"config\"], SUPPORTED_ADAPTERS)\n    if dd.get_trim_reads(data) == \"fastp\":\n        out_files, report_file = _fastp_trim(fastq_files, to_trim, out_dir, data)\n    else:\n        out_files, report_file = _atropos_trim(fastq_files, to_trim, out_dir, data)\n    # quality_format = _get_quality_format(data[\"config\"])\n    # out_files = replace_directory(append_stem(fastq_files, \"_%s.trimmed\" % name), out_dir)\n    # log_file = \"%s_log_cutadapt.txt\" % splitext_plus(out_files[0])[0]\n    # out_files = _cutadapt_trim(fastq_files, quality_format, to_trim, out_files, log_file, data)\n    # if file_exists(log_file):\n    #     content = open(log_file).read().replace(fastq_files[0], name)\n    #     if len(fastq_files) > 1:\n    #         content = content.replace(fastq_files[1], name)\n    #     open(log_file, 'w').write(content)\n    return out_files"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _atropos_trim(fastq_files, adapters, out_dir, data):\n    report_file = os.path.join(out_dir, \"%s-report.json\" % utils.splitext_plus(os.path.basename(fastq_files[0]))[0])\n    out_files = [os.path.join(out_dir, \"%s-trimmed.fq.gz\" % utils.splitext_plus(os.path.basename(x))[0])\n                 for x in fastq_files]\n    if not utils.file_exists(out_files[0]):\n        with file_transaction(data, *[report_file] + out_files) as tx_out:\n            tx_report_file, tx_out1 = tx_out[:2]\n            if len(tx_out) > 2:\n                tx_out2 = tx_out[2]\n            # polyX trimming, anchored to the 3' ends of reads\n            if \"polyx\" in dd.get_adapters(data):\n                adapters += [\"A{200}\", \"C{200}\", \"G{200}\", \"T{200}\"]\n            adapters_args = \" \".join([\"-a '%s'\" % a for a in adapters])\n            adapters_args += \" --overlap 8\"  # Avoid very short internal matches (default is 3)\n            adapters_args += \" --no-default-adapters --no-cache-adapters\"  # Prevent GitHub queries and saving pickles\n            aligner_args = \"--aligner adapter\"\n            if len(fastq_files) == 1:\n                cores = dd.get_num_cores(data)\n                input_args = \"-se %s\" % objectstore.cl_input(fastq_files[0])\n                output_args = \"-o >(bgzip --threads {cores} -c > {tx_out1})\".format(**locals())\n            else:\n                assert len(fastq_files) == 2, fastq_files\n                cores = max(1, dd.get_num_cores(data) // 2)\n                adapters_args = adapters_args + \" \" + \" \".join([\"-A '%s'\" % a for a in adapters])\n                input_args = \"-pe1 %s -pe2 %s\" % tuple([objectstore.cl_input(x) for x in fastq_files])\n                output_args = (\"-o >(bgzip --threads {cores} -c > {tx_out1}) \"\n                               \"-p >(bgzip --threads {cores} -c > {tx_out2})\").format(**locals())\n            quality_base = \"64\" if dd.get_quality_format(data).lower() == \"illumina\" else \"33\"\n            sample_name = dd.get_sample_name(data)\n            report_args = \"--report-file %s --report-formats json --sample-id %s\" % (tx_report_file,\n                                                                                     dd.get_sample_name(data))\n            ropts = \" \".join(str(x) for x in\n                             config_utils.get_resources(\"atropos\", data[\"config\"]).get(\"options\", []))\n            extra_opts = []\n            for k, alt_ks, v, want in [(\"--quality-cutoff\", [\"-q \"], \"5\", True),\n                                       (\"--minimum-length\", [\"-m \"], str(dd.get_min_read_length(data)), True),\n                                       (\"--nextseq-trim\", [], \"25\", (\"polyx\" in dd.get_adapters(data) or\n                                                                     \"polyg\" in dd.get_adapters(data)))]:\n                if k not in ropts and not any(alt_k in ropts for alt_k in alt_ks):\n                    if want:\n                        extra_opts.append(\"%s=%s\" % (k, v))\n            extra_opts = \" \".join(extra_opts)\n            thread_args = (\"--threads %s\" % cores if cores > 1 else \"\")\n            cmd = (\"atropos trim {ropts} {thread_args} --quality-base {quality_base} --format fastq \"\n                   \"{adapters_args} {input_args} {output_args} {report_args} {extra_opts}\")\n            do.run(cmd.format(**locals()), \"Trimming with atropos: %s\" % dd.get_sample_name(data))\n    return out_files, report_file", "response": "Perform multicore trimming with atropos."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _fastp_trim(fastq_files, adapters, out_dir, data):\n    report_file = os.path.join(out_dir, \"%s-report.json\" % utils.splitext_plus(os.path.basename(fastq_files[0]))[0])\n    out_files = [os.path.join(out_dir, \"%s-trimmed.fq.gz\" % utils.splitext_plus(os.path.basename(x))[0])\n                 for x in fastq_files]\n    if not utils.file_exists(out_files[0]):\n        with file_transaction(data, *[report_file] + out_files) as tx_out:\n            tx_report = tx_out[0]\n            tx_out_files = tx_out[1:]\n            cmd = [\"fastp\", \"--thread\", dd.get_num_cores(data)]\n            if dd.get_quality_format(data).lower() == \"illumina\":\n                cmd += [\"--phred64\"]\n            for i, (inf, outf) in enumerate(zip(fastq_files, tx_out_files)):\n                if i == 0:\n                    cmd += [\"-i\", inf, \"-o\", outf]\n                else:\n                    cmd += [\"-I\", inf, \"-O\", outf]\n            cmd += [\"--cut_by_quality3\", \"--cut_mean_quality\", \"5\",\n                    \"--length_required\", str(dd.get_min_read_length(data)),\n                    \"--disable_quality_filtering\"]\n            if \"polyx\" in dd.get_adapters(data):\n                cmd += [\"--trim_poly_x\", \"--poly_x_min_len\", \"8\"]\n            if \"polyx\" in dd.get_adapters(data) or \"polyg\" in dd.get_adapters(data):\n                cmd += [\"--trim_poly_g\", \"--poly_g_min_len\", \"8\"]\n            for a in adapters:\n                cmd += [\"--adapter_sequence\", a]\n            if not adapters:\n                cmd += [\"--disable_adapter_trimming\"]\n            cmd += [\"--json\", report_file, \"--report_title\", dd.get_sample_name(data)]\n            do.run(cmd, \"Trimming with fastp: %s\" % dd.get_sample_name(data))\n    return out_files, report_file", "response": "Perform multicore trimming with Fastp."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntrim the given fastq files with cutadapt.", "response": "def _cutadapt_trim(fastq_files, quality_format, adapters, out_files, log_file, data):\n    \"\"\"Trimming with cutadapt.\n    \"\"\"\n    if all([utils.file_exists(x) for x in out_files]):\n        return out_files\n    cmd = _cutadapt_trim_cmd(fastq_files, quality_format, adapters, out_files, data)\n    if len(fastq_files) == 1:\n        of = [out_files[0], log_file]\n        message = \"Trimming %s in single end mode with cutadapt.\" % (fastq_files[0])\n        with file_transaction(data, of) as of_tx:\n            of1_tx, log_tx = of_tx\n            do.run(cmd.format(**locals()), message)\n    else:\n        of = out_files + [log_file]\n        with file_transaction(data, of) as tx_out_files:\n            of1_tx, of2_tx, log_tx = tx_out_files\n            tmp_fq1 = utils.append_stem(of1_tx, \".tmp\")\n            tmp_fq2 = utils.append_stem(of2_tx, \".tmp\")\n            singles_file = of1_tx + \".single\"\n            message = \"Trimming %s and %s in paired end mode with cutadapt.\" % (fastq_files[0],\n                                                                                fastq_files[1])\n            do.run(cmd.format(**locals()), message)\n    return out_files"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntrim with cutadapt using version installed with bcbio - nextgen.", "response": "def _cutadapt_trim_cmd(fastq_files, quality_format, adapters, out_files, data):\n    \"\"\"Trimming with cutadapt, using version installed with bcbio-nextgen.\n    \"\"\"\n    if all([utils.file_exists(x) for x in out_files]):\n        return out_files\n    if quality_format == \"illumina\":\n        quality_base = \"64\"\n    else:\n        quality_base = \"33\"\n\n    # --times=2 tries twice remove adapters which will allow things like:\n    # realsequenceAAAAAAadapter to remove both the poly-A and the adapter\n    # this behavior might not be what we want; we could also do two or\n    # more passes of cutadapt\n    cutadapt = os.path.join(os.path.dirname(sys.executable), \"cutadapt\")\n    adapter_cmd = \" \".join(map(lambda x: \"-a \" + x, adapters))\n    ropts = \" \".join(str(x) for x in\n                     config_utils.get_resources(\"cutadapt\", data[\"config\"]).get(\"options\", []))\n    base_cmd = (\"{cutadapt} {ropts} --times=2 --quality-base={quality_base} \"\n                \"--quality-cutoff=5 --format=fastq \"\n                \"{adapter_cmd} \").format(**locals())\n    if len(fastq_files) == 2:\n        # support for the single-command paired trimming introduced in\n        # cutadapt 1.8\n        adapter_cmd = adapter_cmd.replace(\"-a \", \"-A \")\n        base_cmd += \"{adapter_cmd} \".format(adapter_cmd=adapter_cmd)\n        return _cutadapt_pe_cmd(fastq_files, out_files, quality_format, base_cmd, data)\n    else:\n        return _cutadapt_se_cmd(fastq_files, out_files, base_cmd, data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns command to cutadapt. se command for reading a sequence of fastq files.", "response": "def _cutadapt_se_cmd(fastq_files, out_files, base_cmd, data):\n    \"\"\"\n    this has to use the -o option, not redirect to stdout in order for\n    gzipping to be supported\n    \"\"\"\n    min_length = dd.get_min_read_length(data)\n    cmd = base_cmd + \" --minimum-length={min_length} \".format(**locals())\n    fq1 = objectstore.cl_input(fastq_files[0])\n    of1 = out_files[0]\n    cmd += \" -o {of1_tx} \" + str(fq1)\n    cmd = \"%s | tee > {log_tx}\" % cmd\n    return cmd"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning cutadapt command to run in paired end mode.", "response": "def _cutadapt_pe_cmd(fastq_files, out_files, quality_format, base_cmd, data):\n    \"\"\"\n    run cutadapt in paired end mode\n    \"\"\"\n    fq1, fq2 = [objectstore.cl_input(x) for x in fastq_files]\n    of1, of2 = out_files\n    base_cmd += \" --minimum-length={min_length} \".format(min_length=dd.get_min_read_length(data))\n    first_cmd = base_cmd + \" -o {of1_tx} -p {of2_tx} \" + fq1 + \" \" + fq2\n    return first_cmd + \"| tee > {log_tx};\""}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a list of interval regions for realignment around indels.", "response": "def gatk_realigner_targets(runner, align_bam, ref_file, config, dbsnp=None,\n                           region=None, out_file=None, deep_coverage=False,\n                           variant_regions=None, known_vrns=None):\n    \"\"\"Generate a list of interval regions for realignment around indels.\n    \"\"\"\n    if not known_vrns:\n        known_vrns = {}\n    if out_file:\n        out_file = \"%s.intervals\" % os.path.splitext(out_file)[0]\n    else:\n        out_file = \"%s-realign.intervals\" % os.path.splitext(align_bam)[0]\n    # check only for file existence; interval files can be empty after running\n    # on small chromosomes, so don't rerun in those cases\n    if not os.path.exists(out_file):\n        with file_transaction(config, out_file) as tx_out_file:\n            logger.debug(\"GATK RealignerTargetCreator: %s %s\" %\n                         (os.path.basename(align_bam), region))\n            params = [\"-T\", \"RealignerTargetCreator\",\n                      \"-I\", align_bam,\n                      \"-R\", ref_file,\n                      \"-o\", tx_out_file,\n                      \"-l\", \"INFO\",\n                      ]\n            region = subset_variant_regions(variant_regions, region, tx_out_file)\n            if region:\n                params += [\"-L\", region, \"--interval_set_rule\", \"INTERSECTION\"]\n            if known_vrns.get(\"train_indels\"):\n                params += [\"--known\", known_vrns[\"train_indels\"]]\n            if deep_coverage:\n                params += [\"--mismatchFraction\", \"0.30\",\n                           \"--maxIntervalSize\", \"650\"]\n            runner.run_gatk(params, memscale={\"direction\": \"decrease\", \"magnitude\": 2})\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npreparing GATK indel realignment command line arguments for GATK indel realignment.", "response": "def gatk_indel_realignment_cl(runner, align_bam, ref_file, intervals,\n                              tmp_dir, region=None, deep_coverage=False,\n                              known_vrns=None):\n    \"\"\"Prepare input arguments for GATK indel realignment.\n    \"\"\"\n    if not known_vrns:\n        known_vrns = {}\n    params = [\"-T\", \"IndelRealigner\",\n              \"-I\", align_bam,\n              \"-R\", ref_file,\n              \"-targetIntervals\", intervals,\n              ]\n    if region:\n        params += [\"-L\", region]\n    if known_vrns.get(\"train_indels\"):\n        params += [\"--knownAlleles\", known_vrns[\"train_indels\"]]\n    if deep_coverage:\n        params += [\"--maxReadsInMemory\", \"300000\",\n                   \"--maxReadsForRealignment\", str(int(5e5)),\n                   \"--maxReadsForConsensuses\", \"500\",\n                   \"--maxConsensuses\", \"100\"]\n    return runner.cl_gatk(params, tmp_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef has_aligned_reads(align_bam, region=None):\n    import pybedtools\n    if region is not None:\n        if isinstance(region, six.string_types) and os.path.isfile(region):\n            regions = [tuple(r) for r in pybedtools.BedTool(region)]\n        else:\n            regions = [region]\n    with pysam.Samfile(align_bam, \"rb\") as cur_bam:\n        if region is not None:\n            for region in regions:\n                if isinstance(region, six.string_types):\n                    for item in cur_bam.fetch(str(region)):\n                        return True\n                else:\n                    for item in cur_bam.fetch(str(region[0]), int(region[1]), int(region[2])):\n                        return True\n        else:\n            for item in cur_bam:\n                if not item.is_unmapped:\n                    return True\n    return False", "response": "Check if the aligned BAM file has any reads in the specified region."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef s(name, parallel, inputs, outputs, image, programs=None, disk=None, cores=None, unlist=None,\n      no_files=False):\n    \"\"\"Represent a step in a workflow.\n\n    name -- The run function name, which must match a definition in distributed/multitasks\n    inputs -- List of input keys required for the function. Each key is of the type:\n      [\"toplevel\", \"sublevel\"] -- an argument you could pass to toolz.get_in.\n    outputs -- List of outputs with information about file type. Use cwlout functions\n    programs -- Required programs for this step, used to define resource usage.\n    disk -- Information about disk usage requirements, specified as multipliers of\n            input files. Ensures enough disk present when that is a limiting factor\n            when selecting cloud node resources.\n    cores -- Maximum cores necessary for this step, for non-multicore processes.\n    unlist -- Variables being unlisted by this process. Useful for parallelization splitting and\n      batching from multiple variables, like variant calling.\n    no_files -- This step does not require file access.\n    parallel -- Parallelization approach. There are three different levels of parallelization,\n      each with subcomponents:\n\n      1. multi -- Multiple samples, parallelizing at the sample level. Used in top-level workflow.\n        - multi-parallel -- Run individual samples in parallel.\n        - multi-combined -- Run all samples together.\n        - multi-batch -- Run all samples together, converting into batches of grouped samples.\n      2. single -- A single sample, used in sub-workflows.\n        - single-split -- Split a sample into sub-components (by read sections).\n        - single-parallel -- Run sub-components of a sample in parallel.\n        - single-merge -- Merge multiple sub-components into a single sample.\n        - single-single -- Single sample, single item, nothing fancy.\n      3. batch -- Several related samples (tumor/normal, or populations). Used in sub-workflows.\n        - batch-split -- Split a batch of samples into sub-components (by genomic region).\n        - batch-parallel -- Run sub-components of a batch in parallel.\n        - batch-merge -- Merge sub-components back into a single batch.\n        - batch-single -- Run on a single batch.\n    \"\"\"\n    Step = collections.namedtuple(\"Step\", \"name parallel inputs outputs image programs disk cores unlist no_files\")\n    if programs is None: programs = []\n    if unlist is None: unlist = []\n    return Step(name, parallel, inputs, outputs, image, programs, disk, cores, unlist, no_files)", "response": "Represent a step in a workflow."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef w(name, parallel, workflow, internal):\n    Workflow = collections.namedtuple(\"Workflow\", \"name parallel workflow internal\")\n    return Workflow(name, parallel, workflow, internal)", "response": "A workflow that allows specification of sub - workflows for nested parallelization."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef et(name, parallel, inputs, outputs, expression):\n    ExpressionTool = collections.namedtuple(\"ExpressionTool\", \"name inputs outputs expression parallel\")\n    return ExpressionTool(name, inputs, outputs, expression, parallel)", "response": "Represent an ExpressionTool that reorders inputs using javascript."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _variant_hla(checkpoints):\n    if not checkpoints.get(\"hla\"):\n        return [], []\n    hla = [s(\"hla_to_rec\", \"multi-batch\",\n             [[\"hla\", \"fastq\"],\n              [\"config\", \"algorithm\", \"hlacaller\"]],\n               [cwlout(\"hla_rec\", \"record\")],\n               \"bcbio-vc\", cores=1, no_files=True),\n           s(\"call_hla\", \"multi-parallel\",\n             [[\"hla_rec\"]],\n             [cwlout([\"hla\", \"hlacaller\"], [\"string\", \"null\"]),\n              cwlout([\"hla\", \"call_file\"], [\"File\", \"null\"])],\n             \"bcbio-vc\", [\"optitype;env=python2\", \"razers3=3.5.0\", \"coincbc\"])]\n    return hla, [[\"hla\", \"call_file\"]]", "response": "Add hla analysis to workflow if configured."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd variant calling to workflow if configured.", "response": "def _variant_vc(checkpoints):\n    \"\"\"Add variant calling to workflow, if configured.\n    \"\"\"\n    if not checkpoints.get(\"vc\"):\n        return [], []\n    vc_wf = [s(\"get_parallel_regions\", \"batch-split\",\n               [[\"batch_rec\"]],\n               [cwlout([\"region_block\"], {\"type\": \"array\", \"items\": \"string\"})],\n               \"bcbio-vc\",\n               disk={\"files\": 2.0}, cores=1),\n             s(\"variantcall_batch_region\", \"batch-parallel\",\n               [[\"batch_rec\"], [\"region_block\"]],\n               [cwlout([\"vrn_file_region\"], [\"File\", \"null\"], [\".tbi\"]),\n                cwlout([\"region_block\"], {\"type\": \"array\", \"items\": \"string\"})],\n               \"bcbio-vc\", [\"bcftools\", \"bedtools\", \"freebayes=1.1.0.46\",\n                            \"gatk4\", \"vqsr_cnn\", \"deepvariant;env=dv\", \"sentieon;env=python2\",\n                            \"htslib\", \"octopus\", \"picard\", \"platypus-variant;env=python2\", \"pythonpy\",\n                            \"samtools\", \"pysam>=0.13.0\", \"strelka;env=python2\", \"vardict\", \"vardict-java\",\n                            \"varscan\", \"moreutils\", \"vcfanno\", \"vcflib\", \"vt\", \"r=3.5.1\", \"r-base\",\n                            \"perl\"],\n               disk={\"files\": 2.0}),\n             s(\"concat_batch_variantcalls\", \"batch-merge\",\n               [[\"batch_rec\"], [\"region_block\"], [\"vrn_file_region\"]],\n               [cwlout([\"vrn_file\"], \"File\", [\".tbi\"])],\n               \"bcbio-vc\", [\"bcftools\", \"htslib\", \"gatk4\"],\n               disk={\"files\": 1.5}, cores=1)]\n    if not checkpoints.get(\"jointvc\"):\n        vc_wf += [s(\"postprocess_variants\", \"batch-single\",\n                    [[\"batch_rec\"], [\"vrn_file\"]],\n                    [cwlout([\"vrn_file\"], \"File\", [\".tbi\"])],\n                    \"bcbio-vc\", [\"snpeff=4.3.1t\"], disk={\"files\": 0.5})]\n    vc_rec_exclude = [[\"align_bam\"]]\n    if not checkpoints.get(\"jointvc\"):\n        vc_rec_exclude.append([\"genome_resources\", \"variation\"])\n    vc_wf += [s(\"compare_to_rm\", \"batch-single\",\n                [[\"batch_rec\"], [\"vrn_file\"]],\n                [cwlout(\"vc_rec\", \"record\",\n                        fields=[cwlout([\"batch_samples\"], [\"null\", {\"type\": \"array\", \"items\": \"string\"}]),\n                                cwlout([\"validate\", \"summary\"], [\"File\", \"null\"]),\n                                cwlout([\"validate\", \"tp\"], [\"File\", \"null\"], [\".tbi\"]),\n                                cwlout([\"validate\", \"fp\"], [\"File\", \"null\"], [\".tbi\"]),\n                                cwlout([\"validate\", \"fn\"], [\"File\", \"null\"], [\".tbi\"]),\n                                cwlout(\"inherit\", exclude=vc_rec_exclude)])],\n                \"bcbio-vc\", [\"bcftools\", \"bedtools\", \"pythonpy\", \"gvcf-regions;env=python2\",\n                             \"htslib\", \"rtg-tools\", \"vcfanno\"],\n                disk={\"files\": 1.5})]\n    batch_in = [[\"analysis\"], [\"genome_build\"], [\"align_bam\"], [\"vrn_file\"],\n                [\"metadata\", \"batch\"], [\"metadata\", \"phenotype\"],\n                [\"config\", \"algorithm\", \"callable_regions\"], [\"regions\", \"sample_callable\"],\n                [\"config\", \"algorithm\", \"variantcaller\"],\n                [\"config\", \"algorithm\", \"ensemble\"],\n                [\"config\", \"algorithm\", \"vcfanno\"],\n                [\"config\", \"algorithm\", \"coverage_interval\"],\n                [\"config\", \"algorithm\", \"effects\"],\n                [\"config\", \"algorithm\", \"min_allele_fraction\"],\n                [\"config\", \"algorithm\", \"exclude_regions\"],\n                [\"config\", \"algorithm\", \"variant_regions\"],\n                [\"config\", \"algorithm\", \"variant_regions_merged\"],\n                [\"config\", \"algorithm\", \"validate\"], [\"config\", \"algorithm\", \"validate_regions\"],\n                [\"config\", \"algorithm\", \"tools_on\"],\n                [\"config\", \"algorithm\", \"tools_off\"],\n                [\"reference\", \"fasta\", \"base\"],\n                [\"reference\", \"rtg\"], [\"reference\", \"genome_context\"],\n                [\"genome_resources\", \"variation\", \"clinvar\"],\n                [\"genome_resources\", \"variation\", \"cosmic\"], [\"genome_resources\", \"variation\", \"dbsnp\"],\n                [\"genome_resources\", \"variation\", \"esp\"], [\"genome_resources\", \"variation\", \"exac\"],\n                [\"genome_resources\", \"variation\", \"gnomad_exome\"],\n                [\"genome_resources\", \"variation\", \"1000g\"],\n                [\"genome_resources\", \"variation\", \"lcr\"], [\"genome_resources\", \"variation\", \"polyx\"],\n                [\"genome_resources\", \"variation\", \"encode_blacklist\"],\n                [\"genome_resources\", \"aliases\", \"ensembl\"], [\"genome_resources\", \"aliases\", \"human\"],\n                [\"genome_resources\", \"aliases\", \"snpeff\"], [\"reference\", \"snpeff\", \"genome_build\"]]\n    if checkpoints.get(\"umi\"):\n        batch_in.append([\"config\", \"algorithm\", \"umi_type\"])\n    if checkpoints.get(\"rnaseq\"):\n        batch_in += [[\"genome_resources\", \"variation\", \"editing\"]]\n    else:\n        batch_in += [[\"genome_resources\", \"variation\", \"train_hapmap\"],\n                     [\"genome_resources\", \"variation\", \"train_indels\"]]\n    vc = [s(\"batch_for_variantcall\", \"multi-batch\", batch_in,\n            [cwlout(\"batch_rec\", \"record\",\n                    fields=[cwlout([\"config\", \"algorithm\", \"variantcaller_order\"], \"int\"),\n                            cwlout(\"inherit\")])],\n            \"bcbio-vc\",\n            disk={\"files\": 2.0}, cores=1,\n            unlist=[[\"config\", \"algorithm\", \"variantcaller\"]], no_files=True),\n          w(\"variantcall\", \"multi-parallel\", vc_wf,\n            [[\"region\"], [\"region_block\"], [\"vrn_file_region\"], [\"vrn_file\"], [\"validate\", \"summary\"]])]\n    if checkpoints.get(\"jointvc\"):\n        vc += _variant_jointvc()\n    if checkpoints.get(\"ensemble\"):\n        vc += _variant_ensemble(checkpoints)\n    summarize_in = [[\"jointvc_rec\" if checkpoints.get(\"jointvc\") else \"vc_rec\"]]\n    if checkpoints.get(\"ensemble\"):\n        summarize_in += [[\"ensemble_rec\"]]\n    vc += [s(\"summarize_vc\", \"multi-combined\", summarize_in,\n             [cwlout([\"variants\", \"calls\"], {\"type\": \"array\", \"items\": [\"File\", \"null\"]}),\n              cwlout([\"variants\", \"gvcf\"], [\"null\", {\"type\": \"array\", \"items\": [\"File\", \"null\"]}]),\n              cwlout([\"variants\", \"samples\"], {\"type\": \"array\", \"items\": {\"type\": \"array\",\n                                                                          \"items\": [\"File\", \"null\"]}}),\n              cwlout([\"validate\", \"grading_summary\"], [\"File\", \"null\"]),\n              cwlout([\"validate\", \"grading_plots\"], {\"type\": \"array\", \"items\": [\"File\", \"null\"]})],\n             \"bcbio-vc\",\n             disk={\"files\": 2.0}, cores=1)]\n    return vc, [[\"validate\", \"grading_summary\"], [\"variants\", \"calls\"], [\"variants\", \"gvcf\"]]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary of all the variant checkpoints that can be used for analysis.", "response": "def _variant_checkpoints(samples):\n    \"\"\"Check sample configuration to identify required steps in analysis.\n    \"\"\"\n    checkpoints = {}\n    checkpoints[\"vc\"] = any([dd.get_variantcaller(d) or d.get(\"vrn_file\") for d in samples])\n    checkpoints[\"sv\"] = any([dd.get_svcaller(d) for d in samples])\n    checkpoints[\"jointvc\"] = any([(dd.get_jointcaller(d) or \"gvcf\" in dd.get_tools_on(d))\n                                  for d in samples])\n    checkpoints[\"hla\"] = any([dd.get_hlacaller(d) for d in samples])\n    checkpoints[\"align\"] = any([(dd.get_aligner(d) or dd.get_bam_clean(d)) for d in samples])\n    checkpoints[\"align_split\"] = not all([(dd.get_align_split_size(d) is False or\n                                           not dd.get_aligner(d))\n                                          for d in samples])\n    checkpoints[\"archive\"] = any([dd.get_archive(d) for d in samples])\n    checkpoints[\"umi\"] = any([dd.get_umi_consensus(d) for d in samples])\n    checkpoints[\"ensemble\"] = any([dd.get_ensemble(d) for d in samples])\n    checkpoints[\"cancer\"] = any(dd.get_phenotype(d) in [\"tumor\"] for d in samples)\n    return checkpoints"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _rnaseq_checkpoints(samples):\n    checkpoints = {}\n    checkpoints[\"rnaseq\"] = True\n    checkpoints[\"vc\"] = any([dd.get_variantcaller(d) for d in samples])\n    return checkpoints", "response": "Check sample configuration to identify required steps in analysis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef breakpoints_by_caller(bed_files):\n    merged = concat(bed_files)\n    if not merged:\n        return []\n    grouped_start = merged.groupby(g=[1, 2, 2], c=4, o=[\"distinct\"]).filter(lambda r: r.end > r.start).saveas()\n    grouped_end = merged.groupby(g=[1, 3, 3], c=4, o=[\"distinct\"]).filter(lambda r: r.end > r.start).saveas()\n    together = concat([grouped_start, grouped_end])\n    if together:\n        final = together.expand(c=4)\n        final = final.sort()\n        return final", "response": "Given a list of BED files of the form caller1 caller2 caller1 caller2 return a list of BedTools of breakpoints as each line with the fourth column the caller2"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a sorted list of all of the structural variant callers", "response": "def _get_sv_callers(items):\n    \"\"\"\n    return a sorted list of all of the structural variant callers run\n    \"\"\"\n    callers = []\n    for data in items:\n        for sv in data.get(\"sv\", []):\n            callers.append(sv[\"variantcaller\"])\n    return list(set([x for x in callers if x != \"sv-ensemble\"])).sort()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_ensemble_bed_files(items):\n    bed_files = []\n    for data in items:\n        for sv in data.get(\"sv\", []):\n            if sv[\"variantcaller\"] == \"sv-ensemble\":\n                if (\"vrn_file\" in sv and not vcfutils.get_paired_phenotype(data) == \"normal\"\n                      and file_exists(sv[\"vrn_file\"])):\n                    bed_files.append(sv[\"vrn_file\"])\n    return bed_files", "response": "Get all ensemble BED files from items."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _prioritize_plot_regions(region_bt, data, out_dir=None):\n    max_plots = 1000\n    max_size = 100 * 1000 # 100kb\n    out_file = \"%s-priority%s\" % utils.splitext_plus(region_bt.fn)\n    if out_dir:\n        out_file = os.path.join(out_dir, os.path.basename(out_file))\n    num_plots = 0\n    if not utils.file_uptodate(out_file, region_bt.fn):\n        with file_transaction(data, out_file) as tx_out_file:\n            with open(tx_out_file, \"w\") as out_handle:\n                for r in region_bt:\n                    if r.stop - r.start < max_size:\n                        if num_plots < max_plots:\n                            num_plots += 1\n                            out_handle.write(\"%s\\t%s\\t%s\\n\" % (r.chrom, r.start, r.stop))\n    return out_file", "response": "Prioritize most interesting regions."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots for a union set of combined ensemble regions across all of the data items.", "response": "def by_regions(items):\n    \"\"\"Plot for a union set of combined ensemble regions across all of the data\n       items.\n    \"\"\"\n    work_dir = os.path.join(dd.get_work_dir(items[0]), \"structural\", \"coverage\")\n    safe_makedir(work_dir)\n    out_file = os.path.join(work_dir, \"%s-coverage.pdf\" % (dd.get_sample_name(items[0])))\n    if file_exists(out_file):\n        items = _add_regional_coverage_plot(items, out_file)\n    else:\n        bed_files = _get_ensemble_bed_files(items)\n        merged = bed.merge(bed_files)\n        breakpoints = breakpoints_by_caller(bed_files)\n        if merged:\n            priority_merged = _prioritize_plot_regions(merged, items[0])\n            out_file = plot_multiple_regions_coverage(items, out_file, items[0],\n                                                      priority_merged, breakpoints)\n            items = _add_regional_coverage_plot(items, out_file)\n    return items"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef finalize_sv(orig_vcf, data, items):\n    paired = vcfutils.get_paired(items)\n    # For paired/somatic, attach combined calls to tumor sample\n    if paired:\n        sample_vcf = orig_vcf if paired.tumor_name == dd.get_sample_name(data) else None\n    else:\n        sample_vcf = \"%s-%s.vcf.gz\" % (utils.splitext_plus(orig_vcf)[0], dd.get_sample_name(data))\n        sample_vcf = vcfutils.select_sample(orig_vcf, dd.get_sample_name(data), sample_vcf, data[\"config\"])\n    if sample_vcf:\n        effects_vcf, _ = effects.add_to_vcf(sample_vcf, data, \"snpeff\")\n    else:\n        effects_vcf = None\n    return effects_vcf or sample_vcf", "response": "Finalize structural variants adding effects and splitting if needed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef annotate_with_depth(in_file, items):\n    bam_file = None\n    if len(items) == 1:\n        bam_file = dd.get_align_bam(items[0])\n    else:\n        paired = vcfutils.get_paired(items)\n        if paired:\n            bam_file = paired.tumor_bam\n    if bam_file:\n        out_file = \"%s-duphold.vcf.gz\" % utils.splitext_plus(in_file)[0]\n        if not utils.file_exists(out_file):\n            with file_transaction(items[0], out_file) as tx_out_file:\n                if not in_file.endswith(\".gz\"):\n                    in_file = vcfutils.bgzip_and_index(in_file, remove_orig=False,\n                                                       out_dir=os.path.dirname(tx_out_file))\n                ref_file = dd.get_ref_file(items[0])\n                # cores for BAM reader thread, so max out at 4 based on recommendations\n                cores = min([dd.get_num_cores(items[0]), 4])\n                cmd = (\"duphold --threads {cores} --vcf {in_file} --bam {bam_file} --fasta {ref_file} \"\n                       \"-o {tx_out_file}\")\n                do.run(cmd.format(**locals()), \"Annotate SV depth with duphold\")\n        vcfutils.bgzip_and_index(out_file)\n        return out_file\n    else:\n        return in_file", "response": "Annotate called VCF file with depth using duphold."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding case and control items in a population of multiple samples.", "response": "def find_case_control(items):\n    \"\"\"Find case/control items in a population of multiple samples.\n    \"\"\"\n    cases = []\n    controls = []\n    for data in items:\n        if population.get_affected_status(data) == 1:\n            controls.append(data)\n        else:\n            cases.append(data)\n    return cases, controls"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves SV file of regions to exclude.", "response": "def _get_sv_exclude_file(items):\n    \"\"\"Retrieve SV file of regions to exclude.\n    \"\"\"\n    sv_bed = utils.get_in(items[0], (\"genome_resources\", \"variation\", \"sv_repeat\"))\n    if sv_bed and os.path.exists(sv_bed):\n        return sv_bed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving variant regions defined in any of the input items.", "response": "def _get_variant_regions(items):\n    \"\"\"Retrieve variant regions defined in any of the input items.\n    \"\"\"\n    return list(filter(lambda x: x is not None,\n                       [tz.get_in((\"config\", \"algorithm\", \"variant_regions\"), data)\n                        for data in items\n                        if tz.get_in([\"config\", \"algorithm\", \"coverage_interval\"], data) != \"genome\"]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef has_variant_regions(items, base_file, chrom=None):\n    if chrom:\n        all_vrs = _get_variant_regions(items)\n        if len(all_vrs) > 0:\n            test = shared.subset_variant_regions(tz.first(all_vrs), chrom, base_file, items)\n            if test == chrom:\n                return False\n    return True", "response": "Determine if we should process this chromosome?"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npreparing a BED file for exclusion.", "response": "def prepare_exclude_file(items, base_file, chrom=None):\n    \"\"\"Prepare a BED file for exclusion.\n\n    Excludes high depth and centromere regions which contribute to long run times and\n    false positive structural variant calls.\n    \"\"\"\n    items = shared.add_highdepth_genome_exclusion(items)\n    out_file = \"%s-exclude%s.bed\" % (utils.splitext_plus(base_file)[0], \"-%s\" % chrom if chrom else \"\")\n    if not utils.file_exists(out_file) and not utils.file_exists(out_file + \".gz\"):\n        with shared.bedtools_tmpdir(items[0]):\n            with file_transaction(items[0], out_file) as tx_out_file:\n                # Get a bedtool for the full region if no variant regions\n                want_bedtool = callable.get_ref_bedtool(tz.get_in([\"reference\", \"fasta\", \"base\"], items[0]),\n                                                        items[0][\"config\"], chrom)\n                want_bedtool = pybedtools.BedTool(shared.subset_variant_regions(want_bedtool.saveas().fn,\n                                                                                chrom, tx_out_file, items))\n                sv_exclude_bed = _get_sv_exclude_file(items)\n                if sv_exclude_bed and len(want_bedtool) > 0:\n                    want_bedtool = want_bedtool.subtract(sv_exclude_bed, nonamecheck=True).saveas()\n                full_bedtool = callable.get_ref_bedtool(tz.get_in([\"reference\", \"fasta\", \"base\"], items[0]),\n                                                        items[0][\"config\"])\n                if len(want_bedtool) > 0:\n                    full_bedtool.subtract(want_bedtool, nonamecheck=True).saveas(tx_out_file)\n                else:\n                    full_bedtool.saveas(tx_out_file)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexcludes calls based on overlap of the ends of exclusion regions.", "response": "def exclude_by_ends(in_file, exclude_file, data, in_params=None):\n    \"\"\"Exclude calls based on overlap of the ends with exclusion regions.\n\n    Removes structural variants with either end being in a repeat: a large\n    source of false positives.\n\n    Parameters tuned based on removal of LCR overlapping false positives in DREAM\n    synthetic 3 data.\n    \"\"\"\n    params = {\"end_buffer\": 50,\n              \"rpt_pct\": 0.9,\n              \"total_rpt_pct\": 0.2,\n              \"sv_pct\": 0.5}\n    if in_params:\n        params.update(in_params)\n    assert in_file.endswith(\".bed\")\n    out_file = \"%s-norepeats%s\" % utils.splitext_plus(in_file)\n    to_filter = collections.defaultdict(list)\n    removed = 0\n    if not utils.file_uptodate(out_file, in_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            with shared.bedtools_tmpdir(data):\n                for coord, end_name in [(1, \"end1\"), (2, \"end2\")]:\n                    base, ext = utils.splitext_plus(tx_out_file)\n                    end_file = _create_end_file(in_file, coord, params, \"%s-%s%s\" % (base, end_name, ext))\n                    to_filter = _find_to_filter(end_file, exclude_file, params, to_filter)\n            with open(tx_out_file, \"w\") as out_handle:\n                with open(in_file) as in_handle:\n                    for line in in_handle:\n                        key = \"%s:%s-%s\" % tuple(line.strip().split(\"\\t\")[:3])\n                        total_rpt_size = sum(to_filter.get(key, [0]))\n                        if total_rpt_size <= (params[\"total_rpt_pct\"] * params[\"end_buffer\"]):\n                            out_handle.write(line)\n                        else:\n                            removed += 1\n    return out_file, removed"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _find_to_filter(in_file, exclude_file, params, to_exclude):\n    for feat in pybedtools.BedTool(in_file).intersect(pybedtools.BedTool(exclude_file), wao=True, nonamecheck=True):\n        us_chrom, us_start, us_end, name, other_chrom, other_start, other_end, overlap = feat.fields\n        if float(overlap) > 0:\n            other_size = float(other_end) - float(other_start)\n            other_pct = float(overlap) / other_size\n            us_pct = float(overlap) / (float(us_end) - float(us_start))\n            if us_pct > params[\"sv_pct\"] or (other_pct > params[\"rpt_pct\"]):\n                to_exclude[name].append(float(overlap))\n    return to_exclude", "response": "Find regions that overlap the exclusion file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving chromosomes to process on avoiding extra skipped chromosomes.", "response": "def get_sv_chroms(items, exclude_file):\n    \"\"\"Retrieve chromosomes to process on, avoiding extra skipped chromosomes.\n    \"\"\"\n    exclude_regions = {}\n    for region in pybedtools.BedTool(exclude_file):\n        if int(region.start) == 0:\n            exclude_regions[region.chrom] = int(region.end)\n    out = []\n    with pysam.Samfile(dd.get_align_bam(items[0]) or dd.get_work_bam(items[0]))as pysam_work_bam:\n        for chrom, length in zip(pysam_work_bam.references, pysam_work_bam.lengths):\n            exclude_length = exclude_regions.get(chrom, 0)\n            if exclude_length < length:\n                out.append(chrom)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _extract_split_and_discordants(in_bam, work_dir, data):\n    sr_file = os.path.join(work_dir, \"%s-sr.bam\" % os.path.splitext(os.path.basename(in_bam))[0])\n    disc_file = os.path.join(work_dir, \"%s-disc.bam\" % os.path.splitext(os.path.basename(in_bam))[0])\n    if not utils.file_exists(sr_file) or not utils.file_exists(disc_file):\n        with file_transaction(data, sr_file) as tx_sr_file:\n            with file_transaction(data, disc_file) as tx_disc_file:\n                cores = dd.get_num_cores(data)\n                ref_file = dd.get_ref_file(data)\n                cmd = (\"extract-sv-reads -e --threads {cores} -T {ref_file} \"\n                       \"-i {in_bam} -s {tx_sr_file} -d {tx_disc_file}\")\n                do.run(cmd.format(**locals()), \"extract split and discordant reads\", data)\n    for fname in [sr_file, disc_file]:\n        bam.index(fname, data[\"config\"])\n    return sr_file, disc_file", "response": "Retrieve split - read alignments from input BAM file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding existing split reads and discordants.", "response": "def find_existing_split_discordants(data):\n    \"\"\"Check for pre-calculated split reads and discordants done as part of alignment streaming.\n    \"\"\"\n    in_bam = dd.get_align_bam(data)\n    sr_file = \"%s-sr.bam\" % os.path.splitext(in_bam)[0]\n    disc_file = \"%s-disc.bam\" % os.path.splitext(in_bam)[0]\n    if utils.file_exists(sr_file) and utils.file_exists(disc_file):\n        return sr_file, disc_file\n    else:\n        sr_file = dd.get_sr_bam(data)\n        disc_file = dd.get_disc_bam(data)\n        if sr_file and utils.file_exists(sr_file) and disc_file and utils.file_exists(disc_file):\n            return sr_file, disc_file\n        else:\n            return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving split and discordant reads potentially calculating with extract_sv_reads as needed.", "response": "def get_split_discordants(data, work_dir):\n    \"\"\"Retrieve split and discordant reads, potentially calculating with extract_sv_reads as needed.\n    \"\"\"\n    align_bam = dd.get_align_bam(data)\n    sr_bam, disc_bam = find_existing_split_discordants(data)\n    if not sr_bam:\n        work_dir = (work_dir if not os.access(os.path.dirname(align_bam), os.W_OK | os.X_OK)\n                    else os.path.dirname(align_bam))\n        sr_bam, disc_bam = _extract_split_and_discordants(align_bam, work_dir, data)\n    return sr_bam, disc_bam"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_cur_batch(items):\n    batches = []\n    for data in items:\n        batch = tz.get_in([\"metadata\", \"batch\"], data, [])\n        batches.append(set(batch) if isinstance(batch, (list, tuple)) else set([batch]))\n    combo_batches = reduce(lambda b1, b2: b1.intersection(b2), batches)\n    if len(combo_batches) == 1:\n        return combo_batches.pop()\n    elif len(combo_batches) == 0:\n        return None\n    else:\n        raise ValueError(\"Found multiple overlapping batches: %s -- %s\" % (combo_batches, batches))", "response": "Retrieve the name of the batch shared between all items in a group."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates mean median and MAD from distances avoiding outliers.", "response": "def insert_size_stats(dists):\n    \"\"\"Calcualtes mean/median and MAD from distances, avoiding outliers.\n\n    MAD is the Median Absolute Deviation: http://en.wikipedia.org/wiki/Median_absolute_deviation\n    \"\"\"\n    med = numpy.median(dists)\n    filter_dists = list(filter(lambda x: x < med + 10 * med, dists))\n    median = numpy.median(filter_dists)\n    return {\"mean\": float(numpy.mean(filter_dists)), \"std\": float(numpy.std(filter_dists)),\n            \"median\": float(median),\n            \"mad\": float(numpy.median([abs(x - median) for x in filter_dists]))}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calc_paired_insert_stats(in_bam, nsample=1000000):\n    dists = []\n    n = 0\n    with pysam.Samfile(in_bam, \"rb\") as in_pysam:\n        for read in in_pysam:\n            if read.is_proper_pair and read.is_read1:\n                n += 1\n                dists.append(abs(read.isize))\n                if n >= nsample:\n                    break\n    return insert_size_stats(dists)", "response": "Retrieve statistics for paired end read insert distances."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calc_paired_insert_stats_save(in_bam, stat_file, nsample=1000000):\n    if utils.file_exists(stat_file):\n        with open(stat_file) as in_handle:\n            return yaml.safe_load(in_handle)\n    else:\n        stats = calc_paired_insert_stats(in_bam, nsample)\n        with open(stat_file, \"w\") as out_handle:\n            yaml.safe_dump(stats, out_handle, default_flow_style=False, allow_unicode=False)\n        return stats", "response": "Calculate paired stats saving to a file for re - runs."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves references to all remote directories and files.", "response": "def _accumulate_remotes(synapse_parent_id, syn):\n    \"\"\"Retrieve references to all remote directories and files.\n    \"\"\"\n    remotes = {}\n    s_base_folder = syn.get(synapse_parent_id)\n    for (s_dirpath, s_dirpath_id), _, s_filenames in synapseutils.walk(syn, synapse_parent_id):\n        remotes[s_dirpath] = s_dirpath_id\n        if s_filenames:\n            for s_filename, s_filename_id in s_filenames:\n                remotes[os.path.join(s_dirpath, s_filename)] = s_filename_id\n    return s_base_folder, remotes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve the remote folder for files creating if necessary.", "response": "def _remote_folder(dirpath, remotes, syn):\n    \"\"\"Retrieve the remote folder for files, creating if necessary.\n    \"\"\"\n    if dirpath in remotes:\n        return remotes[dirpath], remotes\n    else:\n        parent_dir, cur_dir = os.path.split(dirpath)\n        parent_folder, remotes = _remote_folder(parent_dir, remotes, syn)\n        s_cur_dir = syn.store(synapseclient.Folder(cur_dir, parent=parent_folder))\n        remotes[dirpath] = s_cur_dir.id\n        return s_cur_dir.id, remotes"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetects copy number variations from batched set of samples using WHAM.", "response": "def run(items, background=None):\n    \"\"\"Detect copy number variations from batched set of samples using WHAM.\n    \"\"\"\n    if not background: background = []\n    background_bams = []\n    paired = vcfutils.get_paired_bams([x[\"align_bam\"] for x in items], items)\n    if paired:\n        inputs = [paired.tumor_data]\n        if paired.normal_bam:\n            background = [paired.normal_data]\n            background_bams = [paired.normal_bam]\n    else:\n        assert not background\n        inputs, background = shared.find_case_control(items)\n        background_bams = [x[\"align_bam\"] for x in background]\n    orig_vcf = _run_wham(inputs, background_bams)\n    out = []\n    for data in inputs:\n        if \"sv\" not in data:\n            data[\"sv\"] = []\n        final_vcf = shared.finalize_sv(orig_vcf, data, items)\n        data[\"sv\"].append({\"variantcaller\": \"wham\", \"vrn_file\": final_vcf})\n        out.append(data)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _run_wham(inputs, background_bams):\n    out_file = os.path.join(_sv_workdir(inputs[0]), \"%s-wham.vcf.gz\" % dd.get_sample_name(inputs[0]))\n    if not utils.file_exists(out_file):\n        with file_transaction(inputs[0], out_file) as tx_out_file:\n            cores = dd.get_cores(inputs[0])\n            ref_file = dd.get_ref_file(inputs[0])\n            include_chroms = \",\".join([c.name for c in ref.file_contigs(ref_file)\n                                       if chromhacks.is_autosomal_or_x(c.name)])\n            all_bams = \",\".join([x[\"align_bam\"] for x in inputs] + background_bams)\n            cmd = (\"whamg -x {cores} -a {ref_file} -f {all_bams} -c {include_chroms} \"\n                   \"| bgzip -c > {tx_out_file}\")\n            do.run(cmd.format(**locals()), \"WHAM SV caller: %s\" % \", \".join(dd.get_sample_name(d) for d in inputs))\n    return vcfutils.bgzip_and_index(out_file, inputs[0][\"config\"])", "response": "Run WHAM on a set of inputs and targets."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfiltering SV calls also present in background samples.", "response": "def filter_by_background(in_vcf, full_vcf, background, data):\n    \"\"\"Filter SV calls also present in background samples.\n\n    Skips filtering of inversions, which are not characterized differently\n    between cases and controls in test datasets.\n    \"\"\"\n    Filter = collections.namedtuple('Filter', ['id', 'desc'])\n    back_filter = Filter(id='InBackground',\n                         desc='Rejected due to presence in background sample')\n    out_file = \"%s-filter.vcf\" % utils.splitext_plus(in_vcf)[0]\n    if not utils.file_uptodate(out_file, in_vcf) and not utils.file_uptodate(out_file + \".vcf.gz\", in_vcf):\n        with file_transaction(data, out_file) as tx_out_file:\n            with open(tx_out_file, \"w\") as out_handle:\n                reader = vcf.VCFReader(filename=in_vcf)\n                reader.filters[\"InBackground\"] = back_filter\n                full_reader = vcf.VCFReader(filename=full_vcf)\n                writer = vcf.VCFWriter(out_handle, template=reader)\n                for out_rec, rec in zip(reader, full_reader):\n                    rec_type = rec.genotype(dd.get_sample_name(data)).gt_type\n                    if rec_type == 0 or any(rec_type == rec.genotype(dd.get_sample_name(x)).gt_type\n                                            for x in background):\n                        out_rec.add_filter(\"InBackground\")\n                    writer.write_record(out_rec)\n    return vcfutils.bgzip_and_index(out_file, data[\"config\"])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming variant calling on gVCF inputs in a specific genomic region.", "response": "def run_region(data, region, vrn_files, out_file):\n    \"\"\"Perform variant calling on gVCF inputs in a specific genomic region.\n    \"\"\"\n    broad_runner = broad.runner_from_config(data[\"config\"])\n    if broad_runner.gatk_type() == \"gatk4\":\n        genomics_db = _run_genomicsdb_import(vrn_files, region, out_file, data)\n        return _run_genotype_gvcfs_genomicsdb(genomics_db, region, out_file, data)\n    else:\n        vrn_files = _batch_gvcfs(data, region, vrn_files, dd.get_ref_file(data), out_file)\n        return _run_genotype_gvcfs_gatk3(data, region, vrn_files, dd.get_ref_file(data), out_file)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning GenomicsDB import on the input files.", "response": "def _run_genomicsdb_import(vrn_files, region, out_file, data):\n    \"\"\"Create a GenomicsDB reference for all the variation files: GATK4.\n\n    Not yet tested as scale, need to explore --batchSize to reduce memory\n    usage if needed.\n\n    Does not support transactional directories yet, since\n    GenomicsDB databases cannot be moved to new locations. We try to\n    identify half-finished databases and restart:\nhttps://gatkforums.broadinstitute.org/gatk/discussion/10061/using-genomicsdbimport-to-prepare-gvcfs-for-input-to-genotypegvcfs-in-gatk4\n\n    Known issue -- Genomics DB workspace path core dumps on longer paths:\n    (std::string::compare(char const*))\n    \"\"\"\n    out_dir = \"%s_genomicsdb\" % utils.splitext_plus(out_file)[0]\n    if not os.path.exists(out_dir) or _incomplete_genomicsdb(out_dir):\n        if os.path.exists(out_dir):\n            shutil.rmtree(out_dir)\n        with utils.chdir(os.path.dirname(out_file)):\n            with file_transaction(data, out_dir) as tx_out_dir:\n                broad_runner = broad.runner_from_config(data[\"config\"])\n                cores = dd.get_cores(data)\n                params = [\"-T\", \"GenomicsDBImport\",\n                          \"--reader-threads\", str(cores),\n                          \"--genomicsdb-workspace-path\", os.path.relpath(out_dir, os.getcwd()),\n                          \"-L\", bamprep.region_to_gatk(region)]\n                for vrn_file in vrn_files:\n                    vcfutils.bgzip_and_index(vrn_file, data[\"config\"])\n                    params += [\"--variant\", vrn_file]\n                memscale = {\"magnitude\": 0.9 * cores, \"direction\": \"increase\"} if cores > 1 else None\n                broad_runner.run_gatk(params, memscale=memscale)\n    return out_dir"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if a GenomicsDB output is incomplete and we should regenerate.", "response": "def _incomplete_genomicsdb(dbdir):\n    \"\"\"Check if a GenomicsDB output is incomplete and we should regenerate.\n\n    Works around current inability to move GenomicsDB outputs and support\n    transactional directories.\n    \"\"\"\n    for test_file in [\"callset.json\", \"vidmap.json\", \"genomicsdb_array/genomicsdb_meta.json\"]:\n        if not os.path.exists(os.path.join(dbdir, test_file)):\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning genotyping from a merged GenomicsDB file.", "response": "def _run_genotype_gvcfs_genomicsdb(genomics_db, region, out_file, data):\n    \"\"\"GenotypeGVCFs from a merged GenomicsDB input: GATK4.\n            ropts += [str(x) for x in resources.get(\"options\", [])]\n\n    No core scaling -- not yet supported in GATK4.\n    \"\"\"\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            broad_runner = broad.runner_from_config(data[\"config\"])\n            params = [\"-T\", \"GenotypeGVCFs\",\n                      \"--variant\", \"gendb://%s\" % genomics_db,\n                      \"-R\", dd.get_ref_file(data),\n                      \"--output\", tx_out_file,\n                      \"-L\", bamprep.region_to_gatk(region)]\n            params += [\"-ploidy\", str(ploidy.get_ploidy([data], region))]\n            # Avoid slow genotyping runtimes with improved quality score calculation in GATK4\n            # https://gatkforums.broadinstitute.org/gatk/discussion/11471/performance-troubleshooting-tips-for-genotypegvcfs/p1\n            params += [\"--use-new-qual-calculator\"]\n            resources = config_utils.get_resources(\"gatk\", data[\"config\"])\n            params += [str(x) for x in resources.get(\"options\", [])]\n            cores = dd.get_cores(data)\n            memscale = {\"magnitude\": 0.9 * cores, \"direction\": \"increase\"} if cores > 1 else None\n            broad_runner.run_gatk(params, memscale=memscale)\n    return vcfutils.bgzip_and_index(out_file, data[\"config\"])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _run_genotype_gvcfs_gatk3(data, region, vrn_files, ref_file, out_file):\n    if not utils.file_exists(out_file):\n        broad_runner = broad.runner_from_config(data[\"config\"])\n        with file_transaction(data, out_file) as tx_out_file:\n            assoc_files = tz.get_in((\"genome_resources\", \"variation\"), data, {})\n            if not assoc_files: assoc_files = {}\n            params = [\"-T\", \"GenotypeGVCFs\",\n                      \"-R\", ref_file, \"-o\", tx_out_file,\n                      \"-L\", bamprep.region_to_gatk(region),\n                      \"--max_alternate_alleles\", \"4\"]\n            for vrn_file in vrn_files:\n                params += [\"--variant\", vrn_file]\n            if assoc_files.get(\"dbsnp\"):\n                params += [\"--dbsnp\", assoc_files[\"dbsnp\"]]\n            broad_runner.new_resources(\"gatk-haplotype\")\n            cores = dd.get_cores(data)\n            if cores > 1:\n                # GATK performs poorly with memory usage when parallelizing\n                # with a large number of cores but makes use of extra memory,\n                # so we cap at 6 cores.\n                # See issue #1565 for discussion\n                # Recent GATK 3.x versions also have race conditions with multiple\n                # threads, so limit to 1 and keep memory available\n                # https://gatkforums.broadinstitute.org/wdl/discussion/8718/concurrentmodificationexception-in-gatk-3-7-genotypegvcfs\n                # params += [\"-nt\", str(min(6, cores))]\n                memscale = {\"magnitude\": 0.9 * cores, \"direction\": \"increase\"}\n            else:\n                memscale = None\n            broad_runner.run_gatk(params, memscale=memscale, parallel_gc=True)\n    return vcfutils.bgzip_and_index(out_file, data[\"config\"])", "response": "Run genotyping of gVCFs into final VCF files."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _batch_gvcfs(data, region, vrn_files, ref_file, out_file=None):\n    if out_file is None:\n        out_file = vrn_files[0]\n    # group to get below the maximum batch size, using 200 as the baseline\n    max_batch = int(dd.get_joint_group_size(data))\n    if len(vrn_files) > max_batch:\n        out = []\n        num_batches = int(math.ceil(float(len(vrn_files)) / max_batch))\n        for i, batch_vrn_files in enumerate(tz.partition_all(num_batches, vrn_files)):\n            base, ext = utils.splitext_plus(out_file)\n            batch_out_file = \"%s-b%s%s\" % (base, i, ext)\n            out.append(run_combine_gvcfs(batch_vrn_files, region, ref_file, batch_out_file, data))\n        return _batch_gvcfs(data, region, out, ref_file)\n    else:\n        return vrn_files", "response": "Perform batching of gVCF files if above recommended input count."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting parameters through resources and calculate optimal values based on read count.", "response": "def _get_preseq_params(data, preseq_cmd, read_count):\n    \"\"\" Get parameters through resources.\n        If \"step\" or \"extrap\" limit are not provided, then calculate optimal values based on read count.\n    \"\"\"\n    defaults = {\n        'seg_len': 100000,        # maximum segment length when merging paired end bam reads\n        'steps': 300,             # number of points on the plot\n        'extrap_fraction': 3,     # extrapolate up to X times read_count\n        'extrap': None,           # extrapolate up to X reads\n        'step': None,             # step size (number of reads between points on the plot)\n        'options': '',\n    }\n    params = {}\n\n    main_opts = [(\"-e\", \"-extrap\"), (\"-l\", \"-seg_len\"), (\"-s\", \"-step\")]\n    other_opts = config_utils.get_resources(\"preseq\", data[\"config\"]).get(\"options\", [])\n    if isinstance(other_opts, str):\n        other_opts = [other_opts]\n    for sht, lng in main_opts:\n        if sht in other_opts:\n            i = other_opts.index(sht)\n        elif lng in other_opts:\n            i = other_opts.index(lng)\n        else:\n            i = None\n        if i is not None:\n            params[lng[1:]] = other_opts[i + 1]\n            other_opts = other_opts[:i] + other_opts[i + 2:]\n    params['options'] = ' '.join(other_opts)\n    for k, v in config_utils.get_resources(\"preseq\", data[\"config\"]).items():\n        if k != 'options':\n            params[k] = v\n\n    params['steps'] = params.get('steps', defaults['steps'])\n\n    if preseq_cmd == 'c_curve':\n        params['extrap_fraction'] = 1\n\n    else:\n        if params.get('step') is None:\n            if params.get('extrap') is None:\n                unrounded__extrap = read_count * params.get('extrap_fraction', defaults['extrap_fraction'])\n                unrounded__step = unrounded__extrap // params['steps']\n                if params.get('extrap_fraction') is not None:  # extrap_fraction explicitly provided\n                    params['extrap'] = unrounded__extrap\n                    params['step'] = unrounded__step\n                else:\n                    power_of_10 = 10 ** math.floor(math.log(unrounded__step, 10))\n                    rounded__step = int(math.floor(unrounded__step // power_of_10) * power_of_10)\n                    rounded__extrap = int(rounded__step) * params['steps']\n                    params['step'] = rounded__step\n                    params['extrap'] = rounded__extrap\n            else:\n                params['step'] = params['extrap'] // params['steps']\n\n        elif params.get('extrap') is None:\n            params['extrap'] = params['step'] * params['steps']\n\n    params['step'] = params.get('step', defaults['step'])\n    params['extrap'] = params.get('extrap', defaults['extrap'])\n    params['seg_len'] = params.get('seg_len', defaults['seg_len'])\n\n    logger.info(\"Preseq: running {steps} steps of size {step}, extap limit {extrap}\".format(**params))\n    return params"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsplit somatic batches adding a germline target.", "response": "def split_somatic(items):\n    \"\"\"Split somatic batches, adding a germline target.\n\n    Enables separate germline calling of samples using shared alignments.\n    \"\"\"\n    items = [_clean_flat_variantcaller(x) for x in items]\n    somatic_groups, somatic, non_somatic = vcfutils.somatic_batches(items)\n    # extract germline samples to run from normals in tumor/normal pairs\n    germline_added = set([])\n    germline = []\n    for somatic_group in somatic_groups:\n        paired = vcfutils.get_paired(somatic_group)\n        if paired and paired.normal_data:\n            cur = utils.deepish_copy(paired.normal_data)\n            vc = dd.get_variantcaller(cur)\n            if isinstance(vc, dict) and \"germline\" in vc:\n                if cur[\"description\"] not in germline_added:\n                    germline_added.add(cur[\"description\"])\n                    cur[\"rgnames\"][\"sample\"] = cur[\"description\"]\n                    cur[\"metadata\"][\"batch\"] = \"%s-germline\" % cur[\"description\"]\n                    cur[\"metadata\"][\"phenotype\"] = \"germline\"\n                    cur = remove_align_qc_tools(cur)\n                    cur[\"config\"][\"algorithm\"][\"variantcaller\"] = vc[\"germline\"]\n                    germline.append(cur)\n    # Fix variantcalling specification for only somatic targets\n    somatic_out = []\n    for data in somatic:\n        vc = dd.get_variantcaller(data)\n        if isinstance(vc, dict) and \"somatic\" in vc:\n            data[\"config\"][\"algorithm\"][\"variantcaller\"] = vc[\"somatic\"]\n        somatic_out.append(data)\n    return non_somatic + somatic_out + germline"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _clean_flat_variantcaller(data):\n    vc = dd.get_variantcaller(data)\n    if isinstance(vc, (list, tuple)) and all([x.count(\":\") == 1 for x in vc]):\n        out = {}\n        for v in vc:\n            k, v = v.split(\":\")\n            if k in out:\n                out[k].append(v)\n            else:\n                out[k] = [v]\n        data = dd.set_variantcaller(data, out)\n    return data", "response": "Convert flattened CWL representation into a set of strings which we can reconstitute as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving alignment based QC tools we don t need for data replicates.", "response": "def remove_align_qc_tools(data):\n    \"\"\"Remove alignment based QC tools we don't need for data replicates.\n\n    When we do multiple variant calling on a sample file (somatic/germline),\n    avoid re-running QC.\n    \"\"\"\n    align_qc = set([\"qsignature\", \"coverage\", \"picard\", \"samtools\", \"fastqc\"])\n    data[\"config\"][\"algorithm\"][\"qc\"] = [t for t in dd.get_algorithm_qc(data)\n                                         if t not in align_qc]\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts germline calls for the given sample.", "response": "def extract(data, items, out_dir=None):\n    \"\"\"Extract germline calls for the given sample, if tumor only.\n    \"\"\"\n    if vcfutils.get_paired_phenotype(data):\n        if len(items) == 1:\n            germline_vcf = _remove_prioritization(data[\"vrn_file\"], data, out_dir)\n            germline_vcf = vcfutils.bgzip_and_index(germline_vcf, data[\"config\"])\n            data[\"vrn_file_plus\"] = {\"germline\": germline_vcf}\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef filter_to_pass_and_reject(in_file, paired, out_dir=None):\n    from bcbio.heterogeneity import bubbletree\n    out_file = \"%s-prfilter.vcf.gz\" % utils.splitext_plus(in_file)[0]\n    if out_dir:\n        out_file = os.path.join(out_dir, os.path.basename(out_file))\n    if not utils.file_uptodate(out_file, in_file):\n        with file_transaction(paired.tumor_data, out_file) as tx_out_file:\n            max_depth = bubbletree.max_normal_germline_depth(in_file, bubbletree.PARAMS, paired)\n            tx_out_plain = tx_out_file.replace(\".vcf.gz\", \".vcf\")\n            with contextlib.closing(cyvcf2.VCF(in_file)) as reader:\n                reader = _add_db_to_header(reader)\n                with contextlib.closing(cyvcf2.Writer(tx_out_plain, reader)) as writer:\n                    for rec in reader:\n                        filters = rec.FILTER.split(\";\") if rec.FILTER else []\n                        other_filters = [x for x in filters if x not in [\"PASS\", \".\", \"REJECT\"]]\n                        if len(other_filters) == 0 or bubbletree.is_info_germline(rec):\n                            # Germline, check if we should include based on frequencies\n                            if \"REJECT\" in filters or bubbletree.is_info_germline(rec):\n                                stats = bubbletree._is_possible_loh(rec, reader, bubbletree.PARAMS, paired,\n                                                                    use_status=True, max_normal_depth=max_depth)\n                                if stats:\n                                    rec.FILTER = \"PASS\"\n                                    rec.INFO[\"DB\"] = True\n                                    writer.write_record(rec)\n                            # Somatic, always include\n                            else:\n                                writer.write_record(rec)\n            vcfutils.bgzip_and_index(tx_out_plain, paired.tumor_data[\"config\"])\n    return out_file", "response": "Filter VCF to only those with a strict PASS or REJECT."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreplacing germline sample names in a BAM file.", "response": "def fix_germline_samplename(in_file, sample_name, data):\n    \"\"\"Replace germline sample names, originally from normal BAM file.\n    \"\"\"\n    out_file = \"%s-fixnames%s\" % utils.splitext_plus(in_file)\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            sample_file = \"%s-samples.txt\" % utils.splitext_plus(tx_out_file)[0]\n            with open(sample_file, \"w\") as out_handle:\n                out_handle.write(\"%s\\n\" % sample_name)\n            cmd = (\"bcftools reheader -s {sample_file} {in_file} -o {tx_out_file}\")\n            do.run(cmd.format(**locals()), \"Fix germline samplename: %s\" % sample_name)\n    return vcfutils.bgzip_and_index(out_file, data[\"config\"])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove tumor - only prioritization and return non - filtered calls.", "response": "def _remove_prioritization(in_file, data, out_dir=None):\n    \"\"\"Remove tumor-only prioritization and return non-filtered calls.\n    \"\"\"\n    out_file = \"%s-germline.vcf\" % utils.splitext_plus(in_file)[0]\n    if out_dir:\n        out_file = os.path.join(out_dir, os.path.basename(out_file))\n    if not utils.file_uptodate(out_file, in_file) and not utils.file_uptodate(out_file + \".gz\", in_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            reader = cyvcf2.VCF(str(in_file))\n            reader.add_filter_to_header({'ID': 'Somatic', 'Description': 'Variant called as Somatic'})\n            # with open(tx_out_file, \"w\") as out_handle:\n            #     out_handle.write(reader.raw_header)\n            with contextlib.closing(cyvcf2.Writer(tx_out_file, reader)) as writer:\n                for rec in reader:\n                    rec = _update_prioritization_filters(rec)\n                    # out_handle.write(str(rec))\n                    writer.write_record(rec)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting germline calls from non - somatic calls.", "response": "def _extract_germline(in_file, data):\n    \"\"\"Extract germline calls non-somatic, non-filtered calls.\n    \"\"\"\n    out_file = \"%s-germline.vcf\" % utils.splitext_plus(in_file)[0]\n    if not utils.file_uptodate(out_file, in_file) and not utils.file_uptodate(out_file + \".gz\", in_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            reader = cyvcf2.VCF(str(in_file))\n            reader.add_filter_to_header({'ID': 'Somatic', 'Description': 'Variant called as Somatic'})\n            #with contextlib.closing(cyvcf2.Writer(tx_out_file, reader)) as writer:\n            with open(tx_out_file, \"w\") as out_handle:\n                out_handle.write(reader.raw_header)\n                for rec in reader:\n                    rec = _update_germline_filters(rec)\n                    out_handle.write(str(rec))\n                    #writer.write_record(rec)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _remove_germline_filter(rec, name):\n    if _is_germline(rec):\n        if rec.FILTER and name in rec.FILTER:\n            return vcfutils.cyvcf_remove_filter(rec, name)\n    elif not _is_somatic(rec):\n        if rec.FILTER and name in rec.FILTER:\n            return vcfutils.cyvcf_remove_filter(rec, name)\n    return rec", "response": "Remove germline filter from a single record."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _is_somatic(rec):\n    if _has_somatic_flag(rec):\n        return True\n    if _is_mutect2_somatic(rec):\n        return True\n    ss_flag = rec.INFO.get(\"SS\")\n    if ss_flag is not None:\n        if str(ss_flag) == \"2\":\n            return True\n    status_flag = rec.INFO.get(\"STATUS\")\n    if status_flag is not None:\n        if str(status_flag).lower() in [\"somatic\", \"likelysomatic\", \"strongsomatic\", \"samplespecific\"]:\n            return True\n    epr = rec.INFO.get(\"EPR\", \"\").split(\",\")\n    if epr and all([p == \"pass\" for p in epr]):\n        return True\n    return False", "response": "Handle somatic classifications from MuTect MuTect2 VarDict and VarScan\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if the given MuTect record is Germline.", "response": "def _is_germline(rec):\n    \"\"\"Handle somatic INFO classifications from MuTect, MuTect2, VarDict, VarScan and Octopus.\n    \"\"\"\n    if _has_somatic_flag(rec):\n        return False\n    if _is_mutect2_somatic(rec):\n        return False\n    ss_flag = rec.INFO.get(\"SS\")\n    if ss_flag is not None:\n        if str(ss_flag) == \"1\":\n            return True\n    # Octopus, assessed for potentially being Germline and not flagged SOMATIC\n    # https://github.com/luntergroup/octopus/wiki/Calling-models:-Cancer#qual-vs-pp\n    pp = rec.INFO.get(\"PP\")\n    if pp and float(pp) / float(rec.QUAL) >= 0.5:\n        return True\n    status_flag = rec.INFO.get(\"STATUS\")\n    if status_flag is not None:\n        if str(status_flag).lower() in [\"germline\", \"likelyloh\", \"strongloh\", \"afdiff\", \"deletion\"]:\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve GNU coreutils sort command.", "response": "def get_sort_cmd(tmp_dir=None):\n    \"\"\"Retrieve GNU coreutils sort command, using version-sort if available.\n\n    Recent versions of sort have alpha-numeric sorting, which provides\n    more natural sorting of chromosomes (chr1, chr2) instead of (chr1, chr10).\n    This also fixes versions of sort, like 8.22 in CentOS 7.1, that have broken\n    sorting without version sorting specified.\n\n    https://github.com/bcbio/bcbio-nextgen/issues/624\n    https://github.com/bcbio/bcbio-nextgen/issues/1017\n    \"\"\"\n    has_versionsort = subprocess.check_output(\"sort --help | grep version-sort; exit 0\", shell=True).strip()\n    if has_versionsort:\n        cmd = \"sort -V\"\n    else:\n        cmd = \"sort\"\n    if tmp_dir and os.path.exists(tmp_dir) and os.path.isdir(tmp_dir):\n        cmd += \" -T %s\" % tmp_dir\n    return cmd"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_bed_contigs(in_file, data):\n    if not dd.get_ref_file(data):\n        return\n    contigs = set([])\n    with utils.open_gzipsafe(in_file) as in_handle:\n        for line in in_handle:\n            if not line.startswith((\"#\", \"track\", \"browser\", \"@\")) and line.strip():\n                contigs.add(line.split()[0])\n    ref_contigs = set([x.name for x in ref.file_contigs(dd.get_ref_file(data))])\n    if contigs and len(contigs - ref_contigs) / float(len(contigs)) > 0.25:\n        raise ValueError(\"Contigs in BED file %s not in reference genome:\\n %s\\n\"\n                         % (in_file, list(contigs - ref_contigs)) +\n                         \"This is typically due to chr1 versus 1 differences in BED file and reference.\")", "response": "Ensure BED file contigs match reference genome."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking BED file coordinates match reference genome.", "response": "def check_bed_coords(in_file, data):\n    \"\"\"Ensure BED file coordinates match reference genome.\n\n    Catches errors like using a hg38 BED file for an hg19 genome run.\n    \"\"\"\n    if dd.get_ref_file(data):\n        contig_sizes = {}\n        for contig in ref.file_contigs(dd.get_ref_file(data)):\n            contig_sizes[contig.name] = contig.size\n        with utils.open_gzipsafe(in_file) as in_handle:\n            for line in in_handle:\n                if not line.startswith((\"#\", \"track\", \"browser\", \"@\")) and line.strip():\n                    parts = line.split()\n                    if len(parts) > 3:\n                        try:\n                            end = int(parts[2])\n                        except ValueError:\n                            continue\n                        contig = parts[0]\n                        check_size = contig_sizes.get(contig)\n                        if check_size and end > check_size:\n                            raise ValueError(\"Found BED coordinate off the end of the chromosome:\\n%s%s\\n\"\n                                             \"Is the input BED from the right genome build?\" %\n                                             (line, in_file))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clean_file(in_file, data, prefix=\"\", bedprep_dir=None, simple=None):\n    # Remove non-ascii characters. Used in coverage analysis, to support JSON code in one column\n    #   and be happy with sambamba:\n    simple = \"iconv -c -f utf-8 -t ascii | sed 's/ //g' |\" if simple else \"\"\n    if in_file:\n        if not bedprep_dir:\n            bedprep_dir = utils.safe_makedir(os.path.join(data[\"dirs\"][\"work\"], \"bedprep\"))\n        # Avoid running multiple times with same prefix\n        if prefix and os.path.basename(in_file).startswith(prefix):\n            return in_file\n        out_file = os.path.join(bedprep_dir, \"%s%s\" % (prefix, os.path.basename(in_file)))\n        out_file = out_file.replace(\".interval_list\", \".bed\")\n        if out_file.endswith(\".gz\"):\n            out_file = out_file[:-3]\n        if not utils.file_uptodate(out_file, in_file):\n            check_bed_contigs(in_file, data)\n            check_bed_coords(in_file, data)\n            with file_transaction(data, out_file) as tx_out_file:\n                bcbio_py = sys.executable\n                cat_cmd = \"zcat\" if in_file.endswith(\".gz\") else \"cat\"\n                sort_cmd = get_sort_cmd(os.path.dirname(tx_out_file))\n                cmd = (\"{cat_cmd} {in_file} | grep -v ^track | grep -v ^browser | grep -v ^@ | \"\n                       \"grep -v ^# | {simple} \"\n                       \"{bcbio_py} -c 'from bcbio.variation import bedutils; bedutils.remove_bad()' | \"\n                       \"{sort_cmd} -k1,1 -k2,2n > {tx_out_file}\")\n                do.run(cmd.format(**locals()), \"Prepare cleaned BED file\", data)\n        vcfutils.bgzip_and_index(out_file, data.get(\"config\", {}), remove_orig=False)\n        return out_file", "response": "Clean a BED file with headers from one column."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sort_merge(in_file, data, out_dir=None):\n    out_file = \"%s-sortmerge.bed\" % os.path.splitext(in_file)[0]\n    bedtools = config_utils.get_program(\"bedtools\", data, default=\"bedtools\")\n    if out_dir:\n        out_file = os.path.join(out_dir, os.path.basename(out_file))\n    if not utils.file_uptodate(out_file, in_file):\n        column_opt = \"\"\n        with utils.open_gzipsafe(in_file) as in_handle:\n            for line in in_handle:\n                if not line.startswith((\"#\", \"track\", \"browser\", \"@\")):\n                    parts = line.split()\n                    if len(parts) >= 4:\n                        column_opt = \"-c 4 -o distinct\"\n        with file_transaction(data, out_file) as tx_out_file:\n            cat_cmd = \"zcat\" if in_file.endswith(\".gz\") else \"cat\"\n            sort_cmd = get_sort_cmd(os.path.dirname(tx_out_file))\n            cmd = (\"{cat_cmd} {in_file} | {sort_cmd} -k1,1 -k2,2n | \"\n                   \"{bedtools} merge -i - {column_opt} > {tx_out_file}\")\n            do.run(cmd.format(**locals()), \"Sort and merge BED file\", data)\n    return out_file", "response": "Sort and merge a BED file collapsing gene names."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_bad():\n    for line in sys.stdin:\n        parts = line.strip().split(\"\\t\")\n        if len(parts) == 1 and len(line.strip().split()) > 1:\n            parts = line.strip().split()\n        if line.strip() and len(parts) > 2 and int(parts[2]) > int(parts[1]):\n            sys.stdout.write(\"\\t\".join(parts) + \"\\n\")", "response": "Remove non - increasing BED lines which cause variant callers to choke."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef merge_overlaps(in_file, data, distance=None, out_dir=None):\n    config = data[\"config\"]\n    if in_file:\n        bedtools = config_utils.get_program(\"bedtools\", config,\n                                            default=\"bedtools\")\n        work_dir = tz.get_in([\"dirs\", \"work\"], data)\n        if out_dir:\n            bedprep_dir = out_dir\n        elif work_dir:\n            bedprep_dir = utils.safe_makedir(os.path.join(work_dir, \"bedprep\"))\n        else:\n            bedprep_dir = os.path.dirname(in_file)\n        out_file = os.path.join(bedprep_dir, \"%s-merged.bed\" % (utils.splitext_plus(os.path.basename(in_file))[0]))\n        if not utils.file_uptodate(out_file, in_file):\n            with file_transaction(data, out_file) as tx_out_file:\n                distance = \"-d %s\" % distance if distance else \"\"\n                cmd = \"{bedtools} merge {distance} -i {in_file} > {tx_out_file}\"\n                do.run(cmd.format(**locals()), \"Prepare merged BED file\", data)\n        vcfutils.bgzip_and_index(out_file, data[\"config\"], remove_orig=False)\n        return out_file", "response": "Merge overlapping regions into a single BED file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the variant region BED file from a population of items.", "response": "def population_variant_regions(items, merged=False):\n    \"\"\"Retrieve the variant region BED file from a population of items.\n\n    If tumor/normal, return the tumor BED file. If a population, return\n    the BED file covering the most bases.\n    \"\"\"\n    def _get_variant_regions(data):\n        out = dd.get_variant_regions(data) or dd.get_sample_callable(data)\n        # Only need to merge for variant region inputs, not callable BED regions which don't overlap\n        if merged and dd.get_variant_regions(data):\n            merged_out = dd.get_variant_regions_merged(data)\n            if merged_out:\n                out = merged_out\n            else:\n                out = merge_overlaps(out, data)\n        return out\n    import pybedtools\n    if len(items) == 1:\n        return _get_variant_regions(items[0])\n    else:\n        paired = vcfutils.get_paired(items)\n        if paired:\n            return _get_variant_regions(paired.tumor_data)\n        else:\n            vrs = []\n            for data in items:\n                vr_bed = _get_variant_regions(data)\n                if vr_bed:\n                    vrs.append((pybedtools.BedTool(vr_bed).total_coverage(), vr_bed))\n            vrs.sort(reverse=True)\n            if vrs:\n                return vrs[0][1]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncombine multiple BED files into a single output.", "response": "def combine(in_files, out_file, config):\n    \"\"\"Combine multiple BED files into a single output.\n    \"\"\"\n    if not utils.file_exists(out_file):\n        with file_transaction(config, out_file) as tx_out_file:\n            with open(tx_out_file, \"w\") as out_handle:\n                for in_file in in_files:\n                    with open(in_file) as in_handle:\n                        shutil.copyfileobj(in_handle, out_handle)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nintersecting two BED files.", "response": "def intersect_two(f1, f2, work_dir, data):\n    \"\"\"Intersect two regions, handling cases where either file is not present.\n    \"\"\"\n    bedtools = config_utils.get_program(\"bedtools\", data, default=\"bedtools\")\n    f1_exists = f1 and utils.file_exists(f1)\n    f2_exists = f2 and utils.file_exists(f2)\n    if not f1_exists and not f2_exists:\n        return None\n    elif f1_exists and not f2_exists:\n        return f1\n    elif f2_exists and not f1_exists:\n        return f2\n    else:\n        out_file = os.path.join(work_dir, \"%s-merged.bed\" % (utils.splitext_plus(os.path.basename(f1))[0]))\n        if not utils.file_exists(out_file):\n            with file_transaction(data, out_file) as tx_out_file:\n                cmd = \"{bedtools} intersect -a {f1} -b {f2} > {tx_out_file}\"\n                do.run(cmd.format(**locals()), \"Intersect BED files\", data)\n        return out_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef subset_to_genome(in_file, out_file, data):\n    if not utils.file_uptodate(out_file, in_file):\n        contigs = set([x.name for x in ref.file_contigs(dd.get_ref_file(data))])\n        with utils.open_gzipsafe(in_file) as in_handle:\n            with file_transaction(data, out_file) as tx_out_file:\n                with open(tx_out_file, \"w\") as out_handle:\n                    for line in in_handle:\n                        parts = line.split()\n                        if parts and parts[0] in contigs:\n                            out_handle.write(line)\n    return out_file", "response": "Subset a BED file to only contain contigs present in the reference genome."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(align_bams, items, ref_file, assoc_files, region, out_file):\n    if not utils.file_exists(out_file):\n        paired = vcfutils.get_paired_bams(align_bams, items)\n        vrs = bedutils.population_variant_regions(items)\n        target = shared.subset_variant_regions(vrs, region,\n                                               out_file, items=items, do_merge=True)\n        if paired:\n            return _run_somatic(paired, ref_file, target, out_file)\n        else:\n            return _run_germline(align_bams, items, ref_file, target, out_file)\n    return out_file", "response": "Run octopus variant calling handling both somatic and germline calling."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _produce_compatible_vcf(out_file, data, is_somatic):\n    base, ext = utils.splitext_plus(out_file)\n    legacy_file = \"%s.legacy%s\" % (base, ext)\n    if is_somatic:\n        legacy_file = _covert_to_diploid(legacy_file, data)\n    final_file = \"%s.vcf.gz\" % base\n    cat_cmd = \"zcat\" if legacy_file.endswith(\".gz\") else \"cat\"\n    contig_cl = vcfutils.add_contig_to_header_cl(dd.get_ref_file(data), out_file)\n    remove_problem_alleles = r\"sed 's/,\\*\\([A-Z]\\)/,\\1/'\"\n    cmd = (\"{cat_cmd} {legacy_file} | sed 's/fileformat=VCFv4.3/fileformat=VCFv4.2/' | \"\n           \"{remove_problem_alleles} | {contig_cl} | bgzip -c > {final_file}\")\n    do.run(cmd.format(**locals()), \"Produce compatible VCF output file from octopus\")\n    return vcfutils.bgzip_and_index(out_file, data[\"config\"])", "response": "Create a compatible VCF file from octopus."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts non - diploid somatic outputs into diploid.", "response": "def _covert_to_diploid(in_file, data):\n    \"\"\"Converts non-diploid somatic outputs into diploid.\n\n    https://github.com/luntergroup/octopus/wiki/Case-study:-Tumour-only-UMI#evaluate-variant-calls\n    \"\"\"\n    sample = dd.get_sample_name(data)\n    out_file = \"%s-diploid.vcf\" % utils.splitext_plus(in_file)[0]\n    in_vcf = pysam.VariantFile(in_file)\n    out_vcf = pysam.VariantFile(out_file, 'w', header=in_vcf.header)\n    for record in in_vcf:\n        gt = list(record.samples[sample]['GT'])\n        if 'SOMATIC' in record.info:\n            for allele in set(gt):\n                if allele != gt[0]:\n                    record.samples[sample]['GT'] = gt[0], allele\n                    out_vcf.write(record)\n        else:\n            if len(gt) == 1:\n                record.samples[sample]['GT'] = gt\n            else:\n                record.samples[sample]['GT'] = gt[0], gt[1]\n            out_vcf.write(record)\n    in_vcf.close()\n    out_vcf.close()\n    return vcfutils.bgzip_and_index(out_file, data[\"config\"])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _run_germline(align_bams, items, ref_file, target, out_file):\n    align_bams = \" \".join(align_bams)\n    cores = dd.get_num_cores(items[0])\n    cmd = (\"octopus --threads {cores} --reference {ref_file} --reads {align_bams} \"\n           \"--regions-file {target} \"\n           \"--working-directory {tmp_dir} \"\n           \"-o {tx_out_file} --legacy\")\n    with file_transaction(items[0], out_file) as tx_out_file:\n        tmp_dir = os.path.dirname(tx_out_file)\n        do.run(cmd.format(**locals()), \"Octopus germline calling\")\n        _produce_compatible_vcf(tx_out_file, items[0])\n    return out_file", "response": "Run octopus germline calling handling populations."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _run_somatic(paired, ref_file, target, out_file):\n    align_bams = paired.tumor_bam\n    if paired.normal_bam:\n        align_bams += \" %s --normal-sample %s\" % (paired.normal_bam, paired.normal_name)\n    cores = dd.get_num_cores(paired.tumor_data)\n    # Do not try to search below 0.4% currently as leads to long runtimes\n    # https://github.com/luntergroup/octopus/issues/29#issuecomment-428167979\n    min_af = max([float(dd.get_min_allele_fraction(paired.tumor_data)) / 100.0, 0.004])\n    min_af_floor = min_af / 4.0\n    cmd = (\"octopus --threads {cores} --reference {ref_file} --reads {align_bams} \"\n           \"--regions-file {target} \"\n           \"--min-credible-somatic-frequency {min_af_floor} --min-expected-somatic-frequency {min_af} \"\n           \"--downsample-above 4000 --downsample-target 4000 --min-kmer-prune 5 --min-bubble-score 20 \"\n           \"--max-haplotypes 200 --somatic-snv-mutation-rate '5e-4' --somatic-indel-mutation-rate '1e-05' \"\n           \"--target-working-memory 5G --target-read-buffer-footprint 5G --max-somatic-haplotypes 3 \"\n           \"--caller cancer \"\n           \"--working-directory {tmp_dir} \"\n           \"-o {tx_out_file} --legacy\")\n    if not paired.normal_bam:\n        cmd += (\" --tumour-germline-concentration 5\")\n    if dd.get_umi_type(paired.tumor_data) or _is_umi_consensus_bam(paired.tumor_bam):\n        cmd += (\" --allow-octopus-duplicates --overlap-masking 0 \"\n                \"--somatic-filter-expression 'GQ < 200 | MQ < 30 | SB > 0.2 | SD[.25] > 0.1 \"\n                \"| BQ < 40 | DP < 100 | MF > 0.1 | AD < 5 | CC > 1.1 | GQD > 2'\")\n    with file_transaction(paired.tumor_data, out_file) as tx_out_file:\n        tmp_dir = os.path.dirname(tx_out_file)\n        do.run(cmd.format(**locals()), \"Octopus somatic calling\")\n        _produce_compatible_vcf(tx_out_file, paired.tumor_data, is_somatic=True)\n    return out_file", "response": "Run somatic calling with octopus."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _is_umi_consensus_bam(in_file):\n    cmd = \"samtools view -h %s | head -500000 | samtools view -c -f 1024\"\n    count = subprocess.check_output(cmd % in_file, shell=True)\n    return int(count) == 0", "response": "Check if input BAM file generated by fgbio consensus calls on UMIs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the file to an Amazon S3 bucket using server side encryption.", "response": "def update_file(finfo, sample_info, config):\n    \"\"\"Update the file to an Amazon S3 bucket, using server side encryption.\n    \"\"\"\n    ffinal = filesystem.update_file(finfo, sample_info, config, pass_uptodate=True)\n    if os.path.isdir(ffinal):\n        to_transfer = []\n        for path, dirs, files in os.walk(ffinal):\n            for f in files:\n                full_f = os.path.join(path, f)\n                k = full_f.replace(os.path.abspath(config[\"dir\"]) + \"/\", \"\")\n                to_transfer.append((full_f, k))\n    else:\n        k = ffinal.replace(os.path.abspath(config[\"dir\"]) + \"/\", \"\")\n        to_transfer = [(ffinal, k)]\n\n    region = \"@%s\" % config[\"region\"] if config.get(\"region\") else \"\"\n    fname = \"s3://%s%s/%s\" % (config[\"bucket\"], region, to_transfer[0][1])\n    conn = objectstore.connect(fname)\n    bucket = conn.lookup(config[\"bucket\"])\n    if not bucket:\n        bucket = conn.create_bucket(config[\"bucket\"], location=config.get(\"region\", \"us-east-1\"))\n\n    for fname, orig_keyname in to_transfer:\n        keyname = os.path.join(config.get(\"folder\", \"\"), orig_keyname)\n        key = bucket.get_key(keyname) if bucket else None\n        modified = datetime.datetime.fromtimestamp(email.utils.mktime_tz(\n            email.utils.parsedate_tz(key.last_modified))) if key else None\n        no_upload = key and modified >= finfo[\"mtime\"]\n        if not no_upload:\n            _upload_file_aws_cli(fname, config[\"bucket\"], keyname, config, finfo)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _upload_file_aws_cli(local_fname, bucket, keyname, config=None, mditems=None):\n    s3_fname = \"s3://%s/%s\" % (bucket, keyname)\n    args = [\"--sse\", \"--expected-size\", str(os.path.getsize(local_fname))]\n    if config:\n        if config.get(\"region\"):\n            args += [\"--region\", config.get(\"region\")]\n        if config.get(\"reduced_redundancy\"):\n            args += [\"--storage-class\", \"REDUCED_REDUNDANCY\"]\n    cmd = [os.path.join(os.path.dirname(sys.executable), \"aws\"), \"s3\", \"cp\"] + args + \\\n          [local_fname, s3_fname]\n    do.run(cmd, \"Upload to s3: %s %s\" % (bucket, keyname))", "response": "Streaming upload via the standard AWS command line interface."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef upload_file_boto(fname, remote_fname, mditems=None):\n    r_fname = objectstore.parse_remote(remote_fname)\n    conn = objectstore.connect(remote_fname)\n    bucket = conn.lookup(r_fname.bucket)\n    if not bucket:\n        bucket = conn.create_bucket(r_fname.bucket, location=objectstore.get_region(remote_fname))\n    key = bucket.get_key(r_fname.key, validate=False)\n    if mditems is None:\n        mditems = {}\n    if \"x-amz-server-side-encryption\" not in mditems:\n        mditems[\"x-amz-server-side-encryption\"] = \"AES256\"\n    for name, val in mditems.items():\n        key.set_metadata(name, val)\n    key.set_contents_from_filename(fname, encrypt_key=True)", "response": "Upload a file using boto instead of external tools."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(bam_file, sample, out_dir):\n    out = {}\n    # if \"rchipqc\" in dd.get_tools_on(sample):\n    #    out = chipqc(bam_file, sample, out_dir)\n\n    peaks = sample.get(\"peaks_files\", {}).get(\"main\")\n    if peaks:\n        out.update(_reads_in_peaks(bam_file, peaks, sample))\n    return out", "response": "Run chipseq metrics for chipseq"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _reads_in_peaks(bam_file, peaks_file, sample):\n    if not peaks_file:\n        return {}\n    rip = number_of_mapped_reads(sample, bam_file, bed_file = peaks_file)\n    return {\"metrics\": {\"RiP\": rip}}", "response": "Calculate number of reads in peaks"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef chipqc(bam_file, sample, out_dir):\n    sample_name = dd.get_sample_name(sample)\n    logger.warning(\"ChIPQC is unstable right now, if it breaks, turn off the tool.\")\n    if utils.file_exists(out_dir):\n        return _get_output(out_dir)\n    with tx_tmpdir() as tmp_dir:\n        rcode = _sample_template(sample, tmp_dir)\n        if rcode:\n            # local_sitelib = utils.R_sitelib()\n            rscript = utils.Rscript_cmd()\n            do.run([rscript, \"--no-environ\", rcode], \"ChIPQC in %s\" % sample_name, log_error=False)\n            shutil.move(tmp_dir, out_dir)\n    return _get_output(out_dir)", "response": "Attempt code to run ChIPQC in one sample"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _sample_template(sample, out_dir):\n    bam_fn = dd.get_work_bam(sample)\n    genome = dd.get_genome_build(sample)\n    if genome in supported:\n        peaks = sample.get(\"peaks_files\", []).get(\"main\")\n        if peaks:\n            r_code = (\"library(ChIPQC);\\n\"\n                      \"sample = ChIPQCsample(\\\"{bam_fn}\\\",\"\n                      \"\\\"{peaks}\\\", \"\n                      \"annotation = \\\"{genome}\\\",\"\n                      \");\\n\"\n                      \"ChIPQCreport(sample);\\n\")\n            r_code_fn = os.path.join(out_dir, \"chipqc.r\")\n            with open(r_code_fn, 'w') as inh:\n                inh.write(r_code.format(**locals()))\n            return r_code_fn", "response": "Generate R code to get QC for one sample"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncount reads in a single sequence using featureCounts .", "response": "def count(data):\n    \"\"\"\n    count reads mapping to genes using featureCounts\n    http://subread.sourceforge.net\n    \"\"\"\n    in_bam = dd.get_work_bam(data) or dd.get_align_bam(data)\n    out_dir = os.path.join(dd.get_work_dir(data), \"align\", dd.get_sample_name(data))\n    if dd.get_aligner(data) == \"star\":\n        out_dir = os.path.join(out_dir, \"%s_%s\" % (dd.get_sample_name(data), dd.get_aligner(data)))\n    sorted_bam = bam.sort(in_bam, dd.get_config(data), order=\"queryname\", out_dir=safe_makedir(out_dir))\n    gtf_file = dd.get_gtf_file(data)\n    work_dir = dd.get_work_dir(data)\n    out_dir = os.path.join(work_dir, \"htseq-count\")\n    safe_makedir(out_dir)\n    count_file = os.path.join(out_dir, dd.get_sample_name(data)) + \".counts\"\n    summary_file = os.path.join(out_dir, dd.get_sample_name(data)) + \".counts.summary\"\n    if file_exists(count_file):\n        return count_file\n\n    featureCounts = config_utils.get_program(\"featureCounts\", dd.get_config(data))\n    paired_flag = _paired_flag(in_bam)\n    strand_flag = _strand_flag(data)\n\n    filtered_bam = bam.filter_primary(sorted_bam, data)\n\n    cmd = (\"{featureCounts} -a {gtf_file} -o {tx_count_file} -s {strand_flag} \"\n           \"{paired_flag} {filtered_bam}\")\n\n    message = (\"Count reads in {tx_count_file} mapping to {gtf_file} using \"\n               \"featureCounts\")\n    with file_transaction(data, [count_file, summary_file]) as tx_files:\n        tx_count_file, tx_summary_file = tx_files\n        do.run(cmd.format(**locals()), message.format(**locals()))\n    fixed_count_file = _format_count_file(count_file, data)\n    fixed_summary_file = _change_sample_name(\n        summary_file, dd.get_sample_name(data), data=data)\n    shutil.move(fixed_count_file, count_file)\n    shutil.move(fixed_summary_file, summary_file)\n\n    return count_file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchanging sample name in feature counts log file to get the same as the sample name in multiqc report.", "response": "def _change_sample_name(in_file, sample_name, data=None):\n    \"\"\"Fix name in feature counts log file to get the same\n       name in multiqc report.\n    \"\"\"\n    out_file = append_stem(in_file, \"_fixed\")\n    with file_transaction(data, out_file) as tx_out:\n        with open(tx_out, \"w\") as out_handle:\n            with open(in_file) as in_handle:\n                for line in in_handle:\n                    if line.startswith(\"Status\"):\n                        line = \"Status\\t%s.bam\" % sample_name\n                    out_handle.write(\"%s\\n\" % line.strip())\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _format_count_file(count_file, data):\n    COUNT_COLUMN = 5\n    out_file = os.path.splitext(count_file)[0] + \".fixed.counts\"\n    if file_exists(out_file):\n        return out_file\n\n    df = pd.io.parsers.read_table(count_file, sep=\"\\t\", index_col=0, header=1)\n    df_sub = df.ix[:, COUNT_COLUMN]\n    with file_transaction(data, out_file) as tx_out_file:\n        df_sub.to_csv(tx_out_file, sep=\"\\t\", index_label=\"id\", header=False)\n    return out_file", "response": "This cuts the count file produced from featureCounts down to\n    a two column file of gene ids and number of reads mapping to\n    each geneID"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _strand_flag(data):\n    strand_flag = {\"unstranded\": \"0\",\n                   \"firststrand\": \"2\",\n                   \"secondstrand\": \"1\"}\n    stranded = dd.get_strandedness(data)\n\n    assert stranded in strand_flag, (\"%s is not a valid strandedness value. \"\n                                     \"Valid values are 'firststrand', 'secondstrand', \"\n                                     \"and 'unstranded\")\n    return strand_flag[stranded]", "response": "get the strandedness of the base base"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun quality control on a single sample.", "response": "def run_qc(_, data, out_dir):\n    \"\"\"Run quality control in QC environment on a single sample.\n\n    Enables peddy integration with CWL runs.\n    \"\"\"\n    if cwlutils.is_cwl_run(data):\n        qc_data = run_peddy([data], out_dir)\n        if tz.get_in([\"summary\", \"qc\", \"peddy\"], qc_data):\n            return tz.get_in([\"summary\", \"qc\", \"peddy\"], qc_data)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(name, chip_bam, input_bam, genome_build, out_dir, method, resources, data):\n    # output file name need to have the caller name\n    config = dd.get_config(data)\n    out_file = os.path.join(out_dir, name + \"_peaks_macs2.xls\")\n    macs2_file = os.path.join(out_dir, name + \"_peaks.xls\")\n    if utils.file_exists(out_file):\n        _compres_bdg_files(out_dir)\n        return _get_output_files(out_dir)\n    macs2 = config_utils.get_program(\"macs2\", config)\n    options = \" \".join(resources.get(\"macs2\", {}).get(\"options\", \"\"))\n    genome_size = bam.fasta.total_sequence_length(dd.get_ref_file(data))\n    genome_size = \"\" if options.find(\"-g\") > -1 else \"-g %s\" % genome_size\n    paired = \"-f BAMPE\" if bam.is_paired(chip_bam) else \"\"\n    with utils.chdir(out_dir):\n        cmd = _macs2_cmd(method)\n        try:\n            do.run(cmd.format(**locals()), \"macs2 for %s\" % name)\n            utils.move_safe(macs2_file, out_file)\n        except subprocess.CalledProcessError:\n            raise RuntimeWarning(\"macs2 terminated with an error.\\n\"\n                                 \"Please, check the message and report \"\n                                 \"error if it is related to bcbio.\\n\"\n                                 \"You can add specific options for the sample \"\n                                 \"setting resources as explained in docs: \"\n                                 \"https://bcbio-nextgen.readthedocs.org/en/latest/contents/configuration.html#sample-specific-resources\")\n    _compres_bdg_files(out_dir)\n    return _get_output_files(out_dir)", "response": "Run macs2 for chip and input samples avoiding\n    errors due to samples."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _macs2_cmd(method=\"chip\"):\n    if method.lower() == \"chip\":\n        cmd = (\"{macs2} callpeak -t {chip_bam} -c {input_bam} {paired} \"\n                \" {genome_size} -n {name} -B {options}\")\n    elif method.lower() == \"atac\":\n        cmd = (\"{macs2} callpeak -t {chip_bam} --nomodel \"\n               \" {paired} {genome_size} -n {name} -B {options}\"\n               \" --nolambda --keep-dup all\")\n    else:\n        raise ValueError(\"chip_method should be chip or atac.\")\n    return cmd", "response": "Main command for macs2 tool."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert BAM archive files into indexed CRAM.", "response": "def to_cram(data):\n    \"\"\"Convert BAM archive files into indexed CRAM.\n    \"\"\"\n    data = utils.to_single_data(data)\n    cram_file = cram.compress(dd.get_work_bam(data) or dd.get_align_bam(data), data)\n    out_key = \"archive_bam\" if cwlutils.is_cwl_run(data) else \"work_bam\"\n    data[out_key] = cram_file\n    return [[data]]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compress(samples, run_parallel):\n    to_cram = []\n    finished = []\n    for data in [x[0] for x in samples]:\n        if \"cram\" in dd.get_archive(data) or \"cram-lossless\" in dd.get_archive(data):\n            to_cram.append([data])\n        else:\n            finished.append([data])\n    crammed = run_parallel(\"archive_to_cram\", to_cram)\n    return finished + crammed", "response": "Perform compression of output files for long term storage."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nestimates the coding potential cutoff that best classifies the reference VariantSequence", "response": "def get_coding_potential_cutoff(ref_gtf, ref_fasta, data):\n    \"\"\"\n    estimate the coding potential cutoff that best classifies\n    coding/noncoding transcripts by splitting the reference\n    annotation into a test and training set and determining\n    the cutoff where the sensitivity and specificity meet\n    \"\"\"\n    train_gtf, test_gtf = gtf.split_gtf(ref_gtf, sample_size=2000)\n    coding_gtf = gtf.partition_gtf(train_gtf, coding=True)\n    noncoding_gtf = gtf.partition_gtf(train_gtf)\n    noncoding_fasta = gtf.gtf_to_fasta(noncoding_gtf, ref_fasta)\n    cds_fasta = gtf.gtf_to_fasta(coding_gtf, ref_fasta, cds=True)\n    hexamer_content = hexamer_table(cds_fasta, noncoding_fasta, data)\n    coding_fasta = gtf.gtf_to_fasta(coding_gtf, ref_fasta)\n    logit_model = make_logit_model(coding_fasta, noncoding_fasta,\n                                       hexamer_content, data, \"test_gtf\")\n    test_fasta = gtf.gtf_to_fasta(test_gtf, ref_fasta)\n    cpat_fn = cpat(test_fasta, hexamer_content, logit_model, data)\n    cpat_prob = load_cpat_coding_prob(cpat_fn)\n    coding, noncoding = gtf.get_coding_noncoding_transcript_ids(test_gtf)\n    best_score = 1\n    best_cutoff = 0\n    best_sensitivity = 0\n    best_specificity = 0\n    for cutoff in list(numpy.arange(0.1, 1, 0.01)):\n        grade = grade_cpat(coding, noncoding, cpat_prob, cutoff)\n        score = abs(grade[\"sensitivity\"] - grade[\"specificity\"])\n        if score < best_score:\n            best_score = score\n            best_cutoff = cutoff\n            best_sensitivity = grade[\"sensitivity\"]\n            best_specificity = grade[\"specificity\"]\n    return best_cutoff, hexamer_content, logit_model"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(_, data, out_dir=None):\n    stats_file, idxstats_file = _get_stats_files(data, out_dir)\n    samtools = config_utils.get_program(\"samtools\", data[\"config\"])\n    bam_file = dd.get_align_bam(data) or dd.get_work_bam(data)\n    if not utils.file_exists(stats_file):\n        utils.safe_makedir(out_dir)\n        with file_transaction(data, stats_file) as tx_out_file:\n            cores = dd.get_num_cores(data)\n            cmd = \"{samtools} stats -@ {cores} {bam_file}\"\n            cmd += \" > {tx_out_file}\"\n            do.run(cmd.format(**locals()), \"samtools stats\", data)\n    if not utils.file_exists(idxstats_file):\n        utils.safe_makedir(out_dir)\n        with file_transaction(data, idxstats_file) as tx_out_file:\n            cmd = \"{samtools} idxstats {bam_file}\"\n            cmd += \" > {tx_out_file}\"\n            do.run(cmd.format(**locals()), \"samtools index stats\", data)\n    out = {\"base\": idxstats_file, \"secondary\": [stats_file]}\n    out[\"metrics\"] = _parse_samtools_stats(stats_file)\n    return out", "response": "Run samtools stats with reports on mapped reads duplicates and insert sizes."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns QC and save file outputs in data dictionary.", "response": "def run_and_save(data):\n    \"\"\"Run QC, saving file outputs in data dictionary.\n    \"\"\"\n    run(None, data)\n    stats_file, idxstats_file = _get_stats_files(data)\n    data = tz.update_in(data, [\"depth\", \"samtools\", \"stats\"], lambda x: stats_file)\n    data = tz.update_in(data, [\"depth\", \"samtools\", \"idxstats\"], lambda x: idxstats_file)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_stats_files(data, out_dir=None):\n    if not out_dir:\n        out_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data),\n                                                  \"qc\", dd.get_sample_name(data), \"samtools\"))\n    stats_file = tz.get_in([\"depth\", \"samtools\", \"stats\"], data)\n    idxstats_file = tz.get_in([\"depth\", \"samtools\", \"idxstats\"], data)\n    if not stats_file:\n        stats_file = os.path.join(out_dir, \"%s.txt\" % dd.get_sample_name(data))\n    if not idxstats_file:\n        idxstats_file = os.path.join(out_dir, \"%s-idxstats.txt\" % dd.get_sample_name(data))\n    return stats_file, idxstats_file", "response": "Retrieve stats files from pre - existing dictionary or filesystem."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(cmd, descr=None, data=None, checks=None, region=None, log_error=True,\n        log_stdout=False, env=None):\n    \"\"\"Run the provided command, logging details and checking for errors.\n    \"\"\"\n    if descr:\n      descr = _descr_str(descr, data, region)\n      logger.debug(descr)\n    cmd_id = diagnostics.start_cmd(cmd, descr or \"\", data)\n    try:\n        logger_cl.debug(\" \".join(str(x) for x in cmd) if not isinstance(cmd, six.string_types) else cmd)\n        _do_run(cmd, checks, log_stdout, env=env)\n    except:\n        diagnostics.end_cmd(cmd_id, False)\n        if log_error:\n            logger.exception()\n        raise\n    finally:\n        diagnostics.end_cmd(cmd_id)", "response": "Run the provided command and return a dictionary of results."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _descr_str(descr, data, region):\n    if data:\n        name = dd.get_sample_name(data)\n        if name:\n            descr = \"{0} : {1}\".format(descr, name)\n        elif \"work_bam\" in data:\n            descr = \"{0} : {1}\".format(descr, os.path.basename(data[\"work_bam\"]))\n    if region:\n        descr = \"{0} : {1}\".format(descr, region)\n    return descr", "response": "Add additional useful information from data to description string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nnormalizing subprocess arguments to handle list commands string and pipes.", "response": "def _normalize_cmd_args(cmd):\n    \"\"\"Normalize subprocess arguments to handle list commands, string and pipes.\n    Piped commands set pipefail and require use of bash to help with debugging\n    intermediate errors.\n    \"\"\"\n    if isinstance(cmd, six.string_types):\n        # check for standard or anonymous named pipes\n        if cmd.find(\" | \") > 0 or cmd.find(\">(\") or cmd.find(\"<(\"):\n            return \"set -o pipefail; \" + cmd, True, find_bash()\n        else:\n            return cmd, True, None\n    else:\n        return [str(x) for x in cmd], False, None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngroups items into somatic calling batches.", "response": "def somatic_batches(items):\n    \"\"\"Group items into somatic calling batches (tumor-only or tumor/normal).\n\n    Returns batches, where a data item may be in pairs, and somatic and non_somatic\n    (which are the original list of items).\n    \"\"\"\n    non_somatic = []\n    somatic = []\n    data_by_batches = defaultdict(list)\n    for data in items:\n        if not get_paired_phenotype(data):\n            non_somatic.append(data)\n        else:\n            somatic.append(data)\n            batches = dd.get_batches(data)\n            if batches:\n                for batch in batches:\n                    data_by_batches[batch].append(data)\n    return data_by_batches.values(), somatic, non_somatic"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsplitting aligned BAMs into tumor and normal pairs if this is a paired analysis.", "response": "def get_paired_bams(align_bams, items):\n    \"\"\"Split aligned bams into tumor / normal pairs if this is a paired analysis.\n    Allows cases with only tumor BAMs to handle callers that can work without\n    normal BAMs or with normal VCF panels.\n    \"\"\"\n    tumor_bam, tumor_name, normal_bam, normal_name, normal_panel, tumor_config, normal_data = (None,) * 7\n    for bamfile, item in zip(align_bams, items):\n        phenotype = get_paired_phenotype(item)\n        if phenotype == \"normal\":\n            normal_bam = bamfile\n            normal_name = dd.get_sample_name(item)\n            normal_data = item\n        elif phenotype == \"tumor\":\n            tumor_bam = bamfile\n            tumor_name = dd.get_sample_name(item)\n            tumor_data = item\n            tumor_config = item[\"config\"]\n            normal_panel = dd.get_background_variant(item)\n    if tumor_bam or tumor_name:\n        return PairedData(tumor_bam, tumor_name, normal_bam,\n                          normal_name, normal_panel, tumor_config,\n                          tumor_data, normal_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves all variant callers for somatic calling handling somatic / germline.", "response": "def get_somatic_variantcallers(items):\n    \"\"\"Retrieve all variant callers for somatic calling, handling somatic/germline.\n    \"\"\"\n    out = []\n    for data in items:\n        vcs = dd.get_variantcaller(data)\n        if isinstance(vcs, dict) and \"somatic\" in vcs:\n            vcs = vcs[\"somatic\"]\n        if not isinstance(vcs, (list, tuple)):\n            vcs = [vcs]\n        out += vcs\n    return set(vcs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks for incorrectly paired tumor or normal samples in a batch.", "response": "def check_paired_problems(items):\n    \"\"\"Check for incorrectly paired tumor/normal samples in a batch.\n    \"\"\"\n    # ensure we're in a paired batch\n    if not get_paired(items):\n        return\n    num_tumor = len([x for x in items if dd.get_phenotype(x).lower() == \"tumor\"])\n    if num_tumor > 1:\n        raise ValueError(\"Unsupported configuration: found multiple tumor samples in batch %s: %s\" %\n                         (tz.get_in([\"metadata\", \"batch\"], items[0]),\n                          [dd.get_sample_name(data) for data in items]))\n    elif num_tumor == 0 and any(dd.get_phenotype(data).lower() == \"normal\" for data in items):\n        raise ValueError(\"Found normal sample without tumor in batch %s: %s\" %\n                         (tz.get_in([\"metadata\", \"batch\"], items[0]),\n                          [dd.get_sample_name(data) for data in items]))\n    else:\n        vcs = get_somatic_variantcallers(items)\n        if \"mutect\" in vcs or \"mutect2\" in vcs or \"strelka2\" in vcs:\n            paired = get_paired(items)\n            if not (paired.normal_data or paired.normal_panel):\n                raise ValueError(\"MuTect, MuTect2 and Strelka2 somatic calling requires normal sample or panel: %s\" %\n                                 [dd.get_sample_name(data) for data in items])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_paired_phenotype(data):\n    allowed_names = set([\"tumor\", \"normal\"])\n    p = tz.get_in([\"metadata\", \"phenotype\"], data)\n    return p if p in allowed_names else None", "response": "Retrieve the phenotype for a paired tumor or normal analysis."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving string for indelcaller to use or empty string if not specified.", "response": "def get_indelcaller(d_or_c):\n    \"\"\"Retrieve string for indelcaller to use, or empty string if not specified.\n    \"\"\"\n    config = d_or_c if isinstance(d_or_c, dict) and \"config\" in d_or_c else d_or_c\n    indelcaller = config[\"algorithm\"].get(\"indelcaller\", \"\")\n    if not indelcaller:\n        indelcaller = \"\"\n    if isinstance(indelcaller, (list, tuple)):\n        indelcaller = indelcaller[0] if (len(indelcaller) > 0) else \"\"\n    return indelcaller"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_standardonly(in_file, ref_file, data):\n    from bcbio.heterogeneity import chromhacks\n    out_file = \"%s-stdchrs.vcf.gz\" % utils.splitext_plus(in_file)[0]\n    if not utils.file_exists(out_file):\n        stds = []\n        for c in ref.file_contigs(ref_file):\n            if chromhacks.is_nonalt(c.name):\n                stds.append(c.name)\n        if stds:\n            with file_transaction(data, out_file) as tx_out_file:\n                stds = \",\".join(stds)\n                in_file = bgzip_and_index(in_file, data[\"config\"])\n                cmd = \"bcftools view -o {tx_out_file} -O z {in_file} {stds}\"\n                do.run(cmd.format(**locals()), \"Subset to standard chromosomes\")\n    return bgzip_and_index(out_file, data[\"config\"]) if utils.file_exists(out_file) else in_file", "response": "Subset a VCF file to standard chromosomes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef split_snps_indels(orig_file, ref_file, config):\n    base, ext = utils.splitext_plus(orig_file)\n    snp_file = \"{base}-snp{ext}\".format(base=base, ext=ext)\n    indel_file = \"{base}-indel{ext}\".format(base=base, ext=ext)\n    for out_file, select_arg in [(snp_file, \"--types snps\"),\n                                 (indel_file, \"--exclude-types snps\")]:\n        if not utils.file_exists(out_file):\n            with file_transaction(config, out_file) as tx_out_file:\n                bcftools = config_utils.get_program(\"bcftools\", config)\n                output_type = \"z\" if out_file.endswith(\".gz\") else \"v\"\n                cmd = \"{bcftools} view -O {output_type} {orig_file} {select_arg} > {tx_out_file}\"\n                do.run(cmd.format(**locals()), \"Subset to SNPs and indels\")\n        if out_file.endswith(\".gz\"):\n            bgzip_and_index(out_file, config)\n    return snp_file, indel_file", "response": "Split a variant call file into SNPs and INDELs for processing."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_normal_sample(in_file):\n    with utils.open_gzipsafe(in_file) as in_handle:\n        for line in in_handle:\n            if line.startswith(\"##PEDIGREE\"):\n                parts = line.strip().split(\"Original=\")[1][:-1]\n                return parts", "response": "Retrieve normal sample if normal or turmor\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_samples(in_file):\n    with utils.open_gzipsafe(in_file) as in_handle:\n        for line in in_handle:\n            if line.startswith(\"#CHROM\"):\n                parts = line.strip().split(\"\\t\")\n                return parts[9:]\n    raise ValueError(\"Did not find sample header in VCF file %s\" % in_file)", "response": "Retrieve samples present in a VCF file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_exclude_samples(in_file, to_exclude):\n    include, exclude = [], []\n    to_exclude = set(to_exclude)\n    for s in get_samples(in_file):\n        if s in to_exclude:\n            exclude.append(s)\n        else:\n            include.append(s)\n    return include, exclude", "response": "Identify samples in the exclusion list which are actually in the VCF."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef exclude_samples(in_file, out_file, to_exclude, ref_file, config, filters=None):\n    include, exclude = _get_exclude_samples(in_file, to_exclude)\n    # can use the input sample, all exclusions already gone\n    if len(exclude) == 0:\n        out_file = in_file\n    elif not utils.file_exists(out_file):\n        with file_transaction(config, out_file) as tx_out_file:\n            bcftools = config_utils.get_program(\"bcftools\", config)\n            output_type = \"z\" if out_file.endswith(\".gz\") else \"v\"\n            include_str = \",\".join(include)\n            filter_str = \"-f %s\" % filters if filters is not None else \"\"  # filters could be e.g. 'PASS,.'\n            cmd = \"{bcftools} view -O {output_type} -s {include_str} {filter_str} {in_file} > {tx_out_file}\"\n            do.run(cmd.format(**locals()), \"Exclude samples: {}\".format(to_exclude))\n    return out_file", "response": "Exclude specific samples from an input VCF file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef select_sample(in_file, sample, out_file, config, filters=None):\n    if not utils.file_exists(out_file):\n        with file_transaction(config, out_file) as tx_out_file:\n            if len(get_samples(in_file)) == 1:\n                shutil.copy(in_file, tx_out_file)\n            else:\n                if in_file.endswith(\".gz\"):\n                    bgzip_and_index(in_file, config)\n                bcftools = config_utils.get_program(\"bcftools\", config)\n                output_type = \"z\" if out_file.endswith(\".gz\") else \"v\"\n                filter_str = \"-f %s\" % filters if filters is not None else \"\"  # filters could be e.g. 'PASS,.'\n                cmd = \"{bcftools} view -O {output_type} {filter_str} {in_file} -s {sample} > {tx_out_file}\"\n                do.run(cmd.format(**locals()), \"Select sample: %s\" % sample)\n    if out_file.endswith(\".gz\"):\n        bgzip_and_index(out_file, config)\n    return out_file", "response": "Select a single sample from the supplied multisample VCF file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef merge_variant_files(orig_files, out_file, ref_file, config, region=None):\n    in_pipeline = False\n    if isinstance(orig_files, dict):\n        file_key = config[\"file_key\"]\n        in_pipeline = True\n        orig_files = orig_files[file_key]\n    out_file = _do_merge(orig_files, out_file, config, region)\n    if in_pipeline:\n        return [{file_key: out_file, \"region\": region, \"sam_ref\": ref_file, \"config\": config}]\n    else:\n        return out_file", "response": "Combine multiple VCF files with different samples into a single output file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _do_merge(orig_files, out_file, config, region):\n    if not utils.file_exists(out_file):\n        with file_transaction(config, out_file) as tx_out_file:\n            _check_samples_nodups(orig_files)\n            prep_files = run_multicore(p_bgzip_and_index, [[x, config] for x in orig_files], config)\n            input_vcf_file = \"%s-files.txt\" % utils.splitext_plus(out_file)[0]\n            with open(input_vcf_file, \"w\") as out_handle:\n                for fname in prep_files:\n                    out_handle.write(fname + \"\\n\")\n            bcftools = config_utils.get_program(\"bcftools\", config)\n            output_type = \"z\" if out_file.endswith(\".gz\") else \"v\"\n            region_str = \"-r {}\".format(region) if region else \"\"\n            cmd = \"{bcftools} merge -O {output_type} {region_str} `cat {input_vcf_file}` > {tx_out_file}\"\n            do.run(cmd.format(**locals()), \"Merge variants\")\n    if out_file.endswith(\".gz\"):\n        bgzip_and_index(out_file, config)\n    return out_file", "response": "Perform merge with bcftools merge."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _check_samples_nodups(fnames):\n    counts = defaultdict(int)\n    for f in fnames:\n        for s in get_samples(f):\n            counts[s] += 1\n    duplicates = [s for s, c in counts.items() if c > 1]\n    if duplicates:\n        raise ValueError(\"Duplicate samples found in inputs %s: %s\" % (duplicates, fnames))", "response": "Ensure a set of VCFs do not have duplicate samples."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _sort_by_region(fnames, regions, ref_file, config):\n    contig_order = {}\n    for i, sq in enumerate(ref.file_contigs(ref_file, config)):\n        contig_order[sq.name] = i\n    sitems = []\n    assert len(regions) == len(fnames), (regions, fnames)\n    added_fnames = set([])\n    for region, fname in zip(regions, fnames):\n        if fname not in added_fnames:\n            if isinstance(region, (list, tuple)):\n                c, s, e = region\n            elif isinstance(region, six.string_types) and region.find(\":\") >= 0:\n                c, coords = region.split(\":\")\n                s, e = [int(x) for x in coords.split(\"-\")]\n            else:\n                c = region\n                s, e = 0, 0\n            sitems.append(((contig_order[c], s, e), c, fname))\n            added_fnames.add(fname)\n    sitems.sort()\n    return [(x[1], x[2]) for x in sitems]", "response": "Sort a set of regionally split files by region for ordered output."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef concat_variant_files(orig_files, out_file, regions, ref_file, config):\n    if not utils.file_exists(out_file):\n        input_file_list = _get_file_list(orig_files, out_file, regions, ref_file, config)\n        try:\n            out_file = _run_concat_variant_files_gatk4(input_file_list, out_file, config)\n        except subprocess.CalledProcessError as msg:\n            if (\"We require all VCFs to have complete VCF headers\" in str(msg) or\n                  \"Features added out of order\" in str(msg) or\n                  \"The reference allele cannot be missing\" in str(msg)):\n                out_file = _run_concat_variant_files_bcftools(input_file_list, out_file, config, naive=True)\n            else:\n                raise\n    if out_file.endswith(\".gz\"):\n        bgzip_and_index(out_file, config)\n    return out_file", "response": "Concatenate multiple variant files from regions into a single output file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nuse GATK4 GatherVcfs for concatenation of scattered VCFs.", "response": "def _run_concat_variant_files_gatk4(input_file_list, out_file, config):\n    \"\"\"Use GATK4 GatherVcfs for concatenation of scattered VCFs.\n    \"\"\"\n    if not utils.file_exists(out_file):\n        with file_transaction(config, out_file) as tx_out_file:\n            params = [\"-T\", \"GatherVcfs\", \"-I\", input_file_list, \"-O\", tx_out_file]\n            # Use GATK4 for merging, tools_off: [gatk4] applies to variant calling\n            config = utils.deepish_copy(config)\n            if \"gatk4\" in dd.get_tools_off({\"config\": config}):\n                config[\"algorithm\"][\"tools_off\"].remove(\"gatk4\")\n            # Allow specification of verbosity in the unique style this tool uses\n            resources = config_utils.get_resources(\"gatk\", config)\n            opts = [str(x) for x in resources.get(\"options\", [])]\n            if \"--verbosity\" in opts:\n                params += [\"--VERBOSITY:%s\" % opts[opts.index(\"--verbosity\") + 1]]\n            broad_runner = broad.runner_from_config(config)\n            broad_runner.run_gatk(params)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a list of VCFs for concatenating.", "response": "def _get_file_list(orig_files, out_file, regions, ref_file, config):\n    \"\"\"Create file with region sorted list of non-empty VCFs for concatenating.\n    \"\"\"\n    sorted_files = _sort_by_region(orig_files, regions, ref_file, config)\n    exist_files = [(c, x) for c, x in sorted_files if os.path.exists(x) and vcf_has_variants(x)]\n    if len(exist_files) == 0:  # no non-empty inputs, merge the empty ones\n        exist_files = [x for c, x in sorted_files if os.path.exists(x)]\n    elif len(exist_files) > 1:\n        exist_files = _fix_gatk_header(exist_files, out_file, config)\n    else:\n        exist_files = [x for c, x in exist_files]\n    ready_files = run_multicore(p_bgzip_and_index, [[x, config] for x in exist_files], config)\n    input_file_list = \"%s-files.list\" % utils.splitext_plus(out_file)[0]\n    with open(input_file_list, \"w\") as out_handle:\n        for fname in ready_files:\n            out_handle.write(fname + \"\\n\")\n    return input_file_list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _fix_gatk_header(exist_files, out_file, config):\n    from bcbio.variation import ploidy\n    c, base_file = exist_files[0]\n    replace_file = base_file\n    items = [{\"config\": config}]\n    if ploidy.get_ploidy(items, region=(c, 1, 2)) == 1:\n        for c, x in exist_files[1:]:\n            if ploidy.get_ploidy(items, (c, 1, 2)) > 1:\n                replace_file = x\n                break\n    base_fix_file = os.path.join(os.path.dirname(out_file),\n                                 \"%s-fixheader%s\" % utils.splitext_plus(os.path.basename(base_file)))\n    with file_transaction(config, base_fix_file) as tx_out_file:\n        header_file = \"%s-header.vcf\" % utils.splitext_plus(tx_out_file)[0]\n        do.run(\"zgrep ^# %s > %s\"\n                % (replace_file, header_file), \"Prepare header file for merging\")\n        resources = config_utils.get_resources(\"picard\", config)\n        ropts = []\n        if \"options\" in resources:\n            ropts += [str(x) for x in resources.get(\"options\", [])]\n        do.run(\"%s && picard FixVcfHeader HEADER=%s INPUT=%s OUTPUT=%s %s\" %\n               (utils.get_java_clprep(), header_file, base_file, base_fix_file, \" \".join(ropts)),\n               \"Reheader initial VCF file in merge\")\n    bgzip_and_index(base_fix_file, config)\n    return [base_fix_file] + [x for (c, x) in exist_files[1:]]", "response": "Fixes problems for GATK genomes that start with chrM by reheadering the first file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconcatenating variant files using bcftools concat.", "response": "def _run_concat_variant_files_bcftools(in_list, out_file, config, naive=False):\n    \"\"\"Concatenate variant files using bcftools concat, potentially using the fast naive option.\n    \"\"\"\n    if not utils.file_exists(out_file):\n        with file_transaction(config, out_file) as tx_out_file:\n            bcftools = config_utils.get_program(\"bcftools\", config)\n            output_type = \"z\" if out_file.endswith(\".gz\") else \"v\"\n            if naive:\n                args = \"--naive\"\n            else:\n                args = \"--allow-overlaps\"\n            cmd = \"{bcftools} concat {args} -O {output_type} --file-list {in_list} -o {tx_out_file}\"\n            do.run(cmd.format(**locals()), \"bcftools concat variants\")\n    if out_file.endswith(\".gz\"):\n        bgzip_and_index(out_file, config)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef combine_variant_files(orig_files, out_file, ref_file, config,\n                          quiet_out=True, region=None):\n    \"\"\"Combine VCF files from the same sample into a single output file.\n\n    Handles cases where we split files into SNPs/Indels for processing then\n    need to merge back into a final file.\n    \"\"\"\n    in_pipeline = False\n    if isinstance(orig_files, dict):\n        file_key = config[\"file_key\"]\n        in_pipeline = True\n        orig_files = orig_files[file_key]\n    if not utils.file_exists(out_file):\n        with file_transaction(config, out_file) as tx_out_file:\n            exist_files = [x for x in orig_files if os.path.exists(x)]\n            ready_files = run_multicore(p_bgzip_and_index, [[x, config] for x in exist_files], config)\n            dict_file = \"%s.dict\" % utils.splitext_plus(ref_file)[0]\n            cores = dd.get_num_cores({\"config\": config})\n            memscale = {\"magnitude\": 0.9 * cores, \"direction\": \"increase\"} if cores > 1 else None\n            cmd = [\"picard\"] + broad.get_picard_opts(config, memscale) + \\\n                  [\"MergeVcfs\", \"D=%s\" % dict_file, \"O=%s\" % tx_out_file] + \\\n                  [\"I=%s\" % f for f in ready_files]\n            cmd = \"%s && %s\" % (utils.get_java_clprep(), \" \".join(cmd))\n            do.run(cmd, \"Combine variant files\")\n    if out_file.endswith(\".gz\"):\n        bgzip_and_index(out_file, config)\n    if in_pipeline:\n        return [{file_key: out_file, \"region\": region, \"sam_ref\": ref_file, \"config\": config}]\n    else:\n        return out_file", "response": "Combine VCF files from the same sample into a single output file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsorting a VCF file by genome reference and position.", "response": "def sort_by_ref(vcf_file, data):\n    \"\"\"Sort a VCF file by genome reference and position, adding contig information.\n    \"\"\"\n    out_file = \"%s-prep.vcf.gz\" % utils.splitext_plus(vcf_file)[0]\n    if not utils.file_uptodate(out_file, vcf_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            header_file = \"%s-header.txt\" % utils.splitext_plus(tx_out_file)[0]\n            with open(header_file, \"w\") as out_handle:\n                for region in ref.file_contigs(dd.get_ref_file(data), data[\"config\"]):\n                    out_handle.write(\"##contig=<ID=%s,length=%s>\\n\" % (region.name, region.size))\n            cat_cmd = \"zcat\" if vcf_file.endswith(\"vcf.gz\") else \"cat\"\n            cmd = (\"{cat_cmd} {vcf_file} | grep -v ^##contig | bcftools annotate -h {header_file} | \"\n                   \"vt sort -m full -o {tx_out_file} -\")\n            with utils.chdir(os.path.dirname(tx_out_file)):\n                do.run(cmd.format(**locals()), \"Sort VCF by reference\")\n    return bgzip_and_index(out_file, data[\"config\"])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_contig_to_header_cl(ref_file, out_file):\n    header_file = \"%s-contig_header.txt\" % utils.splitext_plus(out_file)[0]\n    with open(header_file, \"w\") as out_handle:\n        for region in ref.file_contigs(ref_file, {}):\n            out_handle.write(\"##contig=<ID=%s,length=%s>\\n\" % (region.name, region.size))\n    return (\"grep -v ^##contig | bcftools annotate -h %s\" % header_file)", "response": "Add update to VCF header for contigs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_contig_to_header(line, ref_file):\n    if line.startswith(\"##fileformat=VCF\"):\n        out = [line]\n        for region in ref.file_contigs(ref_file):\n            out.append(\"##contig=<ID=%s,length=%s>\" % (region.name, region.size))\n        return \"\\n\".join(out)\n    else:\n        return line", "response": "Streaming target to add contigs to a VCF file header."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncombines variants in parallel by chromosome.", "response": "def parallel_combine_variants(orig_files, out_file, ref_file, config, run_parallel):\n    \"\"\"Combine variants in parallel by chromosome, concatenating final outputs.\n    \"\"\"\n    file_key = \"vcf_files\"\n    def split_by_region(data):\n        base, ext = utils.splitext_plus(os.path.basename(out_file))\n        args = []\n        for region in [x.name for x in ref.file_contigs(ref_file, config)]:\n            region_out = os.path.join(os.path.dirname(out_file), \"%s-regions\" % base,\n                                      \"%s-%s%s\" % (base, region, ext))\n            utils.safe_makedir(os.path.dirname(region_out))\n            args.append((region_out, ref_file, config, region))\n        return out_file, args\n    config = copy.deepcopy(config)\n    config[\"file_key\"] = file_key\n    prep_files = run_multicore(p_bgzip_and_index, [[x, config] for x in orig_files], config)\n    items = [[{file_key: prep_files}]]\n    parallel_split_combine(items, split_by_region, run_parallel,\n                           \"merge_variant_files\", \"concat_variant_files\",\n                           file_key, [\"region\", \"sam_ref\", \"config\"], split_outfile_i=0)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmove a VCF file with associated index.", "response": "def move_vcf(orig_file, new_file):\n    \"\"\"Move a VCF file with associated index.\n    \"\"\"\n    for ext in [\"\", \".idx\", \".tbi\"]:\n        to_move = orig_file + ext\n        if os.path.exists(to_move):\n            shutil.move(to_move, new_file + ext)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tabix_index(in_file, config, preset=None, tabix_args=None):\n    in_file = os.path.abspath(in_file)\n    out_file = in_file + \".tbi\"\n    if not utils.file_exists(out_file) or not utils.file_uptodate(out_file, in_file):\n        # Remove old index files to prevent linking into tx directory\n        utils.remove_safe(out_file)\n        with file_transaction(config, out_file) as tx_out_file:\n            tabix = tools.get_tabix_cmd(config)\n            tx_in_file = os.path.splitext(tx_out_file)[0]\n            utils.symlink_plus(in_file, tx_in_file)\n            if tabix_args:\n                cmd = \"{tabix} -f {tabix_args} {tx_in_file}\"\n            else:\n                preset = _guess_preset(in_file) if preset is None else preset\n                cmd = \"{tabix} -f -p {preset} {tx_in_file}\"\n            do.run(cmd.format(**locals()), \"tabix index %s\" % os.path.basename(in_file))\n    return out_file", "response": "Index a file using tabix."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_gvcf_file(in_file):\n    to_check = 100\n    n = 0\n    with utils.open_gzipsafe(in_file) as in_handle:\n        for line in in_handle:\n            if not line.startswith(\"##\"):\n                if n > to_check:\n                    break\n                n += 1\n                parts = line.split(\"\\t\")\n                # GATK\n                if parts[4] == \"<NON_REF>\":\n                    return True\n                # strelka2\n                if parts[4] == \".\" and parts[7].startswith(\"BLOCKAVG\"):\n                    return True\n                # freebayes\n                if parts[4] == \"<*>\":\n                    return True\n                # platypue\n                if parts[4] == \"N\" and parts[6] == \"REFCALL\":\n                    return True", "response": "Check if an input file is GVCF - formatted GATK - specific file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a FILTER value to a cyvcf2 record", "response": "def cyvcf_add_filter(rec, name):\n    \"\"\"Add a FILTER value to a cyvcf2 record\n    \"\"\"\n    if rec.FILTER:\n        filters = rec.FILTER.split(\";\")\n    else:\n        filters = []\n    if name not in filters:\n        filters.append(name)\n        rec.FILTER = filters\n    return rec"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cyvcf_remove_filter(rec, name):\n    if rec.FILTER:\n        filters = rec.FILTER.split(\";\")\n    else:\n        filters = []\n    new_filters = [x for x in filters if not str(x) == name]\n    if len(new_filters) == 0:\n        new_filters = [\"PASS\"]\n    rec.FILTER = new_filters\n    return rec", "response": "Remove a filter with the given name from a cyvcf2 record."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef organize_noalign(data):\n    data = utils.to_single_data(data[0])\n    work_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"align\", dd.get_sample_name(data)))\n    work_bam = os.path.join(work_dir, \"%s-input.bam\" % dd.get_sample_name(data))\n    if data.get(\"files\"):\n        if data[\"files\"][0].endswith(\".cram\"):\n            work_bam = cram.to_bam(data[\"files\"][0], work_bam, data)\n        else:\n            assert data[\"files\"][0].endswith(\".bam\"), data[\"files\"][0]\n            utils.copy_plus(data[\"files\"][0], work_bam)\n        bam.index(work_bam, data[\"config\"])\n    else:\n        work_bam = None\n    data[\"align_bam\"] = work_bam\n    return data", "response": "CWL target to skip alignment and organize input data.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\naligning to the named genome build returning a sorted BAM file.", "response": "def align_to_sort_bam(fastq1, fastq2, aligner, data):\n    \"\"\"Align to the named genome build, returning a sorted BAM file.\n    \"\"\"\n    names = data[\"rgnames\"]\n    align_dir_parts = [data[\"dirs\"][\"work\"], \"align\", names[\"sample\"]]\n    if data.get(\"disambiguate\"):\n        align_dir_parts.append(data[\"disambiguate\"][\"genome_build\"])\n    aligner_index = _get_aligner_index(aligner, data)\n    align_dir = utils.safe_makedir(os.path.join(*align_dir_parts))\n    ref_file = tz.get_in((\"reference\", \"fasta\", \"base\"), data)\n    if fastq1.endswith(\".bam\"):\n        data = _align_from_bam(fastq1, aligner, aligner_index, ref_file,\n                               names, align_dir, data)\n    else:\n        data = _align_from_fastq(fastq1, fastq2, aligner, aligner_index, ref_file,\n                                 names, align_dir, data)\n    if data[\"work_bam\"] and utils.file_exists(data[\"work_bam\"]):\n        if data.get(\"align_split\") and dd.get_mark_duplicates(data):\n            # If merging later with with bamsormadup need query sorted inputs\n            # but CWL requires a bai file. Create a fake one to make it happy.\n            bam.fake_index(data[\"work_bam\"], data)\n        else:\n            bam.index(data[\"work_bam\"], data[\"config\"])\n        for extra in [\"-sr\", \"-disc\"]:\n            extra_bam = utils.append_stem(data['work_bam'], extra)\n            if utils.file_exists(extra_bam):\n                bam.index(extra_bam, data[\"config\"])\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve aligner index retriever including aliases for shared.", "response": "def get_aligner_with_aliases(aligner, data):\n    \"\"\"Retrieve aligner index retriever, including aliases for shared.\n\n    Handles tricky cases like gridss where we need bwa indices even with\n    no aligner specified since they're used internally within GRIDSS.\n    \"\"\"\n    aligner_aliases = {\"sentieon-bwa\": \"bwa\"}\n    from bcbio import structural\n    if not aligner and \"gridss\" in structural.get_svcallers(data):\n        aligner = \"bwa\"\n    return aligner_aliases.get(aligner) or aligner"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhandling multiple specifications of aligner indexes returning value to pass to aligner.", "response": "def _get_aligner_index(aligner, data):\n    \"\"\"Handle multiple specifications of aligner indexes, returning value to pass to aligner.\n\n    Original bcbio case -- a list of indices.\n    CWL case: a single file with secondaryFiles staged in the same directory.\n    \"\"\"\n    aligner_indexes = tz.get_in((\"reference\", get_aligner_with_aliases(aligner, data), \"indexes\"), data)\n    # standard bcbio case\n    if aligner_indexes and isinstance(aligner_indexes, (list, tuple)):\n        aligner_index = os.path.commonprefix(aligner_indexes)\n        if aligner_index.endswith(\".\"):\n            aligner_index = aligner_index[:-1]\n        return aligner_index\n    # single file -- check for standard naming or directory\n    elif aligner_indexes and os.path.exists(aligner_indexes):\n        aligner_dir = os.path.dirname(aligner_indexes)\n        aligner_prefix = os.path.splitext(aligner_indexes)[0]\n        if len(glob.glob(\"%s.*\" % aligner_prefix)) > 0:\n            return aligner_prefix\n        else:\n            return aligner_dir\n\n    if aligner not in allow_noindices():\n        raise ValueError(\"Did not find reference indices for aligner %s in genome: %s\" %\n                         (aligner, data[\"reference\"]))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _align_from_fastq(fastq1, fastq2, aligner, align_ref, sam_ref, names,\n                      align_dir, data):\n    \"\"\"Align from fastq inputs, producing sorted BAM output.\n    \"\"\"\n    config = data[\"config\"]\n    align_fn = TOOLS[aligner].align_fn\n    out = align_fn(fastq1, fastq2, align_ref, names, align_dir, data)\n    # handle align functions that update the main data dictionary in place\n    if isinstance(out, dict):\n        assert out.get(\"work_bam\"), (dd.get_sample_name(data), out.get(\"work_bam\"))\n        return out\n    # handle output of raw SAM files that need to be converted to BAM\n    else:\n        work_bam = bam.sam_to_bam(out, config)\n        data[\"work_bam\"] = bam.sort(work_bam, config)\n        return data", "response": "Align from fastq inputs producing sorted BAM output."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(items, background=None):\n    paired = vcfutils.get_paired(items)\n    if paired:\n        inputs = [paired.tumor_data]\n        background = [paired.normal_data] if paired.normal_bam else []\n    else:\n        assert not background\n        inputs, background = sshared.find_case_control(items)\n    work_dir = _sv_workdir(inputs[0])\n    variant_file = _run_gridss(inputs, background, work_dir)\n    out = []\n    for data in items:\n        sample_file = variant_file\n        if \"sv\" not in data:\n            data[\"sv\"] = []\n        effects_vcf, _ = effects.add_to_vcf(sample_file, data, \"snpeff\")\n        data[\"sv\"].append({\"variantcaller\": \"gridss\",\n                           \"vrn_file\": effects_vcf or sample_file})\n        out.append(data)\n    return out", "response": "Perform detection of structural variations with Manta."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfinalize JVM options to avoid memory usage.", "response": "def _finalize_memory(jvm_opts):\n    \"\"\"GRIDSS does not recommend setting memory between 32 and 48Gb.\n\n    https://github.com/PapenfussLab/gridss#memory-usage\n    \"\"\"\n    avoid_min = 32\n    avoid_max = 48\n    out_opts = []\n    for opt in jvm_opts:\n        if opt.startswith(\"-Xmx\"):\n            spec = opt[4:]\n            val = int(spec[:-1])\n            mod = spec[-1]\n            if mod.upper() == \"M\":\n                adjust = 1024\n                min_val = avoid_min * 1024\n                max_val = avoid_max * 1024\n            else:\n                adjust = 1\n                min_val, max_val = avoid_min, avoid_max\n            if val >= min_val and val < max_val:\n                val = min_val - adjust\n            opt = \"%s%s%s\" % (opt[:4], val, mod)\n        out_opts.append(opt)\n    return out_opts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a reference directory with fasta and bwa indices.", "response": "def _setup_reference_files(data, tx_out_dir):\n    \"\"\"Create a reference directory with fasta and bwa indices.\n\n    GRIDSS requires all files in a single directory, so setup with symlinks.\n    This needs bwa aligner indices available, which we ensure with `get_aligner_with_aliases`\n    during YAML sample setup.\n    \"\"\"\n    aligner = dd.get_aligner(data) or \"bwa\"\n    out_dir = utils.safe_makedir(os.path.join(tx_out_dir, aligner))\n    ref_fasta = dd.get_ref_file(data)\n    ref_files = [\"%s%s\" % (utils.splitext_plus(ref_fasta)[0], ext) for ext in [\".fa\", \".fa.fai\", \".dict\"]]\n    for orig_file in ref_files + tz.get_in((\"reference\", aligner, \"indexes\"), data):\n        utils.symlink_plus(orig_file, os.path.join(out_dir, os.path.basename(orig_file)))\n    return os.path.join(out_dir, os.path.basename(ref_fasta))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsummarizes all quality metrics together", "response": "def summary(*samples):\n    \"\"\"Summarize all quality metrics together\"\"\"\n    samples = list(utils.flatten(samples))\n    work_dir = dd.get_work_dir(samples[0])\n    multiqc = config_utils.get_program(\"multiqc\", samples[0][\"config\"])\n    if not multiqc:\n        logger.debug(\"multiqc not found. Update bcbio_nextgen.py tools to fix this issue.\")\n    out_dir = utils.safe_makedir(os.path.join(work_dir, \"qc\", \"multiqc\"))\n    out_data = os.path.join(out_dir, \"multiqc_data\")\n    out_file = os.path.join(out_dir, \"multiqc_report.html\")\n    file_list = os.path.join(out_dir, \"list_files.txt\")\n    work_samples = cwlutils.unpack_tarballs([utils.deepish_copy(x) for x in samples], samples[0])\n    work_samples = _summarize_inputs(work_samples, out_dir)\n    if not utils.file_exists(out_file):\n        with tx_tmpdir(samples[0], work_dir) as tx_out:\n            in_files = _get_input_files(work_samples, out_dir, tx_out)\n            in_files += _merge_metrics(work_samples, out_dir)\n            if _one_exists(in_files):\n                with utils.chdir(out_dir):\n                    _create_config_file(out_dir, work_samples)\n                    input_list_file = _create_list_file(in_files, file_list)\n                    if dd.get_tmp_dir(samples[0]):\n                        export_tmp = \"export TMPDIR=%s && \" % dd.get_tmp_dir(samples[0])\n                    else:\n                        export_tmp = \"\"\n                    locale_export = utils.locale_export()\n                    path_export = utils.local_path_export()\n                    other_opts = config_utils.get_resources(\"multiqc\", samples[0][\"config\"]).get(\"options\", [])\n                    other_opts = \" \".join([str(x) for x in other_opts])\n                    cmd = (\"{path_export}{export_tmp}{locale_export} \"\n                           \"{multiqc} -f -l {input_list_file} {other_opts} -o {tx_out}\")\n                    do.run(cmd.format(**locals()), \"Run multiqc\")\n                    if utils.file_exists(os.path.join(tx_out, \"multiqc_report.html\")):\n                        shutil.move(os.path.join(tx_out, \"multiqc_report.html\"), out_file)\n                        shutil.move(os.path.join(tx_out, \"multiqc_data\"), out_data)\n    samples = _group_by_sample_and_batch(samples)\n    if utils.file_exists(out_file) and samples:\n        data_files = set()\n        for i, data in enumerate(samples):\n            data_files.add(os.path.join(out_dir, \"report\", \"metrics\", dd.get_sample_name(data) + \"_bcbio.txt\"))\n        data_files.add(os.path.join(out_dir, \"report\", \"metrics\", \"target_info.yaml\"))\n        data_files.add(os.path.join(out_dir, \"multiqc_config.yaml\"))\n        [data_files.add(f) for f in glob.glob(os.path.join(out_dir, \"multiqc_data\", \"*\"))]\n        data_files = [f for f in data_files if f and utils.file_exists(f)]\n        if \"summary\" not in samples[0]:\n            samples[0][\"summary\"] = {}\n        samples[0][\"summary\"][\"multiqc\"] = {\"base\": out_file, \"secondary\": data_files}\n\n        data_json = os.path.join(out_dir, \"multiqc_data\", \"multiqc_data.json\")\n        data_json_final = _save_uploaded_data_json(samples, data_json, os.path.join(out_dir, \"multiqc_data\"))\n        if data_json_final:\n            samples[0][\"summary\"][\"multiqc\"][\"secondary\"].append(data_json_final)\n\n        # Prepare final file list and inputs for downstream usage\n        file_list_final = _save_uploaded_file_list(samples, file_list, out_dir)\n        if file_list_final:\n            samples[0][\"summary\"][\"multiqc\"][\"secondary\"].append(file_list_final)\n            if any([cwlutils.is_cwl_run(d) for d in samples]):\n                for indir in [\"inputs\", \"report\"]:\n                    tarball = os.path.join(out_dir, \"multiqc-%s.tar.gz\" % (indir))\n                    if not utils.file_exists(tarball):\n                        with utils.chdir(out_dir):\n                            cmd = [\"tar\", \"-czvpf\", tarball, indir]\n                            do.run(cmd, \"Compress multiqc inputs: %s\" % indir)\n                    samples[0][\"summary\"][\"multiqc\"][\"secondary\"].append(tarball)\n\n    if any([cwlutils.is_cwl_run(d) for d in samples]):\n        samples = _add_versions(samples)\n\n    return [[data] for data in samples]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding tool and data versions to the summary.", "response": "def _add_versions(samples):\n    \"\"\"Add tool and data versions to the summary.\n    \"\"\"\n    samples[0][\"versions\"] = {\"tools\": programs.write_versions(samples[0][\"dirs\"], samples[0][\"config\"]),\n                              \"data\": provenancedata.write_versions(samples[0][\"dirs\"], samples)}\n    return samples"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _summarize_inputs(samples, out_dir):\n    logger.info(\"summarize target information\")\n    if samples[0].get(\"analysis\", \"\").lower() in [\"variant\", \"variant2\"]:\n        metrics_dir = utils.safe_makedir(os.path.join(out_dir, \"report\", \"metrics\"))\n        samples = _merge_target_information(samples, metrics_dir)\n\n    logger.info(\"summarize fastqc\")\n    out_dir = utils.safe_makedir(os.path.join(out_dir, \"report\", \"fastqc\"))\n    with utils.chdir(out_dir):\n        _merge_fastqc(samples)\n\n    preseq_samples = [s for s in samples if tz.get_in([\"config\", \"algorithm\", \"preseq\"], s)]\n    if preseq_samples:\n        logger.info(\"summarize preseq\")\n        out_dir = utils.safe_makedir(os.path.join(out_dir, \"report\", \"preseq\"))\n        with utils.chdir(out_dir):\n            _merge_preseq(preseq_samples)\n    return samples", "response": "Summarize inputs for MultiQC reporting in display."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _save_uploaded_data_json(samples, data_json_work, out_dir):\n    if not utils.file_exists(data_json_work):\n        return None\n\n    upload_path_mapping = dict()\n    for sample in samples:\n        upload_path_mapping.update(get_all_upload_paths_from_sample(sample))\n    if not upload_path_mapping:\n        return data_json_work\n\n    with io.open(data_json_work, encoding=\"utf-8\") as f:\n        data = json.load(f, object_pairs_hook=OrderedDict)\n    upload_base = samples[0][\"upload\"][\"dir\"]\n    data = walk_json(data, lambda s: _work_path_to_rel_final_path(s, upload_path_mapping, upload_base))\n\n    data_json_final = os.path.join(out_dir, \"multiqc_data_final.json\")\n    with io.open(data_json_final, \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=4)\n    return data_json_final", "response": "Save the data from the multiqc_data_work to out_dir."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving the list of files that have been uploaded to CWL.", "response": "def _save_uploaded_file_list(samples, file_list_work, out_dir):\n    \"\"\" Fixes all absolute work-rooted paths to relative final-rooted paths\n\n    For CWL, prepare paths relative to output directory.\n    \"\"\"\n    if not utils.file_exists(file_list_work):\n        return None\n\n    if any([cwlutils.is_cwl_run(d) for d in samples]):\n        upload_paths = []\n        with open(file_list_work) as f:\n            for p in (l.strip() for l in f.readlines() if os.path.exists(l.strip())):\n                if p.startswith(out_dir):\n                    upload_paths.append(p.replace(out_dir + \"/\", \"\"))\n    else:\n        upload_path_mapping = dict()\n        for sample in samples:\n            upload_path_mapping.update(get_all_upload_paths_from_sample(sample))\n        if not upload_path_mapping:\n            return None\n\n        with open(file_list_work) as f:\n            paths = [l.strip() for l in f.readlines() if os.path.exists(l.strip())]\n        upload_paths = [p for p in [\n            _work_path_to_rel_final_path(path, upload_path_mapping, samples[0][\"upload\"][\"dir\"])\n            for path in paths\n        ] if p]\n        if not upload_paths:\n            return None\n\n    file_list_final = os.path.join(out_dir, \"list_files_final.txt\")\n    with open(file_list_final, \"w\") as f:\n        for path in upload_paths:\n            f.write(path + '\\n')\n    return file_list_final"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _work_path_to_rel_final_path(path, upload_path_mapping, upload_base_dir):\n    if not path or not isinstance(path, str):\n        return path\n    upload_path = None\n\n    # First, check in the mapping: if it's there is a direct reference and\n    # it's a file, we immediately return it (saves lots of iterations)\n    if upload_path_mapping.get(path) is not None and os.path.isfile(path):\n        upload_path = upload_path_mapping[path]\n    else:\n        # Not a file: check for elements in the mapping that contain\n        # it\n        paths_to_check = [key for key in upload_path_mapping\n                          if path.startswith(key)]\n\n        if paths_to_check:\n            for work_path in paths_to_check:\n                if os.path.isdir(work_path):\n                    final_path = upload_path_mapping[work_path]\n                    upload_path = path.replace(work_path, final_path)\n                    break\n\n    if upload_path is not None:\n        return os.path.relpath(upload_path, upload_base_dir)\n    else:\n        return None", "response": "Check if path is a work - rooted path and convert to a relative final - rooted path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _one_exists(input_files):\n    for f in input_files:\n        if os.path.exists(f):\n            return True\n    return False", "response": "Check if at least one file exists for multiqc to run properly"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_input_files(samples, base_dir, tx_out_dir):\n    in_files = collections.defaultdict(list)\n    for data in samples:\n        sum_qc = tz.get_in([\"summary\", \"qc\"], data, {})\n        if sum_qc in [None, \"None\"]:\n            sum_qc = {}\n        elif isinstance(sum_qc, six.string_types):\n            sum_qc = {dd.get_algorithm_qc(data)[0]: sum_qc}\n        elif not isinstance(sum_qc, dict):\n            raise ValueError(\"Unexpected summary qc: %s\" % sum_qc)\n        for program, pfiles in sum_qc.items():\n            if isinstance(pfiles, dict):\n                pfiles = [pfiles[\"base\"]] + pfiles.get(\"secondary\", [])\n            # CWL: presents output files as single file plus associated secondary files\n            elif isinstance(pfiles, six.string_types):\n                if os.path.exists(pfiles):\n                    pfiles = [os.path.join(basedir, f) for basedir, subdir, filenames in os.walk(os.path.dirname(pfiles)) for f in filenames]\n                else:\n                    pfiles = []\n            in_files[(dd.get_sample_name(data), program)].extend(pfiles)\n    staged_files = []\n    for (sample, program), files in in_files.items():\n        cur_dir = utils.safe_makedir(os.path.join(base_dir, \"inputs\", sample, program))\n        for f in files:\n            if _check_multiqc_input(f) and _is_good_file_for_multiqc(f):\n                if _in_temp_directory(f) or any([cwlutils.is_cwl_run(d) for d in samples]):\n                    staged_f = os.path.join(cur_dir, os.path.basename(f))\n                    shutil.copy(f, staged_f)\n                    staged_files.append(staged_f)\n                else:\n                    staged_files.append(f)\n    staged_files.extend(get_qsig_multiqc_files(samples))\n    # Back compatible -- to migrate to explicit specifications in input YAML\n    if not any([cwlutils.is_cwl_run(d) for d in samples]):\n        staged_files += [\"trimmed\", \"htseq-count/*summary\"]\n        # Add in created target_info file\n        if os.path.isfile(os.path.join(base_dir, \"report\", \"metrics\", \"target_info.yaml\")):\n            staged_files += [os.path.join(base_dir, \"report\", \"metrics\", \"target_info.yaml\")]\n    return sorted(list(set(staged_files)))", "response": "Retrieve input files keyed by sample and QC method name."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _group_by_sample_and_batch(samples):\n    out = collections.defaultdict(list)\n    for data in samples:\n        out[(dd.get_sample_name(data), dd.get_align_bam(data), tuple(_get_batches(data)))].append(data)\n    return [xs[0] for xs in out.values()]", "response": "Group samples by sample name and batch."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _create_config_file(out_dir, samples):\n    out_file = os.path.join(out_dir, \"multiqc_config.yaml\")\n    out = {\"table_columns_visible\": dict()}\n\n    # Avoid duplicated bcbio columns with qualimap\n    if any((\"qualimap\" in dd.get_tools_on(d) or \"qualimap_full\" in dd.get_tools_on(d)) for d in samples):\n        # Hiding metrics duplicated by Qualimap\n        out[\"table_columns_visible\"][\"bcbio\"] = {\"Average_insert_size\": False}\n        out[\"table_columns_visible\"][\"FastQC\"] = {\"percent_gc\": False}\n\n        # Setting up thresholds for Qualimap depth cutoff calculations, based on sample avg depths\n        avg_depths = [tz.get_in([\"summary\", \"metrics\", \"Avg_coverage\"], s) for s in samples]\n        avg_depths = [x for x in avg_depths if x]\n        # Picking all thresholds up to the highest sample average depth\n        thresholds = [t for t in coverage.DEPTH_THRESHOLDS if not avg_depths or t <= max(avg_depths)]\n        # ...plus one more\n        if len(thresholds) < len(coverage.DEPTH_THRESHOLDS):\n            thresholds.append(coverage.DEPTH_THRESHOLDS[len(thresholds)])\n\n        # Showing only thresholds surrounding any of average depths\n        thresholds_hidden = []\n        for i, t in enumerate(thresholds):\n            if t > 20:  # Not hiding anything below 20x\n                if any(thresholds[i-1] <= c < thresholds[i] for c in avg_depths if c and i-1 >= 0) or \\\n                   any(thresholds[i] <= c < thresholds[i+1] for c in avg_depths if c and i+1 < len(thresholds)):\n                    pass\n                else:\n                    thresholds_hidden.append(t)\n\n        # Hide coverage unless running full qualimap, downsampled inputs are confusing\n        if not any((\"qualimap_full\" in dd.get_tools_on(d)) for d in samples):\n            thresholds_hidden = thresholds + thresholds_hidden\n            thresholds_hidden.sort()\n            thresholds = []\n        out['qualimap_config'] = {\n            'general_stats_coverage': [str(t) for t in thresholds],\n            'general_stats_coverage_hidden': [str(t) for t in thresholds_hidden]}\n\n    # Avoid confusing peddy outputs, sticking to ancestry and sex prediction\n    out[\"table_columns_visible\"][\"Peddy\"] = {\"family_id\": False, \"sex_het_ratio\": False,\n                                             \"error_sex_check\": False}\n\n    # Setting the module order\n    module_order = []\n    module_order.extend([\n        \"bcbio\",\n        \"samtools\",\n        \"goleft_indexcov\",\n        \"peddy\"\n    ])\n    out['bcftools'] = {'write_separate_table': True}\n    # if germline calling was performed:\n    if any(\"germline\" in (get_active_vcinfo(s) or {}) or  # tumor-only somatic with germline extraction\n           dd.get_phenotype(s) == \"germline\" or           # or paired somatic with germline calling for normal\n           _has_bcftools_germline_stats(s)                # CWL organized statistics\n           for s in samples):\n        # Split somatic and germline variant stats into separate multiqc submodules,\n        # with somatic going into General Stats, and germline going into a separate table:\n        module_order.extend([{\n            'bcftools': {\n                'name': 'Bcftools (somatic)',\n                'info': 'Bcftools stats for somatic variant calls only.',\n                'path_filters': ['*_bcftools_stats.txt'],\n                'write_general_stats': True,\n            }},\n            {'bcftools': {\n                'name': 'Bcftools (germline)',\n                'info': 'Bcftools stats for germline variant calls only.',\n                'path_filters': ['*_bcftools_stats_germline.txt'],\n                'write_general_stats': False\n            }},\n        ])\n    else:\n        module_order.append(\"bcftools\")\n    module_order.extend([\n        \"salmon\",\n        \"picard\",\n        \"qualimap\",\n        \"snpeff\",\n        \"fastqc\",\n        \"preseq\",\n    ])\n    out[\"module_order\"] = module_order\n\n    preseq_samples = [s for s in samples if tz.get_in([\"config\", \"algorithm\", \"preseq\"], s)]\n    if preseq_samples:\n        out[\"preseq\"] = _make_preseq_multiqc_config(preseq_samples)\n\n    with open(out_file, \"w\") as out_handle:\n        yaml.safe_dump(out, out_handle, default_flow_style=False, allow_unicode=False)\n    return out_file", "response": "Create a configuration file for the current sample."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _has_bcftools_germline_stats(data):\n    stats_file = tz.get_in([\"summary\", \"qc\"], data)\n    if isinstance(stats_file, dict):\n        stats_file = tz.get_in([\"variants\", \"base\"], stats_file)\n    if not stats_file:\n        stats_file = \"\"\n    return stats_file.find(\"bcftools_stats_germline\") > 0", "response": "Check for presence of a germline stats file CWL compatible."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning False if the file is binary or image.", "response": "def _is_good_file_for_multiqc(fpath):\n    \"\"\"Returns False if the file is binary or image.\"\"\"\n    # Use mimetypes to exclude binary files where possible\n    (ftype, encoding) = mimetypes.guess_type(fpath)\n    if encoding is not None:\n        return False\n    if ftype is not None and ftype.startswith('image'):\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing disambiguation stats from given file.", "response": "def _parse_disambiguate(disambiguatestatsfilename):\n    \"\"\"Parse disambiguation stats from given file.\n    \"\"\"\n    disambig_stats = [0, 0, 0]\n    with open(disambiguatestatsfilename, \"r\") as in_handle:\n        for i, line in enumerate(in_handle):\n            fields = line.strip().split(\"\\t\")\n            if i == 0:\n                assert fields == ['sample', 'unique species A pairs', 'unique species B pairs', 'ambiguous pairs']\n            else:\n                disambig_stats = [x + int(y) for x, y in zip(disambig_stats, fields[1:])]\n    return disambig_stats"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmerges metrics from multiple QC steps into one QC archive.", "response": "def _merge_metrics(samples, out_dir):\n    \"\"\"Merge metrics from multiple QC steps\n    \"\"\"\n    logger.info(\"summarize metrics\")\n    out_dir = utils.safe_makedir(os.path.join(out_dir, \"report\", \"metrics\"))\n    sample_metrics = collections.defaultdict(dict)\n    for s in samples:\n        s = _add_disambiguate(s)\n        m = tz.get_in(['summary', 'metrics'], s)\n        if isinstance(m, six.string_types):\n            m = json.loads(m)\n        if m:\n            for me in m.keys():\n                if isinstance(m[me], list) or isinstance(m[me], dict) or isinstance(m[me], tuple):\n                    m.pop(me, None)\n            sample_metrics[dd.get_sample_name(s)].update(m)\n    out = []\n    for sample_name, m in sample_metrics.items():\n        sample_file = os.path.join(out_dir, \"%s_bcbio.txt\" % sample_name)\n        with file_transaction(samples[0], sample_file) as tx_out_file:\n            dt = pd.DataFrame(m, index=['1'])\n            dt.columns = [k.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\") for k in dt.columns]\n            dt['sample'] = sample_name\n            dt['rRNA_rate'] = m.get('rRNA_rate', \"NA\")\n            dt['RiP_pct'] = \"%.3f\" % (int(m.get(\"RiP\", 0)) / float(m.get(\"Total_reads\", 1)) * 100)\n            dt = _fix_duplicated_rate(dt)\n            dt.transpose().to_csv(tx_out_file, sep=\"\\t\", header=False)\n        out.append(sample_file)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _merge_fastqc(samples):\n    fastqc_list = collections.defaultdict(list)\n    seen = set()\n    for data in samples:\n        name = dd.get_sample_name(data)\n        if name in seen:\n            continue\n        seen.add(name)\n        fns = glob.glob(os.path.join(dd.get_work_dir(data), \"qc\", dd.get_sample_name(data), \"fastqc\") + \"/*\")\n        for fn in fns:\n            if fn.endswith(\"tsv\"):\n                metric = os.path.basename(fn)\n                fastqc_list[metric].append([name, fn])\n\n    for metric in fastqc_list:\n        dt_by_sample = []\n        for fn in fastqc_list[metric]:\n            dt = pd.read_csv(fn[1], sep=\"\\t\")\n            dt['sample'] = fn[0]\n            dt_by_sample.append(dt)\n        dt = utils.rbind(dt_by_sample)\n        dt.to_csv(metric, sep=\"\\t\", index=False, mode ='w')\n    return samples", "response": "Merge all fastqc samples into one by module\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_plot(tumor, in_glob, out_ext, page=1):\n    out_dir = utils.safe_makedir(\"images\")\n    out_name = os.path.join(out_dir, \"%s-%s\" % (tumor, out_ext))\n    in_file = glob.glob(in_glob)[0]\n    cmd = [\"pdftoppm\", in_file, out_name, \"-png\", \"-f\", page, \"-singlefile\"]\n    if not os.path.exists(out_name + \".png\"):\n        subprocess.check_call([str(x) for x in cmd])\n    return out_name + \".png\"", "response": "Create an output plot for the given PDF in the images directory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_cromwell_execution_dir(base_dir, target_glob):\n    cur_dir = glob.glob(os.path.join(base_dir, target_glob))[0]\n    if os.path.exists(os.path.join(cur_dir, \"cwl.output.json\")):\n        return base_dir\n    else:\n        symlink_dir = os.path.dirname(os.path.realpath(os.path.join(cur_dir, \"script\")))\n        ref_base = os.path.dirname(base_dir)\n        new_guid = symlink_dir[symlink_dir.find(ref_base) + len(ref_base) + 1:].split(\"/\")[0]\n        return _get_cromwell_execution_dir(os.path.join(ref_base, new_guid), target_glob)", "response": "Retrieve the baseline directory with cromwell output files."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprepares expected input BAM files from pre - aligned.", "response": "def prep_bam_inputs(out_dir, sample, call_file, bam_file):\n    \"\"\"Prepare expected input BAM files from pre-aligned.\n    \"\"\"\n    base = utils.splitext_plus(os.path.basename(bam_file))[0]\n    with open(call_file) as in_handle:\n        for cur_hla in (x.strip() for x in in_handle):\n            out_file = os.path.join(utils.safe_makedir(os.path.join(out_dir, base)),\n                                    \"%s.type.%s.filtered.bam\" % (base, cur_hla))\n            if not os.path.exists(out_file):\n                cmd = [\"samtools\", \"view\", \"-b\",\"-o\", out_file, bam_file, cur_hla]\n                subprocess.check_call(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_tumor_bamdir(tumor, tumor_bam, normal_bam, work_dir):\n    bam_dir = utils.safe_makedir(os.path.join(work_dir, tumor, \"in_bams\"))\n    normal_bam_ready = os.path.join(bam_dir, os.path.basename(normal_bam))\n    utils.symlink_plus(normal_bam, normal_bam_ready)\n    tumor_bam_ready = os.path.join(bam_dir, os.path.basename(tumor_bam))\n    utils.symlink_plus(tumor_bam, tumor_bam_ready)\n    return bam_dir, normal_bam_ready", "response": "Create expected input directory with tumor and normal BAMs in one place."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve HLA calls and input fastqs for a sample.", "response": "def get_hla(sample, cromwell_dir, hla_glob):\n    \"\"\"Retrieve HLA calls and input fastqs for a sample.\n    \"\"\"\n    hla_dir = glob.glob(os.path.join(cromwell_dir, hla_glob, \"align\", sample, \"hla\"))[0]\n    fastq = os.path.join(hla_dir, \"OptiType-HLA-A_B_C-input.fq\")\n    calls = os.path.join(hla_dir, \"%s-optitype.csv\" % sample)\n    return fastq, calls"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef name_to_absolute(x):\n    for c in [\"-\", \"*\", \":\"]:\n        x = x.replace(c, \"_\")\n    x = x.lower()\n    return x", "response": "Convert standard hg38 HLA name into ABSOLUTE naming."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve matching HLA with best read support in both tumor and normal.", "response": "def get_hla_choice(h, hlas, normal_bam, tumor_bam):\n    \"\"\"Retrieve matching HLA with best read support in both tumor and normal\n    \"\"\"\n    def get_counts(bam_file):\n        counts = {}\n        for line in subprocess.check_output([\"samtools\", \"idxstats\", bam_file]).split(\"\\n\"):\n            if line.startswith(h):\n                name, _, count, _ = line.split()\n                counts[name] = int(count)\n        return counts\n    tcounts = get_counts(tumor_bam)\n    ncounts = get_counts(normal_bam)\n    check_hlas = [x for x in hlas if x.startswith(h) and tcounts.get(x, 0) > 0 and ncounts.get(x, 0) > 0]\n    cur_hlas = sorted(check_hlas, key=lambda x: ncounts[x], reverse=True)\n    #print(cur_hlas[0], tcounts.get(cur_hlas[0]), ncounts.get(cur_hlas[0]))\n    return cur_hlas[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting HLAs into ABSOLUTE format for use with LOHHLA.", "response": "def prep_hla(work_dir, sample, calls, hlas, normal_bam, tumor_bam):\n    \"\"\"Convert HLAs into ABSOLUTE format for use with LOHHLA.\n\n    LOHHLA hard codes names to hla_a, hla_b, hla_c so need to move\n    \"\"\"\n    work_dir = utils.safe_makedir(os.path.join(work_dir, sample, \"inputs\"))\n    hla_file = os.path.join(work_dir, \"%s-hlas.txt\" % sample)\n    with open(calls) as in_handle:\n        with open(hla_file, \"w\") as out_handle:\n            next(in_handle)  # header\n            for line in in_handle:\n                _, _, a, _, _ = line.strip().split(\",\")\n                a1, a2 = a.split(\";\")\n                out_handle.write(get_hla_choice(name_to_absolute(a1), hlas, normal_bam, tumor_bam) + \"\\n\")\n                out_handle.write(get_hla_choice(name_to_absolute(a2), hlas, normal_bam, tumor_bam) + \"\\n\")\n    return hla_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate LOHHLA compatible input ploidy file from PureCN output.", "response": "def prep_ploidy(work_dir, sample, bam_file, cromwell_dir, sv_glob):\n    \"\"\"Create LOHHLA compatible input ploidy file from PureCN output.\n    \"\"\"\n    purecn_file = _get_cromwell_file(cromwell_dir, sv_glob, dict(sample=sample, method=\"purecn\", ext=\"purecn.csv\"))\n    work_dir = utils.safe_makedir(os.path.join(work_dir, sample, \"inputs\"))\n    out_file = os.path.join(work_dir, \"%s-solutions.txt\" % sample)\n    with open(purecn_file) as in_handle:\n        reader = csv.reader(in_handle)\n        purecn_stats = dict(zip(next(reader), next(reader)))\n    with open(out_file, \"w\") as out_handle:\n        out_handle.write(\"Ploidy\\ttumorPurity\\ttumorPloidy\\n\")\n        lohhla_name = utils.splitext_plus(os.path.basename(bam_file))[0]\n        out_handle.write(\"%s\\t%s\\t%s\\t%s\\n\" % (lohhla_name, purecn_stats[\"Ploidy\"],\n                                               purecn_stats[\"Purity\"], purecn_stats[\"Ploidy\"]))\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _bowtie_args_from_config(data):\n    config = data['config']\n    qual_format = config[\"algorithm\"].get(\"quality_format\", \"\")\n    if qual_format.lower() == \"illumina\":\n        qual_flags = [\"--phred64-quals\"]\n    else:\n        qual_flags = []\n    multi_mappers = config[\"algorithm\"].get(\"multiple_mappers\", True)\n    multi_flags = [\"-M\", 1] if multi_mappers else [\"-m\", 1]\n    multi_flags = [] if data[\"analysis\"].lower().startswith(\"smallrna-seq\") else multi_flags\n    cores = config.get(\"resources\", {}).get(\"bowtie\", {}).get(\"cores\", None)\n    num_cores = config[\"algorithm\"].get(\"num_cores\", 1)\n    core_flags = [\"-p\", str(num_cores)] if num_cores > 1 else []\n    return core_flags + qual_flags + multi_flags", "response": "Configurable high level options for bowtie.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform alignment with Bowtie.", "response": "def align(fastq_file, pair_file, ref_file, names, align_dir, data,\n          extra_args=None):\n    \"\"\"Do standard or paired end alignment with bowtie.\n    \"\"\"\n    num_hits = 1\n    if data[\"analysis\"].lower().startswith(\"smallrna-seq\"):\n        num_hits = 1000\n    config = data['config']\n    out_file = os.path.join(align_dir, \"{0}-sort.bam\".format(dd.get_sample_name(data)))\n    if data.get(\"align_split\"):\n        final_file = out_file\n        out_file, data = alignprep.setup_combine(final_file, data)\n        fastq_file, pair_file = alignprep.split_namedpipe_cls(fastq_file, pair_file, data)\n    else:\n        final_file = None\n        if fastq_file.endswith(\".gz\"):\n            fastq_file = \"<(gunzip -c %s)\" % fastq_file\n            if pair_file:\n                pair_file = \"<(gunzip -c %s)\" % pair_file\n\n    if not utils.file_exists(out_file) and (final_file is None or not utils.file_exists(final_file)):\n        with postalign.tobam_cl(data, out_file, pair_file is not None) as (tobam_cl, tx_out_file):\n            cl = [config_utils.get_program(\"bowtie\", config)]\n            cl += _bowtie_args_from_config(data)\n            cl += extra_args if extra_args is not None else []\n            cl += [\"-q\",\n                   \"-v\", 2,\n                   \"-k\", num_hits,\n                   \"-X\", 2000, # default is too selective for most data\n                   \"--best\",\n                   \"--strata\",\n                   \"--sam\",\n                   ref_file]\n            if pair_file:\n                cl += [\"-1\", fastq_file, \"-2\", pair_file]\n            else:\n                cl += [fastq_file]\n            cl = [str(i) for i in cl]\n            fix_rg_cmd = r\"samtools addreplacerg -r '%s' -\" % novoalign.get_rg_info(data[\"rgnames\"])\n            if fix_rg_cmd:\n                cmd = \" \".join(cl) + \" | \" + fix_rg_cmd + \" | \" + tobam_cl\n            else:\n                cmd = \" \".join(cl) + \" | \" + tobam_cl\n            do.run(cmd, \"Running Bowtie on %s and %s.\" % (fastq_file, pair_file), data)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(vrn_info, cnvs_by_name, somatic_info):\n    cmd = _get_cmd(\"RunTHeTA.py\")\n    if not cmd:\n        logger.info(\"THetA scripts not found in current PATH. Skipping.\")\n    else:\n        from bcbio.structural import cnvkit\n        work_dir = _sv_workdir(somatic_info.tumor_data)\n        assert \"cnvkit\" in cnvs_by_name, \"THetA requires CNVkit calls\"\n        cnv_info = cnvkit.export_theta(cnvs_by_name[\"cnvkit\"], somatic_info.tumor_data)\n        cnv_info[\"theta_input\"] = subset_by_supported(cnv_info[\"theta_input\"], _theta_to_coords,\n                                                      cnvs_by_name, work_dir, somatic_info.tumor_data)\n        return _run_theta(cnv_info, somatic_info.tumor_data, work_dir, run_n3=False)", "response": "Run THetA analysis on a tumor or normal pair."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlimiting CNVkit input to calls with support from another caller.", "response": "def subset_by_supported(input_file, get_coords, calls_by_name, work_dir, data,\n                        headers=(\"#\",)):\n    \"\"\"Limit CNVkit input to calls with support from another caller.\n\n    get_coords is a function that return chrom, start, end from a line of the\n    input_file, allowing handling of multiple input file types.\n    \"\"\"\n    support_files = [(c, tz.get_in([c, \"vrn_file\"], calls_by_name))\n                     for c in convert.SUBSET_BY_SUPPORT[\"cnvkit\"]]\n    support_files = [(c, f) for (c, f) in support_files if f and vcfutils.vcf_has_variants(f)]\n    if len(support_files) == 0:\n        return input_file\n    else:\n        out_file = os.path.join(work_dir, \"%s-havesupport%s\" %\n                                utils.splitext_plus(os.path.basename(input_file)))\n        if not utils.file_uptodate(out_file, input_file):\n            input_bed = _input_to_bed(input_file, work_dir, get_coords, headers)\n            pass_coords = set([])\n            with file_transaction(data, out_file) as tx_out_file:\n                support_beds = \" \".join([_sv_vcf_to_bed(f, c, out_file) for c, f in support_files])\n                tmp_cmp_bed = \"%s-intersectwith.bed\" % utils.splitext_plus(tx_out_file)[0]\n                cmd = \"bedtools intersect -wa -f 0.5 -r -a {input_bed} -b {support_beds} > {tmp_cmp_bed}\"\n                do.run(cmd.format(**locals()), \"Intersect CNVs with support files\")\n                for r in pybedtools.BedTool(tmp_cmp_bed):\n                    pass_coords.add((str(r.chrom), str(r.start), str(r.stop)))\n                with open(input_file) as in_handle:\n                    with open(tx_out_file, \"w\") as out_handle:\n                        for line in in_handle:\n                            passes = True\n                            if not line.startswith(headers):\n                                passes = get_coords(line) in pass_coords\n                            if passes:\n                                out_handle.write(line)\n        return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting input file to BED file for comparisons", "response": "def _input_to_bed(theta_input, work_dir, get_coords, headers):\n    \"\"\"Convert input file to a BED file for comparisons\n    \"\"\"\n    theta_bed = os.path.join(work_dir, \"%s.bed\" % os.path.splitext(os.path.basename(theta_input))[0])\n    with open(theta_input) as in_handle:\n        with open(theta_bed, \"w\") as out_handle:\n            for line in in_handle:\n                if not line.startswith(headers):\n                    chrom, start, end = get_coords(line)\n                    out_handle.write(\"\\t\".join([chrom, start, end]) + \"\\n\")\n    return theta_bed"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _run_theta(cnv_info, data, work_dir, run_n3=True):\n    out = {\"caller\": \"theta\"}\n    max_normal = \"0.9\"\n    opts = [\"-m\", max_normal]\n    n2_result = _safe_run_theta(cnv_info[\"theta_input\"], os.path.join(work_dir, \"n2\"), \".n2.results\",\n                                [\"-n\", \"2\"] + opts, data)\n    if n2_result:\n        out[\"estimate\"] = n2_result\n        if run_n3:\n            n2_bounds = \"%s.withBounds\" % os.path.splitext(n2_result)[0]\n            n3_result = _safe_run_theta(n2_bounds, os.path.join(work_dir, \"n3\"), \".n3.results\",\n                                        [\"-n\", \"3\", \"--RESULTS\", n2_result] + opts,\n                                        data)\n            if n3_result:\n                best_result = _select_model(n2_bounds, n2_result, n3_result,\n                                            os.path.join(work_dir, \"n3\"), data)\n                out[\"estimate\"] = best_result\n                out[\"cnvs\"] = _merge_theta_calls(n2_bounds, best_result, cnv_info[\"vrn_file\"], data)\n    return out", "response": "Run theta on the current set of subpopulations and normal contamination."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _update_with_calls(result_file, cnv_file):\n    results = {}\n    with open(result_file) as in_handle:\n        in_handle.readline()  # header\n        _, _, cs, ps = in_handle.readline().strip().split()\n        for i, (c, p) in enumerate(zip(cs.split(\":\"), ps.split(\",\"))):\n            results[i] = (c, p)\n    cnvs = {}\n    with open(cnv_file) as in_handle:\n        for line in in_handle:\n            chrom, start, end, _, count = line.rstrip().split()[:5]\n            cnvs[(chrom, start, end)] = count\n    def update(i, line):\n        parts = line.rstrip().split(\"\\t\")\n        chrom, start, end = parts[1:4]\n        parts += cnvs.get((chrom, start, end), \".\")\n        parts += list(results[i])\n        return \"\\t\".join(parts) + \"\\n\"\n    return update", "response": "Update bounds with calls from CNVkit inferred copy numbers and p - values from THetA.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _merge_theta_calls(bounds_file, result_file, cnv_file, data):\n    out_file = \"%s-merged.txt\" % (result_file.replace(\".BEST.results\", \"\"))\n    if not utils.file_uptodate(out_file, result_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            updater = _update_with_calls(result_file, cnv_file)\n            with open(bounds_file) as in_handle:\n                with open(tx_out_file, \"w\") as out_handle:\n                    i = 0\n                    for line in in_handle:\n                        if line.startswith(\"#\"):\n                            parts = line.rstrip().split(\"\\t\")\n                            parts += [\"cnv\", \"pop_cnvs\", \"pop_pvals\"]\n                            out_handle.write(\"\\t\".join(parts) + \"\\n\")\n                        else:\n                            out_handle.write(updater(i, line))\n                            i += 1\n    return out_file", "response": "Create a final output file with merged CNVkit and THetA copy and population estimates."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns final model selection from n = 2 and n = 3 options.", "response": "def _select_model(n2_bounds, n2_result, n3_result, out_dir, data):\n    \"\"\"Run final model selection from n=2 and n=3 options.\n    \"\"\"\n    n2_out_file = n2_result.replace(\".n2.results\", \".BEST.results\")\n    n3_out_file = n3_result.replace(\".n3.results\", \".BEST.results\")\n    if not utils.file_exists(n2_out_file) and not utils.file_exists(n3_out_file):\n        cmd = _get_cmd(\"ModelSelection.py\") + [n2_bounds, n2_result, n3_result]\n        do.run(cmd, \"Select best THetA model\")\n    if utils.file_exists(n2_out_file):\n        return n2_out_file\n    else:\n        assert utils.file_exists(n3_out_file)\n        return n3_out_file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _safe_run_theta(input_file, out_dir, output_ext, args, data):\n    out_file = os.path.join(out_dir, _split_theta_ext(input_file) + output_ext)\n    skip_file = out_file + \".skipped\"\n    if utils.file_exists(skip_file):\n        return None\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_dir) as tx_out_dir:\n            utils.safe_makedir(tx_out_dir)\n            cmd = _get_cmd(\"RunTHetA.py\") + args + \\\n                  [input_file, \"--NUM_PROCESSES\", dd.get_cores(data),\n                   \"--FORCE\", \"-d\", tx_out_dir]\n            try:\n                do.run(cmd, \"Run THetA to calculate purity\", log_error=False)\n            except subprocess.CalledProcessError as msg:\n                if (\"Number of intervals must be greater than 1\" in str(msg) or\n                      \"This sample isn't a good candidate for THetA analysis\" in str(msg)):\n                    with open(os.path.join(tx_out_dir, os.path.basename(skip_file)), \"w\") as out_handle:\n                        out_handle.write(\"Expected TheTA failure, skipping\")\n                    return None\n                else:\n                    raise\n    return out_file", "response": "Run THetA and check if there are any errors."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_cmd(cmd):\n    check_cmd = \"RunTHetA.py\"\n    try:\n        local_cmd = subprocess.check_output([\"which\", check_cmd]).strip()\n    except subprocess.CalledProcessError:\n        return None\n    return [sys.executable, \"%s/%s\" % (os.path.dirname(os.path.realpath(local_cmd)), cmd)]", "response": "Retrieve required commands for running THetA with our local bcbio python."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(data):\n    sample = data[0][0]\n    work_dir = dd.get_work_dir(sample)\n    out_dir = os.path.join(work_dir, \"mirge\")\n    lib = _find_lib(sample)\n    mirge = _find_mirge(sample)\n    bowtie = _find_bowtie(sample)\n    sps = dd.get_species(sample)\n    species = SPS.get(sps, \"\")\n    if not species:\n        raise ValueError(\"species not supported (hsa, mmu, rno, dre, cel, dme): %s\" % sps)\n    if not lib:\n        raise ValueError(\"-lib option is not set up in resources for mirge tool.\"\n                         \" Read above warnings lines.\")\n\n    if not utils.file_exists(out_dir):\n        with tx_tmpdir() as tmp_dir:\n            sample_file = _create_sample_file(data, tmp_dir)\n            do.run(_cmd().format(**locals()), \"Running miRge2.0.\")\n            shutil.move(tmp_dir, out_dir)\n    return [os.path.abspath(fn) for fn in glob.glob(os.path.join(out_dir, \"*\", \"*\"))]", "response": "Proxy function to run the tool"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _create_sample_file(data, out_dir):\n    sample_file = os.path.join(out_dir, \"sample_file.txt\")\n    with open(sample_file, 'w') as outh:\n        for sample in data:\n            outh.write(sample[0][\"clean_fastq\"] + \"\\n\")\n    return sample_file", "response": "Create a sample file from the data list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the input sequencing files", "response": "def get_input_sequence_files(data, default=None):\n    \"\"\"\n    returns the input sequencing files, these can be single or paired FASTQ\n    files or BAM files\n    \"\"\"\n    if \"files\" not in data or data.get(\"files\") is None:\n        file1, file2 = None, None\n    elif len(data[\"files\"]) == 2:\n        file1, file2 = data[\"files\"]\n    else:\n        assert len(data[\"files\"]) == 1, data[\"files\"]\n        file1, file2 = data[\"files\"][0], None\n    return file1, file2"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving UMI for consensus based preparation.", "response": "def get_umi_consensus(data):\n    \"\"\"Retrieve UMI for consensus based preparation.\n\n    We specify this either as a separate fastq file or embedded\n    in the read name as `fastq_name`.`\n    \"\"\"\n    consensus_choices = ([\"fastq_name\"])\n    umi = tz.get_in([\"config\", \"algorithm\", \"umi_type\"], data)\n    # don't run consensus UMI calling for scrna-seq\n    if tz.get_in([\"analysis\"], data, \"\").lower() == \"scrna-seq\":\n        return False\n    if umi and (umi in consensus_choices or os.path.exists(umi)):\n        assert tz.get_in([\"config\", \"algorithm\", \"mark_duplicates\"], data, True), \\\n            \"Using consensus UMI inputs requires marking duplicates\"\n        return umi"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_dexseq_gff(config, default=None):\n    dexseq_gff = tz.get_in(tz.get_in(['dexseq_gff', 'keys'], LOOKUPS, {}),\n                           config, None)\n    if not dexseq_gff:\n        return None\n    gtf_file = get_gtf_file(config)\n    if gtf_file:\n        base_dir = os.path.dirname(gtf_file)\n    else:\n        base_dir = os.path.dirname(dexseq_gff)\n    base, _ = os.path.splitext(dexseq_gff)\n    gff_file = os.path.join(base_dir, base + \".gff\")\n    if file_exists(gff_file):\n        return gff_file\n    gtf_file = os.path.join(base_dir, base + \".gff3\")\n    if file_exists(gtf_file):\n        return gtf_file\n    else:\n        return None", "response": "Get the DEXseq gff file for a single item"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the value of a global option if it exists otherwise None", "response": "def get_in_samples(samples, fn):\n    \"\"\"\n    for a list of samples, return the value of a global option\n    \"\"\"\n    for sample in samples:\n        sample = to_single_data(sample)\n        if fn(sample, None):\n            return fn(sample)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate summary_qc with a new section keyed by key.", "response": "def update_summary_qc(data, key, base=None, secondary=None):\n    \"\"\"\n    updates summary_qc with a new section, keyed by key.\n    stick files into summary_qc if you want them propagated forward\n    and available for multiqc\n    \"\"\"\n    summary = get_summary_qc(data, {})\n    if base and secondary:\n        summary[key] = {\"base\": base, \"secondary\": secondary}\n    elif base:\n        summary[key] = {\"base\": base}\n    elif secondary:\n        summary[key] = {\"secondary\": secondary}\n    data = set_summary_qc(data, summary)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef has_variantcalls(data):\n    analysis = get_analysis(data).lower()\n    variant_pipeline = analysis.startswith((\"standard\", \"variant\", \"variant2\"))\n    variantcaller = get_variantcaller(data)\n    return variant_pipeline or variantcaller", "response": "Returns True if the data dictionary contains a variant calling for the current analysis."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nestimate library complexity from the number of reads vs. number of unique start sites", "response": "def estimate_library_complexity(df, algorithm=\"RNA-seq\"):\n    \"\"\"\n    estimate library complexity from the number of reads vs.\n    number of unique start sites. returns \"NA\" if there are\n    not enough data points to fit the line\n    \"\"\"\n    DEFAULT_CUTOFFS = {\"RNA-seq\": (0.25, 0.40)}\n    cutoffs = DEFAULT_CUTOFFS[algorithm]\n    if len(df) < 5:\n        return {\"unique_starts_per_read\": 'nan',\n                \"complexity\": \"NA\"}\n    model = sm.ols(formula=\"starts ~ reads\", data=df)\n    fitted = model.fit()\n    slope = fitted.params[\"reads\"]\n    if slope <= cutoffs[0]:\n        complexity = \"LOW\"\n    elif slope <= cutoffs[1]:\n        complexity = \"MEDIUM\"\n    else:\n        complexity = \"HIGH\"\n\n    # for now don't return the complexity flag\n    return {\"Unique Starts Per Read\": float(slope)}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the details of a run in the Galaxy API.", "response": "def run_details(self, run_bc, run_date=None):\n        \"\"\"Next Gen LIMS specific API functionality.\n        \"\"\"\n        try:\n            details = self._get(\"/nglims/api_run_details\", dict(run=run_bc))\n        except ValueError:\n            raise ValueError(\"Could not find information in Galaxy for run: %s\" % run_bc)\n        if \"error\" in details and run_date is not None:\n            try:\n                details = self._get(\"/nglims/api_run_details\", dict(run=run_date))\n            except ValueError:\n                raise ValueError(\"Could not find information in Galaxy for run: %s\" % run_date)\n        return details"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the next Gen LIMS SQN report.", "response": "def sqn_report(self, start_date, end_date):\n        \"\"\"Next Gen LIMS: report of items sequenced in a time period.\n        \"\"\"\n        return self._get(\"/nglims/api_sqn_report\",\n                dict(start=start_date, end=end_date))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfix read group in a file using samtools addreplacerg.", "response": "def fixrg(in_bam, names, ref_file, dirs, data):\n    \"\"\"Fix read group in a file, using samtools addreplacerg.\n\n    addreplacerg does not remove the old read group, causing confusion when\n    checking. We use reheader to work around this\n    \"\"\"\n    work_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"bamclean\", dd.get_sample_name(data)))\n    out_file = os.path.join(work_dir, \"%s-fixrg.bam\" % utils.splitext_plus(os.path.basename(in_bam))[0])\n    if not utils.file_exists(out_file):\n        out_file = os.path.join(work_dir, \"%s-fixrg.bam\" % dd.get_sample_name(data))\n    if not utils.file_uptodate(out_file, in_bam):\n        with file_transaction(data, out_file) as tx_out_file:\n            rg_info = novoalign.get_rg_info(names)\n            new_header = \"%s-header.txt\" % os.path.splitext(out_file)[0]\n            cores = dd.get_cores(data)\n            do.run(\"samtools view -H {in_bam} | grep -v ^@RG > {new_header}\".format(**locals()),\n                   \"Create empty RG header: %s\" % dd.get_sample_name(data))\n            cmd = (\"samtools reheader {new_header} {in_bam} | \"\n                   \"samtools addreplacerg -@ {cores} -r '{rg_info}' -m overwrite_all -O bam -o {tx_out_file} -\")\n            do.run(cmd.format(**locals()), \"Fix read groups: %s\" % dd.get_sample_name(data))\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves extra contigs from an input BAM file.", "response": "def remove_extracontigs(in_bam, data):\n    \"\"\"Remove extra contigs (non chr1-22,X,Y) from an input BAM.\n\n    These extra contigs can often be arranged in different ways, causing\n    incompatibility issues with GATK and other tools. This also fixes the\n    read group header as in fixrg.\n\n    This does not yet handle mapping over 1 -> chr1 issues since this requires\n    a ton of search/replace which slows down conversion.\n    \"\"\"\n    work_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"bamclean\", dd.get_sample_name(data)))\n    out_file = os.path.join(work_dir, \"%s-noextras.bam\" % utils.splitext_plus(os.path.basename(in_bam))[0])\n    if not utils.file_exists(out_file):\n        out_file = os.path.join(work_dir, \"%s-noextras.bam\" % dd.get_sample_name(data))\n    if not utils.file_uptodate(out_file, in_bam):\n        with file_transaction(data, out_file) as tx_out_file:\n            target_chroms = _target_chroms_and_header(in_bam, data)\n            str_chroms = \" \".join(target_chroms)\n            rg_info = novoalign.get_rg_info(data[\"rgnames\"])\n            bcbio_py = sys.executable\n            ref_file = dd.get_ref_file(data)\n            local_bam = os.path.join(os.path.dirname(tx_out_file), os.path.basename(in_bam))\n            cores = dd.get_cores(data)\n            utils.symlink_plus(in_bam, local_bam)\n            bam.index(local_bam, data[\"config\"])\n            cmd = (\"samtools view -@ {cores} -h {local_bam} {str_chroms} | \"\n                   \"\"\"{bcbio_py} -c 'from bcbio.pipeline import cleanbam; \"\"\"\n                   \"\"\"cleanbam.fix_header(\"{ref_file}\")' | \"\"\"\n                   \"samtools view -@ {cores} -u - | \"\n                   \"samtools addreplacerg -@ {cores} -r '{rg_info}' -m overwrite_all -O bam -o {tx_out_file} - \")\n            do.run(cmd.format(**locals()), \"bamprep, remove extra contigs: %s\" % dd.get_sample_name(data))\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _target_chroms_and_header(bam_file, data):\n    special_remaps = {\"chrM\": \"MT\", \"MT\": \"chrM\"}\n    target_chroms = dict([(x.name, i) for i, x in enumerate(ref.file_contigs(dd.get_ref_file(data)))\n                          if chromhacks.is_autosomal_or_sex(x.name)])\n    out_chroms = []\n    with pysam.Samfile(bam_file, \"rb\") as bamfile:\n        for bami, bam_contig in enumerate([c[\"SN\"] for c in bamfile.header[\"SQ\"]]):\n            if bam_contig in target_chroms:\n                target_chrom = bam_contig\n            elif bam_contig in special_remaps and special_remaps[bam_contig] in target_chroms:\n                target_chrom = special_remaps[bam_contig]\n            elif bam_contig.startswith(\"chr\") and bam_contig.replace(\"chr\", \"\") in target_chroms:\n                target_chrom = bam_contig.replace(\"chr\", \"\")\n            elif \"chr%s\" % bam_contig in target_chroms:\n                target_chrom = \"chr%s\" % bam_contig\n            else:\n                target_chrom = None\n            # target_chrom == bam_contig ensures we don't try chr1 -> 1 style remapping\n            if target_chrom and target_chrom == bam_contig:\n                # Order not required if dealing with SAM file header fixing\n                #assert bami == target_chroms[target_chrom], \\\n                #    (\"remove_extracontigs: Non-matching order of standard contig: %s %s (%s vs %s)\" %\n                #     (bam_file, target_chrom, bami, target_chroms[target_chrom]))\n                out_chroms.append(target_chrom)\n    assert out_chroms, (\"remove_extracontigs: Did not find any chromosomes in reference file: %s %s\" %\n                        (bam_file, target_chroms))\n    return out_chroms", "response": "Get a list of chromosomes to target and new updated ref_file header."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef picard_prep(in_bam, names, ref_file, dirs, data):\n    runner = broad.runner_from_path(\"picard\", data[\"config\"])\n    work_dir = utils.safe_makedir(os.path.join(dirs[\"work\"], \"bamclean\", names[\"sample\"]))\n    runner.run_fn(\"picard_index_ref\", ref_file)\n    reorder_bam = os.path.join(work_dir, \"%s-reorder.bam\" %\n                               os.path.splitext(os.path.basename(in_bam))[0])\n    if not utils.file_exists(reorder_bam):\n        reorder_bam = os.path.join(work_dir, \"%s-reorder.bam\" % dd.get_sample_name(data))\n    reorder_bam = runner.run_fn(\"picard_reorder\", in_bam, ref_file, reorder_bam)\n    rg_bam = runner.run_fn(\"picard_fix_rgs\", reorder_bam, names)\n    return _filter_bad_reads(rg_bam, ref_file, data)", "response": "Prepare input BAM using Picard and GATK cleaning tools."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nuses GATK filter to remove problem reads which choke GATK and Picard.", "response": "def _filter_bad_reads(in_bam, ref_file, data):\n    \"\"\"Use GATK filter to remove problem reads which choke GATK and Picard.\n    \"\"\"\n    bam.index(in_bam, data[\"config\"])\n    out_file = \"%s-gatkfilter.bam\" % os.path.splitext(in_bam)[0]\n    if not utils.file_exists(out_file):\n        with tx_tmpdir(data) as tmp_dir:\n            with file_transaction(data, out_file) as tx_out_file:\n                params = [(\"FixMisencodedBaseQualityReads\"\n                           if dd.get_quality_format(data, \"\").lower() == \"illumina\"\n                           else \"PrintReads\"),\n                          \"-R\", ref_file,\n                          \"-I\", in_bam,\n                          \"-O\", tx_out_file,\n                          \"-RF\", \"MatchingBasesAndQualsReadFilter\",\n                          \"-RF\", \"SeqIsStoredReadFilter\",\n                          \"-RF\", \"CigarContainsNoNOperator\"]\n                jvm_opts = broad.get_gatk_opts(data[\"config\"], tmp_dir)\n                do.run(broad.gatk_cmd(\"gatk\", jvm_opts, params), \"Filter problem reads\")\n    bam.index(out_file, data[\"config\"])\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef qc_to_rec(samples):\n    samples = [utils.to_single_data(x) for x in samples]\n    samples = cwlutils.assign_complex_to_samples(samples)\n    to_analyze, extras = _split_samples_by_qc(samples)\n    recs = cwlutils.samples_to_records([utils.to_single_data(x) for x in to_analyze + extras])\n    return [[x] for x in recs]", "response": "CWL : Convert a set of input samples into records for parallelization."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a list of summary information for alignment and variant calling.", "response": "def generate_parallel(samples, run_parallel):\n    \"\"\"Provide parallel preparation of summary information for alignment and variant calling.\n    \"\"\"\n    to_analyze, extras = _split_samples_by_qc(samples)\n    qced = run_parallel(\"pipeline_summary\", to_analyze)\n    samples = _combine_qc_samples(qced) + extras\n    qsign_info = run_parallel(\"qsignature_summary\", [samples])\n    metadata_file = _merge_metadata([samples])\n    summary_file = write_project_summary(samples, qsign_info)\n    out = []\n    for data in samples:\n        if \"summary\" not in data[0]:\n            data[0][\"summary\"] = {}\n        data[0][\"summary\"][\"project\"] = summary_file\n        data[0][\"summary\"][\"metadata\"] = metadata_file\n        if qsign_info:\n            data[0][\"summary\"][\"mixup_check\"] = qsign_info[0][\"out_dir\"]\n        out.append(data)\n    out = _add_researcher_summary(out, summary_file)\n    # MultiQC must be run after all file outputs are set:\n    return [[utils.to_single_data(d)] for d in run_parallel(\"multiqc_summary\", [out])]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprovide summary information on processing sample.", "response": "def pipeline_summary(data):\n    \"\"\"Provide summary information on processing sample.\n\n    Handles standard and CWL (single QC output) cases.\n    \"\"\"\n    data = utils.to_single_data(data)\n    work_bam = dd.get_align_bam(data) or dd.get_work_bam(data)\n    if not work_bam or not work_bam.endswith(\".bam\"):\n        work_bam = None\n    if dd.get_ref_file(data):\n        if work_bam or (tz.get_in([\"config\", \"algorithm\", \"kraken\"], data)):  # kraken doesn't need bam\n            logger.info(\"QC: %s %s\" % (dd.get_sample_name(data), \", \".join(dd.get_algorithm_qc(data))))\n            work_data = cwlutils.unpack_tarballs(utils.deepish_copy(data), data)\n            data[\"summary\"] = _run_qc_tools(work_bam, work_data)\n            if (len(dd.get_algorithm_qc(data)) == 1 and \"output_cwl_keys\" in data):\n                data[\"summary\"][\"qc\"] = data[\"summary\"][\"qc\"].get(dd.get_algorithm_qc(data)[0])\n    return [[data]]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_qc_tools(data):\n    if dd.get_algorithm_qc(data):\n        return dd.get_algorithm_qc(data)\n    analysis = data[\"analysis\"].lower()\n    to_run = []\n    if tz.get_in([\"config\", \"algorithm\", \"kraken\"], data):\n        to_run.append(\"kraken\")\n    if \"fastqc\" not in dd.get_tools_off(data):\n        to_run.append(\"fastqc\")\n    if any([tool in dd.get_tools_on(data)\n            for tool in [\"qualimap\", \"qualimap_full\"]]):\n        to_run.append(\"qualimap\")\n    if analysis.startswith(\"rna-seq\") or analysis == \"smallrna-seq\":\n        if \"qualimap\" not in dd.get_tools_off(data):\n            if gtf.is_qualimap_compatible(dd.get_gtf_file(data)):\n                to_run.append(\"qualimap_rnaseq\")\n            else:\n                logger.debug(\"GTF not compatible with Qualimap, skipping.\")\n    if analysis.startswith(\"chip-seq\"):\n        to_run.append(\"chipqc\")\n    if analysis.startswith(\"smallrna-seq\"):\n        to_run.append(\"small-rna\")\n        to_run.append(\"atropos\")\n    if \"coverage_qc\" not in dd.get_tools_off(data):\n        to_run.append(\"samtools\")\n    if dd.has_variantcalls(data):\n        if \"coverage_qc\" not in dd.get_tools_off(data):\n            to_run += [\"coverage\", \"picard\"]\n        to_run += [\"qsignature\", \"variants\"]\n        if vcfanno.is_human(data):\n            to_run += [\"contamination\", \"peddy\"]\n        if vcfutils.get_paired_phenotype(data):\n            to_run += [\"viral\"]\n        if damage.should_filter([data]):\n            to_run += [\"damage\"]\n    if dd.get_umi_consensus(data):\n        to_run += [\"umi\"]\n    if tz.get_in([\"config\", \"algorithm\", \"preseq\"], data):\n        to_run.append(\"preseq\")\n    to_run = [tool for tool in to_run if tool not in dd.get_tools_off(data)]\n    to_run.sort()\n    return to_run", "response": "Retrieve a list of QC tools to use based on configuration and analysis type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning a set of third party quality control tools returning QC directory and metrics.", "response": "def _run_qc_tools(bam_file, data):\n    \"\"\"Run a set of third party quality control tools, returning QC directory and metrics.\n\n        :param bam_file: alignments in bam format\n        :param data: dict with all configuration information\n\n        :returns: dict with output of different tools\n    \"\"\"\n    from bcbio.qc import (atropos, contamination, coverage, damage, fastqc, kraken,\n                          qsignature, qualimap, samtools, picard, srna, umi, variant,\n                          viral, preseq, chipseq)\n    tools = {\"fastqc\": fastqc.run,\n             \"atropos\": atropos.run,\n             \"small-rna\": srna.run,\n             \"samtools\": samtools.run,\n             \"qualimap\": qualimap.run,\n             \"qualimap_rnaseq\": qualimap.run_rnaseq,\n             \"qsignature\": qsignature.run,\n             \"contamination\": contamination.run,\n             \"coverage\": coverage.run,\n             \"damage\": damage.run,\n             \"variants\": variant.run,\n             \"peddy\": peddy.run_qc,\n             \"kraken\": kraken.run,\n             \"picard\": picard.run,\n             \"umi\": umi.run,\n             \"viral\": viral.run,\n             \"preseq\": preseq.run,\n             \"chipqc\": chipseq.run\n             }\n    qc_dir = utils.safe_makedir(os.path.join(data[\"dirs\"][\"work\"], \"qc\", data[\"description\"]))\n    metrics = {}\n    qc_out = utils.deepish_copy(dd.get_summary_qc(data))\n    for program_name in dd.get_algorithm_qc(data):\n        if not bam_file and program_name != \"kraken\":  # kraken doesn't need bam\n            continue\n        if dd.get_phenotype(data) == \"germline\" and program_name != \"variants\":\n            continue\n        qc_fn = tools[program_name]\n        cur_qc_dir = os.path.join(qc_dir, program_name)\n        out = qc_fn(bam_file, data, cur_qc_dir)\n        qc_files = None\n        if out and isinstance(out, dict):\n            # Check for metrics output, two cases:\n            # 1. output with {\"metrics\"} and files (\"base\")\n            if \"metrics\" in out:\n                metrics.update(out.pop(\"metrics\"))\n            # 2. a dictionary of metrics\n            elif \"base\" not in out:\n                metrics.update(out)\n            # Check for files only output\n            if \"base\" in out:\n                qc_files = out\n        elif out and isinstance(out, six.string_types) and os.path.exists(out):\n            qc_files = {\"base\": out, \"secondary\": []}\n        if not qc_files:\n            qc_files = _organize_qc_files(program_name, cur_qc_dir)\n        if qc_files:\n            qc_out[program_name] = qc_files\n\n    metrics[\"Name\"] = dd.get_sample_name(data)\n    metrics[\"Quality format\"] = dd.get_quality_format(data).lower()\n    return {\"qc\": qc_out, \"metrics\": metrics}"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\norganizes outputs from quality control runs into a base file and secondary file. Returns None if no files created during processing.", "response": "def _organize_qc_files(program, qc_dir):\n    \"\"\"Organize outputs from quality control runs into a base file and secondary outputs.\n\n    Provides compatibility with CWL output. Returns None if no files created during processing.\n    \"\"\"\n    base_files = {\"fastqc\": \"fastqc_report.html\",\n                  \"qualimap_rnaseq\": \"qualimapReport.html\",\n                  \"qualimap\": \"qualimapReport.html\"}\n    if os.path.exists(qc_dir):\n        out_files = []\n        for fname in [os.path.join(qc_dir, x) for x in os.listdir(qc_dir)]:\n            if os.path.isfile(fname) and not fname.endswith(\".bcbiotmp\"):\n                out_files.append(fname)\n            elif os.path.isdir(fname) and not fname.endswith(\"tx\"):\n                for root, dirs, files in os.walk(fname):\n                    for f in files:\n                        if not f.endswith(\".bcbiotmp\"):\n                            out_files.append(os.path.join(root, f))\n        if len(out_files) > 0 and all([not f.endswith(\"-failed.log\") for f in out_files]):\n            if len(out_files) == 1:\n                base = out_files[0]\n                secondary = []\n            else:\n                base = None\n                if program in base_files:\n                    base_choices = [x for x in out_files if x.endswith(\"/%s\" % base_files[program])]\n                    if len(base_choices) == 1:\n                        base = base_choices[0]\n                if not base:\n                    base = out_files[0]\n                secondary = [x for x in out_files if x != base]\n            return {\"base\": base, \"secondary\": secondary}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsplit data into individual quality control steps for a run.", "response": "def _split_samples_by_qc(samples):\n    \"\"\"Split data into individual quality control steps for a run.\n    \"\"\"\n    to_process = []\n    extras = []\n    for data in [utils.to_single_data(x) for x in samples]:\n        qcs = dd.get_algorithm_qc(data)\n        # kraken doesn't need bam\n        if qcs and (dd.get_align_bam(data) or dd.get_work_bam(data) or\n                    tz.get_in([\"config\", \"algorithm\", \"kraken\"], data)):\n            for qc in qcs:\n                add = copy.deepcopy(data)\n                add[\"config\"][\"algorithm\"][\"qc\"] = [qc]\n                to_process.append([add])\n        else:\n            extras.append([data])\n    return to_process, extras"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncombining split QC analyses into single samples based on BAM files.", "response": "def _combine_qc_samples(samples):\n    \"\"\"Combine split QC analyses into single samples based on BAM files.\n    \"\"\"\n    by_bam = collections.defaultdict(list)\n    for data in [utils.to_single_data(x) for x in samples]:\n        batch = dd.get_batch(data) or dd.get_sample_name(data)\n        if not isinstance(batch, (list, tuple)):\n            batch = [batch]\n        batch = tuple(batch)\n        by_bam[(dd.get_align_bam(data) or dd.get_work_bam(data), batch)].append(data)\n    out = []\n    for data_group in by_bam.values():\n        data = data_group[0]\n        alg_qc = []\n        qc = {}\n        metrics = {}\n        for d in data_group:\n            qc.update(dd.get_summary_qc(d))\n            metrics.update(dd.get_summary_metrics(d))\n            alg_qc.extend(dd.get_algorithm_qc(d))\n        data[\"config\"][\"algorithm\"][\"qc\"] = alg_qc\n        data[\"summary\"][\"qc\"] = qc\n        data[\"summary\"][\"metrics\"] = metrics\n        out.append([data])\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite project summary information on the provided samples.", "response": "def write_project_summary(samples, qsign_info=None):\n    \"\"\"Write project summary information on the provided samples.\n    write out dirs, genome resources,\n\n    \"\"\"\n    work_dir = samples[0][0][\"dirs\"][\"work\"]\n    out_file = os.path.join(work_dir, \"project-summary.yaml\")\n    upload_dir = (os.path.join(work_dir, samples[0][0][\"upload\"][\"dir\"])\n                  if \"dir\" in samples[0][0][\"upload\"] else \"\")\n    date = str(datetime.now())\n    prev_samples = _other_pipeline_samples(out_file, samples)\n    with open(out_file, \"w\") as out_handle:\n        yaml.safe_dump({\"date\": date}, out_handle,\n                       default_flow_style=False, allow_unicode=False)\n        if qsign_info:\n            qsign_out = utils.deepish_copy(qsign_info[0])\n            qsign_out.pop(\"out_dir\", None)\n            yaml.safe_dump({\"qsignature\": qsign_out}, out_handle, default_flow_style=False,\n                           allow_unicode=False)\n        yaml.safe_dump({\"upload\": upload_dir}, out_handle,\n                       default_flow_style=False, allow_unicode=False)\n        yaml.safe_dump({\"bcbio_system\": samples[0][0][\"config\"].get(\"bcbio_system\", \"\")}, out_handle,\n                       default_flow_style=False, allow_unicode=False)\n        yaml.safe_dump({\"samples\": prev_samples + [_save_fields(sample[0]) for sample in samples]}, out_handle,\n                       default_flow_style=False, allow_unicode=False)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmerge all metadata into a single CSV file", "response": "def _merge_metadata(samples):\n    \"\"\"Merge all metadata into CSV file\"\"\"\n    samples = list(utils.flatten(samples))\n    out_dir = dd.get_work_dir(samples[0])\n    logger.info(\"summarize metadata\")\n    out_file = os.path.join(out_dir, \"metadata.csv\")\n    sample_metrics = collections.defaultdict(dict)\n    for s in samples:\n        m = tz.get_in(['metadata'], s)\n        if isinstance(m, six.string_types):\n            m = json.loads(m)\n        if m:\n            for me in list(m.keys()):\n                if isinstance(m[me], list) or isinstance(m[me], dict) or isinstance(m[me], tuple):\n                    m.pop(me, None)\n            sample_metrics[dd.get_sample_name(s)].update(m)\n    pd.DataFrame(sample_metrics).transpose().to_csv(out_file)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve samples produced by another pipeline in the summary output.", "response": "def _other_pipeline_samples(summary_file, cur_samples):\n    \"\"\"Retrieve samples produced previously by another pipeline in the summary output.\n    \"\"\"\n    cur_descriptions = set([s[0][\"description\"] for s in cur_samples])\n    out = []\n    if utils.file_exists(summary_file):\n        with open(summary_file) as in_handle:\n            for s in yaml.safe_load(in_handle).get(\"samples\", []):\n                if s[\"description\"] not in cur_descriptions:\n                    out.append(s)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates summary files per researcher if organized via a LIMS.", "response": "def _add_researcher_summary(samples, summary_yaml):\n    \"\"\"Generate summary files per researcher if organized via a LIMS.\n    \"\"\"\n    by_researcher = collections.defaultdict(list)\n    for data in (x[0] for x in samples):\n        researcher = utils.get_in(data, (\"upload\", \"researcher\"))\n        if researcher:\n            by_researcher[researcher].append(data[\"description\"])\n    out_by_researcher = {}\n    for researcher, descrs in by_researcher.items():\n        out_by_researcher[researcher] = _summary_csv_by_researcher(summary_yaml, researcher,\n                                                                   set(descrs), samples[0][0])\n    out = []\n    for data in (x[0] for x in samples):\n        researcher = utils.get_in(data, (\"upload\", \"researcher\"))\n        if researcher:\n            data[\"summary\"][\"researcher\"] = out_by_researcher[researcher]\n        out.append([data])\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a CSV file with summary information for a researcher on this project.", "response": "def _summary_csv_by_researcher(summary_yaml, researcher, descrs, data):\n    \"\"\"Generate a CSV file with summary information for a researcher on this project.\n    \"\"\"\n    out_file = os.path.join(utils.safe_makedir(os.path.join(data[\"dirs\"][\"work\"], \"researcher\")),\n                            \"%s-summary.tsv\" % run_info.clean_name(researcher))\n    metrics = [\"Total_reads\", \"Mapped_reads\", \"Mapped_reads_pct\", \"Duplicates\", \"Duplicates_pct\"]\n    with open(summary_yaml) as in_handle:\n        with open(out_file, \"w\") as out_handle:\n            writer = csv.writer(out_handle, dialect=\"excel-tab\")\n            writer.writerow([\"Name\"] + metrics)\n            for sample in yaml.safe_load(in_handle)[\"samples\"]:\n                if sample[\"description\"] in descrs:\n                    row = [sample[\"description\"]] + [utils.get_in(sample, (\"summary\", \"metrics\", x), \"\")\n                                                     for x in metrics]\n                    writer.writerow(row)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates PDF from HTML summary outputs in QC directory.", "response": "def prep_pdf(qc_dir, config):\n    \"\"\"Create PDF from HTML summary outputs in QC directory.\n\n    Requires wkhtmltopdf installed: http://www.msweet.org/projects.php?Z1\n    Thanks to: https://www.biostars.org/p/16991/\n\n    Works around issues with CSS conversion on CentOS by adjusting CSS.\n    \"\"\"\n    html_file = os.path.join(qc_dir, \"fastqc\", \"fastqc_report.html\")\n    html_fixed = \"%s-fixed%s\" % os.path.splitext(html_file)\n    try:\n        topdf = config_utils.get_program(\"wkhtmltopdf\", config)\n    except config_utils.CmdNotFound:\n        topdf = None\n    if topdf and utils.file_exists(html_file):\n        out_file = \"%s.pdf\" % os.path.splitext(html_file)[0]\n        if not utils.file_exists(out_file):\n            cmd = (\"sed 's/div.summary/div.summary-no/' %s | sed 's/div.main/div.main-no/' > %s\"\n                   % (html_file, html_fixed))\n            do.run(cmd, \"Fix fastqc CSS to be compatible with wkhtmltopdf\")\n            cmd = [topdf, html_fixed, out_file]\n            do.run(cmd, \"Convert QC HTML to PDF\")\n        return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract signatures and mutational burdens from PureCN rds file.", "response": "def _run_purecn_dx(out, paired):\n    \"\"\"Extract signatures and mutational burdens from PureCN rds file.\n    \"\"\"\n    out_base, out, all_files = _get_purecn_dx_files(paired, out)\n    if not utils.file_uptodate(out[\"mutation_burden\"], out[\"rds\"]):\n        with file_transaction(paired.tumor_data, out_base) as tx_out_base:\n            cmd = [\"PureCN_Dx.R\", \"--rds\", out[\"rds\"], \"--callable\", dd.get_sample_callable(paired.tumor_data),\n                   \"--signatures\", \"--out\", tx_out_base]\n            do.run(cmd, \"PureCN Dx mutational burden and signatures\")\n            for f in all_files:\n                if os.path.exists(os.path.join(os.path.dirname(tx_out_base), f)):\n                    shutil.move(os.path.join(os.path.dirname(tx_out_base), f),\n                                os.path.join(os.path.dirname(out_base), f))\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_purecn_dx_files(paired, out):\n    out_base = \"%s-dx\" % utils.splitext_plus(out[\"rds\"])[0]\n    all_files = []\n    for key, ext in [[(\"mutation_burden\",), \"_mutation_burden.csv\"],\n                     [(\"plot\", \"signatures\"), \"_signatures.pdf\"],\n                     [(\"signatures\",), \"_signatures.csv\"]]:\n        cur_file = \"%s%s\" % (out_base, ext)\n        out = tz.update_in(out, key, lambda x: cur_file)\n        all_files.append(os.path.basename(cur_file))\n    return out_base, out, all_files", "response": "Retrieve files generated by PureCN_Dx\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _run_purecn(paired, work_dir):\n    segfns = {\"cnvkit\": _segment_normalized_cnvkit, \"gatk-cnv\": _segment_normalized_gatk}\n    out_base, out, all_files = _get_purecn_files(paired, work_dir)\n    failed_file = out_base + \"-failed.log\"\n    cnr_file = tz.get_in([\"depth\", \"bins\", \"normalized\"], paired.tumor_data)\n    if not utils.file_uptodate(out[\"rds\"], cnr_file) and not utils.file_exists(failed_file):\n        cnr_file, seg_file = segfns[cnvkit.bin_approach(paired.tumor_data)](cnr_file, work_dir, paired)\n        from bcbio import heterogeneity\n        vcf_file = heterogeneity.get_variants(paired.tumor_data, include_germline=False)[0][\"vrn_file\"]\n        vcf_file = germline.filter_to_pass_and_reject(vcf_file, paired, out_dir=work_dir)\n        with file_transaction(paired.tumor_data, out_base) as tx_out_base:\n            # Use UCSC style naming for human builds to support BSgenome\n            genome = (\"hg19\" if dd.get_genome_build(paired.tumor_data) in [\"GRCh37\", \"hg19\"]\n                      else dd.get_genome_build(paired.tumor_data))\n            cmd = [\"PureCN.R\", \"--seed\", \"42\", \"--out\", tx_out_base, \"--rds\", \"%s.rds\" % tx_out_base,\n                   \"--sampleid\", dd.get_sample_name(paired.tumor_data),\n                   \"--genome\", genome,\n                   \"--vcf\", vcf_file, \"--tumor\", cnr_file,\n                   \"--segfile\", seg_file, \"--funsegmentation\", \"Hclust\", \"--maxnonclonal\", \"0.3\"]\n            if dd.get_num_cores(paired.tumor_data) > 1:\n                cmd += [\"--cores\", str(dd.get_num_cores(paired.tumor_data))]\n            try:\n                cmd = \"export R_LIBS_USER=%s && %s && %s\" % (utils.R_sitelib(), utils.get_R_exports(),\n                                                             \" \".join([str(x) for x in cmd]))\n                do.run(cmd, \"PureCN copy number calling\")\n            except subprocess.CalledProcessError as msg:\n                if _allowed_errors(str(msg)):\n                    logger.info(\"PureCN failed to find solution for %s: skipping\" %\n                                dd.get_sample_name(paired.tumor_data))\n                    with open(failed_file, \"w\") as out_handle:\n                        out_handle.write(str(msg))\n                else:\n                    logger.exception()\n                    raise\n            for f in all_files:\n                if os.path.exists(os.path.join(os.path.dirname(tx_out_base), f)):\n                    shutil.move(os.path.join(os.path.dirname(tx_out_base), f),\n                                os.path.join(os.path.dirname(out_base), f))\n    out = _get_purecn_files(paired, work_dir, require_exist=True)[1]\n    return out if (out.get(\"rds\") and os.path.exists(out[\"rds\"])) else None", "response": "Run PureCN. R wrapper with pre - segmented CNVkit or GATK4 inputs."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove regions that overlap with next region.", "response": "def _remove_overlaps(in_file, out_dir, data):\n    \"\"\"Remove regions that overlap with next region, these result in issues with PureCN.\n    \"\"\"\n    out_file = os.path.join(out_dir, \"%s-nooverlaps%s\" % utils.splitext_plus(os.path.basename(in_file)))\n    if not utils.file_uptodate(out_file, in_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            with open(in_file) as in_handle:\n                with open(tx_out_file, \"w\") as out_handle:\n                    prev_line = None\n                    for line in in_handle:\n                        if prev_line:\n                            pchrom, pstart, pend = prev_line.split(\"\\t\", 4)[:3]\n                            cchrom, cstart, cend = line.split(\"\\t\", 4)[:3]\n                            # Skip if chromosomes match and end overlaps start\n                            if pchrom == cchrom and int(pend) > int(cstart):\n                                pass\n                            else:\n                                out_handle.write(prev_line)\n                        prev_line = line\n                    out_handle.write(prev_line)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_purecn_files(paired, work_dir, require_exist=False):\n    out_base = os.path.join(work_dir, \"%s-purecn\" % (dd.get_sample_name(paired.tumor_data)))\n    out = {\"plot\": {}}\n    all_files = []\n    for plot in [\"chromosomes\", \"local_optima\", \"segmentation\", \"summary\"]:\n        if plot == \"summary\":\n            cur_file = \"%s.pdf\" % out_base\n        else:\n            cur_file = \"%s_%s.pdf\" % (out_base, plot)\n        if not require_exist or os.path.exists(cur_file):\n            out[\"plot\"][plot] = cur_file\n            all_files.append(os.path.basename(cur_file))\n    for key, ext in [[\"hetsummary\", \".csv\"], [\"dnacopy\", \"_dnacopy.seg\"], [\"genes\", \"_genes.csv\"],\n                     [\"log\", \".log\"], [\"loh\", \"_loh.csv\"], [\"rds\", \".rds\"],\n                     [\"variants\", \"_variants.csv\"]]:\n        cur_file = \"%s%s\" % (out_base, ext)\n        if not require_exist or os.path.exists(cur_file):\n            out[key] = cur_file\n            all_files.append(os.path.basename(cur_file))\n    return out_base, out, all_files", "response": "Retrieve organized structure of PureCN output files."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts LOH output into standardized VCF.", "response": "def _loh_to_vcf(cur):\n    \"\"\"Convert LOH output into standardized VCF.\n    \"\"\"\n    cn = int(float(cur[\"C\"]))\n    minor_cn = int(float(cur[\"M\"]))\n    if cur[\"type\"].find(\"LOH\"):\n        svtype = \"LOH\"\n    elif cn > 2:\n        svtype = \"DUP\"\n    elif cn < 1:\n        svtype = \"DEL\"\n    else:\n        svtype = None\n    if svtype:\n        info = [\"SVTYPE=%s\" % svtype, \"END=%s\" % cur[\"end\"],\n                \"SVLEN=%s\" % (int(cur[\"end\"]) - int(cur[\"start\"])),\n                \"CN=%s\" % cn, \"MajorCN=%s\" % (cn - minor_cn), \"MinorCN=%s\" % minor_cn]\n        return [cur[\"chr\"], cur[\"start\"], \".\", \"N\", \"<%s>\" % svtype, \".\", \".\",\n                \";\".join(info), \"GT\", \"0/1\"]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate metrics files when missing.", "response": "def _generate_metrics(bam_fname, config_file, ref_file,\n                      bait_file, target_file):\n    \"\"\"Run Picard commands to generate metrics files when missing.\n    \"\"\"\n    with open(config_file) as in_handle:\n        config = yaml.safe_load(in_handle)\n    broad_runner = broad.runner_from_config(config)\n    bam_fname = os.path.abspath(bam_fname)\n    path = os.path.dirname(bam_fname)\n    out_dir = os.path.join(path, \"metrics\")\n    utils.safe_makedir(out_dir)\n    with utils.chdir(out_dir):\n        with tx_tmpdir() as tmp_dir:\n            cur_bam = os.path.basename(bam_fname)\n            if not os.path.exists(cur_bam):\n                os.symlink(bam_fname, cur_bam)\n            gen_metrics = PicardMetrics(broad_runner, tmp_dir)\n            gen_metrics.report(cur_bam, ref_file,\n                               _bam_is_paired(bam_fname),\n                               bait_file, target_file)\n    return out_dir"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetecting copy number variations from batched set of samples using GATK4 CNV calling.", "response": "def run(items, background=None):\n    \"\"\"Detect copy number variations from batched set of samples using GATK4 CNV calling.\n\n    TODO: implement germline calling with DetermineGermlineContigPloidy and GermlineCNVCaller\n    \"\"\"\n    if not background: background = []\n    paired = vcfutils.get_paired(items + background)\n    if paired:\n        out = _run_paired(paired)\n    else:\n        out = items\n        logger.warn(\"GATK4 CNV calling currently only available for somatic samples: %s\" %\n                    \", \".join([dd.get_sample_name(d) for d in items + background]))\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _run_paired(paired):\n    from bcbio.structural import titancna\n    work_dir = _sv_workdir(paired.tumor_data)\n    seg_files = model_segments(tz.get_in([\"depth\", \"bins\", \"normalized\"], paired.tumor_data),\n                               work_dir, paired)\n    call_file = call_copy_numbers(seg_files[\"seg\"], work_dir, paired.tumor_data)\n    out = []\n    if paired.normal_data:\n        out.append(paired.normal_data)\n    if \"sv\" not in paired.tumor_data:\n        paired.tumor_data[\"sv\"] = []\n    paired.tumor_data[\"sv\"].append({\"variantcaller\": \"gatk-cnv\",\n                                    \"call_file\": call_file,\n                                    \"vrn_file\": titancna.to_vcf(call_file, \"GATK4-CNV\", _get_seg_header,\n                                                                _seg_to_vcf, paired.tumor_data),\n                                    \"seg\": seg_files[\"seg\"],\n                                    \"plot\": plot_model_segments(seg_files, work_dir, paired.tumor_data)})\n    out.append(paired.tumor_data)\n    return out", "response": "Run somatic variant calling pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef call_copy_numbers(seg_file, work_dir, data):\n    out_file = os.path.join(work_dir, \"%s-call.seg\" % dd.get_sample_name(data))\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            params = [\"-T\", \"CallCopyRatioSegments\",\n                      \"-I\", seg_file, \"-O\", tx_out_file]\n            _run_with_memory_scaling(params, tx_out_file, data)\n    return out_file", "response": "Call copy numbers from a normalized and segmented input file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_model_segments(seg_files, work_dir, data):\n    from bcbio.heterogeneity import chromhacks\n    out_file = os.path.join(work_dir, \"%s.modeled.png\" % dd.get_sample_name(data))\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            dict_file = utils.splitext_plus(dd.get_ref_file(data))[0] + \".dict\"\n            plot_dict = os.path.join(os.path.dirname(tx_out_file), os.path.basename(dict_file))\n            with open(dict_file) as in_handle:\n                with open(plot_dict, \"w\") as out_handle:\n                    for line in in_handle:\n                        if line.startswith(\"@SQ\"):\n                            cur_chrom = [x.split(\":\", 1)[1].strip()\n                                         for x in line.split(\"\\t\") if x.startswith(\"SN:\")][0]\n                            if chromhacks.is_autosomal_or_sex(cur_chrom):\n                                out_handle.write(line)\n                        else:\n                            out_handle.write(line)\n            params = [\"-T\", \"PlotModeledSegments\",\n                      \"--denoised-copy-ratios\", tz.get_in([\"depth\", \"bins\", \"normalized\"], data),\n                      \"--segments\", seg_files[\"final_seg\"],\n                      \"--allelic-counts\", seg_files[\"tumor_hets\"],\n                      \"--sequence-dictionary\", plot_dict,\n                      \"--minimum-contig-length\", \"10\",\n                      \"--output-prefix\", dd.get_sample_name(data),\n                      \"-O\", os.path.dirname(tx_out_file)]\n            _run_with_memory_scaling(params, tx_out_file, data)\n    return {\"seg\": out_file}", "response": "Diagnostics of segmentation and inputs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef model_segments(copy_file, work_dir, paired):\n    out_file = os.path.join(work_dir, \"%s.cr.seg\" % dd.get_sample_name(paired.tumor_data))\n    tumor_counts, normal_counts = heterogzygote_counts(paired)\n    if not utils.file_exists(out_file):\n        with file_transaction(paired.tumor_data, out_file) as tx_out_file:\n            params = [\"-T\", \"ModelSegments\",\n                      \"--denoised-copy-ratios\", copy_file,\n                      \"--allelic-counts\", tumor_counts,\n                      \"--output-prefix\", dd.get_sample_name(paired.tumor_data),\n                      \"-O\", os.path.dirname(tx_out_file)]\n            if normal_counts:\n                params += [\"--normal-allelic-counts\", normal_counts]\n            _run_with_memory_scaling(params, tx_out_file, paired.tumor_data)\n            for tx_fname in glob.glob(os.path.join(os.path.dirname(tx_out_file),\n                                                   \"%s*\" % dd.get_sample_name(paired.tumor_data))):\n                shutil.copy(tx_fname, os.path.join(work_dir, os.path.basename(tx_fname)))\n    return {\"seg\": out_file, \"tumor_hets\": out_file.replace(\".cr.seg\", \".hets.tsv\"),\n            \"final_seg\": out_file.replace(\".cr.seg\", \".modelFinal.seg\")}", "response": "Perform segmentation on input copy number log2 ratio file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef denoise(data, pon, work_dir):\n    std_file = os.path.join(work_dir, \"%s-crstandardized.tsv\" % dd.get_sample_name(data))\n    denoise_file = os.path.join(work_dir, \"%s-crdenoised.tsv\" % dd.get_sample_name(data))\n    if not utils.file_exists(std_file):\n        with file_transaction(data, std_file, denoise_file) as (tx_std_file, tx_denoise_file):\n            params = [\"-T\", \"DenoiseReadCounts\",\n                      \"-I\", tz.get_in([\"depth\", \"bins\", \"target\"], data),\n                      \"--standardized-copy-ratios\", tx_std_file,\n                      \"--denoised-copy-ratios\", tx_denoise_file]\n            if pon:\n                params += [\"--count-panel-of-normals\", pon]\n            else:\n                params += [\"--annotated-intervals\", tz.get_in([\"regions\", \"bins\", \"gcannotated\"], data)]\n            _run_with_memory_scaling(params, tx_std_file, data)\n    return denoise_file if pon else std_file", "response": "Normalize read counts using panel of normal background or GC or mappability\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_panel_of_normals(items, group_id, work_dir):\n    out_file = os.path.join(work_dir, \"%s-%s-pon.hdf5\" % (dd.get_sample_name(items[0]), group_id))\n    if not utils.file_exists(out_file):\n        with file_transaction(items[0], out_file) as tx_out_file:\n            params = [\"-T\", \"CreateReadCountPanelOfNormals\",\n                      \"-O\", tx_out_file,\n                      \"--annotated-intervals\", tz.get_in([\"regions\", \"bins\", \"gcannotated\"], items[0])]\n            for data in items:\n                params += [\"-I\", tz.get_in([\"depth\", \"bins\", \"target\"], data)]\n            _run_with_memory_scaling(params, tx_out_file, items[0], ld_preload=True)\n    return out_file", "response": "Create a panel of normals from one or more background read counts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract BED intervals from a GATK4 hdf5 panel of normal file.", "response": "def pon_to_bed(pon_file, out_dir, data):\n    \"\"\"Extract BED intervals from a GATK4 hdf5 panel of normal file.\n    \"\"\"\n    out_file = os.path.join(out_dir, \"%s-intervals.bed\" % (utils.splitext_plus(os.path.basename(pon_file))[0]))\n    if not utils.file_uptodate(out_file, pon_file):\n        import h5py\n        with file_transaction(data, out_file) as tx_out_file:\n            with h5py.File(pon_file, \"r\") as f:\n                with open(tx_out_file, \"w\") as out_handle:\n                    intervals = f[\"original_data\"][\"intervals\"]\n                    for i in range(len(intervals[\"transposed_index_start_end\"][0])):\n                        chrom = intervals[\"indexed_contig_names\"][intervals[\"transposed_index_start_end\"][0][i]]\n                        start = int(intervals[\"transposed_index_start_end\"][1][i]) - 1\n                        end = int(intervals[\"transposed_index_start_end\"][2][i])\n                        out_handle.write(\"%s\\t%s\\t%s\\n\" % (chrom, start, end))\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prepare_intervals(data, region_file, work_dir):\n    target_file = os.path.join(work_dir, \"%s-target.interval_list\" % dd.get_sample_name(data))\n    if not utils.file_uptodate(target_file, region_file):\n        with file_transaction(data, target_file) as tx_out_file:\n            params = [\"-T\", \"PreprocessIntervals\", \"-R\", dd.get_ref_file(data),\n                      \"--interval-merging-rule\", \"OVERLAPPING_ONLY\",\n                      \"-O\", tx_out_file]\n            if dd.get_coverage_interval(data) == \"genome\":\n                params += [\"--bin-length\", \"1000\", \"--padding\", \"0\"]\n            else:\n                params += [\"-L\", region_file, \"--bin-length\", \"0\", \"--padding\", \"250\"]\n            _run_with_memory_scaling(params, tx_out_file, data)\n    return target_file", "response": "Prepare intervals for targeted and gene based regions."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef annotate_intervals(target_file, data):\n    out_file = \"%s-gcannotated.tsv\" % utils.splitext_plus(target_file)[0]\n    if not utils.file_uptodate(out_file, target_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            params = [\"-T\", \"AnnotateIntervals\", \"-R\", dd.get_ref_file(data),\n                      \"-L\", target_file,\n                      \"--interval-merging-rule\", \"OVERLAPPING_ONLY\",\n                      \"-O\", tx_out_file]\n            _run_with_memory_scaling(params, tx_out_file, data)\n    return out_file", "response": "Provide GC annotated intervals for error correction during panels and denoising."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncounts reads in defined bins using CollectReadCounts.", "response": "def collect_read_counts(data, work_dir):\n    \"\"\"Count reads in defined bins using CollectReadCounts.\n    \"\"\"\n    out_file = os.path.join(work_dir, \"%s-target-coverage.hdf5\" % dd.get_sample_name(data))\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            params = [\"-T\", \"CollectReadCounts\", \"-I\", dd.get_align_bam(data),\n                      \"-L\", tz.get_in([\"regions\", \"bins\", \"target\"], data),\n                      \"--interval-merging-rule\", \"OVERLAPPING_ONLY\",\n                      \"-O\", tx_out_file, \"--format\", \"HDF5\"]\n            _run_with_memory_scaling(params, tx_out_file, data)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef heterogzygote_counts(paired):\n    work_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(paired.tumor_data), \"structural\", \"counts\"))\n    key = \"germline_het_pon\"\n    het_bed = tz.get_in([\"genome_resources\", \"variation\", key], paired.tumor_data)\n    vr = bedutils.population_variant_regions([x for x in [paired.tumor_data, paired.normal_data] if x])\n    cur_het_bed = bedutils.intersect_two(het_bed, vr, work_dir, paired.tumor_data)\n    tumor_counts = _run_collect_allelic_counts(cur_het_bed, key, work_dir, paired.tumor_data)\n    normal_counts = (_run_collect_allelic_counts(cur_het_bed, key, work_dir, paired.normal_data)\n                     if paired.normal_data else None)\n    if normal_counts:\n        tumor_counts, normal_counts = _filter_by_normal(tumor_counts, normal_counts, paired.tumor_data)\n    return tumor_counts, normal_counts", "response": "Provide tumor and normal counts at population heterozygote sites with CollectAllelicCounts."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfiltering count files based on normal frequency and median depth.", "response": "def _filter_by_normal(tumor_counts, normal_counts, data):\n    \"\"\"Filter count files based on normal frequency and median depth, avoiding high depth regions.\n\n    For frequency, restricts normal positions to those between 0.4 and 0.65\n\n    For depth, matches approach used in AMBER to try and avoid problematic genomic regions\n    with high count in the normal:\n    https://github.com/hartwigmedical/hmftools/tree/master/amber#usage\n    \"\"\"\n    from bcbio.heterogeneity import bubbletree\n    fparams = bubbletree.NORMAL_FILTER_PARAMS\n    tumor_out = \"%s-normfilter%s\" % utils.splitext_plus(tumor_counts)\n    normal_out = \"%s-normfilter%s\" % utils.splitext_plus(normal_counts)\n    if not utils.file_uptodate(tumor_out, tumor_counts):\n        with file_transaction(data, tumor_out, normal_out) as (tx_tumor_out, tx_normal_out):\n            median_depth = _get_normal_median_depth(normal_counts)\n            min_normal_depth = median_depth * fparams[\"min_depth_percent\"]\n            max_normal_depth = median_depth * fparams[\"max_depth_percent\"]\n            with open(tumor_counts) as tumor_handle:\n                with open(normal_counts) as normal_handle:\n                    with open(tx_tumor_out, \"w\") as tumor_out_handle:\n                        with open(tx_normal_out, \"w\") as normal_out_handle:\n                            header = None\n                            for t, n in zip(tumor_handle, normal_handle):\n                                if header is None:\n                                    if not n.startswith(\"@\"):\n                                        header = n.strip().split()\n                                    tumor_out_handle.write(t)\n                                    normal_out_handle.write(n)\n                                elif (_normal_passes_depth(header, n, min_normal_depth, max_normal_depth) and\n                                      _normal_passes_freq(header, n, fparams)):\n                                    tumor_out_handle.write(t)\n                                    normal_out_handle.write(n)\n    return tumor_out, normal_out"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _run_collect_allelic_counts(pos_file, pos_name, work_dir, data):\n    out_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), \"structural\", \"counts\"))\n    out_file = os.path.join(out_dir, \"%s-%s-counts.tsv\" % (dd.get_sample_name(data), pos_name))\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            params = [\"-T\", \"CollectAllelicCounts\", \"-L\", pos_file, \"-I\", dd.get_align_bam(data),\n                      \"-R\", dd.get_ref_file(data), \"-O\", tx_out_file]\n            _run_with_memory_scaling(params, tx_out_file, data)\n    return out_file", "response": "Collect allelic counts for a specific sample and set of positions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _seg_to_vcf(vals):\n    call_to_cn = {\"+\": 3, \"-\": 1}\n    call_to_type = {\"+\": \"DUP\", \"-\": \"DEL\"}\n    if vals[\"CALL\"] not in [\"0\"]:\n        info = [\"FOLD_CHANGE_LOG=%s\" % vals[\"MEAN_LOG2_COPY_RATIO\"],\n                \"PROBES=%s\" % vals[\"NUM_POINTS_COPY_RATIO\"],\n                \"SVTYPE=%s\" % call_to_type[vals[\"CALL\"]],\n                \"SVLEN=%s\" % (int(vals[\"END\"]) - int(vals[\"START\"])),\n                \"END=%s\" % vals[\"END\"],\n                \"CN=%s\" % call_to_cn[vals[\"CALL\"]]]\n        return [vals[\"CONTIG\"], vals[\"START\"], \".\", \"N\", \"<%s>\" % call_to_type[vals[\"CALL\"]], \".\",\n                \".\", \";\".join(info), \"GT\", \"0/1\"]", "response": "Convert GATK CNV calls seg output to a VCF line."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the initial bcb. rda object using bcbioRNASeq.", "response": "def make_bcbiornaseq_object(data):\n    \"\"\"\n    load the initial bcb.rda object using bcbioRNASeq\n    \"\"\"\n    if \"bcbiornaseq\" not in dd.get_tools_on(data):\n        return data\n    upload_dir = tz.get_in((\"upload\", \"dir\"), data)\n    report_dir = os.path.join(upload_dir, \"bcbioRNASeq\")\n    safe_makedir(report_dir)\n    organism = dd.get_bcbiornaseq(data).get(\"organism\", None)\n    groups = dd.get_bcbiornaseq(data).get(\"interesting_groups\", None)\n    loadstring = create_load_string(upload_dir, groups, organism)\n    r_file = os.path.join(report_dir, \"load_bcbioRNAseq.R\")\n    with file_transaction(r_file) as tmp_file:\n        memoize_write_file(loadstring, tmp_file)\n    rcmd = Rscript_cmd()\n    with chdir(report_dir):\n        do.run([rcmd, \"--no-environ\", r_file], \"Loading bcbioRNASeq object.\")\n    make_quality_report(data)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates and render the bcbioRNASeq quality report", "response": "def make_quality_report(data):\n    \"\"\"\n    create and render the bcbioRNASeq quality report\n    \"\"\"\n    if \"bcbiornaseq\" not in dd.get_tools_on(data):\n        return data\n    upload_dir = tz.get_in((\"upload\", \"dir\"), data)\n    report_dir = os.path.join(upload_dir, \"bcbioRNASeq\")\n    safe_makedir(report_dir)\n    quality_rmd = os.path.join(report_dir, \"quality_control.Rmd\")\n    quality_html = os.path.join(report_dir, \"quality_control.html\")\n    quality_rmd = rmarkdown_draft(quality_rmd, \"quality_control\", \"bcbioRNASeq\")\n    if not file_exists(quality_html):\n        render_rmarkdown_file(quality_rmd)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a draft rmarkdown file from an installed template", "response": "def rmarkdown_draft(filename, template, package):\n    \"\"\"\n    create a draft rmarkdown file from an installed template\n    \"\"\"\n    if file_exists(filename):\n        return filename\n    draft_template = Template(\n        'rmarkdown::draft(\"$filename\", template=\"$template\", package=\"$package\", edit=FALSE)'\n    )\n    draft_string = draft_template.substitute(\n        filename=filename, template=template, package=package)\n    report_dir = os.path.dirname(filename)\n    rcmd = Rscript_cmd()\n    with chdir(report_dir):\n        do.run([rcmd, \"--no-environ\", \"-e\", draft_string], \"Creating bcbioRNASeq quality control template.\")\n        do.run([\"sed\", \"-i\", \"s/YYYY-MM-DD\\///g\", filename], \"Editing bcbioRNAseq quality control template.\")\n    return filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrenders a rmarkdown file using the rmarkdown library", "response": "def render_rmarkdown_file(filename):\n    \"\"\"\n    render a rmarkdown file using the rmarkdown library\n    \"\"\"\n    render_template = Template(\n        'rmarkdown::render(\"$filename\")'\n    )\n    render_string = render_template.substitute(\n        filename=filename)\n    report_dir = os.path.dirname(filename)\n    rcmd = Rscript_cmd()\n    with chdir(report_dir):\n        do.run([rcmd, \"--no-environ\", \"-e\", render_string], \"Rendering bcbioRNASeq quality control report.\")\n    return filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_load_string(upload_dir, groups=None, organism=None):\n    libraryline = 'library(bcbioRNASeq)'\n    load_template = Template(\n        ('bcb <- bcbioRNASeq(uploadDir=\"$upload_dir\",'\n         'interestingGroups=$groups,'\n         'organism=\"$organism\")'))\n    load_noorganism_template = Template(\n        ('bcb <- bcbioRNASeq(uploadDir=\"$upload_dir\",'\n         'interestingGroups=$groups,'\n         'organism=NULL)'))\n    flatline = 'flat <- flatFiles(bcb)'\n    saveline = 'saveData(bcb, flat, dir=\"data\")'\n    if groups:\n        groups = _list2Rlist(groups)\n    else:\n        groups = _quotestring(\"sampleName\")\n    if organism:\n        load_bcbio = load_template.substitute(\n            upload_dir=upload_dir, groups=groups, organism=organism)\n    else:\n        load_bcbio = load_noorganism_template.substitute(upload_dir=upload_dir,\n                                                         groups=groups)\n    return \";\\n\".join([libraryline, load_bcbio, flatline, saveline])", "response": "create the code necessary to load the bcbioRNAseq object"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a python list to an R list", "response": "def _list2Rlist(xs):\n    \"\"\" convert a python list to an R list \"\"\"\n    if isinstance(xs, six.string_types):\n        xs = [xs]\n    rlist = \",\".join([_quotestring(x) for x in xs])\n    return \"c(\" + rlist + \")\""}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun QSNP calling on paired tumor or normal.", "response": "def run_qsnp(align_bams, items, ref_file, assoc_files, region=None,\n             out_file=None):\n    \"\"\"Run qSNP calling on paired tumor/normal.\n    \"\"\"\n    if utils.file_exists(out_file):\n        return out_file\n    paired = get_paired_bams(align_bams, items)\n    if paired.normal_bam:\n        region_files = []\n        regions = _clean_regions(items, region)\n        if regions:\n            for region in regions:\n                out_region_file = out_file.replace(\".vcf.gz\", _to_str(region) + \".vcf.gz\")\n                region_file = _run_qsnp_paired(align_bams, items, ref_file,\n                                               assoc_files, region, out_region_file)\n                region_files.append(region_file)\n            out_file = combine_variant_files(region_files, out_file, ref_file, items[0][\"config\"])\n        if not region:\n            out_file = _run_qsnp_paired(align_bams, items, ref_file,\n                                        assoc_files, region, out_file)\n        return out_file\n    else:\n        raise ValueError(\"qSNP only works on paired samples\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _run_qsnp_paired(align_bams, items, ref_file, assoc_files,\n                     region=None, out_file=None):\n    \"\"\"Detect somatic mutations with qSNP.\n\n    This is used for paired tumor / normal samples.\n    \"\"\"\n    config = items[0][\"config\"]\n    if out_file is None:\n        out_file = \"%s-paired-variants.vcf\" % os.path.splitext(align_bams[0])[0]\n    if not utils.file_exists(out_file):\n        out_file = out_file.replace(\".gz\", \"\")\n        with file_transaction(config, out_file) as tx_out_file:\n            with tx_tmpdir(config) as tmpdir:\n                with utils.chdir(tmpdir):\n                    paired = get_paired_bams(align_bams, items)\n                    qsnp = config_utils.get_program(\"qsnp\", config)\n                    resources = config_utils.get_resources(\"qsnp\", config)\n                    mem = \" \".join(resources.get(\"jvm_opts\", [\"-Xms750m -Xmx4g\"]))\n                    qsnp_log = os.path.join(tmpdir, \"qsnp.log\")\n                    qsnp_init = os.path.join(tmpdir, \"qsnp.ini\")\n                    if region:\n                        paired = _create_bam_region(paired, region, tmpdir)\n                    _create_input(paired, tx_out_file, ref_file, assoc_files['dbsnp'], qsnp_init)\n                    cl = (\"{qsnp} {mem} -i {qsnp_init} -log {qsnp_log}\")\n                    do.run(cl.format(**locals()), \"Genotyping paired variants with Qsnp\", {})\n        out_file = _filter_vcf(out_file)\n        out_file = bgzip_and_index(out_file, config)\n    return out_file", "response": "Detect somatic mutations with QSNP."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nintersecting region with target file if it exists", "response": "def _clean_regions(items, region):\n    \"\"\"Intersect region with target file if it exists\"\"\"\n    variant_regions = bedutils.population_variant_regions(items, merged=True)\n    with utils.tmpfile() as tx_out_file:\n        target = subset_variant_regions(variant_regions, region, tx_out_file, items)\n        if target:\n            if isinstance(target, six.string_types) and os.path.isfile(target):\n                target = _load_regions(target)\n            else:\n                target = [target]\n            return target"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload list of tupples from bed file", "response": "def _load_regions(target):\n    \"\"\"Get list of tupples from bed file\"\"\"\n    regions = []\n    with open(target) as in_handle:\n        for line in in_handle:\n            if not line.startswith(\"#\"):\n                c, s, e = line.strip().split(\"\\t\")\n                regions.append((c, s, e))\n    return regions"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_bam_region(paired, region, tmp_dir):\n    tumor_name, normal_name = paired.tumor_name, paired.normal_name\n    normal_bam = _slice_bam(paired.normal_bam, region, tmp_dir, paired.tumor_config)\n    tumor_bam = _slice_bam(paired.tumor_bam, region, tmp_dir, paired.tumor_config)\n    paired = PairedData(tumor_bam, tumor_name, normal_bam, normal_name, None, None, None)\n    return paired", "response": "create temporal normal bam_file only with reads on that region"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nslice a BAM file into a specific region", "response": "def _slice_bam(in_bam, region, tmp_dir, config):\n    \"\"\"Use sambamba to slice a bam region\"\"\"\n    name_file = os.path.splitext(os.path.basename(in_bam))[0]\n    out_file = os.path.join(tmp_dir, os.path.join(tmp_dir, name_file + _to_str(region) + \".bam\"))\n    sambamba = config_utils.get_program(\"sambamba\", config)\n    region = _to_sambamba(region)\n    with file_transaction(out_file) as tx_out_file:\n        cmd = (\"{sambamba} slice {in_bam} {region} -o {tx_out_file}\")\n        do.run(cmd.format(**locals()), \"Slice region\", {})\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate INI input for qSNP", "response": "def _create_input(paired, out_file, ref_file, snp_file, qsnp_file):\n    \"\"\"Create INI input for qSNP\"\"\"\n    ini_file[\"[inputFiles]\"][\"dbSNP\"] = snp_file\n    ini_file[\"[inputFiles]\"][\"ref\"] = ref_file\n    ini_file[\"[inputFiles]\"][\"normalBam\"] = paired.normal_bam\n    ini_file[\"[inputFiles]\"][\"tumourBam\"] = paired.tumor_bam\n    ini_file[\"[ids]\"][\"normalSample\"] = paired.normal_name\n    ini_file[\"[ids]\"][\"tumourSample\"] = paired.tumor_name\n    ini_file[\"[ids]\"][\"donor\"] = paired.tumor_name\n    ini_file[\"[outputFiles]\"][\"vcf\"] = out_file\n    with open(qsnp_file, \"w\") as out_handle:\n        for k, v in ini_file.items():\n            out_handle.write(\"%s\\n\" % k)\n            for opt, value in v.items():\n                if value != \"\":\n                    out_handle.write(\"%s = %s\\n\" % (opt, value))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfiltering VCF files to include only non - ambiguous samples.", "response": "def _filter_vcf(out_file):\n    \"\"\"Fix sample names, FILTER and FORMAT fields. Remove lines with ambiguous reference.\n    \"\"\"\n    in_file = out_file.replace(\".vcf\", \"-ori.vcf\")\n    FILTER_line = ('##FILTER=<ID=SBIAS,Description=\"Due to bias\">\\n'\n                   '##FILTER=<ID=5BP,Description=\"Due to 5BP\">\\n'\n                   '##FILTER=<ID=REJECT,Description=\"Not somatic due to qSNP filters\">\\n')\n    SOMATIC_line = '##INFO=<ID=SOMATIC,Number=0,Type=Flag,Description=\"somatic event\">\\n'\n    if not utils.file_exists(in_file):\n        shutil.move(out_file, in_file)\n    with file_transaction(out_file) as tx_out_file:\n        with open(in_file) as in_handle, open(tx_out_file, \"w\") as out_handle:\n            for line in in_handle:\n                if line.startswith(\"##normalSample=\"):\n                    normal_name = line.strip().split(\"=\")[1]\n                if line.startswith(\"##patient_id=\"):\n                    tumor_name = line.strip().split(\"=\")[1]\n                if line.startswith(\"#CHROM\"):\n                    line = line.replace(\"Normal\", normal_name)\n                    line = line.replace(\"Tumour\", tumor_name)\n                if line.startswith(\"##INFO=<ID=FS\"):\n                    line = line.replace(\"ID=FS\", \"ID=RNT\")\n                if line.find(\"FS=\") > -1:\n                    line = line.replace(\"FS=\", \"RNT=\")\n                if \"5BP\" in line:\n                    line = sub(\"5BP[0-9]+\", \"5BP\", line)\n                if line.find(\"PASS\") == -1:\n                    line = _set_reject(line)\n                if line.find(\"PASS\") > - 1 and line.find(\"SOMATIC\") == -1:\n                    line = _set_reject(line)\n                if not _has_ambiguous_ref_allele(line):\n                    out_handle.write(line)\n                if line.startswith(\"##FILTER\") and FILTER_line:\n                    out_handle.write(\"%s\" % FILTER_line)\n                    FILTER_line = \"\"\n                if line.startswith(\"##INFO\") and SOMATIC_line:\n                    out_handle.write(\"%s\" % SOMATIC_line)\n                    SOMATIC_line = \"\"\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting REJECT in VCF line or add it if there is something else.", "response": "def _set_reject(line):\n    \"\"\"Set REJECT in VCF line, or add it if there is something else.\"\"\"\n    if line.startswith(\"#\"):\n        return line\n    parts = line.split(\"\\t\")\n    if parts[6] == \"PASS\":\n        parts[6] = \"REJECT\"\n    else:\n        parts[6] += \";REJECT\"\n    return \"\\t\".join(parts)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef svevent_reader(in_file):\n    with open(in_file) as in_handle:\n        while 1:\n            line = next(in_handle)\n            if line.startswith(\">\"):\n                break\n        header = line[1:].rstrip().split(\"\\t\")\n        reader = csv.reader(in_handle, dialect=\"excel-tab\")\n        for parts in reader:\n            out = {}\n            for h, p in zip(header, parts):\n                out[h] = p\n            yield out", "response": "Lazy generator of SV events returned as dictionary of parts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninitialize a WorldWatcher object from a set of samples.", "response": "def initialize_watcher(samples):\n    \"\"\"\n    check to see if cwl_reporting is set for any samples,\n    and if so, initialize a WorldWatcher object from a set of samples,\n    \"\"\"\n    work_dir = dd.get_in_samples(samples, dd.get_work_dir)\n    ww = WorldWatcher(work_dir,\n                      is_on=any([dd.get_cwl_reporting(d[0]) for d in samples]))\n    ww.initialize(samples)\n    return ww"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nguesses if we need to infer the gene extent option when making a gffutils database", "response": "def guess_infer_extent(gtf_file):\n    \"\"\"\n    guess if we need to use the gene extent option when making a gffutils\n    database by making a tiny database of 1000 lines from the original\n    GTF and looking for all of the features\n    \"\"\"\n    _, ext = os.path.splitext(gtf_file)\n    tmp_out = tempfile.NamedTemporaryFile(suffix=\".gtf\", delete=False).name\n    with open(tmp_out, \"w\") as out_handle:\n        count = 0\n        in_handle = utils.open_gzipsafe(gtf_file)\n        for line in in_handle:\n            if count > 1000:\n                break\n            out_handle.write(line)\n            count += 1\n        in_handle.close()\n    db = gffutils.create_db(tmp_out, dbfn=\":memory:\", infer_gene_extent=False)\n    os.remove(tmp_out)\n    features = [x for x in db.featuretypes()]\n    if \"gene\" in features and \"transcript\" in features:\n        return False\n    else:\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_gtf_db(gtf, in_memory=False):\n    db_file = gtf + \".db\"\n    if file_exists(db_file):\n        return gffutils.FeatureDB(db_file)\n    if not os.access(os.path.dirname(db_file), os.W_OK | os.X_OK):\n        in_memory = True\n    db_file = \":memory:\" if in_memory else db_file\n    if in_memory or not file_exists(db_file):\n        infer_extent = guess_infer_extent(gtf)\n        disable_extent = not infer_extent\n        db = gffutils.create_db(gtf, dbfn=db_file,\n                                disable_infer_genes=disable_extent,\n                                disable_infer_transcripts=disable_extent)\n    if in_memory:\n        return db\n    else:\n        return gffutils.FeatureDB(db_file)", "response": "get a gffutils DB for a single GTF"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef complete_features(db):\n    for feature in db.all_features():\n        gene_id = feature.attributes.get('gene_id', [None])[0]\n        transcript_id = feature.attributes.get('transcript_id', [None])[0]\n        if gene_id and transcript_id and feature.featuretype != \"transcript\":\n            yield feature", "response": "Iterator returning features which are complete"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a GTF file to FASTA format.", "response": "def gtf_to_fasta(gtf_file, ref_fasta, cds=False, out_file=None):\n    \"\"\"\n    convert a GTF to FASTA format if cds=True, use the start/stop codons\n    to output only the CDS\n    handles malformed FASTA files where a single transcript is repeated multiple\n    times by just using the first one\n    \"\"\"\n    if out_file and file_exists(out_file):\n        return out_file\n\n    if not out_file:\n        out_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".fa\").name\n\n    tmp_file = out_file + \".tmp\"\n    if cds:\n        cmd = \"gffread -g {ref_fasta} -x {tx_tmp_file} {gtf_file}\"\n    else:\n        cmd = \"gffread -g {ref_fasta} -w {tx_tmp_file} {gtf_file}\"\n    message = \"Converting %s to FASTA format.\" % gtf_file\n    with file_transaction(tmp_file) as tx_tmp_file:\n        do.run(cmd.format(**locals()), message)\n\n    transcript = \"\"\n    skipping = False\n    with file_transaction(out_file) as tx_out_file:\n        with open(tmp_file) as in_handle, open(tx_out_file, \"w\") as out_handle:\n            for line in in_handle:\n                if line.startswith(\">\"):\n                    cur_transcript = line.split(\" \")[0][1:]\n                    if transcript == cur_transcript:\n                        logger.info(\"Transcript %s has already been seen, skipping this \"\n                                    \"version.\" % cur_transcript)\n                        skipping = True\n                    else:\n                        transcript = cur_transcript\n                        skipping = False\n                    line = \">\" + transcript + \"\\n\"\n                if not skipping:\n                    out_handle.write(line)\n    os.remove(tmp_file)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npartition a GTF file into a single non - coding or non - coding GTF file.", "response": "def partition_gtf(gtf, coding=False, out_file=False):\n    \"\"\"\n    return a GTF file of all non-coding or coding transcripts. the GTF must be annotated\n    with gene_biotype = \"protein_coding\" or to have the source column set to the\n    biotype for all coding transcripts. set coding to\n    True to get only the coding, false to get only the non-coding\n    \"\"\"\n    if out_file and file_exists(out_file):\n        return out_file\n    if not out_file:\n        out_file = tempfile.NamedTemporaryFile(delete=False,\n                                               suffix=\".gtf\").name\n\n    if coding:\n        pred = lambda biotype: biotype and biotype == \"protein_coding\"\n    else:\n        pred = lambda biotype: biotype and biotype != \"protein_coding\"\n\n    biotype_lookup = _biotype_lookup_fn(gtf)\n\n    db = get_gtf_db(gtf)\n    with file_transaction(out_file) as tx_out_file:\n        with open(tx_out_file, \"w\") as out_handle:\n            for feature in db.all_features():\n                biotype = biotype_lookup(feature)\n                if pred(biotype):\n                    out_handle.write(str(feature) + \"\\n\")\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsplits a GTF file into two equal parts randomly selecting genes.", "response": "def split_gtf(gtf, sample_size=None, out_dir=None):\n    \"\"\"\n    split a GTF file into two equal parts, randomly selecting genes.\n    sample_size will select up to sample_size genes in total\n    \"\"\"\n    if out_dir:\n        part1_fn = os.path.basename(os.path.splitext(gtf)[0]) + \".part1.gtf\"\n        part2_fn = os.path.basename(os.path.splitext(gtf)[0]) + \".part2.gtf\"\n        part1 = os.path.join(out_dir, part1_fn)\n        part2 = os.path.join(out_dir, part2_fn)\n        if file_exists(part1) and file_exists(part2):\n            return part1, part2\n    else:\n        part1 = tempfile.NamedTemporaryFile(delete=False, suffix=\".part1.gtf\").name\n        part2 = tempfile.NamedTemporaryFile(delete=False, suffix=\".part2.gtf\").name\n\n    db = get_gtf_db(gtf)\n    gene_ids = set([x['gene_id'][0] for x in db.all_features()])\n    if not sample_size or (sample_size and sample_size > len(gene_ids)):\n        sample_size = len(gene_ids)\n    gene_ids = set(random.sample(gene_ids, sample_size))\n    part1_ids = set(random.sample(gene_ids, sample_size / 2))\n    part2_ids = gene_ids.difference(part1_ids)\n    with open(part1, \"w\") as part1_handle:\n        for gene in part1_ids:\n            for feature in db.children(gene):\n                part1_handle.write(str(feature) + \"\\n\")\n    with open(part2, \"w\") as part2_handle:\n        for gene in part2_ids:\n            for feature in db.children(gene):\n                part2_handle.write(str(feature) + \"\\n\")\n    return part1, part2"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a set of coding and non - coding transcript_ids from a GTF", "response": "def get_coding_noncoding_transcript_ids(gtf):\n    \"\"\"\n    return a set of coding and non-coding transcript_ids from a GTF\n    \"\"\"\n    coding_gtf = partition_gtf(gtf, coding=True)\n    coding_db = get_gtf_db(coding_gtf)\n    coding_ids = set([x['transcript_id'][0] for x in coding_db.all_features()\n                  if 'transcript_id' in x.attributes])\n    noncoding_gtf = partition_gtf(gtf)\n    noncoding_db = get_gtf_db(noncoding_gtf)\n    noncoding_ids = set([x['transcript_id'][0] for x in noncoding_db.all_features()\n                     if 'transcript_id' in x.attributes])\n    return coding_ids, noncoding_ids"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a dictionary of the set of all sources for a gene", "response": "def get_gene_source_set(gtf):\n    \"\"\"\n    get a dictionary of the set of all sources for a gene\n    \"\"\"\n    gene_to_source = {}\n    db = get_gtf_db(gtf)\n    for feature in complete_features(db):\n        gene_id = feature['gene_id'][0]\n        sources = gene_to_source.get(gene_id, set([])).union(set([feature.source]))\n        gene_to_source[gene_id] = sources\n    return gene_to_source"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a dictionary of the set of all sources of a given gene for a given transcript", "response": "def get_transcript_source_set(gtf):\n    \"\"\"\n    get a dictionary of the set of all sources of the gene for a given\n    transcript\n    \"\"\"\n    gene_to_source = get_gene_source_set(gtf)\n    transcript_to_source = {}\n    db = get_gtf_db(gtf)\n    for feature in complete_features(db):\n        gene_id = feature['gene_id'][0]\n        transcript_to_source[feature['transcript_id'][0]] = gene_to_source[gene_id]\n    return transcript_to_source"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_rRNA(gtf):\n    rRNA_biotypes = [\"rRNA\", \"Mt_rRNA\", \"tRNA\", \"MT_tRNA\"]\n    features = set()\n    with open_gzipsafe(gtf) as in_handle:\n        for line in in_handle:\n            if not \"gene_id\" in line or not \"transcript_id\" in line:\n                continue\n            if any(x in line for x in rRNA_biotypes):\n                geneid = line.split(\"gene_id\")[1].split(\" \")[1]\n                geneid = _strip_non_alphanumeric(geneid)\n                geneid = _strip_feature_version(geneid)\n                txid = line.split(\"transcript_id\")[1].split(\" \")[1]\n                txid = _strip_non_alphanumeric(txid)\n                txid = _strip_feature_version(txid)\n                features.add((geneid, txid))\n    return features", "response": "extract rRNA genes and transcripts from a gtf file"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a function that will look up the biotype of a feature", "response": "def _biotype_lookup_fn(gtf):\n    \"\"\"\n    return a function that will look up the biotype of a feature\n    this checks for either gene_biotype or biotype being set or for the source\n    column to have biotype information\n    \"\"\"\n    db = get_gtf_db(gtf)\n    sources = set([feature.source for feature in db.all_features()])\n    gene_biotypes = set([feature.attributes.get(\"gene_biotype\", [None])[0]\n                         for feature in db.all_features()])\n    biotypes = set([feature.attributes.get(\"biotype\", [None])[0]\n                    for feature in db.all_features()])\n    if \"protein_coding\" in sources:\n        return lambda feature: feature.source\n    elif \"protein_coding\" in biotypes:\n        return lambda feature: feature.attributes.get(\"biotype\", [None])[0]\n    elif \"protein_coding\" in gene_biotypes:\n        return lambda feature: feature.attributes.get(\"gene_biotype\", [None])[0]\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tx2genedict(gtf, keep_version=False):\n    d = {}\n    with open_gzipsafe(gtf) as in_handle:\n        for line in in_handle:\n            if \"gene_id\" not in line or \"transcript_id\" not in line:\n                continue\n            geneid = line.split(\"gene_id\")[1].split(\" \")[1]\n            geneid = _strip_non_alphanumeric(geneid)\n            txid = line.split(\"transcript_id\")[1].split(\" \")[1]\n            txid = _strip_non_alphanumeric(txid)\n            if keep_version and \"transcript_version\" in line:\n                txversion = line.split(\"transcript_version\")[1].split(\" \")[1]\n                txversion = _strip_non_alphanumeric(txversion)\n                txid  += \".\" + txversion\n            if has_transcript_version(line) and not keep_version:\n                txid = _strip_feature_version(txid)\n                geneid = _strip_feature_version(geneid)\n            d[txid] = geneid\n    return d", "response": "produce a tx2gene dictionary from a GTF file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _strip_feature_version(featureid):\n    version_detector = re.compile(r\"(?P<featureid>.*)(?P<version>\\.\\d+)\")\n    match = version_detector.match(featureid)\n    if match:\n        return match.groupdict()[\"featureid\"]\n    else:\n        return featureid", "response": "strips featureid. version from the featureid if it exists"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite out a file of transcript - > gene mappings.", "response": "def tx2genefile(gtf, out_file=None, data=None, tsv=True, keep_version=False):\n    \"\"\"\n    write out a file of transcript->gene mappings.\n    \"\"\"\n    if tsv:\n        extension = \".tsv\"\n        sep = \"\\t\"\n    else:\n        extension = \".csv\"\n        sep = \",\"\n    if file_exists(out_file):\n        return out_file\n    with file_transaction(data, out_file) as tx_out_file:\n        with open(tx_out_file, \"w\") as out_handle:\n            for k, v in tx2genedict(gtf, keep_version).items():\n                out_handle.write(sep.join([k, v]) + \"\\n\")\n    logger.info(\"tx2gene file %s created from %s.\" % (out_file, gtf))\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_qualimap_compatible(gtf):\n    if not gtf:\n        return False\n    db = get_gtf_db(gtf)\n    def qualimap_compatible(feature):\n        gene_id = feature.attributes.get('gene_id', [None])[0]\n        transcript_id = feature.attributes.get('transcript_id', [None])[0]\n        gene_biotype = feature.attributes.get('gene_biotype', [None])[0]\n        return gene_id and transcript_id and gene_biotype\n    for feature in db.all_features():\n        if qualimap_compatible(feature):\n            return True\n    return False", "response": "Returns True if the GTF is compatible with Qualimap"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef canonical_transcripts(gtf, out_file):\n    if file_exists(out_file):\n        return out_file\n    db = get_gtf_db(gtf)\n    with file_transaction(out_file) as tx_out_file:\n        with open(tx_out_file, \"w\") as out_handle:\n            for gene in db.features_of_type('gene'):\n                exon_list = []\n                for ti, transcript in enumerate(db.children(gene, level=1)):\n                    cds_len = 0\n                    total_len = 0\n                    exons = list(db.children(transcript, level=1))\n                    for exon in exons:\n                        exon_length = len(exon)\n                        if exon.featuretype == 'CDS':\n                            cds_len += exon_length\n                        total_len += exon_length\n\n                    exon_list.append((cds_len, total_len, transcript, exons))\n\n                # If we have CDS, then use the longest coding transcript\n                if max(i[0] for i in exon_list) > 0:\n                    best = sorted(exon_list)[0]\n                # Otherwise, just choose the longest\n                else:\n                    best = sorted(exon_list, key=lambda x: x[1])[0]\n                for exon in db.children(best[2], level=1):\n                    out_handle.write(str(exon) + \"\\n\")\n    return out_file", "response": "produce a new GTF file with only the longest coding transcript for each gene\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_cpat_compatible(gtf):\n    if not gtf:\n        return False\n    db = get_gtf_db(gtf)\n    pred = lambda biotype: biotype and biotype == \"protein_coding\"\n    biotype_lookup = _biotype_lookup_fn(gtf)\n    if not biotype_lookup:\n        return False\n    db = get_gtf_db(gtf)\n    for feature in db.all_features():\n        biotype = biotype_lookup(feature)\n        if pred(biotype):\n            return True\n    return False", "response": "Check if the CPAT is compatible with the given GTF."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef organize(dirs, config, run_info_yaml, sample_names=None, is_cwl=False,\n             integrations=None):\n    \"\"\"Organize run information from a passed YAML file or the Galaxy API.\n\n    Creates the high level structure used for subsequent processing.\n\n    sample_names is a list of samples to include from the overall file, for cases\n    where we are running multiple pipelines from the same configuration file.\n    \"\"\"\n    from bcbio.pipeline import qcsummary\n    if integrations is None: integrations = {}\n    logger.info(\"Using input YAML configuration: %s\" % run_info_yaml)\n    assert run_info_yaml and os.path.exists(run_info_yaml), \\\n        \"Did not find input sample YAML file: %s\" % run_info_yaml\n    run_details = _run_info_from_yaml(dirs, run_info_yaml, config, sample_names,\n                                      is_cwl=is_cwl, integrations=integrations)\n    remote_retriever = None\n    for iname, retriever in integrations.items():\n        if iname in config:\n            run_details = retriever.add_remotes(run_details, config[iname])\n            remote_retriever = retriever\n    out = []\n    for item in run_details:\n        item[\"dirs\"] = dirs\n        if \"name\" not in item:\n            item[\"name\"] = [\"\", item[\"description\"]]\n        elif isinstance(item[\"name\"], six.string_types):\n            description = \"%s-%s\" % (item[\"name\"], clean_name(item[\"description\"]))\n            item[\"name\"] = [item[\"name\"], description]\n            item[\"description\"] = description\n        # add algorithm details to configuration, avoid double specification\n        item[\"resources\"] = _add_remote_resources(item[\"resources\"])\n        item[\"config\"] = config_utils.update_w_custom(config, item)\n        item.pop(\"algorithm\", None)\n        item = add_reference_resources(item, remote_retriever)\n        item[\"config\"][\"algorithm\"][\"qc\"] = qcsummary.get_qc_tools(item)\n        item[\"config\"][\"algorithm\"][\"vcfanno\"] = vcfanno.find_annotations(item, remote_retriever)\n        # Create temporary directories and make absolute, expanding environmental variables\n        tmp_dir = tz.get_in([\"config\", \"resources\", \"tmp\", \"dir\"], item)\n        if tmp_dir:\n            # if no environmental variables, make and normalize the directory\n            # otherwise we normalize later in distributed.transaction:\n            if os.path.expandvars(tmp_dir) == tmp_dir:\n                tmp_dir = utils.safe_makedir(os.path.expandvars(tmp_dir))\n                tmp_dir = genome.abs_file_paths(tmp_dir, do_download=not integrations)\n            item[\"config\"][\"resources\"][\"tmp\"][\"dir\"] = tmp_dir\n        out.append(item)\n    out = _add_provenance(out, dirs, config, not is_cwl)\n    return out", "response": "Organize run information from a YAML file or Galaxy API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_full_paths(fastq_dir, config, config_file):\n    if fastq_dir:\n        fastq_dir = utils.add_full_path(fastq_dir)\n    config_dir = utils.add_full_path(os.path.dirname(config_file))\n    galaxy_config_file = utils.add_full_path(config.get(\"galaxy_config\", \"universe_wsgi.ini\"),\n                                             config_dir)\n    return fastq_dir, os.path.dirname(galaxy_config_file), config_dir", "response": "Retrieve full paths for directories in the case of relative locations."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _add_remote_resources(resources):\n    out = copy.deepcopy(resources)\n    for prog, info in resources.items():\n        for key, val in info.items():\n            if key == \"jar\" and objectstore.is_remote(val):\n                store_dir = utils.safe_makedir(os.path.join(os.getcwd(), \"inputs\", \"jars\", prog))\n                fname = objectstore.download(val, store_dir, store_dir)\n                version_file = os.path.join(store_dir, \"version.txt\")\n                if not utils.file_exists(version_file):\n                    version = install.get_gatk_jar_version(prog, fname)\n                    with open(version_file, \"w\") as out_handle:\n                        out_handle.write(version)\n                else:\n                    with open(version_file) as in_handle:\n                        version = in_handle.read().strip()\n                del out[prog][key]\n                out[prog][\"dir\"] = store_dir\n                out[prog][\"version\"] = version\n    return out", "response": "Retrieve remote resources like GATK orMuTect jars present in S3."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_reference_resources(data, remote_retriever=None):\n    aligner = data[\"config\"][\"algorithm\"].get(\"aligner\", None)\n    if remote_retriever:\n        data[\"reference\"] = remote_retriever.get_refs(data[\"genome_build\"],\n                                                      alignment.get_aligner_with_aliases(aligner, data),\n                                                      data[\"config\"])\n    else:\n        data[\"reference\"] = genome.get_refs(data[\"genome_build\"], alignment.get_aligner_with_aliases(aligner, data),\n                                            data[\"dirs\"][\"galaxy\"], data)\n        _check_ref_files(data[\"reference\"], data)\n    # back compatible `sam_ref` target\n    data[\"sam_ref\"] = utils.get_in(data, (\"reference\", \"fasta\", \"base\"))\n    ref_loc = utils.get_in(data, (\"config\", \"resources\", \"species\", \"dir\"),\n                           utils.get_in(data, (\"reference\", \"fasta\", \"base\")))\n    if remote_retriever:\n        data = remote_retriever.get_resources(data[\"genome_build\"], ref_loc, data)\n    else:\n        data[\"genome_resources\"] = genome.get_resources(data[\"genome_build\"], ref_loc, data)\n    data[\"genome_resources\"] = genome.add_required_resources(data[\"genome_resources\"])\n    if effects.get_type(data) == \"snpeff\" and \"snpeff\" not in data[\"reference\"]:\n        data[\"reference\"][\"snpeff\"] = effects.get_snpeff_files(data)\n    if \"genome_context\" not in data[\"reference\"]:\n        data[\"reference\"][\"genome_context\"] = annotation.get_context_files(data)\n    if \"viral\" not in data[\"reference\"]:\n        data[\"reference\"][\"viral\"] = viral.get_files(data)\n    if not data[\"reference\"][\"viral\"]:\n        data[\"reference\"][\"viral\"] = None\n    if \"versions\" not in data[\"reference\"]:\n        data[\"reference\"][\"versions\"] = _get_data_versions(data)\n\n    data = _fill_validation_targets(data)\n    data = _fill_prioritization_targets(data)\n    data = _fill_capture_regions(data)\n    # Re-enable when we have ability to re-define gemini configuration directory\n    if False:\n        data[\"reference\"][\"gemini\"] = population.get_gemini_files(data)\n    return data", "response": "Add genome reference information to the item to process."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_data_versions(data):\n    genome_dir = install.get_genome_dir(data[\"genome_build\"], data[\"dirs\"].get(\"galaxy\"), data)\n    if genome_dir:\n        version_file = os.path.join(genome_dir, \"versions.csv\")\n        if version_file and os.path.exists(version_file):\n            return version_file\n    return None", "response": "Retrieve CSV file with version information for reference data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfills validation targets pointing to globally installed truth sets.", "response": "def _fill_validation_targets(data):\n    \"\"\"Fill validation targets pointing to globally installed truth sets.\n    \"\"\"\n    ref_file = dd.get_ref_file(data)\n    sv_truth = tz.get_in([\"config\", \"algorithm\", \"svvalidate\"], data, {})\n    sv_targets = (zip(itertools.repeat(\"svvalidate\"), sv_truth.keys()) if isinstance(sv_truth, dict)\n                  else [[\"svvalidate\"]])\n    for vtarget in [list(xs) for xs in [[\"validate\"], [\"validate_regions\"], [\"variant_regions\"]] + list(sv_targets)]:\n        val = tz.get_in([\"config\", \"algorithm\"] + vtarget, data)\n        if val and not os.path.exists(val) and not objectstore.is_remote(val):\n            installed_val = os.path.normpath(os.path.join(os.path.dirname(ref_file), os.pardir, \"validation\", val))\n            if os.path.exists(installed_val):\n                data = tz.update_in(data, [\"config\", \"algorithm\"] + vtarget, lambda x: installed_val)\n            else:\n                raise ValueError(\"Configuration problem. Validation file not found for %s: %s\" %\n                                 (vtarget, val))\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _fill_capture_regions(data):\n    special_targets = {\"sv_regions\": (\"exons\", \"transcripts\")}\n    ref_file = dd.get_ref_file(data)\n    for target in [\"variant_regions\", \"sv_regions\", \"coverage\"]:\n        val = tz.get_in([\"config\", \"algorithm\", target], data)\n        if val and not os.path.exists(val) and not objectstore.is_remote(val):\n            installed_vals = []\n            # Check prioritize directory\n            for ext in [\".bed\", \".bed.gz\"]:\n                installed_vals += glob.glob(os.path.normpath(os.path.join(os.path.dirname(ref_file), os.pardir,\n                                                                          \"coverage\", val + ext)))\n            if len(installed_vals) == 0:\n                if target not in special_targets or not val.startswith(special_targets[target]):\n                    raise ValueError(\"Configuration problem. BED file not found for %s: %s\" %\n                                     (target, val))\n            else:\n                assert len(installed_vals) == 1, installed_vals\n                data = tz.update_in(data, [\"config\", \"algorithm\", target], lambda x: installed_vals[0])\n    return data", "response": "Fill short - hand specification of BED capture regions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _fill_prioritization_targets(data):\n    ref_file = dd.get_ref_file(data)\n    for target in [\"svprioritize\", \"coverage\"]:\n        val = tz.get_in([\"config\", \"algorithm\", target], data)\n        if val and not os.path.exists(val) and not objectstore.is_remote(val):\n            installed_vals = []\n            # Check prioritize directory\n            for ext in [\".bed\", \".bed.gz\"]:\n                installed_vals += glob.glob(os.path.normpath(os.path.join(os.path.dirname(ref_file), os.pardir,\n                                                                          \"coverage\", \"prioritize\",\n                                                                          val + \"*%s\" % ext)))\n            # Check sv-annotation directory for prioritize gene name lists\n            if target == \"svprioritize\":\n                simple_sv_bin = utils.which(\"simple_sv_annotation.py\")\n                if simple_sv_bin:\n                    installed_vals += glob.glob(os.path.join(os.path.dirname(os.path.realpath(simple_sv_bin)),\n                                                             \"%s*\" % os.path.basename(val)))\n            if len(installed_vals) == 0:\n                # some targets can be filled in later\n                if target not in set([\"coverage\"]):\n                    raise ValueError(\"Configuration problem. BED file not found for %s: %s\" %\n                                     (target, val))\n                else:\n                    installed_val = val\n            elif len(installed_vals) == 1:\n                installed_val = installed_vals[0]\n            else:\n                # check for partial matches\n                installed_val = None\n                for v in installed_vals:\n                    if v.endswith(val + \".bed.gz\") or v.endswith(val + \".bed\"):\n                        installed_val = v\n                        break\n                # handle date-stamped inputs\n                if not installed_val:\n                    installed_val = sorted(installed_vals, reverse=True)[0]\n            data = tz.update_in(data, [\"config\", \"algorithm\", target], lambda x: installed_val)\n    return data", "response": "Fill in globally installed files for prioritization."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncleaning algorithm keys handling items that can be specified as lists or single items.", "response": "def _clean_algorithm(data):\n    \"\"\"Clean algorithm keys, handling items that can be specified as lists or single items.\n    \"\"\"\n    # convert single items to lists\n    for key in [\"variantcaller\", \"jointcaller\", \"svcaller\"]:\n        val = tz.get_in([\"algorithm\", key], data)\n        if val:\n            if not isinstance(val, (list, tuple)) and isinstance(val, six.string_types):\n                val = [val]\n            # check for cases like [false] or [None]\n            if isinstance(val, (list, tuple)):\n                if len(val) == 1 and not val[0] or (isinstance(val[0], six.string_types)\n                                                    and val[0].lower() in [\"none\", \"false\"]):\n                    val = False\n            data[\"algorithm\"][key] = val\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _organize_tools_on(data, is_cwl):\n    # want tools_on: [gvcf] if joint calling specified in CWL\n    if is_cwl:\n        if tz.get_in([\"algorithm\", \"jointcaller\"], data):\n            val = tz.get_in([\"algorithm\", \"tools_on\"], data)\n            if not val:\n                val = []\n            if not isinstance(val, (list, tuple)):\n                val = [val]\n            if \"gvcf\" not in val:\n                val.append(\"gvcf\")\n            data[\"algorithm\"][\"tools_on\"] = val\n    return data", "response": "Ensure tools_on inputs match items specified elsewhere."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _clean_background(data):\n    allowed_keys = set([\"variant\", \"cnv_reference\"])\n    val = tz.get_in([\"algorithm\", \"background\"], data)\n    errors = []\n    if val:\n        out = {}\n        # old style specification, single string for variant\n        if isinstance(val, six.string_types):\n            out[\"variant\"] = _file_to_abs(val, [os.getcwd()])\n        elif isinstance(val, dict):\n            for k, v in val.items():\n                if k in allowed_keys:\n                    if isinstance(v, six.string_types):\n                        out[k] = _file_to_abs(v, [os.getcwd()])\n                    else:\n                        assert isinstance(v, dict)\n                        for ik, iv in v.items():\n                            v[ik] = _file_to_abs(iv, [os.getcwd()])\n                        out[k] = v\n                else:\n                    errors.append(\"Unexpected key: %s\" % k)\n        else:\n            errors.append(\"Unexpected input: %s\" % val)\n        if errors:\n            raise ValueError(\"Problematic algorithm background specification for %s:\\n %s\" %\n                             (data[\"description\"], \"\\n\".join(errors)))\n        out[\"cnv_reference\"] = structural.standardize_cnv_reference({\"config\": data,\n                                                                     \"description\": data[\"description\"]})\n        data[\"algorithm\"][\"background\"] = out\n    return data", "response": "Clean up background specification remaining back compatible."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncleans problem characters in sample lane or descriptions.", "response": "def _clean_characters(x):\n    \"\"\"Clean problem characters in sample lane or descriptions.\n    \"\"\"\n    if not isinstance(x, six.string_types):\n        x = str(x)\n    else:\n        if not all(ord(char) < 128 for char in x):\n            msg = \"Found unicode character in input YAML (%s)\" % (x)\n            raise ValueError(repr(msg))\n    for problem in [\" \", \".\", \"/\", \"\\\\\", \"[\", \"]\", \"&\", \";\", \"#\", \"+\", \":\", \")\", \"(\"]:\n        x = x.replace(problem, \"_\")\n    return x"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prep_rg_names(item, config, fc_name, fc_date):\n    if fc_name and fc_date:\n        lane_name = \"%s_%s_%s\" % (item[\"lane\"], fc_date, fc_name)\n    else:\n        lane_name = item[\"description\"]\n    return {\"rg\": item[\"description\"],\n            \"sample\": item[\"description\"],\n            \"lane\": lane_name,\n            \"pl\": (tz.get_in([\"algorithm\", \"platform\"], item)\n                   or tz.get_in([\"algorithm\", \"platform\"], item, \"illumina\")).lower(),\n            \"lb\": tz.get_in([\"metadata\", \"library\"], item),\n            \"pu\": tz.get_in([\"metadata\", \"platform_unit\"], item) or lane_name}", "response": "Generate read group names from item inputs."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nidentifies and raise errors on duplicate items.", "response": "def _check_for_duplicates(xs, attr, check_fn=None):\n    \"\"\"Identify and raise errors on duplicate items.\n    \"\"\"\n    dups = []\n    for key, vals in itertools.groupby(x[attr] for x in xs):\n        if len(list(vals)) > 1:\n            dups.append(key)\n    if len(dups) > 0:\n        psamples = []\n        for x in xs:\n            if x[attr] in dups:\n                psamples.append(x)\n        # option to skip problem based on custom input function.\n        if check_fn and check_fn(psamples):\n            return\n        descrs = [x[\"description\"] for x in psamples]\n        raise ValueError(\"Duplicate '%s' found in input sample configuration.\\n\"\n                         \"Required to be unique for a project: %s\\n\"\n                         \"Problem found in these samples: %s\" % (attr, dups, descrs))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking that batch names do not overlap with sample names.", "response": "def _check_for_batch_clashes(xs):\n    \"\"\"Check that batch names do not overlap with sample names.\n    \"\"\"\n    names = set([x[\"description\"] for x in xs])\n    dups = set([])\n    for x in xs:\n        batches = tz.get_in((\"metadata\", \"batch\"), x)\n        if batches:\n            if not isinstance(batches, (list, tuple)):\n                batches = [batches]\n            for batch in batches:\n                if batch in names:\n                    dups.add(batch)\n    if len(dups) > 0:\n        raise ValueError(\"Batch names must be unique from sample descriptions.\\n\"\n                         \"Clashing batch names: %s\" % sorted(list(dups)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_for_problem_somatic_batches(items, config):\n    to_check = []\n    for data in items:\n        data = copy.deepcopy(data)\n        data[\"config\"] = config_utils.update_w_custom(config, data)\n        to_check.append(data)\n    data_by_batches = collections.defaultdict(list)\n    for data in to_check:\n        batches = dd.get_batches(data)\n        if batches:\n            for batch in batches:\n                data_by_batches[batch].append(data)\n    for batch, items in data_by_batches.items():\n        if vcfutils.get_paired(items):\n            vcfutils.check_paired_problems(items)\n        elif len(items) > 1:\n            vcs = vcfutils.get_somatic_variantcallers(items)\n            if \"vardict\" in vcs:\n                raise ValueError(\"VarDict does not support pooled non-tumor/normal calling, in batch %s: %s\"\n                                 % (batch, [dd.get_sample_name(data) for data in items]))\n            elif \"mutect\" in vcs or \"mutect2\" in vcs:\n                raise ValueError(\"MuTect and MuTect2 require a 'phenotype: tumor' sample for calling, \"\n                                 \"in batch %s: %s\"\n                                 % (batch, [dd.get_sample_name(data) for data in items]))", "response": "Identify problem batches for somatic calling."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_for_misplaced(xs, subkey, other_keys):\n    problems = []\n    for x in xs:\n        check_dict = x.get(subkey, {})\n        for to_check in other_keys:\n            if to_check in check_dict:\n                problems.append((x[\"description\"], to_check, subkey))\n    if len(problems) > 0:\n        raise ValueError(\"\\n\".join([\"Incorrectly nested keys found in sample YAML. These should be top level:\",\n                                    \" sample         |   key name      |   nested under \",\n                                    \"----------------+-----------------+----------------\"] +\n                                   [\"% 15s | % 15s | % 15s\" % (a, b, c) for (a, b, c) in problems]))", "response": "Ensure that the keys in xs are not incorrectly nested under other_keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_for_degenerate_interesting_groups(items):\n    igkey = (\"algorithm\", \"bcbiornaseq\", \"interesting_groups\")\n    interesting_groups = tz.get_in(igkey, items[0], [])\n    if isinstance(interesting_groups, str):\n        interesting_groups = [interesting_groups]\n    for group in interesting_groups:\n        values = [tz.get_in((\"metadata\", group), x, None) for x in items]\n        if all(x is None for x in values):\n            raise ValueError(\"group %s is labelled as an interesting group, \"\n                             \"but does not appear in the metadata.\" % group)\n        if len(list(tz.unique(values))) == 1:\n            raise ValueError(\"group %s is marked as an interesting group, \"\n                             \"but all samples have the same value.\" % group)", "response": "Check that interesting_groups are not all of the same for all of the samples."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks for unexpected keys in the algorithm section.", "response": "def _check_algorithm_keys(item):\n    \"\"\"Check for unexpected keys in the algorithm section.\n\n    Needs to be manually updated when introducing new keys, but avoids silent bugs\n    with typos in key names.\n    \"\"\"\n    problem_keys = [k for k in item[\"algorithm\"].keys() if k not in ALGORITHM_KEYS]\n    if len(problem_keys) > 0:\n        raise ValueError(\"Unexpected configuration keyword in 'algorithm' section: %s\\n\"\n                         \"See configuration documentation for supported options:\\n%s\\n\"\n                         % (problem_keys, ALG_DOC_URL))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _check_algorithm_values(item):\n    problems = []\n    for k, v in item.get(\"algorithm\", {}).items():\n        if v is True and k not in ALG_ALLOW_BOOLEANS:\n            problems.append(\"%s set as true\" % k)\n        elif v is False and (k not in ALG_ALLOW_BOOLEANS and k not in ALG_ALLOW_FALSE):\n            problems.append(\"%s set as false\" % k)\n    if len(problems) > 0:\n        raise ValueError(\"Incorrect settings in 'algorithm' section for %s:\\n%s\"\n                         \"\\nSee configuration documentation for supported options:\\n%s\\n\"\n                         % (item[\"description\"], \"\\n\".join(problems), ALG_DOC_URL))", "response": "Check for misplaced boolean values in the algorithms."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_toplevel_misplaced(item):\n    problem_keys = [k for k in item.keys() if k in ALGORITHM_KEYS]\n    if len(problem_keys) > 0:\n        raise ValueError(\"Unexpected configuration keywords found in top level of %s: %s\\n\"\n                         \"This should be placed in the 'algorithm' section.\"\n                         % (item[\"description\"], problem_keys))\n    problem_keys = [k for k in item.keys() if k not in TOPLEVEL_KEYS]\n    if len(problem_keys) > 0:\n        raise ValueError(\"Unexpected configuration keywords found in top level of %s: %s\\n\"\n                         % (item[\"description\"], problem_keys))", "response": "Check for any keys accidentally placed at the top level."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _check_quality_format(items):\n    SAMPLE_FORMAT = {\"illumina_1.3+\": \"illumina\",\n                     \"illumina_1.5+\": \"illumina\",\n                     \"illumina_1.8+\": \"standard\",\n                     \"solexa\": \"solexa\",\n                     \"sanger\": \"standard\"}\n    fastq_extensions = [\"fq.gz\", \"fastq.gz\", \".fastq\", \".fq\"]\n\n    for item in items:\n        specified_format = item[\"algorithm\"].get(\"quality_format\", \"standard\").lower()\n        if specified_format not in SAMPLE_FORMAT.values():\n            raise ValueError(\"Quality format specified in the YAML file\"\n                             \"is not supported. Supported values are %s.\"\n                             % (SAMPLE_FORMAT.values()))\n\n        fastq_file = next((f for f in item.get(\"files\") or [] if f.endswith(tuple(fastq_extensions))), None)\n\n        if fastq_file and specified_format and not objectstore.is_remote(fastq_file):\n            fastq_format = _detect_fastq_format(fastq_file)\n            detected_encodings = set([SAMPLE_FORMAT[x] for x in fastq_format])\n            if detected_encodings:\n                if specified_format not in detected_encodings:\n                    raise ValueError(\"Quality format specified in the YAML \"\n                                     \"file might be a different encoding. \"\n                                     \"'%s' was specified but possible formats \"\n                                     \"detected were %s.\" % (specified_format,\n                                                            \", \".join(detected_encodings)))", "response": "Check if quality_format is standard and fastq_format is not sanger\n    Check if quality_format is sanger or illumina"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _check_aligner(item):\n    allowed = set(list(alignment.TOOLS.keys()) + [None, False])\n    if item[\"algorithm\"].get(\"aligner\") not in allowed:\n        raise ValueError(\"Unexpected algorithm 'aligner' parameter: %s\\n\"\n                         \"Supported options: %s\\n\" %\n                         (item[\"algorithm\"].get(\"aligner\"), sorted(list(allowed))))", "response": "Ensure specified aligner is valid choice."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nensure specified variantcaller is a valid choice.", "response": "def _check_variantcaller(item):\n    \"\"\"Ensure specified variantcaller is a valid choice.\n    \"\"\"\n    allowed = set(list(genotype.get_variantcallers().keys()) + [None, False])\n    vcs = item[\"algorithm\"].get(\"variantcaller\")\n    if not isinstance(vcs, dict):\n        vcs = {\"variantcaller\": vcs}\n    for vc_set in vcs.values():\n        if not isinstance(vc_set, (tuple, list)):\n            vc_set = [vc_set]\n        problem = [x for x in vc_set if x not in allowed]\n        if len(problem) > 0:\n            raise ValueError(\"Unexpected algorithm 'variantcaller' parameter: %s\\n\"\n                             \"Supported options: %s\\n\" % (problem, sorted(list(allowed))))\n    # Ensure germline somatic calling only specified with tumor/normal samples\n    if \"germline\" in vcs or \"somatic\" in vcs:\n        paired = vcfutils.get_paired_phenotype(item)\n        if not paired:\n            raise ValueError(\"%s: somatic/germline calling in 'variantcaller' \"\n                             \"but tumor/normal metadata phenotype not specified\" % dd.get_sample_name(item))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nensure the provide structural variant caller is valid.", "response": "def _check_svcaller(item):\n    \"\"\"Ensure the provide structural variant caller is valid.\n    \"\"\"\n    allowed = set(reduce(operator.add, [list(d.keys()) for d in structural._CALLERS.values()]) + [None, False])\n    svs = item[\"algorithm\"].get(\"svcaller\")\n    if not isinstance(svs, (list, tuple)):\n        svs = [svs]\n    problem = [x for x in svs if x not in allowed]\n    if len(problem) > 0:\n        raise ValueError(\"Unexpected algorithm 'svcaller' parameters: %s\\n\"\n                         \"Supported options: %s\\n\" % (\" \".join([\"'%s'\" % x for x in problem]),\n                                                      sorted(list(allowed))))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check_hetcaller(item):\n    svs = _get_as_list(item, \"svcaller\")\n    hets = _get_as_list(item, \"hetcaller\")\n    if hets or any([x in svs for x in [\"titancna\", \"purecn\"]]):\n        if not any([x in svs for x in [\"cnvkit\", \"gatk-cnv\"]]):\n            raise ValueError(\"Heterogeneity caller used but need CNV calls. Add `gatk4-cnv` \"\n                             \"or `cnvkit` to `svcaller` in sample: %s\" % item[\"description\"])", "response": "Ensure upstream SV callers requires to heterogeneity analysis are available."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nensure specified jointcaller is valid.", "response": "def _check_jointcaller(data):\n    \"\"\"Ensure specified jointcaller is valid.\n    \"\"\"\n    allowed = set(joint.get_callers() + [None, False])\n    cs = data[\"algorithm\"].get(\"jointcaller\", [])\n    if not isinstance(cs, (tuple, list)):\n        cs = [cs]\n    problem = [x for x in cs if x not in allowed]\n    if len(problem) > 0:\n        raise ValueError(\"Unexpected algorithm 'jointcaller' parameter: %s\\n\"\n                         \"Supported options: %s\\n\" % (problem, sorted(list(allowed), key=lambda x: x or \"\")))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _check_realign(data):\n    if \"gatk4\" not in data[\"algorithm\"].get(\"tools_off\", []) and not \"gatk4\" == data[\"algorithm\"].get(\"tools_off\"):\n        if data[\"algorithm\"].get(\"realign\"):\n            raise ValueError(\"In sample %s, realign specified but it is not supported for GATK4. \"\n                             \"Realignment is generally not necessary for most variant callers.\" %\n                             (dd.get_sample_name(data)))", "response": "Check for realignment which is not supported in GATK4"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking for valid values for trim_reads.", "response": "def _check_trim(data):\n    \"\"\"Check for valid values for trim_reads.\n    \"\"\"\n    trim = data[\"algorithm\"].get(\"trim_reads\")\n    if trim:\n        if trim == \"fastp\" and data[\"algorithm\"].get(\"align_split_size\") is not False:\n            raise ValueError(\"In sample %s, `trim_reads: fastp` currently requires `align_split_size: false`\" %\n                             (dd.get_sample_name(data)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check_sample_config(items, in_file, config):\n    logger.info(\"Checking sample YAML configuration: %s\" % in_file)\n    _check_quality_format(items)\n    _check_for_duplicates(items, \"lane\")\n    _check_for_duplicates(items, \"description\")\n    _check_for_degenerate_interesting_groups(items)\n    _check_for_batch_clashes(items)\n    _check_for_problem_somatic_batches(items, config)\n    _check_for_misplaced(items, \"algorithm\",\n                         [\"resources\", \"metadata\", \"analysis\",\n                          \"description\", \"genome_build\", \"lane\", \"files\"])\n\n    [_check_toplevel_misplaced(x) for x in items]\n    [_check_algorithm_keys(x) for x in items]\n    [_check_algorithm_values(x) for x in items]\n    [_check_aligner(x) for x in items]\n    [_check_variantcaller(x) for x in items]\n    [_check_svcaller(x) for x in items]\n    [_check_hetcaller(x) for x in items]\n    [_check_indelcaller(x) for x in items]\n    [_check_jointcaller(x) for x in items]\n    [_check_hlacaller(x) for x in items]\n    [_check_realign(x) for x in items]\n    [_check_trim(x) for x in items]", "response": "Identify common problems in input sample configuration files."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _file_to_abs(x, dnames, makedir=False):\n    if x is None or os.path.isabs(x):\n        return x\n    elif isinstance(x, six.string_types) and objectstore.is_remote(x):\n        return x\n    elif isinstance(x, six.string_types) and x.lower() == \"none\":\n        return None\n    else:\n        for dname in dnames:\n            if dname:\n                normx = os.path.normpath(os.path.join(dname, x))\n                if os.path.exists(normx):\n                    return normx\n                elif makedir:\n                    utils.safe_makedir(normx)\n                    return normx\n        raise ValueError(\"Did not find input file %s in %s\" % (x, dnames))", "response": "Make a file absolute using the supplied base directory choices."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _normalize_files(item, fc_dir=None):\n    files = item.get(\"files\")\n    if files:\n        if isinstance(files, six.string_types):\n            files = [files]\n        fastq_dir = flowcell.get_fastq_dir(fc_dir) if fc_dir else os.getcwd()\n        files = [_file_to_abs(x, [os.getcwd(), fc_dir, fastq_dir]) for x in files]\n        files = [x for x in files if x]\n        _sanity_check_files(item, files)\n        item[\"files\"] = files\n    return item", "response": "Ensure the files argument is a list of absolute file names."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck that the input files correspond with the currently supported approaches.", "response": "def _sanity_check_files(item, files):\n    \"\"\"Ensure input files correspond with supported approaches.\n\n    Handles BAM, fastqs, plus split fastqs.\n    \"\"\"\n    msg = None\n    file_types = set([(\"bam\" if x.endswith(\".bam\") else \"fastq\") for x in files if x])\n    if len(file_types) > 1:\n        msg = \"Found multiple file types (BAM and fastq)\"\n    file_type = file_types.pop()\n    if file_type == \"bam\":\n        if len(files) != 1:\n            msg = \"Expect a single BAM file input as input\"\n    elif file_type == \"fastq\":\n        if len(files) not in [1, 2] and item[\"analysis\"].lower() != \"scrna-seq\":\n            pair_types = set([len(xs) for xs in fastq.combine_pairs(files)])\n            if len(pair_types) != 1 or pair_types.pop() not in [1, 2]:\n                msg = \"Expect either 1 (single end) or 2 (paired end) fastq inputs\"\n        if len(files) == 2 and files[0] == files[1]:\n            msg = \"Expect both fastq files to not be the same\"\n    if msg:\n        raise ValueError(\"%s for %s: %s\" % (msg, item.get(\"description\", \"\"), files))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_yaml(yaml_in, yaml_fn):\n    try:\n        import yamllint.linter as linter\n        from yamllint.config import YamlLintConfig\n    except ImportError:\n        return\n    conf = \"\"\"{\"extends\": \"relaxed\",\n               \"rules\": {\"trailing-spaces\": {\"level\": \"warning\"},\n                         \"new-lines\": {\"level\": \"warning\"},\n                         \"new-line-at-end-of-file\": {\"level\": \"warning\"}}}\"\"\"\n    if utils.file_exists(yaml_in):\n        with open(yaml_in) as in_handle:\n            yaml_in = in_handle.read()\n    out = linter.run(yaml_in, YamlLintConfig(conf))\n\n    for problem in out:\n        msg = '%(fn)s:%(line)s:%(col)s: [%(level)s] %(msg)s' % {'fn': yaml_fn,\n                                                                'line': problem.line,\n                                                                'col': problem.column,\n                                                                'level': problem.level,\n                                                                'msg': problem.message}\n        if problem.level == \"error\":\n            raise ValueError(msg)", "response": "Check with yamllint the yaml syntaxes\n    Looking for duplicate keys."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread run information from a YAML file.", "response": "def _run_info_from_yaml(dirs, run_info_yaml, config, sample_names=None,\n                        is_cwl=False, integrations=None):\n    \"\"\"Read run information from a passed YAML file.\n    \"\"\"\n    validate_yaml(run_info_yaml, run_info_yaml)\n    with open(run_info_yaml) as in_handle:\n        loaded = yaml.safe_load(in_handle)\n    fc_name, fc_date = None, None\n    if dirs.get(\"flowcell\"):\n        try:\n            fc_name, fc_date = flowcell.parse_dirname(dirs.get(\"flowcell\"))\n        except ValueError:\n            pass\n    global_config = {}\n    global_vars = {}\n    resources = {}\n    integration_config = {}\n    if isinstance(loaded, dict):\n        global_config = copy.deepcopy(loaded)\n        del global_config[\"details\"]\n        if \"fc_name\" in loaded:\n            fc_name = loaded[\"fc_name\"].replace(\" \", \"_\")\n        if \"fc_date\" in loaded:\n            fc_date = str(loaded[\"fc_date\"]).replace(\" \", \"_\")\n        global_vars = global_config.pop(\"globals\", {})\n        resources = global_config.pop(\"resources\", {})\n        for iname in [\"arvados\"]:\n            integration_config[iname] = global_config.pop(iname, {})\n        loaded = loaded[\"details\"]\n    if sample_names:\n        loaded = [x for x in loaded if x[\"description\"] in sample_names]\n\n    if integrations:\n        for iname, retriever in integrations.items():\n            if iname in config:\n                config[iname] = retriever.set_cache(config[iname])\n                loaded = retriever.add_remotes(loaded, config[iname])\n\n    run_details = []\n    for i, item in enumerate(loaded):\n        item = _normalize_files(item, dirs.get(\"flowcell\"))\n        if \"lane\" not in item:\n            item[\"lane\"] = str(i + 1)\n        item[\"lane\"] = _clean_characters(item[\"lane\"])\n        if \"description\" not in item:\n            if _item_is_bam(item):\n                item[\"description\"] = get_sample_name(item[\"files\"][0])\n            else:\n                raise ValueError(\"No `description` sample name provided for input #%s\" % (i + 1))\n        description = _clean_characters(item[\"description\"])\n        item[\"description\"] = description\n        # make names R safe if we are likely to use R downstream\n        if item[\"analysis\"].lower() in R_DOWNSTREAM_ANALYSIS:\n            if description[0].isdigit():\n                valid = \"X\" + description\n                logger.info(\"%s is not a valid R name, converting to %s.\" % (description, valid))\n                item[\"description\"] = valid\n        if \"upload\" not in item and not is_cwl:\n            upload = global_config.get(\"upload\", {})\n            # Handle specifying a local directory directly in upload\n            if isinstance(upload, six.string_types):\n                upload = {\"dir\": upload}\n            if not upload:\n                upload[\"dir\"] = \"../final\"\n            if fc_name:\n                upload[\"fc_name\"] = fc_name\n            if fc_date:\n                upload[\"fc_date\"] = fc_date\n            upload[\"run_id\"] = \"\"\n            if upload.get(\"dir\"):\n                upload[\"dir\"] = _file_to_abs(upload[\"dir\"], [dirs.get(\"work\")], makedir=True)\n            item[\"upload\"] = upload\n        item[\"algorithm\"] = _replace_global_vars(item[\"algorithm\"], global_vars)\n        item[\"algorithm\"] = genome.abs_file_paths(item[\"algorithm\"],\n                                                  ignore_keys=ALGORITHM_NOPATH_KEYS,\n                                                  fileonly_keys=ALGORITHM_FILEONLY_KEYS,\n                                                  do_download=all(not x for x in integrations.values()))\n        item[\"genome_build\"] = str(item.get(\"genome_build\", \"\"))\n        item[\"algorithm\"] = _add_algorithm_defaults(item[\"algorithm\"], item.get(\"analysis\", \"\"), is_cwl)\n        item[\"metadata\"] = add_metadata_defaults(item.get(\"metadata\", {}))\n        item[\"rgnames\"] = prep_rg_names(item, config, fc_name, fc_date)\n        if item.get(\"files\"):\n            item[\"files\"] = [genome.abs_file_paths(f, do_download=all(not x for x in integrations.values()))\n                             for f in item[\"files\"]]\n        elif \"files\" in item:\n            del item[\"files\"]\n        if item.get(\"vrn_file\") and isinstance(item[\"vrn_file\"], six.string_types):\n            item[\"vrn_file\"] = genome.abs_file_paths(item[\"vrn_file\"],\n                                                     do_download=all(not x for x in integrations.values()))\n            if os.path.isfile(item[\"vrn_file\"]):\n                # Try to prepare in place (or use ready to go inputs)\n                try:\n                    item[\"vrn_file\"] = vcfutils.bgzip_and_index(item[\"vrn_file\"], config,\n                                                                remove_orig=False)\n                # In case of permission errors, fix in inputs directory\n                except IOError:\n                    inputs_dir = utils.safe_makedir(os.path.join(dirs.get(\"work\", os.getcwd()), \"inputs\",\n                                                                 item[\"description\"]))\n                    item[\"vrn_file\"] = vcfutils.bgzip_and_index(item[\"vrn_file\"], config,\n                                                                remove_orig=False, out_dir=inputs_dir)\n            if not tz.get_in((\"metadata\", \"batch\"), item) and tz.get_in([\"algorithm\", \"validate\"], item):\n                raise ValueError(\"%s: Please specify a metadata batch for variant file (vrn_file) input.\\n\" %\n                                 (item[\"description\"]) +\n                                 \"Batching with a standard sample provides callable regions for validation.\")\n        item = _clean_metadata(item)\n        item = _clean_algorithm(item)\n        item = _organize_tools_on(item, is_cwl)\n        item = _clean_background(item)\n        # Add any global resource specifications\n        if \"resources\" not in item:\n            item[\"resources\"] = {}\n        for prog, pkvs in resources.items():\n            if prog not in item[\"resources\"]:\n                item[\"resources\"][prog] = {}\n            if pkvs is not None:\n                for key, val in pkvs.items():\n                    item[\"resources\"][prog][key] = val\n        for iname, ivals in integration_config.items():\n            if ivals:\n                if iname not in item:\n                    item[iname] = {}\n                for k, v in ivals.items():\n                    item[iname][k] = v\n\n        run_details.append(item)\n    _check_sample_config(run_details, run_info_yaml, config)\n    return run_details"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding default metadata for algorithm inputs.", "response": "def add_metadata_defaults(md):\n    \"\"\"Central location for defaults for algorithm inputs.\n    \"\"\"\n    defaults = {\"batch\": None,\n                \"phenotype\": \"\"}\n    for k, v in defaults.items():\n        if k not in md:\n            md[k] = v\n    return md"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd default values for the algorithm inputs.", "response": "def _add_algorithm_defaults(algorithm, analysis, is_cwl):\n    \"\"\"Central location specifying defaults for algorithm inputs.\n\n    Converts allowed multiple inputs into lists if specified as a single item.\n    Converts required single items into string if specified as a list\n    \"\"\"\n    if not algorithm:\n        algorithm = {}\n    defaults = {\"archive\": None,\n                \"tools_off\": [],\n                \"tools_on\": [],\n                \"qc\": [],\n                \"trim_reads\": False,\n                \"adapters\": [],\n                \"effects\": \"snpeff\",\n                \"quality_format\": \"standard\",\n                \"expression_caller\": [\"salmon\"] if analysis.lower().find(\"rna-seq\") >= 0 else None,\n                \"align_split_size\": None,\n                \"bam_clean\": False,\n                \"nomap_split_size\": 250,\n                \"nomap_split_targets\": _get_nomap_split_targets(analysis, is_cwl),\n                \"mark_duplicates\": False if not algorithm.get(\"aligner\") else True,\n                \"coverage_interval\": None,\n                \"min_allele_fraction\": 10.0,\n                \"recalibrate\": False,\n                \"realign\": False,\n                \"ensemble\": None,\n                \"exclude_regions\": [],\n                \"variant_regions\": None,\n                \"svcaller\": [],\n                \"svvalidate\": None,\n                \"svprioritize\": None,\n                \"validate\": None,\n                \"validate_regions\": None,\n                \"vcfanno\": []}\n    convert_to_list = set([\"tools_off\", \"tools_on\", \"hetcaller\", \"variantcaller\", \"svcaller\", \"qc\", \"disambiguate\",\n                           \"vcfanno\", \"adapters\", \"custom_trim\", \"exclude_regions\"])\n    convert_to_single = set([\"hlacaller\", \"indelcaller\", \"validate_method\"])\n    for k, v in defaults.items():\n        if k not in algorithm:\n            algorithm[k] = v\n    for k, v in algorithm.items():\n        if k in convert_to_list:\n            if v and not isinstance(v, (list, tuple)) and not isinstance(v, dict):\n                algorithm[k] = [v]\n            # ensure dictionary specified inputs get converted into individual lists\n            elif v and not isinstance(v, (list, tuple)) and isinstance(v, dict):\n                new = {}\n                for innerk, innerv in v.items():\n                    if innerv and not isinstance(innerv, (list, tuple)) and not isinstance(innerv, dict):\n                        innerv = [innerv]\n                    new[innerk] = innerv\n                algorithm[k] = new\n            elif v is None:\n                algorithm[k] = []\n        elif k in convert_to_single:\n            if v and not isinstance(v, six.string_types):\n                if isinstance(v, (list, tuple)) and len(v) == 1:\n                    algorithm[k] = v[0]\n                else:\n                    raise ValueError(\"Unexpected input in sample YAML; need a single item for %s: %s\" % (k, v))\n    return algorithm"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreplaces globally shared names from input header with value.", "response": "def _replace_global_vars(xs, global_vars):\n    \"\"\"Replace globally shared names from input header with value.\n\n    The value of the `algorithm` item may be a pointer to a real\n    file specified in the `global` section. If found, replace with\n    the full value.\n    \"\"\"\n    if isinstance(xs, (list, tuple)):\n        return [_replace_global_vars(x) for x in xs]\n    elif isinstance(xs, dict):\n        final = {}\n        for k, v in xs.items():\n            if isinstance(v, six.string_types) and v in global_vars:\n                v = global_vars[v]\n            final[k] = v\n        return final\n    else:\n        return xs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprepare system configuration information from an input configuration file.", "response": "def prep_system(run_info_yaml, bcbio_system=None):\n    \"\"\"Prepare system configuration information from an input configuration file.\n\n    This does the work of parsing the system input file and setting up directories\n    for use in 'organize'.\n    \"\"\"\n    work_dir = os.getcwd()\n    config, config_file = config_utils.load_system_config(bcbio_system, work_dir)\n    dirs = setup_directories(work_dir, os.path.normpath(os.path.dirname(os.path.dirname(run_info_yaml))),\n                             config, config_file)\n    return [dirs, config, run_info_yaml]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun platypus variant calling on a set of aligned BAM files.", "response": "def run(align_bams, items, ref_file, assoc_files, region, out_file):\n    \"\"\"Run platypus variant calling, germline whole genome or exome.\n    \"\"\"\n    assert out_file.endswith(\".vcf.gz\")\n    if not utils.file_exists(out_file):\n        with file_transaction(items[0], out_file) as tx_out_file:\n            for align_bam in align_bams:\n                bam.index(align_bam, items[0][\"config\"])\n            cmd = [\"platypus\", \"callVariants\", \"--regions=%s\" % _subset_regions(region, out_file, items),\n                   \"--bamFiles=%s\" % \",\".join(align_bams),\n                   \"--refFile=%s\" % dd.get_ref_file(items[0]), \"--output=-\",\n                   \"--logFileName\", \"/dev/null\", \"--verbosity=1\"]\n            resources = config_utils.get_resources(\"platypus\", items[0][\"config\"])\n            if resources.get(\"options\"):\n                # normalize options so we can set defaults without overwriting user specified\n                for opt in resources[\"options\"]:\n                    if \"=\" in opt:\n                        key, val = opt.split(\"=\")\n                        cmd.extend([key, val])\n                    else:\n                        cmd.append(opt)\n            if any(\"gvcf\" in dd.get_tools_on(d) for d in items):\n                cmd += [\"--outputRefCalls\", \"1\", \"--refCallBlockSize\", \"50000\"]\n            # Adjust default filter thresholds to achieve similar sensitivity/specificity to other callers\n            # Currently not used after doing more cross validation as they increase false positives\n            # which seems to be a major advantage for Platypus users.\n            # tuned_opts = [\"--hapScoreThreshold\", \"10\", \"--scThreshold\", \"0.99\", \"--filteredReadsFrac\", \"0.9\",\n            #               \"--rmsmqThreshold\", \"20\", \"--qdThreshold\", \"0\", \"--abThreshold\", \"0.0001\",\n            #               \"--minVarFreq\", \"0.0\", \"--assemble\", \"1\"]\n            # for okey, oval in utils.partition_all(2, tuned_opts):\n            #     if okey not in cmd:\n            #         cmd.extend([okey, oval])\n\n            # Avoid filtering duplicates on high depth targeted regions where we don't mark duplicates\n            if any(not dd.get_mark_duplicates(data) for data in items):\n                cmd += [\"--filterDuplicates=0\"]\n            post_process_cmd = (\" | %s | %s | %s | vcfallelicprimitives -t DECOMPOSED --keep-geno | vcffixup - | \"\n                                \"vcfstreamsort | bgzip -c > %s\" %\n                                (vcfutils.fix_ambiguous_cl(), vcfutils.fix_ambiguous_cl(5),\n                                 vcfutils.add_contig_to_header_cl(dd.get_ref_file(items[0]), tx_out_file),\n                                 tx_out_file))\n            do.run(\" \".join(cmd) + post_process_cmd, \"platypus variant calling\")\n        out_file = vcfutils.bgzip_and_index(out_file, items[0][\"config\"])\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _subset_regions(region, base_file, items):\n    variant_regions = bedutils.population_variant_regions(items, merged=True)\n    target = pshared.subset_variant_regions(variant_regions, region, base_file, items)\n    if isinstance(target, six.string_types) and os.path.isfile(target):\n        return target\n    else:\n        return bamprep.region_to_gatk(target)", "response": "Subset a region to a BED file or genomic region for calling."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates several log files", "response": "def run(bam_file, data, out_dir):\n    \"\"\"Create several log files\"\"\"\n    m = {\"base\": None, \"secondary\": []}\n    m.update(_mirbase_stats(data, out_dir))\n    m[\"secondary\"].append(_seqcluster_stats(data, out_dir))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _mirbase_stats(data, out_dir):\n    utils.safe_makedir(out_dir)\n    out_file = os.path.join(out_dir, \"%s_bcbio_mirbase.txt\" % dd.get_sample_name(data))\n    out_file_novel = os.path.join(out_dir, \"%s_bcbio_mirdeeep2.txt\" % dd.get_sample_name(data))\n    mirbase_fn = data.get(\"seqbuster\", None)\n    if mirbase_fn:\n        _get_stats_from_miraligner(mirbase_fn, out_file, \"seqbuster\")\n    mirdeep_fn = data.get(\"seqbuster_novel\", None)\n    if mirdeep_fn:\n        _get_stats_from_miraligner(mirdeep_fn, out_file_novel, \"mirdeep2\")\n    return {\"base\": out_file, \"secondary\": [out_file_novel]}", "response": "Create stats from miraligner"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a flowcell into a samplesheet for demultiplexing.", "response": "def from_flowcell(run_folder, lane_details, out_dir=None):\n    \"\"\"Convert a flowcell into a samplesheet for demultiplexing.\n    \"\"\"\n    fcid = os.path.basename(run_folder)\n    if out_dir is None:\n        out_dir = run_folder\n    out_file = os.path.join(out_dir, \"%s.csv\" % fcid)\n    with open(out_file, \"w\") as out_handle:\n        writer = csv.writer(out_handle)\n        writer.writerow([\"FCID\", \"Lane\", \"Sample_ID\", \"SampleRef\", \"Index\",\n                         \"Description\", \"Control\", \"Recipe\", \"Operator\", \"SampleProject\"])\n        for ldetail in lane_details:\n            writer.writerow(_lane_detail_to_ss(fcid, ldetail))\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _lane_detail_to_ss(fcid, ldetail):\n    return [fcid, ldetail[\"lane\"], ldetail[\"name\"], ldetail[\"genome_build\"],\n            ldetail[\"bc_index\"], ldetail[\"description\"].encode(\"ascii\", \"ignore\"), \"N\", \"\", \"\",\n            ldetail[\"project_name\"]]", "response": "Convert information about a lane into Illumina samplesheet output."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\norganizing flat lane information into nested YAML structure.", "response": "def _organize_lanes(info_iter, barcode_ids):\n    \"\"\"Organize flat lane information into nested YAML structure.\n    \"\"\"\n    all_lanes = []\n    for (fcid, lane, sampleref), info in itertools.groupby(info_iter, lambda x: (x[0], x[1], x[1])):\n        info = list(info)\n        cur_lane = dict(flowcell_id=fcid, lane=lane, genome_build=info[0][3], analysis=\"Standard\")\n        if not _has_barcode(info):\n            cur_lane[\"description\"] = info[0][1]\n        else: # barcoded sample\n            cur_lane[\"description\"] = \"Barcoded lane %s\" % lane\n            multiplex = []\n            for (_, _, sample_id, _, bc_seq) in info:\n                bc_type, bc_id = barcode_ids[bc_seq]\n                multiplex.append(dict(barcode_type=bc_type,\n                                      barcode_id=bc_id,\n                                      sequence=bc_seq,\n                                      name=sample_id))\n            cur_lane[\"multiplex\"] = multiplex\n        all_lanes.append(cur_lane)\n    return all_lanes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate unique barcode IDs assigned to sequences", "response": "def _generate_barcode_ids(info_iter):\n    \"\"\"Create unique barcode IDs assigned to sequences\n    \"\"\"\n    bc_type = \"SampleSheet\"\n    barcodes = list(set([x[-1] for x in info_iter]))\n    barcodes.sort()\n    barcode_ids = {}\n    for i, bc in enumerate(barcodes):\n        barcode_ids[bc] = (bc_type, i+1)\n    return barcode_ids"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _read_input_csv(in_file):\n    with io.open(in_file, newline=None) as in_handle:\n        reader = csv.reader(in_handle)\n        next(reader) # header\n        for line in reader:\n            if line: # empty lines\n                (fc_id, lane, sample_id, genome, barcode) = line[:5]\n                yield fc_id, lane, sample_id, genome, barcode", "response": "Parse useful details from SampleSheet CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the unique flowcell id represented in the SampleSheet.", "response": "def _get_flowcell_id(in_file, require_single=True):\n    \"\"\"Retrieve the unique flowcell id represented in the SampleSheet.\n    \"\"\"\n    fc_ids = set([x[0] for x in _read_input_csv(in_file)])\n    if require_single and len(fc_ids) > 1:\n        raise ValueError(\"There are several FCIDs in the same samplesheet file: %s\" % in_file)\n    else:\n        return fc_ids"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef csv2yaml(in_file, out_file=None):\n    if out_file is None:\n        out_file = \"%s.yaml\" % os.path.splitext(in_file)[0]\n    barcode_ids = _generate_barcode_ids(_read_input_csv(in_file))\n    lanes = _organize_lanes(_read_input_csv(in_file), barcode_ids)\n    with open(out_file, \"w\") as out_handle:\n        out_handle.write(yaml.safe_dump(lanes, default_flow_style=False))\n    return out_file", "response": "Convert a CSV SampleSheet to YAML run_info format."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_has_samplesheet(fc_dir, config, require_single=True):\n    fc_name, _ = flowcell.parse_dirname(fc_dir)\n    sheet_dirs = config.get(\"samplesheet_directories\", [])\n    fcid_sheet = {}\n    for ss_dir in (s for s in sheet_dirs if os.path.exists(s)):\n        with utils.chdir(ss_dir):\n            for ss in glob.glob(\"*.csv\"):\n                fc_ids = _get_flowcell_id(ss, require_single)\n                for fcid in fc_ids:\n                    if fcid:\n                        fcid_sheet[fcid] = os.path.join(ss_dir, ss)\n    # difflib handles human errors while entering data on the SampleSheet.\n    # Only one best candidate is returned (if any). 0.85 cutoff allows for\n    # maximum of 2 mismatches in fcid\n\n    potential_fcids = difflib.get_close_matches(fc_name, fcid_sheet.keys(), 1, 0.85)\n    if len(potential_fcids) > 0 and potential_fcids[0] in fcid_sheet:\n        return fcid_sheet[potential_fcids[0]]\n    else:\n        return None", "response": "Checks if there s a suitable SampleSheet. csv present for the run\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparallel target to combine multiple BAM files into one BAM file.", "response": "def combine_bam(in_files, out_file, config):\n    \"\"\"Parallel target to combine multiple BAM files.\n    \"\"\"\n    runner = broad.runner_from_path(\"picard\", config)\n    runner.run_fn(\"picard_merge\", in_files, out_file)\n    for in_file in in_files:\n        save_diskspace(in_file, \"Merged into {0}\".format(out_file), config)\n    bam.index(out_file, config)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving contigs without alternatives as defined in bwa. alts files.", "response": "def get_noalt_contigs(data):\n    \"\"\"Retrieve contigs without alternatives as defined in bwa *.alts files.\n\n    If no alt files present (when we're not aligning with bwa), work around\n    with standard set of alts based on hg38 -- anything with HLA, _alt or\n    _decoy in the name.\n    \"\"\"\n    alts = set([])\n    alt_files = [f for f in tz.get_in([\"reference\", \"bwa\", \"indexes\"], data, []) if f.endswith(\"alt\")]\n    if alt_files:\n        for alt_file in alt_files:\n            with open(alt_file) as in_handle:\n                for line in in_handle:\n                    if not line.startswith(\"@\"):\n                        alts.add(line.split()[0].strip())\n    else:\n        for contig in ref.file_contigs(dd.get_ref_file(data)):\n            if (\"_alt\" in contig.name or \"_decoy\" in contig.name or\n                  contig.name.startswith(\"HLA-\") or \":\" in contig.name):\n                alts.add(contig.name)\n    return [c for c in ref.file_contigs(dd.get_ref_file(data)) if c.name not in alts]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_nochr_reads(in_file, out_file, config):\n    if not file_exists(out_file):\n        with file_transaction(config, out_file) as tx_out_file:\n            samtools = config_utils.get_program(\"samtools\", config)\n            cmd = \"{samtools} view -b -f 4 {in_file} > {tx_out_file}\"\n            do.run(cmd.format(**locals()), \"Select unmapped reads\")\n    return out_file", "response": "Write a BAM file of reads that are not mapped on a reference chromosome."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_noanalysis_reads(in_file, region_file, out_file, config):\n    if not file_exists(out_file):\n        with file_transaction(config, out_file) as tx_out_file:\n            bedtools = config_utils.get_program(\"bedtools\", config)\n            sambamba = config_utils.get_program(\"sambamba\", config)\n            cl = (\"{sambamba} view -f bam -l 0 -L {region_file} {in_file} | \"\n                  \"{bedtools} intersect -abam - -b {region_file} -f 1.0 -nonamecheck\"\n                  \"> {tx_out_file}\")\n            do.run(cl.format(**locals()), \"Select unanalyzed reads\")\n    return out_file", "response": "Write a BAM file of reads that are not analyzed in a specified region."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef subset_bam_by_region(in_file, region, config, out_file_base=None):\n    if out_file_base is not None:\n        base, ext = os.path.splitext(out_file_base)\n    else:\n        base, ext = os.path.splitext(in_file)\n    out_file = \"%s-subset%s%s\" % (base, region, ext)\n    if not file_exists(out_file):\n        with pysam.Samfile(in_file, \"rb\") as in_bam:\n            target_tid = in_bam.gettid(region)\n            assert region is not None, \\\n                   \"Did not find reference region %s in %s\" % \\\n                   (region, in_file)\n            with file_transaction(config, out_file) as tx_out_file:\n                with pysam.Samfile(tx_out_file, \"wb\", template=in_bam) as out_bam:\n                    for read in in_bam:\n                        if read.tid == target_tid:\n                            out_bam.write(read)\n    return out_file", "response": "Subset BAM files based on specified chromosome region."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef subset_bed_by_chrom(in_file, chrom, data, out_dir=None):\n    if out_dir is None:\n        out_dir = os.path.dirname(in_file)\n    base, ext = os.path.splitext(os.path.basename(in_file))\n    out_file = os.path.join(out_dir, \"%s-%s%s\" % (base, chrom, ext))\n    if not utils.file_uptodate(out_file, in_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            _rewrite_bed_with_chrom(in_file, tx_out_file, chrom)\n    return out_file", "response": "Subset a BED file to only have items from the specified chromosome."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_lcr_regions(orig_bed, items):\n    lcr_bed = tz.get_in([\"genome_resources\", \"variation\", \"lcr\"], items[0])\n    if lcr_bed and os.path.exists(lcr_bed) and \"lcr\" in get_exclude_regions(items):\n        return _remove_regions(orig_bed, [lcr_bed], \"nolcr\", items[0])\n    else:\n        return orig_bed", "response": "Remove low complexity regions from a BED file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_polyx_regions(in_file, items):\n    ex_bed = tz.get_in([\"genome_resources\", \"variation\", \"polyx\"], items[0])\n    if ex_bed and os.path.exists(ex_bed):\n        return _remove_regions(in_file, [ex_bed], \"nopolyx\", items[0])\n    else:\n        return in_file", "response": "Remove polyX stretches contributing to long variant runtimes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_highdepth_genome_exclusion(items):\n    out = []\n    for d in items:\n        d = utils.deepish_copy(d)\n        if dd.get_coverage_interval(d) == \"genome\":\n            e = dd.get_exclude_regions(d)\n            if \"highdepth\" not in e:\n                e.append(\"highdepth\")\n                d = dd.set_exclude_regions(d, e)\n        out.append(d)\n    return out", "response": "Add exclusions to input items to avoid slow runtimes on whole genomes."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove high depth regions from a BED file for analyzing a set of items.", "response": "def remove_highdepth_regions(in_file, items):\n    \"\"\"Remove high depth regions from a BED file for analyzing a set of calls.\n\n    Tries to avoid spurious errors and slow run times in collapsed repeat regions.\n\n    Also adds ENCODE blacklist regions which capture additional collapsed repeats\n    around centromeres.\n    \"\"\"\n    encode_bed = tz.get_in([\"genome_resources\", \"variation\", \"encode_blacklist\"], items[0])\n    if encode_bed and os.path.exists(encode_bed):\n        return _remove_regions(in_file, [encode_bed], \"glimit\", items[0])\n    else:\n        return in_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsubtract a list of BED files from an input BED.", "response": "def _remove_regions(in_file, remove_beds, ext, data):\n    \"\"\"Subtract a list of BED files from an input BED.\n\n    General approach handling none, one and more remove_beds.\n    \"\"\"\n    from bcbio.variation import bedutils\n    out_file = \"%s-%s.bed\" % (utils.splitext_plus(in_file)[0], ext)\n    if not utils.file_uptodate(out_file, in_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            with bedtools_tmpdir(data):\n                if len(remove_beds) == 0:\n                    to_remove = None\n                elif len(remove_beds) == 1:\n                    to_remove = remove_beds[0]\n                else:\n                    to_remove = \"%s-all.bed\" % utils.splitext_plus(tx_out_file)[0]\n                    with open(to_remove, \"w\") as out_handle:\n                        for b in remove_beds:\n                            with utils.open_gzipsafe(b) as in_handle:\n                                for line in in_handle:\n                                    parts = line.split(\"\\t\")\n                                    out_handle.write(\"\\t\".join(parts[:4]).rstrip() + \"\\n\")\n                    if utils.file_exists(to_remove):\n                        to_remove = bedutils.sort_merge(to_remove, data)\n                if to_remove and utils.file_exists(to_remove):\n                    cmd = \"bedtools subtract -nonamecheck -a {in_file} -b {to_remove} > {tx_out_file}\"\n                    do.run(cmd.format(**locals()), \"Remove problematic regions: %s\" % ext)\n                else:\n                    utils.symlink_plus(in_file, out_file)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve regions to exclude from a set of items.", "response": "def get_exclude_regions(items):\n    \"\"\"Retrieve regions to exclude from a set of items.\n\n    Includes back compatibility for older custom ways of specifying different\n    exclusions.\n    \"\"\"\n    def _get_sample_excludes(d):\n        excludes = dd.get_exclude_regions(d)\n        # back compatible\n        if tz.get_in((\"config\", \"algorithm\", \"remove_lcr\"), d, False):\n            excludes.append(\"lcr\")\n        return excludes\n    out = reduce(operator.add, [_get_sample_excludes(d) for d in items])\n    return sorted(list(set(out)))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove regions to exclude based on configuration: polyA, LCR, high depth.", "response": "def remove_exclude_regions(f):\n    \"\"\"Remove regions to exclude based on configuration: polyA, LCR, high depth.\n    \"\"\"\n    exclude_fns = {\"lcr\": remove_lcr_regions, \"highdepth\": remove_highdepth_regions,\n                   \"polyx\": remove_polyx_regions}\n    @functools.wraps(f)\n    def wrapper(variant_regions, region, out_file, items=None, do_merge=True, data=None):\n        region_bed = f(variant_regions, region, out_file, items, do_merge, data)\n        if region_bed and isinstance(region_bed, six.string_types) and os.path.exists(region_bed) and items:\n            for e in get_exclude_regions(items):\n                if e in exclude_fns:\n                    region_bed = exclude_fns[e](region_bed, items)\n        return region_bed\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_multiregion(region):\n    assert isinstance(region, (list, tuple)), region\n    if isinstance(region[0], (list, tuple)):\n        return region\n    else:\n        assert len(region) == 3\n        return [tuple(region)]", "response": "Convert a single region or multiple region specification into a multiregion list."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns BED file subset by a specified chromosome region. variant_regions is a BED file, region is a chromosome name or tuple of (name, start, end) for a genomic region.", "response": "def subset_variant_regions(variant_regions, region, out_file, items=None, do_merge=True, data=None):\n    \"\"\"Return BED file subset by a specified chromosome region.\n\n    variant_regions is a BED file, region is a chromosome name or tuple\n    of (name, start, end) for a genomic region.\n    \"\"\"\n    if region is None:\n        return variant_regions\n    elif variant_regions is None:\n        return region\n    elif not isinstance(region, (list, tuple)) and region.find(\":\") > 0:\n        raise ValueError(\"Partial chromosome regions not supported\")\n    else:\n        merge_text = \"-unmerged\" if not do_merge else \"\"\n        subset_file = \"{0}\".format(utils.splitext_plus(out_file)[0])\n        subset_file += \"%s-regions.bed\" % (merge_text)\n        if not os.path.exists(subset_file):\n            data = items[0] if items else data\n            with file_transaction(data, subset_file) as tx_subset_file:\n                if isinstance(region, (list, tuple)):\n                    _subset_bed_by_region(variant_regions, tx_subset_file, to_multiregion(region),\n                                          dd.get_ref_file(data), do_merge=do_merge)\n                else:\n                    _rewrite_bed_with_chrom(variant_regions, tx_subset_file, region)\n        if os.path.getsize(subset_file) == 0:\n            return region\n        else:\n            return subset_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npreparing a delly - specific exclude file eliminating chromosomes.", "response": "def _delly_exclude_file(items, base_file, chrom):\n    \"\"\"Prepare a delly-specific exclude file eliminating chromosomes.\n    Delly wants excluded chromosomes listed as just the chromosome, with no coordinates.\n    \"\"\"\n    base_exclude = sshared.prepare_exclude_file(items, base_file, chrom)\n    out_file = \"%s-delly%s\" % utils.splitext_plus(base_exclude)\n    with file_transaction(items[0], out_file) as tx_out_file:\n        with open(tx_out_file, \"w\") as out_handle:\n            with open(base_exclude) as in_handle:\n                for line in in_handle:\n                    parts = line.split(\"\\t\")\n                    if parts[0] == chrom:\n                        out_handle.write(line)\n                    else:\n                        out_handle.write(\"%s\\n\" % parts[0])\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _run_delly(bam_files, chrom, ref_file, work_dir, items):\n    batch = sshared.get_cur_batch(items)\n    ext = \"-%s-svs\" % batch if batch else \"-svs\"\n    out_file = os.path.join(work_dir, \"%s%s-%s.bcf\"\n                            % (os.path.splitext(os.path.basename(bam_files[0]))[0], ext, chrom))\n    final_file = \"%s.vcf.gz\" % (utils.splitext_plus(out_file)[0])\n    cores = min(utils.get_in(items[0], (\"config\", \"algorithm\", \"num_cores\"), 1),\n                len(bam_files))\n    if not utils.file_exists(out_file) and not utils.file_exists(final_file):\n        with file_transaction(items[0], out_file) as tx_out_file:\n            if sshared.has_variant_regions(items, out_file, chrom):\n                exclude = [\"-x\", _delly_exclude_file(items, out_file, chrom)]\n                cmd = [\"delly\", \"call\", \"-g\", ref_file, \"-o\", tx_out_file] + exclude + bam_files\n                multi_cmd = \"export OMP_NUM_THREADS=%s && export LC_ALL=C && \" % cores\n                try:\n                    do.run(multi_cmd + \" \".join(cmd), \"delly structural variant\")\n                except subprocess.CalledProcessError as msg:\n                    # Small input samples, write an empty vcf\n                    if \"Sample has not enough data to estimate library parameters\" in str(msg):\n                        pass\n                    # delly returns an error exit code if there are no variants\n                    elif \"No structural variants found\" not in str(msg):\n                        raise\n    return [_bgzip_and_clean(out_file, items)]", "response": "Run delly on the specified set of samples."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _bgzip_and_clean(bcf_file, items):\n    out_file = \"%s.vcf.gz\" % (utils.splitext_plus(bcf_file)[0])\n    if not utils.file_exists(out_file):\n        with file_transaction(items[0], out_file) as tx_out_file:\n            if not utils.file_exists(bcf_file):\n                vcfutils.write_empty_vcf(tx_out_file, samples=[dd.get_sample_name(d) for d in items])\n            else:\n                cmd = (\"bcftools view {bcf_file} | sed 's/\\.,\\.,\\././' | bgzip -c > {tx_out_file}\")\n                do.run(cmd.format(**locals()), \"Convert and clean delly output\")\n    return vcfutils.bgzip_and_index(out_file, items[0][\"config\"])", "response": "Create a bgzipped VCF output file from bcf for downstream processing."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npreparing subsampled BAM files for delly.", "response": "def _prep_subsampled_bams(data, work_dir):\n    \"\"\"Prepare a subsampled BAM file with discordants from samblaster and minimal correct pairs.\n\n    This attempts to minimize run times by pre-extracting useful reads mixed\n    with subsampled normal pairs to estimate paired end distributions:\n\n    https://groups.google.com/d/msg/delly-users/xmia4lwOd1Q/uaajoBkahAIJ\n\n    Subsamples correctly aligned reads to 100 million based on speedseq defaults and\n    evaluations on NA12878 whole genome data:\n\n    https://github.com/cc2qe/speedseq/blob/ca624ba9affb0bd0fb88834ca896e9122639ec94/bin/speedseq#L1102\n\n    XXX Currently not used as new versions of delly do not get good sensitivity\n    with downsampled BAMs.\n    \"\"\"\n    sr_bam, disc_bam = sshared.get_split_discordants(data, work_dir)\n    ds_bam = bam.downsample(dd.get_align_bam(data), data, 1e8,\n                            read_filter=\"-F 'not secondary_alignment and proper_pair'\",\n                            always_run=True, work_dir=work_dir)\n    out_bam = \"%s-final%s\" % utils.splitext_plus(ds_bam)\n    if not utils.file_exists(out_bam):\n        bam.merge([ds_bam, sr_bam, disc_bam], out_bam, data[\"config\"])\n    bam.index(out_bam, data[\"config\"])\n    return [out_bam]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfilter delly outputs based on read support and evidence.", "response": "def _delly_count_evidence_filter(in_file, data):\n    \"\"\"Filter delly outputs based on read support (DV) and evidence (split and paired).\n\n    We require DV > 4 and either both paired end and split read evidence or\n    5 or more evidence for either individually.\n    \"\"\"\n    filtname = \"DVSupport\"\n    filtdoc = \"FMT/DV < 4 || (SR < 1 && PE < 5) || (SR < 5 && PE < 1)\"\n    out_file = \"%s-filter%s\" % utils.splitext_plus(in_file)\n    cur_out_file = out_file.replace(\".vcf.gz\", \".vcf\")\n    if not utils.file_exists(out_file):\n        with file_transaction(data, cur_out_file) as tx_out_file:\n            with utils.open_gzipsafe(in_file) as in_handle:\n                with open(tx_out_file, \"w\") as out_handle:\n                    inp = vcf.Reader(in_handle, in_file)\n                    inp.filters[\"DVSupport\"] = vcf.parser._Filter(filtname, filtdoc)\n                    outp = vcf.Writer(out_handle, inp)\n                    for rec in inp:\n                        sr = rec.INFO.get(\"SR\", 0)\n                        pe = rec.INFO.get(\"PE\", 0)\n                        call = rec.samples[0].data\n                        dv = call.DV if hasattr(call, \"DV\") else 0\n                        if dv < 4 or (sr < 1 and pe < 5) or (sr < 5 and pe < 1):\n                            rec.add_filter(filtname)\n                        outp.write_record(rec)\n    if out_file.endswith(\".vcf.gz\"):\n        out_file = vcfutils.bgzip_and_index(cur_out_file, data[\"config\"])\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(items):\n    work_dir = utils.safe_makedir(os.path.join(items[0][\"dirs\"][\"work\"], \"structural\",\n                                               dd.get_sample_name(items[0]), \"delly\"))\n    # Add core request for delly\n    config = copy.deepcopy(items[0][\"config\"])\n    delly_config = utils.get_in(config, (\"resources\", \"delly\"), {})\n    delly_config[\"cores\"] = 1\n    config[\"resources\"][\"delly\"] = delly_config\n    parallel = {\"type\": \"local\", \"cores\": config[\"algorithm\"].get(\"num_cores\", 1),\n                \"progs\": [\"delly\"]}\n    work_bams = [dd.get_align_bam(d) for d in items]\n    ref_file = dd.get_ref_file(items[0])\n    exclude_file = _get_full_exclude_file(items, work_bams, work_dir)\n    bytype_vcfs = run_multicore(_run_delly,\n                                [(work_bams, chrom, ref_file, work_dir, items)\n                                 for chrom in sshared.get_sv_chroms(items, exclude_file)],\n                                config, parallel)\n    out_file = \"%s.vcf.gz\" % sshared.outname_from_inputs(bytype_vcfs)\n    combo_vcf = vcfutils.combine_variant_files(bytype_vcfs, out_file, ref_file, config)\n    out = []\n    upload_counts = collections.defaultdict(int)\n    for data in items:\n        if \"sv\" not in data:\n            data[\"sv\"] = []\n        base, ext = utils.splitext_plus(combo_vcf)\n        final_vcf = sshared.finalize_sv(combo_vcf, data, items)\n        if final_vcf:\n            delly_vcf = _delly_count_evidence_filter(final_vcf, data)\n            data[\"sv\"].append({\"variantcaller\": \"delly\", \"vrn_file\": delly_vcf,\n                               \"do_upload\": upload_counts[final_vcf] == 0,  # only upload a single file per batch\n                               \"exclude\": exclude_file})\n            upload_counts[final_vcf] += 1\n        out.append(data)\n    return out", "response": "Perform detection of structural variations with delly."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _oncofuse_tissue_arg_from_config(data):\n\n    \"\"\"Retrieve oncofuse arguments supplied through input configuration.\n    tissue_type is the library argument, which tells Oncofuse to use its\n    own pre-built gene expression libraries. There are four pre-built\n    libraries, corresponding to the four supported tissue types:\n    EPI (epithelial origin),\n    HEM (hematological origin),\n    MES (mesenchymal origin) and\n    AVG (average expression, if tissue source is unknown).\n    \"\"\"\n    SUPPORTED_TISSUE_TYPE = [\"EPI\", \"HEM\", \"MES\", \"AVG\"]\n    tissue_type = tz.get_in((\"metadata\", \"tissue\"), data, None)\n    if not tissue_type:\n        logger.info(\"Oncofuse: tissue type not set, using average expression (AVG).\")\n        tissue_type = \"AVG\"\n    elif tissue_type not in SUPPORTED_TISSUE_TYPE:\n        logger.info(\"Oncofuse: %s not a supported tissue type, using average \"\n                    \"expression (AVG).\" % tissue_type)\n        tissue_type = \"AVG\"\n    else:\n        logger.info(\"Oncofuse: using %s as tissue type.\" % tissue_type)\n    return tissue_type", "response": "Retrieve tissue type from configuration data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndisambiguating detected fusions based on alignments to another species.", "response": "def _disambiguate_star_fusion_junctions(star_junction_file, contamination_bam, disambig_out_file, data):\n    \"\"\" Disambiguate detected fusions based on alignments to another species.\n    \"\"\"\n    out_file = disambig_out_file\n    fusiondict = {}\n    with open(star_junction_file, \"r\") as in_handle:\n        for my_line in in_handle:\n            my_line_split = my_line.strip().split(\"\\t\")\n            if len(my_line_split) < 10:\n                continue\n            fusiondict[my_line_split[9]] = my_line.strip(\"\\n\")\n    with pysam.Samfile(contamination_bam, \"rb\") as samfile:\n        for my_read in samfile:\n            if my_read.is_unmapped or my_read.is_secondary:\n                continue\n            if my_read.qname in fusiondict:\n                fusiondict.pop(my_read.qname)\n    with file_transaction(data, out_file) as tx_out_file:\n        with open(tx_out_file, 'w') as myhandle:\n            for my_key in fusiondict:\n                print(fusiondict[my_key], file=myhandle)\n\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a colorscheme using Colorz.", "response": "def gen_colors(img):\n    \"\"\"Generate a colorscheme using Colorz.\"\"\"\n    # pylint: disable=not-callable\n    raw_colors = colorz.colorz(img, n=6, bold_add=0)\n    return [util.rgb_to_hex([*color[0]]) for color in raw_colors]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_desktop_env():\n    desktop = os.environ.get(\"XDG_CURRENT_DESKTOP\")\n    if desktop:\n        return desktop\n\n    desktop = os.environ.get(\"DESKTOP_SESSION\")\n    if desktop:\n        return desktop\n\n    desktop = os.environ.get(\"GNOME_DESKTOP_SESSION_ID\")\n    if desktop:\n        return \"GNOME\"\n\n    desktop = os.environ.get(\"MATE_DESKTOP_SESSION_ID\")\n    if desktop:\n        return \"MATE\"\n\n    desktop = os.environ.get(\"SWAYSOCK\")\n    if desktop:\n        return \"SWAY\"\n\n    desktop = os.environ.get(\"DESKTOP_STARTUP_ID\")\n    if desktop and \"awesome\" in desktop:\n        return \"AWESOME\"\n\n    return None", "response": "Identify the current running desktop environment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the wallpaper for non desktop environments.", "response": "def set_wm_wallpaper(img):\n    \"\"\"Set the wallpaper for non desktop environments.\"\"\"\n    if shutil.which(\"feh\"):\n        util.disown([\"feh\", \"--bg-fill\", img])\n\n    elif shutil.which(\"nitrogen\"):\n        util.disown([\"nitrogen\", \"--set-zoom-fill\", img])\n\n    elif shutil.which(\"bgs\"):\n        util.disown([\"bgs\", \"-z\", img])\n\n    elif shutil.which(\"hsetroot\"):\n        util.disown([\"hsetroot\", \"-fill\", img])\n\n    elif shutil.which(\"habak\"):\n        util.disown([\"habak\", \"-mS\", img])\n\n    elif shutil.which(\"display\"):\n        util.disown([\"display\", \"-backdrop\", \"-window\", \"root\", img])\n\n    else:\n        logging.error(\"No wallpaper setter found.\")\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the wallpaper for the desktop environment.", "response": "def set_desktop_wallpaper(desktop, img):\n    \"\"\"Set the wallpaper for the desktop environment.\"\"\"\n    desktop = str(desktop).lower()\n\n    if \"xfce\" in desktop or \"xubuntu\" in desktop:\n        # XFCE requires two commands since they differ between versions.\n        xfconf(\"/backdrop/screen0/monitor0/image-path\", img)\n        xfconf(\"/backdrop/screen0/monitor0/workspace0/last-image\", img)\n\n    elif \"muffin\" in desktop or \"cinnamon\" in desktop:\n        util.disown([\"gsettings\", \"set\",\n                     \"org.cinnamon.desktop.background\",\n                     \"picture-uri\", \"file://\" + urllib.parse.quote(img)])\n\n    elif \"gnome\" in desktop or \"unity\" in desktop:\n        util.disown([\"gsettings\", \"set\",\n                     \"org.gnome.desktop.background\",\n                     \"picture-uri\", \"file://\" + urllib.parse.quote(img)])\n\n    elif \"mate\" in desktop:\n        util.disown([\"gsettings\", \"set\", \"org.mate.background\",\n                     \"picture-filename\", img])\n\n    elif \"sway\" in desktop:\n        util.disown([\"swaymsg\", \"output\", \"*\", \"bg\", img, \"fill\"])\n\n    elif \"awesome\" in desktop:\n        util.disown([\"awesome-client\",\n                     \"require('gears').wallpaper.maximized('{img}')\"\n                     .format(**locals())])\n\n    else:\n        set_wm_wallpaper(img)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_mac_wallpaper(img):\n    db_file = \"Library/Application Support/Dock/desktoppicture.db\"\n    db_path = os.path.join(HOME, db_file)\n    img_dir, _ = os.path.split(img)\n\n    # Clear the existing picture data and write the image paths\n    sql = \"delete from data; \"\n    sql += \"insert into data values(\\\"%s\\\"); \" % img_dir\n    sql += \"insert into data values(\\\"%s\\\"); \" % img\n\n    # Set all monitors/workspaces to the selected image\n    sql += \"update preferences set data_id=2 where key=1 or key=2 or key=3; \"\n    sql += \"update preferences set data_id=1 where key=10 or key=20 or key=30;\"\n\n    subprocess.call([\"sqlite3\", db_path, sql])\n\n    # Kill the dock to fix issues with cached wallpapers.\n    # macOS caches wallpapers and if a wallpaper is set that shares\n    # the filename with a cached wallpaper, the cached wallpaper is\n    # used instead.\n    subprocess.call([\"killall\", \"Dock\"])", "response": "Set the wallpaper on macOS."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_win_wallpaper(img):\n    # There's a different command depending on the architecture\n    # of Windows. We check the PROGRAMFILES envar since using\n    # platform is unreliable.\n    if \"x86\" in os.environ[\"PROGRAMFILES\"]:\n        ctypes.windll.user32.SystemParametersInfoW(20, 0, img, 3)\n    else:\n        ctypes.windll.user32.SystemParametersInfoA(20, 0, img, 3)", "response": "Set the wallpaper on Windows."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(cache_dir=CACHE_DIR):\n    current_wall = os.path.join(cache_dir, \"wal\")\n\n    if os.path.isfile(current_wall):\n        return util.read_file(current_wall)[0]\n\n    return \"None\"", "response": "Get the current wallpaper."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save_file(data, export_file):\n    create_dir(os.path.dirname(export_file))\n\n    try:\n        with open(export_file, \"w\") as file:\n            file.write(data)\n    except PermissionError:\n        logging.warning(\"Couldn't write to %s.\", export_file)", "response": "Write data to a file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_file_json(data, export_file):\n    create_dir(os.path.dirname(export_file))\n\n    with open(export_file, \"w\") as file:\n        json.dump(data, file, indent=4)", "response": "Write data to a json file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef darken_color(color, amount):\n    color = [int(col * (1 - amount)) for col in hex_to_rgb(color)]\n    return rgb_to_hex(color)", "response": "Darken a hex color."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lighten_color(color, amount):\n    color = [int(col + (255 - col) * amount) for col in hex_to_rgb(color)]\n    return rgb_to_hex(color)", "response": "Lighten a hex color."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef blend_color(color, color2):\n    r1, g1, b1 = hex_to_rgb(color)\n    r2, g2, b2 = hex_to_rgb(color2)\n\n    r3 = int(0.5 * r1 + 0.5 * r2)\n    g3 = int(0.5 * g1 + 0.5 * g2)\n    b3 = int(0.5 * b1 + 0.5 * b2)\n\n    return rgb_to_hex((r3, g3, b3))", "response": "Blend two colors together."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef saturate_color(color, amount):\n    r, g, b = hex_to_rgb(color)\n    r, g, b = [x/255.0 for x in (r, g, b)]\n    h, l, s = colorsys.rgb_to_hls(r, g, b)\n    s = amount\n    r, g, b = colorsys.hls_to_rgb(h, l, s)\n    r, g, b = [x*255.0 for x in (r, g, b)]\n\n    return rgb_to_hex((int(r), int(g), int(b)))", "response": "Saturate a hex color."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef disown(cmd):\n    subprocess.Popen(cmd,\n                     stdout=subprocess.DEVNULL,\n                     stderr=subprocess.DEVNULL)", "response": "Call a system command in the background and hide it s output."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if process is running by name.", "response": "def get_pid(name):\n    \"\"\"Check if process is running by name.\"\"\"\n    if not shutil.which(\"pidof\"):\n        return False\n\n    try:\n        subprocess.check_output([\"pidof\", \"-s\", name])\n    except subprocess.CalledProcessError:\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting all images in a directory.", "response": "def get_image_dir(img_dir):\n    \"\"\"Get all images in a directory.\"\"\"\n    current_wall = wallpaper.get()\n    current_wall = os.path.basename(current_wall)\n\n    file_types = (\".png\", \".jpg\", \".jpeg\", \".jpe\", \".gif\")\n\n    return [img.name for img in os.scandir(img_dir)\n            if img.name.lower().endswith(file_types)], current_wall"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npick a random image file from a directory.", "response": "def get_random_image(img_dir):\n    \"\"\"Pick a random image file from a directory.\"\"\"\n    images, current_wall = get_image_dir(img_dir)\n\n    if len(images) > 2 and current_wall in images:\n        images.remove(current_wall)\n\n    elif not images:\n        logging.error(\"No images found in directory.\")\n        sys.exit(1)\n\n    random.shuffle(images)\n    return os.path.join(img_dir, images[0])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the next image in a dir.", "response": "def get_next_image(img_dir):\n    \"\"\"Get the next image in a dir.\"\"\"\n    images, current_wall = get_image_dir(img_dir)\n    images.sort(key=lambda img: [int(x) if x.isdigit() else x\n                                 for x in re.split('([0-9]+)', img)])\n\n    try:\n        next_index = images.index(current_wall) + 1\n\n    except ValueError:\n        next_index = 0\n\n    try:\n        image = images[next_index]\n\n    except IndexError:\n        image = images[0]\n\n    return os.path.join(img_dir, image)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts list of colors to pywal format.", "response": "def colors_to_dict(colors, img):\n    \"\"\"Convert list of colors to pywal format.\"\"\"\n    return {\n        \"wallpaper\": img,\n        \"alpha\": util.Color.alpha_num,\n\n        \"special\": {\n            \"background\": colors[0],\n            \"foreground\": colors[15],\n            \"cursor\": colors[15]\n        },\n\n        \"colors\": {\n            \"color0\": colors[0],\n            \"color1\": colors[1],\n            \"color2\": colors[2],\n            \"color3\": colors[3],\n            \"color4\": colors[4],\n            \"color5\": colors[5],\n            \"color6\": colors[6],\n            \"color7\": colors[7],\n            \"color8\": colors[8],\n            \"color9\": colors[9],\n            \"color10\": colors[10],\n            \"color11\": colors[11],\n            \"color12\": colors[12],\n            \"color13\": colors[13],\n            \"color14\": colors[14],\n            \"color15\": colors[15]\n        }\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cache_fname(img, backend, light, cache_dir, sat=\"\"):\n    color_type = \"light\" if light else \"dark\"\n    file_name = re.sub(\"[/|\\\\|.]\", \"_\", img)\n    file_size = os.path.getsize(img)\n\n    file_parts = [file_name, color_type, backend,\n                  sat, file_size, __cache_version__]\n    return [cache_dir, \"schemes\", \"%s_%s_%s_%s_%s_%s.json\" % (*file_parts,)]", "response": "Create the cache file name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfigure out which backend to use.", "response": "def get_backend(backend):\n    \"\"\"Figure out which backend to use.\"\"\"\n    if backend == \"random\":\n        backends = list_backends()\n        random.shuffle(backends)\n        return backends[0]\n\n    return backend"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a palette from the colors.", "response": "def palette():\n    \"\"\"Generate a palette from the colors.\"\"\"\n    for i in range(0, 16):\n        if i % 8 == 0:\n            print()\n\n        if i > 7:\n            i = \"8;5;%s\" % i\n\n        print(\"\\033[4%sm%s\\033[0m\" % (i, \" \" * (80 // 20)), end=\"\")\n\n    print(\"\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gen_colors(img):\n    palette = Haishoku.getPalette(img)\n    return [util.rgb_to_hex(col[1]) for col in palette]", "response": "Generate a colorscheme using Colorz."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gen_colors(img):\n    color_cmd = ColorThief(img).get_palette\n\n    for i in range(0, 10, 1):\n        raw_colors = color_cmd(color_count=8 + i)\n\n        if len(raw_colors) >= 8:\n            break\n\n        elif i == 10:\n            logging.error(\"ColorThief couldn't generate a suitable palette.\")\n            sys.exit(1)\n\n        else:\n            logging.warning(\"ColorThief couldn't generate a palette.\")\n            logging.warning(\"Trying a larger palette size %s\", 8 + i)\n\n    return [util.rgb_to_hex(color) for color in raw_colors]", "response": "Loop until 16 colors are generated."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_out():\n    dark_themes = [theme.name.replace(\".json\", \"\")\n                   for theme in list_themes()]\n    ligh_themes = [theme.name.replace(\".json\", \"\")\n                   for theme in list_themes(dark=False)]\n    user_themes = [theme.name.replace(\".json\", \"\")\n                   for theme in list_themes_user()]\n\n    if user_themes:\n        print(\"\\033[1;32mUser Themes\\033[0m:\")\n        print(\" -\", \"\\n - \".join(sorted(user_themes)))\n\n    print(\"\\033[1;32mDark Themes\\033[0m:\")\n    print(\" -\", \"\\n - \".join(sorted(dark_themes)))\n\n    print(\"\\033[1;32mLight Themes\\033[0m:\")\n    print(\" -\", \"\\n - \".join(sorted(ligh_themes)))\n\n    print(\"\\033[1;32mExtra\\033[0m:\")\n    print(\" - random (select a random dark theme)\")\n    print(\" - random_dark (select a random dark theme)\")\n    print(\" - random_light (select a random light theme)\")", "response": "List all themes in a pretty format."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_themes(dark=True):\n    dark = \"dark\" if dark else \"light\"\n    themes = os.scandir(os.path.join(MODULE_DIR, \"colorschemes\", dark))\n    return [t for t in themes if os.path.isfile(t.path)]", "response": "List all installed theme files."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists user theme files.", "response": "def list_themes_user():\n    \"\"\"List user theme files.\"\"\"\n    themes = [*os.scandir(os.path.join(CONF_DIR, \"colorschemes/dark/\")),\n              *os.scandir(os.path.join(CONF_DIR, \"colorschemes/light/\"))]\n    return [t for t in themes if os.path.isfile(t.path)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert terminal. sexy json schema to wal.", "response": "def terminal_sexy_to_wal(data):\n    \"\"\"Convert terminal.sexy json schema to wal.\"\"\"\n    data[\"colors\"] = {}\n    data[\"special\"] = {\n        \"foreground\": data[\"foreground\"],\n        \"background\": data[\"background\"],\n        \"cursor\": data[\"color\"][9]\n    }\n\n    for i, color in enumerate(data[\"color\"]):\n        data[\"colors\"][\"color%s\" % i] = color\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(theme_file):\n    data = util.read_file_json(theme_file)\n\n    if \"wallpaper\" not in data:\n        data[\"wallpaper\"] = \"None\"\n\n    if \"alpha\" not in data:\n        data[\"alpha\"] = util.Color.alpha_num\n\n    # Terminal.sexy format.\n    if \"color\" in data:\n        data = terminal_sexy_to_wal(data)\n\n    return data", "response": "Parse the theme file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_random_theme(dark=True):\n    themes = [theme.path for theme in list_themes(dark)]\n    random.shuffle(themes)\n    return themes[0]", "response": "Get a random theme file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef file(input_file, light=False):\n    util.create_dir(os.path.join(CONF_DIR, \"colorschemes/light/\"))\n    util.create_dir(os.path.join(CONF_DIR, \"colorschemes/dark/\"))\n\n    theme_name = \".\".join((input_file, \"json\"))\n    bri = \"light\" if light else \"dark\"\n\n    user_theme_file = os.path.join(CONF_DIR, \"colorschemes\", bri, theme_name)\n    theme_file = os.path.join(MODULE_DIR, \"colorschemes\", bri, theme_name)\n\n    # Find the theme file.\n    if input_file in (\"random\", \"random_dark\"):\n        theme_file = get_random_theme()\n\n    elif input_file == \"random_light\":\n        theme_file = get_random_theme(light)\n\n    elif os.path.isfile(user_theme_file):\n        theme_file = user_theme_file\n\n    elif os.path.isfile(input_file):\n        theme_file = input_file\n\n    # Parse the theme file.\n    if os.path.isfile(theme_file):\n        logging.info(\"Set theme to \\033[1;37m%s\\033[0m.\",\n                     os.path.basename(theme_file))\n        return parse(theme_file)\n\n    logging.error(\"No %s colorscheme file found.\", bri)\n    logging.error(\"Try adding   '-l' to set light themes.\")\n    logging.error(\"Try removing '-l' to set dark themes.\")\n    sys.exit(1)", "response": "Imports colorscheme from json file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalls Imagemagick to generate a scheme.", "response": "def imagemagick(color_count, img, magick_command):\n    \"\"\"Call Imagemagick to generate a scheme.\"\"\"\n    flags = [\"-resize\", \"25%\", \"-colors\", str(color_count),\n             \"-unique-colors\", \"txt:-\"]\n    img += \"[0]\"\n\n    return subprocess.check_output([*magick_command, img, *flags]).splitlines()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef has_im():\n    if shutil.which(\"magick\"):\n        return [\"magick\", \"convert\"]\n\n    if shutil.which(\"convert\"):\n        return [\"convert\"]\n\n    logging.error(\"Imagemagick wasn't found on your system.\")\n    logging.error(\"Try another backend. (wal --backend)\")\n    sys.exit(1)", "response": "Check to see if the user has im installed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gen_colors(img):\n    magick_command = has_im()\n\n    for i in range(0, 20, 1):\n        raw_colors = imagemagick(16 + i, img, magick_command)\n\n        if len(raw_colors) > 16:\n            break\n\n        elif i == 19:\n            logging.error(\"Imagemagick couldn't generate a suitable palette.\")\n            sys.exit(1)\n\n        else:\n            logging.warning(\"Imagemagick couldn't generate a palette.\")\n            logging.warning(\"Trying a larger palette size %s\", 16 + i)\n\n    return [re.search(\"#.{6}\", str(col)).group(0) for col in raw_colors[1:]]", "response": "Format the output from imagemagick into a list\n       of hex colors."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadjusting the generated colors and store them in a dict that we will later save in json format.", "response": "def adjust(colors, light):\n    \"\"\"Adjust the generated colors and store them in a dict that\n       we will later save in json format.\"\"\"\n    raw_colors = colors[:1] + colors[8:16] + colors[8:-1]\n\n    # Manually adjust colors.\n    if light:\n        for color in raw_colors:\n            color = util.saturate_color(color, 0.5)\n\n        raw_colors[0] = util.lighten_color(colors[-1], 0.85)\n        raw_colors[7] = colors[0]\n        raw_colors[8] = util.darken_color(colors[-1], 0.4)\n        raw_colors[15] = colors[0]\n\n    else:\n        # Darken the background color slightly.\n        if raw_colors[0][1] != \"0\":\n            raw_colors[0] = util.darken_color(raw_colors[0], 0.40)\n\n        raw_colors[7] = util.blend_color(raw_colors[7], \"#EEEEEE\")\n        raw_colors[8] = util.darken_color(raw_colors[7], 0.30)\n        raw_colors[15] = util.blend_color(raw_colors[15], \"#EEEEEE\")\n\n    return raw_colors"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_args():\n    description = \"wal - Generate colorschemes on the fly\"\n    arg = argparse.ArgumentParser(description=description)\n\n    arg.add_argument(\"-a\", metavar=\"\\\"alpha\\\"\",\n                     help=\"Set terminal background transparency. \\\n                           *Only works in URxvt*\")\n\n    arg.add_argument(\"-b\", metavar=\"background\",\n                     help=\"Custom background color to use.\")\n\n    arg.add_argument(\"--backend\", metavar=\"backend\",\n                     help=\"Which color backend to use. \\\n                           Use 'wal --backend' to list backends.\",\n                     const=\"list_backends\", type=str, nargs=\"?\")\n\n    arg.add_argument(\"--theme\", \"-f\", metavar=\"/path/to/file or theme_name\",\n                     help=\"Which colorscheme file to use. \\\n                           Use 'wal --theme' to list builtin themes.\",\n                     const=\"list_themes\", nargs=\"?\")\n\n    arg.add_argument(\"--iterative\", action=\"store_true\",\n                     help=\"When pywal is given a directory as input and this \"\n                          \"flag is used: Go through the images in order \"\n                          \"instead of shuffled.\")\n\n    arg.add_argument(\"--saturate\", metavar=\"0.0-1.0\",\n                     help=\"Set the color saturation.\")\n\n    arg.add_argument(\"--preview\", action=\"store_true\",\n                     help=\"Print the current color palette.\")\n\n    arg.add_argument(\"--vte\", action=\"store_true\",\n                     help=\"Fix text-artifacts printed in VTE terminals.\")\n\n    arg.add_argument(\"-c\", action=\"store_true\",\n                     help=\"Delete all cached colorschemes.\")\n\n    arg.add_argument(\"-i\", metavar=\"\\\"/path/to/img.jpg\\\"\",\n                     help=\"Which image or directory to use.\")\n\n    arg.add_argument(\"-l\", action=\"store_true\",\n                     help=\"Generate a light colorscheme.\")\n\n    arg.add_argument(\"-n\", action=\"store_true\",\n                     help=\"Skip setting the wallpaper.\")\n\n    arg.add_argument(\"-o\", metavar=\"\\\"script_name\\\"\", action=\"append\",\n                     help=\"External script to run after \\\"wal\\\".\")\n\n    arg.add_argument(\"-q\", action=\"store_true\",\n                     help=\"Quiet mode, don\\'t print anything.\")\n\n    arg.add_argument(\"-r\", action=\"store_true\",\n                     help=\"'wal -r' is deprecated: Use \\\n                           (cat ~/.cache/wal/sequences &) instead.\")\n\n    arg.add_argument(\"-R\", action=\"store_true\",\n                     help=\"Restore previous colorscheme.\")\n\n    arg.add_argument(\"-s\", action=\"store_true\",\n                     help=\"Skip changing colors in terminals.\")\n\n    arg.add_argument(\"-t\", action=\"store_true\",\n                     help=\"Skip changing colors in tty.\")\n\n    arg.add_argument(\"-v\", action=\"store_true\",\n                     help=\"Print \\\"wal\\\" version.\")\n\n    arg.add_argument(\"-e\", action=\"store_true\",\n                     help=\"Skip reloading gtk/xrdb/i3/sway/polybar\")\n\n    return arg", "response": "Get the script arguments."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprocess args that exit.", "response": "def parse_args_exit(parser):\n    \"\"\"Process args that exit.\"\"\"\n    args = parser.parse_args()\n\n    if len(sys.argv) <= 1:\n        parser.print_help()\n        sys.exit(1)\n\n    if args.v:\n        parser.exit(0, \"wal %s\\n\" % __version__)\n\n    if args.preview:\n        print(\"Current colorscheme:\", sep='')\n        colors.palette()\n        sys.exit(0)\n\n    if args.i and args.theme:\n        parser.error(\"Conflicting arguments -i and -f.\")\n\n    if args.r:\n        reload.colors()\n        sys.exit(0)\n\n    if args.c:\n        scheme_dir = os.path.join(CACHE_DIR, \"schemes\")\n        shutil.rmtree(scheme_dir, ignore_errors=True)\n        sys.exit(0)\n\n    if not args.i and \\\n       not args.theme and \\\n       not args.R and \\\n       not args.backend:\n        parser.error(\"No input specified.\\n\"\n                     \"--backend, --theme, -i or -R are required.\")\n\n    if args.theme == \"list_themes\":\n        theme.list_out()\n        sys.exit(0)\n\n    if args.backend == \"list_backends\":\n        print(\"\\n - \".join([\"\\033[1;32mBackends\\033[0m:\",\n                            *colors.list_backends()]))\n        sys.exit(0)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a hex color to a special sequence.", "response": "def set_special(index, color, iterm_name=\"h\", alpha=100):\n    \"\"\"Convert a hex color to a special sequence.\"\"\"\n    if OS == \"Darwin\" and iterm_name:\n        return \"\\033]P%s%s\\033\\\\\" % (iterm_name, color.strip(\"#\"))\n\n    if index in [11, 708] and alpha != \"100\":\n        return \"\\033]%s;[%s]%s\\033\\\\\" % (index, alpha, color)\n\n    return \"\\033]%s;%s\\033\\\\\" % (index, color)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_color(index, color):\n    if OS == \"Darwin\" and index < 20:\n        return \"\\033]P%1x%s\\033\\\\\" % (index, color.strip(\"#\"))\n\n    return \"\\033]4;%s;%s\\033\\\\\" % (index, color)", "response": "Convert a hex color to a text color sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates the escape sequences.", "response": "def create_sequences(colors, vte_fix=False):\n    \"\"\"Create the escape sequences.\"\"\"\n    alpha = colors[\"alpha\"]\n\n    # Colors 0-15.\n    sequences = [set_color(index, colors[\"colors\"][\"color%s\" % index])\n                 for index in range(16)]\n\n    # Special colors.\n    # Source: https://goo.gl/KcoQgP\n    # 10 = foreground, 11 = background, 12 = cursor foregound\n    # 13 = mouse foreground, 708 = background border color.\n    sequences.extend([\n        set_special(10, colors[\"special\"][\"foreground\"], \"g\"),\n        set_special(11, colors[\"special\"][\"background\"], \"h\", alpha),\n        set_special(12, colors[\"special\"][\"cursor\"], \"l\"),\n        set_special(13, colors[\"special\"][\"foreground\"], \"j\"),\n        set_special(17, colors[\"special\"][\"foreground\"], \"k\"),\n        set_special(19, colors[\"special\"][\"background\"], \"m\"),\n        set_color(232, colors[\"special\"][\"background\"]),\n        set_color(256, colors[\"special\"][\"foreground\"])\n    ])\n\n    if not vte_fix:\n        sequences.extend(\n            set_special(708, colors[\"special\"][\"background\"], \"\", alpha)\n        )\n\n    if OS == \"Darwin\":\n        sequences += set_iterm_tab_color(colors[\"special\"][\"background\"])\n\n    return \"\".join(sequences)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send(colors, cache_dir=CACHE_DIR, to_send=True, vte_fix=False):\n    if OS == \"Darwin\":\n        tty_pattern = \"/dev/ttys00[0-9]*\"\n\n    else:\n        tty_pattern = \"/dev/pts/[0-9]*\"\n\n    sequences = create_sequences(colors, vte_fix)\n\n    # Writing to \"/dev/pts/[0-9] lets you send data to open terminals.\n    if to_send:\n        for term in glob.glob(tty_pattern):\n            util.save_file(sequences, term)\n\n    util.save_file(sequences, os.path.join(cache_dir, \"sequences\"))\n    logging.info(\"Set terminal colors.\")", "response": "Send colors to all open terminals."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread template file substitute markers and save the file elsewhere.", "response": "def template(colors, input_file, output_file=None):\n    \"\"\"Read template file, substitute markers and\n       save the file elsewhere.\"\"\"\n    template_data = util.read_file_raw(input_file)\n\n    try:\n        template_data = \"\".join(template_data).format(**colors)\n    except ValueError:\n        logging.error(\"Syntax error in template file '%s'.\", input_file)\n        return\n\n    util.save_file(template_data, output_file)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef flatten_colors(colors):\n    all_colors = {\"wallpaper\": colors[\"wallpaper\"],\n                  \"alpha\": colors[\"alpha\"],\n                  **colors[\"special\"],\n                  **colors[\"colors\"]}\n    return {k: util.Color(v) for k, v in all_colors.items()}", "response": "Prepare colors to be exported.\n       Flatten dicts and convert colors to util. Color"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef every(colors, output_dir=CACHE_DIR):\n    colors = flatten_colors(colors)\n    template_dir = os.path.join(MODULE_DIR, \"templates\")\n    template_dir_user = os.path.join(CONF_DIR, \"templates\")\n    util.create_dir(template_dir_user)\n\n    join = os.path.join  # Minor optimization.\n    for file in [*os.scandir(template_dir),\n                 *os.scandir(template_dir_user)]:\n        if file.name != \".DS_Store\" and not file.name.endswith(\".swp\"):\n            template(colors, file.path, join(output_dir, file.name))\n\n    logging.info(\"Exported all files.\")\n    logging.info(\"Exported all user files.\")", "response": "Export all template files."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexporting a single template file.", "response": "def color(colors, export_type, output_file=None):\n    \"\"\"Export a single template file.\"\"\"\n    all_colors = flatten_colors(colors)\n\n    template_name = get_export_type(export_type)\n    template_file = os.path.join(MODULE_DIR, \"templates\", template_name)\n    output_file = output_file or os.path.join(CACHE_DIR, template_name)\n\n    if os.path.isfile(template_file):\n        template(all_colors, template_file, output_file)\n        logging.info(\"Exported %s.\", export_type)\n    else:\n        logging.warning(\"Template '%s' doesn't exist.\", export_type)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tty(tty_reload):\n    tty_script = os.path.join(CACHE_DIR, \"colors-tty.sh\")\n    term = os.environ.get(\"TERM\")\n\n    if tty_reload and term == \"linux\":\n        subprocess.Popen([\"sh\", tty_script])", "response": "Load colors in tty."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef xrdb(xrdb_files=None):\n    xrdb_files = xrdb_files or \\\n        [os.path.join(CACHE_DIR, \"colors.Xresources\")]\n\n    if shutil.which(\"xrdb\") and OS != \"Darwin\":\n        for file in xrdb_files:\n            subprocess.run([\"xrdb\", \"-merge\", \"-quiet\", file])", "response": "Merge the colors into the X db so new terminals use them."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gtk():\n    # Here we call a Python 2 script to reload the GTK themes.\n    # This is done because the Python 3 GTK/Gdk libraries don't\n    # provide a way of doing this.\n    if shutil.which(\"python2\"):\n        gtk_reload = os.path.join(MODULE_DIR, \"scripts\", \"gtk_reload.py\")\n        util.disown([\"python2\", gtk_reload])\n\n    else:\n        logging.warning(\"GTK2 reload support requires Python 2.\")", "response": "Reload GTK theme on the fly."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef open_resource(self, filename, mode='r'):\n        filename = os.path.abspath(filename)\n        self._emit_resource_added(filename)  # TODO: maybe non-blocking?\n        return open(filename, mode)", "response": "Open a file and save it as a resource."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_resource(self, filename):\n        filename = os.path.abspath(filename)\n        self._emit_resource_added(filename)", "response": "Add a file to the set of available resources."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_artifact(\n            self,\n            filename,\n            name=None,\n            metadata=None,\n            content_type=None,\n    ):\n        \"\"\"Add a file as an artifact.\n\n        In Sacred terminology an artifact is a file produced by the experiment\n        run. In case of a MongoObserver that means storing the file in the\n        database.\n\n        See also :py:meth:`sacred.Experiment.add_artifact`.\n\n        Parameters\n        ----------\n        filename : str\n            name of the file to be stored as artifact\n        name : str, optional\n            optionally set the name of the artifact.\n            Defaults to the filename.\n        metadata: dict\n            optionally attach metadata to the artifact.\n            This only has an effect when using the MongoObserver.\n        content_type: str, optional\n            optionally attach a content-type to the artifact.\n            This only has an effect when using the MongoObserver.\n        \"\"\"\n        filename = os.path.abspath(filename)\n        name = os.path.basename(filename) if name is None else name\n        self._emit_artifact_added(name, filename, metadata, content_type)", "response": "Add an artifact to the internal data store."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef log_scalar(self, metric_name, value, step=None):\n        # Method added in change https://github.com/chovanecm/sacred/issues/4\n        # The same as Experiment.log_scalar (if something changes,\n        # update the docstring too!)\n\n        return self._metrics.log_scalar_metric(metric_name, value, step)", "response": "Log a scalar measurement."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_best_match(path, prefixes):\n    path_parts = path.split('.')\n    for p in prefixes:\n        if len(p) <= len(path_parts) and p == path_parts[:len(p)]:\n            return '.'.join(p), '.'.join(path_parts[len(p):])\n    return '', path", "response": "Find the Ingredient that shares the longest prefix with path."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a string representation of the object.", "response": "def _non_unicode_repr(objekt, context, maxlevels, level):\n    \"\"\"\n    Used to override the pprint format method to get rid of unicode prefixes.\n\n    E.g.: 'John' instead of u'John'.\n    \"\"\"\n    repr_string, isreadable, isrecursive = pprint._safe_repr(objekt, context,\n                                                             maxlevels, level)\n    if repr_string.startswith('u\"') or repr_string.startswith(\"u'\"):\n        repr_string = repr_string[1:]\n    return repr_string, isreadable, isrecursive"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef print_config(_run):\n    final_config = _run.config\n    config_mods = _run.config_modifications\n    print(_format_config(final_config, config_mods))", "response": "Print the updated configuration and exit."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a command function that prints the available named configs for the given ingredient and exits.", "response": "def print_named_configs(ingredient):\n    \"\"\"\n    Returns a command function that prints the available named configs for the\n     ingredient and all sub-ingredients and exits.\n\n     The output is highlighted:\n       white: config names\n       grey:  doc\n     \"\"\"\n\n    def print_named_configs():\n        \"\"\"Print the available named configs and exit.\"\"\"\n        named_configs = OrderedDict(ingredient.gather_named_configs())\n        print(_format_named_configs(named_configs, 2))\n\n    return print_named_configs"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the help text for a command.", "response": "def help_for_command(command):\n    \"\"\"Get the help text (signature + docstring) for a command (function).\"\"\"\n    help_text = pydoc.text.document(command)\n    # remove backspaces\n    return re.subn('.\\\\x08', '', help_text)[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef print_dependencies(_run):\n    print('Dependencies:')\n    for dep in _run.experiment_info['dependencies']:\n        pack, _, version = dep.partition('==')\n        print('  {:<20} == {}'.format(pack, version))\n\n    print('\\nSources:')\n    for source, digest in _run.experiment_info['sources']:\n        print('  {:<43}  {}'.format(source, digest))\n\n    if _run.experiment_info['repositories']:\n        repos = _run.experiment_info['repositories']\n        print('\\nVersion Control:')\n        for repo in repos:\n            mod = COLOR_DIRTY + 'M' if repo['dirty'] else ' '\n            print('{} {:<43}  {}'.format(mod, repo['url'], repo['commit']) +\n                  ENDC)\n    print('')", "response": "Print the detected source - files and dependencies."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving the updated configuration to a file.", "response": "def save_config(_config, _log, config_filename='config.json'):\n    \"\"\"\n    Store the updated configuration in a file.\n\n    By default uses the filename \"config.json\", but that can be changed by\n    setting the config_filename config entry.\n    \"\"\"\n    if 'config_filename' in _config:\n        del _config['config_filename']\n    _log.info('Saving config to \"{}\"'.format(config_filename))\n    save_config_file(flatten(_config), config_filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef log_metrics(self, metrics_by_name, info):\n        try:\n            metrics_path = os.path.join(self.dir, \"metrics.json\")\n            with open(metrics_path, 'r') as f:\n                saved_metrics = json.load(f)\n        except IOError:\n            # We haven't recorded anything yet. Start Collecting.\n            saved_metrics = {}\n\n        for metric_name, metric_ptr in metrics_by_name.items():\n\n            if metric_name not in saved_metrics:\n                saved_metrics[metric_name] = {\"values\": [],\n                                              \"steps\": [],\n                                              \"timestamps\": []}\n\n            saved_metrics[metric_name][\"values\"] += metric_ptr[\"values\"]\n            saved_metrics[metric_name][\"steps\"] += metric_ptr[\"steps\"]\n\n            # Manually convert them to avoid passing a datetime dtype handler\n            # when we're trying to convert into json.\n            timestamps_norm = [ts.isoformat()\n                               for ts in metric_ptr[\"timestamps\"]]\n            saved_metrics[metric_name][\"timestamps\"] += timestamps_norm\n\n        self.save_json(saved_metrics, 'metrics.json')", "response": "Store new measurements into metrics. json."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main(self, function):\n        captured = self.command(function)\n        self.default_command = captured.__name__\n        return captured", "response": "Decorator to define the main function of an experiment."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef automain(self, function):\n        captured = self.main(function)\n        if function.__module__ == '__main__':\n            # Ensure that automain is not used in interactive mode.\n            import inspect\n            main_filename = inspect.getfile(function)\n            if (main_filename == '<stdin>' or\n                    (main_filename.startswith('<ipython-input-') and\n                     main_filename.endswith('>'))):\n                raise RuntimeError('Cannot use @ex.automain decorator in '\n                                   'interactive mode. Use @ex.main instead.')\n\n            self.run_commandline()\n        return captured", "response": "Decorator that defines and runs the main function of the experiment."}
